<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüé® üîî üõ¢Ô∏è Ceph. Anatom√≠a del desastre üîΩ üõÄüèø üëáüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ceph es un almacenamiento de objetos dise√±ado para ayudar a construir un cl√∫ster de conmutaci√≥n por error. A√∫n as√≠, las fallas suceden. Todos los que ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph. Anatom√≠a del desastre</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/431536/">  Ceph es un almacenamiento de objetos dise√±ado para ayudar a construir un cl√∫ster de conmutaci√≥n por error.  A√∫n as√≠, las fallas suceden.  Todos los que trabajan con Ceph conocen la leyenda sobre CloudMouse o Rosreestr.  Desafortunadamente, no se acostumbra compartir experiencias negativas con nosotros, las causas de los fracasos a menudo se silencian y no permiten que las generaciones futuras aprendan de los errores de los dem√°s. <br><br>  Bueno, configuremos un grupo de prueba, pero cercano al real, y analicemos el desastre por huesos.  Mediremos todas las reducciones de rendimiento, encontraremos p√©rdidas de memoria y analizaremos el proceso de recuperaci√≥n del servicio.  Y todo esto bajo el liderazgo de Artemy Kapitula, quien pas√≥ casi un a√±o estudiando las trampas, hizo que el rendimiento del cl√∫ster fallara a cero y la latencia no saltara a valores indecentes.  Y obtuve un gr√°fico rojo, que es mucho mejor. <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>  A continuaci√≥n, encontrar√° una versi√≥n en video y texto de uno de los mejores informes de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DevOpsConf Russia</a> 2018. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_fWYUl2QsoI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  <strong>Sobre el orador:</strong> Artemy Kapitula, arquitecto del sistema RCNTEC.  La compa√±√≠a ofrece soluciones de telefon√≠a IP (colaboraci√≥n, organizaci√≥n de una oficina remota, sistemas de almacenamiento definidos por software y sistemas de administraci√≥n / distribuci√≥n de energ√≠a).  La compa√±√≠a trabaja principalmente en el sector empresarial, por lo tanto, no es muy conocida en el mercado DevOps.  Sin embargo, se ha acumulado cierta experiencia con Ceph, que en muchos proyectos se utiliza como elemento b√°sico de la infraestructura de almacenamiento. <br><br>  <strong>Ceph es un repositorio definido por software con muchos componentes de software.</strong> <br><img src="https://habrastorage.org/webt/dw/ow/hm/dwowhmqvjfugd0u-ljhhz3fy2ji.png"><br><br>  En el diagrama: <br><br><ul><li>  El nivel superior es la red interna del cl√∫ster a trav√©s de la cual se comunica el cl√∫ster; </li><li>  El nivel inferior, en realidad Ceph, es un conjunto de demonios internos de Ceph (MON, MDS y OSD) que almacenan datos. </li></ul><br>  Como regla general, todos los datos se replican. En el diagrama, seleccion√© deliberadamente tres grupos, cada uno con tres OSD, y cada uno de estos grupos generalmente contiene una r√©plica de datos.  Como resultado, los datos se almacenan en tres copias. <br><br>  Una red de cl√∫ster de nivel superior es la red a trav√©s de la cual los clientes Ceph acceden a los datos.  A trav√©s de √©l, los clientes se comunican con el monitor, con MDS (qui√©n lo necesita) y con OSD.  Cada cliente trabaja con cada OSD y con cada monitor de forma independiente.  Por lo tanto, el <strong>sistema carece de un solo punto de falla</strong> , lo cual es muy agradable. <br><br><h2>  Los clientes <br></h2><br>  ‚óè clientes S3 <br><br>  S3 es una API para HTTP.  Los clientes S3 trabajan a trav√©s de HTTP y se conectan a los componentes de Ceph Rados Gateway (RGW).  Casi siempre se comunican con un componente a trav√©s de una red dedicada.  Esta red (la llam√© red S3) usa solo HTTP, las excepciones son raras. <br><br>  ‚óè Hipervisor con m√°quinas virtuales. <br><br>  Este grupo de clientes es de uso frecuente.  Trabajan con monitores y con OSD, de los cuales reciben informaci√≥n general sobre el estado del cl√∫ster y la distribuci√≥n de datos.  Para obtener datos, estos clientes van directamente a los demonios OSD a trav√©s de la red p√∫blica Cluster. <br><br>  ‚óè clientes RBD <br><br>  Tambi√©n hay hosts f√≠sicos de metales BR, que generalmente son Linux.  Son clientes RBD y obtienen acceso a im√°genes almacenadas dentro de un cl√∫ster Ceph (im√°genes de disco de m√°quina virtual). <br><br>  ‚óè clientes CephFS <br><br>  El cuarto grupo de clientes, que no muchos a√∫n tienen, pero que son de creciente inter√©s, son los clientes del sistema de archivos de cl√∫ster CephFS.  El sistema de cl√∫ster CephFS se puede montar simult√°neamente desde muchos nodos, y todos los nodos obtienen acceso a los mismos datos, trabajando con cada OSD.  Es decir, no hay Gateways como tales (Samba, NFS y otros).  El problema es que dicho cliente solo puede ser Linux y una versi√≥n bastante moderna. <br><img src="https://habrastorage.org/webt/fw/nm/xc/fwnmxcaiig0yy6tkofrljqri3ck.png"><br><br>  Nuestra empresa trabaja en el mercado corporativo, y all√≠ la bola est√° regida por ESXi, HyperV y otros.  En consecuencia, el cl√∫ster Ceph, que de alguna manera se usa en el sector corporativo, debe soportar las t√©cnicas apropiadas.  Esto no fue suficiente para nosotros en Ceph, por lo que tuvimos que refinar y expandir el cl√∫ster Ceph con nuestros componentes, de hecho, construimos algo m√°s que Ceph, nuestra propia plataforma para almacenar datos. <br><br>  Adem√°s, los clientes del sector corporativo no est√°n en Linux, pero la mayor√≠a de ellos Windows, ocasionalmente Mac OS, no pueden ir al cl√∫ster Ceph.  Tienen que atravesar alg√∫n tipo de puertas de enlace, que en este caso se convierten en cuellos de botella. <br><br>  Tuvimos que agregar todos estos componentes y obtuvimos un cl√∫ster ligeramente m√°s ancho. <br><img src="https://habrastorage.org/webt/p2/tg/j2/p2tgj2qtpzrzsnst5bophkclywi.png"><br><br>  Tenemos dos componentes centrales: el <strong>grupo SCSI Gateways</strong> , que proporciona acceso a datos en un cl√∫ster Ceph a trav√©s de FibreChannel o iSCSI.  Estos componentes se utilizan para conectar HyperV y ESXi a un cl√∫ster Ceph.  Los clientes de PROXMOX todav√≠a trabajan a su manera, a trav√©s de RBD. <br><br>  No permitimos que los clientes de archivos ingresen directamente a la red del cl√∫ster; se les asignan varios Gateways tolerantes a fallas.  Cada puerta de enlace proporciona acceso al sistema de cl√∫ster de archivos a trav√©s de NFS, AFP o SMB.  En consecuencia, casi cualquier cliente, ya sea Linux, FreeBSD o no solo un cliente, servidor (OS X, Windows), obtiene acceso a CephFS. <br><br>  Para gestionar todo esto, tuvimos que desarrollar nuestra propia orquesta Ceph y todos nuestros componentes, que son numerosos all√≠.  Pero hablar de eso ahora no tiene sentido, ya que este es nuestro desarrollo.  La mayor√≠a probablemente estar√° interesada en el propio Ceph "desnudo". <br><br>  Ceph se usa mucho donde, y ocasionalmente ocurren fallas.  Seguramente todos los que trabajan con Ceph conocen la leyenda sobre CloudMouse.  Esta es una leyenda urbana terrible, pero no todo es tan malo como parece.  Hay un nuevo cuento de hadas sobre Rosreestr.  Ceph estaba girando en todas partes, y en todas partes estaba fallando.  En alg√∫n lugar termin√≥ fatalmente, en alg√∫n lugar logr√≥ eliminar r√°pidamente las consecuencias. <br><br>  Desafortunadamente, no es habitual que compartamos experiencias negativas, todos est√°n tratando de ocultar la informaci√≥n relevante.  Las empresas extranjeras son un poco m√°s abiertas, en particular, DigitalOcean (un conocido proveedor que distribuye m√°quinas virtuales) tambi√©n sufri√≥ una falla de Ceph durante casi un d√≠a, fue el 1 de abril, ¬°un d√≠a maravilloso!  Publicaron algunos de los informes, un breve registro a continuaci√≥n. <br><img src="https://habrastorage.org/webt/qo/sb/ds/qosbdsczlkvzh-zqsfvid86er5u.png"><br><br>  Los problemas comenzaron a las 7 de la ma√±ana, a las 11 entendieron lo que estaba sucediendo y comenzaron a eliminar la falla.  Para hacer esto, asignaron dos comandos: uno por alguna raz√≥n corri√≥ alrededor de los servidores e instal√≥ memoria all√≠, y el segundo por alguna raz√≥n inici√≥ manualmente un servidor tras otro y monitore√≥ cuidadosamente todos los servidores.  Por qu√©  Todos estamos acostumbrados a todo lo que se enciende con un solo clic. <br><br>  <em>¬øQu√© sucede b√°sicamente en un sistema distribuido cuando se construye de manera efectiva y funciona casi al l√≠mite de sus capacidades?</em> <br><br>  Para responder a esta pregunta, debemos analizar c√≥mo funciona el cl√∫ster Ceph y c√≥mo se produce la falla. <br><img src="https://habrastorage.org/webt/ln/ks/rd/lnksrda1mb-lmfbymyacym1f8aw.png"><br><br><h2>  Escenario de falla cef√°lica <br></h2><br>  Al principio, el cl√∫ster funciona bien, todo va bien.  Entonces sucede algo, despu√©s de lo cual los demonios OSD, donde se almacenan los datos, pierden contacto con los componentes centrales del cl√∫ster (monitores).  En este punto, se produce un tiempo de espera y todo el cl√∫ster obtiene una apuesta.  El cl√∫ster permanece en pie por un tiempo hasta que se da cuenta de que algo anda mal y luego corrige su conocimiento interno.  Despu√©s de eso, el servicio al cliente se restaura hasta cierto punto y el cl√∫ster vuelve a funcionar en modo degradado.  Y lo curioso es que funciona m√°s r√°pido que en el modo normal; este es un hecho sorprendente. <br><br>  Luego eliminamos el fracaso.  Supongamos que perdimos energ√≠a, el bastidor se cort√≥ por completo.  Los electricistas llegaron corriendo, todos restauraron, suministraron energ√≠a, los servidores se encendieron y luego <strong>comenz√≥ la diversi√≥n</strong> . <br><br><blockquote>  Todos est√°n acostumbrados al hecho de que cuando un servidor falla, todo se vuelve malo, y cuando lo encendemos, todo se vuelve bueno.  Todo est√° completamente mal aqu√≠. <br></blockquote><br>  El cl√∫ster pr√°cticamente se detiene, realiza la sincronizaci√≥n primaria y luego comienza una recuperaci√≥n suave y lenta, volviendo gradualmente al modo normal. <br><img src="https://habrastorage.org/webt/ml/r_/i3/mlr_i3llw-lsdaybp4vbedxeuhi.png"><br><br>  Arriba hay un gr√°fico del rendimiento del cl√∫ster Ceph a medida que se desarrolla una falla.  Tenga en cuenta que aqu√≠ los intervalos de los que hablamos est√°n claramente trazados: <br><br><ul><li>  Operaci√≥n normal hasta aproximadamente 70 segundos; </li><li>  Falla por un minuto a aproximadamente 130 segundos; </li><li>  Una meseta que es notablemente m√°s alta que la operaci√≥n normal es el trabajo de grupos degradados; </li><li>  Luego activamos el nodo faltante: este es un cl√∫ster de entrenamiento, solo hay 3 servidores y 15 SSD.  Iniciamos el servidor en alg√∫n lugar alrededor de 260 segundos. </li><li>  El servidor se encendi√≥, ingres√≥ al cl√∫ster, IOPS se cay√≥. </li></ul><br>  Tratemos de descubrir qu√© sucedi√≥ realmente all√≠.  Lo primero que nos interesa es una ca√≠da al comienzo del gr√°fico. <br><br><h3>  Falla de OSD <br></h3><br>  Considere un ejemplo de un cl√∫ster con tres bastidores, varios nodos en cada uno.  Si falla el rack izquierdo, todos los demonios OSD (¬°no los hosts!) Se hacen ping con mensajes Ceph en un intervalo determinado.  Si hay una p√©rdida de varios mensajes, se env√≠a un mensaje al monitor: "Yo, OSD tal y tal, no puedo llegar a OSD tal y tal". <br><img src="https://habrastorage.org/webt/zh/1s/ge/zh1sge1ljlclxjmgfygxpyyyc8i.png"><br><br>  En este caso, los mensajes generalmente se agrupan por hosts, es decir, si dos mensajes de diferentes OSD llegan al mismo host, se combinan en un solo mensaje.  En consecuencia, si OSD 11 y OSD 12 informan que no pueden alcanzar OSD 1, esto se interpretar√° como el Host 11 se quej√≥ de OSD 1. Cuando se informaron OSD 21 y OSD 22, se interpreta como Host 21 insatisfecho con OSD 1 Despu√©s de lo cual el monitor considera que OSD 1 est√° en estado inactivo y notifica a todos los miembros del cl√∫ster (al cambiar el mapa OSD), el trabajo contin√∫a en modo degradado. <br><img src="https://habrastorage.org/webt/uu/-c/1w/uu-c1wnwflbqk6ueyumhohtlvjy.png"><br><br>  Entonces, aqu√≠ est√° nuestro cl√∫ster y rack fallido (Host 5 y Host 6).  Encendemos Host 5 y Host 6, cuando apareci√≥ el poder, y ... <br><br><h3>  Comportamiento interno de Ceph <br></h3><br>  Y ahora la parte m√°s interesante es que estamos comenzando la <strong>sincronizaci√≥n de datos inicial</strong> .  Como hay muchas r√©plicas, deben ser sincr√≥nicas y estar en la misma versi√≥n.  En el proceso de iniciar el inicio de OSD: <br><br><ul><li>  OSD lee las versiones disponibles, el historial disponible (pg_log - para determinar las versiones actuales de los objetos). </li><li>  Despu√©s de lo cual determina qu√© OSD est√°n activadas las √∫ltimas versiones de objetos degradados (missing_loc) y cu√°les est√°n detr√°s. </li><li>  Cuando se almacenan las versiones anteriores, es necesaria la sincronizaci√≥n y se pueden utilizar nuevas versiones como referencia para leer y escribir datos. </li></ul><br>  Se utiliza una historia que se recopila de todos los OSD, y esta historia puede ser bastante;  Se determina la ubicaci√≥n real del conjunto de objetos en el cl√∫ster donde se encuentran las versiones correspondientes.  Cu√°ntos objetos hay en el grupo, cu√°ntos registros se obtienen, si el grupo ha permanecido durante mucho tiempo en modo degradado, entonces la historia es larga. <br><br>  <strong>A modo de comparaci√≥n: el</strong> tama√±o t√≠pico de un objeto cuando trabajamos con una imagen RBD es de 4 MB.  Cuando trabajamos en c√≥digo de borrado - 1 MB.  Si tenemos un disco de 10 TB, obtenemos un mill√≥n de objetos de megabyte en el disco.  Si tenemos 10 discos en el servidor, entonces ya hay 10 millones de objetos, si hay 32 discos (estamos creando un cl√∫ster eficiente, tenemos una asignaci√≥n ajustada), entonces 32 millones de objetos deben mantenerse en la memoria.  Adem√°s, de hecho, la informaci√≥n sobre cada objeto se almacena en varias copias, porque cada copia indica que en este lugar se encuentra en esta versi√≥n, y en esta, en esta. <br><br>  Resulta una gran cantidad de datos, que se encuentra en la RAM: <br><br><ul><li>  cuantos m√°s objetos, mayor es la historia de missing_loc; </li><li>  cuanto m√°s PG, m√°s pg_log y mapa OSD; </li></ul><br>  adem√°s: <br><br><ul><li>  cuanto mayor sea el tama√±o del disco; </li><li>  cuanto mayor sea la densidad (el n√∫mero de discos en cada servidor); </li><li>  cuanto mayor sea la carga en el cl√∫ster y m√°s r√°pido sea su cl√∫ster; </li><li>  cuanto m√°s tiempo est√© inactivo el OSD (en estado sin conexi√≥n); </li></ul><br>  en otras palabras, cuanto <strong>m√°s empinado sea el cl√∫ster que construimos, y cuanto m√°s tiempo no responda la parte del cl√∫ster, m√°s RAM se necesitar√° al inicio</strong> . <br><br><h2>  Las optimizaciones extremas son la ra√≠z de todo mal <br></h2><br><blockquote>  <em>"... y el OOM negro llega a los ni√±os y ni√±as malos por la noche y mata todos los procesos de izquierda a derecha"</em> <br><br>  Leyenda del administrador de la ciudad <br></blockquote><br>  Entonces, la RAM requiere mucho, el consumo de memoria est√° creciendo (comenzamos de inmediato en un tercio del cl√∫ster) y el sistema en teor√≠a puede entrar en SWAP, si lo cre√≥, por supuesto.  Creo que hay muchas personas que piensan que SWAP es malo y no lo crean: "¬øPor qu√©?  ¬°Tenemos mucha memoria!  Pero este es el enfoque equivocado. <br><br>  Si el archivo SWAP no se ha creado de antemano, ya que se decidi√≥ que Linux funcionar√≠a de manera m√°s eficiente, tarde o temprano suceder√° sin memoria (OOM-killer). Y no el hecho de que matar√° al que se comi√≥ toda la memoria, no el que primero tuvo mala suerte.  Sabemos qu√© es una ubicaci√≥n optimista: pedimos un recuerdo, nos lo prometen y decimos: "Ahora danos uno", en respuesta: "¬°Pero no!"  - Y sin memoria asesino. <br><br>  Este es un trabajo normal de Linux, a menos que est√© configurado en el √°rea de memoria virtual. <br><br>  El proceso se queda sin memoria y se cae r√°pidamente y sin piedad.  Adem√°s, no se conocen otros procesos que √©l haya muerto.  No tuvo tiempo de notificar nada a nadie, simplemente lo despidieron. <br><br>  Luego, el proceso, por supuesto, se reiniciar√°: tenemos systemd, tambi√©n lanza, si es necesario, OSD que han ca√≠do.  Los OSD ca√≠dos comienzan y ... comienza una reacci√≥n en cadena. <br><img src="https://habrastorage.org/webt/9p/s8/4z/9ps84zkjtmuamxyllkcgffsgxkq.png"><br><br>  En nuestro caso, comenzamos OSD 8 y OSD 9, comenzaron a aplastar todo, pero no hubo suerte OSD 0 y OSD 5. Un asesino sin memoria vol√≥ hacia ellos y los termin√≥.  Reiniciaron: leyeron sus datos, comenzaron a sincronizarse y aplastar al resto.  Tres m√°s desafortunados (OSD 9, OSD 4 y OSD 7).  Estos tres reiniciaron, comenzaron a presionar a todo el grupo, el siguiente paquete no tuvo suerte. <br><br>  <strong>El grupo comienza a desmoronarse literalmente ante nuestros ojos</strong> .  La degradaci√≥n ocurre muy r√°pidamente, y este "muy r√°pido" generalmente se expresa en minutos, m√°ximo decenas de minutos.  Si tiene 30 nodos (10 nodos por bastidor) y corta el bastidor debido a una falla de energ√≠a, despu√©s de 6 minutos, la mitad del cl√∫ster yace. <br><br>  Entonces, obtenemos algo como lo siguiente. <br><img src="https://habrastorage.org/webt/1b/hq/bu/1bhqburpjt74vwnpbgqn5ehdhh0.png"><br><br>  En casi todos los servidores, tenemos un OSD fallido.  Y si en cada servidor lo es, es decir, en cada dominio de falla que tenemos para el OSD fallido, entonces la <strong>mayor√≠a de nuestros datos son inaccesibles</strong> .  Cualquier solicitud est√° bloqueada, para escribir, para leer, no hay diferencia.  Eso es todo!  Nos levantamos <br><br>  ¬øQu√© hacer en tal situaci√≥n?  M√°s precisamente, <strong>¬øqu√© hab√≠a que hacer</strong> ? <br><br>  <strong>Respuesta:</strong> No inicies el grupo de inmediato, es decir, todo el estante, pero levanta cuidadosamente un demonio cada uno. <br><br>  Pero no lo sab√≠amos.  Comenzamos de inmediato y obtuvimos lo que obtuvimos.  En este caso, lanzamos uno de los cuatro demonios (8, 9, 10, 11), el consumo de memoria aumentar√° en aproximadamente un 20%.  Como regla, damos un gran salto.  Luego, el consumo de memoria comienza a disminuir, porque algunas de las estructuras que se usaron para contener informaci√≥n sobre c√≥mo se degrad√≥ el cl√∫ster se est√°n yendo.  Es decir, parte de los Grupos de Colocaci√≥n ha vuelto a su estado normal, y todo lo que se necesita para mantener el estado degradado se libera, <strong>en teor√≠a se libera</strong> . <br><br>  Veamos un ejemplo.  El c√≥digo C a la izquierda y a la derecha es casi id√©ntico, la diferencia es solo en constantes. <br><img src="https://habrastorage.org/webt/sy/1j/u0/sy1ju0rfqjg507jxvk_4wax9_o4.png"><br><br>  Estos dos ejemplos solicitan una cantidad diferente de memoria del sistema: <br><br><ul><li>  izquierda - 2048 piezas de 1 MB cada una; </li><li>  derecha - 2097152 piezas de 1 Kbyte. </li></ul><br>  Luego, ambos ejemplos esperan que los fotografiemos en la parte superior.  Y despu√©s de presionar ENTER, liberan memoria, todo excepto la √∫ltima pieza.  Esto es muy importante: queda la √∫ltima pieza.  Y nuevamente est√°n esperando que los fotografiemos. <br><br>  A continuaci√≥n se muestra lo que realmente sucedi√≥. <br><img src="https://habrastorage.org/webt/zx/ah/ug/zxahugrdasantcktho7dbu-tnes.png"><br><br><ul><li>  Primero, ambos procesos comenzaron y se comieron la memoria.  Suena como la verdad - 2 GB RSS. </li><li>  Presione ENTER y se sorprender√°.  El primer programa que se destac√≥ en grandes fragmentos devolvi√≥ la memoria.  Pero el segundo programa no regres√≥. </li></ul><br>  La respuesta a por qu√© sucedi√≥ esto radica en el malloc de Linux. <br><br>  Si solicitamos memoria en trozos grandes, se emite utilizando el mecanismo an√≥nimo mmap, que se entrega al espacio de direcciones del procesador, desde donde se nos corta la memoria.  Cuando hacemos free (), la memoria se libera y las p√°ginas se devuelven al cach√© de p√°ginas (sistema). <br><br>  Si asignamos memoria en partes peque√±as, hacemos sbrk ().  sbrk () desplaza el puntero a la cola del mont√≥n; en teor√≠a, la cola desplazada puede devolverse devolviendo p√°ginas de memoria al sistema si no se utiliza la memoria. <br><br>  Ahora mira la ilustraci√≥n.  Ten√≠amos muchos registros en la historia de la ubicaci√≥n de los objetos degradados, y luego vino la sesi√≥n del usuario, un objeto de larga duraci√≥n.  Nos sincronizamos y todas las estructuras adicionales desaparecieron, pero el objeto de larga vida permaneci√≥ y no podemos mover sbrk () hacia atr√°s. <br><img src="https://habrastorage.org/webt/06/wf/eg/06wfegwyvu0ibae8xjlwizrwteo.png"><br><br>  Todav√≠a tenemos mucho espacio sin usar que podr√≠a liberarse si tuvi√©ramos SWAP.  Pero somos inteligentes: deshabilitamos SWAP. <br><br>  Por supuesto, se utilizar√° una parte de la memoria desde el principio del mont√≥n, pero esto es solo una parte, y un resto muy significativo se mantendr√° ocupado. <br><br>  ¬øQu√© hacer en tal situaci√≥n?  La respuesta est√° abajo. <br><br><h3>  Lanzamiento controlado <br></h3><br><ul><li>  Comenzamos un demonio OSD. </li><li>  Esperamos mientras est√° sincronizado, verificamos los presupuestos de memoria. </li><li>  Si entendemos que sobreviviremos al comienzo del pr√≥ximo demonio, comenzaremos al siguiente. </li><li>  De lo contrario, reinicie r√°pidamente el demonio que ocupa la mayor cantidad de memoria.  Pudo bajar por un corto tiempo, no tiene mucha historia, faltan locomotoras y otras cosas, por lo que comer√° menos memoria, el presupuesto de memoria aumentar√° ligeramente. </li><li>  Corremos alrededor del cl√∫ster, lo controlamos y gradualmente lo elevamos todo. </li><li>  Verificamos si es posible continuar con el siguiente OSD, ve a √©l. </li></ul><br>  DigitalOcean realmente logr√≥ esto: <br>  <em>"Nuestro equipo de Datacenter realiza aumentos de memoria mientras que otro equipo contin√∫a lentamente activando nodos mientras administra manualmente el presupuesto de memoria de cada host".</em> <br><img src="https://habrastorage.org/webt/nr/yg/a1/nryga17av_ez5yj0mt3lm5grkk0.png"><br><br>  Volvamos a nuestra configuraci√≥n y situaci√≥n actual.  Ahora tenemos un cl√∫ster colapsado despu√©s de una reacci√≥n en cadena de un asesino sin memoria.  Prohibimos el reinicio autom√°tico de OSD en el dominio rojo, y uno por uno comenzamos los nodos desde los dominios azules.  Porque <strong>nuestra primera tarea es siempre restaurar el servicio</strong> , sin entender por qu√© sucedi√≥ esto.  Lo entenderemos m√°s tarde, cuando restauremos el servicio.  En funcionamiento, este es siempre el caso. <br><br>  Llevamos el cl√∫ster al estado de destino para restaurar el servicio y luego comenzamos a ejecutar un OSD despu√©s de otro de acuerdo con nuestra metodolog√≠a.  Observamos el primero, si es necesario, reinicie los otros para ajustar el presupuesto de memoria, el siguiente, 9, 10, 11, y el cl√∫ster parece estar sincronizado y listo para comenzar el mantenimiento. <br><br>  El problema es c√≥mo se realiza el <strong>mantenimiento de escritura en Ceph</strong> . <br><img src="https://habrastorage.org/webt/hl/rp/ek/hlrpekm0rvjgrjwl11zgdklwecc.png"><br><br>  Tenemos 3 r√©plicas: una OSD maestra y dos esclavas para ella.  Aclararemos que el maestro / esclavo en cada grupo de ubicaci√≥n tiene el suyo propio, pero cada uno tiene un maestro y dos esclavos. <br><br>  La operaci√≥n de escritura o lectura recae en el maestro.  Al leer, si el maestro tiene la versi√≥n correcta, se la dar√° al cliente.  La grabaci√≥n es un poco m√°s complicada, la grabaci√≥n debe repetirse en todas las r√©plicas.  En consecuencia, cuando el cliente escribe 64 KB en OSD 0, los mismos 64 KB en nuestro ejemplo van a OSD 5 y OSD 8. <br><br>  Pero el hecho es que nuestro OSD 8 est√° muy degradado, porque reiniciamos muchos procesos. <br><img src="https://habrastorage.org/webt/es/_z/fr/es_zfrsvdaq8a7f_rgn7hcakpi4.png"><br><br>  Dado que en Ceph cualquier cambio es una transici√≥n de versi√≥n a versi√≥n, en OSD 0 y OSD 5 tendremos una nueva versi√≥n, en OSD 8, la anterior.  ,   ,    ( 64 )    OSD 8   ‚Äî   4  ( ).     4   OSD 0,   OSD 8,  ,    .       ,      64 . <br><br>    ‚Äî  . <br><img src="https://habrastorage.org/webt/ch/uc/l_/chucl_b0vhoi-jvuhl3xokm26qg.png"><br><br>   : <br><br><ul><li>    4   1 ,  1000 /  1 . </li><li>   4  ( )  22 ,  45 /. </li></ul><br> ,      ,       ,        ,         . <br><br>      ‚Äî     . <br><img src="https://habrastorage.org/webt/it/0p/34/it0p34kbqfs3u9hvyhmextflvqc.png"><br><br>    4   22 ,  22 ,   1    4   .   45          SSD,       1  ‚Äî <strong>   45 </strong> . <br><br>       ,    . <br><br><h2>    <br></h2><br><br><ul><li>   <strong> </strong> ,    ‚Äî (45+1) / 2 = <strong>23 .</strong> </li><li>   <strong>75% </strong> ,  (45 * 3 + 1) / 4 = <strong>34 </strong> . </li><li>  90% ‚Äî(45 * 9 + 1) / 10 = 41  ‚Äî  40  ,   . </li></ul><br>     Ceph,      .                 ,     ,    ,     . <br><br>      Ceph       . <br><img src="https://habrastorage.org/webt/ng/jj/od/ngjjodzmfd4n6kes71g4n6pg7os.png"><br><br><ol><li>     ‚Äî   :  , ,  ,  ,    . <br></li><li>  ‚Äî latency.   latency  ,   .      100%    (    ,          ). Latency  60     ,       . <br></li></ol><br><img src="https://habrastorage.org/webt/z3/pb/ob/z3pbobkev0bfszscnwgprpop3xe.png"><br><br>       ,       .  10 ,   1 200 /,    300      ,    ,   .  10 SSD ‚Äî   300   ,   ‚Äî ,  - 300   . <br><br><blockquote>    ,     . <br></blockquote><br>  ,     .       900 / (  SSD).     2 500   128    ( , ESXi  HyperV     128 ).      degraded,   225   .     file store,   object store,         ( ),    110   ,     - . <br><br> SSD  110    ‚Äî ! <br><br> <strong>   ?</strong> <br><br> <strong> 1:</strong>     ‚Äî <b>   </b> . <br><img src="https://habrastorage.org/webt/ls/ib/rh/lsibrhcfnucjiox9f8gzxbk1cc8.png"><br><br>    :   ;   PG; <br>       . <br><br>    : <br><br><ul><li>    ,  45  ‚Äî   . </li><li>     (     . ),   14 . </li><li>    ,  8  (  10% PG). </li></ul><br>   <strong>  ,  </strong> ,       , ,  ,     . <br><br> <strong> 2:</strong>   ‚Äî <b>  </b> (order, objectsize)  . <br><br>     , , ,   4   2  1 .      ,     ,   .  : <br><br><ul><li>     ; </li><li>     (latency)     . </li></ul><br>     : <br><br><ul><li>    ; </li><li>     ; </li><li>   ‚Äî        .     4 ,   . </li></ul><br>        (32  ) ‚Äî      ! <br><br> <strong> 3:</strong>    ‚Äî  <b> Ceph</b> . <br><br>     ,   -,  <strong> Ceph</strong> .                  ,      ,      .     . <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>     ,   ‚Äî Latency.  ‚Äî  ,  ‚Äî . Latency      30% ,       ,      . <br><br>  Community     ,     preproduction .     ,     .      ,   . <br><br><h1>  Conclusi√≥n <br></h1><br>      -  ,     .        ,   Ceph    - ,  ,    . <br><br> ‚óè <strong>   -  </strong> . <br>     ,     .  ,  <strong>     </strong> .       .  ,         ,    production.  ,       ,     ,    DigitalOcean  ,   .   ,  ,    ,  . <br><br>   ,        ,        .    ,  : ¬´    !  ?!¬ª     ,  ,     .   ,      : ,   ,    down time. <br><br> ‚óè <strong>    (OSD).</strong> <br>  ,       ,     ‚Äî     , ,  -      ,   . <strong>     OSD ‚Äî    ‚Äî   </strong> .    ,     . <br><br> ‚óè <strong>  .</strong> <br>        OSD       . <strong>   ,   </strong> .  ,     ,     ,   . <br><br> ‚óè <strong>  RAM   OSD.</strong> <br><br> ‚óè <strong>  SWAP.</strong> <br>   SWAP    Ceph' ,    Linux' .         . <br><br> ‚óè <strong>    .</strong> <br>         100%,    10%. ,    ,      ,   . <br><br> ‚óè <strong>        RBD      Rados Getway.</strong> <br>  ,         . <strong>   SWAP ‚Äî    .</strong> ,    SWAP  ‚Äî    , ,  ,    ,     . <br><br><blockquote>   ‚Äî      DevOpsConf Russia.             .  ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">youtube</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,            DevOps-. <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es431536/">https://habr.com/ru/post/es431536/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es431526/index.html">Influencia corrupta: c√≥mo la Stasi defendi√≥ a Alemania Oriental de los videojuegos</a></li>
<li><a href="../es431528/index.html">Misterioso genio matem√°tico y escritor promueven la soluci√≥n del problema de la permutaci√≥n</a></li>
<li><a href="../es431530/index.html">Lecci√≥n abierta "Dise√±o de material Android: resumen de actualizaci√≥n"</a></li>
<li><a href="../es431532/index.html">Memorias que constan de piezas de 2 nm de espesor.</a></li>
<li><a href="../es431534/index.html">Identidades problem√°ticas entre desarrolladores</a></li>
<li><a href="../es431538/index.html">Case Rate & Goods y Mobio: aumento gradual en todos los indicadores</a></li>
<li><a href="../es431540/index.html">Paquetes y gestores de paquetes para k8s</a></li>
<li><a href="../es431542/index.html">Desarrollo efectivo y mantenimiento de roles Ansible</a></li>
<li><a href="../es431544/index.html">Llevar DevOps a las masas</a></li>
<li><a href="../es431546/index.html">¬øPor qu√© estamos diciendo bien?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>