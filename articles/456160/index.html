<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüé§ üíõ üôèüèº ¬øAprendizaje de refuerzo o estrategias evolutivas? - Tanto eso como otro üõ¥ üïõ ‚ôªÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! 

 Raramente decidimos publicar aqu√≠ traducciones de textos hace dos a√±os, sin un c√≥digo y un enfoque claramente acad√©mico, pero hoy haremo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øAprendizaje de refuerzo o estrategias evolutivas? - Tanto eso como otro</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/456160/">  Hola Habr! <br><br>  Raramente decidimos publicar aqu√≠ traducciones de textos hace dos a√±os, sin un c√≥digo y un enfoque claramente acad√©mico, pero hoy haremos una excepci√≥n.  Esperamos que el dilema en el t√≠tulo del art√≠culo sea motivo de preocupaci√≥n para muchos de nuestros lectores, y que ya haya le√≠do el trabajo original o leer√° el trabajo fundamental sobre estrategias evolutivas con el que esta publicaci√≥n est√° pol√©mica.  ¬°Bienvenido a cat! <br><br><img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br><a name="habracut"></a><br>  En marzo de 2017, OpenAI hizo un esc√°ndalo en la comunidad de aprendizaje profundo al publicar el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Estrategias de evoluci√≥n como una alternativa escalable al aprendizaje de refuerzo</a> ".  En este trabajo, se describieron resultados impresionantes a favor del hecho de que la luz no convergi√≥ en el entrenamiento con refuerzo (RL), y es aconsejable probar otros m√©todos al entrenar redes neuronales complejas.  Luego surgi√≥ una discusi√≥n sobre la importancia del aprendizaje reforzado y cu√°nto merece el estado de la tecnolog√≠a "obligatoria" para aprender a resolver problemas.  Aqu√≠ quiero hablar sobre el hecho de que no debe considerar estas dos tecnolog√≠as como competidoras, una de las cuales es claramente mejor que la otra;  por el contrario, finalmente se complementan entre s√≠.  De hecho, si piensa un poco sobre lo que se requiere para crear una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">IA com√∫n</a> y sistemas que, a lo largo de su existencia, sean capaces de aprender, juzgar y planificar, entonces seguramente llegaremos a la conclusi√≥n de que esta o aquella soluci√≥n combinada ser√° necesaria .  Por cierto, fue la naturaleza la que lleg√≥ a una soluci√≥n combinada, que dot√≥ de la compleja inteligencia de los mam√≠feros y otros animales superiores durante la evoluci√≥n. <br><br><h4>  Estrategias evolutivas </h4><br>  La tesis principal del art√≠culo de OpenAI fue que, en lugar de utilizar el aprendizaje de refuerzo combinado con la propagaci√≥n hacia atr√°s tradicional, entrenaron con √©xito la red neuronal para resolver problemas complejos utilizando la llamada "estrategia evolutiva" (ES).  Tal enfoque de ES consiste en mantener la distribuci√≥n de los valores de peso en una b√°scula de red, con muchos agentes trabajando en paralelo y utilizando par√°metros seleccionados de esta distribuci√≥n.  Cada agente opera en su propio entorno y al completar un n√∫mero determinado de episodios o etapas de un episodio, el algoritmo devuelve una recompensa total, expresada como una puntuaci√≥n de aptitud.  Dado este valor, la distribuci√≥n de par√°metros puede cambiarse hacia agentes m√°s exitosos, privando a los menos exitosos.  Millones de veces repitiendo una operaci√≥n de este tipo que involucra a cientos de agentes, es posible trasladar la distribuci√≥n de pesos a un espacio que nos permita formular una pol√≠tica de calidad para que los agentes resuelvan su tarea.  De hecho, los resultados presentados en el art√≠culo son impresionantes: se muestra que si ejecuta miles de agentes en paralelo, el movimiento antropom√≥rfico en dos piernas puede estudiarse en menos de media hora (mientras que incluso los m√©todos RL m√°s avanzados requieren m√°s de una hora).  Para una revisi√≥n m√°s detallada, recomiendo leer una excelente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n</a> de los autores del experimento, as√≠ como el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo cient√≠fico en</a> s√≠. <br><br><img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br><br>  <i>Diversas estrategias de aprendizaje para la postura vertical antropom√≥rfica, estudiadas usando el m√©todo ES de OpenAI.</i> <br><br><h4>  Caja negra </h4><br>  El gran beneficio de este m√©todo es que es f√°cil de paralelizar.  Mientras que los m√©todos RL, por ejemplo, A3C, requieren el intercambio de informaci√≥n entre los flujos de trabajo y el servidor de par√°metros, ES solo necesita estimaciones de validez e informaci√≥n generalizada sobre la distribuci√≥n de par√°metros.  Gracias a tal simplicidad, este m√©todo evita los m√©todos modernos de RL en escalabilidad.  Sin embargo, todo esto no es en vano: debe optimizar la red seg√∫n el principio de una caja negra.  En este caso, el "recuadro negro" significa que durante el entrenamiento la estructura interna de la red se ignora por completo, y solo se utiliza el resultado general (recompensa por el episodio), y depende de si las futuras generaciones heredar√°n el peso de una red en particular.  En situaciones en las que no recibimos una respuesta pronunciada del entorno, y al resolver muchas tareas tradicionales relacionadas con RL, el flujo de recompensa es muy raro, el problema pasa de ser una "caja parcialmente negra" a una "caja completamente negra".  En este caso, es posible aumentar seriamente la productividad, por lo que, por supuesto, tal compromiso est√° justificado.  "¬øQui√©n necesita gradientes si todav√≠a son irremediablemente ruidosos?"  - Esta es la opini√≥n general. <br><br>  Sin embargo, en situaciones donde la retroalimentaci√≥n es m√°s activa, los asuntos de ES est√°n empezando a salir mal.  El equipo de OpenAI describe c√≥mo se entren√≥ la red de clasificaci√≥n simple MNIST usando ES, y esta vez la capacitaci√≥n fue 1000 veces m√°s lenta.  El hecho es que la se√±al de gradiente en la clasificaci√≥n de im√°genes es extremadamente informativa sobre c√≥mo ense√±ar a la red una mejor clasificaci√≥n.  Por lo tanto, el problema se asocia no tanto con la t√©cnica RL como con las recompensas dispersas en entornos que producen gradientes ruidosos. <br><br><h4>  Soluci√≥n encontrada por la naturaleza </h4><br>  Si intenta aprender del ejemplo de la naturaleza, pensando en formas de desarrollar IA, en algunos casos, la IA puede representarse como un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque orientado a los problemas</a> .  Al final, la naturaleza opera dentro de tales limitaciones que los inform√°ticos simplemente no tienen.  Existe la opini√≥n de que un enfoque puramente te√≥rico para resolver un problema particular puede proporcionar soluciones m√°s efectivas que las alternativas emp√≠ricas.  Sin embargo, sigo pensando que ser√≠a aconsejable verificar c√≥mo un sistema din√°mico que opera bajo ciertas condiciones (Tierra) form√≥ agentes (animales, en particular mam√≠feros), capaces de un comportamiento flexible y complejo.  Si bien algunas de estas limitaciones no son aplicables en los mundos simulados de la ciencia de datos, otras son muy buenas. <br><br>  Tras examinar el comportamiento intelectual de los mam√≠feros, vemos que se forma como resultado de la interacci√≥n compleja de dos procesos estrechamente interrelacionados: <i>aprender de la experiencia de los dem√°s</i> y <i>aprender de nuestra propia experiencia</i> .  El primero a menudo se identifica con la evoluci√≥n debido a la selecci√≥n natural, pero aqu√≠ utilizo un t√©rmino m√°s amplio para tener en cuenta la epigen√©tica, los microbiomas y otros mecanismos que aseguran el intercambio de experiencias entre organismos que no est√°n gen√©ticamente relacionados entre s√≠.  El segundo proceso, el aprendizaje de primera mano, es toda la informaci√≥n que un animal logra asimilar a lo largo de la vida, y esta informaci√≥n est√° directamente relacionada con la interacci√≥n de este animal con el mundo exterior.  Esta categor√≠a incluye todo, desde aprender a reconocer objetos hasta dominar la comunicaci√≥n inherente al proceso educativo. <br><br>  En t√©rminos generales, estos dos procesos que ocurren en la naturaleza se pueden comparar con dos opciones para optimizar las redes neuronales.  Las estrategias evolutivas, donde la informaci√≥n de gradiente se utiliza para actualizar la informaci√≥n sobre el cuerpo, se acercan al aprendizaje de la experiencia de otra persona.  Del mismo modo, los m√©todos de gradiente, donde la recepci√≥n de una experiencia particular conduce a uno u otro cambio en el comportamiento del agente, son comparables a aprender de la experiencia.  Si piensa en las variedades de comportamiento intelectual o en las habilidades que cada uno de estos dos enfoques desarrolla en los animales, dicha comparaci√≥n es m√°s pronunciada.  En ambos casos, los "m√©todos evolutivos" contribuyen al estudio de los comportamientos reactivos que permiten el desarrollo de un cierto estado f√≠sico (suficiente para mantenerse con vida).  Aprender a caminar o escapar del cautiverio en muchos casos es equivalente a comportamientos m√°s "instintivos" que est√°n "conectados" en muchos animales a nivel gen√©tico.  Adem√°s, este ejemplo confirma que los m√©todos evolutivos son aplicables en los casos en que la recompensa de se√±al es extremadamente rara (como, por ejemplo, el hecho de criar con √©xito un cachorro).  En tal caso, es imposible correlacionar la recompensa con un conjunto espec√≠fico de acciones que puedan haberse cometido muchos a√±os antes del inicio de este hecho.  Por otro lado, si consideramos el caso en el que la ES falla, es decir, la clasificaci√≥n de las im√°genes, los resultados ser√°n notablemente comparables con los resultados del entrenamiento de animales logrados durante innumerables experimentos psicol√≥gicos conductuales realizados durante m√°s de cien a√±os. <br><br><h4>  Entrenamiento animal </h4><br>  Los m√©todos utilizados en el aprendizaje por refuerzo se toman en muchos casos directamente de la literatura psicol√≥gica sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el condicionamiento operante</a> , y el condicionamiento operante se ha estudiado utilizando la psicolog√≠a animal.  Por cierto, Richard Sutton, uno de los dos fundadores del entrenamiento de refuerzo, tiene una licenciatura en psicolog√≠a.  En el contexto del condicionamiento operante, los animales aprenden a asociar la recompensa o el castigo con patrones de comportamiento espec√≠ficos.  Los entrenadores e investigadores pueden de alguna manera manipular tal asociaci√≥n con recompensas, provocando que los animales muestren ingenio o ciertos comportamientos.  Sin embargo, el condicionamiento operante utilizado en el estudio de los animales no es m√°s que una forma m√°s pronunciada de ese condicionamiento, en base al cual los animales se entrenan durante toda la vida.  Constantemente recibimos se√±ales positivas de refuerzo del entorno y ajustamos nuestro comportamiento en consecuencia.  De hecho, muchos neurofisi√≥logos y cient√≠ficos cognitivos creen que, de hecho, las personas y otros animales act√∫an incluso un nivel m√°s alto y est√°n constantemente aprendiendo a predecir los resultados de su comportamiento en situaciones futuras, contando con recompensas potenciales. <br><br>  El papel central de la predicci√≥n en el autoestudio es cambiar la din√°mica descrita anteriormente de la manera m√°s significativa.  La se√±al que anteriormente se consideraba muy enrarecida (recompensa epis√≥dica) es muy densa.  Te√≥ricamente, la situaci√≥n es aproximadamente la siguiente: en cada momento, el cerebro de los mam√≠feros calcula los resultados en funci√≥n de un complejo flujo de est√≠mulos y acciones sensoriales, mientras que el animal simplemente est√° inmerso en este flujo.  En este caso, el comportamiento final del animal da una se√±al s√≥lida, que debe guiarse por la correcci√≥n de los pron√≥sticos y el desarrollo del comportamiento.  El cerebro utiliza todas estas se√±ales para optimizar los pron√≥sticos (y, en consecuencia, la calidad de las acciones tomadas) en el futuro.  Una visi√≥n general de este enfoque se da en el excelente libro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Surfing Uncertainty</a> " del cient√≠fico cognitivo y fil√≥sofo Andy Clark.  Si extrapolamos tales argumentos al entrenamiento de agentes artificiales, entonces el entrenamiento de refuerzo revela un defecto fundamental: la se√±al utilizada en este paradigma es irremediablemente d√©bil en comparaci√≥n con lo que podr√≠a ser (o deber√≠a ser).  En los casos en que es imposible aumentar la saturaci√≥n de la se√±al (quiz√°s porque, por definici√≥n, es d√©bil o est√° asociada con una reactividad de bajo nivel), probablemente sea mejor preferir un m√©todo de entrenamiento que est√© bien paralelo, por ejemplo, ES. <br><br><h4>  Mejor aprendizaje de las redes neuronales. </h4><br>  Basado en los principios de una mayor actividad nerviosa inherente al cerebro de los mam√≠feros, que se dedica constantemente a la predicci√≥n, √∫ltimamente ha sido posible lograr ciertos √©xitos en el entrenamiento de refuerzo, que ahora tiene en cuenta la importancia de tales predicciones.  Te puedo recomendar dos trabajos similares: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendiendo a actuar prediciendo el futuro</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje de refuerzo con tareas auxiliares no supervisadas</a> </li></ul><br>  En ambos art√≠culos, los autores complementan las pol√≠ticas de red neuronal predeterminadas t√≠picas con resultados de pron√≥stico con respecto a las condiciones ambientales futuras.  En el primer art√≠culo, el pron√≥stico se aplica a una variedad de variables de medici√≥n, y en el segundo, los cambios en el entorno y el comportamiento del agente como tal.  En ambos casos, la se√±al dispersa asociada con el refuerzo positivo se vuelve mucho m√°s saturada e informativa, proporcionando un aprendizaje acelerado y la asimilaci√≥n de modelos conductuales m√°s complejos.  Dichas mejoras solo est√°n disponibles cuando se trabaja con m√©todos que utilizan la se√±al de gradiente, pero no con m√©todos que funcionan seg√∫n el principio de "recuadro negro", como, por ejemplo, ES. <br><br>  Adem√°s, el aprendizaje de primera mano y los m√©todos de gradiente son mucho m√°s efectivos.  Incluso en aquellos casos en los que era posible estudiar un problema particular usando el m√©todo de ES en lugar de usar el entrenamiento de refuerzo, la ganancia se logr√≥ debido al hecho de que muchas veces m√°s datos estaban involucrados en la estrategia de ES que con RL.  Pensando en este caso sobre los principios del aprendizaje en animales, notamos que el resultado del entrenamiento en un ejemplo extra√±o se manifiesta despu√©s de muchas generaciones, mientras que a veces un solo evento, experimentado en persona, es suficiente para que el animal aprenda la lecci√≥n para siempre.  Si bien dicha <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capacitaci√≥n sin ejemplos</a> a√∫n no se ajusta completamente a los m√©todos de gradiente tradicionales, es mucho m√°s inteligible que ES.  Hay, por ejemplo, enfoques como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el control neural epis√≥dico</a> , donde los valores Q se almacenan durante el proceso de entrenamiento, despu√©s de lo cual el programa se verifica con ellos antes de realizar acciones.  Resulta un m√©todo de gradiente que le permite aprender a resolver problemas mucho m√°s r√°pido que antes.  En el art√≠culo sobre el control neural epis√≥dico, los autores mencionan el hipocampo humano, que puede almacenar informaci√≥n sobre el evento incluso despu√©s de una experiencia que alguna vez tuvo experiencia y, por lo tanto, juega un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">papel cr√≠tico</a> en el proceso de recuperaci√≥n.  Dichos mecanismos requieren acceso a la organizaci√≥n interna del agente, que tambi√©n es, por definici√≥n, imposible en el paradigma ES. <br><br><h4>  Entonces, ¬øpor qu√© no combinarlos? </h4><br>  Probablemente la mayor parte de este art√≠culo podr√≠a haber dejado la impresi√≥n de que estaba abogando por los m√©todos de RL.  Sin embargo, de hecho, creo que a la larga, la mejor soluci√≥n ser√≠a una combinaci√≥n de ambos m√©todos, de modo que cada uno se use en aquellas situaciones en las que es m√°s adecuado.  Es obvio que en el caso de muchas pol√≠ticas reactivas o en situaciones con se√±ales muy escasas de refuerzo positivo, ES gana, especialmente si tiene el poder de c√≥mputo en el que puede realizar un entrenamiento paralelo en masa.  Por otro lado, los m√©todos de gradiente que utilizan el aprendizaje reforzado o la formaci√≥n del profesorado ser√°n √∫tiles cuando tengamos una amplia retroalimentaci√≥n, y la soluci√≥n del problema debe aprenderse r√°pidamente y con menos datos. <br><br>  Volviendo a la naturaleza, encontramos que el primer m√©todo, en esencia, sienta las bases para el segundo.  Es por eso que, durante la evoluci√≥n, los mam√≠feros han desarrollado un cerebro que permite un aprendizaje extremadamente eficiente del material de las se√±ales complejas que provienen del medio ambiente.  Entonces, la pregunta permanece abierta.  Quiz√°s las estrategias evolutivas nos ayudar√°n a inventar arquitecturas de aprendizaje efectivas que ser√°n √∫tiles para los m√©todos de aprendizaje gradiente.  Despu√©s de todo, la soluci√≥n encontrada por la naturaleza es realmente muy exitosa. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456160/">https://habr.com/ru/post/456160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456150/index.html">Petty little joy # 4: Rad√≥n - calidad del c√≥digo medida en n√∫meros</a></li>
<li><a href="../456152/index.html">Patrones de dise√±o de niveles para juegos 2D</a></li>
<li><a href="../456154/index.html">Caracter√≠sticas principales de UX y MVP al crear un producto</a></li>
<li><a href="../456156/index.html">Es por eso que se necesita √°lgebra escolar.</a></li>
<li><a href="../456158/index.html">Un poco sobre las fuentes de combustible nuclear.</a></li>
<li><a href="../456162/index.html">Aurora, una empresa fundada por inmigrantes de Google, Tesla y Uber, comenz√≥ a trabajar con las preocupaciones automotrices.</a></li>
<li><a href="../456164/index.html">Los globos Loon brindan conexi√≥n de emergencia a la red e Internet en Per√∫ despu√©s de un terremoto de magnitud - 8.0</a></li>
<li><a href="../456168/index.html">¬øD√≥nde estaba tu casa hace millones de a√±os?</a></li>
<li><a href="../456170/index.html">C√≥mo crear una aplicaci√≥n de finanzas: 5 API para ayudar al desarrollador</a></li>
<li><a href="../456172/index.html">Parte 2: RocketChip: conexi√≥n de RAM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>