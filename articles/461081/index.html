<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßìüèº üíÖüèΩ üñ•Ô∏è El d√≠a que Dodo se detuvo. Script asincr√≥nico üë≥ üßîüèΩ üëçüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Cada SRE en nuestro equipo alguna vez so√±√≥ con dormir tranquilo por la noche. Los sue√±os se hacen realidad. En este art√≠culo hablar√© sobre ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El d√≠a que Dodo se detuvo. Script asincr√≥nico</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dodopizzadev/blog/461081/">  Hola Habr!  Cada SRE en nuestro equipo alguna vez so√±√≥ con dormir tranquilo por la noche.  Los sue√±os se hacen realidad.  En este art√≠culo hablar√© sobre esto y c√≥mo logramos el rendimiento y la estabilidad de nuestro sistema Dodo IS. <br><br><img src="https://habrastorage.org/webt/wk/2o/t6/wk2ot6razkmzgly1s69fdwz5quq.png"><a name="habracut"></a><br><blockquote>  <b>Una serie de art√≠culos sobre el colapso del sistema Dodo IS *</b> : <br><br>  1. El <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">d√≠a que Dodo se detuvo.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Script sincr√≥nico.</a> <br>  2. El d√≠a en que Dodo se detuvo.  Script asincr√≥nico <br><br>  * Los <i>materiales fueron escritos en base a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mi desempe√±o en DotNext 2018 en Mosc√∫</a></i> . </blockquote>  En un art√≠culo anterior, vimos problemas de c√≥digo de bloqueo en el paradigma de la multitarea preventiva.  Se supon√≠a que era necesario reescribir el c√≥digo de bloqueo en async / wait.  Entonces lo hicimos.  Ahora hablemos sobre los problemas que surgieron cuando hicimos esto. <br><br><h2>  Introducimos el t√©rmino concurrencia </h2><br>  Antes de llegar a as√≠ncrono, debe ingresar el t√©rmino Concurrencia. <br><blockquote>  En la teor√≠a de colas, la <b>concurrencia</b> es el n√∫mero de clientes que est√°n actualmente dentro del sistema.  La concurrencia a veces se confunde con el paralelismo, pero en realidad estas son dos cosas diferentes. </blockquote>  Para aquellos nuevos en Concurrency por primera vez, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">les recomiendo el video de Rob Pike</a> .  La concurrencia es cuando estamos tratando con muchas cosas al mismo tiempo, y el paralelismo es cuando estamos haciendo muchas cosas al mismo tiempo. <br><br>  En las computadoras, no suceden muchas cosas en paralelo.  Una de esas cosas es la computaci√≥n en m√∫ltiples procesadores.  El grado de paralelismo est√° limitado por el n√∫mero de subprocesos de la CPU. <br><br>  De hecho, Threads es parte del concepto de multitarea preventiva, una de las formas de modelar la concurrencia en un programa cuando confiamos en el sistema operativo en la pregunta de concurrencia.  Este modelo sigue siendo √∫til siempre y cuando comprendamos que estamos tratando espec√≠ficamente con el modelo de concurrencia, y no con la concurrencia. <br><br>  Async / await es az√∫car sint√°ctico para State Machine, otro modelo de concurrencia √∫til que puede ejecutarse en un entorno de subproceso √∫nico.  En esencia, esto es multitarea cooperativa: el modelo en s√≠ mismo no tiene en cuenta el paralelismo en absoluto.  En combinaci√≥n con Multithreading, tenemos un modelo encima de otro, y la vida es muy complicada. <br><br><h2>  Comparaci√≥n de los dos modelos. </h2><br><h4>  C√≥mo funcion√≥ en el modelo de multitarea preventiva </h4><br>  Digamos que tenemos 20 subprocesos y 20 solicitudes en proceso por segundo.  La imagen muestra un pico: 200 solicitudes en el sistema al mismo tiempo.  ¬øC√≥mo podr√≠a suceder esto? <br><br><ul><li>  las solicitudes podr√≠an agruparse si 200 clientes hac√≠an clic en un bot√≥n al mismo tiempo; </li><li>  el recolector de basura podr√≠a detener las solicitudes de varias decenas de milisegundos; </li><li>  las solicitudes podr√≠an retrasarse en cualquier cola si el proxy admite la cola. </li></ul><br>  Hay muchas razones por las cuales las solicitudes por un corto per√≠odo de tiempo se han acumulado y vienen en un solo paquete.  En cualquier caso, no pas√≥ nada terrible, se pararon en la cola de Thread Pool y se completaron lentamente.  No hay m√°s picos, todo contin√∫a, como si nada hubiera pasado. <br><br>  Suponga que el algoritmo inteligente de Thread Pool (y hay elementos de aprendizaje autom√°tico all√≠) decidi√≥ que hasta ahora no hay raz√≥n para aumentar el n√∫mero de Threads.  El Pool de conexiones en MySql tambi√©n es 20 porque Threads = 20.  En consecuencia, solo necesitamos 20 conexiones a SQL. <br><br><img src="https://habrastorage.org/webt/gm/ch/pz/gmchpzxpvegoyljdauranwzgn7k.png"><br><br>  En este caso, el nivel de concurrencia del servidor desde el punto de vista del sistema externo = 200. El servidor ya ha recibido estas solicitudes, pero a√∫n no lo ha completado.  Sin embargo, para una aplicaci√≥n que se ejecuta en el paradigma Multithreading, el n√∫mero de solicitudes simult√°neas est√° limitado por el tama√±o actual de Thread Pool = 20. Por lo tanto, estamos tratando con el grado de Concurrencia = 20. <br><br><h4>  C√≥mo funciona todo ahora en el modelo as√≠ncrono </h4><br><img width="33%" height="33%" src="https://habrastorage.org/webt/dg/yz/pz/dgyzpzj-nl9rawn5ctxdfr3gxgm.png"><br><br>  Veamos qu√© sucede en una aplicaci√≥n que se ejecuta async / wait con la misma carga y distribuci√≥n de solicitudes.  No hay cola antes de crear una Tarea, y la solicitud se procesa inmediatamente.  Por supuesto, Thread de ThreadPool se usa por un corto tiempo, y la primera parte de la solicitud, antes de contactar con la base de datos, se ejecuta de inmediato.  Debido a que Thread regresa r√°pidamente a Thread Pool, no necesitamos muchos Threads para procesar.  En este diagrama no mostramos Thread Pool en absoluto, es transparente. <br><br><img src="https://habrastorage.org/webt/bm/6h/to/bm6hto7o6gxnrlg9ruzhfsxfet0.png"><br><br>  ¬øQu√© significar√° esto para nuestra aplicaci√≥n?  La imagen externa es la misma: el nivel de concurrencia = 200. Al mismo tiempo, la situaci√≥n en el interior ha cambiado.  Anteriormente, las solicitudes estaban "llenas" en la cola ThreadPool, ahora el grado de concurrencia de la aplicaci√≥n tambi√©n es 200, porque no tenemos restricciones por parte de TaskScheduler.  ¬°Hurra!  Hemos logrado el objetivo de as√≠ncrono: la aplicaci√≥n "hace frente" a casi cualquier grado de concurrencia. <br><br><h4>  Consecuencias: degradaci√≥n no lineal del sistema. </h4><br>  La aplicaci√≥n se ha vuelto transparente desde el punto de vista de la concurrencia, por lo que ahora se proyecta la concurrencia en la base de datos.  Ahora necesitamos un grupo de conexiones del mismo tama√±o = 200. La base de datos es la CPU, memoria, red, almacenamiento.  Este es el mismo servicio con sus problemas, como cualquier otro.  Cuantas m√°s solicitudes intentemos ejecutar al mismo tiempo, m√°s lento se ejecutar√°n. <br><br>  A plena carga en la base de datos, en el mejor de los casos, el tiempo de respuesta se degrada linealmente: usted dio el doble de consultas, comenz√≥ a funcionar el doble de lento.  En la pr√°ctica, debido a la competencia de consultas, necesariamente se producir√° una sobrecarga y puede resultar que el sistema se degradar√° de manera no lineal. <br><br><h4>  ¬øPor qu√© est√° pasando esto? </h4><br>  Razones para el segundo orden: <br><br><ul><li>  Ahora la base de datos debe mantenerse simult√°neamente en la memoria de la estructura de datos para atender m√°s solicitudes; </li><li>  Ahora la base de datos necesita servir colecciones m√°s grandes (y esto es algor√≠tmicamente desventajoso). </li></ul><br>  Motivo de primer orden: <br><br><ul><li>  contenci√≥n, que se discuti√≥ un poco <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el art√≠culo anterior</a> . </li></ul><br>  Al final, async lucha contra recursos limitados y ... ¬°gana!  La base de datos falla y comienza a ralentizarse.  A partir de esto, el servidor aumenta a√∫n m√°s la concurrencia y el sistema ya no puede salir de esta situaci√≥n con honor. <br><br><h2>  S√≠ndrome de muerte s√∫bita del servidor </h2><br>  A veces ocurre una situaci√≥n interesante.  Tenemos un servidor  √âl trabaja para s√≠ mismo as√≠, todo est√° en orden.  Hay suficientes recursos, incluso con un margen.  Luego, de repente, recibimos un mensaje de los clientes de que el servidor se est√° ralentizando.  Observamos el gr√°fico y vemos que hubo un aumento repentino en la actividad del cliente, pero ahora todo es normal.  Pensando en un ataque de DOS o una coincidencia.  Ahora todo parece estar bien.  Solo que ahora el servidor contin√∫a siendo est√∫pido, y todo es m√°s dif√≠cil hasta que los tiempos de espera comienzan a llegar.  Despu√©s de un tiempo, otro servidor que usa la misma base de datos tambi√©n comienza a doblarse.  ¬øUna situaci√≥n familiar? <br><br><h4>  ¬øPor qu√© muri√≥ el sistema? </h4><br>  Puede intentar explicar esto por el hecho de que en alg√∫n momento el servidor recibi√≥ un n√∫mero m√°ximo de solicitudes y se "rompi√≥".  Pero s√≠ sabemos que la carga se redujo, y el servidor despu√©s de eso no mejor√≥ durante mucho tiempo, hasta que la carga desapareci√≥ por completo. <br><br>  La pregunta ret√≥rica: ¬øse supon√≠a que el servidor se romper√≠a debido a una carga excesiva?  ¬øHacen eso? <br><br><h4>  Simulamos una situaci√≥n de bloqueo del servidor </h4><br>  Aqu√≠ no analizaremos gr√°ficos de un sistema de producci√≥n real.  En el momento del bloqueo del servidor, a menudo no podemos obtener ese horario.  El servidor se est√° quedando sin recursos de CPU y, como resultado, no puede escribir registros, dar m√©tricas.  En los diagramas en el momento del desastre, a menudo se observa una ruptura en todos los gr√°ficos. <br><br>  Los SRE deber√≠an poder producir sistemas de monitoreo que sean menos propensos a este efecto.  Los sistemas que en cualquier situaci√≥n proporcionan al menos algo de informaci√≥n y, al mismo tiempo, pueden analizar sistemas post-mortem utilizando informaci√≥n fragmentaria.  Para fines educativos, utilizamos un enfoque ligeramente diferente en este art√≠culo. <br><br>  Intentemos crear un modelo que funcione matem√°ticamente como un servidor bajo carga.  A continuaci√≥n, estudiaremos las caracter√≠sticas del servidor.  Descartamos la no linealidad de los servidores reales y simulamos una situaci√≥n en la que se produce una desaceleraci√≥n lineal cuando la carga crece por encima del nominal.  El doble de solicitudes que sea necesario: atendemos el doble de lento. <br><br>  Este enfoque permitir√°: <br><br><ul><li>  considere lo que suceder√° en el mejor de los casos; </li><li>  tomar m√©tricas precisas </li></ul><br>  Navegaci√≥n programada: <br><br><ul><li>  azul: el n√∫mero de solicitudes al servidor; </li><li>  verde: respuestas del servidor; </li><li>  amarillo - tiempos de espera; </li><li>  gris oscuro: solicitudes que se desperdiciaron en los recursos del servidor porque el cliente no esper√≥ una respuesta de tiempo de espera.  A veces, un cliente puede informar esto al servidor desconect√°ndose, pero en general, tal lujo puede ser t√©cnicamente imposible, por ejemplo, si el servidor realiza un trabajo vinculado a la CPU, sin cooperaci√≥n con el cliente. </li></ul><br><br><img src="https://habrastorage.org/webt/8d/r8/lr/8dr8lr7gizm-ovozc0laylwadaa.png"><br><br>  ¬øPor qu√© el gr√°fico de solicitud del cliente (azul en el diagrama) result√≥ ser as√≠?  Por lo general, el horario de pedidos en nuestras pizzer√≠as crece suavemente por la ma√±ana y disminuye por la noche.  Pero observamos tres picos en el contexto de la curva uniforme habitual.  Esta forma del gr√°fico no fue elegida para el modelo por casualidad, sino m√°s bien.  La modelo naci√≥ durante la investigaci√≥n de un incidente real con el servidor del centro de contacto de la pizzer√≠a en Rusia durante la Copa del Mundo. <br><br><h2>  Caso "Copa Mundial" </h2><br>  Nos sentamos y esperamos m√°s pedidos.  Preparados para el Campeonato, ahora los servidores podr√°n pasar una prueba de fuerza. <br><br>  El primer pico: los fan√°ticos del f√∫tbol van a ver el campeonato, tienen hambre y compran pizza.  Durante la primera mitad, est√°n ocupados y no pueden ordenar.  Pero las personas que son indiferentes al f√∫tbol pueden, as√≠ que en la tabla todo contin√∫a como de costumbre. <br><br>  Y luego termina la primera mitad, y llega el segundo pico.  Los fan√°ticos se pusieron nerviosos, hambrientos e hicieron tres veces m√°s pedidos que en el primer pico.  La pizza se compra a un precio terrible.  Luego comienza la segunda mitad, y de nuevo no a la pizza. <br><br>  Mientras tanto, el servidor del centro de contacto comienza a doblarse lentamente y atender solicitudes cada vez m√°s lentamente.  El componente del sistema, en este caso, el servidor web de Call Center, est√° desestabilizado. <br><br>  El tercer pico llegar√° cuando termine el partido.  Los fan√°ticos y el sistema esperan una penalizaci√≥n. <br><br><h4>  Analizamos los motivos del bloqueo del servidor. </h4><br>  Que paso  El servidor podr√≠a contener 100 solicitudes condicionales.  Entendemos que est√° dise√±ado para este poder y ya no lo soportar√°.  Llega un pico, que en s√≠ mismo no es tan grande.  Pero el √°rea gris de la concurrencia es mucho m√°s alta. <br><br>  El modelo est√° dise√±ado para que la concurrencia sea num√©ricamente igual al n√∫mero de √≥rdenes por segundo, por lo que visualmente en el gr√°fico debe ser de la misma escala.  Sin embargo, es mucho m√°s alto porque se acumula. <br><br>  Aqu√≠ vemos una sombra del gr√°fico: estas son solicitudes que comenzaron a regresar al cliente, ejecutadas (mostradas por la primera flecha roja).  La escala de tiempo es condicional para ver el desplazamiento de tiempo.  El segundo pico ya noque√≥ a nuestro servidor.  Se estrell√≥ y comenz√≥ a procesar cuatro veces menos solicitudes de lo habitual. <br><br><img src="https://habrastorage.org/webt/n1/92/qw/n192qwxtwdrfatt_a-lb8y8eire.png"><br><br>  En la segunda mitad del gr√°fico, est√° claro que algunas solicitudes todav√≠a se ejecutaron al principio, pero luego aparecieron manchas amarillas: las solicitudes se detuvieron por completo. <br><br><img src="https://habrastorage.org/webt/pi/la/nv/pilanvzebdl_vl3k3hno9vwezga.png"><br><br>  Una vez m√°s todo el horario.  Se puede ver que la concurrencia se est√° volviendo loca.  Aparece una gran monta√±a. <br><br><img src="https://habrastorage.org/webt/ci/l6/kb/cil6kblebzhjokmuvzkwolmngvo.png"><br><br>  Por lo general, analizamos m√©tricas completamente diferentes: qu√© tan lento se complet√≥ la solicitud, cu√°ntas solicitudes por segundo.  Ni siquiera miramos la concurrencia, ni siquiera pensamos en esta m√©trica.  Pero en vano, porque es precisamente esta cantidad la que mejor muestra el momento del fallo del servidor. <br><br>  ¬øPero de d√≥nde vino una monta√±a tan grande?  ¬°El pico de carga m√°s grande ya ha pasado! <br><br><h2>  Poca ley </h2><br>  La ley de Little rige la concurrencia. <br><br>  <i>L (n√∫mero de clientes dentro del sistema) = Œª (velocidad de su estad√≠a) ‚àó W (tiempo que pasan dentro del sistema)</i> <br><br>  Este es un promedio.  Sin embargo, nuestra situaci√≥n se est√° desarrollando dram√°ticamente, el promedio no nos conviene.  Diferenciaremos esta ecuaci√≥n, luego la integraremos.  Para hacer esto, mire el libro de John Little, quien invent√≥ esta f√≥rmula, y vea la integral all√≠. <br><br><img src="https://habrastorage.org/webt/sx/f5/dz/sxf5dzgwc9l7low8fild5cpsrf0.png"><br><br>  Tenemos el n√∫mero de entradas en el sistema y el n√∫mero de quienes abandonan el sistema.  La solicitud llega y se va cuando todo est√° completo.  A continuaci√≥n se muestra una regi√≥n de crecimiento gr√°fico correspondiente al crecimiento lineal de la concurrencia. <br><br><img src="https://habrastorage.org/webt/ax/rv/du/axrvdu3vyx1iw9afieohv5pe-cs.png"><br><br>  Hay pocas solicitudes verdes.  Estos son los que realmente se est√°n implementando.  Los azules son los que vienen.  Entre tiempos, tenemos el n√∫mero habitual de solicitudes, la situaci√≥n es estable.  Pero la concurrencia sigue creciendo.  El servidor ya no har√° frente a esta situaci√≥n en s√≠.  Esto significa que caer√° pronto. <br><br>  Pero, ¬øpor qu√© aumenta la concurrencia?  Nos fijamos en la integral de la constante.  Nada cambia en nuestro sistema, pero la integral parece una funci√≥n lineal que solo crece. <br><br><h2>  Vamos a jugar? </h2><br>  La explicaci√≥n con las integrales es complicada si no recuerdas las matem√°ticas.  Aqu√≠ te propongo calentar y jugar el juego. <br><br><h4>  Juego n√∫mero 1 </h4><br>  <b>Requisitos previos</b> : el servidor recibe solicitudes, cada una requiere tres per√≠odos de procesamiento en la CPU.  El recurso de la CPU se divide en partes iguales entre todas las tareas.  Esto es similar a c√≥mo se consumen los recursos de la CPU durante la multitarea preventiva.  El n√∫mero en la celda significa la cantidad de trabajo que queda despu√©s de esta medida.  Para cada paso condicional, llega una nueva solicitud. <br><br>  Imagine que recibi√≥ una solicitud.  Solo quedan 3 unidades de trabajo, al final del primer per√≠odo de procesamiento quedan 2 unidades. <br><br>  En el segundo per√≠odo, hay otra solicitud en capas, ahora ambas CPU est√°n ocupadas.  Hicieron una unidad de trabajo para las dos primeras consultas.  Queda por completar 1 y 2 unidades para la primera y segunda solicitud, respectivamente. <br><br>  Ahora ha llegado la tercera solicitud, y comienza la diversi√≥n.  Parece que la primera solicitud deber√≠a haberse completado, pero en este per√≠odo tres solicitudes ya comparten el recurso de la CPU, por lo que el grado de finalizaci√≥n de las tres solicitudes ahora es fraccional al final del tercer per√≠odo de procesamiento: <br><br><img src="https://habrastorage.org/webt/k-/tq/mv/k-tqmvjsqabbv0zgwt_vmkfkhy4.png"><br><br>  Adem√°s m√°s interesante!  Se agrega la cuarta solicitud, y ahora el grado de concurrencia ya es 4, ya que las cuatro solicitudes requieren un recurso en este per√≠odo.  Mientras tanto, la primera solicitud al final del cuarto per√≠odo ya se ha completado, no pasa al siguiente per√≠odo y le quedan 0 trabajos para la CPU. <br><br>  Como la primera solicitud ya se complet√≥, resumamos para √©l: se ejecut√≥ un tercio m√°s de lo que esper√°bamos.  Se supuso que la longitud de cada tarea horizontalmente idealmente = 3, de acuerdo con la cantidad de trabajo.  Lo marcamos con naranja, como se√±al de que no estamos completamente satisfechos con el resultado. <br><br><img src="https://habrastorage.org/webt/ap/9n/uy/ap9nuyfqun_gd1ggeidqdlomxuy.png"><br><br>  Llega la quinta solicitud.  El grado de concurrencia sigue siendo 4, pero vemos que en la quinta columna el trabajo restante es m√°s total.  Esto se debe a que queda m√°s trabajo en la cuarta columna que en la tercera. <br><br>  Continuamos tres per√≠odos m√°s.  Esperando respuestas. <br>  - Servidor, hola! <br>  - ... <br><br><img src="https://habrastorage.org/webt/tv/2m/8r/tv2m8r8selzumgub75zkvs78deu.png"><br><br>  "Tu llamada es muy importante para nosotros ..." <br><br><img src="https://habrastorage.org/webt/ud/k9/rk/udk9rk7ynqduovmhyymp7shqe0q.png"><br><br>  Bueno, finalmente lleg√≥ la respuesta a la segunda solicitud.  Los tiempos de respuesta son el doble de lo esperado. <br><br><img src="https://habrastorage.org/webt/tt/7m/iv/tt7mivq-stfincjhlwxm7wqznsq.png"><br><br>  El grado de concurrencia ya se ha triplicado, y nada augura que la situaci√≥n cambiar√° para mejor.  No dibuj√© m√°s, porque el tiempo de respuesta a la tercera solicitud ya no cabe en la imagen. <br><br><blockquote>  Nuestro servidor ha entrado en un estado no deseado, del cual nunca saldr√° por s√≠ solo.  <b>Juego terminado</b> </blockquote><br><h2>  ¬øQu√© caracteriza el estado GameOver del servidor? </h2><br>  Las solicitudes se acumulan en la memoria indefinidamente.  Tarde o temprano, la memoria simplemente terminar√°.  Adem√°s, con un aumento en la escala, aumenta la sobrecarga de la CPU para dar servicio a diversas estructuras de datos.  Por ejemplo, el grupo de conexiones ahora debe rastrear los tiempos de espera para m√°s conexiones, el recolector de basura ahora debe verificar dos veces m√°s objetos en el mont√≥n, y as√≠ sucesivamente. <br><br>  Investigar todas las posibles consecuencias de la acumulaci√≥n de objetos activos no es el prop√≥sito de este art√≠culo, pero incluso una simple acumulaci√≥n de datos en la RAM ya es suficiente para llenar el servidor.  Adem√°s, ya hemos visto que el servidor del cliente proyecta sus problemas de concurrencia en el servidor de la base de datos y en otros servidores que utiliza como cliente. <br><br>  Lo m√°s interesante: ahora, incluso si env√≠a una carga menor al servidor, a√∫n no se recuperar√°.  Todas las solicitudes finalizar√°n con un tiempo de espera y el servidor consumir√° todos los recursos disponibles. <br><br>  ¬øY qu√© esper√°bamos realmente?  Despu√©s de todo, a sabiendas le dimos al servidor una cantidad de trabajo que no pod√≠a manejar. <br><br>  Cuando se trata de arquitectura de sistema distribuido, es √∫til pensar en c√≥mo la gente com√∫n resuelve tales problemas.  Tome, por ejemplo, una discoteca.  Dejar√° de funcionar si ingresan demasiadas personas.  El portero hace frente al problema simplemente: mira cu√°ntas personas hay dentro.  Una izquierda - lanza otra.  Un nuevo invitado vendr√° y apreciar√° el tama√±o de la cola.  Si la cola es larga, se ir√° a casa.  ¬øQu√© pasa si aplica este algoritmo al servidor? <br><br><img src="https://habrastorage.org/webt/wg/tn/p3/wgtnp3n6qq57dqzfi-ac9s0rj1u.png"><br><br>  Juguemos de nuevo. <br><br><h4>  Juego n√∫mero 2 </h4><br>  <b>Requisitos previos</b> : nuevamente tenemos dos CPU, las mismas tareas de 3 unidades, llegando a cada per√≠odo, pero ahora configuraremos el dispositivo de seguridad, y las tareas ser√°n inteligentes: si ven que la longitud de la cola es 2, se van a casa de inmediato. <br><br><img src="https://habrastorage.org/webt/b4/cs/gt/b4csgtkcmi3hw1qog4fko6aus2o.png"><br><br><img src="https://habrastorage.org/webt/uw/gi/-v/uwgi-vopihzere8fs_c5f5yctfo.png"><br><br>  Lleg√≥ la tercera solicitud.  En este per√≠odo, √©l hace cola.  Tiene el n√∫mero 3 al final del per√≠odo.  No hay n√∫meros fraccionarios en los residuos, porque dos CPU realizan dos tareas, una por un per√≠odo. <br><br>  Aunque tenemos tres solicitudes en capas, el grado de concurrencia dentro del sistema = 2. La tercera est√° en la cola y no cuenta. <br><br><img src="https://habrastorage.org/webt/nm/x4/yw/nmx4ywd6xdqyte5b6vkwgco3hdq.png"><br><br>  Lleg√≥ el cuarto: la misma imagen, aunque ya se ha acumulado m√°s trabajo. <br><br><img src="https://habrastorage.org/webt/uq/jb/_5/uqjb_5b8whjwjzzetwqwrzjusfy.png"><br>  ... <br>  ... <br><br>  En el sexto per√≠odo, la tercera solicitud se complet√≥ con un tercer retraso, y el grado de concurrencia ya es = 4. <br><br><img src="https://habrastorage.org/webt/xs/i1/sf/xsi1sf60jniqqbamvlko-bmd_xa.png"><br><br>  El grado de concurrencia se ha duplicado.  Ella ya no puede crecer, porque hemos establecido una prohibici√≥n clara sobre esto.  Con la velocidad m√°xima, solo se completaron las dos primeras solicitudes: las que vinieron primero al club, mientras hab√≠a suficiente espacio para todos. <br><br>  Las solicitudes amarillas estuvieron en el sistema durante m√°s tiempo, pero se mantuvieron en l√≠nea y no retrasaron el recurso de la CPU.  Por lo tanto, los que estaban adentro se estaban divirtiendo.  Esto podr√≠a continuar hasta que un hombre viniera y dijera que no har√≠a cola, sino que se ir√≠a a casa.  Esta es una solicitud fallida: <br><br><img src="https://habrastorage.org/webt/tf/qf/qr/tfqfqrsfqvhxphy3wzrdqf_cpvk.png"><br><br>  La situaci√≥n se puede repetir sin cesar, mientras que el tiempo de ejecuci√≥n de la consulta permanece en el mismo nivel, exactamente el doble de lo que quisi√©ramos. <br><br><img src="https://habrastorage.org/webt/xh/l2/8-/xhl28-uoe_jao9hjppy5zc30in8.png"><br><br>  Vemos que una restricci√≥n simple en el nivel de concurrencia elimina el problema de viabilidad del servidor. <br><br><h4>  C√≥mo aumentar la viabilidad del servidor a trav√©s del l√≠mite de nivel de concurrencia </h4><br>  Puedes escribir el "gorila" m√°s simple t√∫ mismo.  A continuaci√≥n se muestra el c√≥digo que utiliza el sem√°foro.  No hay l√≠mite para la longitud de la l√≠nea exterior.    ,    . <br><br><pre><code class="swift hljs">const int <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span> = <span class="hljs-number"><span class="hljs-number">100</span></span>; <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span> bulkhead = new <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span>(<span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>, <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> async <span class="hljs-type"><span class="hljs-type">Task</span></span> <span class="hljs-type"><span class="hljs-type">ProcessRequest</span></span>() { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!await bulkhead.<span class="hljs-type"><span class="hljs-type">WaitAsync</span></span>()) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> new <span class="hljs-type"><span class="hljs-type">OperationCanceledException</span></span>(); } <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { await <span class="hljs-type"><span class="hljs-type">ProcessRequestInternal</span></span>(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } finally { bulkhead.<span class="hljs-type"><span class="hljs-type">Release</span></span>(); } }</code> </pre> <br>  Para crear una cola limitada, necesita dos sem√°foros.  Para esto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la biblioteca Polly</a> , que Microsoft recomienda, es adecuada.  Presta atenci√≥n al patr√≥n de mamparo.  Traducido literalmente como "mamparo": un elemento estructural que permite que el barco no se hunda.  Para ser sincero, creo que el t√©rmino gorila es m√°s adecuado.  Lo importante es que este patr√≥n permite al servidor sobrevivir en situaciones desesperadas. <br><br>  Primero, exprimimos todo lo que es posible en el banco de carga del servidor hasta que determinamos cu√°ntas solicitudes puede contener.  Por ejemplo, determinamos que es 100. Ponemos mamparo. <br><br>  Adem√°s, el servidor omitir√° solo el n√∫mero requerido de solicitudes, el resto se pondr√° en cola.  Ser√≠a aconsejable elegir un n√∫mero un poco m√°s peque√±o para que haya un margen.  No tengo ninguna recomendaci√≥n sobre este tema, porque existe una fuerte dependencia del contexto y la situaci√≥n espec√≠fica. <br><br><ol><li>  Si el comportamiento del servidor depende de manera estable de la carga en t√©rminos de recursos, entonces este n√∫mero puede acercarse al l√≠mite. </li><li>  Si el medio est√° sujeto a fluctuaciones de carga, se debe elegir un n√∫mero m√°s conservador, teniendo en cuenta el tama√±o de estas fluctuaciones.  Tales fluctuaciones pueden ocurrir por varias razones, por ejemplo, el entorno de rendimiento con GC se caracteriza por peque√±os picos de carga en la CPU. </li><li>  Si el servidor realiza tareas peri√≥dicas en un horario, esto tambi√©n debe ser considerado.  Incluso puede desarrollar un mamparo adaptativo que calcular√° cu√°ntas consultas se pueden enviar simult√°neamente sin degradaci√≥n del servidor (pero esto ya est√° fuera del alcance de este estudio). </li></ol><br><h2>  Consultar experimentos </h2><br>  Eche un vistazo a este post-mortem por √∫ltimo, no volveremos a ver esto. <br><img src="https://habrastorage.org/webt/n1/qn/1i/n1qn1irfrgm-fezifzonark7j94.png"><br>  Todo este mont√≥n gris se correlaciona inequ√≠vocamente con el bloqueo del servidor.  Gray es la muerte del servidor.  Vamos a cortarlo y ver qu√© pasa.  Parece que un cierto n√∫mero de solicitudes se enviar√°n a casa, simplemente no se cumplir√°n.  Pero cuanto? <br><br><h4>  100 adentro, 100 afuera </h4><br><img src="https://habrastorage.org/webt/u0/6c/m5/u06cm5odrt-dxltnomhwatmhwze.png"><br>  Result√≥ que nuestro servidor comenz√≥ a vivir muy bien y divertido.  √âl constantemente ara a la m√°xima potencia.  Por supuesto, cuando ocurre un pico, lo expulsa, pero no por mucho tiempo. <br><br>  Inspirado por el √©xito, trataremos de asegurarnos de que no sea rebotado en absoluto.  Intentemos aumentar la longitud de la cola. <br><br><h4>  100 adentro, 500 afuera </h4><br><img src="https://habrastorage.org/webt/sk/5i/gi/sk5igi9cfgyjajp8lujawwunace.png"><br><br>  Mejor√≥, pero la cola creci√≥.  Estas son las solicitudes que se ejecutan mucho tiempo despu√©s. <br><br><h4>  100 adentro, 1000 afuera </h4><br>  Como algo ha mejorado, intentemos llevarlo al punto del absurdo.  Resolvamos la longitud de la cola 10 veces m√°s de lo que podemos servir simult√°neamente: <br><br><img src="https://habrastorage.org/webt/2g/qd/s6/2gqds68piszleawyid_yk_rtdhg.png"><br><br>  Si hablamos de la met√°fora del club y los gorilas, esta situaci√≥n es casi imposible: nadie quiere esperar m√°s tiempo en la entrada que pasar tiempo en el club.  Tampoco vamos a fingir que esta es una situaci√≥n normal para nuestro sistema. <br><br>  Es mejor no atender al cliente en absoluto que atormentarlo en el sitio o en la aplicaci√≥n m√≥vil cargando cada pantalla durante 30 segundos y estropeando la reputaci√≥n de la empresa.  Es mejor decirle honestamente de inmediato a una peque√±a parte de los clientes que ahora no podemos atenderlos.  De lo contrario, serviremos a todos los clientes varias veces m√°s lentamente, porque el gr√°fico muestra que la situaci√≥n persiste durante bastante tiempo. <br><br>  Hay un riesgo m√°s: otros componentes del sistema pueden no estar dise√±ados para dicho comportamiento del servidor y, como ya sabemos, la concurrencia se proyecta en los clientes. <br><br>  Por lo tanto, volvemos a la primera opci√≥n "100 por 100" y pensamos en c√≥mo escalar nuestras capacidades. <br><br><h4>  Ganador: 100 adentro, 100 afuera </h4><br><img src="https://habrastorage.org/webt/z5/cg/pv/z5cgpvn4qs9abj5ifewfelapa6g.png"><br><br>  ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  Con estos par√°metros, la mayor degradaci√≥n en tiempo de ejecuci√≥n es exactamente 2 veces la "nominal".  Al mismo tiempo, es una degradaci√≥n del 100% en el tiempo de ejecuci√≥n de la consulta. <br><br>  Si su cliente es sensible al tiempo de ejecuci√≥n (y esto suele ser cierto tanto con clientes humanos como con clientes de servidor), puede pensar en reducir a√∫n m√°s la longitud de la cola.  En este caso, podemos tomar un porcentaje de la concurrencia interna, y sabremos con certeza que el servicio no se degrada en tiempo de respuesta en m√°s de este porcentaje en promedio. <br><br>  De hecho, no estamos tratando de crear una cola, estamos tratando de protegernos de las fluctuaciones de carga.  Aqu√≠, al igual que en el caso de determinar el primer par√°metro del mamparo (cantidad dentro), es √∫til determinar qu√© fluctuaciones en la carga puede causar el cliente.  Entonces sabremos en qu√© casos, en t√©rminos generales, perderemos el beneficio del servicio potencial. <br><br>  Es a√∫n m√°s importante determinar qu√© fluctuaciones de latencia pueden soportar otros componentes del sistema que interact√∫an con el servidor.  Entonces sabremos que realmente estamos exprimiendo al m√°ximo el sistema existente sin el peligro de perder el servicio por completo. <br><br><h2>  Diagn√≥stico y tratamiento. </h2><br>  Estamos tratando la concurrencia no controlada con aislamiento de mamparo. <br>  Este m√©todo, como los otros discutidos en esta serie de art√≠culos, es implementado convenientemente por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la biblioteca Polly</a> . <br><br>  La ventaja del m√©todo es que ser√° extremadamente dif√≠cil desestabilizar un componente individual del sistema como tal.  El sistema adquiere un comportamiento muy predecible en t√©rminos de tiempo para solicitudes exitosas y posibilidades mucho mayores para solicitudes completas exitosas. <br><br>  Sin embargo, no resolvemos todos los problemas.  Por ejemplo, el problema de la insuficiente potencia del servidor.  En esta situaci√≥n, obviamente debe decidir "soltar el lastre" en caso de un salto en la carga, lo que consideramos excesivo. <br><br>  Otras medidas que nuestro estudio no aborda pueden incluir, por ejemplo, el escalado din√°mico. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/461081/">https://habr.com/ru/post/461081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461071/index.html">Optimizaci√≥n de consultas de bases de datos en el ejemplo de servicio B2B para constructores</a></li>
<li><a href="../461073/index.html">Conectamos mapas en l√≠nea al navegador en el tel√©fono inteligente. Parte 3 - OverpassTurbo</a></li>
<li><a href="../461075/index.html">Inteligencia empresarial. Objetos inform√°ticos, componentes, herramientas.</a></li>
<li><a href="../461077/index.html">¬øC√≥mo se cocinan los pentesters? Pruebas de entrada para pasantes de seguridad digital</a></li>
<li><a href="../461079/index.html">Ciudad sin atascos</a></li>
<li><a href="../461083/index.html">Escritura de software con la funcionalidad de las utilidades cliente-servidor de Windows, parte 02</a></li>
<li><a href="../461085/index.html">Cambiar idioma en la aplicaci√≥n de Android</a></li>
<li><a href="../461087/index.html">Generando mazmorras y cuevas para mi juego</a></li>
<li><a href="../461091/index.html">L√°mparas LED Camelion</a></li>
<li><a href="../461093/index.html">Noticias del mundo de OpenStreetMap No. 469 (07/09/2019 - 07/07/2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>