<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤟🏿 👂🏾 🚍 Bagaimana kami membantu CDN MegaFon.TV tidak mencapai Piala Dunia 2018 👩🏽‍🤝‍👨🏾 👩🏽‍🤝‍👨🏿 🤵🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada 2016, kami berbicara tentang bagaimana MegaFon.TV mengatasi semua orang yang ingin menonton musim baru Game of Thrones. Pengembangan layanan tida...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana kami membantu CDN MegaFon.TV tidak mencapai Piala Dunia 2018</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/megafon/blog/425229/">  Pada 2016, kami berbicara tentang bagaimana MegaFon.TV mengatasi semua orang yang ingin menonton musim baru Game of Thrones.  Pengembangan layanan tidak berhenti di situ, dan pada pertengahan 2017 kami harus berurusan dengan beban beberapa kali lebih banyak.  Dalam posting ini kami akan menceritakan bagaimana pertumbuhan yang begitu cepat menginspirasi kami untuk secara radikal mengubah pendekatan untuk mengorganisir CDN dan bagaimana pendekatan baru ini diuji oleh Piala Dunia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a42/af2/54b/a42af254b7c4d8528fb0d495c27ab6d9.png"><br><a name="habracut"></a><br><h2>  Secara singkat tentang MegaFon.TV </h2><br>  MegaFon.TV adalah layanan OTT untuk melihat berbagai konten video - film, acara TV, saluran TV, dan program yang direkam.  Melalui MegaFon.TV, akses ke konten dapat diperoleh di hampir semua perangkat: di ponsel dan tablet dengan iOS dan Android, di TV pintar LG, Samsung, Philips, Panasonic dari tahun rilis yang berbeda, dengan kebun binatang OS lengkap (Apple TV, TV Android), di browser desktop di Windows, MacOS, Linux, di browser seluler di iOS dan Android, dan bahkan pada perangkat eksotis seperti STB dan proyektor Android anak-anak.  Praktis tidak ada batasan pada perangkat - hanya ketersediaan Internet dengan bandwidth 700 Kbps yang penting.  Tentang bagaimana kami mengatur dukungan untuk banyak perangkat, akan ada artikel terpisah di masa depan. <br>  Sebagian besar pengguna layanan ini adalah pelanggan MegaFon, yang dijelaskan oleh penawaran menguntungkan (dan paling sering bahkan gratis) yang termasuk dalam paket tarif pelanggan.  Meskipun kami juga mencatat peningkatan yang signifikan dalam pengguna operator lain.  Sesuai dengan distribusi ini, 80% dari lalu lintas MegaFon.TV dikonsumsi dalam jaringan MegaFon. <br><br>  Secara arsitektur, sejak peluncuran layanan, konten telah didistribusikan melalui CDN.  Kami memiliki <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pos</a> terpisah yang didedikasikan untuk pekerjaan CDN ini.  Di dalamnya, kami berbicara tentang bagaimana hal itu memungkinkan kami untuk menangani lalu lintas puncak yang pergi ke layanan pada akhir 2016, selama rilis musim baru Game of Thrones.  Dalam posting ini kita akan berbicara tentang pengembangan lebih lanjut dari MegaFon.TV dan tentang petualangan baru yang telah jatuh pada layanan bersama dengan Piala Dunia 2018 <br><br><h2>  Pertumbuhan layanan.  Dan masalah </h2><br>  Dibandingkan dengan peristiwa dari pos terakhir, pada akhir 2017 jumlah pengguna Megafon.TV telah meningkat beberapa kali, film dan serial juga menjadi urutan besarnya lebih besar.  Fungsionalitas baru diluncurkan, paket baru muncul, tersedia "dengan berlangganan".  Puncak lalu lintas sejak "Game of Thrones" sekarang kita lihat setiap hari, proporsi film dan acara TV dalam aliran total terus meningkat. <br><br>  Seiring dengan ini, masalah dimulai dengan redistribusi traffic.  Pemantauan kami, yang dikonfigurasikan untuk mengunduh potongan untuk berbagai jenis lalu lintas dalam berbagai format, semakin mulai menghasilkan kesalahan saat mengunduh potongan video saat batas waktu habis.  Dalam layanan MegaFon.TV, panjang chunk adalah 8 detik.  Jika potongan tidak memiliki waktu untuk memuat dalam 8 detik, kesalahan dapat terjadi. <br><br>  Puncak kesalahan diharapkan terjadi pada jam-jam paling sibuk.  Bagaimana hal ini memengaruhi pengguna?  Paling tidak, mereka dapat mengamati penurunan kualitas video.  Itu tidak selalu terlihat dengan mata telanjang, karena cukup banyak profil multi-bitrate.  Dalam kasus terburuk, videonya macet. <br><br>  Pencarian masalah dimulai.  Hampir segera, menjadi jelas bahwa kesalahan kickback terjadi pada server EDGE CDN.  Di sini kita perlu melakukan penyimpangan kecil dan memberi tahu bagaimana server bekerja dengan lalu lintas langsung dan VOD.  Skemanya sedikit berbeda.  Seorang pengguna yang datang ke server EDGE untuk konten (daftar putar atau potongan), jika ada konten dalam cache, mendapatkannya dari sana.  Kalau tidak, server EDGE berlaku untuk konten di Origin, memuat saluran utama.  Bersama dengan playlist atau chunk, header <b>Cache-Control: max-age</b> diberikan, yang memberi tahu server EDGE berapa banyak untuk menembolok unit konten tertentu.  Perbedaan antara LIVE dan VOD terletak pada waktu yang dibutuhkan untuk cache cache.  Untuk potongan langsung, waktu caching pendek diatur, biasanya dari 30 detik hingga beberapa menit - ini disebabkan oleh waktu relevansi konten langsung yang singkat.  Cache ini disimpan dalam RAM, karena Anda harus terus-menerus memberikan potongan dan menulis ulang cache.  Untuk potongan VOD, lebih banyak waktu yang ditetapkan, dari beberapa jam hingga berminggu-minggu dan bahkan berbulan-bulan - tergantung pada ukuran pustaka konten dan distribusi pandangannya di antara pengguna.  Adapun playlist, mereka biasanya di-cache dalam waktu tidak lebih dari dua detik, atau mereka tidak di-cache sama sekali.  Perlu diklarifikasi bahwa kita hanya berbicara tentang apa yang disebut PULL-mode CDN, di mana server kami bekerja.  Menggunakan mode PUSH dalam kasus kami tidak sepenuhnya dibenarkan. <br><br>  Tetapi kembali untuk menemukan masalah.  Seperti yang telah kita perhatikan, semua server secara bersamaan bekerja untuk mengembalikan kedua jenis konten.  Pada saat yang sama, server sendiri memiliki konfigurasi yang berbeda.  Akibatnya, beberapa mesin kelebihan beban menggunakan IOPS.  Bongkahan tidak punya waktu untuk menulis / membaca karena kinerja kecil, kuantitas, volume disk, pustaka konten yang besar.  Di sisi lain, mesin yang lebih kuat yang menerima lebih banyak lalu lintas mulai gagal pada penggunaan CPU.  Sumber daya CPU dihabiskan untuk melayani lalu lintas SSL dan potongan yang dikirimkan melalui https, sementara IOPS pada disk hampir tidak mencapai 35%. <br><br>  Yang dibutuhkan adalah skema yang akan, dengan biaya minimal, memungkinkan untuk menggunakan kapasitas yang tersedia secara optimal.  Selain itu, enam bulan kemudian Piala Dunia akan dimulai, dan menurut perhitungan awal, puncak lalu lintas langsung seharusnya meningkat enam kali lipat ... <br><br><h2>  Pendekatan baru terhadap CDN </h2><br>  Setelah menganalisis masalah, kami memutuskan untuk memisahkan VOD dan lalu lintas langsung menurut PAD yang berbeda yang terdiri dari server dengan konfigurasi yang berbeda.  Dan juga membuat fungsi distribusi lalu lintas dan penyeimbangannya di berbagai kelompok server.  Total ada tiga kelompok: <br><br><ul><li>  Server dengan sejumlah besar disk berkinerja tinggi yang paling cocok untuk caching konten VOD.  Faktanya, disk SSD RI dengan kapasitas maksimum akan paling cocok, tetapi tidak ada, dan akan membutuhkan terlalu banyak anggaran untuk membeli jumlah yang tepat.  Pada akhirnya, diputuskan untuk menggunakan yang terbaik yang tersedia.  Setiap server berisi delapan disk SAS 1TB 10k di RAID5.  Dari server ini VOD_PAD dikompilasi. <br></li><li>  Server dengan sejumlah besar RAM untuk caching semua format yang mungkin untuk pengiriman potongan langsung, dengan prosesor yang mampu menangani lalu lintas ssl, dan antarmuka jaringan "tebal".  Kami menggunakan konfigurasi berikut: 2 prosesor 8 core / 192GB RAM / 4 antarmuka 10GB.  Dari server ini, EDGE_PAD dikompilasi. <br></li><li>  Grup server yang tersisa tidak dapat menangani lalu lintas VOD, tetapi cocok untuk volume kecil konten langsung.  Mereka dapat digunakan sebagai cadangan.  Dari server RESERVE_PAD dikompilasi. <br></li></ul><br>  Distribusi adalah sebagai berikut: <br><img src="https://habrastorage.org/getpro/habr/post_images/3ed/17c/dbf/3ed17cdbf13920eae4c06067e2edd296.png"><br>  Modul logika khusus bertanggung jawab untuk memilih PAD dari mana pengguna seharusnya menerima konten.  Inilah tugasnya: <br><ul><li>  Analisis URL, terapkan skema di atas untuk setiap permintaan streaming dan keluarkan PAD yang diperlukan <br></li><li>  Untuk menghapus beban dari antarmuka EDGE_PAD setiap 5 menit ( <i>dan ini adalah kesalahan kami</i> ), dan ketika batas tercapai, alihkan lalu lintas berlebih ke RESERVE_PAD.  Untuk meringankan beban, skrip perl kecil ditulis yang mengembalikan data berikut: <br>  - <b>cap waktu</b> - tanggal dan waktu memperbarui data pemuatan (dalam format RFC 3339); <br>  - <b>total_bandwidth</b> - beban antarmuka saat ini (total), Kbps; <br>  - <b>rx_bandwidth</b> - beban antarmuka saat ini (lalu lintas masuk), Kbps; <br>  - <b>tx_badwidth</b> - beban antarmuka saat ini (lalu lintas keluar), Kbps. <br></li><li>  Lalu lintas langsung dalam mode manual ke server PAD atau Asal jika terjadi situasi yang tidak terduga, atau jika perlu, bekerja di salah satu PAD.  Konfigurasi berada di server dalam format yaml dan diizinkan untuk mengambil semua lalu lintas ke PAD yang diinginkan, atau lalu lintas sesuai dengan salah satu parameter: <br>  - Jenis Konten <br>  - enkripsi lalu lintas <br>  - Lalu lintas berbayar <br>  - tipe perangkat <br>  - Jenis daftar putar <br>  - Wilayah <br></li></ul><br>  Server asal adalah SSD yang kekurangan staf.  Sayangnya, ketika mengalihkan lalu lintas ke Asal, HIT_RATE pada potongan VOD menyisakan banyak yang harus diinginkan (sekitar 30%), tetapi mereka melakukan tugas mereka, jadi kami tidak mengamati masalah dengan pembuat paket di CNN. <br><br>  Karena ada beberapa server untuk konfigurasi EDGE_PAD, diputuskan untuk mengalokasikannya di wilayah dengan pangsa lalu lintas terbesar - Moskow dan wilayah Volga.  Dengan bantuan GeoDNS, lalu lintas dikirim ke wilayah Volga dari wilayah Volga dan distrik federal Ural.  Hub Moskow melayani sisanya.  Kami tidak benar-benar menyukai gagasan untuk mengirimkan lalu lintas ke Siberia dan Timur Jauh dari Moskow, tetapi secara total wilayah-wilayah ini menyumbang sekitar 1/20 dari semua lalu lintas, dan saluran MegaFon ternyata cukup lebar untuk volume seperti itu. <br>  Setelah pengembangan rencana, pekerjaan berikut dilakukan: <br><br><ul><li>  Dalam dua minggu, dikembangkan fungsi switching CDN <br></li><li>  Butuh waktu satu bulan untuk menginstal dan mengonfigurasi server EDGE_PAD, serta memperluas saluran untuk mereka <br></li><li>  Butuh dua minggu untuk membagi grup server saat ini menjadi dua bagian, ditambah dua minggu lagi untuk menerapkan pengaturan ke semua server di jaringan dan peralatan server <br></li><li>  Dan, akhirnya, minggu dihabiskan untuk pengujian (sayangnya, tidak di bawah beban, yang kemudian terpengaruh) <br></li></ul><br>  Ternyata untuk memparalelkan beberapa pekerjaan, dan pada akhirnya semua itu memakan waktu enam minggu. <br><br><h2>  Hasil Pertama dan Rencana Masa Depan </h2><br>  Setelah penyetelan, kinerja sistem secara keseluruhan adalah 250 Gb / s.  Solusi dengan transfer lalu lintas VOD ke server terpisah segera menunjukkan efektivitasnya setelah diluncurkan ke produksi.  Sejak awal Piala Dunia, tidak ada masalah dengan lalu lintas VOD.  Beberapa kali, karena berbagai alasan, saya harus mengalihkan lalu lintas VOD ke Asal, tetapi pada prinsipnya, mereka juga mengatasinya.  Mungkin skema ini tidak terlalu efektif karena penggunaan cache yang sangat kecil, karena kami memaksa SSD untuk terus-menerus menimpa konten.  Tetapi rangkaian itu bekerja. <br><br>  Adapun lalu lintas langsung, volume yang sesuai untuk menguji keputusan kami muncul dengan dimulainya Piala Dunia.  Masalah dimulai ketika kedua kalinya kami menghadapi peralihan lalu lintas ketika kami mencapai batas selama pertandingan Rusia-Mesir.  Ketika peralihan lalu lintas berfungsi, semuanya mengalir ke PAD cadangan.  Dalam lima menit ini, jumlah permintaan (kurva pertumbuhan) begitu besar sehingga cadangan CDN tersumbat sepenuhnya dan mulai menuangkan kesalahan.  Pada saat yang sama, PAD utama dirilis selama waktu ini dan mulai sedikit diam: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae7/5a6/804/ae75a68044b8150556324ccfbde19827.png"><br><br>  3 kesimpulan diambil dari ini: <br><br><ol><li>  Lima menit masih terlalu banyak.  Diputuskan untuk mengurangi periode bongkar menjadi 30 detik.  Akibatnya, lalu lintas pada PAD siaga berhenti tumbuh secara spasmodik: <br><img src="https://habrastorage.org/getpro/habr/post_images/f73/e8d/47a/f73e8d47a26d62c8fc6d82f88d442fad.png"><br></li><li>  Minimal diperlukan untuk mentransfer pengguna antara PAD setiap kali sakelar dipicu.  Ini harus memberikan kelancaran tambahan switching.  Kami memutuskan untuk memberikan cookie kepada setiap pengguna (atau lebih tepatnya perangkat), yang menurutnya modul yang bertanggung jawab untuk distribusi memahami apakah pengguna harus dibiarkan menggunakan PAD saat ini atau jika pengalihan harus dilakukan.  Di sini teknologi mungkin atas kebijaksanaan orang yang mengimplementasikannya.  Akibatnya, kami tidak menjatuhkan lalu lintas di PAD utama. <br></li><li>  Ambang untuk beralih ditetapkan terlalu rendah, sebagai akibatnya, lalu lintas pada cadangan PAD tumbuh seperti longsoran salju.  Dalam kasus kami, ini adalah reasuransi - kami tidak sepenuhnya yakin bahwa kami membuat penyetelan server yang benar (ide yang diambil dari Habr).  Ambang batas telah ditingkatkan ke kinerja fisik antarmuka jaringan. <br></li></ol><br>  Perbaikan membutuhkan waktu tiga hari, dan sudah di pertandingan Rusia-Kroasia, kami memeriksa apakah optimasi kami berhasil.  Secara umum, hasilnya memuaskan kami.  Pada puncaknya, sistem memproses 215 Gbit / s lalu lintas campuran.  Ini bukan batas teoritis pada kinerja sistem - kami masih memiliki margin yang substansial.  Jika perlu, sekarang kita dapat menghubungkan CDN eksternal apa pun, jika perlu, dan "membuang" lalu lintas berlebih di sana.  Model seperti itu bagus ketika Anda tidak ingin membayar uang padat setiap bulan untuk menggunakan CDN orang lain. <br><br>  Rencana kami meliputi pengembangan lebih lanjut dari CDN.  Untuk mulai dengan, saya ingin memperluas skema EDGE_PAD ke semua distrik federal - ini akan menyebabkan lebih sedikit penggunaan saluran.  Tes sirkuit redundansi VOD_PAD juga sedang dilakukan, dan beberapa hasil sekarang terlihat cukup mengesankan. <br><br>  Secara umum, semua yang dilakukan selama setahun terakhir membuat saya berpikir bahwa CDN dari layanan yang mendistribusikan konten video harus dimiliki.  Dan bahkan bukan karena itu memungkinkan Anda untuk menghemat banyak uang, tetapi lebih karena CDN menjadi bagian dari layanan itu sendiri, secara langsung mempengaruhi kualitas dan fungsionalitas.  Dalam keadaan seperti itu, memberikannya ke tangan yang salah setidaknya tidak masuk akal. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id425229/">https://habr.com/ru/post/id425229/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id425219/index.html">Apa itu kesehatan mental: perspektif dari psikologi / psikoterapi</a></li>
<li><a href="../id425221/index.html">Cara membuat plastik untuk pencetakan 3D</a></li>
<li><a href="../id425223/index.html">Aplikasi JPHP Android</a></li>
<li><a href="../id425225/index.html">Cara melihat tautan di dalam modul PowerShell Anda</a></li>
<li><a href="../id425227/index.html">Para peneliti telah menemukan cara untuk mendeteksi dan memotong kunci Honeytoken di sejumlah layanan Amazon.</a></li>
<li><a href="../id425231/index.html">FAQ tentang pekerjaan pramugari</a></li>
<li><a href="../id425233/index.html">Python 3 di Facebook</a></li>
<li><a href="../id425235/index.html">Lebih banyak tentang grafik, atau cara mendeteksi ketergantungan antara aplikasi Anda</a></li>
<li><a href="../id425237/index.html">Pengukuran Waktu dengan Akurasi Nanosecond</a></li>
<li><a href="../id425241/index.html">Pengembang 20 tahun kemudian: Dengan mudah Lebedev tentang ICRE, pendidikan, bukunya, dan pemrogramannya</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>