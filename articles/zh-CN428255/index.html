<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏾‍🤝‍🧑🏼 🤬 ♨️ 转移学习：如何在数据上快速训练神经网络 ⬆️ 🤲🏿 👩🏿‍🤝‍👨🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="机器学习正变得越来越容易获得，有更多的机会使用“现成的组件”来应用该技术。 例如，转移学习使您可以利用在解决一个问题中获得的经验来解决另一个类似的问题。 首先在大量数据上训练神经网络，然后在目标集上训练神经网络。 



 在本文中，我将以带食物的图像识别示例为例，介绍如何使用转移学习方法。 我将在...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>转移学习：如何在数据上快速训练神经网络</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/binarydistrict/blog/428255/"> 机器学习正变得越来越容易获得，有更多的机会使用“现成的组件”来应用该技术。 例如，转移学习使您可以利用在解决一个问题中获得的经验来解决另一个类似的问题。 首先在大量数据上训练神经网络，然后在目标集上训练神经网络。 <br><br><img src="https://habrastorage.org/webt/q-/wr/cn/q-wrcns6clfsv1n2k6gki-sdoea.jpeg" alt="食品认可"><br><br> 在本文中，我将以带食物的图像识别示例为例，介绍如何使用转移学习方法。 我将在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">面向开发人员的机器学习和神经网络</a>研讨会上讨论其他机器学习工具。 <br><a name="habracut"></a><br> 如果我们面临图像识别的任务，则可以使用现成的服务。 但是，如果需要在自己的数据集上训练模型，则必须自己完成。 <br><br> 对于图像分类等典型任务，您可以使用现成的体系结构（AlexNet，VGG，Inception，ResNet等），并在数据上训练神经网络。 已经使用各种框架实现了此类网络，因此在此阶段，您可以将其中一个用作黑匣子，而无需深入研究其操作原理。 <br><br> 但是，深度神经网络需要大量数据才能使学习趋于一致。 通常在我们的特定任务中，没有足够的数据来正确训练神经网络的所有层。 转移学习解决了这个问题。 <br><br><h1> 转移学习进行图像分类 </h1><br> 用于分类的神经网络通常在最后一层包含<code>N</code>输出神经元，其中<code>N</code>是类别数。 这样的输出矢量被视为属于一个类别的一组概率。 在我们识别食物图像的任务中，类别的数量可能与原始数据集中的类别数量不同。 在这种情况下，我们将必须完全淘汰最后一层，并放入新一层，并使用正确数量的输出神经元 <br><br><img src="https://habrastorage.org/webt/u_/n3/k3/u_n3k3qpkps6nw9tjjzwc0njl-y.jpeg" alt="转移学习"><br><br> 通常在分类网络的末尾使用完全连接的层。 由于我们替换了该层，因此对其使用预训练权重将不起作用。 您将不得不从头开始训练他，并使用随机值初始化他的权重。 我们从预先训练的快照中为所有其他层加载权重。 <br><br> 有多种策略可以进一步训练模型。 我们将使用以下内容：我们将端到端（ <i>end-to-end</i> ）训练整个网络，并且我们将不固定预先训练的权重，以允许他们稍微调整并调整我们的数据。 此过程称为<i>微调</i> 。 <br><br><h1> 结构组件 </h1><br> 要解决此问题，我们需要以下组件： <br><br><ol><li> 神经网络模型的描述 </li><li> 学习管道 </li><li> 干扰管道 </li><li> 该模型的预训练权重 </li><li> 培训和验证数据 </li></ol><br><img src="https://habrastorage.org/webt/tf/xp/2o/tfxp2on4o-rxj6hnt4dij7u8vlk.jpeg" alt="组成部分"><br><br> 在我们的示例中，我将从<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">自己的存储库中</a>获取组件（1），（2）和（3），该<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">存储库</a>包含最轻量的代码-如果需要，您可以轻松地找出来。 我们的示例将在流行的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">TensorFlow</a>框架上实现。 如果适合于所选框架的预训练权重（4）与经典架构之一相对应，则可以找到它们。 作为演示的数据集（5），我将选择<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Food-101</a> 。 <br><br><h1> 型号 </h1><br> 作为模型，我们使用经典的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">VGG</a>神经网络（更确切地说是<i>VGG19</i> ）。 尽管有一些缺点，该模型仍显示出相当高的质量。 另外，它很容易分析。 在TensorFlow Slim上，模型描述看起来非常紧凑： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vgg_19</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inputs, num_classes, is_training, scope=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'vgg_19'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, weight_decay=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0005</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, weights_regularizer=slim.l2_regularizer(weight_decay), biases_initializer=tf.zeros_initializer(), padding=<span class="hljs-string"><span class="hljs-string">'SAME'</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(scope, <span class="hljs-string"><span class="hljs-string">'vgg_19'</span></span>, [inputs]): net = slim.repeat(inputs, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">64</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv1'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool1'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">128</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv2'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool2'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">256</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv3'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool3'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv4'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool4'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv5'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool5'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Use conv2d instead of fully_connected layers net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop6') net = slim.conv2d(net, 4096, [1, 1], scope='fc7') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop7') net = slim.conv2d(net, num_classes, [1, 1], scope='fc8', activation_fn=None) net = tf.squeeze(net, [1, 2], name='fc8/squeezed') return net</span></span></code> </pre><br> 从ImageNet上训练并与TensorFlow兼容的VGG19权重是从GitHub上的存储库中的`` <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">预训练模型''</a>部分下载的。 <br><br><pre> <code class="bash hljs">mkdir data &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz tar -xzf vgg_19_2016_08_28.tar.gz</code> </pre><br><h1> 数据中心 </h1><br> 作为培训和验证样本，我们将使用公开的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Food-101</a>数据集，其中包含超过10万个食物图像，分为101类。 <br><br><img src="https://habrastorage.org/webt/re/oh/pb/reohpbmt76_3qfzplzccsz-hbf8.jpeg" alt="Food-101数据集"><br><br> 下载并解压缩数据集： <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz tar -xzf food-101.tar.gz</code> </pre><br> 我们在培训中设计了数据管道，因此需要从数据集中解析以下内容： <br><br><ol><li> 班级清单（类别） </li><li> 教程：图片的路径列表和正确答案的列表 </li><li> 验证集：图片的路径列表和正确答案的列表 </li></ol><br> 如果是数据集，则为了<i>训练</i>和<i>验证，</i>您需要自己破坏集合。  Food-101已经具有这样的分区，并且此信息存储在<code>meta</code>目录中。 <br><br><pre> <code class="python hljs">DATASET_ROOT = <span class="hljs-string"><span class="hljs-string">'data/food-101/'</span></span> train_data, val_data, classes = data.food101(DATASET_ROOT) num_classes = len(classes)</code> </pre><br> 所有负责数据处理的辅助功能都移至单独的<code>data.py</code>文件中： <br><br><div class="spoiler">  <b class="spoiler_title">data.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> os.path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> join <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> opj <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_ds_subset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img_root, list_fpath, classes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Parse a meta file with image paths and labels -&gt; img_root: path to the root of image folders -&gt; list_fpath: path to the file with the list (eg train.txt) -&gt; classes: list of class names &lt;- (list_of_img_paths, integer_labels) '''</span></span> fpaths = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: class_name, image_id = line.strip().split(<span class="hljs-string"><span class="hljs-string">'/'</span></span>) fpaths.append(opj(img_root, class_name, image_id+<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) labels.append(classes.index(class_name)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fpaths, labels <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">food101</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset_root)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Get lists of train and validation examples for Food-101 dataset -&gt; dataset_root: root of the Food-101 dataset &lt;- ((train_fpaths, train_labels), (val_fpaths, val_labels), classes) '''</span></span> img_root = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'images'</span></span>) train_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'train.txt'</span></span>) test_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'test.txt'</span></span>) classes_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'classes.txt'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(classes_list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: classes = [line.strip() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f] train_data = parse_ds_subset(img_root, train_list_fpath, classes) val_data = parse_ds_subset(img_root, test_list_fpath, classes) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_data, val_data, classes <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">imread_and_crop</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(fpath, inp_size, margin=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_crop=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Construct TF graph for image preparation: Read the file, crop and resize -&gt; fpath: path to the JPEG image file (TF node) -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin -&gt; random_crop: perform random crop or central crop &lt;- prepared image (TF node) '''</span></span> data = tf.read_file(fpath) img = tf.image.decode_jpeg(data, channels=<span class="hljs-number"><span class="hljs-number">3</span></span>) img = tf.image.convert_image_dtype(img, dtype=tf.float32) shape = tf.shape(img) crop_size = tf.minimum(shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) - <span class="hljs-number"><span class="hljs-number">2</span></span> * margin <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random_crop: img = tf.random_crop(img, (crop_size, crop_size, <span class="hljs-number"><span class="hljs-number">3</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-comment"><span class="hljs-comment"># central crop ho = (shape[0] - crop_size) // 2 wo = (shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = tf.image.resize_images(img, (inp_size, inp_size), method=tf.image.ResizeMethod.AREA) return img def train_dataset(data, batch_size, epochs, inp_size, margin): ''' Prepare training data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: training batch size -&gt; epochs: number of training epochs -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin &lt;- (dataset, number_of_train_iterations) ''' num_examples = len(data[0]) iters = (epochs * num_examples) // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, margin, random_crop=True) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.shuffle(buffer_size=num_examples) dataset = dataset.map(fpath_to_image) dataset = dataset.repeat(epochs) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters def val_dataset(data, batch_size, inp_size): ''' Prepare validation data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: validation batch size -&gt; inp_size: size of the network input (eg 224) &lt;- (dataset, number_of_val_iterations) ''' num_examples = len(data[0]) iters = num_examples // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, 0, random_crop=False) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.map(fpath_to_image) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters</span></span></code> </pre><br></div></div><br><h1> 模型训练 </h1><br> 模型训练代码包括以下步骤： <br><br><ol><li> 建立<i>火车/验证</i>数据管道 </li><li> 构建<i>训练/验证</i>图（网络） </li><li> 损失（ <i>交叉熵损失</i> ）分类函数在<i>火车</i>图上的附着。 </li><li> 在训练期间计算验证样本上预测的准确性所需的代码 </li><li> 从快照加载预训练的比例尺的逻辑 </li><li> 建立各种培训结构 </li><li> 学习周期本身（迭代优化） </li></ol><br> 图的最后一层是使用所需数量的神经元构造的，并从预先训练的快照加载的参数列表中排除。 <br><br><div class="spoiler">  <b class="spoiler_title">示范培训守则</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim tf.logging.set_verbosity(tf.logging.INFO) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### INPUT_SIZE = 224 RANDOM_CROP_MARGIN = 10 TRAIN_EPOCHS = 20 TRAIN_BATCH_SIZE = 64 VAL_BATCH_SIZE = 128 LR_START = 0.001 LR_END = LR_START / 1e4 MOMENTUM = 0.9 VGG_PRETRAINED_CKPT = 'data/vgg_19.ckpt' CHECKPOINT_DIR = 'checkpoints/vgg19_food' LOG_LOSS_EVERY = 10 CALC_ACC_EVERY = 500 ########################################################### ### Build training and validation data pipelines ########################################################### train_ds, train_iters = data.train_dataset(train_data, TRAIN_BATCH_SIZE, TRAIN_EPOCHS, INPUT_SIZE, RANDOM_CROP_MARGIN) train_ds_iterator = train_ds.make_one_shot_iterator() train_x, train_y = train_ds_iterator.get_next() val_ds, val_iters = data.val_dataset(val_data, VAL_BATCH_SIZE, INPUT_SIZE) val_ds_iterator = val_ds.make_initializable_iterator() val_x, val_y = val_ds_iterator.get_next() ########################################################### ### Construct training and validation graphs ########################################################### with tf.variable_scope('', reuse=tf.AUTO_REUSE): train_logits = model.vgg_19(train_x, num_classes, is_training=True) val_logits = model.vgg_19(val_x, num_classes, is_training=False) ########################################################### ### Construct training loss ########################################################### loss = tf.losses.sparse_softmax_cross_entropy( labels=train_y, logits=train_logits) tf.summary.scalar('loss', loss) ########################################################### ### Construct validation accuracy ### and related functions ########################################################### def calc_accuracy(sess, val_logits, val_y, val_iters): acc_total = 0.0 acc_denom = 0 for i in range(val_iters): logits, y = sess.run((val_logits, val_y)) y_pred = np.argmax(logits, axis=1) correct = np.count_nonzero(y == y_pred) acc_denom += y_pred.shape[0] acc_total += float(correct) tf.logging.info('Validating batch [{} / {}] correct = {}'.format( i, val_iters, correct)) acc_total /= acc_denom return acc_total def accuracy_summary(sess, acc_value, iteration): acc_summary = tf.Summary() acc_summary.value.add(tag="accuracy", simple_value=acc_value) sess._hooks[1]._summary_writer.add_summary(acc_summary, iteration) ########################################################### ### Define set of VGG variables to restore ### Create the Restorer ### Define init callback (used by monitored session) ########################################################### vars_to_restore = tf.contrib.framework.get_variables_to_restore( exclude=['vgg_19/fc8']) vgg_restorer = tf.train.Saver(vars_to_restore) def init_fn(scaffold, sess): vgg_restorer.restore(sess, VGG_PRETRAINED_CKPT) ########################################################### ### Create various training structures ########################################################### global_step = tf.train.get_or_create_global_step() lr = tf.train.polynomial_decay(LR_START, global_step, train_iters, LR_END) tf.summary.scalar('learning_rate', lr) optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=MOMENTUM) training_op = slim.learning.create_train_op( loss, optimizer, global_step=global_step) scaffold = tf.train.Scaffold(init_fn=init_fn) ########################################################### ### Create monitored session ### Run training loop ########################################################### with tf.train.MonitoredTrainingSession(checkpoint_dir=CHECKPOINT_DIR, save_checkpoint_secs=600, save_summaries_steps=30, scaffold=scaffold) as sess: start_iter = sess.run(global_step) for iteration in range(start_iter, train_iters): # Gradient Descent loss_value = sess.run(training_op) # Loss logging if iteration % LOG_LOSS_EVERY == 0: tf.logging.info('[{} / {}] Loss = {}'.format( iteration, train_iters, loss_value)) # Accuracy logging if iteration % CALC_ACC_EVERY == 0: sess.run(val_ds_iterator.initializer) acc_value = calc_accuracy(sess, val_logits, val_y, val_iters) accuracy_summary(sess, acc_value, iteration) tf.logging.info('[{} / {}] Validation accuracy = {}'.format( iteration, train_iters, acc_value))</span></span></code> </pre><br></div></div><br> 开始训练后，您可以使用TensorBoard实用程序查看其进度，该实用程序与TensorFlow捆绑在一起，用于可视化各种指标和其他参数。 <br><br><pre> <code class="bash hljs">tensorboard --logdir checkpoints/</code> </pre><br> 在TensorBoard的培训结束时，我们看到了几乎完美的画面： <i>火车损失</i>减少， <i>验证准确性</i>增加 <br><br><img src="https://habrastorage.org/webt/bk/hc/ay/bkhcayg7tn4nczu3dx2flgfukrc.jpeg" alt="TensorBoard的损失和准确性"><br><br> 结果，我们将保存的快照保存在<code>checkpoints/vgg19_food</code> ，我们将在模型测试期间使用该快照（ <i>推理</i> ）。 <br><br><h1> 模型测试 </h1><br> 现在测试我们的模型。 为此： <br><br><ol><li> 我们构造了一个专门为推理而设计的新图（ <code>is_training=False</code> ） </li><li> 从快照加载经过训练的权重 </li><li> 下载并预处理输入的测试图像。 </li><li> 让我们通过神经网络驱动图像并获得预测 </li></ol><br><div class="spoiler">  <b class="spoiler_title">推断</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> imageio <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.transform <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> resize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### CLASSES_FPATH = 'data/food-101/meta/labels.txt' INP_SIZE = 224 # Input will be cropped and resized CHECKPOINT_DIR = 'checkpoints/vgg19_food' IMG_FPATH = 'data/food-101/images/bruschetta/3564471.jpg' ########################################################### ### Get all class names ########################################################### with open(CLASSES_FPATH, 'r') as f: classes = [line.strip() for line in f] num_classes = len(classes) ########################################################### ### Construct inference graph ########################################################### x = tf.placeholder(tf.float32, (1, INP_SIZE, INP_SIZE, 3), name='inputs') logits = model.vgg_19(x, num_classes, is_training=False) ########################################################### ### Create TF session and restore from a snapshot ########################################################### sess = tf.Session() snapshot_fpath = tf.train.latest_checkpoint(CHECKPOINT_DIR) restorer = tf.train.Saver() restorer.restore(sess, snapshot_fpath) ########################################################### ### Load and prepare input image ########################################################### def crop_and_resize(img, input_size): crop_size = min(img.shape[0], img.shape[1]) ho = (img.shape[0] - crop_size) // 2 wo = (img.shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = resize(img, (input_size, input_size), order=3, mode='reflect', anti_aliasing=True, preserve_range=True) return img img = imageio.imread(IMG_FPATH) img = img.astype(np.float32) img = crop_and_resize(img, INP_SIZE) img = img[None, ...] ########################################################### ### Run inference ########################################################### out = sess.run(logits, feed_dict={x:img}) pred_class = classes[np.argmax(out)] print('Input: {}'.format(IMG_FPATH)) print('Prediction: {}'.format(pred_class))</span></span></code> </pre><br></div></div><br><img src="https://habrastorage.org/webt/j6/e6/jv/j6e6jv72cuvsl3ztjo_392quidm.jpeg" alt="推论"><br><br> 所有代码，包括用于构建和运行带有所有必需版本库的Docker容器的资源，都在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">此存储库中</a> -在阅读本文时，存储库中的代码可能已更新。 <br><br> 在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">“面向开发人员的机器学习和神经网络”</a>研讨会上<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">，</a>我将分析机器学习的其他任务，并且学生们将在密集课程结束时介绍他们的项目。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN428255/">https://habr.com/ru/post/zh-CN428255/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN428239/index.html">闪存驱动器即将到2019年-是过去的遗迹还是仍然是必需品？</a></li>
<li><a href="../zh-CN428243/index.html">GeekBrains将教授C ++编程语言</a></li>
<li><a href="../zh-CN428249/index.html">WDM技术：将数据中心整合到防灾集群中</a></li>
<li><a href="../zh-CN428251/index.html">应用程序“我的直线”中的愚蠢漏洞</a></li>
<li><a href="../zh-CN428253/index.html">嵌入式语言：为什么选择Lua？</a></li>
<li><a href="../zh-CN428257/index.html">研究：95％的儿童应用程序包含广告</a></li>
<li><a href="../zh-CN428259/index.html">《为什么我们错了》一书 在行动中思考陷阱。” 摘录第2部分</a></li>
<li><a href="../zh-CN428261/index.html">小行星带的日本周</a></li>
<li><a href="../zh-CN428263/index.html">“我的手真的很瘦”：专业游戏玩家去健身房</a></li>
<li><a href="../zh-CN428265/index.html">我们可以访问WinCE桌面并在Keysight DSOX1102G示波器上运行Doom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>