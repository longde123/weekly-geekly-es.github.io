<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∑üèΩ üîä ‚õ¥Ô∏è FAQ zu Architektur und Arbeit VKontakte üôÑ üê§ üõÄüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Geschichte von VKontakte ist auf Wikipedia, wurde von Pavel selbst erz√§hlt. Es scheint, dass jeder sie bereits kennt. Pavel sprach bereits 2010 √ºb...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>FAQ zu Architektur und Arbeit VKontakte</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/449254/">  Die Geschichte von VKontakte ist auf Wikipedia, wurde von Pavel selbst erz√§hlt.  Es scheint, dass jeder sie bereits kennt.  Pavel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sprach bereits 2010</a> √ºber das Innere, die Architektur und das Design der Website in HighLoad ++.  Seitdem sind viele Server durchgesickert, daher werden wir die Informationen aktualisieren: Wir sezieren, ziehen die Innenseiten heraus, wiegen - wir betrachten das VK-Ger√§t aus technischer Sicht. <br><br><img src="https://habrastorage.org/webt/_x/zc/wp/_xzcwpb5ze_4e-yx_jw_-8nvnei.jpeg"><br><br>  <strong>Alexey Akulovich</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">AterCattus</a> ) ist ein Backend-Entwickler im VKontakte-Team.  Das Transkript dieses Berichts ist eine kollektive Antwort auf h√§ufig gestellte Fragen zum Betrieb der Plattform, der Infrastruktur, der Server und der Interaktion zwischen ihnen, jedoch nicht zur Entwicklung, n√§mlich <strong>zur Hardware</strong> .  Separat - √ºber Datenbanken und was VK an ihrer Stelle hat, √ºber das Sammeln von Protokollen und die √úberwachung des gesamten Projekts als Ganzes.  Details unter dem Schnitt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_GqcriadL-s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Seit mehr als vier Jahren erledige ich alle Arten von Aufgaben im Zusammenhang mit dem Backend. <br><br><ul><li>  Herunterladen, Speichern, Verarbeiten, Verteilen von Medien: Video, Live-Streaming, Audio, Fotos, Dokumente. </li><li>  Infrastruktur, Plattform, Entwickler√ºberwachung, Protokolle, regionale Caches, CDN, propriet√§res RPC-Protokoll. </li><li>  Integration mit externen Diensten: Push-Mailing, Analyse externer Links, RSS-Feed. </li><li>  Helfen Sie Kollegen bei verschiedenen Fragen, um Antworten zu erhalten, auf die Sie in einen unbekannten Code eintauchen m√ºssen. </li></ul><br>  W√§hrend dieser Zeit war ich an vielen Komponenten der Website beteiligt.  Ich m√∂chte diese Erfahrung teilen. <br><br><h2>  Allgemeine Architektur </h2><br>  Alles beginnt wie gewohnt mit einem Server oder einer Gruppe von Servern, die Anforderungen annehmen. <br><br><h3>  Frontserver </h3><br>  Der Frontserver akzeptiert Anforderungen √ºber HTTPS, RTMP und WSS. <br><br>  <strong>HTTPS</strong> sind Anfragen f√ºr die Haupt- und mobilen Webversionen der Website: vk.com und m.vk.com sowie f√ºr andere offizielle und inoffizielle Clients unserer API: mobile Clients, Instant Messenger.  Wir haben einen Empfang von <strong>RTMP-</strong> Verkehr f√ºr Live-√úbertragungen mit separaten Frontservern und <strong>WSS-</strong> Verbindungen f√ºr die Streaming-API. <br><br>  F√ºr HTTPS und WSS ist <strong>nginx</strong> auf den Servern installiert.  F√ºr RTMP-Sendungen haben wir k√ºrzlich auf unsere eigene <strong>Kive-</strong> L√∂sung <strong>umgestellt</strong> , die jedoch den Rahmen des Berichts <strong>sprengt</strong> .  Aus Gr√ºnden der Fehlertoleranz geben diese Server gemeinsame IP-Adressen bekannt und fungieren als Gruppen, damit bei einem Problem auf einem der Server Benutzeranforderungen nicht verloren gehen.  Bei HTTPS und WSS verschl√ºsseln dieselben Server den Datenverkehr, um einen Teil der CPU-Last f√ºr sich selbst zu √ºbernehmen. <br><br>  Au√üerdem werden wir nicht √ºber WSS und RTMP sprechen, sondern nur √ºber Standard-HTTPS-Anforderungen, die normalerweise mit einem Webprojekt verbunden sind. <br><br><h3>  Backend </h3><br>  Hinter der Front befinden sich normalerweise die Backend-Server.  Sie verarbeiten Anforderungen, die der Frontserver von Clients empf√§ngt. <br><br>  Dies sind <strong>kPHP-Server, auf</strong> denen der HTTP-D√§mon ausgef√ºhrt wird, da HTTPS bereits entschl√ºsselt ist.  kPHP ist ein Server, der nach dem <strong>Prefork-Modell arbeitet</strong> : Er startet den Master-Prozess, eine Reihe von <strong>untergeordneten</strong> Prozessen, √ºbergibt ihnen Listening-Sockets und sie verarbeiten ihre Anforderungen.  Gleichzeitig werden Prozesse nicht zwischen jeder Anforderung des Benutzers neu gestartet, sondern setzen einfach ihren Status auf den anf√§nglichen Nullwertstatus zur√ºck - Anforderung f√ºr Anforderung, anstatt neu zu starten. <br><br><h4>  Lastverteilung </h4><br>  Alle unsere Backends sind kein riesiger Pool von Maschinen, die jede Anfrage bearbeiten k√∂nnen.  Wir <strong>teilen</strong> sie <strong>in separate Gruppen ein</strong> : Allgemein, Mobil, API, Video, Inszenierung ... Das Problem auf einer separaten Gruppe von Computern betrifft nicht alle anderen.  Bei Problemen mit dem Video wei√ü der Benutzer, der Musik h√∂rt, nicht einmal √ºber die Probleme Bescheid.  An welches Backend die Anfrage gesendet werden soll, wird von nginx auf der Vorderseite in der Konfiguration gel√∂st. <br><br><h4>  Erfassung und Neuausrichtung von Metriken </h4><br>  Um zu verstehen, wie viele Autos Sie in jeder Gruppe ben√∂tigen, verlassen wir <strong>uns nicht auf QPS</strong> .  Die Backends sind unterschiedlich, sie haben unterschiedliche Anforderungen, jede Anforderung hat unterschiedliche Komplexit√§t bei der QPS-Berechnung.  Daher verwenden wir das <strong>Konzept der Auslastung des gesamten Servers - der CPU und der Leistung</strong> . <br><br>  Wir haben Tausende solcher Server.  Die kPHP-Gruppe wird auf jedem physischen Server ausgef√ºhrt, um alle Kernel zu verwenden (da kPHP Single-Threaded ist). <br><br><h3>  Inhaltsserver </h3><br>  <strong>CS oder Content Server ist Speicher</strong> .  CS ist ein Server, der Dateien speichert und auch hochgeladene Dateien verarbeitet. Dabei handelt es sich um alle Arten von synchronen Hintergrundaufgaben, die das Haupt-Web-Frontend f√ºr ihn bereitstellt. <br><br>  Wir haben Zehntausende von physischen Servern, auf denen Dateien gespeichert sind.  Benutzer lieben es, Dateien hochzuladen, und wir lieben es, sie zu speichern und zu teilen.  Einige dieser Server werden von speziellen Pu / PP-Servern geschlossen. <br><br><h3>  pu / pp </h3><br>  Wenn Sie die Registerkarte Netzwerk in VK ge√∂ffnet haben, haben Sie pu / pp gesehen. <br><br><img src="https://habrastorage.org/webt/fd/am/xo/fdamxolkfxlplnc5h5flbihru3g.png"><br><br>  Was ist pu / pp?  Wenn wir einen Server nach dem anderen schlie√üen, gibt es zwei M√∂glichkeiten, eine Datei hochzuladen und auf einen Server herunterzuladen, der geschlossen wurde: <strong>direkt</strong> √ºber <code>http://cs100500.userapi.com/path</code> oder <strong>√ºber einen Zwischenserver</strong> - <code>http://pu.vk.com/c100500/path</code> . <br><br>  <strong>Pu ist der historische Name f√ºr das Hochladen von Fotos und pp ist der Foto-Proxy</strong> .  Das hei√üt, ein Server zum Hochladen von Fotos und ein anderer zum Geben.  Jetzt werden nicht nur Fotos geladen, sondern der Name bleibt erhalten. <br><br>  Diese Server <strong>beenden HTTPS-Sitzungen</strong> , um die Prozessorlast aus dem Speicher zu entfernen.  Da Benutzerdateien auf diesen Servern verarbeitet werden, ist es umso besser, je weniger vertrauliche Informationen auf diesen Computern gespeichert sind.  Zum Beispiel HTTPS-Verschl√ºsselungsschl√ºssel. <br><br>  Da die Maschinen von unseren anderen Maschinen geschlossen werden, k√∂nnen wir es uns leisten, ihnen keine ‚Äûwei√üen‚Äú externen IPs und keine <strong>‚Äûgrauen‚Äú</strong> IPs zu <strong>geben</strong> .  Wir haben also im IP-Pool gespeichert und garantiert, dass die Computer vor dem Zugriff von au√üen gesch√ºtzt sind - es gibt einfach keine IP, um darauf zuzugreifen. <br><br>  <strong>Fehlertoleranz durch gemeinsam genutzte IP</strong> .  In Bezug auf die Fehlertoleranz funktioniert das Schema auf die gleiche Weise: Mehrere physische Server haben eine gemeinsame physische IP, und das Eisenst√ºck vor ihnen w√§hlt aus, wohin die Anforderung gesendet werden soll.  Sp√§ter werde ich √ºber andere Optionen sprechen. <br><br>  Der umstrittene Punkt ist, dass in diesem Fall der <strong>Client weniger Verbindungen hat</strong> .  Wenn auf mehreren Computern dieselbe IP-Adresse vorhanden ist - mit demselben Host: pu.vk.com oder pp.vk.com - ist die Anzahl der gleichzeitigen Anforderungen an einen Host im Client-Browser begrenzt.  Aber w√§hrend des allgegenw√§rtigen HTTP / 2 glaube ich, dass dies nicht mehr der Fall ist. <br><br>  Das offensichtliche Minus des Schemas ist, dass Sie den <strong>gesamten Datenverkehr</strong> , der zum Speicher gelangt, √ºber einen anderen Server <strong>pumpen m√ºssen</strong> .  Da wir den Verkehr durch Autos pumpen, k√∂nnen wir noch nicht auf die gleiche Weise starken Verkehr pumpen, z. B. Video.  Wir √ºbertragen es direkt - eine separate direkte Verbindung f√ºr einzelne Repositories speziell f√ºr Video.  Wir √ºbertragen leichtere Inhalte √ºber einen Proxy. <br><br>  Vor nicht allzu langer Zeit haben wir eine verbesserte Version von Proxy.  Jetzt werde ich Ihnen sagen, wie sie sich von gew√∂hnlichen unterscheiden und warum dies notwendig ist. <br><br><h3>  Sonne </h3><br>  Im September 2017 entlie√ü Oracle, das zuvor Sun gekauft hatte, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine gro√üe Anzahl von Sun-Mitarbeitern</a> .  Wir k√∂nnen sagen, dass das Unternehmen zu diesem Zeitpunkt aufgeh√∂rt hat zu existieren.  Unsere Administratoren w√§hlten einen Namen f√ºr das neue System und beschlossen, diesem Unternehmen Tribut und Respekt zu zollen. Sie nannten das neue Sun-System.  Unter uns nennen wir es einfach "Sonnenschein". <br><br><img src="https://habrastorage.org/webt/d3/6f/0j/d36f0jjqbwlst9mk2-lcncltkq4.png"><br><br>  Pp hatte ein paar Probleme.  <strong>Eine IP pro Gruppe ist ein ineffizienter Cache</strong> .  Mehrere physische Server haben eine gemeinsame IP-Adresse, und es gibt keine M√∂glichkeit zu steuern, an welchen Server die Anforderung gesendet wird.  Wenn also verschiedene Benutzer f√ºr dieselbe Datei kommen und sich auf diesen Servern ein Cache befindet, wird die Datei im Cache jedes Servers abgelegt.  Dies ist ein sehr ineffizientes Schema, aber es konnte nichts unternommen werden. <br><br>  Infolgedessen k√∂nnen <strong>wir keine Inhalte sharden</strong> , da wir keinen bestimmten Server f√ºr diese Gruppe ausw√§hlen k√∂nnen - sie haben eine gemeinsame IP.  Aus internen Gr√ºnden hatten wir <strong>auch nicht die M√∂glichkeit, solche Server in den Regionen zu platzieren</strong> .  Sie standen nur in St. Petersburg. <br><br>  Mit den Sonnen haben wir das Auswahlsystem ge√§ndert.  Jetzt haben wir <strong>Anycast-Routing</strong> : dynamisches Routing, Anycast, Self-Check-Daemon.  Jeder Server hat seine eigene IP, gleichzeitig aber ein gemeinsames Subnetz.  Alles ist so konfiguriert, dass bei Verlust eines Servers der Datenverkehr automatisch auf andere Server derselben Gruppe verteilt wird.  Jetzt ist es m√∂glich, einen bestimmten Server auszuw√§hlen, <strong>es gibt kein √ºberm√§√üiges Caching</strong> und die Zuverl√§ssigkeit wird nicht beeintr√§chtigt. <br><br>  <strong>Gewichtsunterst√ºtzung</strong> .  Jetzt k√∂nnen wir es uns leisten, Autos mit unterschiedlichen Kapazit√§ten nach Bedarf zu platzieren und auch bei vor√ºbergehenden Problemen das Gewicht der arbeitenden ‚ÄûSonnen‚Äú zu √§ndern, um die Belastung f√ºr sie zu verringern, damit sie ‚Äûruhen‚Äú und wieder arbeiten. <br><br>  <strong>Sharding nach Inhalts-ID</strong> .  Das Lustige am Sharding ist, dass wir normalerweise Inhalte sharden, sodass verschiedene Benutzer derselben Datei durch dieselbe ‚ÄûSonne‚Äú folgen, sodass sie einen gemeinsamen Cache haben. <br><br>  Wir haben k√ºrzlich die Clover-App gestartet.  Dies ist ein Online-Live-Quiz, bei dem der Pr√§sentator Fragen stellt und Benutzer in Echtzeit antworten, indem sie Optionen ausw√§hlen.  Die Anwendung verf√ºgt √ºber einen Chat, in dem Benutzer √ºberfluten k√∂nnen.  <strong>Mehr als 100.000 Menschen</strong> k√∂nnen gleichzeitig eine Verbindung zur Sendung herstellen.  Sie alle schreiben Nachrichten, die an alle Teilnehmer gesendet werden, zusammen mit der Nachricht kommt ein weiterer Avatar.  Wenn 100.000 Menschen f√ºr einen Avatar in einer ‚ÄûSonne‚Äú kommen, kann er manchmal √ºber eine Wolke rollen. <br><br>  Um Ausbr√ºchen von Anfragen aus derselben Datei standzuhalten, f√ºgen wir f√ºr eine Art von Inhalt ein dummes Schema hinzu, das Dateien √ºber alle verf√ºgbaren ‚ÄûSonnen‚Äú in der Region verteilt. <br><br><h4>  Sonne drinnen </h4><br>  Reverse Proxy zu Nginx, Cache in RAM oder Optane / NVMe Fast Disks.  Beispiel: <code>http://sun4-2.userapi.com/c100500/path</code> - Link zur "Sonne", die sich in der vierten Region befindet, der zweiten Servergruppe.  Es schlie√üt die Pfaddatei, die physisch auf dem Server 100500 liegt. <br><br><h3>  Cache </h3><br>  Wir f√ºgen unserem Architekturschema einen weiteren Knoten hinzu - die Caching-Umgebung. <br><br><img src="https://habrastorage.org/webt/jp/4p/xv/jp4pxvydhstms-z9bpbmpjy0htk.png"><br><br>  Unten sehen Sie das Layout der <strong>regionalen Caches</strong> , von denen es ungef√§hr 20 gibt.  Dies sind die Orte, an denen sich genau die Caches und "Sonnen" befinden, die den Verkehr durch sich selbst zwischenspeichern k√∂nnen. <br><br><img src="https://habrastorage.org/webt/yh/9i/qk/yh9iqkv3dyqd3uox9wgd2cmgqba.png"><br><br>  Dies ist das Zwischenspeichern von Multimedia-Inhalten. Benutzerdaten werden hier nicht gespeichert - nur Musik, Videos, Fotos. <br><br>  Um die Region des Benutzers zu bestimmen, <strong>erfassen</strong> wir <strong>die in den Regionen angek√ºndigten BGP-Netzwerkpr√§fixe</strong> .  Im Fall eines Fallbacks haben wir immer noch eine Analyse der Geoip-Basis, wenn wir IP nicht anhand von Pr√§fixen finden konnten.  <strong>Basierend auf der IP des Benutzers bestimmen wir die Region</strong> .  Im Code k√∂nnen wir eine oder mehrere Regionen des Benutzers betrachten - die Punkte, denen er geografisch am n√§chsten ist. <br><br><h4>  Wie funktioniert es </h4><br>  <strong>Wir betrachten die Beliebtheit von Dateien nach Regionen</strong> .  Es gibt eine regionale Cache-Nummer, in der sich der Benutzer befindet, und eine Dateikennung. Wir nehmen dieses Paar und erh√∂hen die Bewertung f√ºr jeden Download. <br><br>  Gleichzeitig kommen von Zeit zu Zeit D√§monen - Dienste in den Regionen - zur API und sagen: "Ich habe so und so einen Cache, gib mir eine Liste der beliebtesten Dateien in meiner Region, die ich noch nicht habe."  Die API gibt eine Reihe von Dateien nach Bewertung sortiert aus, der Daemon pumpt sie aus, √ºbertr√§gt sie in die Regionen und gibt ihnen Dateien von dort.  Dies ist ein grundlegender Unterschied zwischen pu / pp und Sun gegen√ºber Caches: Sie geben die Datei sofort durch sich selbst weiter, auch wenn die Datei nicht im Cache vorhanden ist, und der Cache pumpt die Datei zuerst in sich selbst und beginnt dann, sie weiterzugeben. <br><br>  Gleichzeitig bringen wir <strong>Inhalte n√§her an die Benutzer heran</strong> und verschmieren die Netzwerklast.  Beispielsweise verteilen wir nur aus dem Moskauer Cache w√§hrend der Sto√üzeiten mehr als 1 Tbit / s. <br><br>  Es gibt jedoch Probleme - <strong>Cache-Server sind nicht aus Gummi</strong> .  F√ºr sehr beliebte Inhalte gibt es manchmal nicht gen√ºgend Netzwerk auf einem separaten Server.  Wir haben 40-50 Gbit / s-Cache-Server, aber es gibt Inhalte, die einen solchen Kanal vollst√§ndig verstopfen.  Wir bem√ºhen uns, die Speicherung von mehr als einer Kopie beliebter Dateien in der Region zu realisieren.  Ich hoffe, dass wir es bis Ende des Jahres realisieren werden. <br><br>  Wir haben die allgemeine Architektur untersucht. <br><br><ul><li>  Frontserver, die Anforderungen annehmen. </li><li>  Backends, die Anforderungen verarbeiten. </li><li>  Tresore, die von zwei Arten von Proxys geschlossen werden. </li><li>  Regionale Caches. </li></ul><br>  Was fehlt in diesem Schema?  Nat√ºrlich die Datenbanken, in denen wir Daten speichern. <br><br><h2>  Datenbanken oder Engines </h2><br>  Wir nennen sie keine Datenbanken, sondern Engines-Engines, weil wir im allgemein akzeptierten Sinne praktisch keine Datenbanken haben. <br><br><img src="https://habrastorage.org/webt/n6/zm/lj/n6zmlj5pwxsnqoqp0xgfhxza_ic.png"><br><br>  <strong>Dies ist eine notwendige Ma√ünahme</strong> .  Dies geschah, weil in den Jahren 2008-2009, als VK einen explosionsartigen Anstieg der Popularit√§t verzeichnete, das Projekt vollst√§ndig auf MySQL und Memcache funktionierte und es Probleme gab.  MySQL fiel gern und ruinierte Dateien, danach stieg es nicht mehr an, und Memcache verschlechterte sich allm√§hlich in der Leistung und musste neu gestartet werden. <br><br>  Es stellte sich heraus, dass es in dem Projekt, das immer beliebter wurde, einen dauerhaften Speicher gab, der die Daten besch√§digte, und einen Cache, der langsamer wurde.  Unter solchen Bedingungen ist es schwierig, ein wachsendes Projekt zu entwickeln.  Es wurde beschlossen, die kritischen Dinge, auf denen das Projekt beruhte, auf ihren eigenen Motorr√§dern neu zu schreiben. <br><br>  <strong>Die L√∂sung war erfolgreich</strong> .  Die M√∂glichkeit dazu bestand ebenso wie ein dringender Bedarf, da zu diesem Zeitpunkt keine anderen Skalierungsmethoden existierten.  Es gab keinen Haufen Basen, NoSQL gab es noch nicht, es gab nur MySQL, Memcache, PostrgreSQL - und das ist alles. <br><br>  <strong>Universalbetrieb</strong> .  Die Entwicklung wurde von unserem Team von C-Entwicklern geleitet, und alles wurde auf die gleiche Weise durchgef√ºhrt.  Unabh√§ngig von der Engine gab es √ºberall ungef√§hr das gleiche Format der auf die Festplatte geschriebenen Dateien, die gleichen Startparameter, die Signale wurden gleich verarbeitet und verhielten sich bei Randbedingungen und Problemen gleich.  Mit dem Wachstum der Engines ist es f√ºr Administratoren bequem, das System zu bedienen - es gibt keinen Zoo, der gewartet werden muss, und es muss gelernt werden, jede neue Basis von Drittanbietern erneut zu bedienen, wodurch es m√∂glich wurde, ihre Anzahl schnell und bequem zu erh√∂hen. <br><br><h3>  Motortypen </h3><br>  Das Team hat einige Motoren geschrieben.  Hier sind nur einige davon: Freund, Hinweise, Bild, IPdb, Briefe, Listen, Protokolle, Memcached, Meowdb, Nachrichten, Nostradamus, Foto, Wiedergabelisten, PMemcached, Sandbox, Suche, Speicherung, Likes, Aufgaben, ... <br><br>  F√ºr jede Aufgabe, die eine bestimmte Datenstruktur erfordert oder atypische Anforderungen verarbeitet, schreibt das C-Team eine neue Engine.  Warum nicht. <br><br>  Wir haben eine separate <strong>Memcached-</strong> Engine, die der √ºblichen √§hnelt, aber ein paar Br√∂tchen enth√§lt und die nicht langsamer wird.  Nicht ClickHouse, funktioniert aber auch.  Es gibt einen separaten <strong>pmemcached</strong> - es handelt sich um einen <strong>dauerhaften memcached</strong> , der Daten auch auf der Festplatte speichern kann und mehr als in den RAM gelangt, um beim Neustart keine Daten zu verlieren.  Es gibt verschiedene Engines f√ºr einzelne Aufgaben: Warteschlangen, Listen, Sets - alles, was unser Projekt ben√∂tigt. <br><br><h3>  Cluster </h3><br>  Aus Sicht des Codes besteht keine Notwendigkeit, sich Engines oder Datenbanken als bestimmte Prozesse, Entit√§ten oder Instanzen vorzustellen.  Der Code funktioniert speziell mit Clustern mit Gruppen von Engines - <strong>ein Typ pro Cluster</strong> .  Angenommen, es gibt einen zwischengespeicherten Cluster - es ist nur eine Gruppe von Maschinen. <br><br><blockquote>  Der Code muss den physischen Standort, die Gr√∂√üe und die Anzahl der Server nicht kennen.  Er geht mit einer Kennung zum Cluster. </blockquote><br>  Damit dies funktioniert, m√ºssen Sie eine weitere Entit√§t hinzuf√ºgen, die sich zwischen dem Code und den Engines befindet - den <strong>Proxy</strong> . <br><br><h3>  RPC-Proxy </h3><br>  Proxy - ein <strong>Verbindungsbus</strong> , der fast den gesamten Standort betreibt.  Gleichzeitig haben wir <strong>keine Serviceerkennung</strong> - stattdessen gibt es eine Konfiguration dieses Proxys, die den Speicherort aller Cluster und aller Shards dieses Clusters kennt.  Dies wird von Admins durchgef√ºhrt. <br><br>  Programmierer k√ºmmern sich im Allgemeinen nicht darum, wie viel, wo und was es kostet - sie gehen einfach zum Cluster.  Das erlaubt uns viel.  Nach Erhalt der Anfrage leitet der Proxy die Anfrage um und wei√ü, wo - er bestimmt dies. <br><br><img src="https://habrastorage.org/webt/7k/pf/ia/7kpfiagxzy2a4mrosc_f4otqnw8.png"><br><br>  Gleichzeitig ist Proxy ein Schutzpunkt gegen Dienstausf√§lle.  Wenn eine Engine langsamer wird oder abst√ºrzt, versteht der Proxy dies und reagiert dementsprechend auf die Client-Seite.  Auf diese Weise k√∂nnen Sie das Zeitlimit entfernen. Der Code wartet nicht auf die Antwort der Engine, versteht jedoch, dass dies nicht funktioniert und Sie sich anders verhalten m√ºssen.  Der Code sollte darauf vorbereitet sein, dass die Datenbanken nicht immer funktionieren. <br><br><h4>  Spezifische Implementierungen </h4><br>  Manchmal wollen wir immer noch eine kundenspezifische L√∂sung als Motor.  Gleichzeitig wurde beschlossen, nicht unseren vorgefertigten RPC-Proxy zu verwenden, der speziell f√ºr unsere Engines erstellt wurde, sondern einen separaten Proxy f√ºr die Aufgabe zu erstellen. <br><br>  F√ºr MySQL, das wir an einigen Stellen noch haben, verwenden wir db-proxy und f√ºr ClickHouse - <strong>Kittenhouse</strong> . <br><br>  Das funktioniert insgesamt so.  Es gibt einen Server, auf dem kPHP, Go und Python ausgef√ºhrt werden - im Allgemeinen jeder Code, der unserem RPC-Protokoll folgen kann.  Der Code wird lokal an den RPC-Proxy gesendet. Auf jedem Server, auf dem Code vorhanden ist, wird ein eigener lokaler Proxy gestartet.  Auf Anfrage versteht der Proxy, wohin er gehen soll. <br><br><img src="https://habrastorage.org/webt/f-/dx/ro/f-dxrox3o97ckejzygz8mgf4tcs.png"><br><br>  Wenn eine Engine zu einer anderen wechseln m√∂chte, auch wenn es sich um einen Nachbarn handelt, wird ein Proxy verwendet, da sich der Nachbar in einem anderen Rechenzentrum befinden kann.  Der Motor sollte nicht daran gebunden sein, den Standort von etwas anderem als sich selbst zu kennen - wir haben diese Standardl√∂sung.  Aber nat√ºrlich gibt es Ausnahmen :) <br><br>  Ein Beispiel f√ºr ein TL-Schema, nach dem alle Motoren arbeiten. <br><br><pre> <code class="plaintext hljs">memcache.not_found = memcache.Value; memcache.strvalue value:string flags:int = memcache.Value; memcache.addOrIncr key:string flags:int delay:int value:long = memcache.Value; tasks.task fields_mask:# flags:int tag:%(Vector int) data:string id:fields_mask.0?long retries:fields_mask.1?int scheduled_time:fields_mask.2?int deadline:fields_mask.3?int = tasks.Task; tasks.addTask type_name:string queue_id:%(Vector int) task:%tasks.Task = Long;</code> </pre> <br>  Dies ist ein bin√§res Protokoll, dessen n√§chstgelegenes Analogon <strong>protobuf ist.</strong>  Das Schema beschreibt im Voraus optionale Felder, komplexe Typen - Erweiterungen integrierter Skalare und Abfragen.  Alles funktioniert nach diesem Protokoll. <br><br><h4>  RPC √ºber TL √ºber TCP / UDP ... UDP? </h4><br>  Wir haben ein RPC-Protokoll zum Abfragen der Engine, das √ºber dem TL-Schema ausgef√ºhrt wird.  Dies alles funktioniert √ºber die TCP / UDP-Verbindung.  TCP - es ist klar, warum wir oft nach UDP gefragt werden. <br><br>  UDP hilft, <strong>das Problem einer gro√üen Anzahl von Verbindungen zwischen Servern</strong> zu <strong>vermeiden</strong> .  Wenn auf jedem Server ein RPC-Proxy vorhanden ist und dieser im Allgemeinen an eine beliebige Engine gesendet werden kann, erhalten Sie Zehntausende von TCP-Verbindungen zum Server.  Es gibt eine Last, aber sie ist nutzlos.  Bei UDP ist dies kein Problem. <br><br>  <strong>Kein redundanter TCP-Handshake</strong> .  Dies ist ein typisches Problem: Wenn eine neue Engine oder ein neuer Server gestartet wird, werden viele TCP-Verbindungen gleichzeitig hergestellt.  Bei kleinen, leichten Anforderungen, z. B. UDP-Nutzdaten, besteht die gesamte Kommunikation zwischen dem Code und der Engine aus <strong>zwei UDP-Paketen:</strong> eines fliegt in die eine Richtung, das andere in die andere.  Eine Hin- und R√ºckfahrt - und der Code erhielt eine Antwort vom Motor ohne Handschlag. <br><br>  Ja, alles funktioniert nur <strong>mit einem sehr geringen Prozentsatz des Paketverlusts</strong> .  Das Protokoll unterst√ºtzt Neu√ºbertragungen und Zeit√ºberschreitungen. Wenn wir jedoch viel verlieren, erhalten wir praktisch TCP, was nicht rentabel ist.  Fahren Sie √ºber die Ozeane kein UDP. <br><br>  Wir haben Tausende solcher Server, und das gleiche Schema gibt es: Auf jedem physischen Server befindet sich ein Paket von Engines.  Grunds√§tzlich sind sie Single-Threaded, um so schnell wie m√∂glich ohne Blockierung zu arbeiten, und werden als Single-Threaded-L√∂sungen zerkleinert.  Gleichzeitig haben wir nichts zuverl√§ssigeres als diese Engines, und der dauerhaften Datenspeicherung wird viel Aufmerksamkeit geschenkt. <br><br><h3>  Permanente Datenspeicherung </h3><br>  <strong>Motoren schreiben Binlogs</strong> .  Ein Binlog ist eine Datei, an deren Ende ein Ereignis hinzugef√ºgt wird, um einen Status oder Daten zu √§ndern.  In verschiedenen L√∂sungen wird es unterschiedlich genannt: Bin√§rprotokoll, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WAL</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AOF</a> , aber das Prinzip ist eins. <br><br>  Damit die Engine w√§hrend eines Neustarts √ºber viele Jahre nicht das gesamte Binlog erneut liest, schreiben die Engines <strong>Snapshots - den aktuellen Status</strong> .  Bei Bedarf lesen sie zuerst daraus und dann aus dem Binlog.  Alle Binlogs werden im gleichen Bin√§rformat geschrieben - gem√§√ü dem TL-Schema, damit Administratoren sie mit ihren Tools gleicherma√üen verwalten k√∂nnen.  Schnappsch√ºsse sind nicht erforderlich.  Es gibt eine allgemeine √úberschrift, die angibt, wessen Schnappschuss die Int, die Magie des Motors ist und welcher K√∂rper f√ºr niemanden wichtig ist.  Dies ist das Problem der Engine, die den Schnappschuss aufgezeichnet hat. <br><br>  Ich werde kurz das Prinzip der Arbeit beschreiben.  Es gibt einen Server, auf dem die Engine ausgef√ºhrt wird.  Er √∂ffnet ein neues leeres Binlog f√ºr die Aufzeichnung und schreibt ein √Ñnderungsereignis hinein. <br><br><img src="https://habrastorage.org/webt/dd/w9/9p/ddw99p7g6upg9hci9ou6aln6d_c.png"><br><br>  Irgendwann beschlie√üt er entweder, einen Schnappschuss zu machen, oder er erh√§lt ein Signal.  Der Server erstellt eine neue Datei, schreibt ihren Status vollst√§ndig in sie, h√§ngt die aktuelle Gr√∂√üe des Binlog-Offsets an das Ende der Datei an und schreibt weiter.  Ein neues Binlog wird nicht erstellt. <br><br><img src="https://habrastorage.org/webt/ec/fq/yt/ecfqytibh2tsm5ncd8mfli-b1ta.png"><br><br>  Irgendwann, wenn die Engine neu gestartet wird, befinden sich ein Binlog und ein Snapshot auf der Festplatte.  Der Motor liest den vollst√§ndigen Schnappschuss ein und erh√∂ht seinen Zustand an einem bestimmten Punkt. <br><br><img src="https://habrastorage.org/webt/bg/ph/-u/bgph-uu68nqedhby4a2kf3r9c5u.png"><br><br>  Subtrahiert die Position, die zum Zeitpunkt der Erstellung des Snapshots war, und die Gr√∂√üe des Binlogs. <br><br><img src="https://habrastorage.org/webt/ar/gs/lq/argslqv8ewosmtic8-zaobq4g5o.png"><br><br>  Liest das Ende des Binlogs, um den aktuellen Status abzurufen, und schreibt weitere Ereignisse.  Dies ist ein einfaches Schema, an dem alle unsere Motoren arbeiten. <br><br><h4>  Datenreplikation </h4><br>  Infolgedessen ist die Datenreplikation anweisungsbasiert. Wir schreiben keine Seiten√§nderungen in das Binlog, sondern fordern <strong>√Ñnderungen an</strong> .  Sehr √§hnlich zu dem, was √ºber das Netzwerk kommt, nur wenig ver√§ndert. <br><br>  Das gleiche Schema wird nicht nur f√ºr die Replikation, sondern auch <strong>f√ºr die Erstellung von Sicherungen verwendet</strong> .  Wir haben eine Engine - einen Schreibmeister, der in ein Binlog schreibt.  An jedem anderen Ort, an dem sich die Administratoren eingerichtet haben, steigt das Kopieren dieses Binlogs, und das ist alles - wir haben ein Backup. <br><br><img src="https://habrastorage.org/webt/og/al/sz/ogalszm0wfe3f_064sbjnpo4p9c.png"><br><br>  Wenn Sie ein Lesereplikat ben√∂tigen, um die Belastung des Lesens auf der CPU zu verringern, steigt die Lese-Engine nur an, wodurch das Ende des Binlogs gelesen und diese Befehle lokal in sich selbst ausgef√ºhrt werden. <br><br>  Die Verz√∂gerung hier ist sehr gering, und es besteht die M√∂glichkeit herauszufinden, wie weit sich die Replik hinter dem Master befindet. <br><br><h3>  Daten-Sharding im RPC-Proxy </h3><br>  Wie funktioniert Sharding?  Wie versteht der Proxy, an welchen Cluster-Shard gesendet werden soll?  Der Code sagt nicht: "An 15 Shard senden!"  - Nein, es macht einen Proxy. <br><br>  <strong>Das einfachste Schema ist firstint</strong> , die erste Nummer in der Anfrage. <br><br> <code>get(photo100_500) =&gt; 100 % N.</code> <br> <br>  Dies ist ein Beispiel f√ºr ein einfaches Memcached-Text-Protokoll, aber Anforderungen sind nat√ºrlich komplex und strukturiert.  Das Beispiel verwendet die erste Zahl in der Abfrage und den Rest der Division durch die Clustergr√∂√üe. <br><br>  Dies ist n√ºtzlich, wenn wir die Datenlokalit√§t einer Entit√§t haben m√∂chten.  Angenommen, 100 ist eine Benutzer- oder Gruppen-ID, und wir m√∂chten, dass sich alle Daten einer Entit√§t f√ºr komplexe Abfragen auf demselben Shard befinden. <br><br>  Wenn es uns egal ist, wie die Anforderungen √ºber den Cluster verteilt sind, gibt es eine andere Option - das <strong>Hashing des gesamten Shards</strong> . <br><br> <code>hash(photo100_500) =&gt; 3539886280 % N</code> <br> <br>  Wir bekommen auch den Hash, den Rest der Division und die Nummer der Scherbe. <br><br>  Beide Optionen funktionieren nur, wenn wir darauf vorbereitet sind, dass wir den Cluster um ein Vielfaches aufteilen oder vergr√∂√üern, wenn wir ihn vergr√∂√üern.  Zum Beispiel hatten wir 16 Shards, wir fehlen, wir wollen mehr - Sie k√∂nnen sicher 32 ohne Ausfallzeiten erhalten.  Wenn wir mehrmals aufbauen wollen, kommt es zu Ausfallzeiten, da nicht alles ohne Verlust sorgf√§ltig zerkleinert werden kann.  Diese Optionen sind n√ºtzlich, aber nicht immer. <br><br>  Wenn wir eine beliebige Anzahl von Servern hinzuf√ºgen oder entfernen m√ºssen, wird <strong>konsistentes Hashing auf dem a la Ketama-Ring verwendet</strong> .  Gleichzeitig verlieren wir die Lokalit√§t der Daten vollst√§ndig. Wir m√ºssen eine Zusammenf√ºhrungsanforderung an den Cluster senden, damit jedes Teil seine kleine Antwort zur√ºckgibt, und die Antworten an den Proxy bereits kombinieren. <br><br>  Es gibt superspezifische Abfragen.   : RPC-proxy  , ,       .     , ,     ,      .    proxy. <br><br><img src="https://habrastorage.org/webt/jx/6t/f9/jx6tf9jlkkmva1qfifzmwrx58wc.png"><br><br><h2>  </h2><br>     .     ‚Äî <strong>   memcache</strong> . <br><br> <code>ring-buffer: prefix.idx = line</code> <br> <br>    ‚Äî  , ,      ‚Äî  .     0     1.   memcache ‚Äî       .        . <br><br>    ,   <strong>Multi Get</strong>  ,   ,         .  ,   -      ,   ,         ,      . <br><br>         <strong>logs-engine</strong> .      ,       .       600   . <br><br>   ,  ,    6‚Äì7 .    ,    , ,    ClickHouse   . <br><br><h3>    ClickHouse </h3><br>   ,      . <br><br><img src="https://habrastorage.org/webt/jm/-j/s0/jm-js04tjh8lb8pii1_dzl_sfa4.png"><br><br>  ,   RPC    RPC-proxy,   ,    .       ClickHouse,        : <br><br><ul><li>  -   ClickHouse; </li><li>  RPC-proxy,      ClickHouse,  - ,  ,   RPC. </li></ul><br>    ‚Äî          ClickHouse. <br><br>     ClickHouse,   <strong>KittenHouse</strong> .      KittenHouse  ClickHouse ‚Äî   .   ,  HTTP-     .   ,    ClickHouse <strong>  reverse proxy</strong> ,   ,     .         . <br><br><img src="https://habrastorage.org/webt/zj/fy/5y/zjfy5yuay9-6wqe3nrgjkeznvny.png"><br><br>      RPC-   , ,  nginx.   KittenHouse      UDP. <br><br><img src="https://habrastorage.org/webt/hq/wl/v_/hqwlv_vnujb-maxakxksbmrf6xo.png"><br><br>         ,    UDP-      .       RPC     ,      UDP.      . <br><br><h2>  √úberwachung </h2><br>     : ,        ,     .     : <strong>  </strong> . <br><br><h3>   </h3><br>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netdata</a> ,        <strong>Graphite Carbon</strong> .      ClickHouse,   Whisper, .       ClickHouse,   <strong>Grafana</strong>  ,   .  ,   Netdata  Grafana  . <br><br><h3>   </h3><br>      . ,    ,    Counts, UniqueCounts   ,   - . <br><br><pre> <code class="plaintext hljs">statlogsCountEvent ( 'stat_name', $key1, $key2, ‚Ä¶) statlogsUniqueCount ( 'stat_name', $uid, $key1, $key2, ‚Ä¶) statlogsValuetEvent ( 'stat_name', $value, $key1, $key2, ‚Ä¶) $stats = statlogsStatData($params)</code> </pre><br>      ,    ,     ‚Äî  ,  Wathdogs. <br><br>    <strong> ,</strong>    600   1   .       <strong>   </strong> ,     .     ‚Äî  ,     . ,      . <br><br>    ,     <strong>  memcache</strong> ,    .         <strong>stats-daemon</strong>   .         <strong>logs-collectors</strong> ,       ,      . <br><br><img src="https://habrastorage.org/webt/ih/ab/oy/ihaboy4luh5hriorej9seodbx6u.png"><br><br>        logs-collectors. <br><br><img src="https://habrastorage.org/webt/fq/ta/bj/fqtabjgq556wqfdz5_kfq3mj94c.png"><br><br>          stas-daemom ‚Äî   ,      collector.  ,    -        memcache stats-daemon,   ,    . <br><br>  logs-collectors    <strong>meowDB</strong> ‚Äî   ,      . <br><br><img src="https://habrastorage.org/webt/v_/gb/_y/v_gb_ya-9ywkra7xdh5h_qtqsc4.png"><br><br>      ¬´-SQL¬ª  . <br><br><img src="https://habrastorage.org/webt/1q/gw/wp/1qgwwpyj3ewcwuonshvty_zcfhc.png"><br><br><h3>  </h3><br>  2018     ,          -,      ClickHouse.      ClickHouse ‚Äî    ? <br><br><img src="https://habrastorage.org/webt/wg/mz/kl/wgmzklw41x7ilj0-5hbr_kfdif8.png"><br><br>    ,     KittenHouse. <br><br><img src="https://habrastorage.org/webt/kq/s7/uj/kqs7ujzbhnqzt5f8djepldmwxia.png"><br><br>   <strong>     ¬´*House¬ª</strong> ,        ,       UDP.   *House    inserts,  ,   KittenHouse.        ClickHouse,     . <br><br><img src="https://habrastorage.org/webt/ff/k3/th/ffk3thypln9exuhyuhr-nuj9hr4.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><img src="https://habrastorage.org/webt/r4/g3/e9/r4g3e9yakpzbx5gscmgyl6keqsa.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><ul><li>     ,     StatsHouse. </li><li> StatsHouse   KittenHouse UDP-,    SQL-inserts, . </li><li> KittenHouse    ClickHouse. </li><li>     ,      StatsHouse ‚Äî   ClickHouse  SQL. </li></ul><br>    <strong></strong> ,   ,  .    , , ,    .     . <br><br>  <strong>  </strong> .   ,    stats-daemons  logs-collectors,  ClickHouse   ,  ,     . <strong>  ,       </strong> . <br><br><h2>  </h2><br>     PHP.    <strong>git</strong> :  <strong>GitLab</strong>  <strong>TeamCity</strong>  .     -,       ,   ‚Äî  . <br><br>        ,     diff  ‚Äî : , , .     binlog   copyfast,          .     ,  <strong>gossip replication</strong> ,       ,  ‚Äî  ,   .            .      ,       <strong>  </strong> .       . <br><br>     kPHP         <strong>git</strong>   .    <strong> HTTP-</strong> ,      diff ‚Äî     .     ‚Äî    <strong>binlog copyfast</strong> .     ,      .  <strong>  </strong> .  copyfast' ,   binlog   ,     gossip replication     ,    -,      .   <strong>graceful </strong>   . <br><br>   ,     ,   : <br><br><ul><li> git master branch; </li><li>   <strong>.deb</strong> ; </li><li>    binlog copyfast; </li><li>   ; </li><li>     .dep; </li><li> <strong>dpkg -i</strong> ; </li><li> graceful    . </li></ul><br>   ,        <strong>.deb</strong> ,     <strong>dpkg -i</strong>   .    kPHP  ,   ‚Äî dpkg?  .  ‚Äî  . <br><br> <b> :</b> <br><br><ul><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">¬´  Vkontakte. ?¬ª</a>    copyfast  gossip. </li><li>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">¬´ VK    CLickHouse    ¬ª</a> . </li><li>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">¬´     ¬ª</a> ,     ,   . </li></ul><br><blockquote>     ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PHP Russia</a>  17          PHP-. ,     ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> (     PHP!) ‚Äî ,      PHP,   . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de449254/">https://habr.com/ru/post/de449254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de449236/index.html">Ok Google: Wie komme ich durch das Captcha?</a></li>
<li><a href="../de449240/index.html">Die Geschichte eines jungen Daida-Dienstes (Abonnementkunst)</a></li>
<li><a href="../de449246/index.html">AX200 - Intel Wi-Fi 6</a></li>
<li><a href="../de449248/index.html">Moderne IDE. Auf jeden Fall D, bis zu einem gewissen Grad E, und schon gar nicht ich</a></li>
<li><a href="../de449252/index.html">Zombie-Projekte - Benutzerdaten auch nach seinem Tod zusammenf√ºhren</a></li>
<li><a href="../de449256/index.html">Ich habe 80 Lebensl√§ufe gelesen, ich habe Fragen</a></li>
<li><a href="../de449260/index.html">Was ist automatisiertes maschinelles Lernen (AutoML)</a></li>
<li><a href="../de449262/index.html">Neuestes IRM - Siebel-Upgrade auf IP17 +</a></li>
<li><a href="../de449264/index.html">Erstellen eines Berichtssystems f√ºr 1C: ERP basierend auf OLAP und Excel</a></li>
<li><a href="../de449266/index.html">3 Berichte mit RusCrypto: Konferenzen mit Erfahrung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>