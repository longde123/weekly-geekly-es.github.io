<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚öæÔ∏è ‚ö∞Ô∏è ü§∂üèª Sugerencias para vulnerabilidades y protecci√≥n de modelos de aprendizaje autom√°tico üë©üèΩ‚Äçüç≥ üö¥üèº üõí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recientemente, los expertos abordan cada vez m√°s la cuesti√≥n de la seguridad de los modelos de aprendizaje autom√°tico y ofrecen diversos m√©todos de pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sugerencias para vulnerabilidades y protecci√≥n de modelos de aprendizaje autom√°tico</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/458892/"><img src="https://habrastorage.org/webt/gn/da/kl/gndaklzm6lwmn9ijp2pb9sxnmza.jpeg"><br><br>  Recientemente, los expertos abordan cada vez m√°s la cuesti√≥n de la seguridad de los modelos de aprendizaje autom√°tico y ofrecen diversos m√©todos de protecci√≥n.  Es hora de estudiar en detalle las vulnerabilidades y defensas potenciales en el contexto de los sistemas de modelado tradicionales populares, como los modelos lineales y de √°rbol, entrenados en conjuntos de datos est√°ticos.  Aunque el autor de este art√≠culo no es un experto en seguridad, sigue cuidadosamente temas como la depuraci√≥n, explicaciones, equidad, interpretabilidad y privacidad en el aprendizaje autom√°tico. <br><br>  En este art√≠culo, presentamos varios vectores probables de ataques en un sistema de aprendizaje autom√°tico t√≠pico en una organizaci√≥n t√≠pica, ofrecemos soluciones tentativas para la protecci√≥n y consideramos algunos problemas comunes y las pr√°cticas m√°s prometedoras. <br><a name="habracut"></a><br><h2>  1. Ataques de corrupci√≥n de datos </h2><br>  La distorsi√≥n de datos significa que alguien cambia sistem√°ticamente los datos de entrenamiento para manipular las predicciones de su modelo (tales ataques tambi√©n se llaman ataques "causales").  Para distorsionar los datos, un atacante debe tener acceso a algunos o todos sus datos de entrenamiento.  Y en ausencia de un control adecuado en muchas empresas, diferentes empleados, consultores y contratistas pueden tener dicho acceso.  Un atacante fuera del per√≠metro de seguridad tambi√©n puede obtener un acceso no autorizado a algunos o todos los datos de entrenamiento. <br><br>  Un ataque directo a datos corruptos puede incluir cambiar las etiquetas del conjunto de datos.  Por lo tanto, sea cual sea el uso comercial de su modelo, un atacante puede gestionar sus pron√≥sticos, por ejemplo, cambiando las etiquetas para que su modelo pueda aprender a otorgar grandes pr√©stamos, grandes descuentos o establecer peque√±as primas de seguro para los atacantes.  Obligar a un modelo a hacer predicciones falsas en inter√©s de un atacante a veces se denomina violaci√≥n de la "integridad" del modelo. <br><br>  Un atacante tambi√©n puede usar la corrupci√≥n de datos para entrenar su modelo con el prop√≥sito de discriminar deliberadamente a un grupo de personas, priv√°ndoles de un pr√©stamo grande, grandes descuentos o primas de seguro bajas a las que tienen derecho.  En esencia, este ataque es similar a DDoS.  Forzar a un modelo a hacer predicciones falsas para da√±ar a otros a veces se llama una violaci√≥n de la "accesibilidad" del modelo. <br><br>  Aunque parezca que es m√°s f√°cil distorsionar los datos que cambiar los valores en las filas existentes de un conjunto de datos, tambi√©n puede introducir distorsiones agregando columnas aparentemente inofensivas o adicionales al conjunto de datos.  Los valores modificados en estas columnas pueden hacer que cambien las predicciones del modelo. <br><br>  Ahora veamos algunas posibles soluciones protectoras y expertas (forenses) en caso de corrupci√≥n de datos: <br><br><ul><li>  <b>An√°lisis de impacto diferenciado</b> .  Muchos bancos ya est√°n realizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">an√°lisis de</a> impacto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">diferencial</a> para pr√©stamos justos para determinar si su modelo es discriminado por diferentes categor√≠as de personas.  Sin embargo, muchas otras organizaciones a√∫n no han llegado tan lejos.  Existen varias herramientas excelentes de c√≥digo abierto para detectar discriminaci√≥n y realizar an√°lisis de impacto diferencial.  Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aequitas,</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Themis</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AIF360</a> . <br></li><li> <b>Modelos justos o privados</b> .  Los modelos como el aprendizaje de representaciones justas (LFR) y la agregaci√≥n privada de conjuntos de docentes (PATE) tienden a prestar menos atenci√≥n a las propiedades demogr√°ficas individuales al generar pron√≥sticos.  Adem√°s, estos modelos pueden ser menos susceptibles a ataques discriminatorios para distorsionar los datos. <br></li><li>  <b>Rechazo al Impacto Negativo (RONI)</b> .  RONI es un m√©todo para eliminar filas de datos de un conjunto de datos que reduce la precisi√≥n de la predicci√≥n.  Para obtener m√°s informaci√≥n sobre RONI, consulte la Secci√≥n 8, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Seguridad del aprendizaje autom√°tico</a> . <br></li><li>  <b>An√°lisis residual</b> .  Busque patrones extra√±os y notables en los residuos de las predicciones de su modelo, especialmente las relacionadas con empleados, consultores o contratistas. <br></li><li>  <b>Autorreflexi√≥n</b>  Eval√∫e modelos de sus empleados, consultores y contratistas para identificar pron√≥sticos anormalmente favorables. <br></li></ul><br>  El an√°lisis de impacto diferenciado, el an√°lisis residual y la autorreflexi√≥n pueden llevarse a cabo durante la capacitaci√≥n y en el marco del monitoreo en tiempo real de los modelos. <br><br><h2>  2. Ataques de marca de agua </h2><br>  Una marca de agua es un t√©rmino tomado de la literatura sobre la seguridad del aprendizaje profundo, que a menudo se refiere a la adici√≥n de p√≠xeles especiales a la imagen para obtener el resultado deseado de su modelo.  Es completamente posible hacer lo mismo con los datos de clientes o transacciones. <br><br>  Considere un escenario en el que un empleado, consultor, contratista o atacante externo tenga acceso al c√≥digo para el uso de producci√≥n de su modelo que haga pron√≥sticos en tiempo real.  Dicha persona puede cambiar el c√≥digo para reconocer una combinaci√≥n extra√±a o poco probable de valores variables de entrada para obtener el resultado de predicci√≥n deseado.  Al igual que la corrupci√≥n de datos, los ataques de marcas de agua se pueden usar para violar la integridad o la accesibilidad de su modelo.  Por ejemplo, para violar la integridad, un atacante puede insertar una "carga √∫til" en el c√≥digo de evaluaci√≥n para el uso de producci√≥n del modelo, como resultado de lo cual reconoce una combinaci√≥n de 0 a√±os en la direcci√≥n 99, lo que dar√° lugar a un pron√≥stico positivo para el atacante.  Y para bloquear la disponibilidad del modelo, puede insertar una regla discriminatoria artificial en el c√≥digo de evaluaci√≥n, que no permitir√° que el modelo d√© resultados positivos para un determinado grupo de personas. <br><br>  Los enfoques protectores y expertos para los ataques con marcas de agua pueden incluir: <br><br><ul><li>  <b>Detecci√≥n de anomal√≠as</b> .  Autocoders es un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo de detecci√≥n de fraude</a> que puede identificar entradas complejas y extra√±as, o que no se parecen a otros datos.  Potencialmente, los codificadores autom√°ticos pueden detectar cualquier marca de agua utilizada para activar mecanismos maliciosos. <br></li><li>  <b>Limitaciones de integridad de datos</b> .  Muchas bases de datos no permiten combinaciones extra√±as o poco realistas de variables de entrada, lo que podr√≠a prevenir ataques de marcas de agua.  El mismo efecto puede funcionar para las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">restricciones de integridad</a> de los flujos de datos que se reciben en tiempo real. <br></li><li>  <b>An√°lisis de exposici√≥n diferenciada</b> : ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 1</a> . <br></li><li>  <b>Control de versiones</b> .  El c√≥digo de evaluaci√≥n para la aplicaci√≥n de producci√≥n del modelo debe ser versionado y controlado, como cualquier otro producto de software cr√≠tico. <br></li></ul><br>  La detecci√≥n de anomal√≠as, las limitaciones de integridad de datos y el an√°lisis de impacto diferencial se pueden usar durante el entrenamiento y como parte del monitoreo del modelo en tiempo real. <br><br><h2>  3. Inversi√≥n de modelos sustitutos </h2><br>  Por lo general, "inversi√≥n" se llama obtener informaci√≥n no autorizada de un modelo, en lugar de colocar informaci√≥n en √©l.  Adem√°s, la inversi√≥n puede ser un ejemplo de un "ataque de reconocimiento de ingenier√≠a inversa".  Si un atacante puede obtener muchas predicciones de la API de su modelo u otro punto final (sitio web, aplicaci√≥n, etc.), puede entrenar a su propio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelo sustituto</a> .  En pocas palabras, esta es una simulaci√≥n de su modelo predictivo.  Te√≥ricamente, un atacante puede entrenar un modelo sustituto entre los datos de entrada utilizados para generar los pron√≥sticos recibidos y los pron√≥sticos mismos.  Dependiendo del n√∫mero de predicciones que se pueden recibir, el modelo sustituto puede convertirse en una simulaci√≥n bastante precisa de su modelo.  Despu√©s de entrenar el modelo sustituto, el atacante tendr√° una "caja de arena" desde la cual puede planificar la impersonalizaci√≥n (es decir, "imitaci√≥n") o un ataque con un ejemplo competitivo sobre la integridad de su modelo, o ganar el potencial para comenzar a recuperar algunos aspectos de sus datos confidenciales de entrenamiento.  Los modelos sustitutos tambi√©n se pueden entrenar utilizando fuentes de datos externas que de alguna manera sean consistentes con sus predicciones, como, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ProPublica</a> hizo con el modelo de reincidencia del autor COMPAS. <br><br>  Para proteger su modelo de la inversi√≥n utilizando un modelo sustituto, puede confiar en dichos enfoques: <br><br><ul><li>  <b>Acceso autorizado</b>  Solicite autenticaci√≥n adicional (por ejemplo, dos factores) para obtener un pron√≥stico. <br></li><li>  <b>Predicciones del acelerador</b>  Limite una gran cantidad de pron√≥sticos r√°pidos de usuarios individuales;  considere la posibilidad de aumentar artificialmente los retrasos de predicci√≥n. <br></li><li>  <b>Modelos sustitutos "blancos" (sombrero blanco)</b> .  Como ejercicio de hacker blanco, intente lo siguiente: entrene sus propios modelos sustitutos entre su entrada y los pron√≥sticos del modelo para una aplicaci√≥n de producci√≥n, y observe cuidadosamente los siguientes aspectos: <br><ul><li>  l√≠mites de precisi√≥n de varios tipos de modelos sustitutos "blancos";  trate de comprender hasta qu√© punto el modelo sustituto puede usarse para obtener datos no deseados sobre su modelo. <br></li><li>  tipos de tendencias de datos que se pueden aprender de su modelo sustituto "blanco", por ejemplo, tendencias lineales representadas por coeficientes de modelo lineal. <br></li><li>  tipos de segmentos o distribuciones demogr√°ficas que pueden estudiarse analizando el n√∫mero de personas asignadas a ciertos nodos del √°rbol de decisi√≥n sustituto "blanco". <br></li><li>  las reglas que se pueden aprender del √°rbol de decisi√≥n sustituto "blanco", por ejemplo, c√≥mo representar con precisi√≥n a una persona que recibir√° un pron√≥stico positivo. <br></li></ul><br></li></ul><br><h2>  4. Ataques de rivalidad </h2><br>  En teor√≠a, un pirata inform√°tico dedicado puede aprender, por ejemplo, prueba y error (es decir, "inteligencia" o "an√°lisis de sensibilidad"), invertir un modelo sustituto o ingenier√≠a social, c√≥mo jugar con su modelo para obtener el resultado de predicci√≥n deseado o evitar lo no deseado previsi√≥n.  Intentar alcanzar tales objetivos utilizando una cadena de datos especialmente dise√±ada se denomina ataque de confrontaci√≥n.  (a veces un ataque para investigar la integridad).  Un atacante puede usar un ataque de confrontaci√≥n para obtener un pr√©stamo grande o una prima de seguro baja, o para evitar la negaci√≥n de la libertad condicional con una alta evaluaci√≥n del riesgo penal.  Algunas personas llaman al uso de ejemplos competitivos para excluir un resultado indeseable de un pron√≥stico como "evasi√≥n". <br><br>  Pruebe los m√©todos descritos a continuaci√≥n para defender o detectar un ataque con un ejemplo competitivo: <br><br><ul><li>  <b>An√°lisis de activaci√≥n</b> .  El an√°lisis de activaci√≥n requiere que sus modelos predictivos tengan mecanismos internos comparativos, por ejemplo, la activaci√≥n promedio de las neuronas en su red neuronal o la proporci√≥n de observaciones relacionadas con cada nodo final en su bosque aleatorio.  Luego, compara esta informaci√≥n con el comportamiento del modelo con flujos de datos entrantes reales.  Como dijo uno de mis colegas: " <i>Es lo mismo que ver un nodo final en un bosque aleatorio que corresponde al 0.1% de los datos de entrenamiento, pero es adecuado para el 75% de las l√≠neas de puntuaci√≥n por hora</i> ". <br></li><li>  <b>Detecci√≥n de anomal√≠as</b> .  ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 2</a> . <br></li><li>  <b>Acceso autorizado</b>  ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 3</a> . <br></li><li>  <b>Modelos comparativos</b> .  Al evaluar nuevos datos, adem√°s de un modelo m√°s complejo, use un modelo comparativo de alta transparencia.  Los modelos interpretados son m√°s dif√≠ciles de descifrar porque sus mecanismos son transparentes.  Al evaluar nuevos datos, compare el nuevo modelo con un modelo transparente confiable, o un modelo capacitado en datos verificados y en un proceso confiable.  Si la diferencia entre el modelo m√°s complejo y opaco y el interpretado (o verificado) es demasiado grande, regrese a los pron√≥sticos conservadores del modelo o procese la l√≠nea de datos manualmente.  Registre este incidente, podr√≠a ser un ataque con un ejemplo competitivo. <br></li><li>  <b>Pron√≥sticos del acelerador</b> : ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 3</a> . <br></li><li>  <b>An√°lisis de sensibilidad "blanco"</b> .  Utilice el an√°lisis de sensibilidad para realizar sus propios ataques de investigaci√≥n para comprender qu√© valores variables (o combinaciones de ellos) pueden causar grandes fluctuaciones en los pron√≥sticos.  Busque estos valores o combinaciones de valores al evaluar nuevos datos.  Para realizar un an√°lisis de investigaci√≥n "blanco", puede usar el paquete de c√≥digo abierto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cleverhans</a> . <br></li><li>  Modelos sustitutos blancos: ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 3</a> . <br></li></ul><br>  El an√°lisis de activaci√≥n o los modelos comparativos se pueden usar durante el entrenamiento y como parte del monitoreo en tiempo real de los modelos. <br><br><h2>  5. La impersonalizaci√≥n </h2><br>  Un pirata inform√°tico intencionado puede descubrir, de nuevo, mediante prueba y error, mediante la inversi√≥n con un modelo sustituto o ingenier√≠a social, qu√© datos de entrada o personas espec√≠ficas obtienen el resultado de predicci√≥n deseado.  Un atacante puede suplantar a esta persona para beneficiarse del pron√≥stico.  Los ataques de impersonalizaci√≥n a veces se llaman ataques "simulados", y desde el punto de vista del modelo, esto recuerda el robo de identidad.  Como en el caso de un ejemplo de ataque competitivo, con la impersonalizaci√≥n, los datos de entrada cambian artificialmente de acuerdo con su modelo.  Pero, a diferencia del mismo ataque con un ejemplo competitivo, en el que se puede usar una combinaci√≥n de valores potencialmente aleatoria para enga√±ar, en la impersonalizaci√≥n, para obtener el pron√≥stico asociado con este tipo de objeto, informaci√≥n asociada con otro objeto modelado (por ejemplo, un cliente condenado) , empleado, transacci√≥n financiera, paciente, producto, etc.).  Supongamos que un atacante puede averiguar de qu√© caracter√≠sticas de su modelo depende la provisi√≥n de grandes descuentos o beneficios.  Luego puede falsificar la informaci√≥n que utiliza para obtener dicho descuento.  Un atacante puede compartir su estrategia con otros, lo que puede generar grandes p√©rdidas para su empresa. <br><br>  Si est√° utilizando un modelo de dos etapas, tenga cuidado con un ataque "al√©rgico": un atacante puede simular una cadena de datos de entrada ordinarios para la primera etapa de su modelo para atacar su segunda etapa. <br><br>  Los enfoques protectores y expertos para ataques con impersonalizaci√≥n pueden incluir: <br><br><ul><li>  An√°lisis de activaci√≥n.  ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 4</a> . <br></li><li>  Acceso autorizado  ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">secci√≥n 3</a> . <br></li><li>  Verifica si hay duplicados.  En la etapa de puntuaci√≥n, realice un seguimiento de la cantidad de registros similares para los que est√° disponible su modelo.  Esto se puede hacer en un espacio dimensional reducido utilizando autocodificadores, escalamiento multidimensional (MDS) o m√©todos similares de reducci√≥n dimensional.  Si hay demasiadas l√≠neas similares en un per√≠odo de tiempo determinado, tome medidas correctivas. <br></li><li> Caracter√≠sticas de notificaci√≥n de amenazas.  Guarde la funci√≥n <code>num_similar_queries</code> en su canalizaci√≥n, que puede ser in√∫til inmediatamente despu√©s de entrenar o implementar su modelo, pero se puede usar durante la evaluaci√≥n (o durante la <code>num_similar_queries</code> capacitaci√≥n) para notificar al modelo o la tuber√≠a de amenazas.  Por ejemplo, si en el momento de la calificaci√≥n, el valor de <code>num_similar_queries</code> mayor que cero, entonces la solicitud de evaluaci√≥n puede enviarse para su an√°lisis manual.  En el futuro, cuando vuelva a entrenar el modelo, podr√° ense√±arle a producir resultados de predicci√≥n negativos para l√≠neas de entrada con un alto n√∫mero de consultas <code>num_similar_queries</code> . <br></li></ul><br>  El an√°lisis de activaci√≥n, la verificaci√≥n duplicada y la notificaci√≥n de posibles amenazas se pueden utilizar durante la capacitaci√≥n y en el monitoreo de modelos en tiempo real. <br><br><h2>  6. Problemas comunes </h2><br>  Algunos usos comunes del aprendizaje autom√°tico tambi√©n plantean problemas de seguridad m√°s generales. <br><br>  <b>Cajas negras y complejidad innecesaria</b> .  Aunque los avances recientes en modelos interpretados y explicaciones de modelos hacen posible el uso de clasificadores y regresores no lineales precisos y transparentes, muchos procesos de aprendizaje autom√°tico contin√∫an enfoc√°ndose en modelos de caja negra.  Son solo un tipo de complejidad a menudo innecesaria en el flujo de trabajo est√°ndar del aprendizaje autom√°tico comercial.  Otros ejemplos de complejidad potencialmente da√±ina pueden ser especificaciones demasiado ex√≥ticas o una gran cantidad de dependencias de paquetes.  Esto puede ser un problema por al menos dos razones: <br><br><ol><li>  Un hacker persistente y motivado puede aprender m√°s acerca de su sistema de simulaci√≥n de caja negra excesivamente complejo que usted o su equipo (especialmente en el mercado moderno sobrecalentado y r√°pidamente cambiante para "analizar" datos).  Para esto, un atacante puede usar muchos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√©todos</a> nuevos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">explicaci√≥n</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">independientes del</a> modelo y un an√°lisis de sensibilidad cl√°sico, adem√°s de muchas otras herramientas de pirater√≠a m√°s comunes.  Este desequilibrio de conocimiento puede utilizarse potencialmente para llevar a cabo los ataques descritos en las secciones 1-5, o para otros tipos de ataques que a√∫n se desconocen. <br></li><li>  El aprendizaje autom√°tico en entornos de investigaci√≥n y desarrollo depende en gran medida de un ecosistema diverso de paquetes de software de c√≥digo abierto.  Algunos de estos paquetes tienen muchos participantes y usuarios, otros son altamente especializados y los necesita un peque√±o c√≠rculo de investigadores y profesionales.  Se sabe que muchos paquetes est√°n respaldados por brillantes estad√≠sticos e investigadores de aprendizaje autom√°tico que se centran en las matem√°ticas o los algoritmos, en lugar de la ingenier√≠a de software y ciertamente no la seguridad.  Hay casos frecuentes en los que el proceso de aprendizaje autom√°tico depende de docenas o incluso cientos de paquetes externos, cada uno de los cuales puede ser pirateado para ocultar una "carga √∫til" maliciosa. <br></li></ol><br>  <b>Sistemas y modelos distribuidos</b> .  Afortunadamente o desafortunadamente, vivimos en una era de big data.  En la actualidad, muchas organizaciones utilizan sistemas de procesamiento de datos distribuidos y de aprendizaje autom√°tico.  La inform√°tica distribuida puede ser un gran objetivo para ataques desde dentro o desde fuera.  Los datos se pueden distorsionar solo en uno o varios nodos de trabajo de un gran sistema distribuido de almacenamiento o procesamiento de datos.  La puerta trasera para marcas de agua se puede codificar en un modelo de un conjunto grande.  En lugar de depurar un conjunto de datos o modelo simple, los profesionales ahora deber√≠an estudiar datos o modelos dispersos en grandes grupos inform√°ticos. <br><br>  <b>Ataques distribuidos de denegaci√≥n de servicio (DDoS)</b> .  Si un servicio de modelado predictivo desempe√±a un papel clave en las actividades de su organizaci√≥n, aseg√∫rese de tener en cuenta al menos los ataques DDoS distribuidos m√°s populares cuando los atacantes atacan un servicio predictivo con una cantidad incre√≠blemente grande de solicitudes para retrasar o dejar de hacer pron√≥sticos para usuarios leg√≠timos. <br><br><h2>  7. Decisiones generales </h2><br>  Puede utilizar varios m√©todos comunes, antiguos y nuevos, m√°s efectivos para reducir las vulnerabilidades del sistema de seguridad y aumentar la equidad, la capacidad de control, la transparencia y la confianza en los sistemas de aprendizaje autom√°tico. <br><br>  <b>Acceso autorizado y predicci√≥n de regulaci√≥n de frecuencia (aceleraci√≥n)</b> .  Las caracter√≠sticas de seguridad est√°ndar, como la autenticaci√≥n adicional y el ajuste de la frecuencia de predicci√≥n, pueden ser muy efectivas para bloquear una serie de vectores de ataque descritos en las secciones 1-5. <br><br>  <b>Modelos comparativos</b> .  Como modelo comparativo para determinar si se realizaron manipulaciones con el pron√≥stico, puede usar la tuber√≠a de modelado antigua y comprobada u otra herramienta de pron√≥stico interpretada con alta transparencia.  La manipulaci√≥n incluye corrupci√≥n de datos, ataques de marcas de agua o ejemplos competitivos.  Si la diferencia entre el pron√≥stico de su modelo probado y el pron√≥stico de un modelo m√°s complejo y opaco es demasiado grande, escriba tales casos.  Env√≠elos a analistas o tome otras medidas para analizar o corregir la situaci√≥n.  Se deben tomar precauciones serias para garantizar que su punto de referencia y el transportador permanezcan seguros y sin cambios desde su condici√≥n original y confiable. <br><br>  <b>Modelos interpretados, justos o privados</b> .  Actualmente, existen m√©todos (p. Ej., <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GBM mon√≥tono (M-GBM),</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">listas de reglas bayesianas escalables (SBRL)</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">explicaciones de redes neuronales (XNN)</a> ) que proporcionan precisi√≥n e interpretabilidad.  Estos modelos precisos e interpretables son m√°s f√°ciles de documentar y depurar que los cuadros negros cl√°sicos de aprendizaje autom√°tico.  Los tipos m√°s nuevos de modelos justos y privados (por ejemplo, LFR, PATE) tambi√©n se pueden capacitar sobre c√≥mo prestar menos atenci√≥n a las caracter√≠sticas demogr√°ficas externas visibles que est√°n disponibles para la observaci√≥n, utilizando la ingenier√≠a social durante un ataque con un ejemplo competitivo, o impersonalizaci√≥n  ¬øEst√° considerando crear un nuevo proceso de aprendizaje autom√°tico en el futuro?  Considere construirlo sobre la base de modelos privados o justos interpretados menos riesgosos.  Son m√°s f√°ciles de depurar y potencialmente resistentes a los cambios en las caracter√≠sticas de los objetos individuales. <br><br>  <b>Depuraci√≥n de un modelo de seguridad</b> .  Una nueva √°rea para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">depurar modelos est√°</a> dedicada a detectar errores en los mecanismos y pron√≥sticos de modelos de aprendizaje autom√°tico y corregirlos.  Las herramientas de depuraci√≥n, como los modelos sustitutos, el an√°lisis residual y el an√°lisis de sensibilidad, se pueden usar en pruebas blancas para identificar sus vulnerabilidades, o en ejercicios anal√≠ticos para identificar posibles ataques que puedan ocurrir o puedan ocurrir. <br><br>  <b>Documentaci√≥n del modelo y m√©todos de explicaci√≥n</b> .  La documentaci√≥n del modelo es una estrategia de reducci√≥n de riesgos que se ha utilizado en la banca durante d√©cadas.  Le permite guardar y transferir conocimientos sobre sistemas de modelado complejos a medida que cambia la composici√≥n de los propietarios del modelo.  La documentaci√≥n se ha utilizado tradicionalmente para modelos lineales de alta transparencia.  Pero con el advenimiento de herramientas de explicaci√≥n poderosas y precisas (como el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">√°rbol SHAP</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">los atributos basados ‚Äã‚Äãen derivados de funciones locales</a> para redes neuronales), los flujos de trabajo de modelos de caja negra preexistentes pueden explicarse, depurarse y documentarse al menos un poco.  Obviamente, la documentaci√≥n ahora debe incluir todos los objetivos de seguridad, incluidas las vulnerabilidades conocidas, fijas o esperadas. <br><br>  <b>Monitoree y administre modelos directamente por razones de seguridad</b> .  Los profesionales serios entienden que la mayor√≠a de los modelos est√°n entrenados en "instant√°neas" est√°ticas de la realidad en forma de conjuntos de datos, y que en tiempo real la precisi√≥n de los pron√≥sticos disminuye, ya que el estado actual de las cosas se aleja de la informaci√≥n recopilada anteriormente.  Hoy, el monitoreo de la mayor√≠a de los modelos tiene como objetivo identificar tal sesgo en la distribuci√≥n de las variables de entrada, lo que, en √∫ltima instancia, conducir√° a una disminuci√≥n en la precisi√≥n.  El monitoreo del modelo debe dise√±arse para rastrear los ataques descritos en las secciones 1-5 y cualquier otra amenaza potencial que salga a la luz al depurar su modelo.  Aunque esto no siempre est√° directamente relacionado con la seguridad, los modelos tambi√©n deben evaluarse en tiempo real para detectar efectos diferenciados.  Junto con la documentaci√≥n del modelo, todos los artefactos de modelado, el c√≥digo fuente y los metadatos asociados deben administrarse, versionarse y verificarse por seguridad, as√≠ como los valiosos activos comerciales que son. <br><br>  <b>Caracter√≠sticas de notificaci√≥n de amenazas</b> .  Las funciones, reglas y etapas del procesamiento preliminar o posterior pueden incluirse en sus modelos o procesos equipados con medios de notificaci√≥n de posibles amenazas: por ejemplo, el n√∫mero de l√≠neas similares en el modelo;  si la l√≠nea actual representa a un empleado, contratista o consultor;  ¬øLos valores en la l√≠nea actual son similares a los obtenidos con ataques blancos con un ejemplo competitivo?  Estas funciones pueden o no ser necesarias durante la primera capacitaci√≥n del modelo.  Pero ahorrarles espacio alg√∫n d√≠a puede ser muy √∫til para evaluar nuevos datos o con el posterior reciclaje del modelo. <br><br>  <b>Detecci√≥n de anomal√≠as del sistema</b> .  Capacite el metamodo para detectar anomal√≠as basadas en un codificador autom√°tico en las estad√≠sticas operativas de todo su sistema de modelado predictivo (el n√∫mero de pron√≥sticos para un cierto per√≠odo de tiempo, demoras, carga de CPU, memoria y disco, el n√∫mero de usuarios simult√°neos, etc.), y luego monitoree cuidadosamente este metamodelo para anomal√≠as  Una anomal√≠a puede decir si algo sale mal.  Se requerir√°n investigaciones de seguimiento o mecanismos especiales para rastrear con precisi√≥n la causa del problema. <br><br><h2>  8. Referencias e informaci√≥n para lecturas adicionales. </h2><br>  Una gran cantidad de literatura acad√©mica moderna sobre seguridad en el aprendizaje autom√°tico se centra en el aprendizaje adaptativo, el aprendizaje profundo y el cifrado.  Sin embargo, hasta ahora el autor no conoce a los practicantes que realmente har√≠an todo esto.  Por lo tanto, adem√°s de los art√≠culos y publicaciones publicados recientemente, presentamos art√≠culos de la d√©cada de 1990 y principios de la d√©cada de 2000 sobre violaciones de la red, detecci√≥n de virus, filtrado de spam y temas relacionados, que tambi√©n fueron fuentes √∫tiles.  Si desea obtener m√°s informaci√≥n sobre el fascinante tema de proteger los modelos de aprendizaje autom√°tico, estos son los enlaces principales, del pasado y del presente, que se utilizaron para escribir la publicaci√≥n. <br><br><ul><li>  Bareno, Marco y otros, Machine Learning Safety.  Machine Learning 81.2 (2010): 121-148.  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://people.eecs.berkeley.edu/ <sub>~</sub> adj / publications / paper-files / SecML-MLJ2010.pdf</a> <br></li><li>  Kumar, Agites.  "Ataques de seguridad: un an√°lisis de modelos de aprendizaje autom√°tico".  DZone (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://dzone.com/articles/security-attacks-analysis-of-machine-learning-mode</a> <br></li><li>  Lorica, Ben y Lucidis, Mike.  ‚ÄúHas creado una aplicaci√≥n de aprendizaje autom√°tico.  Ahora aseg√∫rate de que sea seguro.  Ideas O'Reilly (2019).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure</a> <br></li><li>  Paperno, Nicholas.  ‚ÄúEl mapa de los merodeadores de la seguridad y la privacidad en el aprendizaje autom√°tico: una revisi√≥n de las tendencias de investigaci√≥n actuales y futuras para lograr la seguridad y la privacidad del aprendizaje autom√°tico‚Äù  Actas del XI Taller ACM sobre Inteligencia Artificial y Seguridad.  ACM (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://arxiv.org/pdf/1811.01134.pdf</a> <br></li></ul><br><h2>  Conclusi√≥n </h2><br>  Los que se preocupan por la ciencia y la pr√°ctica del aprendizaje autom√°tico est√°n preocupados por el hecho de que la amenaza de piratear con el aprendizaje autom√°tico, junto con las crecientes amenazas de violaci√≥n de la confidencialidad y la discriminaci√≥n algor√≠tmica, pueden aumentar el creciente escepticismo p√∫blico y pol√≠tico sobre el aprendizaje autom√°tico y la inteligencia artificial.  Todos necesitamos recordar los tiempos dif√≠ciles para la IA en el pasado reciente.  Las vulnerabilidades de seguridad, las infracciones de privacidad y la discriminaci√≥n algor√≠tmica podr√≠an combinarse, lo que llevar√≠a a una reducci√≥n de los fondos para la investigaci√≥n en capacitaci√≥n inform√°tica o a medidas draconianas para regular esta √°rea.  Continuemos la discusi√≥n y la resoluci√≥n de estos importantes asuntos para prevenir una crisis y no interrumpir sus consecuencias. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458892/">https://habr.com/ru/post/458892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458882/index.html">Hablar en p√∫blico Brevemente sobre lo principal</a></li>
<li><a href="../458884/index.html">Un poco sobre los est√°ndares de comunicaciones espaciales</a></li>
<li><a href="../458886/index.html">Los documentos m√°s √∫tiles de Mail.ru Design Conf √ó Dribbble Meetup 2019 de True Engineering</a></li>
<li><a href="../458888/index.html">Meetup de Summer Droid</a></li>
<li><a href="../458890/index.html">Muestreo y precisi√≥n de c√°lculo</a></li>
<li><a href="../458894/index.html">Las personas t√≠picas y las redes en las que viven.</a></li>
<li><a href="../458896/index.html">JavaScript funcional: ¬øqu√© son las funciones de orden superior y por qu√© son necesarias?</a></li>
<li><a href="../458900/index.html">Cartuchos de consola como m√≥dems</a></li>
<li><a href="../458902/index.html">5 errores comunes de Python para principiantes</a></li>
<li><a href="../458904/index.html">Visualizaci√≥n del n√∫mero de victorias para los equipos de la NBA utilizando gr√°ficos de barras animados en R</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>