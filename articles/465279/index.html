<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ü ü§ë üëÉüèª Reconocimiento facial con redes siamesas ü§æüèø üë®üèª üé∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La red neuronal siamesa es uno de los algoritmos de aprendizaje √∫nico m√°s simples y populares. M√©todos en los que para cada clase se toma solo un estu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconocimiento facial con redes siamesas</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/465279/"><img src="https://habrastorage.org/webt/mg/dh/mb/mgdhmb0xf-4onn6dny1renuuhla.jpeg"><br><br>  La red neuronal siamesa es uno de los algoritmos de aprendizaje √∫nico m√°s simples y populares.  M√©todos en los que para cada clase se toma solo un estudio de caso.  Por lo tanto, la red siamesa se usa generalmente en aplicaciones donde no hay muchas unidades de datos en cada clase. <br><br>  Supongamos que necesitamos hacer un modelo de reconocimiento facial para una organizaci√≥n que emplea a unas 500 personas.  Si hacemos un modelo desde cero basado en la red neuronal convolucional (CNN), para entrenar el modelo y lograr una buena precisi√≥n de reconocimiento, necesitaremos muchas im√°genes de cada una de estas 500 personas.  Pero es obvio que no podemos compilar dicho conjunto de datos, por lo que no debe hacer un modelo basado en CNN o cualquier otro algoritmo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprendizaje profundo</a> si no tenemos suficientes datos.  En tales casos, puede utilizar el complejo algoritmo de aprendizaje de una sola vez, como la red siamesa, que puede entrenarse con menos datos. <br><a name="habracut"></a><br>  De hecho, las redes siamesas consisten en dos redes neuronales sim√©tricas, con los mismos pesos y arquitectura, que al final combinan y usan la funci√≥n de energ√≠a - E. <br>  Veamos la red siamesa, creando un modelo de reconocimiento facial basado en ella.  Le ense√±aremos a determinar cu√°ndo dos caras son iguales y cu√°ndo no.  Y para empezar, utilizaremos el conjunto de datos AT&amp;T Database of Faces, que se puede descargar del sitio web del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">laboratorio de computaci√≥n de la Universidad de Cambridge</a> . <br><br>  Descargue, descomprima y vea carpetas de s1 a s40: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/152/0fc/158/1520fc1584e7ba6335ba2ea99460d8f6.png"><br><br>  Cada carpeta contiene 10 fotograf√≠as diferentes de una sola persona tomadas desde diferentes √°ngulos.  Aqu√≠ est√° el contenido de la carpeta s1: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d4/02b/86f/5d402b86fc12e1b23512b1a54c7fea57.png"><br><br>  Y esto es lo que hay en la carpeta s13: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/819/716/b7c8197164861f8568e86d32a2294bc2.png"><br><br>  Las redes siamesas necesitan ingresar valores emparejados con marcas, as√≠ que creemos tales conjuntos.  Tome dos fotos al azar de la misma carpeta y m√°rquelas como un par "genuino".  Luego tomamos dos fotos de diferentes carpetas y las marcamos como un par "falso" (falso): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b3/1f3/8bf/5b31f38bf3363aaa53110772b7643e3b.png"><br><br>  Habiendo distribuido todas las fotos en pares marcados, estudiaremos la red.  De cada par, transferiremos una foto a la red A, y la segunda a la red B. Ambas redes solo extraen vectores de propiedad.  Para hacer esto, utilizaremos dos capas convolucionales con activaci√≥n de unidad lineal rectificada (ReLU).  Despu√©s de estudiar las propiedades, transferimos los vectores generados por ambas redes a una funci√≥n de energ√≠a que estima la similitud.  Usamos la distancia euclidiana como una funci√≥n. <br><br>
<h2>  Ahora considere todos estos pasos con m√°s detalle. </h2><br>  Primero, importe las bibliotecas necesarias: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> PIL <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential, Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RMSprop</code> </pre> <br>  Ahora definimos una funci√≥n para leer im√°genes de entrada.  La funci√≥n <code>read_image</code> toma una imagen y devuelve una matriz NumPy: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, byteorder=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'&gt;'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#first we read the image, as a raw file to the buffer with open(filename, 'rb') as f: buffer = f.read() #using regex, we extract the header, width, height and maxval of the image header, width, height, maxval = re.search( b"(^P5\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n])*" b"(\d+)\s(?:\s*#.*[\r\n]\s)*)", buffer).groups() #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array return np.frombuffer(buffer, dtype='u1' if int(maxval)</span></span></code> </pre> <br>  Por ejemplo, abra esta foto: <br><br><pre> <code class="python hljs">Image.open(<span class="hljs-string"><span class="hljs-string">"data/orl_faces/s1/1.pgm"</span></span>)</code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/ee2/954/697/ee2954697908ce55ffaf4fc8080914c7.png"><br><br>  Lo pasamos a la funci√≥n <code>read_image</code> y obtenemos una matriz NumPy: <br><br><pre> <code class="python hljs">img = read_image(<span class="hljs-string"><span class="hljs-string">'data/orl_faces/s1/1.pgm'</span></span>) img.shape (<span class="hljs-number"><span class="hljs-number">112</span></span>, <span class="hljs-number"><span class="hljs-number">92</span></span>)</code> </pre> <br>  Ahora definimos la funci√≥n <code>get_data</code> que generar√° los datos.  Perm√≠tame recordarle que las redes siamesas deben enviar pares de datos (genuinos e imposibles) con marcado binario. <br><br>  Primero, lea las im√°genes ( <code>img1</code> , <code>img2</code> ) de un directorio, <code>x_genuine_pair,</code> en la matriz <code>x_genuine_pair,</code> establezca <code>y_genuine</code> en <code>1</code> .  Luego leemos las im√°genes ( <code>img1</code> , <code>img2</code> ) de diferentes directorios, las <code>x_imposite,</code> en el par <code>x_imposite,</code> y establecemos <code>y_imposite</code> en <code>0</code> . <br><br>  Concatenar <code>x_genuine_pair</code> y <code>x_imposite</code> en <code>X</code> , y <code>y_genuine</code> e <code>y_imposite</code> en <code>Y</code> : <br><br><pre> <code class="python hljs">size = <span class="hljs-number"><span class="hljs-number">2</span></span> total_sample_size = <span class="hljs-number"><span class="hljs-number">10000</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(size, total_sample_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#read the image image = read_image('data/orl_faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+') #reduce the size image = image[::size, ::size] #get the new size dim1 = image.shape[0] dim2 = image.shape[1] count = 0 #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2] x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) # 2 is for pairs y_genuine = np.zeros([total_sample_size, 1]) for i in range(40): for j in range(int(total_sample_size/40)): ind1 = 0 ind2 = 0 #read images from same directory (genuine pair) while ind1 == ind2: ind1 = np.random.randint(10) ind2 = np.random.randint(10) # read the two images img1 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+') #reduce the size img1 = img1[::size, ::size] img2 = img2[::size, ::size] #store the images to the initialized numpy array x_geuine_pair[count, 0, 0, :, :] = img1 x_geuine_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the same directory we assign label as 1. (genuine pair) y_genuine[count] = 1 count += 1 count = 0 x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) y_imposite = np.zeros([total_sample_size, 1]) for i in range(int(total_sample_size/10)): for j in range(10): #read images from different directory (imposite pair) while True: ind1 = np.random.randint(40) ind2 = np.random.randint(40) if ind1 != ind2: break img1 = read_image('data/orl_faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+') img2 = read_image('data/orl_faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+') img1 = img1[::size, ::size] img2 = img2[::size, ::size] x_imposite_pair[count, 0, 0, :, :] = img1 x_imposite_pair[count, 1, 0, :, :] = img2 #as we are drawing images from the different directory we assign label as 0. (imposite pair) y_imposite[count] = 0 count += 1 #now, concatenate, genuine pairs and imposite pair to get the whole data X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255 Y = np.concatenate([y_genuine, y_imposite], axis=0) return X, Y</span></span></code> </pre> <br>  Ahora generaremos los datos y verificaremos su tama√±o.  Tenemos 20,000 fotos, de las cuales se recolectaron 10,000 pares genuinos y 10,000 falsos: <br><br><pre> <code class="python hljs">X, Y = get_data(size, total_sample_size) X.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">46</span></span>) Y.shape (<span class="hljs-number"><span class="hljs-number">20000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Compartiremos toda la gama de informaci√≥n: el 75% de las parejas ir√°n a entrenamiento y el 25% a pruebas: <br> <code>x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)</code> <br> <br>  Ahora crea una red siamesa.  Primero definimos la red central: ser√° una red neuronal convolucional para extraer propiedades.  Cree dos capas convolucionales usando activaciones ReLU y una capa con la agrupaci√≥n m√°xima despu√©s de una capa plana: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_base_network</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape)</span></span></span><span class="hljs-function">:</span></span> seq = Sequential() nb_filter = [<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>] kernel_size = <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-comment"><span class="hljs-comment">#convolutional layer 1 seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2))) seq.add(Dropout(.25)) #convolutional layer 2 seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th')) seq.add(Activation('relu')) seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) seq.add(Dropout(.25)) #flatten seq.add(Flatten()) seq.add(Dense(128, activation='relu')) seq.add(Dropout(0.1)) seq.add(Dense(50, activation='relu')) return seq</span></span></code> </pre> <br><br>  Luego transferiremos un par de im√°genes de la red central, que devolver√°n representaciones vectoriales, es decir, vectores de propiedad: <br><br><pre> <code class="python hljs">input_dim = x_train.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>:] img_a = Input(shape=input_dim) img_b = Input(shape=input_dim) base_network = build_base_network(input_dim) feat_vecs_a = base_network(img_a) feat_vecs_b = base_network(img_b)</code> </pre><br>  <code>feat_vecs_a</code> y <code>feat_vecs_b</code> son vectores de propiedad de un par de im√°genes.  Pasemos sus funciones de energ√≠a para calcular la distancia entre ellos.  Y en funci√≥n de la energ√≠a, usamos la distancia euclidiana: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">euclidean_distance</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vects)</span></span></span><span class="hljs-function">:</span></span> x, y = vects <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.sqrt(K.sum(K.square(x - y), axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdims=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">eucl_dist_output_shape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(shapes)</span></span></span><span class="hljs-function">:</span></span> shape1, shape2 = shapes <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (shape1[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])</code> </pre> <br>  Establecemos el n√∫mero de √©pocas en 13, aplicamos la propiedad RMS para la optimizaci√≥n y declaramos el modelo: <br><br><pre> <code class="python hljs">epochs = <span class="hljs-number"><span class="hljs-number">13</span></span> rms = RMSprop() model = Model(input=[input_a, input_b], output=distance)</code> </pre> <br>  Ahora definimos la funci√≥n de p√©rdida <code>contrastive_loss</code> y compilamos el modelo: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">contrastive_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> margin = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(y_true * K.square(y_pred) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - y_true) * K.square(K.maximum(margin - y_pred, <span class="hljs-number"><span class="hljs-number">0</span></span>))) model.compile(loss=contrastive_loss, optimizer=rms)</code> </pre> <br>  Estudiemos el modelo: <br><br><pre> <code class="python hljs">img_1 = x_train[:, <span class="hljs-number"><span class="hljs-number">0</span></span>] img_2 = x_train[:, <span class="hljs-number"><span class="hljs-number">1</span></span>] model.fit([img_1, img_2], y_train, validation_split=<span class="hljs-number"><span class="hljs-number">.25</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>, nb_epoch=epochs)</code> </pre> <br>  Usted ve c√≥mo las p√©rdidas disminuyen a medida que pasan las eras: <br><br><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">11250</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">3750</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">60</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.2179</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2156</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1520</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2102</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">53</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1190</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1545</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">55</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0959</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1705</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0801</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1181</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0684</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0821</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0591</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0762</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0526</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0655</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0475</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0662</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0444</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0469</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0408</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0478</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">52</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0381</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0498</span></span> Epoch <span class="hljs-number"><span class="hljs-number">13</span></span>/<span class="hljs-number"><span class="hljs-number">13</span></span> - <span class="hljs-number"><span class="hljs-number">54</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0356</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0363</span></span></code> </pre> <br>  Y ahora probemos el modelo en los datos de prueba: <br><br><pre> <code class="python hljs">pred = model.predict([x_test[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], x_test[:, <span class="hljs-number"><span class="hljs-number">1</span></span>]])</code> </pre> <br>  Defina una funci√≥n para calcular la precisi√≥n: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compute_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(predictions, labels)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> labels[predictions.ravel()</code> </pre> <br>  Calculamos la precisi√≥n: <br><br><pre> <code class="plaintext hljs">compute_accuracy(pred, y_test) 0.9779092702169625</code> </pre><br><h2>  Conclusiones </h2><br>  En esta gu√≠a, aprendimos c√≥mo crear modelos de reconocimiento facial basados ‚Äã‚Äãen redes siamesas.  La arquitectura de tales redes consiste en dos redes neuronales id√©nticas que tienen el mismo peso y estructura, y los resultados de su trabajo se transfieren a una funci√≥n de energ√≠a: esto determina la identidad de los datos de entrada.  Para obtener m√°s informaci√≥n sobre el metaaprendizaje con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Python,</a> consulte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Meta-Learning</a> pr√°ctico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">con Python.</a> <br><br><h2>  Mi comentario </h2><br>  Actualmente se requiere conocimiento de las redes siamesas cuando se trabaja con im√°genes.  Existen muchos enfoques para capacitar redes en muestras peque√±as, nueva generaci√≥n de datos, m√©todos de aumento.  Este m√©todo permite obtener resultados relativamente ‚Äúbaratos‚Äù, aqu√≠ hay un ejemplo m√°s cl√°sico de la red siamesa en ‚ÄúHello world‚Äù para redes neuronales: conjunto de datos MNIST <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">keras.io/examples/mnist_siamese</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/465279/">https://habr.com/ru/post/465279/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../465269/index.html">Entrenamiento Cisco 200-125 CCNA v3.0. D√≠a 26. DNS y DHCP</a></li>
<li><a href="../465271/index.html">Los hackers roban y lavan dinero mediante la entrega de alimentos y los servicios de reserva de hotel.</a></li>
<li><a href="../465273/index.html">C√≥mo los desarrolladores de software de Microgaming protegen a los usuarios de los hacks</a></li>
<li><a href="../465275/index.html">Alice adquiere habilidad</a></li>
<li><a href="../465277/index.html">An√°lisis y an√°lisis de sem√°ntica para SEO: 5 plantillas gratuitas de Google Sheets</a></li>
<li><a href="../465281/index.html">Monitoreo continuo de glucosa (NMH) con bomba Medtronic 640g</a></li>
<li><a href="../465283/index.html">"Hay todo lo que se necesita y nada enfurece": la verdad habla a trav√©s de los labios del cliente</a></li>
<li><a href="../465285/index.html">Como escribimos, la interfaz de nuestro propio panel de control de hosting: framework y puertas traseras</a></li>
<li><a href="../465289/index.html">El resumen de eventos para profesionales de recursos humanos en el campo de TI para septiembre de 2019</a></li>
<li><a href="../465291/index.html">M√°s cerca del suelo: c√≥mo cambi√© el coworking a una casa de pueblo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>