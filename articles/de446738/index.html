<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîÄ üßîüèø üõÑ Grundlagen der Verarbeitung nat√ºrlicher Sprache f√ºr Text üë®‚Äçüë©‚Äçüëß üêõ üßïüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Verarbeitung nat√ºrlicher Sprache wird nur noch in sehr konservativen Bereichen verwendet. In den meisten technologischen L√∂sungen ist die Erkennun...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grundlagen der Verarbeitung nat√ºrlicher Sprache f√ºr Text</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/446738/"> Die Verarbeitung nat√ºrlicher Sprache wird nur noch in sehr konservativen Bereichen verwendet.  In den meisten technologischen L√∂sungen ist die Erkennung und Verarbeitung von ‚Äûmenschlichen‚Äú Sprachen seit langem eingef√ºhrt: Aus diesem Grund geh√∂rt die √ºbliche IVR mit fest codierten Antwortoptionen allm√§hlich der Vergangenheit an, Chatbots beginnen ohne die Teilnahme eines Live-Betreibers angemessener zu kommunizieren, Mail-Filter arbeiten mit einem Knall usw.  Wie ist die Erkennung von aufgezeichneter Sprache, dh Text?  Oder vielmehr, auf welcher Grundlage werden moderne Erkennungs- und Verarbeitungstechniken eingesetzt?  Unsere heutige angepasste √úbersetzung reagiert gut darauf - unter dem Schnitt finden Sie einen Longride, der die L√ºcken in den Grundlagen von NLP schlie√üt.  Viel Spa√ü beim Lesen! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nw/vz/qn/nwvzqnbpjgc_ndxnjt7eixdynro.jpeg"></div><br><a name="habracut"></a><br><h2>  Was ist die Verarbeitung nat√ºrlicher Sprache? </h2><br>  Verarbeitung nat√ºrlicher Sprache (im Folgenden: NLP) - Die Verarbeitung nat√ºrlicher Sprache ist ein Unterabschnitt der Informatik und KI, der sich mit der Analyse nat√ºrlicher (menschlicher) Sprachen durch Computer befasst.  NLP erm√∂glicht die Verwendung von Algorithmen f√ºr maschinelles Lernen f√ºr Text und Sprache. <br><br>  Zum Beispiel k√∂nnen wir NLP verwenden, um Systeme wie Spracherkennung, Verallgemeinerung von Dokumenten, maschinelle √úbersetzung, Spam-Erkennung, Erkennung benannter Entit√§ten, Antworten auf Fragen, automatische Vervollst√§ndigung, vorausschauende Texteingabe usw. zu erstellen. <br><br>  Heutzutage haben viele von uns Spracherkennungs-Smartphones - sie verwenden NLP, um unsere Sprache zu verstehen.  Au√üerdem verwenden viele Benutzer Laptops mit integrierter Spracherkennung im Betriebssystem. <br><br><h2>  Beispiele </h2><br><h3>  Cortana </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ui/aa/hy/uiaahycfbatakz2q9dl0fqfhr-y.png"></div><br><br>  Windows verf√ºgt √ºber einen virtuellen Cortana-Assistenten, der Sprache erkennt.  Mit Cortana k√∂nnen Sie Erinnerungen erstellen, Anwendungen √∂ffnen, Briefe senden, Spiele spielen, das Wetter herausfinden usw. <br><br><h3>  Siri </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ew/h7/9p/ewh79pl_rjkufl6seqyih-c_u4c.jpeg"></div><br>  Siri ist Assistent f√ºr Apples Betriebssystem: iOS, watchOS, macOS, HomePod und tvOS.  Viele Funktionen funktionieren auch √ºber die Sprachsteuerung: jemanden anrufen / schreiben, eine E-Mail senden, einen Timer einstellen, ein Foto aufnehmen usw. <br><br><h3>  Google Mail </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ec/rw/ii/ecrwii6nml6c6uvxn8vensihku0.gif"></div><br><br>  Ein bekannter E-Mail-Dienst erkennt Spam, damit er nicht in den Posteingang Ihres Posteingangs gelangt. <br><br><h3>  Dialogfluss </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mq/vg/iy/mqvgiyi8zv1dwfvwsqj6lvcfzsm.png"></div><br>  Eine Plattform von Google, mit der Sie NLP-Bots erstellen k√∂nnen.  Sie k√∂nnen beispielsweise einen Pizza-Bestellbot erstellen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, f√ºr dessen Annahme keine altmodische IVR erforderlich ist</a> . <br><br><hr><br><h2>  NLTK Python Library </h2><br>  NLTK (Natural Language Toolkit) ist eine f√ºhrende Plattform zum Erstellen von NLP-Programmen in Python.  Es verf√ºgt √ºber benutzerfreundliche Schnittstellen f√ºr viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachf√§lle</a> sowie Bibliotheken f√ºr die Textverarbeitung zur Klassifizierung, Tokenisierung, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stemming</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markup</a> , Filterung und zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">semantischen Denken</a> .  Nun, und dies ist ein kostenloses Open Source-Projekt, das mit Hilfe der Community entwickelt wird. <br>  Wir werden dieses Tool verwenden, um die Grundlagen von NLP zu zeigen.  F√ºr alle nachfolgenden Beispiele gehe ich davon aus, dass NLTK bereits importiert wurde.  Dies kann mit dem <code>import nltk</code> <br><br><h2>  NLP-Grundlagen f√ºr Text </h2><br>  In diesem Artikel werden wir folgende Themen behandeln: <br><br><ol><li>  Tokenisierung durch Angebote. </li><li>  Tokenisierung durch Worte. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lemmatisierung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stempelung des</a> Textes. </li><li>  H√∂r auf mit Worten. </li><li>  Regul√§re Ausdr√ºcke. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tasche voller W√∂rter</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-IDF</a> . </li></ol><br><h3>  1. Tokenisierung durch Angebote </h3><br>  Bei der Tokenisierung (manchmal Segmentierung) von S√§tzen wird eine geschriebene Sprache in Komponentens√§tze unterteilt.  Die Idee sieht ziemlich einfach aus.  In Englisch und einigen anderen Sprachen k√∂nnen wir einen Satz jedes Mal isolieren, wenn wir ein bestimmtes Interpunktionszeichen finden - einen Punkt. <br><br>  Aber auch auf Englisch ist diese Aufgabe nicht trivial, da der Punkt auch in Abk√ºrzungen verwendet wird.  Die Abk√ºrzungstabelle kann bei der Textverarbeitung sehr hilfreich sein, um zu vermeiden, dass Satzgrenzen falsch platziert werden.  In den meisten F√§llen werden daf√ºr Bibliotheken verwendet, sodass Sie sich nicht wirklich um Implementierungsdetails k√ºmmern m√ºssen. <br><br>  <b>Ein Beispiel:</b> <br><br>  Nehmen Sie einen kurzen Text √ºber das Backgammon-Brettspiel: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br>  Um die Tokenisierung von Angeboten mithilfe von NLTK <code>nltk.sent_tokenize</code> , k√∂nnen Sie die Methode <code>nltk.sent_tokenize</code> verwenden <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/39237759c087ac4151b3c06d4e566747.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92859547" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-sentence-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-sentence-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-sentence-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">text</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice."</span></td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-sentence-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentences</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">sent_tokenize</span>(<span class="pl-s1">text</span>)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-sentence-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">sentence</span> <span class="pl-c1">in</span> <span class="pl-s1">sentences</span>:</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-sentence-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-sentence-tokenization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">sentence tokenization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Am Ausgang erhalten wir 3 separate S√§tze: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br><h3>  2. Tokenisierung nach Worten </h3><br>  Tokenisierung (manchmal Segmentierung) nach W√∂rtern ist der Prozess des Teilens von S√§tzen in einzelne W√∂rter.  In Englisch und vielen anderen Sprachen, die die eine oder andere Version des lateinischen Alphabets verwenden, ist ein Leerzeichen ein gutes Worttrennzeichen. <br><br>  Probleme k√∂nnen jedoch auftreten, wenn wir nur ein Leerzeichen verwenden. Im Englischen werden zusammengesetzte Substantive unterschiedlich geschrieben und manchmal durch Leerzeichen getrennt.  Und hier helfen uns wieder Bibliotheken. <br><br>  <b>Ein Beispiel:</b> <br><br>  Nehmen wir die S√§tze aus dem vorherigen Beispiel und wenden die Methode <code>nltk.word_tokenize</code> auf sie an <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7befd293c570afd70158e954270fc98d.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92859647" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-word-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-word-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-word-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">sentence</span> <span class="pl-c1">in</span> <span class="pl-s1">sentences</span>:</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-word-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line">    <span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-word-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s1">words</span>)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-word-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">word tokenization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.'] ['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.'] ['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']</code> </pre> <br><h3>  3. Lemmatisierung und Stempelung des Textes </h3><br>  Normalerweise enthalten Texte unterschiedliche grammatikalische Formen desselben Wortes, und es k√∂nnen auch W√∂rter mit einer Wurzel vorkommen.  Lemmatisierung und Stemming zielen darauf ab, alle vorkommenden Wortformen zu einer einzigen, normalen Vokabularform zusammenzuf√ºhren. <br><br>  <b>Beispiele:</b> <br><br>  Verschiedene Wortformen zu einer bringen: <br><br><pre> <code class="plaintext hljs">dog, dogs, dog's, dogs' =&gt; dog</code> </pre> <br>  Das gleiche, aber mit Bezug auf den ganzen Satz: <br><br><pre> <code class="plaintext hljs">the boy's dogs are different sizes =&gt; the boy dog be differ size</code> </pre> <br>  Lemmatisierung und Stemming sind Sonderf√§lle der Normalisierung und unterscheiden sich. <br><br>  Stemming ist ein grober heuristischer Prozess, der ‚Äû√úbersch√ºsse‚Äú von der Wortwurzel abschneidet. Dies f√ºhrt h√§ufig zum Verlust von Suffixen zur Wortbildung. <br><br>  Die Lemmatisierung ist ein subtilerer Prozess, bei dem Vokabeln und morphologische Analysen verwendet werden, um das Wort letztendlich in seine kanonische Form zu bringen - das Lemma. <br><br>  Der Unterschied besteht darin, dass der Stemmer (eine spezifische Implementierung des Stemming-Algorithmus - √úbersetzerkommentar) ohne Kenntnis des Kontexts arbeitet und dementsprechend den Unterschied zwischen W√∂rtern, die je nach Wortart unterschiedliche Bedeutungen haben, nicht versteht.  Die Stemmers haben jedoch ihre eigenen Vorteile: Sie sind einfacher zu implementieren und arbeiten schneller.  Au√üerdem spielt eine geringere ‚ÄûGenauigkeit‚Äú in einigen F√§llen m√∂glicherweise keine Rolle. <br><br>  <b>Beispiele:</b> <br><br><ol><li>  Das Wort gut ist ein Lemma f√ºr das Wort besser.  Stemmer wird diese Verbindung nicht sehen, da Sie hier das W√∂rterbuch konsultieren m√ºssen. </li><li>  Das Wortspiel ist die Grundform des Wortspiels.  Hier werden sowohl Stemming als auch Lemmatisierung bew√§ltigt. </li><li>  Das Wort Treffen kann je nach Kontext entweder eine normale Form eines Substantivs oder eine Form des zu treffenden Verbs sein.  Im Gegensatz zum Stemming wird bei der Lemmatisierung versucht, das richtige Lemma basierend auf dem Kontext auszuw√§hlen. </li></ol><br>  Nachdem wir nun wissen, was der Unterschied ist, schauen wir uns ein Beispiel an: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/8c69db03e92337c9bc9a612361c9bcfb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92909693" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stemming-vs-lemmatization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stemming-vs-lemmatization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stemming-vs-lemmatization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">stem</span> <span class="pl-k">import</span> <span class="pl-v">PorterStemmer</span>, <span class="pl-v">WordNetLemmatizer</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stemming-vs-lemmatization-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">corpus</span> <span class="pl-k">import</span> <span class="pl-s1">wordnet</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stemming-vs-lemmatization-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stemming-vs-lemmatization-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-k">def</span> <span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span>, <span class="pl-s1">pos</span>):</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stemming-vs-lemmatization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-s">"""</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stemming-vs-lemmatization-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stemming-vs-lemmatization-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    """</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stemming-vs-lemmatization-py-LC8" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s">"Stemmer:"</span>, <span class="pl-s1">stemmer</span>.<span class="pl-en">stem</span>(<span class="pl-s1">word</span>))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stemming-vs-lemmatization-py-LC9" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s">"Lemmatizer:"</span>, <span class="pl-s1">lemmatizer</span>.<span class="pl-en">lemmatize</span>(<span class="pl-s1">word</span>, <span class="pl-s1">pos</span>))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stemming-vs-lemmatization-py-LC10" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-stemming-vs-lemmatization-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-stemming-vs-lemmatization-py-LC12" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">lemmatizer</span> <span class="pl-c1">=</span> <span class="pl-v">WordNetLemmatizer</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-stemming-vs-lemmatization-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stemmer</span> <span class="pl-c1">=</span> <span class="pl-v">PorterStemmer</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-stemming-vs-lemmatization-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span> <span class="pl-c1">=</span> <span class="pl-s">"seen"</span>, <span class="pl-s1">pos</span> <span class="pl-c1">=</span> <span class="pl-s1">wordnet</span>.<span class="pl-v">VERB</span>)</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-stemming-vs-lemmatization-py-LC15" class="blob-code blob-code-inner js-file-line"><span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span> <span class="pl-c1">=</span> <span class="pl-s">"drove"</span>, <span class="pl-s1">pos</span> <span class="pl-c1">=</span> <span class="pl-s1">wordnet</span>.<span class="pl-v">VERB</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stemming vs lemmatization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">Stemmer: seen Lemmatizer: see Stemmer: drove Lemmatizer: drive</code> </pre> <br><h3>  4. Stoppen Sie W√∂rter </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xr/fq/ju/xrfqju0nbugayjd8cnkvlcbiuwa.png"></div><br><br>  Stoppw√∂rter sind W√∂rter, die vor / nach der Textverarbeitung aus dem Text geworfen werden.  Wenn wir maschinelles Lernen auf Texte anwenden, k√∂nnen solche W√∂rter viel L√§rm verursachen, sodass Sie irrelevante W√∂rter entfernen m√ºssen. <br><br>  Stoppw√∂rter werden normalerweise von Artikeln, Interjektionen, Gewerkschaften usw. verstanden, die keine semantische Last tragen.  Es versteht sich, dass es keine universelle Liste von Stoppw√∂rtern gibt, alles h√§ngt vom jeweiligen Fall ab. <br><br>  NLTK verf√ºgt √ºber eine vordefinierte Liste von Stoppw√∂rtern.  Vor der ersten Verwendung m√ºssen Sie es herunterladen: <code>nltk.download(‚Äústopwords‚Äù)</code> .  Nach dem Herunterladen k√∂nnen Sie das <code>stopwords</code> importieren und die W√∂rter selbst <code>stopwords</code> : <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7d2e8f81219656f6d2e82933c6994cfe.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916250" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">corpus</span> <span class="pl-k">import</span> <span class="pl-s1">stopwords</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</code> </pre> <br>  √úberlegen Sie, wie Sie Stoppw√∂rter aus einem Satz entfernen k√∂nnen: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/b1c69457cc0d8eab7b3661533725485a.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916498" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-1-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-1-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-1-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stop_words</span> <span class="pl-c1">=</span> <span class="pl-en">set</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-1-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games."</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-1-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-1-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-1-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">without_stop_words</span> <span class="pl-c1">=</span> [<span class="pl-s1">word</span> <span class="pl-k">for</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">words</span> <span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">stop_words</span>]</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-1-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">without_stop_words</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 1.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']</code> </pre> <br>  Wenn Sie mit Listenverst√§ndnissen nicht vertraut sind, erfahren Sie hier mehr.  Hier ist ein anderer Weg, um das gleiche Ergebnis zu erzielen: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/bbfab6573e886bd122aba972048d54cb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916520" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-2-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-2-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-2-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stop_words</span> <span class="pl-c1">=</span> <span class="pl-en">set</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-2-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games."</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-2-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-2-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-2-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">without_stop_words</span> <span class="pl-c1">=</span> []</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-2-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">words</span>:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stop-words-example-2-py-LC7" class="blob-code blob-code-inner js-file-line">    <span class="pl-k">if</span> <span class="pl-s1">word</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">stop_words</span>:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stop-words-example-2-py-LC8" class="blob-code blob-code-inner js-file-line">        <span class="pl-s1">without_stop_words</span>.<span class="pl-en">append</span>(<span class="pl-s1">word</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stop-words-example-2-py-LC9" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stop-words-example-2-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">without_stop_words</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 2.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Denken Sie jedoch daran, dass das Listenverst√§ndnis schneller ist, weil es optimiert ist. Der Interpreter zeigt w√§hrend der Schleife ein Vorhersagemuster an. <br><br>  Sie fragen sich vielleicht, warum wir die Liste in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele</a> konvertiert haben.  Ein Satz ist ein abstrakter Datentyp, der eindeutige Werte in einer undefinierten Reihenfolge speichern kann.  Die Suche nach Set ist viel schneller als das Durchsuchen einer Liste.  F√ºr eine kleine Anzahl von W√∂rtern spielt dies keine Rolle, aber wenn wir √ºber eine gro√üe Anzahl von W√∂rtern sprechen, wird die Verwendung von Mengen dringend empfohlen.  Wenn Sie mehr √ºber die Zeit erfahren m√∂chten, die f√ºr die Durchf√ºhrung verschiedener Operationen erforderlich ist, sehen Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen wunderbaren Spickzettel an</a> . <br><br><h3>  5. Regul√§re Ausdr√ºcke. </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/mm/jx/hnmmjxjubpvt7p1uc-lv-t-auhi.jpeg"></div><br>  Ein regul√§rer Ausdruck (Regex, Regexp, Regex) ist eine Folge von Zeichen, die ein Suchmuster definiert.  Zum Beispiel: <br><br><ul><li>  .  - jedes Zeichen au√üer Zeilenvorschub; </li><li>  \ w ist ein Wort; </li><li>  \ d - eine Ziffer; </li><li>  \ s - ein Leerzeichen; </li><li>  \ W ist ein NICHT-Wort; </li><li>  \ D - eine nichtstellige Zahl; </li><li>  \ S - ein Nicht-Leerzeichen; </li><li>  [abc] - stellt fest, dass eines der angegebenen Zeichen mit a, b oder c √ºbereinstimmt; </li><li>  [^ abc] - findet ein beliebiges Zeichen au√üer den angegebenen; </li><li>  [ag] - Findet ein Zeichen im Bereich von a bis g. </li></ul><br>  Auszug aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Dokumentation</a> : <br><blockquote>  Regul√§re Ausdr√ºcke verwenden den Backslash <code>(\)</code> , um Sonderformen anzugeben oder die Verwendung von Sonderzeichen zu erm√∂glichen.  Dies widerspricht der Verwendung des Backslash in Python: <code>'\\\\'</code> beispielsweise den Backslash w√∂rtlich zu bezeichnen, m√ºssen Sie <code>'\\\\'</code> als Suchmuster schreiben, da der regul√§re Ausdruck wie <code>\\</code> aussehen sollte, wobei jeder Backslash maskiert werden muss. <br><br>  Die L√∂sung besteht darin, die rohe Zeichenfolgennotation f√ºr Suchmuster zu verwenden.  Backslashes werden nicht speziell verarbeitet, wenn sie mit dem Pr√§fix <code>'r'</code> .  Somit ist <code>r‚Äù\n‚Äù</code> eine Zeichenfolge mit zwei Zeichen <code>('\'  'n')</code> , und <code>‚Äú\n‚Äù</code> ist eine Zeichenfolge mit einem Zeichen (Zeilenvorschub). <br></blockquote>  Wir k√∂nnen Stammg√§ste verwenden, um unseren Text weiter zu filtern.  Sie k√∂nnen beispielsweise alle Zeichen entfernen, die keine W√∂rter sind.  In vielen F√§llen ist keine Interpunktion erforderlich und kann mithilfe von Stammg√§sten leicht entfernt werden. <br><br>  Das Modul <b>re</b> in Python repr√§sentiert Operationen mit regul√§ren Ausdr√ºcken.  Wir k√∂nnen die Funktion <b>re.sub</b> verwenden, um alles, was zum Suchmuster passt, durch die angegebene Zeichenfolge zu ersetzen.  So k√∂nnen Sie alle Nichtw√∂rter durch Leerzeichen ersetzen: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/a9a29588061a8c9bb8aeb28140a69f89.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92925716" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-regex-substitute-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-regex-substitute-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-regex-substitute-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">re</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-regex-substitute-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-regex-substitute-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pattern</span> <span class="pl-c1">=</span> <span class="pl-s">r"[^\w]"</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-regex-substitute-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">re</span>.<span class="pl-en">sub</span>(<span class="pl-s1">pattern</span>, <span class="pl-s">" "</span>, <span class="pl-s1">sentence</span>))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">regex substitute.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">'The development of snowboarding was inspired by skateboarding sledding surfing and skiing '</code> </pre> <br>  Stammg√§ste sind ein leistungsstarkes Werkzeug, mit dem viel komplexere Muster erstellt werden k√∂nnen.  Wenn Sie mehr √ºber regul√§re Ausdr√ºcke erfahren m√∂chten, kann ich diese beiden Webanwendungen empfehlen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regex</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regex101</a> . <br><br><h3>  6. Tasche voller W√∂rter </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oh/va/sp/ohvaspwwyxr6mjgfz1w4vq8znku.png"></div><br>  Algorithmen f√ºr maschinelles Lernen k√∂nnen nicht direkt mit Rohtext arbeiten, daher m√ºssen Sie den Text in Zahlenmengen (Vektoren) konvertieren.  Dies wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Merkmalsextraktion bezeichnet</a> . <br><br>  Eine Worttasche ist eine beliebte und einfache Funktion zum Extrahieren von Funktionen, die beim Arbeiten mit Text verwendet wird.  Es beschreibt das Vorkommen jedes Wortes im Text. <br><br>  Um das Modell verwenden zu k√∂nnen, ben√∂tigen wir: <br><br><ol><li>  Definieren Sie ein W√∂rterbuch bekannter W√∂rter (Token). </li><li>  W√§hlen Sie den Grad der Pr√§senz ber√ºhmter W√∂rter. </li></ol><br>  Alle Informationen √ºber die Reihenfolge oder Struktur von W√∂rtern werden ignoriert.  Deshalb wird es eine TASCHE mit W√∂rtern genannt.  Dieses Modell versucht zu verstehen, ob ein bekanntes Wort in einem Dokument vorkommt, wei√ü jedoch nicht, wo genau es vorkommt. <br><br>  Die Intuition legt nahe, dass <b>√§hnliche Dokumente</b> einen <b>√§hnlichen Inhalt haben</b> .  Dank des Inhalts k√∂nnen wir auch etwas √ºber die Bedeutung des Dokuments lernen. <br><br>  <b>Ein Beispiel:</b> <br>  Beachten Sie die Schritte zum Erstellen dieses Modells.  Wir verwenden nur 4 S√§tze, um zu verstehen, wie das Modell funktioniert.  Im wirklichen Leben werden Sie auf mehr Daten sto√üen. <br><br><h4>  1. Daten herunterladen </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/xz/w9/trxzw9m1s7psallg0ulf6wepnsu.png"></div><br>  Stellen Sie sich vor, dies sind unsere Daten und wir m√∂chten sie als Array laden: <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  Lesen Sie dazu einfach die Datei und teilen Sie sie durch die Zeile: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/15abcc02fefb2782ba78ac695d4dda59.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93035402" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-read-the-movie-reviews-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-read-the-movie-reviews-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-read-the-movie-reviews-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s">"simple movie reviews.txt"</span>, <span class="pl-s">"r"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-read-the-movie-reviews-py-LC2" class="blob-code blob-code-inner js-file-line">    <span class="pl-s1">documents</span> <span class="pl-c1">=</span> <span class="pl-s1">file</span>.<span class="pl-en">read</span>().<span class="pl-en">splitlines</span>()</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-read-the-movie-reviews-py-LC3" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-read-the-movie-reviews-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">read the movie reviews.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']</code> </pre> <br><br><h4>  2. Definieren Sie ein W√∂rterbuch </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/vg/7p/lkvg7pwbgfcd130zn66zx_qhjxq.png"></div><br>  Wir werden alle eindeutigen W√∂rter aus 4 geladenen S√§tzen sammeln, wobei Gro√ü- und Kleinschreibung, Interpunktion und Token mit einem Zeichen ignoriert werden.  Dies wird unser W√∂rterbuch sein (ber√ºhmte W√∂rter). <br><br>  Um ein W√∂rterbuch zu erstellen, k√∂nnen Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CountVectorizer-</a> Klasse aus der sklearn-Bibliothek verwenden.  Fahren Sie mit dem n√§chsten Schritt fort. <br><br><h4>  3. Erstellen Sie Dokumentvektoren </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4q/jj/hh/4qjjhhlyqga--r6eh9kkery5jsy.png"></div><br>  Als n√§chstes m√ºssen wir die W√∂rter im Dokument bewerten.  In diesem Schritt ist es unser Ziel, Rohtext in eine Reihe von Zahlen umzuwandeln.  Danach verwenden wir diese Mengen als Eingabe f√ºr das Modell des maschinellen Lernens.  Die einfachste Bewertungsmethode besteht darin, das Vorhandensein von W√∂rtern zu notieren, dh 1, wenn ein Wort vorhanden ist, und 0, wenn es nicht vorhanden ist. <br><br>  Jetzt k√∂nnen wir mit der oben genannten CountVectorizer-Klasse eine Wortsammlung erstellen. <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/155e97ad22862a340d941a63e43295d9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93036006" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-simple-bag-of-words-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-simple-bag-of-words-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-simple-bag-of-words-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Import the libraries we need</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-simple-bag-of-words-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">feature_extraction</span>.<span class="pl-s1">text</span> <span class="pl-k">import</span> <span class="pl-v">CountVectorizer</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-simple-bag-of-words-example-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-simple-bag-of-words-example-py-LC4" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-simple-bag-of-words-example-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Step 2. Design the Vocabulary</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-simple-bag-of-words-example-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># The default token pattern removes tokens of a single character. That's why we don't have the "I" and "s" tokens in the output</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-simple-bag-of-words-example-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">count_vectorizer</span> <span class="pl-c1">=</span> <span class="pl-v">CountVectorizer</span>()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-simple-bag-of-words-example-py-LC8" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-simple-bag-of-words-example-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Step 3. Create the Bag-of-Words Model</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-simple-bag-of-words-example-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">bag_of_words</span> <span class="pl-c1">=</span> <span class="pl-s1">count_vectorizer</span>.<span class="pl-en">fit_transform</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-simple-bag-of-words-example-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-simple-bag-of-words-example-py-LC12" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Show the Bag-of-Words Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-simple-bag-of-words-example-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">feature_names</span> <span class="pl-c1">=</span> <span class="pl-s1">count_vectorizer</span>.<span class="pl-en">get_feature_names</span>()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-simple-bag-of-words-example-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pd</span>.<span class="pl-v">DataFrame</span>(<span class="pl-s1">bag_of_words</span>.<span class="pl-en">toarray</span>(), <span class="pl-s1">columns</span> <span class="pl-c1">=</span> <span class="pl-s1">feature_names</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">simple bag-of-words example.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ee/pf/-g/eepf-gw0it8d6a_fwcoew7bwbns.png"></div><br>  Dies sind unsere Vorschl√§ge.  Jetzt sehen wir, wie das Modell ‚ÄûBag of Words‚Äú funktioniert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/l3/ep/95/l3ep95r4s_rdnvjfmfy7ebcwuag.png"></div><br><h3>  Ein paar Worte √ºber die Wortsammlung </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k4/mo/s1/k4mos1faome-c00hjo84v7nt7no.png"></div><br>  Die Komplexit√§t dieses Modells besteht darin, wie das W√∂rterbuch bestimmt und das Auftreten von W√∂rtern gez√§hlt wird. <br><br>  Wenn die W√∂rterbuchgr√∂√üe zunimmt, w√§chst auch der Dokumentvektor.  Im obigen Beispiel ist die L√§nge des Vektors gleich der Anzahl bekannter W√∂rter. <br><br>  In einigen F√§llen k√∂nnen wir eine unglaublich gro√üe Datenmenge haben, und dann kann der Vektor aus Tausenden oder Millionen von Elementen bestehen.  Dar√ºber hinaus kann jedes Dokument nur einen kleinen Teil der W√∂rter aus dem W√∂rterbuch enthalten. <br><br>  Infolgedessen enth√§lt die Vektordarstellung viele Nullen.  Vektoren mit vielen Nullen werden als sp√§rliche Vektoren bezeichnet. Sie erfordern mehr Speicher und Rechenressourcen. <br><br>  Wir k√∂nnen jedoch die Anzahl der bekannten W√∂rter reduzieren, wenn wir dieses Modell verwenden, um die Anforderungen an die Rechenressourcen zu verringern.  Dazu k√∂nnen Sie dieselben Techniken verwenden, die wir bereits vor dem Erstellen einer Worttasche in Betracht gezogen haben: <br><br><ul><li>  den Fall von W√∂rtern ignorieren; </li><li>  Interpunktion ignorieren; </li><li>  Stoppw√∂rter auswerfen; </li><li>  Reduktion von W√∂rtern auf ihre Grundformen (Lemmatisierung und Stemming); </li><li>  Korrektur falsch geschriebener W√∂rter. </li></ul><br>  Eine andere, kompliziertere M√∂glichkeit, ein W√∂rterbuch zu erstellen, besteht darin, gruppierte W√∂rter zu verwenden.  Dadurch wird die Gr√∂√üe des W√∂rterbuchs ge√§ndert und der Worttasche mehr Details zum Dokument hinzugef√ºgt.  Dieser Ansatz wird als " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">N-Gramm</a> " bezeichnet. <br><br>  N-Gramm ist eine Folge beliebiger Entit√§ten (W√∂rter, Buchstaben, Zahlen, Zahlen usw.).  Im Kontext von Sprachk√∂rpern wird das N-Gramm normalerweise als eine Folge von W√∂rtern verstanden.  Ein Unigramm ist ein Wort, ein Bigram ist eine Folge von zwei W√∂rtern, ein Trigramm ist drei W√∂rter und so weiter.  Die Zahl N gibt an, wie viele gruppierte W√∂rter im N-Gramm enthalten sind.  Nicht alle m√∂glichen N-Gramm fallen in das Modell, sondern nur die, die im Fall erscheinen. <br><br>  <b>Ein Beispiel:</b> <br><br>  Betrachten Sie den folgenden Satz: <br><br><pre> <code class="plaintext hljs">The office building is open today</code> </pre> <br>  Hier sind seine Bigramme: <br><br><ul><li>  das B√ºro </li><li>  B√ºrogeb√§ude </li><li>  Geb√§ude ist </li><li>  ist offen </li><li>  heute ge√∂ffnet </li></ul><br>  Wie Sie sehen k√∂nnen, ist eine T√ºte Bigrams ein effektiverer Ansatz als eine T√ºte W√∂rter. <br><br>  <b>Bewertung (Bewertung) von W√∂rtern</b> <br><br>  Wenn ein W√∂rterbuch erstellt wird, sollte das Vorhandensein von W√∂rtern bewertet werden.  Wir haben bereits einen einfachen bin√§ren Ansatz in Betracht gezogen (1 - es gibt ein Wort, 0 - es gibt kein Wort). <br><br>  Es gibt andere Methoden: <br><br><ol><li>  Menge.  Es wird berechnet, wie oft jedes Wort im Dokument erscheint. </li><li>  Frequenz  Es wird berechnet, wie oft jedes Wort im Text vorkommt (bezogen auf die Gesamtzahl der W√∂rter). </li></ol><br><br><h3>  7. TF-IDF </h3><br>  Die Frequenzbewertung hat ein Problem: W√∂rter mit der h√∂chsten Frequenz haben jeweils die h√∂chste Bewertung.  In diesen Worten gibt es m√∂glicherweise nicht so viel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationsgewinn</a> f√ºr das Modell wie in weniger h√§ufigen Worten.  Eine M√∂glichkeit, die Situation zu korrigieren, besteht darin, die Wortbewertung zu senken, die h√§ufig <b>in allen √§hnlichen Dokumenten zu finden ist</b> .  Dies wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-IDF bezeichnet</a> . <br><br>  TF-IDF (kurz f√ºr Term Frequency - Inverse Document Frequency) ist ein statistisches Ma√ü zur Bewertung der Wichtigkeit eines Wortes in einem Dokument, das Teil einer Sammlung oder eines Korpus ist. <br><br>  Die Bewertung durch TF-IDF w√§chst proportional zur H√§ufigkeit des Auftretens eines Wortes in einem Dokument. Dies wird jedoch durch die Anzahl der Dokumente ausgeglichen, die dieses Wort enthalten. <br><br>  Bewertungsformel f√ºr das Wort X in Dokument Y: <br><br><img src="https://habrastorage.org/webt/_3/bb/xo/_3bbxoimlox11_am3gzyequcjic.png"><br>  <font color="grey">Formel TF-IDF.</font>  <font color="grey">Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html</a></font> <br><br>  TF (Termh√§ufigkeit) ist das Verh√§ltnis der Anzahl der Vorkommen eines Wortes zur Gesamtzahl der W√∂rter in einem Dokument. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/p0/wk/aip0wkqcynj8q1cxwxlufspqqds.png"></div><br>  IDF (inverse Dokumenth√§ufigkeit) ist die Umkehrung der H√§ufigkeit, mit der ein Wort in Sammlungsdokumenten vorkommt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6j/xd/32/6jxd32ydlpkmixkjw6hdgmp6f6m.png"></div><br>  Infolgedessen kann TF-IDF f√ºr den Wortterm wie folgt berechnet werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hl/tp/n0/hltpn0vg_gdo8bn1pfimbvu60no.png"></div><br>  <b>Ein Beispiel:</b> <br><br>  Sie k√∂nnen die <b>TfidfVectorizer-</b> Klasse aus der sklearn-Bibliothek verwenden, um TF-IDF zu berechnen.  Lassen Sie uns dies mit denselben Nachrichten tun, die wir im Beispiel f√ºr die Worttasche verwendet haben. <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  Code: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/c84cfc6fef2dc131236a9fa5c72de3c9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93050848" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-tf-idf-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-tf-idf-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-tf-idf-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">feature_extraction</span>.<span class="pl-s1">text</span> <span class="pl-k">import</span> <span class="pl-v">TfidfVectorizer</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-tf-idf-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-tf-idf-example-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-tf-idf-example-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">tfidf_vectorizer</span> <span class="pl-c1">=</span> <span class="pl-v">TfidfVectorizer</span>()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-tf-idf-example-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">values</span> <span class="pl-c1">=</span> <span class="pl-s1">tfidf_vectorizer</span>.<span class="pl-en">fit_transform</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-tf-idf-example-py-LC6" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-tf-idf-example-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Show the Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-tf-idf-example-py-LC8" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">feature_names</span> <span class="pl-c1">=</span> <span class="pl-s1">tfidf_vectorizer</span>.<span class="pl-en">get_feature_names</span>()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-tf-idf-example-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pd</span>.<span class="pl-v">DataFrame</span>(<span class="pl-s1">values</span>.<span class="pl-en">toarray</span>(), <span class="pl-s1">columns</span> <span class="pl-c1">=</span> <span class="pl-s1">feature_names</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">tf-idf example.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ad/vj/kt/advjktxg44hyhjj63m27igwssiu.png"></div><br><h2>  Fazit </h2><br>  Dieser Artikel hat die Grundlagen von NLP f√ºr Text behandelt, n√§mlich: <br><br><ul><li>  NLP erm√∂glicht die Verwendung von Algorithmen f√ºr maschinelles Lernen f√ºr Text und Sprache. </li><li>  NLTK (Natural Language Toolkit) - eine f√ºhrende Plattform zum Erstellen von NLP-Programmen in Python; </li><li>  Bei der Tokenisierung von Vorschl√§gen wird eine geschriebene Sprache in einzelne S√§tze unterteilt. </li><li>  Wort-Tokenisierung ist der Prozess des Teilens von S√§tzen in einzelne W√∂rter; </li><li>  Lemmatisierung und Stemming zielen darauf ab, alle Wortformen zu einer einzigen, normalen Vokabularform zusammenzuf√ºhren. </li><li>  Stoppw√∂rter sind W√∂rter, die vor / nach der Textverarbeitung aus dem Text geworfen werden. </li><li>  Regex (Regex, Regexp, Regex) ist eine Folge von Zeichen, die ein Suchmuster definiert. </li><li>  Eine T√ºte mit W√∂rtern ist eine beliebte und einfache Technik zum Extrahieren von Merkmalen, die beim Arbeiten mit Text verwendet wird.  Es beschreibt das Vorkommen jedes Wortes im Text. </li></ul><br>  Gro√üartig!  Nachdem Sie die Grundlagen der Merkmalsextraktion kennen, k√∂nnen Sie Merkmale als Eingabe f√ºr Algorithmen f√ºr maschinelles Lernen verwenden. <br><br>  Wenn Sie alle beschriebenen Konzepte in einem gro√üen Beispiel sehen m√∂chten, dann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sind Sie hier</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446738/">https://habr.com/ru/post/de446738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446726/index.html">Deep Learning in der optischen Flussberechnung</a></li>
<li><a href="../de446728/index.html">Wie sich die vom kabellosen Laden empfangene Leistung √§ndert, h√§ngt vom Standort des Telefons ab</a></li>
<li><a href="../de446730/index.html">Backend-Bereich zu DUMP: Serverless, Postgres and Go, .NET Core, GraphQL und mehr</a></li>
<li><a href="../de446732/index.html">Feropoden werden nicht helfen: Forschung und mathematische Modellierung von Grubenfallen f√ºr Ameisenl√∂wenlarven</a></li>
<li><a href="../de446736/index.html">Oracle APEX Berichte</a></li>
<li><a href="../de446740/index.html">Verwenden von Python f√ºr die Berichterstellung in einem einzelnen Unternehmen</a></li>
<li><a href="../de446742/index.html">Themen der Top 3D Expo 2019: ‚ÄûAnisoprinting - die Technologie zur Herstellung von Verbundstrukturen einer neuen Generation‚Äú, Fedor Antonov</a></li>
<li><a href="../de446744/index.html">VR mit neuronalen Schnittstellen - ein vollst√§ndiges Eintauchen in die virtuelle Realit√§t</a></li>
<li><a href="../de446746/index.html">Ein UBS-Mitarbeiter hat ein Gespr√§ch √ºber einen Nachbarn des Eurostar-Zuges mitgeh√∂rt und einen Deal √ºber 15 Milliarden US-Dollar herausgefunden. Jetzt werden er und die Bank mit einer Geldstrafe belegt</a></li>
<li><a href="../de446750/index.html">Nachrichten von unten: IT-Giganten haben begonnen, aktiv ihre eigenen U-Boot-Backbone-Netzwerke aufzubauen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>