<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔀 🧔🏿 🛄 Grundlagen der Verarbeitung natürlicher Sprache für Text 👨‍👩‍👧 🐛 🧕🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Verarbeitung natürlicher Sprache wird nur noch in sehr konservativen Bereichen verwendet. In den meisten technologischen Lösungen ist die Erkennun...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grundlagen der Verarbeitung natürlicher Sprache für Text</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/446738/"> Die Verarbeitung natürlicher Sprache wird nur noch in sehr konservativen Bereichen verwendet.  In den meisten technologischen Lösungen ist die Erkennung und Verarbeitung von „menschlichen“ Sprachen seit langem eingeführt: Aus diesem Grund gehört die übliche IVR mit fest codierten Antwortoptionen allmählich der Vergangenheit an, Chatbots beginnen ohne die Teilnahme eines Live-Betreibers angemessener zu kommunizieren, Mail-Filter arbeiten mit einem Knall usw.  Wie ist die Erkennung von aufgezeichneter Sprache, dh Text?  Oder vielmehr, auf welcher Grundlage werden moderne Erkennungs- und Verarbeitungstechniken eingesetzt?  Unsere heutige angepasste Übersetzung reagiert gut darauf - unter dem Schnitt finden Sie einen Longride, der die Lücken in den Grundlagen von NLP schließt.  Viel Spaß beim Lesen! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nw/vz/qn/nwvzqnbpjgc_ndxnjt7eixdynro.jpeg"></div><br><a name="habracut"></a><br><h2>  Was ist die Verarbeitung natürlicher Sprache? </h2><br>  Verarbeitung natürlicher Sprache (im Folgenden: NLP) - Die Verarbeitung natürlicher Sprache ist ein Unterabschnitt der Informatik und KI, der sich mit der Analyse natürlicher (menschlicher) Sprachen durch Computer befasst.  NLP ermöglicht die Verwendung von Algorithmen für maschinelles Lernen für Text und Sprache. <br><br>  Zum Beispiel können wir NLP verwenden, um Systeme wie Spracherkennung, Verallgemeinerung von Dokumenten, maschinelle Übersetzung, Spam-Erkennung, Erkennung benannter Entitäten, Antworten auf Fragen, automatische Vervollständigung, vorausschauende Texteingabe usw. zu erstellen. <br><br>  Heutzutage haben viele von uns Spracherkennungs-Smartphones - sie verwenden NLP, um unsere Sprache zu verstehen.  Außerdem verwenden viele Benutzer Laptops mit integrierter Spracherkennung im Betriebssystem. <br><br><h2>  Beispiele </h2><br><h3>  Cortana </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ui/aa/hy/uiaahycfbatakz2q9dl0fqfhr-y.png"></div><br><br>  Windows verfügt über einen virtuellen Cortana-Assistenten, der Sprache erkennt.  Mit Cortana können Sie Erinnerungen erstellen, Anwendungen öffnen, Briefe senden, Spiele spielen, das Wetter herausfinden usw. <br><br><h3>  Siri </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ew/h7/9p/ewh79pl_rjkufl6seqyih-c_u4c.jpeg"></div><br>  Siri ist Assistent für Apples Betriebssystem: iOS, watchOS, macOS, HomePod und tvOS.  Viele Funktionen funktionieren auch über die Sprachsteuerung: jemanden anrufen / schreiben, eine E-Mail senden, einen Timer einstellen, ein Foto aufnehmen usw. <br><br><h3>  Google Mail </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ec/rw/ii/ecrwii6nml6c6uvxn8vensihku0.gif"></div><br><br>  Ein bekannter E-Mail-Dienst erkennt Spam, damit er nicht in den Posteingang Ihres Posteingangs gelangt. <br><br><h3>  Dialogfluss </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mq/vg/iy/mqvgiyi8zv1dwfvwsqj6lvcfzsm.png"></div><br>  Eine Plattform von Google, mit der Sie NLP-Bots erstellen können.  Sie können beispielsweise einen Pizza-Bestellbot erstellen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, für dessen Annahme keine altmodische IVR erforderlich ist</a> . <br><br><hr><br><h2>  NLTK Python Library </h2><br>  NLTK (Natural Language Toolkit) ist eine führende Plattform zum Erstellen von NLP-Programmen in Python.  Es verfügt über benutzerfreundliche Schnittstellen für viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachfälle</a> sowie Bibliotheken für die Textverarbeitung zur Klassifizierung, Tokenisierung, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stemming</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markup</a> , Filterung und zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">semantischen Denken</a> .  Nun, und dies ist ein kostenloses Open Source-Projekt, das mit Hilfe der Community entwickelt wird. <br>  Wir werden dieses Tool verwenden, um die Grundlagen von NLP zu zeigen.  Für alle nachfolgenden Beispiele gehe ich davon aus, dass NLTK bereits importiert wurde.  Dies kann mit dem <code>import nltk</code> <br><br><h2>  NLP-Grundlagen für Text </h2><br>  In diesem Artikel werden wir folgende Themen behandeln: <br><br><ol><li>  Tokenisierung durch Angebote. </li><li>  Tokenisierung durch Worte. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lemmatisierung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stempelung des</a> Textes. </li><li>  Hör auf mit Worten. </li><li>  Reguläre Ausdrücke. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tasche voller Wörter</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-IDF</a> . </li></ol><br><h3>  1. Tokenisierung durch Angebote </h3><br>  Bei der Tokenisierung (manchmal Segmentierung) von Sätzen wird eine geschriebene Sprache in Komponentensätze unterteilt.  Die Idee sieht ziemlich einfach aus.  In Englisch und einigen anderen Sprachen können wir einen Satz jedes Mal isolieren, wenn wir ein bestimmtes Interpunktionszeichen finden - einen Punkt. <br><br>  Aber auch auf Englisch ist diese Aufgabe nicht trivial, da der Punkt auch in Abkürzungen verwendet wird.  Die Abkürzungstabelle kann bei der Textverarbeitung sehr hilfreich sein, um zu vermeiden, dass Satzgrenzen falsch platziert werden.  In den meisten Fällen werden dafür Bibliotheken verwendet, sodass Sie sich nicht wirklich um Implementierungsdetails kümmern müssen. <br><br>  <b>Ein Beispiel:</b> <br><br>  Nehmen Sie einen kurzen Text über das Backgammon-Brettspiel: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br>  Um die Tokenisierung von Angeboten mithilfe von NLTK <code>nltk.sent_tokenize</code> , können Sie die Methode <code>nltk.sent_tokenize</code> verwenden <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/39237759c087ac4151b3c06d4e566747.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92859547" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-sentence-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-sentence-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-sentence-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">text</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice."</span></td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-sentence-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentences</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">sent_tokenize</span>(<span class="pl-s1">text</span>)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-sentence-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">sentence</span> <span class="pl-c1">in</span> <span class="pl-s1">sentences</span>:</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-sentence-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-sentence-tokenization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">sentence tokenization.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Am Ausgang erhalten wir 3 separate Sätze: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br><h3>  2. Tokenisierung nach Worten </h3><br>  Tokenisierung (manchmal Segmentierung) nach Wörtern ist der Prozess des Teilens von Sätzen in einzelne Wörter.  In Englisch und vielen anderen Sprachen, die die eine oder andere Version des lateinischen Alphabets verwenden, ist ein Leerzeichen ein gutes Worttrennzeichen. <br><br>  Probleme können jedoch auftreten, wenn wir nur ein Leerzeichen verwenden. Im Englischen werden zusammengesetzte Substantive unterschiedlich geschrieben und manchmal durch Leerzeichen getrennt.  Und hier helfen uns wieder Bibliotheken. <br><br>  <b>Ein Beispiel:</b> <br><br>  Nehmen wir die Sätze aus dem vorherigen Beispiel und wenden die Methode <code>nltk.word_tokenize</code> auf sie an <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7befd293c570afd70158e954270fc98d.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92859647" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-word-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-word-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-word-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">sentence</span> <span class="pl-c1">in</span> <span class="pl-s1">sentences</span>:</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-word-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line">    <span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-word-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s1">words</span>)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-word-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">word tokenization.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.'] ['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.'] ['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']</code> </pre> <br><h3>  3. Lemmatisierung und Stempelung des Textes </h3><br>  Normalerweise enthalten Texte unterschiedliche grammatikalische Formen desselben Wortes, und es können auch Wörter mit einer Wurzel vorkommen.  Lemmatisierung und Stemming zielen darauf ab, alle vorkommenden Wortformen zu einer einzigen, normalen Vokabularform zusammenzuführen. <br><br>  <b>Beispiele:</b> <br><br>  Verschiedene Wortformen zu einer bringen: <br><br><pre> <code class="plaintext hljs">dog, dogs, dog's, dogs' =&gt; dog</code> </pre> <br>  Das gleiche, aber mit Bezug auf den ganzen Satz: <br><br><pre> <code class="plaintext hljs">the boy's dogs are different sizes =&gt; the boy dog be differ size</code> </pre> <br>  Lemmatisierung und Stemming sind Sonderfälle der Normalisierung und unterscheiden sich. <br><br>  Stemming ist ein grober heuristischer Prozess, der „Überschüsse“ von der Wortwurzel abschneidet. Dies führt häufig zum Verlust von Suffixen zur Wortbildung. <br><br>  Die Lemmatisierung ist ein subtilerer Prozess, bei dem Vokabeln und morphologische Analysen verwendet werden, um das Wort letztendlich in seine kanonische Form zu bringen - das Lemma. <br><br>  Der Unterschied besteht darin, dass der Stemmer (eine spezifische Implementierung des Stemming-Algorithmus - Übersetzerkommentar) ohne Kenntnis des Kontexts arbeitet und dementsprechend den Unterschied zwischen Wörtern, die je nach Wortart unterschiedliche Bedeutungen haben, nicht versteht.  Die Stemmers haben jedoch ihre eigenen Vorteile: Sie sind einfacher zu implementieren und arbeiten schneller.  Außerdem spielt eine geringere „Genauigkeit“ in einigen Fällen möglicherweise keine Rolle. <br><br>  <b>Beispiele:</b> <br><br><ol><li>  Das Wort gut ist ein Lemma für das Wort besser.  Stemmer wird diese Verbindung nicht sehen, da Sie hier das Wörterbuch konsultieren müssen. </li><li>  Das Wortspiel ist die Grundform des Wortspiels.  Hier werden sowohl Stemming als auch Lemmatisierung bewältigt. </li><li>  Das Wort Treffen kann je nach Kontext entweder eine normale Form eines Substantivs oder eine Form des zu treffenden Verbs sein.  Im Gegensatz zum Stemming wird bei der Lemmatisierung versucht, das richtige Lemma basierend auf dem Kontext auszuwählen. </li></ol><br>  Nachdem wir nun wissen, was der Unterschied ist, schauen wir uns ein Beispiel an: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/8c69db03e92337c9bc9a612361c9bcfb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92909693" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stemming-vs-lemmatization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stemming-vs-lemmatization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stemming-vs-lemmatization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">stem</span> <span class="pl-k">import</span> <span class="pl-v">PorterStemmer</span>, <span class="pl-v">WordNetLemmatizer</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stemming-vs-lemmatization-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">corpus</span> <span class="pl-k">import</span> <span class="pl-s1">wordnet</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stemming-vs-lemmatization-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stemming-vs-lemmatization-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-k">def</span> <span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span>, <span class="pl-s1">pos</span>):</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stemming-vs-lemmatization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-s">"""</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stemming-vs-lemmatization-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stemming-vs-lemmatization-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    """</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stemming-vs-lemmatization-py-LC8" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s">"Stemmer:"</span>, <span class="pl-s1">stemmer</span>.<span class="pl-en">stem</span>(<span class="pl-s1">word</span>))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stemming-vs-lemmatization-py-LC9" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>(<span class="pl-s">"Lemmatizer:"</span>, <span class="pl-s1">lemmatizer</span>.<span class="pl-en">lemmatize</span>(<span class="pl-s1">word</span>, <span class="pl-s1">pos</span>))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stemming-vs-lemmatization-py-LC10" class="blob-code blob-code-inner js-file-line">    <span class="pl-en">print</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-stemming-vs-lemmatization-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-stemming-vs-lemmatization-py-LC12" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">lemmatizer</span> <span class="pl-c1">=</span> <span class="pl-v">WordNetLemmatizer</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-stemming-vs-lemmatization-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stemmer</span> <span class="pl-c1">=</span> <span class="pl-v">PorterStemmer</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-stemming-vs-lemmatization-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span> <span class="pl-c1">=</span> <span class="pl-s">"seen"</span>, <span class="pl-s1">pos</span> <span class="pl-c1">=</span> <span class="pl-s1">wordnet</span>.<span class="pl-v">VERB</span>)</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-stemming-vs-lemmatization-py-LC15" class="blob-code blob-code-inner js-file-line"><span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-s1">stemmer</span>, <span class="pl-s1">lemmatizer</span>, <span class="pl-s1">word</span> <span class="pl-c1">=</span> <span class="pl-s">"drove"</span>, <span class="pl-s1">pos</span> <span class="pl-c1">=</span> <span class="pl-s1">wordnet</span>.<span class="pl-v">VERB</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stemming vs lemmatization.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">Stemmer: seen Lemmatizer: see Stemmer: drove Lemmatizer: drive</code> </pre> <br><h3>  4. Stoppen Sie Wörter </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xr/fq/ju/xrfqju0nbugayjd8cnkvlcbiuwa.png"></div><br><br>  Stoppwörter sind Wörter, die vor / nach der Textverarbeitung aus dem Text geworfen werden.  Wenn wir maschinelles Lernen auf Texte anwenden, können solche Wörter viel Lärm verursachen, sodass Sie irrelevante Wörter entfernen müssen. <br><br>  Stoppwörter werden normalerweise von Artikeln, Interjektionen, Gewerkschaften usw. verstanden, die keine semantische Last tragen.  Es versteht sich, dass es keine universelle Liste von Stoppwörtern gibt, alles hängt vom jeweiligen Fall ab. <br><br>  NLTK verfügt über eine vordefinierte Liste von Stoppwörtern.  Vor der ersten Verwendung müssen Sie es herunterladen: <code>nltk.download(“stopwords”)</code> .  Nach dem Herunterladen können Sie das <code>stopwords</code> importieren und die Wörter selbst <code>stopwords</code> : <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7d2e8f81219656f6d2e82933c6994cfe.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916250" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">nltk</span>.<span class="pl-s1">corpus</span> <span class="pl-k">import</span> <span class="pl-s1">stopwords</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</code> </pre> <br>  Überlegen Sie, wie Sie Stoppwörter aus einem Satz entfernen können: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/b1c69457cc0d8eab7b3661533725485a.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916498" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-1-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-1-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-1-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stop_words</span> <span class="pl-c1">=</span> <span class="pl-en">set</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-1-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games."</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-1-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-1-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-1-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">without_stop_words</span> <span class="pl-c1">=</span> [<span class="pl-s1">word</span> <span class="pl-k">for</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">words</span> <span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">stop_words</span>]</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-1-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">without_stop_words</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 1.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']</code> </pre> <br>  Wenn Sie mit Listenverständnissen nicht vertraut sind, erfahren Sie hier mehr.  Hier ist ein anderer Weg, um das gleiche Ergebnis zu erzielen: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/bbfab6573e886bd122aba972048d54cb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92916520" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-2-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-2-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-2-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">stop_words</span> <span class="pl-c1">=</span> <span class="pl-en">set</span>(<span class="pl-s1">stopwords</span>.<span class="pl-en">words</span>(<span class="pl-s">"english"</span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-2-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"Backgammon is one of the oldest known board games."</span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-2-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-2-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">words</span> <span class="pl-c1">=</span> <span class="pl-s1">nltk</span>.<span class="pl-en">word_tokenize</span>(<span class="pl-s1">sentence</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-2-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">without_stop_words</span> <span class="pl-c1">=</span> []</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-2-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> <span class="pl-s1">word</span> <span class="pl-c1">in</span> <span class="pl-s1">words</span>:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stop-words-example-2-py-LC7" class="blob-code blob-code-inner js-file-line">    <span class="pl-k">if</span> <span class="pl-s1">word</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">stop_words</span>:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stop-words-example-2-py-LC8" class="blob-code blob-code-inner js-file-line">        <span class="pl-s1">without_stop_words</span>.<span class="pl-en">append</span>(<span class="pl-s1">word</span>)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stop-words-example-2-py-LC9" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stop-words-example-2-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">without_stop_words</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 2.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Denken Sie jedoch daran, dass das Listenverständnis schneller ist, weil es optimiert ist. Der Interpreter zeigt während der Schleife ein Vorhersagemuster an. <br><br>  Sie fragen sich vielleicht, warum wir die Liste in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele</a> konvertiert haben.  Ein Satz ist ein abstrakter Datentyp, der eindeutige Werte in einer undefinierten Reihenfolge speichern kann.  Die Suche nach Set ist viel schneller als das Durchsuchen einer Liste.  Für eine kleine Anzahl von Wörtern spielt dies keine Rolle, aber wenn wir über eine große Anzahl von Wörtern sprechen, wird die Verwendung von Mengen dringend empfohlen.  Wenn Sie mehr über die Zeit erfahren möchten, die für die Durchführung verschiedener Operationen erforderlich ist, sehen Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen wunderbaren Spickzettel an</a> . <br><br><h3>  5. Reguläre Ausdrücke. </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/mm/jx/hnmmjxjubpvt7p1uc-lv-t-auhi.jpeg"></div><br>  Ein regulärer Ausdruck (Regex, Regexp, Regex) ist eine Folge von Zeichen, die ein Suchmuster definiert.  Zum Beispiel: <br><br><ul><li>  .  - jedes Zeichen außer Zeilenvorschub; </li><li>  \ w ist ein Wort; </li><li>  \ d - eine Ziffer; </li><li>  \ s - ein Leerzeichen; </li><li>  \ W ist ein NICHT-Wort; </li><li>  \ D - eine nichtstellige Zahl; </li><li>  \ S - ein Nicht-Leerzeichen; </li><li>  [abc] - stellt fest, dass eines der angegebenen Zeichen mit a, b oder c übereinstimmt; </li><li>  [^ abc] - findet ein beliebiges Zeichen außer den angegebenen; </li><li>  [ag] - Findet ein Zeichen im Bereich von a bis g. </li></ul><br>  Auszug aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Dokumentation</a> : <br><blockquote>  Reguläre Ausdrücke verwenden den Backslash <code>(\)</code> , um Sonderformen anzugeben oder die Verwendung von Sonderzeichen zu ermöglichen.  Dies widerspricht der Verwendung des Backslash in Python: <code>'\\\\'</code> beispielsweise den Backslash wörtlich zu bezeichnen, müssen Sie <code>'\\\\'</code> als Suchmuster schreiben, da der reguläre Ausdruck wie <code>\\</code> aussehen sollte, wobei jeder Backslash maskiert werden muss. <br><br>  Die Lösung besteht darin, die rohe Zeichenfolgennotation für Suchmuster zu verwenden.  Backslashes werden nicht speziell verarbeitet, wenn sie mit dem Präfix <code>'r'</code> .  Somit ist <code>r”\n”</code> eine Zeichenfolge mit zwei Zeichen <code>('\'  'n')</code> , und <code>“\n”</code> ist eine Zeichenfolge mit einem Zeichen (Zeilenvorschub). <br></blockquote>  Wir können Stammgäste verwenden, um unseren Text weiter zu filtern.  Sie können beispielsweise alle Zeichen entfernen, die keine Wörter sind.  In vielen Fällen ist keine Interpunktion erforderlich und kann mithilfe von Stammgästen leicht entfernt werden. <br><br>  Das Modul <b>re</b> in Python repräsentiert Operationen mit regulären Ausdrücken.  Wir können die Funktion <b>re.sub</b> verwenden, um alles, was zum Suchmuster passt, durch die angegebene Zeichenfolge zu ersetzen.  So können Sie alle Nichtwörter durch Leerzeichen ersetzen: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/a9a29588061a8c9bb8aeb28140a69f89.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist92925716" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-regex-substitute-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-regex-substitute-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-regex-substitute-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">re</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-regex-substitute-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">sentence</span> <span class="pl-c1">=</span> <span class="pl-s">"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-regex-substitute-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pattern</span> <span class="pl-c1">=</span> <span class="pl-s">r"[^\w]"</span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-regex-substitute-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">re</span>.<span class="pl-en">sub</span>(<span class="pl-s1">pattern</span>, <span class="pl-s">" "</span>, <span class="pl-s1">sentence</span>))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">regex substitute.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">'The development of snowboarding was inspired by skateboarding sledding surfing and skiing '</code> </pre> <br>  Stammgäste sind ein leistungsstarkes Werkzeug, mit dem viel komplexere Muster erstellt werden können.  Wenn Sie mehr über reguläre Ausdrücke erfahren möchten, kann ich diese beiden Webanwendungen empfehlen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regex</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regex101</a> . <br><br><h3>  6. Tasche voller Wörter </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oh/va/sp/ohvaspwwyxr6mjgfz1w4vq8znku.png"></div><br>  Algorithmen für maschinelles Lernen können nicht direkt mit Rohtext arbeiten, daher müssen Sie den Text in Zahlenmengen (Vektoren) konvertieren.  Dies wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Merkmalsextraktion bezeichnet</a> . <br><br>  Eine Worttasche ist eine beliebte und einfache Funktion zum Extrahieren von Funktionen, die beim Arbeiten mit Text verwendet wird.  Es beschreibt das Vorkommen jedes Wortes im Text. <br><br>  Um das Modell verwenden zu können, benötigen wir: <br><br><ol><li>  Definieren Sie ein Wörterbuch bekannter Wörter (Token). </li><li>  Wählen Sie den Grad der Präsenz berühmter Wörter. </li></ol><br>  Alle Informationen über die Reihenfolge oder Struktur von Wörtern werden ignoriert.  Deshalb wird es eine TASCHE mit Wörtern genannt.  Dieses Modell versucht zu verstehen, ob ein bekanntes Wort in einem Dokument vorkommt, weiß jedoch nicht, wo genau es vorkommt. <br><br>  Die Intuition legt nahe, dass <b>ähnliche Dokumente</b> einen <b>ähnlichen Inhalt haben</b> .  Dank des Inhalts können wir auch etwas über die Bedeutung des Dokuments lernen. <br><br>  <b>Ein Beispiel:</b> <br>  Beachten Sie die Schritte zum Erstellen dieses Modells.  Wir verwenden nur 4 Sätze, um zu verstehen, wie das Modell funktioniert.  Im wirklichen Leben werden Sie auf mehr Daten stoßen. <br><br><h4>  1. Daten herunterladen </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/xz/w9/trxzw9m1s7psallg0ulf6wepnsu.png"></div><br>  Stellen Sie sich vor, dies sind unsere Daten und wir möchten sie als Array laden: <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  Lesen Sie dazu einfach die Datei und teilen Sie sie durch die Zeile: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/15abcc02fefb2782ba78ac695d4dda59.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93035402" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-read-the-movie-reviews-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-read-the-movie-reviews-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-read-the-movie-reviews-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s">"simple movie reviews.txt"</span>, <span class="pl-s">"r"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-read-the-movie-reviews-py-LC2" class="blob-code blob-code-inner js-file-line">    <span class="pl-s1">documents</span> <span class="pl-c1">=</span> <span class="pl-s1">file</span>.<span class="pl-en">read</span>().<span class="pl-en">splitlines</span>()</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-read-the-movie-reviews-py-LC3" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-read-the-movie-reviews-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-en">print</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">read the movie reviews.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><pre> <code class="plaintext hljs">["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']</code> </pre> <br><br><h4>  2. Definieren Sie ein Wörterbuch </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/vg/7p/lkvg7pwbgfcd130zn66zx_qhjxq.png"></div><br>  Wir werden alle eindeutigen Wörter aus 4 geladenen Sätzen sammeln, wobei Groß- und Kleinschreibung, Interpunktion und Token mit einem Zeichen ignoriert werden.  Dies wird unser Wörterbuch sein (berühmte Wörter). <br><br>  Um ein Wörterbuch zu erstellen, können Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CountVectorizer-</a> Klasse aus der sklearn-Bibliothek verwenden.  Fahren Sie mit dem nächsten Schritt fort. <br><br><h4>  3. Erstellen Sie Dokumentvektoren </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4q/jj/hh/4qjjhhlyqga--r6eh9kkery5jsy.png"></div><br>  Als nächstes müssen wir die Wörter im Dokument bewerten.  In diesem Schritt ist es unser Ziel, Rohtext in eine Reihe von Zahlen umzuwandeln.  Danach verwenden wir diese Mengen als Eingabe für das Modell des maschinellen Lernens.  Die einfachste Bewertungsmethode besteht darin, das Vorhandensein von Wörtern zu notieren, dh 1, wenn ein Wort vorhanden ist, und 0, wenn es nicht vorhanden ist. <br><br>  Jetzt können wir mit der oben genannten CountVectorizer-Klasse eine Wortsammlung erstellen. <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/155e97ad22862a340d941a63e43295d9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93036006" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-simple-bag-of-words-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-simple-bag-of-words-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-simple-bag-of-words-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Import the libraries we need</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-simple-bag-of-words-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">feature_extraction</span>.<span class="pl-s1">text</span> <span class="pl-k">import</span> <span class="pl-v">CountVectorizer</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-simple-bag-of-words-example-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-simple-bag-of-words-example-py-LC4" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-simple-bag-of-words-example-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Step 2. Design the Vocabulary</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-simple-bag-of-words-example-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># The default token pattern removes tokens of a single character. That's why we don't have the "I" and "s" tokens in the output</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-simple-bag-of-words-example-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">count_vectorizer</span> <span class="pl-c1">=</span> <span class="pl-v">CountVectorizer</span>()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-simple-bag-of-words-example-py-LC8" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-simple-bag-of-words-example-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Step 3. Create the Bag-of-Words Model</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-simple-bag-of-words-example-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">bag_of_words</span> <span class="pl-c1">=</span> <span class="pl-s1">count_vectorizer</span>.<span class="pl-en">fit_transform</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-simple-bag-of-words-example-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-simple-bag-of-words-example-py-LC12" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Show the Bag-of-Words Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-simple-bag-of-words-example-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">feature_names</span> <span class="pl-c1">=</span> <span class="pl-s1">count_vectorizer</span>.<span class="pl-en">get_feature_names</span>()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-simple-bag-of-words-example-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pd</span>.<span class="pl-v">DataFrame</span>(<span class="pl-s1">bag_of_words</span>.<span class="pl-en">toarray</span>(), <span class="pl-s1">columns</span> <span class="pl-c1">=</span> <span class="pl-s1">feature_names</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">simple bag-of-words example.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ee/pf/-g/eepf-gw0it8d6a_fwcoew7bwbns.png"></div><br>  Dies sind unsere Vorschläge.  Jetzt sehen wir, wie das Modell „Bag of Words“ funktioniert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/l3/ep/95/l3ep95r4s_rdnvjfmfy7ebcwuag.png"></div><br><h3>  Ein paar Worte über die Wortsammlung </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k4/mo/s1/k4mos1faome-c00hjo84v7nt7no.png"></div><br>  Die Komplexität dieses Modells besteht darin, wie das Wörterbuch bestimmt und das Auftreten von Wörtern gezählt wird. <br><br>  Wenn die Wörterbuchgröße zunimmt, wächst auch der Dokumentvektor.  Im obigen Beispiel ist die Länge des Vektors gleich der Anzahl bekannter Wörter. <br><br>  In einigen Fällen können wir eine unglaublich große Datenmenge haben, und dann kann der Vektor aus Tausenden oder Millionen von Elementen bestehen.  Darüber hinaus kann jedes Dokument nur einen kleinen Teil der Wörter aus dem Wörterbuch enthalten. <br><br>  Infolgedessen enthält die Vektordarstellung viele Nullen.  Vektoren mit vielen Nullen werden als spärliche Vektoren bezeichnet. Sie erfordern mehr Speicher und Rechenressourcen. <br><br>  Wir können jedoch die Anzahl der bekannten Wörter reduzieren, wenn wir dieses Modell verwenden, um die Anforderungen an die Rechenressourcen zu verringern.  Dazu können Sie dieselben Techniken verwenden, die wir bereits vor dem Erstellen einer Worttasche in Betracht gezogen haben: <br><br><ul><li>  den Fall von Wörtern ignorieren; </li><li>  Interpunktion ignorieren; </li><li>  Stoppwörter auswerfen; </li><li>  Reduktion von Wörtern auf ihre Grundformen (Lemmatisierung und Stemming); </li><li>  Korrektur falsch geschriebener Wörter. </li></ul><br>  Eine andere, kompliziertere Möglichkeit, ein Wörterbuch zu erstellen, besteht darin, gruppierte Wörter zu verwenden.  Dadurch wird die Größe des Wörterbuchs geändert und der Worttasche mehr Details zum Dokument hinzugefügt.  Dieser Ansatz wird als " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">N-Gramm</a> " bezeichnet. <br><br>  N-Gramm ist eine Folge beliebiger Entitäten (Wörter, Buchstaben, Zahlen, Zahlen usw.).  Im Kontext von Sprachkörpern wird das N-Gramm normalerweise als eine Folge von Wörtern verstanden.  Ein Unigramm ist ein Wort, ein Bigram ist eine Folge von zwei Wörtern, ein Trigramm ist drei Wörter und so weiter.  Die Zahl N gibt an, wie viele gruppierte Wörter im N-Gramm enthalten sind.  Nicht alle möglichen N-Gramm fallen in das Modell, sondern nur die, die im Fall erscheinen. <br><br>  <b>Ein Beispiel:</b> <br><br>  Betrachten Sie den folgenden Satz: <br><br><pre> <code class="plaintext hljs">The office building is open today</code> </pre> <br>  Hier sind seine Bigramme: <br><br><ul><li>  das Büro </li><li>  Bürogebäude </li><li>  Gebäude ist </li><li>  ist offen </li><li>  heute geöffnet </li></ul><br>  Wie Sie sehen können, ist eine Tüte Bigrams ein effektiverer Ansatz als eine Tüte Wörter. <br><br>  <b>Bewertung (Bewertung) von Wörtern</b> <br><br>  Wenn ein Wörterbuch erstellt wird, sollte das Vorhandensein von Wörtern bewertet werden.  Wir haben bereits einen einfachen binären Ansatz in Betracht gezogen (1 - es gibt ein Wort, 0 - es gibt kein Wort). <br><br>  Es gibt andere Methoden: <br><br><ol><li>  Menge.  Es wird berechnet, wie oft jedes Wort im Dokument erscheint. </li><li>  Frequenz  Es wird berechnet, wie oft jedes Wort im Text vorkommt (bezogen auf die Gesamtzahl der Wörter). </li></ol><br><br><h3>  7. TF-IDF </h3><br>  Die Frequenzbewertung hat ein Problem: Wörter mit der höchsten Frequenz haben jeweils die höchste Bewertung.  In diesen Worten gibt es möglicherweise nicht so viel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Informationsgewinn</a> für das Modell wie in weniger häufigen Worten.  Eine Möglichkeit, die Situation zu korrigieren, besteht darin, die Wortbewertung zu senken, die häufig <b>in allen ähnlichen Dokumenten zu finden ist</b> .  Dies wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF-IDF bezeichnet</a> . <br><br>  TF-IDF (kurz für Term Frequency - Inverse Document Frequency) ist ein statistisches Maß zur Bewertung der Wichtigkeit eines Wortes in einem Dokument, das Teil einer Sammlung oder eines Korpus ist. <br><br>  Die Bewertung durch TF-IDF wächst proportional zur Häufigkeit des Auftretens eines Wortes in einem Dokument. Dies wird jedoch durch die Anzahl der Dokumente ausgeglichen, die dieses Wort enthalten. <br><br>  Bewertungsformel für das Wort X in Dokument Y: <br><br><img src="https://habrastorage.org/webt/_3/bb/xo/_3bbxoimlox11_am3gzyequcjic.png"><br>  <font color="grey">Formel TF-IDF.</font>  <font color="grey">Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html</a></font> <br><br>  TF (Termhäufigkeit) ist das Verhältnis der Anzahl der Vorkommen eines Wortes zur Gesamtzahl der Wörter in einem Dokument. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/p0/wk/aip0wkqcynj8q1cxwxlufspqqds.png"></div><br>  IDF (inverse Dokumenthäufigkeit) ist die Umkehrung der Häufigkeit, mit der ein Wort in Sammlungsdokumenten vorkommt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6j/xd/32/6jxd32ydlpkmixkjw6hdgmp6f6m.png"></div><br>  Infolgedessen kann TF-IDF für den Wortterm wie folgt berechnet werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hl/tp/n0/hltpn0vg_gdo8bn1pfimbvu60no.png"></div><br>  <b>Ein Beispiel:</b> <br><br>  Sie können die <b>TfidfVectorizer-</b> Klasse aus der sklearn-Bibliothek verwenden, um TF-IDF zu berechnen.  Lassen Sie uns dies mit denselben Nachrichten tun, die wir im Beispiel für die Worttasche verwendet haben. <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  Code: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/c84cfc6fef2dc131236a9fa5c72de3c9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-13f839f7454b3a5b3bfbfd6d1e34ec9d.css"><div id="gist93050848" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-tf-idf-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-tf-idf-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-tf-idf-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">feature_extraction</span>.<span class="pl-s1">text</span> <span class="pl-k">import</span> <span class="pl-v">TfidfVectorizer</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-tf-idf-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-tf-idf-example-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-tf-idf-example-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">tfidf_vectorizer</span> <span class="pl-c1">=</span> <span class="pl-v">TfidfVectorizer</span>()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-tf-idf-example-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">values</span> <span class="pl-c1">=</span> <span class="pl-s1">tfidf_vectorizer</span>.<span class="pl-en">fit_transform</span>(<span class="pl-s1">documents</span>)</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-tf-idf-example-py-LC6" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-tf-idf-example-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-c"># Show the Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-tf-idf-example-py-LC8" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">feature_names</span> <span class="pl-c1">=</span> <span class="pl-s1">tfidf_vectorizer</span>.<span class="pl-en">get_feature_names</span>()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-tf-idf-example-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-s1">pd</span>.<span class="pl-v">DataFrame</span>(<span class="pl-s1">values</span>.<span class="pl-en">toarray</span>(), <span class="pl-s1">columns</span> <span class="pl-c1">=</span> <span class="pl-s1">feature_names</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">tf-idf example.py</a>
        hosted with ❤ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Fazit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ad/vj/kt/advjktxg44hyhjj63m27igwssiu.png"></div><br><h2>  Fazit </h2><br>  Dieser Artikel hat die Grundlagen von NLP für Text behandelt, nämlich: <br><br><ul><li>  NLP ermöglicht die Verwendung von Algorithmen für maschinelles Lernen für Text und Sprache. </li><li>  NLTK (Natural Language Toolkit) - eine führende Plattform zum Erstellen von NLP-Programmen in Python; </li><li>  Bei der Tokenisierung von Vorschlägen wird eine geschriebene Sprache in einzelne Sätze unterteilt. </li><li>  Wort-Tokenisierung ist der Prozess des Teilens von Sätzen in einzelne Wörter; </li><li>  Lemmatisierung und Stemming zielen darauf ab, alle Wortformen zu einer einzigen, normalen Vokabularform zusammenzuführen. </li><li>  Stoppwörter sind Wörter, die vor / nach der Textverarbeitung aus dem Text geworfen werden. </li><li>  Regex (Regex, Regexp, Regex) ist eine Folge von Zeichen, die ein Suchmuster definiert. </li><li>  Eine Tüte mit Wörtern ist eine beliebte und einfache Technik zum Extrahieren von Merkmalen, die beim Arbeiten mit Text verwendet wird.  Es beschreibt das Vorkommen jedes Wortes im Text. </li></ul><br>  Großartig!  Nachdem Sie die Grundlagen der Merkmalsextraktion kennen, können Sie Merkmale als Eingabe für Algorithmen für maschinelles Lernen verwenden. <br><br>  Wenn Sie alle beschriebenen Konzepte in einem großen Beispiel sehen möchten, dann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sind Sie hier</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446738/">https://habr.com/ru/post/de446738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446726/index.html">Deep Learning in der optischen Flussberechnung</a></li>
<li><a href="../de446728/index.html">Wie sich die vom kabellosen Laden empfangene Leistung ändert, hängt vom Standort des Telefons ab</a></li>
<li><a href="../de446730/index.html">Backend-Bereich zu DUMP: Serverless, Postgres and Go, .NET Core, GraphQL und mehr</a></li>
<li><a href="../de446732/index.html">Feropoden werden nicht helfen: Forschung und mathematische Modellierung von Grubenfallen für Ameisenlöwenlarven</a></li>
<li><a href="../de446736/index.html">Oracle APEX Berichte</a></li>
<li><a href="../de446740/index.html">Verwenden von Python für die Berichterstellung in einem einzelnen Unternehmen</a></li>
<li><a href="../de446742/index.html">Themen der Top 3D Expo 2019: „Anisoprinting - die Technologie zur Herstellung von Verbundstrukturen einer neuen Generation“, Fedor Antonov</a></li>
<li><a href="../de446744/index.html">VR mit neuronalen Schnittstellen - ein vollständiges Eintauchen in die virtuelle Realität</a></li>
<li><a href="../de446746/index.html">Ein UBS-Mitarbeiter hat ein Gespräch über einen Nachbarn des Eurostar-Zuges mitgehört und einen Deal über 15 Milliarden US-Dollar herausgefunden. Jetzt werden er und die Bank mit einer Geldstrafe belegt</a></li>
<li><a href="../de446750/index.html">Nachrichten von unten: IT-Giganten haben begonnen, aktiv ihre eigenen U-Boot-Backbone-Netzwerke aufzubauen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>