<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍👩‍👧‍👧 🤳🏼 ☝🏿 Comment connecter des clusters Kubernetes dans différents centres de données 🛸 🧖🏿 ✍🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bienvenue dans la série de didacticiels rapides Kubernetes. Ceci est une chronique régulière avec les questions les plus intéressantes que nous recevo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment connecter des clusters Kubernetes dans différents centres de données</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/454056/"><p><img src="https://habrastorage.org/webt/po/7g/cr/po7gcry-sgg-lsjjyc7ro8mraoa.jpeg"><br>  <strong>Bienvenue dans la série de didacticiels rapides Kubernetes.</strong>  Ceci est une chronique régulière avec les questions les plus intéressantes que nous recevons en ligne et lors de nos formations.  L'expert de Kubernetes répond. </p><br><blockquote>  L'expert d'aujourd'hui est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Daniele Polencic</a> .  Daniel est instructeur et développeur de logiciels chez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Learnk8s</a> . </blockquote><p>  Si vous souhaitez répondre à votre question dans le prochain post, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">veuillez nous contacter par email</a> ou sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Twitter: @ learnk8s</a> . </p><br><p>  Messages précédents ignorés?  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cherchez-les ici</a> . </p><br><h3 id="kak-soedinit-klastery-kubernetes-v-raznyh-data-centrah">  Comment connecter les clusters Kubernetes dans différents centres de données? </h3><br><blockquote>  <strong>En bref</strong> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubefed v2 arrive bientôt</a> , et je vous conseille également de lire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Shipper</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le projet multi-cluster-scheduler</a> . <br><a name="habracut"></a><br><br>  Très souvent, l'infrastructure est répliquée et distribuée dans différentes régions, en particulier dans des environnements contrôlés. </blockquote><p>  <strong>Si une région n'est pas disponible, le trafic est redirigé vers une autre pour éviter les interruptions.</strong> </p><br><p>  Avec Kubernetes, vous pouvez utiliser une stratégie similaire et répartir les charges de travail entre différentes régions. </p><br><p>  Vous pouvez avoir un ou plusieurs clusters par équipe, région, environnement ou une combinaison de ces éléments. </p><br><p>  Vos clusters peuvent être hébergés dans différents clouds et dans un environnement local. </p><br><p>  <em>Mais comment planifier l'infrastructure pour une telle répartition géographique?</em> <em><br></em>  <em>Besoin de créer un grand cluster dans plusieurs environnements cloud sur un seul réseau?</em> <em><br></em>  <em>Ou démarrer un grand nombre de petits clusters et trouver un moyen de les contrôler et de les synchroniser?</em> </p><br><h3 id="odin-rukovodyaschiy-klaster">  Un cluster principal </h3><br><p> <em>La création d'un cluster unique sur un seul réseau n'est pas si simple.</em> </p><br><p>  Imaginez que vous ayez un accident, une perte de connectivité entre les segments du cluster. </p><br><p>  Si vous avez un serveur maître, la moitié des ressources ne pourront pas recevoir de nouvelles commandes, car elles ne pourront pas contacter le maître. </p><br><p> Et en même temps, vous avez d'anciennes tables de routage ( <code>kube-proxy</code> ne peut pas en charger de nouvelles) et aucun pod supplémentaire (kubelet ne peut pas demander de mises à jour). </p><br><p>  Pour aggraver les choses, si Kubernetes ne voit pas le nœud, il le marque comme perdu et distribue les pods manquants aux nœuds existants. </p><br><p>  <strong>En conséquence, vous avez deux fois plus de pods.</strong> </p><br><p>  Si vous créez un serveur maître pour chaque région, il y aura des problèmes avec l'algorithme de consensus dans la base de données etcd.  ( <em>environ Ed. - En fait, la base de données etcd n'a pas besoin d'être située sur les serveurs maîtres. Elle peut être exécutée sur un groupe distinct de serveurs dans la même région. Cependant, en même temps, elle a reçu un point de défaillance de cluster. Mais rapidement.</em> ) </p><br><p>  etcd utilise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'algorithme raft</a> pour négocier la valeur avant de l'écrire sur le disque. <br>  Autrement dit, la plupart des instances doivent parvenir à un consensus avant que l'État puisse être écrit sur etcd. </p><br><p>  Si le délai entre les instances de etcd augmente considérablement, comme c'est le cas avec trois instances de etcd dans différentes régions, il faut beaucoup de temps pour réconcilier la valeur et l'écrire sur le disque. <br>  Cela se reflète dans les contrôleurs Kubernetes. </p><br><p>  Le gestionnaire du contrôleur a besoin de plus de temps pour en savoir plus sur la modification et écrire la réponse dans la base de données. </p><br><p>  Et comme le contrôleur n'est pas un, mais plusieurs, <strong>une réaction en chaîne est obtenue et l'ensemble du cluster commence à fonctionner très lentement</strong> . </p><br><p>  etcd est si sensible à la latence que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la documentation officielle recommande d'utiliser des SSD au lieu des disques durs ordinaires</a> . </p><br><p>  <strong>Il n'existe actuellement aucun bon exemple de grand réseau pour un seul cluster.</strong> </p><br><p>  Fondamentalement, la communauté de développement et le groupe de clusters SIG tentent de comprendre comment orchestrer les clusters de la même manière que Kubernetes orchestre les conteneurs. </p><br><h3 id="variant-1-federaciya-klasterov-s-kubefed">  Option 1: fédération de clusters avec kubefed </h3><br><p>  La réponse officielle de SIG-cluster est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubefed2, une nouvelle version de la fédération kube client et opérateur d'origine</a> . </p><br><p>  Pour la première fois, ils ont essayé de gérer la collection de clusters en tant qu'objet unique à l'aide de l'outil de fédération kube. </p><br><p>  Le début a été bon, mais au final, la fédération kube n'est pas devenue populaire car elle ne supportait pas toutes les ressources. </p><br><p>  Il a soutenu les livraisons et les services communs, mais pas, par exemple, les StatefulSets. <br>  Et la configuration de la fédération a été transmise sous forme d'annotations et n'était pas flexible. </p><br><p>  <em>Imaginez comment vous pouvez décrire la séparation des répliques pour chaque cluster dans une fédération en utilisant uniquement des annotations.</em> </p><br><p>  <strong>Cela s'est avéré être un gâchis complet.</strong> </p><br><p>  SIG-cluster a fait un excellent travail après kubefed v1 et a décidé d'aborder le problème de l'autre côté. </p><br><p>  <strong>Au lieu d'annotations, ils ont décidé de libérer un contrôleur installé sur les clusters.</strong>  <strong>Il peut être configuré à l'aide de la définition de ressource personnalisée (CRD).</strong> </p><br><p>  Pour chaque ressource qui fera partie de la fédération, vous disposez d'une définition CRD personnalisée de trois sections: </p><br><ul><li>  définition standard d'une ressource, telle que déploiement; </li><li>  section de <code>placement</code> , où vous déterminez comment la ressource sera distribuée dans la fédération; </li><li>  <code>override</code> section, où, pour une ressource particulière, vous pouvez remplacer le poids et les paramètres du placement. </li></ul><br><p>  Voici un exemple de livraison combinée avec des sections de placement et de remplacement. </p><br><pre> <code class="plaintext hljs">apiVersion: types.federation.k8s.io/v1alpha1 kind: FederatedDeployment metadata: name: test-deployment namespace: test-namespace spec: template: metadata: labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx placement: clusterNames: - cluster2 - cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: spec.replicas value: 5</code> </pre> <br><p>  Comme vous pouvez le voir, l'approvisionnement est réparti sur deux clusters: <code>cluster1</code> et <code>cluster2</code> . </p><br><p>  Le premier cluster fournit trois répliques, tandis que le second est défini sur 5. </p><br><p>  Si vous avez besoin de plus de contrôle sur le nombre de répliques, kubefed2 fournit un nouvel objet ReplicaSchedulingPreference, où les répliques peuvent être pondérées: </p><br><pre> <code class="plaintext hljs">apiVersion: scheduling.federation.k8s.io/v1alpha1 kind: ReplicaSchedulingPreference metadata: name: test-deployment namespace: test-ns spec: targetKind: FederatedDeployment totalReplicas: 9 clusters: A: weight: 1 B: weight: 2</code> </pre> <br><p>  La structure et l'API de CRD ne sont pas encore prêtes et un travail actif est en cours dans le référentiel officiel du projet. </p><br><p>  <strong>Attention à kubefed2, mais n'oubliez pas qu'il n'est pas encore adapté à l'environnement de travail.</strong> </p><br><p>  En savoir plus sur kubefed2 dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article officiel de</a> kubefed2 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le</a> blog de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes</a> et dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">référentiel de projets kubefed officiel</a> . </p><br><h3 id="variant-2-obedinenie-klasterov-v-stile-bookingcom">  Option 2: clustering de clusters de style Booking.com </h3><br><p>  Les développeurs de Booking.com n'ont pas traité de kubefed v2, mais ils ont trouvé Shipper, un opérateur pour la livraison sur plusieurs clusters, dans plusieurs régions et dans plusieurs nuages. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Shipper est</a> quelque chose de similaire à kubefed2. </p><br><p>  Les deux outils vous permettent de configurer la stratégie de déploiement pour plusieurs clusters (quels clusters sont utilisés et combien de réplicas ils ont). </p><br><p>  Mais <strong>l'objectif de Shipper est de réduire le risque d'erreurs de livraison.</strong> </p><br><p>  Dans Shipper, vous pouvez définir une série d'étapes qui décrivent la séparation des répliques entre le déploiement précédent et actuel et la quantité de trafic entrant. </p><br><p>  Lorsque vous soumettez une ressource à un cluster, le contrôleur Shipper déploie cette modification étape par étape sur tous les clusters combinés. </p><br><p>  <em>Et l'expéditeur est très limité.</em> </p><br><p>  Par exemple, <strong>il accepte les graphiques Helm en entrée</strong> et ne prend pas en charge les ressources vanilla. <br>  De manière générale, l'expéditeur fonctionne comme suit. </p><br><p>  Au lieu de la livraison standard, vous devez créer une ressource d'application qui inclut le graphique Helm: </p><br><pre> <code class="plaintext hljs">apiVersion: shipper.booking.com/v1alpha1 kind: Application metadata: name: super-server spec: revisionHistoryLimit: 3 template: chart: name: nginx repoUrl: https://storage.googleapis.com/shipper-demo version: 0.0.1 clusterRequirements: regions: - name: local strategy: steps: - capacity: contender: 1 incumbent: 100 name: staging traffic: contender: 0 incumbent: 100 - capacity: contender: 100 incumbent: 0 name: full on traffic: contender: 100 incumbent: 0 values: replicaCount: 3</code> </pre> <br><p>  <strong>Shipper est une bonne option pour gérer plusieurs clusters, mais sa relation étroite avec Helm n'interfère que.</strong> </p><br><p>  <em>Et si nous passions tous de Helm à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kustomize</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kapitan</a> ?</em> </p><br><p>  En savoir plus sur Shipper et sa philosophie dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce communiqué de presse officiel</a> . </p><br><p>  Si vous souhaitez vous plonger dans le code, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">accédez au référentiel officiel du projet</a> . </p><br><h3 id="variant-3-magicheskoe-obedinenie-klasterov">  Option 3: rejoindre un cluster «magique» </h3><br><p>  Kubefed v2 et Shipper fonctionnent avec la fédération de clusters, fournissant aux clusters de nouvelles ressources via des définitions de ressources personnalisées. </p><br><p>  <em>Mais que se passe-t-il si vous ne souhaitez pas réécrire toutes les fournitures, StatefulSets, DaemonSets, etc. à combiner?</em> </p><br><p>  <em>Comment inclure un cluster existant dans une fédération sans changer YAML?</em> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">multi-cluster-scheduler est un projet Admirality</a> qui gère la planification des charges de travail dans les clusters. </p><br><p>  Mais au lieu de proposer une nouvelle façon d'interagir avec le cluster et d'envelopper les ressources dans des définitions définies par l'utilisateur, le planificateur multi-cluster est intégré dans le cycle de vie Kubernetes standard et intercepte tous les appels créés par les pods. </p><br><p>  <strong>Chacun créé sous immédiatement remplacé par un mannequin.</strong> </p><br><blockquote>  multi-cluster-scheduler utilise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des crochets Web pour modifier l'accès</a> pour intercepter l'appel et créer un mannequin de pod inactif. </blockquote><p>  Le module source passe par un autre cycle de planification, où après une enquête auprès de l'ensemble de la fédération, une décision est prise sur le placement. </p><br><p>  Enfin, le pod est livré au cluster cible. </p><br><p>  En conséquence, vous avez un pod supplémentaire qui ne fait rien, prend juste de l'espace. </p><br><p>  L'avantage est que vous n'avez pas eu à écrire de nouvelles ressources pour combiner les fournitures. </p><br><p>  <strong>Chaque ressource créant un module est automatiquement prête à être fusionnée.</strong> </p><br><p>  C'est intéressant, car vous avez soudainement des livraisons réparties sur plusieurs régions, mais vous ne l'avez pas remarqué.  Cependant, c'est assez risqué, car ici tout repose sur la magie. </p><br><p>  Mais si Shipper essaie principalement d'atténuer les effets des fournitures, le planificateur multi-cluster effectue des tâches plus générales et est probablement mieux adapté aux travaux par lots. </p><br><p>  Il ne dispose pas d'un mécanisme d'approvisionnement progressif avancé. </p><br><p>  Vous pouvez en savoir plus sur le planificateur multi-cluster sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la page officielle du référentiel</a> . </p><br><p>  Si vous souhaitez en savoir plus sur le planificateur multi-cluster en action, Admiralty a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cas d'utilisation intéressant avec Argo</a> - workflows, événements, CI et CD Kubernetes. </p><br><h3 id="drugie-instrumenty-i-resheniya">  Autres outils et solutions </h3><br><p>  Connecter et gérer plusieurs clusters est une tâche complexe, il n'y a pas de solution universelle. </p><br><p>  Si vous souhaitez en savoir plus sur ce sujet, voici quelques ressources: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Rancher's Submariner</a> est un outil qui relie les réseaux de superposition de différents clusters Kubernetes. </li><li>  La chaîne de vente au détail de Target utilise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Unimatrix en combinaison avec Spinnaker pour orchestrer les déploiements sur plusieurs clusters</a> . </li><li>  Essayez d'utiliser IPV6 et un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">seul réseau dans plusieurs régions</a> . </li><li>  Vous pouvez utiliser un maillage de service, comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Istio, pour connecter plusieurs clusters</a> . </li><li>  Cilium, un plugin d'interface réseau de conteneurs, offre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une fonction de maillage de cluster</a> qui permet de combiner plusieurs clusters </li></ul><br><h3 id="vot-i-vse-na-segodnya">  C'est tout pour aujourd'hui </h3><br><p>  <em>Merci d'avoir lu jusqu'au bout!</em> </p><br><p>  Si vous savez comment connecter plus efficacement plusieurs clusters, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dites-le-nous</a> . </p><br><p>  Nous ajouterons votre méthode aux liens. </p><br><p>  Un merci spécial à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chris Nesbitt-Smith</a> et Vincent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">De Smet</a> (ingénieur en fiabilité chez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">swatmobile.io</a> ) pour avoir lu cet article et partagé quelques informations utiles sur le fonctionnement de la fédération. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr454056/">https://habr.com/ru/post/fr454056/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr454042/index.html">Payez ce que vous voulez: comment ce modèle s'est montré dans la musique et qui a essayé de gagner de l'argent comme ça</a></li>
<li><a href="../fr454044/index.html">Créativité sur iPad et iPhone</a></li>
<li><a href="../fr454046/index.html">La motivation. Faites-le vous-même</a></li>
<li><a href="../fr454048/index.html">Encore une fois, des centaines de milliers de paiements effectués par des citoyens au STSI et au FSSP étaient du domaine public</a></li>
<li><a href="../fr454052/index.html">"Ahh, patron, chapeau parlant!" - casque intelligent pour les productions</a></li>
<li><a href="../fr454058/index.html">Outil pratique pour mesurer le code C #</a></li>
<li><a href="../fr454060/index.html">Un regard inattendu sur les circuits asynchrones indépendants de la vitesse</a></li>
<li><a href="../fr454062/index.html">Téléphone d'entreprise - comme un couteau suisse: pour l'inventaire, le chat, les appels d'assistance et les demandes de renseignements</a></li>
<li><a href="../fr454064/index.html">L'histoire inédite de l'IA</a></li>
<li><a href="../fr454066/index.html">Comment arrêter d'oublier les index et commencer à vérifier le plan d'exécution dans les tests</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>