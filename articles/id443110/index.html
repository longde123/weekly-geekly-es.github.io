<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòµ üëßüèø ü¶ä Konfigurasikan Kubernetes HA cluster pada bare metal dengan GlusterFS & MetalLB. Bagian 2/3 üö∂üèæ üëèüèø üò∫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagian 1/3 di sini 
 Bagian 3/3 di sini 


 Halo dan selamat datang kembali! Ini adalah bagian kedua dari artikel tentang pengaturan cluster Kubernete...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konfigurasikan Kubernetes HA cluster pada bare metal dengan GlusterFS & MetalLB. Bagian 2/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Bagian 1/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>di sini</strong></a> <br>  <strong>Bagian 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>di sini</strong></a> </p><br><p>  Halo dan selamat datang kembali!  Ini adalah bagian kedua dari artikel tentang pengaturan cluster Kubernetes pada logam kosong.  Sebelumnya kami mengkonfigurasi Kubernetes HA cluster menggunakan etcd eksternal, master-master dan load balancing.  Nah, sekarang saatnya untuk mengatur lingkungan dan utilitas tambahan untuk membuat cluster lebih berguna dan sedekat mungkin dengan keadaan kerja. </p><br><p>  Pada bagian artikel ini, kami akan fokus pada konfigurasi penyeimbang beban internal layanan cluster - ini akan menjadi MetalLB.  Kami juga akan menginstal dan mengkonfigurasi penyimpanan file terdistribusi antara node yang bekerja kami.  Kami akan menggunakan GlusterFS untuk volume persisten yang tersedia di Kubernetes. <br>  Setelah menyelesaikan semua langkah, diagram klaster kami akan terlihat seperti ini: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Atur MetalLB sebagai penyeimbang beban internal. </h3><br><p>  Beberapa kata tentang MetalLB, langsung dari halaman dokumen: </p><br><blockquote> MetalLB adalah implementasi penyeimbang beban untuk kluster logam kosong Kubernetes dengan protokol routing standar. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes</a> tidak menawarkan implementasi penyeimbang beban jaringan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">jenis layanan LoadBalancer</a> ) untuk bare metal.  Semua opsi implementasi Network LB yang disertakan dengan Kubernet adalah middleware, dan mengakses berbagai platform IaaS (GCP, AWS, Azure, dll.).  Jika Anda tidak bekerja pada platform yang didukung oleh IaaS (GCP, AWS, Azure, dll.), LoadBalancer akan tetap dalam status "siaga" untuk jangka waktu tidak terbatas setelah pembuatan. <br><br>  Operator server BM memiliki dua alat yang kurang efektif untuk memasukkan lalu lintas pengguna ke dalam kelompok mereka, NodePort dan layanan externalIPs.  Kedua opsi ini memiliki kelemahan produksi yang signifikan, yang mengubah kluster BM menjadi warga kelas dua di ekosistem Kubernetes. <br><br>  MetalLB berupaya untuk memperbaiki ketidakseimbangan ini dengan menawarkan implementasi Network LB yang terintegrasi dengan peralatan jaringan standar, sehingga layanan eksternal pada kluster BM juga "hanya bekerja" pada kecepatan maksimum. </blockquote><p>  Dengan demikian, menggunakan alat ini, kami meluncurkan layanan di kluster Kubernetes menggunakan penyeimbang beban, yang banyak terima kasih kepada tim MetalLB.  Proses pengaturannya sangat sederhana dan mudah. </p><br><p>  Sebelumnya dalam contoh, kami memilih subnet 192.168.0.0/24 untuk kebutuhan cluster kami.  Sekarang ambil beberapa subnet ini untuk load balancer di masa depan. </p><br><p>  Kami memasuki sistem mesin dengan utilitas <strong>kubectl yang</strong> dikonfigurasi dan menjalankan: </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  Ini akan menggunakan MetalLB di cluster, di <code>metallb-system</code> .  Pastikan semua komponen MetalLB berfungsi dengan baik: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Sekarang konfigurasikan MetalLB menggunakan configmap.  Dalam contoh ini, kami menggunakan kustomisasi Layer 2. Untuk informasi tentang opsi kustomisasi lainnya, lihat dokumentasi MetalLB. </p><br><p>  Buat file <strong>metallb-config.yaml</strong> di direktori mana pun di dalam rentang IP yang dipilih dari subnet dari kluster kami: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p>  Dan terapkan pengaturan ini: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  Periksa dan ubah configmap nanti jika perlu: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  Sekarang kami memiliki penyeimbang muatan lokal yang dikonfigurasi sendiri.  Mari kita lihat cara kerjanya, menggunakan layanan Nginx sebagai contoh. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Kemudian buat uji coba penyebaran dan layanan Nginx: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  Dan sekarang - periksa hasilnya: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Membuat 3 Nginx pods, seperti yang kami tunjukkan dalam penerapan sebelumnya.  Layanan Nginx akan mengarahkan lalu lintas ke semua pod ini sesuai dengan skema balancing siklik.  Dan Anda juga dapat melihat IP eksternal yang diterima dari penyeimbang beban MetalLB kami. </p><br><p>  Sekarang cobalah menggulung ke alamat IP 192.168.0.240 dan Anda akan melihat halaman Nginx index.html.  Ingatlah untuk menghapus penyebaran uji dan layanan Nginx. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Baiklah, itu saja dengan MetalLB, mari kita lanjutkan - kami akan mengonfigurasi GlusterFS untuk volume Kubernetes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Mengkonfigurasi GlusterFS dengan Heketi pada node yang berfungsi. </h3><br><p>  Faktanya, kluster Kubernetes tidak dapat digunakan tanpa volume di dalamnya.  Seperti yang Anda tahu, perapian itu fana, yaitu  mereka dapat dibuat dan dihapus kapan saja.  Semua data di dalamnya akan hilang.  Dengan demikian, dalam sebuah cluster nyata, penyimpanan terdistribusi diperlukan untuk memastikan pertukaran pengaturan dan data antara node dan aplikasi di dalamnya. </p><br><p>  Di Kubernetes, volume tersedia dalam berbagai cara, pilih yang Anda inginkan.  Dalam contoh ini, saya akan menunjukkan cara membuat penyimpanan GlusterFS untuk aplikasi internal apa pun, seperti volume persisten.  Sebelumnya, saya menggunakan "sistem" instalasi GlusterFS untuk semua simpul kerja Kubernet untuk ini, dan kemudian cukup membuat volume hostPath di direktori GlusterFS. </p><br><p>  Sekarang kami memiliki alat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><strong>Heketi</strong></a> berguna baru. </p><br><p>  Beberapa kata dari dokumentasi Heketi: </p><br><blockquote>  Tenang infrastruktur manajemen volume untuk GlusterFS. <br><br>  Heketi menawarkan antarmuka manajemen yang tenang yang dapat digunakan untuk mengelola siklus hidup volume GlusterFS.  Berkat Heketi, layanan cloud seperti OpenStack Manila, Kubernetes, dan OpenShift dapat secara dinamis menyediakan volume GlusterFS dengan segala jenis keandalan yang didukung.  Heketi secara otomatis menentukan lokasi blok dalam sebuah cluster, menyediakan lokasi blok dan replika mereka di berbagai area kegagalan.  Heketi juga mendukung sejumlah cluster GlusterFS, memungkinkan layanan cloud untuk menawarkan penyimpanan file online, bukan hanya satu cluster GlusterFS. </blockquote><p>  Kedengarannya bagus, dan di samping itu, alat ini akan membawa kluster VM kami lebih dekat ke kluster cloud Kubernetes yang besar.  Pada akhirnya, Anda akan dapat membuat <strong>PersistentVolumeClaims</strong> , yang akan dihasilkan secara otomatis, dan banyak lagi. </p><br><p>  Anda dapat mengambil hard drive sistem tambahan untuk mengkonfigurasi GlusterFS atau hanya membuat beberapa perangkat blok dummy.  Dalam contoh ini, saya akan menggunakan metode kedua. </p><br><p>  Buat perangkat blok dummy di ketiga node yang bekerja: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  Anda akan mendapatkan file berukuran sekitar 10 GB.  Kemudian gunakan <strong>losetup</strong> - untuk menambahkannya ke node ini, sebagai perangkat loopback: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Harap dicatat: jika Anda sudah memiliki semacam perangkat loopback 0, maka Anda harus memilih nomor lainnya.</em> </blockquote><p>  Saya mengambil waktu dan mencari tahu mengapa Heketi tidak ingin bekerja dengan baik.  Oleh karena itu, untuk mencegah masalah dalam konfigurasi di masa mendatang, pertama-tama pastikan bahwa kita telah memuat <strong>modul</strong> kernel <strong>dm_thin_pool</strong> dan menginstal paket <strong>glusterfs-client</strong> pada semua node yang berfungsi. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Nah, sekarang Anda membutuhkan file <strong>/ home / gluster / image</strong> dan perangkat <strong>/ dev / loop0 untuk</strong> hadir di semua node yang berfungsi.  Ingatlah untuk membuat layanan systemd yang secara otomatis akan memulai <strong>losetup</strong> dan <strong>modprobe</strong> setiap kali server ini boot. </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  Dan nyalakan: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  Pekerjaan persiapan selesai, dan kami siap untuk menyebarkan GlusterFS dan Heketi ke cluster kami.  Untuk ini, saya akan menggunakan <a href="">panduan</a> keren ini.  Sebagian besar perintah diluncurkan dari komputer kontrol eksternal, dan perintah yang sangat kecil diluncurkan dari sembarang master node di dalam cluster. </p><br><p>  Pertama, salin repositori dan buat DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Sekarang mari kita tandai tiga simpul kerja kami untuk GlusterFS;  setelah memberi label, pod GlusterFS akan dibuat: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Sekarang buat akun layanan Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  Kami menyediakan untuk akun layanan ini kemampuan untuk mengelola pod gluster.  Untuk melakukan ini, buat fungsi kluster yang diperlukan untuk akun layanan kami yang baru dibuat: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Sekarang mari kita buat kunci rahasia Kubernetes yang memblokir konfigurasi instance Heketi kita: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Buat sumber pertama di bawah Heketi, yang kami gunakan untuk operasi pengaturan pertama dan kemudian hapus: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  Setelah membuat dan memulai layanan Bootstrap Heketi, kita perlu beralih ke salah satu node master kami, di sana kami akan menjalankan beberapa perintah, karena simpul kontrol eksternal kami tidak ada di dalam kluster kami, jadi kami tidak dapat mengakses pod yang berfungsi dan jaringan internal kluster. </p><br><p>  Pertama, mari unduh utilitas heketi-client dan salin ke folder system bin: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Sekarang temukan alamat IP pod heketi dan ekspor sebagai variabel sistem: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Sekarang mari kita beri Heketi informasi tentang cluster GlusterFS yang harus dikelola.  Kami menyediakannya melalui file topologi.  Topologi adalah manifes JSON dengan daftar semua node, disk, dan kluster yang digunakan oleh GlusterFS. </p><br><blockquote>  CATATAN  Pastikan bahwa <code>hostnames/manage</code> menunjukkan nama yang tepat, seperti di bagian <code>kubectl get node</code> , dan bahwa <code>hostnames/storage</code> adalah alamat IP dari node penyimpanan. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  Kemudian unduh file ini: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Selanjutnya, kami menggunakan Heketi untuk menyediakan volume untuk menyimpan database.  Nama tim agak aneh, tetapi semuanya teratur.  Juga buat repositori heketi: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  Ini semua adalah perintah yang perlu Anda jalankan dari master node.  Mari kita kembali ke node kontrol dan melanjutkan dari sana;  Pertama-tama, pastikan bahwa perintah yang terakhir dijalankan berhasil dijalankan: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  Dan pekerjaan heketi-storage-copy-job selesai. </p><br><blockquote>  Jika saat ini tidak ada paket <strong>klien-glusterfs</strong> yang diinstal yang diinstal pada node kerja Anda, maka kesalahan terjadi. </blockquote><p>  Saatnya untuk menghapus file instalasi Heketi Bootstrap dan melakukan sedikit pembersihan: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  Pada tahap terakhir, kita perlu membuat salinan Heketi jangka panjang: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  Jika saat ini tidak ada paket klien-glusterfs yang diinstal yang diinstal pada node kerja Anda, maka kesalahan terjadi.  Dan kita hampir selesai, sekarang basis data Heketi disimpan dalam volume GlusterFS dan tidak diatur ulang setiap kali perapian Heketi dihidupkan ulang. </p><br><p>  Untuk mulai menggunakan cluster GlusterFS dengan alokasi sumber daya dinamis, kita perlu membuat StorageClass. </p><br><p>  Pertama, mari kita temukan titik akhir penyimpanan Gluster, yang akan diteruskan ke StorageClass sebagai parameter (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Sekarang buat beberapa file: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Gunakan file-file ini untuk membuat kelas dan pvc: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  Kami juga dapat melihat volume PV: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Kami sekarang memiliki volume GlusterFS yang dibuat secara dinamis terkait dengan <strong>PersistentVolumeClaim</strong> , dan kami dapat menggunakan pernyataan ini di subplot apa pun. </p><br><p>  Buat yang sederhana di bawah Nginx dan mengujinya: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Jelajahi di bawah (tunggu beberapa menit, Anda mungkin perlu mengunduh gambar jika belum ada): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Sekarang masuk ke wadah dan buat file index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  Anda perlu menemukan alamat IP internal perapian dan menggulungnya dari simpul master apa pun: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  Dengan demikian, kita cukup menguji volume persisten baru kita. </p><br><blockquote>  Beberapa perintah yang berguna untuk memeriksa cluster GlusterFS baru adalah: <code>heketi-cli cluster list</code> <code>heketi-cli volume list</code> .  Mereka dapat dijalankan di komputer Anda jika <strong>heketi-cli diinstal</strong> .  Dalam contoh ini, ini adalah simpul <strong>master1</strong> . </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  Pada tahap ini, kami berhasil mengatur penyeimbang beban internal dengan penyimpanan file, dan cluster kami sekarang lebih dekat dengan keadaan operasional. </p><br><p>  Pada bagian selanjutnya dari artikel ini, kami akan fokus pada pembuatan sistem pemantauan cluster, dan juga meluncurkan proyek uji di dalamnya untuk menggunakan semua sumber daya yang kami konfigurasi. </p><br><p>  Tetap berkomunikasi dan semua yang terbaik! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id443110/">https://habr.com/ru/post/id443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id443098/index.html">Data ditulis ke disk menggunakan magnet dan laser</a></li>
<li><a href="../id443100/index.html">Menghitung bug di kalkulator Windows</a></li>
<li><a href="../id443102/index.html">Perubahan perilaku sebagai produk: mengapa Marie Kondo mengumpulkan $ 40 juta dengan Sequoia Capital?</a></li>
<li><a href="../id443104/index.html">Hitung ekspresi simbolik dengan angka segitiga fuzzy dalam python</a></li>
<li><a href="../id443106/index.html">Diumumkan USB4: apa yang diketahui tentang standar</a></li>
<li><a href="../id443112/index.html">Apakah Anda yakin dapat mempercayai VPN Anda?</a></li>
<li><a href="../id443114/index.html">Penghargaan DevProject: Pidato Saya di DeveloperWeek 2019</a></li>
<li><a href="../id443120/index.html">Duma Negara akan melanjutkan perjuangan melawan penjualan ilegal kartu SIM</a></li>
<li><a href="../id443122/index.html">Kebocoran 809 juta alamat email layanan Verifikasi.io karena MongoDB terbuka untuk umum</a></li>
<li><a href="../id443124/index.html">React.lazy? Tetapi bagaimana jika Anda tidak memiliki komponen?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>