<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßúüèΩ üåò ‚úåÔ∏è Uma nova realiza√ß√£o de curiosidade em IA. Treinar com uma recompensa que depende da dificuldade de prever o resultado üîí üòà üçë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O progresso no jogo "A Vingan√ßa de Montezuma" foi considerado por muitos como sin√¥nimo de conquistas no estudo de ambientes desconhecidos 

 Desenvolv...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Uma nova realiza√ß√£o de curiosidade em IA. Treinar com uma recompensa que depende da dificuldade de prever o resultado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">O progresso no jogo "A Vingan√ßa de Montezuma" foi considerado por muitos como sin√¥nimo de conquistas no estudo de ambientes desconhecidos</font></i> <br><br>  Desenvolvemos um m√©todo de destila√ß√£o aleat√≥ria em rede (RND) baseado em previs√£o que incentiva agentes de aprendizado refor√ßados a explorar o ambiente por curiosidade.  Este m√©todo excedeu pela primeira vez a m√©dia de resultados humanos no jogo de computador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Montezuma's Revenge"</a> (exceto o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aplicativo</a> an√¥nimo no ICLR, onde o resultado √© pior que o nosso).  <b>O RND demonstra efici√™ncia ultramoderna, encontra periodicamente todas as 24 salas e passa o primeiro n√≠vel sem demonstra√ß√£o preliminar e sem acesso ao estado b√°sico do jogo.</b> <br><a name="habracut"></a><br>  O m√©todo RND estimula a transi√ß√£o de um agente para estados desconhecidos, medindo a complexidade de prever o resultado da sobreposi√ß√£o de uma rede neural aleat√≥ria aleat√≥ria nos dados do estado.  Se a condi√ß√£o n√£o for familiar, √© dif√≠cil prever o resultado final, o que significa que a recompensa √© alta.  O m√©todo pode ser aplicado a qualquer algoritmo de aprendizado por refor√ßo; √© simples de implementar e eficaz para o dimensionamento.  Abaixo est√° um link para a implementa√ß√£o do RND, que reproduz os resultados de nosso artigo. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Texto de um artigo cient√≠fico</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digo</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Resultados na vingan√ßa de Montezuma </h1><br>  Para atingir o objetivo desejado, o agente deve primeiro estudar quais a√ß√µes s√£o poss√≠veis no ambiente e o que constitui progresso em dire√ß√£o ao objetivo.  Muitos sinais de recompensa nos jogos fornecem um curr√≠culo, portanto, mesmo estrat√©gias simples de pesquisa s√£o suficientes para atingir a meta.  No <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho inicial com a apresenta√ß√£o do DQN,</a> o <b>jogo</b> Revenge de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Montezuma</a> foi o <b>√∫nico jogo em que o DQN mostrou o resultado de 0% da pontua√ß√£o m√©dia humana (4700)</b> .  √â improv√°vel que estrat√©gias simples de intelig√™ncia colecionem recompensas e n√£o encontrem mais do que algumas salas no n√≠vel.  Desde ent√£o, o progresso no jogo A Vingan√ßa de Montezuma tem sido visto por muitos como sin√¥nimo de avan√ßos no estudo de ambientes desconhecidos. <br><br>  Progresso significativo foi alcan√ßado em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2016</a> combinando o DQN com um b√¥nus no balc√£o, como resultado, o agente conseguiu encontrar 15 quartos e obter a pontua√ß√£o mais alta de 6600 com uma m√©dia de cerca de 3700. Desde ent√£o, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">melhorias</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">significativas</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no</a> resultado foram alcan√ßadas apenas atrav√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">demonstra√ß√µes</a> de pessoas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">especializadas</a> ou acessando os estados de base do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">emulador</a> . <br><br>  Realizamos um experimento de RND em larga escala com 1024 trabalhadores, obtendo um <b>resultado m√©dio de 10.000 ao longo de 9 partidas</b> e um <b>melhor resultado m√©dio de 14.500</b> .  Em cada caso, o agente encontrou de 20 a 22 quartos.  Al√©m disso, em um lan√ßamento menor, mas mais longo (de 10), o <b>resultado m√°ximo √© 17.500, o que corresponde a passar no primeiro n√≠vel e encontrar todas as 24 salas</b> .  O gr√°fico abaixo compara essas duas experi√™ncias, mostrando o valor m√©dio, dependendo dos par√¢metros de atualiza√ß√£o. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  A visualiza√ß√£o abaixo mostra o progresso do experimento em uma escala menor.  O agente, sob a influ√™ncia da curiosidade, abre novas salas e encontra maneiras de marcar pontos. Durante o treinamento, essa recompensa externa o obriga a retornar a essas salas mais tarde. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">As salas descobertas pelo agente e o resultado m√©dio durante o treinamento.</font></i>  <i><font color="gray">O grau de transpar√™ncia da sala corresponde a quantas vezes em cada 10 passagens do agente foram detectadas.</font></i>  <i><font color="gray"><a href="">V√≠deo</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Estudo de aprendizagem em larga escala baseado na curiosidade</a> </h1><br>  Antes de desenvolver o RND, n√≥s, juntamente com a equipe da Universidade da Calif√≥rnia em Berkeley, exploramos o aprendizado sem nenhuma recompensa ambiental.  A curiosidade fornece uma maneira mais f√°cil de ensinar os agentes a interagir com <i>qualquer</i> ambiente, em vez de usar uma fun√ß√£o de recompensa especialmente projetada para uma tarefa espec√≠fica, que ainda n√£o √© um fato que corresponde √† solu√ß√£o do problema.  Em projetos como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Universe</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Malmo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gym</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gym Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CommAI</a> , um grande n√∫mero de ambientes simulados √© aberto para o agente por meio de uma interface padronizada.  Um agente que usa uma fun√ß√£o de recompensa generalizada que n√£o √© espec√≠fica para um ambiente espec√≠fico pode adquirir um n√≠vel b√°sico de compet√™ncia em uma ampla variedade de ambientes.  Isso permite que ele determine um comportamento √∫til, mesmo na aus√™ncia de recompensas elaboradas. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Texto de um artigo cient√≠fico</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">c√≥digo</a> </blockquote><br>  Em ambientes de treinamento padr√£o, com refor√ßo a cada passo discreto, o agente envia a a√ß√£o ao ambiente e reage, dando ao agente uma nova observa√ß√£o, recompensa pela transi√ß√£o e um indicador do final do epis√≥dio.  Em nosso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo anterior,</a> configuramos o ambiente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">para produzir</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">apenas a</a> seguinte observa√ß√£o.  L√°, o agente estuda o modelo preditivo do pr√≥ximo estado com base em sua experi√™ncia e usa o erro de previs√£o como recompensa interna.  Como resultado, ele √© atra√≠do pela imprevisibilidade.  Por exemplo, uma altera√ß√£o na conta do jogo √© recompensada apenas se a conta for exibida na tela e a altera√ß√£o for dif√≠cil de prever.  Um agente, via de regra, encontra intera√ß√µes √∫teis com novos objetos, pois os resultados dessas intera√ß√µes geralmente s√£o mais dif√≠ceis de prever do que outros aspectos do ambiente. <br><br>  Como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">outros</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisadores</a> , tentamos evitar modelar todos os aspectos do ambiente, independentemente de serem relevantes ou n√£o, escolhendo os recursos de observa√ß√£o para modelagem.  Surpreendentemente, descobrimos que mesmo fun√ß√µes aleat√≥rias funcionam bem. <br><br><h1>  O que os agentes curiosos fazem? </h1><br>  Testamos nosso agente em mais de 50 ambientes diferentes e observamos uma s√©rie de compet√™ncias, desde a√ß√µes aparentemente aleat√≥rias at√© intera√ß√£o consciente com o ambiente.  Para nossa surpresa, em alguns casos, o agente conseguiu passar pelo jogo, apesar de n√£o ter sido informado do gol por meio de uma recompensa externa. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Remunera√ß√£o interna no in√≠cio do treinamento</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">O salto na recompensa interna na primeira passagem do n√≠vel</font></i> <br><br>  <b>Breakout</b> - salta na recompensa interna quando o agente v√™ uma nova configura√ß√£o de blocos em um est√°gio inicial do treinamento e quando o n√≠vel passa pela primeira vez ap√≥s o treinamento por v√°rias horas. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - treinamos o agente para controlar as duas plataformas simultaneamente e ele aprendeu a manter a bola no jogo, o que levou a lutas prolongadas.  Mesmo durante o treinamento contra a IA no jogo, o agente tentou maximizar o jogo, e n√£o vencer. <br><br>  <b><a href="">Boliche</a></b> - o agente aprendeu a jogar melhor do que outros agentes treinados diretamente para maximizar a recompensa externa.  Achamos que isso acontece porque o agente √© atra√≠do pelo piscar dificilmente previs√≠vel do placar ap√≥s os lances. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - A recompensa interna est√° particularmente bem alinhada com o objetivo do jogo: progress√£o de n√≠vel.  O agente √© recompensado por procurar novas √°reas, pois os detalhes da √°rea rec√©m-encontrada n√£o podem ser previstos.  Como resultado, o agente descobriu 11 n√≠veis, encontrou salas secretas e at√© mesmo chefes derrotados. <br><br><h1>  Problema de TV barulhento </h1><br>  Como jogador de uma m√°quina ca√ßa-n√≠queis, atra√≠do por resultados aleat√≥rios, o agente √†s vezes cai na armadilha de sua curiosidade como resultado do "barulhento problema de TV".  O agente encontra uma fonte de aleatoriedade no ambiente e continua a observ√°-la, sempre experimentando uma alta recompensa interna por essas transi√ß√µes.  Um exemplo dessa armadilha √© assistir a uma televis√£o que produz ru√≠do est√°tico.  Demonstramos isso literalmente, colocando o agente no labirinto do Unity com uma TV que reproduz canais aleat√≥rios. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agente em um labirinto com uma TV barulhenta</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agente em um labirinto sem uma TV barulhenta</font></i> <br><br>  Teoricamente, o problema de uma TV barulhenta √© realmente s√©rio, mas ainda esper√°vamos que em muitos ambientes determin√≠sticos como a Vingan√ßa de Montezuma, a curiosidade levasse o agente a encontrar salas e interagir com objetos.  Tentamos v√°rias op√ß√µes para prever o pr√≥ximo estado com base na curiosidade, combinando um b√¥nus de pesquisa com uma conta de jogo. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  Nesses experimentos, o agente controla o ambiente atrav√©s de um controlador de ru√≠do, que com alguma probabilidade repete a √∫ltima a√ß√£o em vez da atual.  Essa configura√ß√£o com a√ß√µes repetitivas e "aderentes" foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">proposta</a> como uma pr√°tica recomendada para treinar agentes em jogos totalmente determin√≠sticos, como o Atari, para impedir a memoriza√ß√£o.  A√ß√µes "aderentes" tornam a transi√ß√£o de sala em sala imprevis√≠vel. <br><br><h1>  Destila√ß√£o de rede aleat√≥ria </h1><br>  Como prever o pr√≥ximo estado √© inerentemente suscet√≠vel ao problema de uma TV barulhenta, identificamos as seguintes fontes relevantes de erros de previs√£o: <br><br><ul><li>  <b>Fator 1</b> .  O erro de previs√£o √© alto se o preditor falhar na generaliza√ß√£o dos exemplos considerados anteriormente.  Nova experi√™ncia corresponde a um erro de previs√£o alto. </li><li>  <b>Fator 2</b> .  O erro de previs√£o √© alto devido √† meta de previs√£o estoc√°stica. </li><li>  <b>Fator 3</b> .  O erro de previs√£o √© alto devido √† falta de informa√ß√µes necess√°rias para a previs√£o ou porque a classe do modelo de previs√£o √© muito limitada para atender √† complexidade da fun√ß√£o objetivo. </li></ul><br>  Determinamos que o fator 1 √© uma fonte √∫til de erros porque quantifica a novidade da experi√™ncia, enquanto os fatores 2 e 3 levam ao problema de uma TV barulhenta.  Para evitar os fatores 2 e 3, desenvolvemos o RND - um novo b√¥nus de pesquisa baseado na <b>previs√£o da entrega de uma rede neural constante e inicializada aleatoriamente no pr√≥ximo estado, levando em considera√ß√£o o pr√≥ximo estado em si</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  A intui√ß√£o sugere que os modelos preditivos apresentam um baixo erro ao prever as condi√ß√µes em que ela foi treinada.  Em particular, as previs√µes do agente sobre a emiss√£o de uma rede neural inicializada aleatoriamente ser√£o menos precisas nos novos estados do que nos estados que o agente costumava conhecer antes.  A vantagem de usar o problema de previs√£o sint√©tica √© que ele pode ser determin√≠stico (fator de desvio 2) e, dentro da classe de fun√ß√µes, o preditor pode escolher um preditor da mesma arquitetura da rede de destino (fator de desvio 3).  Isso elimina o problema de RND de uma TV barulhenta. <br><br>  Combinamos o b√¥nus de pesquisa com recompensas externas por meio de uma variante da otimiza√ß√£o de pol√≠tica mais pr√≥xima - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Proximal Policy Optimization</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PPO</a> ), que usa <b>dois valores de valor para dois fluxos de recompensa</b> .  Isso permite que voc√™ use descontos diferentes para recompensas diferentes e combine recompensas epis√≥dicas e n√£o epis√≥dicas.  <b>Devido a essa flexibilidade adicional, nosso melhor agente geralmente encontra 22 de 24 quartos no primeiro n√≠vel na Vingan√ßa de Montezuma e √†s vezes passa no primeiro n√≠vel depois de encontrar os dois quartos restantes.</b>  O mesmo m√©todo demonstra desempenho recorde nos jogos Venture e Gravitar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  A visualiza√ß√£o abaixo mostra um gr√°fico da recompensa interna no epis√≥dio de Vingan√ßa de Montezuma, onde o agente encontra a tocha pela primeira vez. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  A implementa√ß√£o competente √© importante </h1><br>  Para selecionar um bom algoritmo, √© importante considerar considera√ß√µes gerais, como a suscetibilidade ao problema de uma TV com ru√≠do.  No entanto, descobrimos que mudan√ßas aparentemente muito pequenas em nosso algoritmo simples afetam bastante sua efic√°cia: de um agente que n√£o pode sair da primeira sala a um agente que passa pelo primeiro n√≠vel.  Para adicionar estabilidade ao treinamento, evitamos a satura√ß√£o de caracter√≠sticas e trouxemos recompensas internas para um intervalo previs√≠vel.  Tamb√©m observamos <b>melhorias significativas na efic√°cia do RND toda vez que encontramos e corrigimos um bug</b> (o nosso favorito inclu√≠a zerar aleatoriamente a matriz, o que levou ao fato de que as recompensas externas eram consideradas n√£o epis√≥dicas; s√≥ percebemos isso depois de pensar na fun√ß√£o de valor externo , que parecia suspeitosamente peri√≥dico).  A corre√ß√£o desses detalhes tornou-se uma parte importante da obten√ß√£o de alto desempenho, mesmo ao usar algoritmos conceitualmente semelhantes ao trabalho anterior.  Essa √© uma das raz√µes pelas quais √© melhor escolher algoritmos simples sempre que poss√≠vel. <br><br><h1>  Trabalho futuro </h1><br>  Oferecemos as seguintes √°reas para pesquisas adicionais: <br><br><ul><li>  An√°lise das vantagens dos diferentes m√©todos de pesquisa e busca de novas formas de combin√°-los. </li><li>  Treinar um agente curioso em muitos ambientes diferentes sem recompensas e aprender a transferir para um ambiente de destino com recompensas. </li><li>  Intelig√™ncia global, incluindo solu√ß√µes coordenadas a longo prazo. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt428776/">https://habr.com/ru/post/pt428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt428766/index.html">O resumo de materiais frescos do mundo do front-end da √∫ltima semana n ¬∞ 337 (29 de outubro a 4 de novembro de 2018)</a></li>
<li><a href="../pt428768/index.html">Em tr√™s artigos sobre m√≠nimos quadrados: programa educacional sobre teoria das probabilidades</a></li>
<li><a href="../pt428770/index.html">Macros de teclado para tarefas di√°rias</a></li>
<li><a href="../pt428772/index.html">Democratiza√ß√£o dos dados do Uber</a></li>
<li><a href="../pt428774/index.html">Firewall GPS para data centers - por que √© necess√°rio e como funciona</a></li>
<li><a href="../pt428778/index.html">Veja o invis√≠vel. Infravermelho pr√≥ximo (0,9-1,7Œºm)</a></li>
<li><a href="../pt428786/index.html">Processador qu√¢ntico baseado em resson√¢ncia de rota√ß√£o e manipula√ß√µes com um sistema singlet-tripleto</a></li>
<li><a href="../pt428788/index.html">Sob o cap√¥ do Bitfury Clarke - como nosso novo chip de minera√ß√£o funciona</a></li>
<li><a href="../pt428790/index.html">Estamos escrevendo um chat de bot para o VKontakte em python usando longpoll. Parte Dois La√ßos duplos, exce√ß√µes e outras heresias</a></li>
<li><a href="../pt428792/index.html">Novo chip Apple T2 dificulta a audi√ß√£o atrav√©s do microfone embutido no laptop</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>