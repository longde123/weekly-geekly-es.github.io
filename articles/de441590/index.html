<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçç üôÖüèæ ‚ûó Die Entwicklung der Cluster-Interaktion. Wie wir ActiveMQ und Hazelcast implementiert haben üìÉ ‚ñ™Ô∏è üêù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den letzten 7 Jahren habe ich zusammen mit dem Team den Kern des Miro-Produkts (ex-RealtimeBoard) unterst√ºtzt und weiterentwickelt: Client-Server- ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Entwicklung der Cluster-Interaktion. Wie wir ActiveMQ und Hazelcast implementiert haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/441590/">  In den letzten 7 Jahren habe ich zusammen mit dem Team den Kern des Miro-Produkts (ex-RealtimeBoard) unterst√ºtzt und weiterentwickelt: Client-Server- und Cluster-Interaktion mit der Datenbank. <br><br>  Wir haben Java mit verschiedenen Bibliotheken an Bord.  Alles wird au√üerhalb des Containers √ºber das Maven-Plugin gestartet.  Es basiert auf der Plattform unserer Partner, die es uns erm√∂glicht, mit der Datenbank und den Flows zu arbeiten, die Client-Server-Interaktion zu verwalten usw.  DB - Redis und PostgreSQL (mein Kollege <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hat dar√ºber geschrieben, wie wir von einer Datenbank in eine andere wechseln</a> ). <br><br>  In Bezug auf die Gesch√§ftslogik enth√§lt die Anwendung: <br><br><ul><li>  mit benutzerdefinierten Boards und deren Inhalten arbeiten; </li><li>  Funktionen zur Benutzerregistrierung, Erstellung und Verwaltung von Boards; </li><li>  Benutzerdefinierter Ressourcengenerator.  Beispielsweise werden gro√üe Bilder optimiert, die in die Anwendung hochgeladen werden, damit sie f√ºr unsere Kunden nicht langsamer werden. </li><li> viele Integrationen mit Diensten von Drittanbietern. </li></ul><br>  Im Jahr 2011, als wir gerade anfingen, befand sich der gesamte Miro auf demselben Server.  Alles war drauf: Nginx, auf dem PHP f√ºr eine Site gedreht wurde, eine Java-Anwendung und Datenbanken. <br><br>  Das entwickelte Produkt, die Anzahl der Benutzer und der Inhalt, den sie den Boards hinzuf√ºgten, nahmen zu, sodass auch die Belastung des Servers zunahm.  Aufgrund der gro√üen Anzahl von Anwendungen auf unserem Server konnten wir zu diesem Zeitpunkt nicht verstehen, was genau die Last ergibt, und konnten sie dementsprechend nicht optimieren. Um dies zu beheben, haben wir alles in verschiedene Server aufgeteilt und einen Webserver, einen Server, erhalten mit unserem Anwendungs- und Datenbankserver. <br><br>  Leider traten nach einiger Zeit wieder Probleme auf, da die Belastung der Anwendung weiter zunahm.  Dann haben wir dar√ºber nachgedacht, wie die Infrastruktur skaliert werden kann. <br><br><img src="https://habrastorage.org/webt/_5/zq/_3/_5zq_3c16pydjklapiqamfzyxcg.png"><br><br>  Als N√§chstes werde ich auf die Schwierigkeiten eingehen, die bei der Entwicklung von Clustern und der Skalierung von Java-Anwendungen und -Infrastruktur aufgetreten sind. <a name="habracut"></a><br><br><h2>  Infrastruktur horizontal skalieren </h2><br>  Wir haben zun√§chst Metriken gesammelt: die Verwendung von Speicher und CPU, die Zeit, die zum Ausf√ºhren von Benutzerabfragen ben√∂tigt wird, die Verwendung von Systemressourcen und die Arbeit mit der Datenbank.  Aus den Metriken ging hervor, dass die Generierung von Benutzerressourcen ein unvorhersehbarer Prozess war.  Wir k√∂nnen den Prozessor zu 100% laden und einige zehn Sekunden warten, bis alles erledigt ist.  Benutzeranfragen f√ºr Boards f√ºhrten manchmal auch zu einer unerwarteten Belastung.  Zum Beispiel, wenn ein Benutzer tausend Widgets ausw√§hlt und sie spontan verschiebt. <br><br>  Wir begannen dar√ºber nachzudenken, wie diese Teile des Systems skaliert werden k√∂nnen, und kamen zu offensichtlichen L√∂sungen. <br><br>  <b>Skalieren Sie die Arbeit mit Boards und Inhalten</b> .  Der Benutzer √∂ffnet die Karte folgenderma√üen: Der Benutzer √∂ffnet den Client ‚Üí gibt an, welche Karte er √∂ffnen m√∂chte ‚Üí stellt eine Verbindung zum Server her ‚Üí ein Stream wird auf dem Server erstellt ‚Üí alle Benutzer dieser Karte stellen eine Verbindung zu einem Stream her ‚Üí jede √Ñnderung oder Erstellung des Widgets erfolgt innerhalb dieses Streams.  Es stellt sich heraus, dass alle Arbeiten mit der Karte streng durch den Fluss begrenzt sind, was bedeutet, dass wir diese Fl√ºsse auf die Server verteilen k√∂nnen. <br><br>  <b>Skalieren Sie die Generierung von Benutzerressourcen</b> .  Wir k√∂nnen den Server herausnehmen, um Ressourcen separat zu generieren, und er empf√§ngt Nachrichten zur Generierung und antwortet dann, dass alles generiert wird. <br><br>  Alles scheint einfach zu sein.  Sobald wir uns jedoch eingehender mit diesem Thema befassten, stellte sich heraus, dass wir zus√§tzlich einige indirekte Probleme l√∂sen mussten.  Wenn beispielsweise ein kostenpflichtiges Abonnement f√ºr Benutzer abl√§uft, m√ºssen wir sie dar√ºber informieren, unabh√§ngig davon, auf welchem ‚Äã‚ÄãBoard sie sich befinden.  Wenn der Benutzer die Version der Ressource aktualisiert hat, m√ºssen Sie sicherstellen, dass der Cache auf allen Servern korrekt geleert ist und wir die richtige Version angeben. <br><br>  Wir haben Systemanforderungen identifiziert.  Der n√§chste Schritt besteht darin, zu verstehen, wie dies in die Praxis umgesetzt werden kann.  Tats√§chlich brauchten wir ein System, mit dem die Server im Cluster miteinander kommunizieren k√∂nnen und auf dessen Grundlage wir alle unsere Ideen verwirklichen k√∂nnen. <br><br><h2>  Der erste sofort einsatzbereite Cluster </h2><br>  Wir haben die erste Version des Systems nicht ausgew√§hlt, da sie bereits teilweise in der von uns verwendeten Partnerplattform implementiert war.  Darin waren alle Server √ºber TCP miteinander verbunden, und √ºber diese Verbindung konnten wir RPC-Nachrichten gleichzeitig an einen oder alle Server senden. <br><br>  Zum Beispiel haben wir drei Server, die √ºber TCP miteinander verbunden sind, und in Redis haben wir eine Liste dieser Server.  Wir starten einen neuen Server im Cluster ‚Üí er f√ºgt sich der Liste in Redis hinzu ‚Üí liest die Liste, um Informationen zu allen Servern im Cluster zu erhalten ‚Üí stellt eine Verbindung zu allen her. <br><br><img src="https://habrastorage.org/webt/yj/9c/hv/yj9chvfavcbrixqnn_12jho2k7k.png"><br><br>  Basierend auf RPC wurde bereits die Unterst√ºtzung f√ºr das Leeren des Caches und das Umleiten von Benutzern auf den gew√ºnschten Server implementiert.  Wir mussten eine Generation von Benutzerressourcen erstellen und Benutzer dar√ºber informieren, dass etwas passiert war (z. B. war ein Konto abgelaufen).  Um Ressourcen zu generieren, haben wir einen beliebigen Server ausgew√§hlt und ihm eine Anfrage zur Generierung gesendet. F√ºr Benachrichtigungen √ºber den Ablauf eines Abonnements haben wir einen Befehl an alle Server gesendet, in der Hoffnung, dass die Nachricht das Ziel erreicht. <br><br><h3>  Der Server selbst bestimmt, an wen die Nachricht gesendet werden soll. </h3><br>  Es klingt wie eine Funktion, kein Problem.  Der Server konzentriert sich jedoch nur auf die Verbindung zu einem anderen Server.  Wenn es Verbindungen gibt, gibt es einen Kandidaten zum Senden einer Nachricht. <br><br>  Das Problem ist, dass Server Nummer 1 nicht wei√ü, dass Server Nummer 4 derzeit unter hoher Last steht und nicht schnell genug antworten kann.  Infolgedessen werden Server-1-Anforderungen langsamer verarbeitet als sie k√∂nnten. <br><br><img src="https://habrastorage.org/webt/g7/mw/ez/g7mwezzba78vsgcvx8mpa_fzdou.png"><br><br><h3>  Der Server wei√ü nicht, dass der zweite Server eingefroren ist </h3><br>  Was aber, wenn der Server nicht nur stark ausgelastet ist, sondern im Allgemeinen einfriert?  Au√üerdem h√§ngt es so, dass es nicht mehr zum Leben erweckt wird.  Zum Beispiel habe ich den gesamten verf√ºgbaren Speicher ersch√∂pft. <br><br>  In diesem Fall wei√ü Server 1 nicht, wo das Problem liegt, und wartet weiterhin auf eine Antwort.  Die verbleibenden Server im Cluster wissen auch nichts √ºber die Situation mit Server Nr. 4, daher senden sie viele Nachrichten an Server Nr. 4 und warten auf eine Antwort.  So wird es sein, bis Server Nummer 4 stirbt. <br><br><img src="https://habrastorage.org/webt/5y/et/pg/5yetpgodx1zi38he2nchwnnniiq.png"><br><br>  Was zu tun ist?  Wir k√∂nnen dem System unabh√§ngig eine Serverstatuspr√ºfung hinzuf√ºgen.  Oder wir k√∂nnen Nachrichten von "kranken" Servern auf "gesunde" umleiten.  All dies wird den Entwicklern zu viel Zeit kosten.  Im Jahr 2012 hatten wir wenig Erfahrung in diesem Bereich und suchten nach vorgefertigten L√∂sungen f√ºr alle unsere Probleme gleichzeitig. <br><br><h2>  Nachrichtenbroker.  Activemq </h2><br>  Wir haben uns entschlossen, in Richtung Message Broker zu gehen, um die Kommunikation zwischen Servern korrekt zu konfigurieren.  Sie entschieden sich f√ºr ActiveMQ, da der Empfang von Nachrichten auf dem Consumer zu einem bestimmten Zeitpunkt konfiguriert werden kann.  Wir haben diese Gelegenheit zwar nie genutzt, also k√∂nnten wir uns zum Beispiel f√ºr RabbitMQ entscheiden. <br><br>  Infolgedessen haben wir unser gesamtes Clustersystem auf ActiveMQ √ºbertragen.  Was gab es: <br><br><ol><li>  Der Server bestimmt nicht mehr selbst, an wen die Nachricht gesendet wird, da alle Nachrichten die Warteschlange durchlaufen. </li><li>  Konfigurierte Fehlertoleranz.  Um die Warteschlange zu lesen, k√∂nnen Sie nicht einen, sondern mehrere Server ausf√ºhren.  Selbst wenn einer von ihnen f√§llt, funktioniert das System weiter. </li><li>  Auf den Servern wurden Rollen angezeigt, mit denen der Server nach Lasttyp aufgeteilt werden konnte.  Beispielsweise kann ein Ressourcengenerator nur eine Verbindung zu einer Warteschlange zum Lesen von Nachrichten herstellen, um Ressourcen zu generieren, und ein Server mit Karten kann eine Verbindung zu einer Warteschlange zum √ñffnen von Karten herstellen. </li><li>  Hat RPC-Kommunikation, d.h.  Jeder Server verf√ºgt √ºber eine eigene private Warteschlange, an die andere Server Ereignisse senden. </li><li>  Sie k√∂nnen Nachrichten √ºber Topic an alle Server senden, mit denen wir Abonnements zur√ºcksetzen. </li></ol><br><br>  Das Schema sieht einfach aus: Alle Server sind mit dem Broker verbunden und verwalten die Kommunikation zwischen ihnen.  Alles funktioniert, Nachrichten werden gesendet und empfangen, Ressourcen werden erstellt.  Aber es gibt neue Probleme. <br><br><h3>  Was tun, wenn alle erforderlichen Server l√ºgen? </h3><br>  Angenommen, Server 3 m√∂chte eine Nachricht senden, um Ressourcen in einer Warteschlange zu generieren.  Er erwartet, dass seine Nachricht verarbeitet wird.  Aber er wei√ü nicht, dass es aus irgendeinem Grund keinen einzigen Empf√§nger der Nachricht gibt.  Beispielsweise st√ºrzten Empf√§nger aufgrund eines Fehlers ab. <br><br>  W√§hrend der gesamten Wartezeit sendet der Server viele Nachrichten mit einer Anfrage, weshalb eine Warteschlange mit Nachrichten angezeigt wird.  Wenn Arbeitsserver angezeigt werden, m√ºssen sie daher zuerst die akkumulierte Warteschlange verarbeiten, was einige Zeit in Anspruch nimmt.  Auf der Benutzerseite f√ºhrt dies dazu, dass das von ihm hochgeladene Bild nicht sofort angezeigt wird.  Er ist nicht bereit zu warten, also verl√§sst er das Brett. <br><br>  Infolgedessen geben wir Serverkapazit√§t f√ºr die Generierung von Ressourcen aus, und niemand ben√∂tigt das Ergebnis. <br><br><img src="https://habrastorage.org/webt/oq/p8/fd/oqp8fd0mdctqarlauh84jdjn8rc.png"><br><br>  Wie kann ich das Problem l√∂sen?  Wir k√∂nnen eine √úberwachung einrichten, die Sie √ºber das Geschehen informiert.  Aber von dem Moment an, in dem die √úberwachung etwas meldet, bis zu dem Moment, in dem wir verstehen, dass unsere Server schlecht sind, wird die Zeit vergehen.  Das passt nicht zu uns. <br><br>  Eine andere Option ist das Ausf√ºhren von Service Discovery oder einer Registrierung von Diensten, die wissen, welche Server mit welchen Rollen ausgef√ºhrt werden.  In diesem Fall erhalten wir sofort eine Fehlermeldung, wenn keine freien Server vorhanden sind. <br><br><h3>  Einige Dienste k√∂nnen nicht horizontal skaliert werden </h3><br>  Dies ist ein Problem unseres fr√ºhen Codes, nicht von ActiveMQ.  Lassen Sie mich Ihnen ein Beispiel zeigen: <br><br><pre><code class="plaintext hljs">Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER);</code> </pre> <br>  Wir haben einen Service f√ºr die Arbeit mit Benutzerrechten auf dem Board: Der Benutzer kann der Eigent√ºmer des Boards oder dessen Editor sein.  Es kann nur ein Eigent√ºmer an der Tafel sein.  Angenommen, wir haben ein Szenario, in dem wir das Eigentum an einem Board von einem Benutzer auf einen anderen √ºbertragen m√∂chten.  In der ersten Zeile erhalten wir den aktuellen Eigent√ºmer des Boards, in der zweiten den Benutzer, der der Herausgeber war, und werden nun zum Eigent√ºmer.  Als jetzigen Eigent√ºmer haben wir die Rolle des HERAUSGEBERS und des ehemaligen Herausgebers die Rolle des EIGENT√úMERS √ºbernommen. <br><br>  Mal sehen, wie dies in einer Multithread-Umgebung funktioniert.  Wenn der erste Thread die EDITOR-Rolle einrichtet und der zweite Thread versucht, den aktuellen EIGENT√úMER zu √ºbernehmen, kann es vorkommen, dass der EIGENT√úMER nicht vorhanden ist, es jedoch zwei EDITOR gibt. <br><br>  Der Grund ist die fehlende Synchronisation.  Wir k√∂nnen das Problem l√∂sen, indem wir einen Synchronisationsblock auf der Karte hinzuf√ºgen. <br><br><pre> <code class="plaintext hljs">synchronized (board) { Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER); }</code> </pre><br>  Diese L√∂sung funktioniert im Cluster nicht.  Die SQL-Datenbank k√∂nnte uns dabei mit Hilfe von Transaktionen helfen.  Aber wir haben Redis. <br><br>  Eine andere L√∂sung besteht darin, dem Cluster verteilte Sperren hinzuzuf√ºgen, sodass die Synchronisierung im gesamten Cluster und nicht nur auf einem Server erfolgt. <br><br><h2>  Ein einzelner Fehlerpunkt beim Betreten der Platine </h2><br>  Das Modell der Interaktion zwischen Client und Server ist statusbehaftet.  Wir m√ºssen also den Status der Karte auf dem Server speichern.  Aus diesem Grund haben wir eine separate Rolle f√ºr Server festgelegt - BoardServer, der Benutzeranforderungen in Bezug auf Boards verarbeitet. <br><br>  Stellen Sie sich vor, wir haben drei BoardServer, von denen einer der wichtigste ist.  Der Benutzer sendet ihm eine Anfrage ‚Äû√ñffne mir die Karte mit der ID = 123‚Äú ‚Üí Der Server pr√ºft in seiner Datenbank, ob die Karte ge√∂ffnet ist und auf welchem ‚Äã‚ÄãServer sie sich befindet.  In diesem Beispiel ist die Karte ge√∂ffnet. <br><br><img src="https://habrastorage.org/webt/ej/kf/sd/ejkfsdptym30e-gdvkycc225zpw.png"><br><br>  Der Hauptserver antwortet, dass Sie eine Verbindung zum Server Nr. 1 herstellen m√ºssen ‚Üí Der Benutzer stellt eine Verbindung her.  Wenn der Hauptserver ausf√§llt, kann der Benutzer offensichtlich nicht mehr auf neue Karten zugreifen. <br><br>  Warum brauchen wir dann einen Server, der wei√ü, wo die Boards ge√∂ffnet sind?  Damit wir einen einzigen Entscheidungspunkt haben.  Wenn etwas mit den Servern passiert, m√ºssen wir verstehen, ob die Karte tats√§chlich verf√ºgbar ist, um die Karte aus der Registrierung zu entfernen oder an einer anderen Stelle erneut zu √∂ffnen.  Es w√§re m√∂glich, dies mit Hilfe eines Quorums zu organisieren, wenn mehrere Server ein √§hnliches Problem l√∂sen, aber zu diesem Zeitpunkt hatten wir nicht das Wissen, das Quorum unabh√§ngig zu implementieren. <br><br><h2>  Wechseln Sie zu Hazelcast </h2><br>  Auf die eine oder andere Weise haben wir die aufgetretenen Probleme bew√§ltigt, aber es ist vielleicht nicht der sch√∂nste Weg.  Jetzt mussten wir verstehen, wie man sie richtig l√∂st, und formulierten eine Liste der Anforderungen f√ºr eine neue Clusterl√∂sung: <br><br><ol><li>  Wir brauchen etwas, das den Status aller Server und ihre Rollen √ºberwacht.  Nennen Sie es Service Discovery. </li><li>  Wir ben√∂tigen Clustersperren, die die Konsistenz bei der Ausf√ºhrung gef√§hrlicher Abfragen gew√§hrleisten. </li><li>  Wir ben√∂tigen eine verteilte Datenstruktur, die sicherstellt, dass sich die Karten auf bestimmten Servern befinden, und dar√ºber informiert, wenn ein Fehler aufgetreten ist. </li></ol><br>  Es war das Jahr 2015.  Wir haben uns f√ºr Hazelcast - In-Memory Data Grid entschieden, ein Clustersystem zum Speichern von Informationen im RAM.  Dann dachten wir, wir h√§tten eine Wunderl√∂sung gefunden, den heiligen Gral der Welt der Cluster-Interaktion, ein Wunder-Framework, das alles kann und verteilte Datenstrukturen, Sperren, RPC-Nachrichten und Warteschlangen kombiniert. <br><br><img src="https://habrastorage.org/webt/ce/ws/c9/cewsc9gdgzsmtebczs9jxbs4j2e.png"><br><br>  Wie bei ActiveMQ haben wir fast alles auf Hazelcast √ºbertragen: <br><br><ul><li>  Generierung von Benutzerressourcen durch ExecutorService; </li><li>  verteilte Sperre, wenn Rechte ge√§ndert werden; </li><li>  Rollen und Attribute von Servern (Service Discovery); </li><li>  eine einzige Registrierung von offenen Brettern usw. </li></ul><br><h3>  Hazelcast-Topologien </h3><br>  Hazelcast kann in zwei Topologien konfiguriert werden.  Die erste Option ist Client-Server. Wenn sich Mitglieder getrennt von der Hauptanwendung befinden, bilden sie selbst einen Cluster und alle Anwendungen stellen als Datenbank eine Verbindung zu ihnen her. <br><br><img src="https://habrastorage.org/webt/r4/lg/vm/r4lgvmm7ni0dmyb6yp60cueklwm.png"><br><br>  Die zweite Topologie ist eingebettet, wenn Hazelcast-Mitglieder in die Anwendung selbst eingebettet sind.  In diesem Fall k√∂nnen wir weniger Instanzen verwenden. Der Zugriff auf Daten ist schneller, da sich die Daten und die Gesch√§ftslogik selbst an derselben Stelle befinden. <br><br><img src="https://habrastorage.org/webt/gq/rz/fa/gqrzfappt3yspdlfpfe5sm3mhyg.png"><br><br>  Wir haben uns f√ºr die zweite L√∂sung entschieden, weil wir die Implementierung f√ºr effektiver und wirtschaftlicher hielten.  Effektiv, da die Geschwindigkeit f√ºr den Zugriff auf Hazelcast-Daten geringer ist, weil  M√∂glicherweise befinden sich diese Daten auf dem aktuellen Server.  Wirtschaftlich, weil wir kein Geld f√ºr zus√§tzliche Instanzen ausgeben m√ºssen. <br><br><h3>  Cluster h√§ngt, wenn Mitglied h√§ngt </h3><br>  Ein paar Wochen nach dem Einschalten von Hazelcast traten Probleme auf dem Produkt auf. <br><br>  Unsere √úberwachung ergab zun√§chst, dass einer der Server den Speicher allm√§hlich √ºberlastete.  W√§hrend wir diesen Server beobachteten, wurde auch der Rest der Server geladen: Die CPU wuchs, dann der RAM, und nach f√ºnf Minuten verwendeten alle Server den gesamten verf√ºgbaren Speicher. <br><br>  An diesem Punkt in den Konsolen sahen wir diese Nachrichten: <br><br><pre> <code class="java hljs"><span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">466</span></span> [WARN] (cached18) com.hazelcast.spi.impl.operationservice.impl.Invocation: [my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] Asking ifoperation execution has been started: com.hazelcast.spi.impl.operationservice.impl.IsStillRunningService$InvokeIsStillRunningOperationRunnable@<span class="hljs-number"><span class="hljs-number">6</span></span>d4274d7 <span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">467</span></span> [WARN] (hz._hzInstance_1_dev.async.thread-<span class="hljs-number"><span class="hljs-number">3</span></span>) com.hazelcast.spi.impl.operationservice.impl.Invocation:[my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] <span class="hljs-string"><span class="hljs-string">'is-executing'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> -&gt; Invocation{ serviceName=<span class="hljs-string"><span class="hljs-string">'hz:impl:executorService'</span></span>, op=com.hazelcast.executor.impl.operations.MemberCallableTaskOperation{serviceName=<span class="hljs-string"><span class="hljs-string">'null'</span></span>, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, callId=<span class="hljs-number"><span class="hljs-number">18062</span></span>, invocationTime=<span class="hljs-number"><span class="hljs-number">1436974430783</span></span>, waitTimeout=-<span class="hljs-number"><span class="hljs-number">1</span></span>,callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>}, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, replicaIndex=<span class="hljs-number"><span class="hljs-number">0</span></span>, tryCount=<span class="hljs-number"><span class="hljs-number">250</span></span>, tryPauseMillis=<span class="hljs-number"><span class="hljs-number">500</span></span>, invokeCount=<span class="hljs-number"><span class="hljs-number">1</span></span>, callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>,target=Address[my.host2.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span>, backupsExpected=<span class="hljs-number"><span class="hljs-number">0</span></span>, backupsCompleted=<span class="hljs-number"><span class="hljs-number">0</span></span>}</code> </pre><br>  Hier pr√ºft Hazelcast, ob der Vorgang ausgef√ºhrt wird, der an den ersten "sterbenden" Server gesendet wurde.  Hazelcast versuchte, auf dem Laufenden zu bleiben und √ºberpr√ºfte mehrmals pro Sekunde den Status der Operation.  Infolgedessen hat er alle anderen Server mit dieser Operation als Spam versendet, und nach einigen Minuten ist ihnen der Speicher ausgegangen, und wir haben mehrere GB Protokolle von jedem von ihnen gesammelt. <br><br>  Die Situation wurde mehrmals wiederholt.  Es stellte sich heraus, dass dies ein Fehler in Hazelcast Version 3.5 ist, in der der Heartbeat-Mechanismus implementiert wurde, der den Status von Anforderungen √ºberpr√ºft.  Einige der Grenzf√§lle, auf die wir gesto√üen sind, wurden nicht √ºberpr√ºft.  Ich musste die Anwendung optimieren, um nicht in diese F√§lle zu geraten, und nach einigen Wochen konnte Hazelcast den Fehler zu Hause beheben. <br><br><h3>  H√§ufig Hinzuf√ºgen und Entfernen von Mitgliedern zu Hazelcast </h3><br>  Das n√§chste Problem, das wir entdeckt haben, ist das Hinzuf√ºgen und Entfernen von Mitgliedern zu Hazelcast. <br><br>  Zun√§chst werde ich kurz beschreiben, wie Hazelcast mit Partitionen funktioniert.  Zum Beispiel gibt es vier Server, von denen jeder einen Teil der Daten speichert (in der Abbildung haben sie unterschiedliche Farben).  Die Einheit ist die prim√§re Partition, die Zwei ist die sekund√§re Partition, d.h.  Sicherung der Hauptpartition. <br><br><img src="https://habrastorage.org/webt/ex/qz/vj/exqzvjxs9rxlmfgssghnrnqxnn8.png"><br><br>  Wenn ein Server ausgeschaltet ist, werden Partitionen an andere Server gesendet.  Falls der Server ausf√§llt, werden Partitionen nicht von ihm √ºbertragen, sondern von den Servern, die noch am Leben sind und eine Sicherung dieser Partitionen enthalten. <br><br><img src="https://habrastorage.org/webt/eu/ds/-0/euds-0xurnqjlbhisjoj8k9ucis.png"><br><br>  Dies ist ein zuverl√§ssiger Mechanismus.  Das Problem ist, dass wir Server h√§ufig ein- und ausschalten, um die Last auszugleichen, und das Neuausgleichen von Partitionen auch Zeit in Anspruch nimmt.  Und je mehr Server ausgef√ºhrt werden und je mehr Daten wir in Hazelcast speichern, desto l√§nger dauert es, die Partitionen neu auszugleichen. <br><br>  Nat√ºrlich k√∂nnen wir die Anzahl der Sicherungen reduzieren, d. H.  sekund√§re Partitionen.  Dies ist jedoch nicht sicher, da definitiv etwas schief gehen wird. <br><br>  Eine andere L√∂sung besteht darin, zur Client-Server-Topologie zu wechseln, damit das Ein- und Ausschalten von Servern keinen Einfluss auf den Hazelcast-Kerncluster hat.  Wir haben versucht, dies zu tun, und es stellte sich heraus, dass RPC-Anforderungen nicht auf Clients ausgef√ºhrt werden k√∂nnen.  Mal sehen warum. <br><br>  Betrachten Sie dazu das Beispiel des Sendens einer RPC-Anforderung an einen anderen Server.  Wir verwenden den ExecutorService, mit dem Sie RPC-Nachrichten senden und mit einer neuen Aufgabe senden k√∂nnen. <br><br><pre> <code class="plaintext hljs">hazelcastInstance .getExecutorService(...) .submit(new Task(), ...);</code> </pre><br>  Die Aufgabe selbst sieht aus wie eine regul√§re Java-Klasse, die Callable implementiert. <br><pre> <code class="plaintext hljs">public class Task implements Callable&lt;Long&gt; { @Override public Long call() { return 42; } }</code> </pre><br>  Das Problem ist, dass Hazelcast-Clients nicht nur Java-Anwendungen sein k√∂nnen, sondern auch C ++ - Anwendungen, .NET und andere.  Nat√ºrlich k√∂nnen wir unsere Java-Klasse nicht generieren und auf eine andere Plattform konvertieren. <br><br>  Eine M√∂glichkeit besteht darin, auf http-Anfragen umzusteigen, falls wir etwas von einem Server an einen anderen senden und eine Antwort erhalten m√∂chten.  Aber dann m√ºssen wir Hazelcast teilweise aufgeben. <br><br>  Als L√∂sung haben wir uns daher f√ºr die Verwendung von Warteschlangen anstelle von ExecutorService entschieden.  Zu diesem Zweck haben wir unabh√§ngig einen Mechanismus implementiert, der darauf wartet, dass ein Element in der Warteschlange ausgef√ºhrt wird. Dieser verarbeitet Grenzf√§lle und gibt das Ergebnis an den anfordernden Server zur√ºck. <br><br><h2>  Was haben wir gelernt? </h2><br>  <b>Legen Sie Flexibilit√§t in das System.</b>  Die Zukunft √§ndert sich st√§ndig, daher gibt es keine perfekten L√∂sungen.  Richtig zu machen ‚Äûrichtig‚Äú funktioniert nicht, aber Sie k√∂nnen versuchen, flexibel zu sein und es in das System zu integrieren.  Dies erm√∂glichte es uns, wichtige architektonische Entscheidungen so lange aufzuschieben, bis es nicht mehr unm√∂glich ist, sie zu akzeptieren. <br><br>  Robert Martin in Clean Architecture schreibt √ºber dieses Prinzip: <br><blockquote>  ‚ÄûZiel des Architekten ist es, eine Form f√ºr das System zu schaffen, die die Politik zum wichtigsten Element macht und die Details nicht mit der Politik in Verbindung bringt.  Dies wird Entscheidungen √ºber Details verz√∂gern und verz√∂gern. ‚Äú </blockquote><br><br>  <b>Universelle Werkzeuge und L√∂sungen existieren nicht.</b>  Wenn es Ihnen so scheint, als ob ein Framework alle Ihre Probleme l√∂st, ist dies h√∂chstwahrscheinlich nicht der Fall.  Daher ist es bei der Implementierung eines Frameworks wichtig, nicht nur zu verstehen, welche Probleme es l√∂sen wird, sondern auch welche. <br><br>  <b>Schreiben Sie nicht sofort alles neu.</b>  Wenn Sie mit einem Problem in der Architektur konfrontiert sind und es scheint, dass die einzig richtige L√∂sung darin besteht, alles von Grund auf neu zu schreiben, warten Sie.  Wenn das Problem wirklich schwerwiegend ist, finden Sie eine schnelle L√∂sung und beobachten Sie, wie das System in Zukunft funktionieren wird.  H√∂chstwahrscheinlich wird dies nicht das einzige Problem in der Architektur sein, mit der Zeit werden Sie mehr finden.  Und nur wenn Sie eine ausreichende Anzahl von Problembereichen erfassen, k√∂nnen Sie mit dem Refactoring beginnen.  Nur in diesem Fall ergeben sich mehr Vorteile als sein Wert. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441590/">https://habr.com/ru/post/de441590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441578/index.html">React Tutorial Teil 19: Methoden des Komponentenlebenszyklus</a></li>
<li><a href="../de441580/index.html">React Tutorial Teil 20: Erste Lektion zum bedingten Rendern</a></li>
<li><a href="../de441582/index.html">Optimierung des LQR-Steuerungssystems</a></li>
<li><a href="../de441584/index.html">PHP Digest Nr. 150 (11. - 25. Februar 2019)</a></li>
<li><a href="../de441586/index.html">Wie man Musik empfiehlt, die fast niemand geh√∂rt hat. Yandex-Bericht</a></li>
<li><a href="../de441594/index.html">Firmennapalm</a></li>
<li><a href="../de441596/index.html">Der erste private Raumhafen wird in Russland gebaut</a></li>
<li><a href="../de441598/index.html">Mondmission "Bereshit" - ein Online-Portal mit Flugbahnsimulator und √úberwachung der aktuellen Flugparameter</a></li>
<li><a href="../de441600/index.html">Schwache Benutzeroberfl√§che, schwacher Programmierer</a></li>
<li><a href="../de441602/index.html">Warum ein klassisches Automatikauto unm√∂glich ist und keine kommerziellen Perspektiven hat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>