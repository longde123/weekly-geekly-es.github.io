<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ΩüèΩ üôçüèΩ üìï Redes neuronales y aprendizaje profundo, cap√≠tulo 3, parte 2: ¬øpor qu√© la regularizaci√≥n ayuda a reducir el reciclaje? üë©üèø‚Äçüíª ‚óæÔ∏è ü§∂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Contenido 

- Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano 
- Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y aprendizaje profundo, cap√≠tulo 3, parte 2: ¬øpor qu√© la regularizaci√≥n ayuda a reducir el reciclaje?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: mejorar el m√©todo de entrenamiento de redes neuronales</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: ¬øPor qu√© la regularizaci√≥n ayuda a reducir el reciclaje?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3: ¬øc√≥mo elegir hiperpar√°metros de red neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 4: prueba visual de que las redes neuronales son capaces de calcular cualquier funci√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 5: ¬øpor qu√© las redes neuronales profundas son tan dif√≠ciles de entrenar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: progreso reciente en el reconocimiento de im√°genes</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ep√≠logo: ¬øexiste un algoritmo simple para crear inteligencia?</a> </li></ul></div></div><br>  Emp√≠ricamente, hemos visto que la regularizaci√≥n ayuda a reducir el reciclaje.  Esto es inspirador, pero, desafortunadamente, no es obvio por qu√© ayuda la regularizaci√≥n.  Por lo general, las personas lo explican de alguna manera: en cierto sentido, los pesos m√°s peque√±os tienen menos complejidad, lo que proporciona una explicaci√≥n m√°s simple y m√°s eficiente de los datos, por lo que deber√≠an preferirse.  Sin embargo, esta es una explicaci√≥n demasiado corta, y algunas partes pueden parecer dudosas o misteriosas.  Desplieguemos esta historia y examin√©mosla con ojo cr√≠tico.  Para hacer esto, supongamos que tenemos un conjunto de datos simple para el que queremos crear un modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br>  En t√©rminos de significado, aqu√≠ estudiamos el fen√≥meno del mundo real, y x e y denotan datos reales.  Nuestro objetivo es construir un modelo que nos permita predecir y en funci√≥n de x.  Podr√≠amos intentar usar una red neuronal para crear dicho modelo, pero sugiero algo m√°s simple: intentar√© modelar y como un polinomio en x.  Har√© esto en lugar de las redes neuronales, ya que el uso de polinomios hace que la explicaci√≥n sea especialmente clara.  Tan pronto como tratemos el caso del polinomio, pasaremos a la Asamblea Nacional.  Hay diez puntos en el gr√°fico anterior, lo que significa que podemos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">encontrar un polinomio √∫nico de</a> noveno orden y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... + un <sub>9</sub> que se ajusta exactamente a los datos.  Y aqu√≠ est√° el gr√°fico de este polinomio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br>  Golpe perfecto.  Pero podemos obtener una buena aproximaci√≥n usando el modelo lineal y = 2x <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br>  Cual es mejor?  ¬øCu√°l es m√°s probable que sea cierto?  ¬øCu√°l se generalizar√° mejor a otros ejemplos del mismo fen√≥meno del mundo real? <br><br>  Preguntas dif√≠ciles  Y no se pueden responder exactamente sin informaci√≥n adicional sobre el fen√≥meno subyacente del mundo real.  Sin embargo, veamos dos posibilidades: (1) un modelo con un polinomio de noveno orden realmente describe el fen√≥meno del mundo real y, por lo tanto, se generaliza perfectamente;  (2) el modelo correcto es y = 2x, pero tenemos ruido adicional asociado con el error de medici√≥n, por lo que el modelo no encaja perfectamente. <br><br>  A priori, no se puede decir cu√°l de las dos posibilidades es correcta (o si no hay una tercera).  L√≥gicamente, cualquiera de ellos puede ser cierto.  Y la diferencia entre ellos no es trivial.  S√≠, seg√∫n los datos disponibles, se puede decir que solo hay una ligera diferencia entre los modelos.  Pero supongamos que queremos predecir el valor de y correspondiente a alg√∫n valor grande de x, mucho m√°s grande que cualquiera de los que se muestran en el gr√°fico.  Si intentamos hacer esto, aparecer√° una gran diferencia entre las predicciones de los dos modelos, ya que el t√©rmino x <sup>9</sup> domina en el polinomio de noveno orden, y el modelo lineal sigue siendo lineal. <br><br>  Un punto de vista sobre lo que est√° sucediendo es afirmar que, si es posible, se debe usar una explicaci√≥n m√°s simple en la ciencia.  Cuando encontramos un modelo simple que explica muchos puntos de referencia, solo queremos gritar: "¬°Eureka!"  Despu√©s de todo, es poco probable que aparezca una explicaci√≥n simple por casualidad.  Sospechamos que el modelo deber√≠a producir alguna verdad asociada con el fen√≥meno.  En este caso, el modelo y = 2x + ruido parece mucho m√°s simple que y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... Ser√≠a sorprendente si la simplicidad surgiera por casualidad, por lo que sospechamos que y = 2x + ruido expresa algo verdad subyacente  Desde este punto de vista, el modelo de noveno orden simplemente estudia el efecto del ruido local.  Aunque el modelo de noveno orden funciona perfectamente para estos puntos de referencia espec√≠ficos, no puede generalizarse a otros puntos, como resultado de lo cual el modelo lineal con ruido tendr√° mejores capacidades predictivas. <br><br>  Veamos qu√© significa este punto de vista para las redes neuronales.  Supongamos que, en nuestra red, existen principalmente pesos bajos, como suele ser el caso en las redes regularizadas.  Debido a su peque√±o peso, el comportamiento de la red no cambia mucho cuando se cambian varias entradas aleatorias aqu√≠ y all√°.  Como resultado, la red regularizada es dif√≠cil de aprender los efectos del ruido local presente en los datos.  Esto es similar al deseo de garantizar que la evidencia individual no afecte en gran medida la salida de la red en su conjunto.  En cambio, la red regularizada est√° capacitada para responder a la evidencia que a menudo se encuentra en los datos de capacitaci√≥n.  Por el contrario, una red con grandes pesos puede cambiar su comportamiento con bastante fuerza en respuesta a peque√±os cambios en los datos de entrada.  Por lo tanto, una red irregular puede usar grandes pesos para entrenar un modelo complejo que contiene mucha informaci√≥n de ruido en los datos de entrenamiento.  En resumen, las limitaciones de las redes regularizadas les permiten crear modelos relativamente simples basados ‚Äã‚Äãen patrones que a menudo se encuentran en los datos de entrenamiento, y son resistentes a las desviaciones causadas por el ruido en los datos de entrenamiento.  Existe la esperanza de que esto haga que nuestras redes estudien el fen√≥meno en s√≠ y generalicen mejor el conocimiento adquirido. <br><br>  Dicho todo esto, la idea de dar preferencia a explicaciones m√°s simples deber√≠a ponerlo nervioso.  A veces las personas llaman a esta idea "la navaja de afeitar de Occam" y la aplican celosamente, como si tuviera el estatus de un principio cient√≠fico general.  Pero esto, por supuesto, no es un principio cient√≠fico general.  No hay una raz√≥n l√≥gica a priori para preferir explicaciones simples a las complejas.  A veces una explicaci√≥n m√°s complicada es correcta. <br><br>  Perm√≠tanme describir dos ejemplos de c√≥mo una explicaci√≥n m√°s compleja result√≥ ser correcta.  En la d√©cada de 1940, el f√≠sico Marcel Shane anunci√≥ el descubrimiento de una nueva part√≠cula.  La compa√±√≠a para la que trabaj√≥, General Electric, estaba encantada y distribuy√≥ ampliamente la publicaci√≥n de este evento.  Sin embargo, el f√≠sico Hans Bethe se mostr√≥ esc√©ptico.  Bethe visit√≥ a Shane y estudi√≥ las placas con rastros de la nueva part√≠cula de Shane.  Shane mostr√≥ Beta placa tras placa, pero Bete encontr√≥ en cada una de ellas un problema que indicaba la necesidad de rechazar estos datos.  Finalmente, Shane le mostr√≥ a Beta un registro que parec√≠a adecuado.  Bethe dijo que probablemente era solo una desviaci√≥n estad√≠stica.  Shane: "S√≠, pero las posibilidades de que se deba a estad√≠sticas, incluso seg√∫n su propia f√≥rmula, son una de cada cinco".  Bethe: "Sin embargo, ya he visto cinco registros".  Finalmente, Shane dijo: "Pero usted explic√≥ cada uno de mis registros, cada buena imagen con alguna otra teor√≠a, y tengo una hip√≥tesis que explica todos los registros a la vez, de lo que se deduce que estamos hablando de una nueva part√≠cula".  Bethe respondi√≥: ‚ÄúLa √∫nica diferencia entre mis explicaciones y las tuyas es que las tuyas est√°n equivocadas y las m√≠as son correctas.  Tu √∫nica explicaci√≥n es incorrecta, y todas mis explicaciones son correctas.  Posteriormente, result√≥ que la naturaleza estaba de acuerdo con Bethe, y la part√≠cula de Shane se evapor√≥. <br><br>  En el segundo ejemplo, en 1859, el astr√≥nomo Urbain Jean Joseph Le Verrier descubri√≥ que la forma de la √≥rbita de Mercurio no se corresponde con la teor√≠a de la gravitaci√≥n universal de Newton.  Hubo una peque√±a desviaci√≥n de esta teor√≠a, y luego se propusieron varias opciones para resolver el problema, que se redujo al hecho de que la teor√≠a de Newton en su conjunto es correcta y requiere solo un ligero cambio.  Y en 1916, Einstein demostr√≥ que esta desviaci√≥n puede explicarse bien utilizando su teor√≠a general de la relatividad, radicalmente diferente de la gravedad newtoniana y basada en matem√°ticas mucho m√°s complejas.  A pesar de esta complejidad adicional, hoy se acepta generalmente que la explicaci√≥n de Einstein es correcta y que la gravedad newtoniana es incorrecta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">incluso en una forma modificada</a> .  Esto sucede, en particular, porque hoy sabemos que la teor√≠a de Einstein explica muchos otros fen√≥menos con los que la teor√≠a de Newton tuvo dificultades.  Adem√°s, a√∫n m√°s sorprendente, la teor√≠a de Einstein predice con precisi√≥n varios fen√≥menos que la gravedad newtoniana no predijo en absoluto.  Sin embargo, estas cualidades impresionantes no eran obvias en el pasado.  A juzgar por la simple simplicidad, alguna forma modificada de la teor√≠a newtoniana habr√≠a parecido m√°s atractiva. <br><br>  Se pueden extraer tres moralidades de estas historias.  Primero, a veces es bastante dif√≠cil decidir cu√°l de las dos explicaciones ser√° "m√°s f√°cil".  En segundo lugar, incluso si tomamos esa decisi√≥n, ¬°la simplicidad debe guiarse con extremo cuidado!  En tercer lugar, la verdadera prueba del modelo no es la simplicidad, sino qu√© tan bien predice nuevos fen√≥menos en nuevas condiciones de comportamiento. <br><br>  Teniendo en cuenta todo esto y teniendo cuidado, aceptaremos un hecho emp√≠rico: los NS regularizados generalmente est√°n mejor generalizados que los irregulares.  Por lo tanto, m√°s adelante en el libro usaremos la regularizaci√≥n.  Las historias mencionadas solo son necesarias para explicar por qu√© nadie ha desarrollado una explicaci√≥n te√≥rica completamente convincente de por qu√© la regularizaci√≥n ayuda a las redes a generalizarse.  Los investigadores contin√∫an publicando trabajos donde intentan probar diferentes enfoques para la regularizaci√≥n, compararlos, ver qu√© funciona mejor y tratar de entender por qu√© los diferentes enfoques funcionan peor o mejor.  Entonces la regularizaci√≥n puede ser tratada como una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nube</a> .  Cuando ayuda bastante a menudo, no tenemos una comprensi√≥n sist√©mica completamente satisfactoria de lo que est√° sucediendo, solo reglas heur√≠sticas y pr√°cticas incompletas. <br><br>  Aqu√≠ yace un conjunto m√°s profundo de problemas que van al coraz√≥n de la ciencia.  Este es un problema de generalizaci√≥n.  La regularizaci√≥n puede darnos una varita m√°gica computacional que ayuda a nuestras redes a generalizar mejor los datos, pero no brinda una comprensi√≥n b√°sica de c√≥mo funciona la generalizaci√≥n y cu√°l es el mejor enfoque para ella. <br><br>  Estos problemas se remontan al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">problema de la inducci√≥n</a> , cuya interpretaci√≥n conocida fue realizada por el fil√≥sofo escoc√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">David Hume</a> en el libro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">A Study on Human Cognition</a> " (1748).  El problema de la inducci√≥n es el tema del " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">teorema sobre la ausencia de comidas gratuitas</a> " de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">David Walpert y William Macredie</a> (1977). <br><br>  Y esto es especialmente molesto, porque en la vida cotidiana las personas son fenomenalmente capaces de generalizar los datos.  Muestre algunas im√°genes del elefante al ni√±o, y √©l aprender√° r√°pidamente a reconocer a otros elefantes.  Por supuesto, a veces puede cometer un error, por ejemplo, confundir un rinoceronte con un elefante, pero en general, este proceso funciona sorprendentemente con precisi√≥n.  Ahora, tenemos un sistema, el cerebro humano, con una gran cantidad de par√°metros libres.  Y despu√©s de que se le muestre una o m√°s im√°genes de entrenamiento, el sistema aprende a generalizarlas a otras im√°genes.  ¬°Nuestro cerebro, en cierto sentido, es incre√≠blemente bueno para regularizar!  ¬øPero c√≥mo hacemos esto?  Por el momento, esto es desconocido para nosotros.  Creo que en el futuro desarrollaremos tecnolog√≠as de regularizaci√≥n m√°s potentes en redes neuronales artificiales, t√©cnicas que finalmente permitir√°n a la Asamblea Nacional generalizar datos basados ‚Äã‚Äãen conjuntos de datos a√∫n m√°s peque√±os. <br><br>  De hecho, nuestras redes ya se est√°n generalizando mucho mejor de lo que podr√≠a esperarse a priori.  Una red con 100 neuronas ocultas tiene casi 80,000 par√°metros.  Tenemos solo 50,000 im√°genes en datos de entrenamiento.  Esto es lo mismo que tratar de estirar un polinomio de 80,000 √≥rdenes sobre 50,000 puntos de referencia.  Seg√∫n todas las indicaciones, nuestra red debe volverse a entrenar terriblemente.  Y, sin embargo, como hemos visto, una red de este tipo en realidad se generaliza bastante bien.  ¬øPor qu√© est√° pasando esto?  Esto no est√° del todo claro.  Se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">plante√≥</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hip√≥tesis de</a> que "la din√°mica del aprendizaje por descenso de gradiente en redes multicapa est√° sujeta a autorregulaci√≥n".  Esta es una fortuna extrema, pero tambi√©n un hecho bastante inquietante, ya que no entendemos por qu√© sucede esto.  Mientras tanto, adoptaremos un enfoque pragm√°tico y utilizaremos la regularizaci√≥n siempre que sea posible.  Esto ser√° beneficioso para nuestra Asamblea Nacional. <br><br>  Perm√≠tanme terminar esta secci√≥n volviendo a lo que no expliqu√© antes: que la regularizaci√≥n de L2 no limita los desplazamientos.  Naturalmente, ser√≠a f√°cil cambiar el procedimiento de regularizaci√≥n para que regularice los desplazamientos.  Pero emp√≠ricamente, esto a menudo no cambia los resultados de manera notable, por lo tanto, hasta cierto punto, tratar con la regularizaci√≥n de sesgos, o no, es una cuesti√≥n de acuerdo.  Sin embargo, vale la pena se√±alar que un desplazamiento grande no hace que una neurona sea sensible a los insumos como los pesos grandes.  Por lo tanto, no debemos preocuparnos por las grandes compensaciones que permiten que nuestras redes aprendan el ruido en los datos de entrenamiento.  Al mismo tiempo, al permitir grandes desplazamientos, hacemos que nuestras redes sean m√°s flexibles en su comportamiento, en particular, los grandes desplazamientos facilitan la saturaci√≥n de las neuronas, lo que nos gustar√≠a.  Por esta raz√≥n, generalmente no incluimos compensaciones en la regularizaci√≥n. <br><br><h2>  Otras t√©cnicas de regularizaci√≥n. </h2><br>  Hay muchas t√©cnicas de regularizaci√≥n adem√°s de L2.  De hecho, ya se han desarrollado tantas t√©cnicas que, con todo el deseo, no podr√≠a describirlas brevemente.  En esta secci√≥n, describir√© brevemente otros tres enfoques para reducir el reentrenamiento: regularizar L1, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">abandonar</a> y aumentar artificialmente el conjunto de entrenamiento.  No los estudiaremos tan profundamente como los temas anteriores.  En cambio, solo los conocemos y, al mismo tiempo, apreciamos la variedad de t√©cnicas de regularizaci√≥n existentes. <br><br><h3>  Regularizaci√≥n L1 </h3><br>  En este enfoque, modificamos la funci√≥n de costo irregular agregando la suma de los valores absolutos de los pesos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>95</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="40.447ex" height="2.66ex" viewBox="0 -832 17414.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2B" x="3486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="4736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="5287" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="5738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="6268" y="0"></use><g transform="translate(6701,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="10140" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-73" x="10991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-75" x="11460" y="0"></use><g transform="translate(12033,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-7C" x="13518" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="13797" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-7C" x="14513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="15042" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="15403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="15933" y="0"></use><g transform="translate(16413,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-35" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>95</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \ frac {\ lambda} {n} \ sum_w | w | \ tag {95} </script></p><br><br>  Intuitivamente, esto es similar a la regularizaci√≥n de L2, que multas por grandes pesos y hace que la red prefiera pesos bajos.  Por supuesto, el t√©rmino de regularizaci√≥n L1 no es como el t√©rmino de regularizaci√≥n L2, por lo que no debe esperar exactamente el mismo comportamiento.  Tratemos de entender c√≥mo el comportamiento de una red entrenada con regularizaci√≥n L1 difiere de una red entrenada con regularizaci√≥n L2. <br><br>  Para hacer esto, observe las derivadas parciales de la funci√≥n de costo.  Diferenciando (95), obtenemos: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>96</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="86.162ex" height="2.66ex" viewBox="0 -832 37097.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="3269" y="0"></use></g><g transform="translate(6245,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-3D" x="10508" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="11815" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="12365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="12817" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="13346" y="0"></use><g transform="translate(13780,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(18218,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2B" x="22427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="23677" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="24228" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="24679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="25209" y="0"></use><g transform="translate(25642,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="29081" y="0"></use><g transform="translate(29849,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="701" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-73" x="1580" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="2049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="2530" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-28" x="32979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="33369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-29" x="34085" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="34725" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="35086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="35616" y="0"></use><g transform="translate(36096,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-36" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>96</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \ frac {\ partial C} {\ partial w} = \ frac {\ partial C_0} {\ partial w} + \ frac {\ lambda} {n} \, {\ rm sgn} (w) \ tag {96 } </script></p><br><br>  donde sgn (w) es el signo de w, es decir, +1 si w es positivo y -1 si w es negativo.  Usando esta expresi√≥n, modificamos ligeramente la propagaci√≥n hacia atr√°s para que realice un descenso de gradiente estoc√°stico usando la regularizaci√≥n L1.  La regla de actualizaci√≥n final para la red regularizada L1: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>97</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.249ex" height="2.78ex" viewBox="0 -883.9 39718.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2212" x="9100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="10901" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="11353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="11882" y="0"></use><g transform="translate(12316,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="17362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="18213" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-62" x="19091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6F" x="19521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-78" x="20006" y="0"></use><g transform="translate(20579,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="950" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-28" x="22129" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="22519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-29" x="23235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2212" x="23847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="25098" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="25564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="25926" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="26705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="27256" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="27707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="28237" y="0"></use><g transform="translate(28670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33109,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="37345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="37707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="38236" y="0"></use><g transform="translate(38717,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-37" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>97</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \ rightarrow w '= w- \ frac {\ eta \ lambda} {n} \ mbox {sgn} (w) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {97} </script></p><br><br>  donde, como de costumbre, ‚àÇC / ‚àÇw se puede estimar opcionalmente utilizando el valor promedio del mini-paquete.  Compare esto con la regla de actualizaci√≥n de regularizaci√≥n L2 (93): <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>98</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.272ex" height="2.78ex" viewBox="0 -883.9 39728.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="9128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="9426" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="9893" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="10443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-28" x="10805" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-31" x="11194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2212" x="11917" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="13168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="13718" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="14170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="14699" y="0"></use><g transform="translate(15133,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6E" x="20179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="21030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="21481" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="21827" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-68" x="22307" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="22884" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-29" x="23245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2212" x="23857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="25108" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="25574" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="25936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-66" x="26715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="27266" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="27717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-63" x="28247" y="0"></use><g transform="translate(28680,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33119,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="37355" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="37717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="38246" y="0"></use><g transform="translate(38727,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>98</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \ rightarrow w '= w \ left (1 - \ frac {\ eta \ lambda} {n} \ right) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {98} </script></p><br><br>  En ambas expresiones, el efecto de la regularizaci√≥n es reducir los pesos.  Esto coincide con la noci√≥n intuitiva de que ambos tipos de regularizaci√≥n penalizan grandes pesos.  Sin embargo, los pesos se reducen de diferentes maneras.  En la regularizaci√≥n de L1, los pesos disminuyen en un valor constante, tendiendo a 0. En la regularizaci√≥n de L2, los pesos disminuyen en un valor proporcional a w.  Por lo tanto, cuando algo de peso tiene un gran valor | w |, la regularizaci√≥n de L1 reduce el peso no tanto como L2.  Y viceversa, cuando | w |  peque√±o, la regularizaci√≥n de L1 reduce el peso mucho m√°s que la regularizaci√≥n de L2.  Como resultado, la regularizaci√≥n de L1 tiende a concentrar los pesos de la red en un n√∫mero relativamente peque√±o de enlaces de gran importancia, mientras que otros pesos tienden a cero. <br><br>  Alivi√© un problema en la discusi√≥n anterior: la derivada parcial ‚àÇC / ‚àÇw no est√° definida cuando w = 0.  Esto se debe a que la funci√≥n | w |  hay un "doblez" agudo en el punto w = 0, por lo tanto, no se puede diferenciar all√≠.  Pero esto no da miedo.  Solo aplicamos la regla habitual e irregular para el descenso de gradiente estoc√°stico cuando w = 0.  Intuitivamente, no hay nada de malo en eso: la regularizaci√≥n deber√≠a reducir los pesos, y obviamente no puede reducir los pesos que ya son iguales a 0. M√°s precisamente, utilizaremos las ecuaciones (96) y (97) con la condici√≥n de que sgn (0) = 0.  Esto nos dar√° una regla conveniente y compacta para el descenso de gradiente estoc√°stico con regularizaci√≥n L1. <br><br><h3>  Excepci√≥n [abandono] </h3><br>  Una excepci√≥n es una t√©cnica de regularizaci√≥n completamente diferente.  A diferencia de la regularizaci√≥n de L1 y L2, la excepci√≥n no trata con un cambio en la funci√≥n de costo.  En cambio, estamos cambiando la red misma.  Perm√≠tanme explicar la mec√°nica b√°sica del funcionamiento de una excepci√≥n antes de profundizar en el tema de por qu√© funciona y con qu√© resultados. <br><br>  Supongamos que estamos tratando de entrenar una red: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br>  En particular, digamos que tenemos entrada de entrenamiento x y la salida deseada correspondiente y.  Por lo general, lo entrenamos distribuyendo directamente x a trav√©s de la red, y luego propag√°ndonos nuevamente para determinar la contribuci√≥n del gradiente.  Una excepci√≥n modifica este proceso.  Comenzamos eliminando aleatoria y temporalmente la mitad de las neuronas ocultas en la red, dejando las neuronas de entrada y salida sin cambios.  Despu√©s de eso, tendremos aproximadamente dicha red.  Tenga en cuenta que las neuronas excluidas, aquellas que se eliminan temporalmente, todav√≠a est√°n marcadas en el diagrama: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br>  Pasamos x por distribuci√≥n directa a trav√©s de la red modificada, y luego distribuimos de nuevo el resultado, tambi√©n a trav√©s de la red modificada.  Despu√©s de hacer esto con un mini paquete de ejemplos, actualizamos los pesos y compensaciones correspondientes.  Luego, repetimos este proceso, primero restaurando las neuronas excluidas, luego eligiendo un nuevo subconjunto aleatorio de neuronas ocultas para eliminar, evaluar el gradiente para otro mini paquete y actualizar los pesos y las compensaciones de la red. <br><br>  Repitiendo este proceso una y otra vez, obtenemos una red que ha aprendido algunos pesos y desplazamientos.  Naturalmente, estos pesos y desplazamientos se aprendieron en condiciones en las que se excluy√≥ la mitad de las neuronas ocultas.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Y cuando lancemos la red en su totalidad, tendremos el doble de neuronas ocultas activas. </font><font style="vertical-align: inherit;">Para compensar esto, reducimos a la mitad los pesos que provienen de las neuronas ocultas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El procedimiento de exclusi√≥n puede parecer extra√±o y arbitrario. ¬øPor qu√© deber√≠a ayudarla con la regularizaci√≥n? Para explicar lo que est√° sucediendo, quiero que se olviden de la excepci√≥n por un tiempo y presenten la capacitaci√≥n de la Asamblea Nacional de manera est√°ndar. En particular, imagine que entrenamos varios NS diferentes utilizando los mismos datos de entrenamiento. Por supuesto, las redes pueden variar al principio y, a veces, la capacitaci√≥n puede producir resultados diferentes. En tales casos, podr√≠amos aplicar alg√∫n tipo de promedio o esquema de votaci√≥n para decidir qu√© resultados aceptar. Por ejemplo, si capacitamos a cinco redes, y tres de ellas clasifican el n√∫mero como "3", entonces este es probablemente el verdadero tres. Y las otras dos redes probablemente est√©n equivocadas. Tal esquema de promedios es a menudo una forma √∫til (aunque costosa) de reducir el reciclaje. El motivo esque diferentes redes pueden volver a capacitarse de diferentes maneras, y el promedio puede ayudar a eliminar dicha reentrenamiento.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øC√≥mo se relaciona todo esto con la excepci√≥n? Heur√≠sticamente, cuando excluimos diferentes conjuntos de neutrones, es como si estuvi√©ramos entrenando diferentes NS. Por lo tanto, el procedimiento de exclusi√≥n es similar a los efectos promedio en una gran cantidad de redes diferentes. Las diferentes redes se vuelven a entrenar de diferentes maneras, por lo que se espera que el efecto promedio de la exclusi√≥n reduzca la reentrenamiento. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una explicaci√≥n heur√≠stica relacionada de los beneficios de la exclusi√≥n se da en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">uno de los primeros trabajos.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">usando esta t√©cnica: ‚ÄúEsta t√©cnica reduce la compleja adaptaci√≥n articular de las neuronas, porque la neurona no puede confiar en la presencia de ciertos vecinos. Al final, tiene que aprender rasgos m√°s confiables que pueden ser √∫tiles para trabajar junto con muchos subconjuntos aleatorios diferentes de neuronas ". En otras palabras, si imaginamos a nuestra Asamblea Nacional como un modelo que hace predicciones, entonces una excepci√≥n ser√° una forma de garantizar la estabilidad del modelo ante la p√©rdida de partes individuales de evidencia. En este sentido, la t√©cnica se asemeja a las regularizaciones de L1 y L2, que buscan reducir los pesos, y de esta manera hacen que la red sea m√°s resistente a la p√©rdida de cualquier conexi√≥n individual en la red. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturalmente, la verdadera medida de la utilidad de la exclusi√≥n es su tremendo √©xito en mejorar la eficiencia de las redes neuronales. En la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obra original</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">donde se introdujo este m√©todo, se aplic√≥ a muchas tareas diferentes. </font><font style="vertical-align: inherit;">Estamos particularmente interesados ‚Äã‚Äãen el hecho de que los autores aplicaron la excepci√≥n a la clasificaci√≥n de n√∫meros de MNIST, usando una red de distribuci√≥n directa simple similar a la que examinamos. </font><font style="vertical-align: inherit;">El documento se√±ala que hasta entonces, el mejor resultado para dicha arquitectura era el 98.4% de precisi√≥n. </font><font style="vertical-align: inherit;">Lo mejoraron al 98.7% usando una combinaci√≥n de exclusi√≥n y una forma modificada de regularizaci√≥n L2. </font><font style="vertical-align: inherit;">Se obtuvieron resultados igualmente impresionantes para muchas otras tareas, incluido el reconocimiento de patrones y del habla, y el procesamiento del lenguaje natural. </font><font style="vertical-align: inherit;">La excepci√≥n fue especialmente √∫til en el entrenamiento de grandes redes profundas, donde a menudo surge el problema del reciclaje.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Conjunto de datos de entrenamiento que se expande artificialmente </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vimos anteriormente que nuestra precisi√≥n de clasificaci√≥n MNIST cay√≥ al 80 por ciento, cuando usamos solo 1,000 im√°genes de entrenamiento. Y no es de extra√±ar: con menos datos, nuestra red encontrar√° menos opciones para escribir n√∫meros por personas. Intentemos entrenar a nuestra red de 30 neuronas ocultas, utilizando diferentes vol√∫menes del conjunto de entrenamiento para observar el cambio en la eficiencia. Entrenamos usando el tama√±o de mini-paquete de 10, la velocidad de aprendizaje Œ∑ = 0.5, el par√°metro de regularizaci√≥n Œª = 5.0 y la funci√≥n de costo con entrop√≠a cruzada. Entrenaremos una red de 30 eras utilizando un conjunto completo de datos y aumentaremos el n√∫mero de eras en proporci√≥n a la disminuci√≥n en el volumen de datos de entrenamiento. Para garantizar el mismo factor de reducci√≥n de peso para diferentes conjuntos de datos de entrenamiento, utilizaremos el par√°metro de regularizaci√≥n Œª = 5,0 con un conjunto de entrenamiento completo y reduzca proporcionalmente con una disminuci√≥n en los vol√∫menes de datos.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se puede ver que la precisi√≥n de la clasificaci√≥n crece significativamente con el aumento de los datos de entrenamiento. Es probable que este crecimiento contin√∫e con un aumento adicional en los vol√∫menes. Por supuesto, a juzgar por el gr√°fico anterior, nos estamos acercando a la saturaci√≥n. Sin embargo, supongamos que rehacemos este gr√°fico a una dependencia logar√≠tmica de la cantidad de datos de entrenamiento:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se puede ver que al final el gr√°fico todav√≠a tiende a subir. Esto sugiere que si tomamos una cantidad de datos mucho m√°s masiva, por ejemplo, millones o incluso miles de millones de ejemplos escritos a mano, en lugar de 50,000, entonces probablemente obtendremos una red de trabajo mucho mejor, incluso de un tama√±o tan peque√±o. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obtener m√°s datos de entrenamiento es una gran idea. Desafortunadamente, esto puede ser costoso, por lo que en la pr√°ctica no siempre es posible. Sin embargo, existe otra idea que puede funcionar casi tan bien: aumentar artificialmente el conjunto de datos. Por ejemplo, supongamos que tomamos una imagen de cinco de MNIST y la rotamos un poco, grados por 15:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta es claramente la misma figura. Pero a nivel de p√≠xeles, es muy diferente de las im√°genes disponibles en la base de datos MNIST. Es razonable suponer que agregar esta imagen al conjunto de datos de capacitaci√≥n puede ayudar a nuestra red a aprender m√°s sobre la clasificaci√≥n de im√°genes. Adem√°s, obviamente no estamos limitados a la capacidad de agregar solo una imagen. Podemos expandir nuestros datos de entrenamiento haciendo algunas vueltas peque√±as de todas las im√°genes de entrenamiento de MNIST, y luego usando el conjunto extendido de datos de entrenamiento para aumentar la eficiencia de la red. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esta idea es muy poderosa y se usa ampliamente. Veamos los resultados del </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabajo cient√≠fico.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">quien aplic√≥ varias variaciones de esta idea a MNIST. Una de las arquitecturas de las redes consideradas por ellos era similar a la que usamos: una red de distribuci√≥n directa con 800 neuronas ocultas, que utiliza la funci√≥n de costo con entrop√≠a cruzada. Al lanzar esta red con el conjunto de entrenamiento MNIST est√°ndar, obtuvieron una precisi√≥n de clasificaci√≥n del 98.4%. Pero luego expandieron los datos de entrenamiento, utilizando no solo la rotaci√≥n que describ√≠ anteriormente, sino tambi√©n la transferencia y distorsi√≥n de las im√°genes. Habiendo entrenado a la red en datos avanzados, aumentaron su precisi√≥n al 98.9%. Tambi√©n experimentaron con el llamado La "distorsi√≥n el√°stica", un tipo especial de distorsi√≥n de la imagen, dise√±ada para eliminar las vibraciones aleatorias de los m√∫sculos de la mano. Utilizando distorsiones el√°sticas para expandir los datos, lograron una precisi√≥n del 99,3%. En esencia, ampliaron su experiencia de red,d√°ndole varias variaciones manuscritas encontradas en la escritura real.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las variantes de esta idea se pueden usar para mejorar el rendimiento de muchas tareas de aprendizaje, no solo para el reconocimiento de escritura a mano. El principio general es expandir los datos de capacitaci√≥n mediante la aplicaci√≥n de operaciones que reflejen las variaciones encontradas en la realidad. Tales variaciones son f√°ciles de inventar. Supongamos que estamos creando NS para el reconocimiento de voz. Las personas pueden reconocer el habla incluso con distorsiones como el ruido de fondo. Por lo tanto, puede expandir los datos agregando ruido de fondo. Tambi√©n podemos reconocer el habla acelerada y lenta. Esta es otra forma de expandir los datos de entrenamiento. Estas t√©cnicas no siempre se usan; por ejemplo, en lugar de expandir el conjunto de entrenamiento agregando ruido, puede ser m√°s eficiente limpiar la entrada al aplicarles un filtro de ruido. Y, sin embargo, debes tener en cuenta la idea de expandir el conjunto de entrenamiento,y busca maneras de usarlo.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ejercicio </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como discutimos anteriormente, una forma de extender los datos de entrenamiento de MNIST es usar peque√±as rotaciones de las im√°genes de entrenamiento. </font><font style="vertical-align: inherit;">¬øQu√© problema puede aparecer si permitimos la rotaci√≥n de las im√°genes en cualquier √°ngulo?</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Digresi√≥n de grandes datos y el significado de comparar la precisi√≥n de la clasificaci√≥n </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Echemos un vistazo nuevamente a c√≥mo la precisi√≥n de nuestro NS var√≠a seg√∫n el tama√±o del conjunto de entrenamiento: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suponga que, en lugar de utilizar NS, usar√≠amos otra tecnolog√≠a de aprendizaje autom√°tico para clasificar los n√∫meros. Por ejemplo, intentemos usar el m√©todo de m√°quina de vectores de soporte (SVM), que conocimos brevemente en el Cap√≠tulo 1. Como entonces, no se preocupe si no est√° familiarizado con SVM, no necesitamos comprender sus detalles. Usaremos SVM a trav√©s de la biblioteca scikit-learn. As√≠ es como la efectividad de SVM var√≠a con el tama√±o del conjunto de entrenamiento. A modo de comparaci√≥n, puse el calendario y los resultados de la Asamblea Nacional.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Probablemente lo primero que llame la atenci√≥n: NS supera la SVM en cualquier tama√±o del conjunto de entrenamiento. Esto es bueno, aunque no vale la pena sacar conclusiones de gran alcance de esto, ya que utilic√© la configuraci√≥n predefinida de aprendizaje de scikit y trabajamos bastante en serio en nuestro NS. Un hecho menos v√≠vido, pero m√°s interesante, que se desprende del gr√°fico, es que si entrenamos nuestro SVM usando 50,000 im√°genes, funcionar√° mejor (94.48% de precisi√≥n) que nuestro NS entrenado con 5000 im√°genes ( 93,24%). En otras palabras, un aumento en el volumen de datos de entrenamiento a veces compensa la diferencia en los algoritmos MO.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Algo m√°s interesante puede suceder. Supongamos que estamos tratando de resolver un problema usando dos algoritmos MO, A y B. A veces sucede que el algoritmo A est√° por delante del algoritmo B en un conjunto de datos de entrenamiento, y el algoritmo B est√° por delante del algoritmo A en otro conjunto de datos de entrenamiento. No vimos esto arriba, entonces las gr√°ficas se intersectar√≠an, pero </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esto sucede</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . La respuesta correcta a la pregunta: "¬øEs el algoritmo A superior al algoritmo B?" de hecho, esto: "¬øQu√© conjunto de datos de entrenamiento est√°s usando?"</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Todo esto debe tenerse en cuenta, tanto durante el desarrollo como al leer art√≠culos cient√≠ficos. Muchos trabajos se concentran en encontrar nuevos trucos para obtener mejores resultados en conjuntos de datos de medici√≥n est√°ndar. ‚ÄúNuestra tecnolog√≠a de superalimento nos dio una mejora del X% en el conjunto comparativo est√°ndar Y‚Äù, el formulario de solicitud can√≥nico en dicho estudio. A veces, tales declaraciones son realmente interesantes, pero vale la pena entender que son aplicables solo en el contexto de un conjunto de capacitaci√≥n espec√≠fico. Imagine una historia alternativa en la que las personas que inicialmente crearon un conjunto comparativo recibieron una subvenci√≥n de investigaci√≥n m√°s grande. Podr√≠an usar dinero extra para recopilar datos adicionales. Es posible que la "mejora" de la tecnolog√≠a super-duper desaparezca en un conjunto de datos m√°s grande. En otras palabrasLa esencia de la mejora puede ser un accidente. A partir de esto, la siguiente moralidad debe tomarse en el campo de la aplicaci√≥n pr√°ctica: necesitamos algoritmos mejorados y datos de entrenamiento mejorados. No hay nada malo en buscar algoritmos mejorados, pero aseg√∫rese de no concentrarse en esto, ignorando la forma m√°s f√°cil de ganar aumentando el volumen o la calidad de los datos de entrenamiento.</font></font><br><br><h3>  Desaf√≠o </h3><br><ul><li>  .              ?                 .        ‚Äì  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   </a> ,  ,   ,      .   ,             .        -   ?   ,      . </li></ul><br><h3>  Resumen </h3><br>  Hemos completado nuestra inmersi√≥n en el reciclaje y la regularizaci√≥n.  Por supuesto, volveremos a estos problemas.  Como ya he mencionado varias veces, la recapacitaci√≥n es un gran problema en el campo de NS, especialmente a medida que las computadoras se vuelven m√°s potentes y podemos entrenar redes m√°s grandes.  Como resultado, existe una necesidad urgente de desarrollar t√©cnicas efectivas de regularizaci√≥n para reducir el reciclaje, por lo que esta √°rea es muy activa hoy. <br><br><h2>  Inicializaci√≥n de peso </h2><br>  Cuando creamos nuestro NS, debemos elegir los valores iniciales de los pesos y las compensaciones.  Hasta ahora, los hemos elegido de acuerdo con las pautas brevemente descritas en el Cap√≠tulo 1. Perm√≠tanme recordarles que elegimos pesos y compensaciones basados ‚Äã‚Äãen una distribuci√≥n gaussiana independiente con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1. Este enfoque funcion√≥ bien, pero parece bastante arbitrario, por lo que vale la pena. rev√≠selo y piense si es posible encontrar una mejor manera de asignar los pesos y desplazamientos iniciales y, tal vez, ayudar a nuestros NS a aprender m√°s r√°pido. <br><br>  Resulta que el proceso de inicializaci√≥n se puede mejorar seriamente en comparaci√≥n con la distribuci√≥n gaussiana normalizada.  Para entender esto, digamos que trabajamos con una red con una gran cantidad de neuronas de entrada, digamos, desde 1000. Y digamos que usamos la distribuci√≥n Gaussiana normalizada para inicializar los pesos conectados a la primera capa oculta.  Hasta ahora, me enfocar√© solo en las escalas que conectan las neuronas de entrada a la primera neurona en la capa oculta, e ignorar√© el resto de la red: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br>  Para simplificar, imaginemos que estamos tratando de entrenar la red con la entrada x, en la que la mitad de las neuronas de entrada est√°n activadas, es decir, tienen un valor de 1 y la otra mitad est√°n desactivadas, es decir, tienen un valor de 0. El siguiente argumento funciona en un caso m√°s general, pero es m√°s f√°cil para usted lo entender√© en este ejemplo particular.  Considere la suma ponderada z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b de entradas para una neurona oculta.  Los 500 miembros de la suma desaparecen porque la x <sub>j</sub> correspondiente es 0. Por lo tanto, z es la suma de 501 variables aleatorias gaussianas normalizadas, 500 pesos y 1 desplazamiento adicional.  Por lo tanto, el valor z en s√≠ tiene una distribuci√≥n gaussiana con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de ‚àö501 ‚âà 22.4.  Es decir, z tiene una distribuci√≥n gaussiana bastante amplia, sin picos agudos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br>  En particular, este gr√°fico muestra que | z | es probable que sea bastante grande, es decir, z ‚â´ 1 o z ‚â´ -1.  En este caso, la salida de las neuronas ocultas œÉ (z) estar√° muy cerca de 1 o 0. Esto significa que nuestra neurona oculta estar√° saturada.  Y cuando esto sucede, como ya sabemos, peque√±os cambios en los pesos producir√°n peque√±os cambios en la activaci√≥n de una neurona oculta.  Estos peque√±os cambios, a su vez, pr√°cticamente no afectar√°n a los neutrones restantes en la red, y veremos los peque√±os cambios correspondientes en la funci√≥n de costo.  Como resultado, estos pesos se entrenar√°n muy lentamente cuando usemos el algoritmo de descenso de gradiente.  Esto es similar a la tarea que ya discutimos en este cap√≠tulo, en la que las neuronas de salida saturadas con valores incorrectos hacen que el aprendizaje se ralentice.  Sol√≠amos resolver este problema eligiendo inteligentemente una funci√≥n de costo.  Desafortunadamente, aunque esto ayud√≥ con las neuronas de salida saturadas, no ayuda en absoluto con la saturaci√≥n de las neuronas ocultas. <br><br>  Ahora habl√© sobre las escalas entrantes de la primera capa oculta.  Naturalmente, los mismos argumentos se aplican a las siguientes capas ocultas: si los pesos en las capas ocultas posteriores se inicializan utilizando distribuciones gaussianas normalizadas, su activaci√≥n a menudo ser√° cercana a 0 o 1, y el entrenamiento ir√° muy lentamente. <br><br>  ¬øHay alguna manera de elegir las mejores opciones de inicializaci√≥n para pesos y compensaciones, para que no tengamos tanta saturaci√≥n y podamos evitar retrasos en el aprendizaje?  Supongamos que tenemos una neurona con el n√∫mero de pesos entrantes n <sub>en</sub> .  Luego, necesitamos inicializar estos pesos con distribuciones gaussianas aleatorias con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1 / ‚àön <sub>en</sub> .  Es decir, comprimimos los gaussianos y reducimos la probabilidad de saturaci√≥n de la neurona.  Luego, elegimos una distribuci√≥n gaussiana para desplazamientos con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1, por razones a las que volver√© m√°s adelante.  Una vez hecha esta elecci√≥n, nuevamente encontramos que z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b ser√° una variable aleatoria con una distribuci√≥n gaussiana con una expectativa matem√°tica de 0, pero con un pico mucho m√°s pronunciado que antes.  Suponga, como antes, que 500 entradas son 0 y 500 son 1. Entonces es f√°cil mostrar (vea el ejercicio a continuaci√≥n) que z tiene una distribuci√≥n gaussiana con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de ‚àö (3/2) = 1.22 ... Este gr√°fico tiene un pico mucho m√°s n√≠tido, tanto que incluso en la imagen a continuaci√≥n la situaci√≥n est√° algo subestimada, porque tuve que cambiar la escala del eje vertical en comparaci√≥n con el gr√°fico anterior: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br>  Tal neurona estar√° saturada con una probabilidad mucho m√°s baja y, en consecuencia, con una probabilidad m√°s baja encontrar√° una desaceleraci√≥n en el aprendizaje. <br><br><h3>  Ejercicio </h3><br><ul><li>  Confirme que la desviaci√≥n est√°ndar de z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b del p√°rrafo anterior es ‚àö (3/2).  Consideraciones a favor de esto: la varianza de la suma de las variables aleatorias independientes es igual a la suma de las varianzas de las variables aleatorias individuales;  la varianza es igual al cuadrado de la desviaci√≥n est√°ndar. </li></ul><br>  Mencion√© anteriormente que continuaremos inicializando los desplazamientos, como antes, basados ‚Äã‚Äãen una distribuci√≥n gaussiana independiente con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1. Y esto es normal, porque no aumenta en gran medida la probabilidad de saturaci√≥n de nuestras neuronas.  En realidad, la inicializaci√≥n de las compensaciones no importa mucho si logramos evitar el problema de saturaci√≥n.  Algunos incluso intentan inicializar todas las compensaciones a cero, y conf√≠an en el hecho de que el descenso de gradiente puede aprender las compensaciones apropiadas.  Pero dado que la probabilidad de que esto afecte algo es peque√±a, continuaremos usando el mismo procedimiento de inicializaci√≥n que antes. <br><br>  Comparemos los resultados de los enfoques antiguos y nuevos para inicializar pesos utilizando la tarea de clasificar n√∫meros de MNIST.  Como antes, utilizaremos 30 neuronas ocultas, un mini paquete de tama√±o 10, un par√°metro de regularizaci√≥n &amp; lambda = 5.0 y una funci√≥n de costo con entrop√≠a cruzada.  Reduciremos gradualmente la velocidad de aprendizaje de Œ∑ = 0.5 a 0.1, ya que de esta forma los resultados ser√°n ligeramente mejores visibles en los gr√°ficos.  Puede aprender usando el antiguo m√©todo de inicializaci√≥n de peso: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.large_weight_initializer() &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Tambi√©n puede aprender usando el nuevo enfoque para inicializar pesos.  Esto es a√∫n m√°s simple, porque por defecto network2 inicializa los pesos usando un nuevo enfoque.  Esto significa que podemos omitir la llamada net.large_weight_initializer () anterior: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Trazamos (usando el programa weight_initialization.py): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br>  En ambos casos, se obtiene una precisi√≥n de clasificaci√≥n del 96%.  La precisi√≥n resultante es casi la misma en ambos casos.  Pero la nueva t√©cnica de inicializaci√≥n llega a este punto mucho, mucho m√°s r√°pido.  Al final de la √∫ltima era de entrenamiento, el antiguo enfoque para inicializar pesos alcanza una precisi√≥n del 87%, y el nuevo enfoque ya se acerca al 93%.  Aparentemente, un nuevo enfoque para inicializar pesos comienza desde una posici√≥n mucho mejor, por lo que obtenemos buenos resultados mucho m√°s r√°pido.  El mismo fen√≥meno se observa si construimos los resultados para una red con 100 neuronas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br>  En este caso, no se producen dos curvas.  Sin embargo, mis experimentos dicen que si agrega un poco m√°s de eras, entonces la precisi√≥n comienza a coincidir.  Por lo tanto, sobre la base de estos experimentos, podemos decir que mejorar la inicializaci√≥n de los pesos solo acelera el entrenamiento, pero no cambia la eficiencia general de la red.  Sin embargo, en el cap√≠tulo 4 veremos ejemplos de NS en los que la eficiencia a largo plazo mejora significativamente como resultado de la inicializaci√≥n de los pesos a trav√©s de 1 / ‚àön.  Por lo tanto, mejora no solo la velocidad de aprendizaje, sino a veces la efectividad resultante. <br><br>  El enfoque para inicializar pesos a trav√©s de 1 / ‚àön ayuda a mejorar el entrenamiento de las redes neuronales.  Se han propuesto otras t√©cnicas para inicializar pesos, muchas de las cuales se basan en esta idea b√°sica.  No los considerar√© aqu√≠, ya que 1 / ‚àön funciona bien para nuestros prop√≥sitos.  Si est√° interesado, le recomiendo leer la discusi√≥n en las p√°ginas 14 y 15 en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documento de 2012 de</a> Yoshua Benggio. <br><br><h3>  Desaf√≠o </h3><br><ul><li>  La combinaci√≥n de regularizaci√≥n y un m√©todo mejorado de inicializaci√≥n de peso.  A veces, la regularizaci√≥n de L2 autom√°ticamente nos da resultados similares a un nuevo m√©todo de inicializaci√≥n de pesos.  Digamos que usamos el viejo enfoque para inicializar pesos.  Resuma un argumento heur√≠stico que demuestre que: (1) si Œª no es demasiado peque√±o, entonces, en las primeras √©pocas de entrenamiento, el debilitamiento de los pesos dominar√° casi por completo;  (2) si Œ∑Œª ‚â™ n, entonces los pesos se debilitar√°n e <sup>‚àíŒ∑Œª / m</sup> veces en la √©poca;  (3) si Œª no es demasiado grande, el debilitamiento de los pesos se ralentizar√° cuando los pesos disminuyan a aproximadamente 1 / ‚àön, donde n es el n√∫mero total de pesos en la red.  Demuestre que estas condiciones se cumplen en los ejemplos para los que se construyen gr√°ficos en esta secci√≥n. </li></ul><br><br><h2>  Volviendo al reconocimiento de escritura a mano: c√≥digo </h2><br>  Pongamos en pr√°ctica las ideas descritas en este cap√≠tulo.  Desarrollaremos un nuevo programa, network2.py, una versi√≥n mejorada del programa network.py que creamos en el cap√≠tulo 1. Si no ha visto su c√≥digo durante mucho tiempo, es posible que deba revisarlo r√°pidamente.  Estas son solo 74 l√≠neas de c√≥digo, y es f√°cil de entender. <br><br>  Al igual que con network.py, la estrella de network2.py es la clase Network, que usamos para representar nuestros NS.  Inicializamos la instancia de clase con una lista de tama√±os de las capas de red correspondientes, y con la elecci√≥n de la funci√≥n de costo, por defecto ser√° entrop√≠a cruzada: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes, cost=CrossEntropyCost)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost</code> </pre> <br>  Las primeras dos l√≠neas del m√©todo __init__ son las mismas que network.py, y se entienden por s√≠ mismas.  Las siguientes dos l√≠neas son nuevas y debemos comprender en detalle lo que est√°n haciendo. <br><br>  Comencemos con el m√©todo default_weight_initializer.  Utiliza un enfoque nuevo y mejorado para inicializar pesas.  Como hemos visto, en este enfoque, los pesos que ingresan a la neurona se inicializan sobre la base de una distribuci√≥n gaussiana independiente con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1 dividida por la ra√≠z cuadrada del n√∫mero de enlaces entrantes a la neurona.  Adem√°s, este m√©todo inicializar√° los desplazamientos utilizando la distribuci√≥n gaussiana con una media de 0 y una desviaci√≥n est√°ndar de 1. Aqu√≠ est√° el c√≥digo: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">default_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Para entenderlo, debe recordar que np es una biblioteca de Numpy que se ocupa del √°lgebra lineal.  Lo importamos al comienzo del programa.  Tambi√©n tenga en cuenta que no inicializamos desplazamientos en la primera capa de neuronas.  La primera capa es entrante, por lo que no se utilizan desplazamientos.  Lo mismo fue network.py. <br><br>  Adem√°s del m√©todo default_weight_initializer, crearemos un m√©todo large_weight_initializer.  Inicializa los pesos y las compensaciones utilizando el enfoque anterior del Cap√≠tulo 1, donde los pesos y las compensaciones se inicializan en funci√≥n de una distribuci√≥n gaussiana independiente con una expectativa matem√°tica de 0 y una desviaci√≥n est√°ndar de 1. Este c√≥digo, por supuesto, no es muy diferente de default_weight_initializer: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">large_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Inclu√≠ este m√©todo principalmente porque era m√°s conveniente para nosotros comparar los resultados de este cap√≠tulo y el cap√≠tulo 1. ¬°No puedo imaginar ninguna opci√≥n real en la que recomendar√≠a usarlo! <br><br>  La segunda novedad del m√©todo __init__ ser√° la inicializaci√≥n del atributo de costo.  Para comprender c√≥mo funciona esto, veamos la clase que usamos para representar la funci√≥n de costo de entrop√≠a cruzada (la directiva @staticmethod le dice al int√©rprete que este m√©todo es independiente del objeto, por lo que el par√°metro self no se pasa a los m√©todos fn y delta). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropyCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-a))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay)</code> </pre> <br>  Vamos a resolverlo.  Lo primero que se puede ver aqu√≠ es que, aunque la entrop√≠a cruzada es una funci√≥n desde un punto de vista matem√°tico, la implementamos como una clase de python, no como una funci√≥n de python.  ¬øPor qu√© decid√≠ hacer esto?  En nuestra red, el valor juega dos roles diferentes.  Obvio: es una medida de qu√© tan bien la activaci√≥n de salida a corresponde a la salida deseada y.  Este rol lo proporciona el m√©todo CrossEntropyCost.fn.  (Por cierto, tenga en cuenta que llamar a np.nan_to_num dentro de CrossEntropyCost.fn asegura que Numpy procese correctamente el logaritmo de los n√∫meros cercanos a cero).  Sin embargo, la funci√≥n de costo se utiliza en nuestra red de la segunda manera.  Recordamos del Cap√≠tulo 2 que al iniciar el algoritmo de retropropagaci√≥n, debemos considerar el error de salida de la red Œ¥ <sup>L.</sup>  La forma del error de salida depende de la funci√≥n de costo: diferentes funciones de costo tendr√°n diferentes formas de error de salida.  Para la entrop√≠a cruzada, el error de salida, como sigue de la ecuaci√≥n (66), ser√° igual a: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>99</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.761ex" height="2.901ex" viewBox="0 -987.6 9799.8 1249" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-65" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-6C" x="1240" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="1538" y="0"></use><g transform="translate(1900,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-3D" x="3289" y="0"></use><g transform="translate(4345,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-2212" x="5679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-79" x="6679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-74" x="7427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-61" x="7788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMATHI-67" x="8318" y="0"></use><g transform="translate(8798,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhh_o42b6cckA3w6QHIhs-qtCQTj3g#MJMAIN-39" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>‚àí</mo><mi>y</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>99</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \ delta ^ L = a ^ L-y \ tag {99} </script></p><br><br>  Por lo tanto, defino un segundo m√©todo, CrossEntropyCost.delta, cuyo objetivo es explicar a la red c√≥mo calcular el error de salida.  Y luego combinamos estos dos m√©todos en una clase que contiene todo lo que nuestra red necesita saber sobre la funci√≥n de costos. <br><br>  Por una raz√≥n similar, network2.py contiene una clase que representa una funci√≥n de costo cuadr√°tico.  Incluyendo esto para comparar con los resultados del Cap√≠tulo 1, ya que en el futuro utilizaremos principalmente la entrop√≠a cruzada.  El c√≥digo est√° abajo.  El m√©todo QuadraticCost.fn es un c√°lculo simple del costo cuadr√°tico asociado con la salida ay la salida deseada y.  El valor devuelto por QuadraticCost.delta se basa en la expresi√≥n (30) para el error de salida del valor cuadr√°tico, que derivamos en el Cap√≠tulo 2. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">QuadraticCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*np.linalg.norm(ay)**<span class="hljs-number"><span class="hljs-number">2</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay) * sigmoid_prime(z)</code> </pre> <br>  Ahora hemos descubierto las principales diferencias entre network2.py y network2.py.  Todo es muy sencillo.  Hay otros peque√±os cambios que describir√© a continuaci√≥n, incluida la implementaci√≥n de la regularizaci√≥n de L2.  Antes de eso, veamos el c√≥digo completo network2.py.  No es necesario estudiarlo en detalle, pero vale la pena entender la estructura b√°sica, en particular, leer los comentarios para comprender qu√© hace cada una de las partes del programa.  ¬°Por supuesto, no proh√≠bo profundizar en esta pregunta tanto como quieras!  Si se pierde, intente leer el texto despu√©s del programa y vuelva al c√≥digo nuevamente.  En general, aqu√≠ est√°: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network2.py ~~~~~~~~~~~~~~   network.py,            .   ‚Äì      , ,   .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import json import random import sys #  import numpy as np ####   ,      class QuadraticCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. """ return 0.5*np.linalg.norm(ay)**2 @staticmethod def delta(z, a, y): """  delta   .""" return (ay) * sigmoid_prime(z) class CrossEntropyCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """  delta   .  ``z``    ,          delta     . """ return (ay) ####   Network class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,   ``self.default_weight_initializer`` (.  ). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """            0    1,       ,       .          0    1.    ,         ,           . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """          0    1.          0    1.    ,         ,           .         1,    .       . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """  ,  ``a``  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """     -    . ``training_data`` ‚Äì   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   . """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: {}".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: {} / {}".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: {}".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: {} / {}".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """    ,          -. ``mini_batch`` ‚Äì    ``(x, y)``, ``eta`` ‚Äì  , ``lmbda`` -  , ``n`` -     .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """  l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      . """ for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """    ``data``,      .   ‚Äì        .  ``convert``  False,    ‚Äì    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     ‚Äì       ,      .   ,      .       mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """      ``data``.  ``convert``   False,   ‚Äì  (),   True,   ‚Äì   . .    ,      ``accuracy``, . """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """    ``filename``.""" data = {"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)} f = open(filename, "w") json.dump(data, f) f.close() ####  Network def load(filename): """    ``filename``.    Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net ####   def vectorized_result(j): """  10-    1.0   j     .      (0..9)     . """ e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Entre los cambios m√°s interesantes est√° la inclusi√≥n de la regularizaci√≥n L2.  Aunque este es un gran cambio conceptual, es tan f√°cil de implementar que es posible que no lo note en el c√≥digo.  En su mayor parte, esto es simplemente pasar el par√°metro lmbda a diferentes m√©todos, especialmente Network.SGD.  Todo el trabajo se lleva a cabo en una l√≠nea del programa, la cuarta desde el final en el m√©todo Network.update_mini_batch.  All√≠ cambiamos la regla de actualizaci√≥n de descenso de gradiente para incluir la reducci√≥n de peso.  ¬°El cambio es peque√±o, pero afecta gravemente los resultados! <br><br>  Esto, por cierto, a menudo sucede cuando se implementan nuevas t√©cnicas en redes neuronales.  Pasamos miles de palabras discutiendo la regularizaci√≥n.  Conceptualmente, esto es algo bastante sutil y dif√≠cil de entender.  Sin embargo, se puede agregar trivialmente al programa.  Inesperadamente, se pueden implementar t√©cnicas complejas con cambios menores en el c√≥digo. <br><br>  Otro cambio peque√±o pero importante en el c√≥digo es la adici√≥n de varios indicadores opcionales al m√©todo de descenso de gradiente estoc√°stico Network.SGD.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estas banderas hacen posible rastrear el costo y la precisi√≥n en training_data o Evaluation_data, que pueden transmitirse a Network.SGD. </font><font style="vertical-align: inherit;">Anteriormente en el cap√≠tulo, a menudo usamos estas banderas, pero perm√≠tanme dar un ejemplo de su uso, solo como recordatorio:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Configuramos Evaluation_data a trav√©s de validation_data. Sin embargo, podr√≠amos rastrear el rendimiento en test_data y cualquier otro conjunto de datos. Tambi√©n tenemos cuatro indicadores que especifican la necesidad de rastrear el costo y la precisi√≥n tanto en Evaluation_data como en Training_data. Estas banderas est√°n configuradas en False de manera predeterminada, sin embargo, se incluyen aqu√≠ para rastrear la efectividad de la Red. Adem√°s, el m√©todo Network.SGD de network2.py devuelve una tupla de cuatro elementos que representa los resultados del seguimiento. Puedes usarlo as√≠:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>evaluation_cost, evaluation_accuracy, ... training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, por ejemplo, Evaluation_cost ser√° una lista de 30 elementos que contienen el costo de los datos estimados al final de cada era. Dicha informaci√≥n es extremadamente √∫til para comprender el comportamiento de una red neuronal. Dicha informaci√≥n es extremadamente √∫til para comprender el comportamiento de la red. Por ejemplo, puede usarse para dibujar gr√°ficos de aprendizaje en red a lo largo del tiempo. As√≠ es como constru√≠ todos los gr√°ficos de este cap√≠tulo. Sin embargo, si uno de los indicadores no est√° configurado, el elemento de tupla correspondiente ser√° una lista vac√≠a.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Otras adiciones de c√≥digo incluyen el m√©todo Network.save, que guarda el objeto de red en el disco, y la funci√≥n de cargarlo en la memoria. El guardado y la carga se realizan a trav√©s de JSON, no los m√≥dulos Python pickle o cPickle, que generalmente se usan para guardar en el disco y cargar en python. El uso de JSON requiere m√°s c√≥digo del que ser√≠a necesario para pickle o cPickle. Para entender por qu√© eleg√≠ JSON, imagine que en alg√∫n momento en el futuro decidimos cambiar nuestra clase de red para que haya m√°s neuronas sigmoideas. Para implementar este cambio, lo m√°s probable es que cambiemos los atributos definidos en el m√©todo Network .__ init__. Y si solo us√°ramos pickle para guardar, nuestra funci√≥n de carga no funcionar√≠a. El uso de JSON con serializaci√≥n expl√≠cita nos facilita garantizarque las versiones anteriores del objeto de red se pueden descargar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hay muchos peque√±os cambios en el c√≥digo, pero estas son solo peque√±as variaciones de network.py. </font><font style="vertical-align: inherit;">El resultado final es una extensi√≥n de nuestro programa de 74 l√≠neas a un programa mucho m√°s funcional de 152 l√≠neas.</font></font><br><br><h3>  Desaf√≠o </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modifique el siguiente c√≥digo introduciendo la regularizaci√≥n L1, y util√≠cela para clasificar los d√≠gitos MNIST por una red con 30 neuronas ocultas. </font><font style="vertical-align: inherit;">¬øPuede elegir un par√°metro de regularizaci√≥n que le permita mejorar el resultado en comparaci√≥n con una red sin regularizaci√≥n?</font></font></li><li>    Network.cost_derivative method  network.py.      .        ?     ,       ?  network2.py      Network.cost_derivative,     CrossEntropyCost.delta.      ? </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/459816/">https://habr.com/ru/post/459816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../459802/index.html">Habr Weekly # 9 / Burnout en j√≥venes, interfaces japonesas, red neuronal Battle.net, juegos y crueldad</a></li>
<li><a href="../459804/index.html">Crear tarjetas de ayuda de crowdsourcing en WordPress + shMapper</a></li>
<li><a href="../459806/index.html">C√≥mo tratamos al gato Lapuna</a></li>
<li><a href="../459810/index.html">Microservicios o monolitos: buscando una soluci√≥n</a></li>
<li><a href="../459814/index.html">¬øQu√© eres, motor de renderizado? O c√≥mo funciona el m√≥dulo de visualizaci√≥n del navegador</a></li>
<li><a href="../459820/index.html">Simplemente deslice la tarjeta: c√≥mo se usa OS / 2 en el metro de Nueva York</a></li>
<li><a href="../459822/index.html">Un ejemplo de una red neuronal simple, como resultado, descubre qu√© es qu√©</a></li>
<li><a href="../459824/index.html">Lista de verificaci√≥n para escribir excelentes extensiones de Visual Studio</a></li>
<li><a href="../459828/index.html">Noticias semanales: precio del boleto Hyperloop en Rusia, miner√≠a convencional de computadoras Apollo, bot AI en StarCraft II</a></li>
<li><a href="../459830/index.html">Por supuesto, dieron poder y una l√≠nea de una ametralladora. C√°ncer y m√°s ... experiencia con medicina</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>