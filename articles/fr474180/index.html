<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçüî¨ üë®üèº‚Äçü§ù‚Äçüë®üèª üë©‚Äçüî¨ Comment l'architecture Web √† tol√©rance de pannes est impl√©ment√©e dans la plate-forme Mail.ru Cloud Solutions üëâüèø üíÖüèº üò≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je suis Artyom Karamyshev, chef de l'√©quipe d'administration syst√®me chez Mail.Ru Cloud Solutions (MCS) . Au cours de la derni√®re ann√©e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment l'architecture Web √† tol√©rance de pannes est impl√©ment√©e dans la plate-forme Mail.ru Cloud Solutions</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/474180/"><img src="https://habrastorage.org/webt/gd/wp/de/gdwpdevye3ploqkbmd4rwjqkvva.jpeg"><br><br>  Bonjour, Habr!  Je suis Artyom Karamyshev, chef de l'√©quipe d'administration syst√®me chez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mail.Ru Cloud Solutions (MCS)</a> .  Au cours de la derni√®re ann√©e, nous avons eu de nombreux lancements de nouveaux produits.  Nous voulions que les services API √©voluent facilement, soient tol√©rants aux pannes et pr√™ts pour la croissance rapide de la charge utilisateur.  Notre plateforme est impl√©ment√©e sur OpenStack, et je veux vous dire quels probl√®mes de tol√©rance aux pannes de composants nous avons d√ª fermer afin d'obtenir un syst√®me tol√©rant aux pannes.  Je pense que cela sera int√©ressant pour ceux qui d√©veloppent √©galement des produits sur OpenStack. <br><br>  La tol√©rance de panne globale de la plate-forme consiste en la stabilit√© de ses composants.  Nous allons donc progressivement passer par tous les niveaux auxquels nous avons d√©couvert les risques et les avons ferm√©s. <br><br>  Une version vid√©o de cette histoire, dont la source √©tait un reportage lors de la conf√©rence Uptime day 4 organis√©e par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ITSumma</a> , peut √™tre visionn√©e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur la cha√Æne YouTube Uptime Community</a> . <br><a name="habracut"></a><br>
<h2>  Tol√©rance aux pannes de l'architecture physique </h2><br>  La partie publique du cloud MCS est d√©sormais bas√©e dans deux centres de donn√©es de niveau III, entre eux il y a une fibre noire qui lui est propre, r√©serv√©e sur la couche physique par diff√©rentes routes, avec un d√©bit de 200 Gb / s.  Le niveau de niveau III fournit le niveau n√©cessaire de r√©silience de l'infrastructure physique. <br><br>  La fibre noire est r√©serv√©e aux niveaux physique et logique.  Le processus de r√©servation de canaux a √©t√© it√©ratif, des probl√®mes sont survenus et nous am√©liorons constamment la communication entre les centres de donn√©es. <br><br><blockquote>  Par exemple, il n'y a pas si longtemps, lorsqu'elle travaillait dans un puits √† c√¥t√© de l'un des centres de donn√©es, une excavatrice a perfor√© un tuyau, √† l'int√©rieur de ce tuyau, il y avait √† la fois un c√¢ble optique principal et un c√¢ble optique de secours.  Notre canal de communication tol√©rant aux pannes avec le centre de donn√©es s'est r√©v√©l√© vuln√©rable √† un moment donn√©, dans le puits.  En cons√©quence, nous avons perdu une partie de l'infrastructure.  Nous avons tir√© des conclusions, pris un certain nombre de mesures, notamment la pose d'optiques suppl√©mentaires le long d'un puits voisin. </blockquote><br>  Dans les centres de donn√©es, il existe des points de pr√©sence de fournisseurs de communication auxquels nous diffusons nos pr√©fixes via BGP.  Pour chaque direction de r√©seau, la meilleure m√©trique est s√©lectionn√©e, ce qui permet √† diff√©rents clients de fournir la meilleure qualit√© de connexion.  Si la communication via un fournisseur est d√©connect√©e, nous reconstruisons notre routage via les fournisseurs disponibles. <br><br>  En cas de d√©faillance d'un fournisseur, nous passons automatiquement au suivant.  En cas de d√©faillance de l'un des centres de donn√©es, nous avons une copie miroir de nos services dans le deuxi√®me centre de donn√©es, qui prend tout le fardeau sur eux-m√™mes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d0/m8/ly/d0m8lykvifmc-gum-h9mbppyn3g.jpeg"></div><br>  <i>R√©silience de l'infrastructure physique</i> <br><br><h2>  Ce que nous utilisons pour la tol√©rance aux pannes au niveau de l'application </h2><br>  Notre service est construit sur un certain nombre de composants open source. <br><br>  <b>ExaBGP</b> est un service qui impl√©mente un certain nombre de fonctions utilisant le protocole de routage dynamique bas√© sur BGP.  Nous l'utilisons activement pour annoncer nos adresses IP blanches par lesquelles les utilisateurs ont acc√®s √† l'API. <br><br>  <b>HAProxy</b> est un √©quilibreur tr√®s charg√© qui vous permet de configurer des r√®gles tr√®s flexibles pour √©quilibrer le trafic √† diff√©rents niveaux du mod√®le OSI.  Nous l'utilisons pour √©quilibrer tous les services: bases de donn√©es, courtiers de messages, services API, services Web, nos projets internes - tout est derri√®re HAProxy. <br><br>  <b>Application API</b> - une <b>application</b> web √©crite en python, avec laquelle l'utilisateur contr√¥le son infrastructure, son service. <br><br>  <b>Application de travail</b> (ci-apr√®s simplement appel√©e travailleur) - dans les services OpenStack, il s'agit d'un d√©mon d'infrastructure qui vous permet de traduire des commandes API dans l'infrastructure.  Par exemple, un disque est cr√©√© dans Worker et une demande de cr√©ation se trouve dans l'API d'application. <br><br><h2>  Architecture d'application OpenStack standard </h2><br>  La plupart des services d√©velopp√©s pour OpenStack essaient de suivre un seul paradigme.  Un service se compose g√©n√©ralement de 2 parties: l'API et les travailleurs (ex√©cuteurs principaux).  En r√®gle g√©n√©rale, une API est une application WSGI python qui s'ex√©cute en tant que processus autonome (d√©mon) ou en utilisant un serveur Web Nginx pr√™t √† l'emploi, Apache.  L'API traite la demande de l'utilisateur et transmet d'autres instructions √† l'application de travail.  La transmission se fait √† l'aide d'un courtier de messages, g√©n√©ralement RabbitMQ, les autres sont mal pris en charge.  Lorsque les messages parviennent au courtier, ils sont trait√©s par les employ√©s et, si n√©cessaire, renvoient une r√©ponse. <br><br>  Ce paradigme implique des points de d√©faillance communs isol√©s: RabbitMQ et la base de donn√©es.  Mais RabbitMQ est isol√© au sein d'un service et, en th√©orie, peut √™tre individuel pour chaque service.  Donc, chez MCS, nous partageons ces services autant que possible, pour chaque projet individuel, nous cr√©ons une base de donn√©es distincte, un RabbitMQ distinct.  Cette approche est bonne car en cas d'accident sur certains points vuln√©rables, pas toutes les ruptures de service, mais seulement une partie. <br><br>  Le nombre d'applications de travail est illimit√©, de sorte que l'API peut facilement √©voluer horizontalement derri√®re les √©quilibreurs afin d'augmenter la productivit√© et la tol√©rance aux pannes. <br><br><blockquote>  Certains services n√©cessitent une coordination au sein du service - lorsque des op√©rations s√©quentielles complexes se produisent entre les API et les travailleurs.  Dans ce cas, un seul centre de coordination est utilis√©, un syst√®me de cluster tel que Redis, Memcache, etcd, qui permet √† un travailleur de dire √† l'autre que cette t√¢che lui est assign√©e ("s'il vous pla√Æt ne le prenez pas").  Nous utilisons etcd.  En r√®gle g√©n√©rale, les travailleurs communiquent activement avec la base de donn√©es, y √©crivent et lisent des informations.  En tant que base de donn√©es, nous utilisons mariadb, que nous avons dans le cluster multima√Ætre. <br></blockquote><br>  Un tel service mono-utilisateur classique est organis√© d'une mani√®re g√©n√©ralement accept√©e pour OpenStack.  Il peut √™tre consid√©r√© comme un syst√®me ferm√©, pour lequel les m√©thodes de mise √† l'√©chelle et de tol√©rance aux pannes sont assez √©videntes.  Par exemple, pour la tol√©rance aux pannes de l'API, il suffit de placer un √©quilibreur devant eux.  La mise √† l'√©chelle des travailleurs est obtenue en augmentant leur nombre. <br><br>  Les points faibles de l'ensemble du sch√©ma sont RabbitMQ et MariaDB.  Leur architecture m√©rite un article s√©par√©. Dans cet article, je veux me concentrer sur la tol√©rance aux pannes de l'API. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1a/ab/i1/1aabi1ew0ctxnlrcm2j78nbhefk.jpeg"></div><br>  <i>Architecture d'application Openstack</i>  <i>√âquilibrage et r√©silience des plateformes cloud</i> <br><br><h2>  Rendre HAProxy Balancer r√©silient avec ExaBGP </h2><br>  Pour rendre nos API √©volutives, rapides et tol√©rantes aux pannes, nous avons plac√© un √©quilibreur devant elles.  Nous avons choisi HAProxy.  √Ä mon avis, il poss√®de toutes les caract√©ristiques n√©cessaires √† notre t√¢che: √©quilibrage √† plusieurs niveaux OSI, interface de gestion, flexibilit√© et √©volutivit√©, un grand nombre de m√©thodes d'√©quilibrage, prise en charge des tables de session. <br><br>  Le premier probl√®me qui devait √™tre r√©solu √©tait la tol√©rance aux pannes de l'√©quilibreur lui-m√™me.  L'installation de l'√©quilibreur cr√©e √©galement un point d'√©chec: l'√©quilibreur se casse - le service tombe.  Pour √©viter cela, nous avons utilis√© HAProxy avec ExaBGP. <br><br>  ExaBGP vous permet de mettre en ≈ìuvre un m√©canisme de v√©rification de l'√©tat d'un service.  Nous avons utilis√© ce m√©canisme pour v√©rifier la fonctionnalit√© de HAProxy et en cas de probl√®me, d√©sactiver le service HAProxy de BGP. <br><br>  <b>Sch√©ma ExaBGP + HAProxy</b> <br><br><ol><li>  Nous installons le logiciel n√©cessaire sur trois serveurs, ExaBGP et HAProxy. </li><li>  Sur chacun des serveurs, nous cr√©ons une interface de bouclage. </li><li>  Sur les trois serveurs, nous attribuons la m√™me adresse IP blanche √† cette interface. </li><li>  Une adresse IP blanche est annonc√©e sur Internet via ExaBGP. </li></ol><br>  La tol√©rance aux pannes est obtenue en annon√ßant la m√™me adresse IP √† partir des trois serveurs.  Du point de vue du r√©seau, la m√™me adresse est accessible √† partir de trois prochains espoirs diff√©rents.  Le routeur voit trois itin√©raires identiques, s√©lectionne le plus prioritaire d'entre eux en fonction de sa propre m√©trique (c'est g√©n√©ralement la m√™me option), et le trafic ne va qu'√† l'un des serveurs. <br><br>  En cas de probl√®mes avec le fonctionnement HAProxy ou de d√©faillance du serveur, ExaBGP arr√™te d'annoncer l'itin√©raire et le trafic bascule en douceur vers un autre serveur. <br><br>  Ainsi, nous avons atteint la tol√©rance aux pannes de l'√©quilibreur. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ij/c_/cz/ijc_cz1jvhlug0axiwiiqjqpcww.jpeg"></div><br>  <i>Tol√©rance aux pannes des √©quilibreurs d'HAProxy</i> <br><br>  Le sch√©ma s'est av√©r√© imparfait: nous avons appris √† r√©server HAProxy, mais nous n'avons pas appris √† r√©partir la charge √† l'int√©rieur des services.  Par cons√©quent, nous avons d√©velopp√© un peu ce sch√©ma: nous sommes pass√©s √† l'√©quilibre entre plusieurs adresses IP blanches. <br><br><h2>  DNS Based Balancing Plus BGP </h2><br>  Le probl√®me de l'√©quilibrage de charge avant notre HAProxy n'est pas encore r√©solu.  N√©anmoins, cela peut √™tre r√©solu tout simplement, comme nous l'avons fait √† la maison. <br><br>  Pour √©quilibrer les trois serveurs, vous aurez besoin de 3 adresses IP blanches et d'un bon vieux DNS.  Chacune de ces adresses est d√©finie sur l'interface de bouclage de chaque HAProxy et est annonc√©e sur Internet. <br><br>  OpenStack utilise un catalogue de services pour g√©rer les ressources, qui d√©finit l'API de noeud final d'un service.  Dans ce r√©pertoire, nous prescrivons un nom de domaine - public.infra.mail.ru, qui se r√©sout via DNS avec trois adresses IP diff√©rentes.  Par cons√©quent, nous obtenons un √©quilibrage de charge entre les trois adresses via DNS. <br><br>  Mais puisque lors de l'annonce des adresses IP blanches, nous ne contr√¥lons pas les priorit√©s de s√©lection du serveur, jusqu'√† pr√©sent, cela n'est pas √©quilibr√©.  En r√®gle g√©n√©rale, un seul serveur sera s√©lectionn√© par priorit√© de l'adresse IP et les deux autres seront inactifs, car aucune m√©trique n'est sp√©cifi√©e dans BGP. <br><br>  Nous avons commenc√© √† proposer des itin√©raires via ExaBGP avec diff√©rentes m√©triques.  Chaque √©quilibreur annonce les trois adresses IP blanches, mais l'une d'entre elles, la principale pour cet √©quilibreur, est annonc√©e avec une m√©trique minimale.  Ainsi, pendant que les trois √©quilibreurs fonctionnent, les appels √† la premi√®re adresse IP tombent sur le premier √©quilibreur, les appels √† la seconde √† la seconde, au troisi√®me au troisi√®me. <br><br>  Que se passe-t-il lorsque l'un des √©quilibreurs tombe?  En cas de d√©faillance d'un √©quilibreur par sa base, l'adresse est toujours annonc√©e par les deux autres, le trafic entre eux est redistribu√©.  Ainsi, nous donnons √† l'utilisateur via le DNS plusieurs adresses IP √† la fois.  En √©quilibrant sur DNS et diff√©rentes m√©triques, nous obtenons une distribution de charge uniforme sur les trois √©quilibreurs.  Et en m√™me temps, nous ne perdons pas la tol√©rance aux fautes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ek/xc/3z/ekxc3zsz5oazwdiqziwp_idfk7a.jpeg"></div><br>  <i>√âquilibrage HAProxy bas√© sur DNS + BGP</i> <br><br><h2>  Interaction entre ExaBGP et HAProxy </h2><br>  Nous avons donc impl√©ment√© la tol√©rance aux pannes en cas de d√©part du serveur, en fonction de la fin de l'annonce des itin√©raires.  Mais HAProxy peut √©galement √™tre d√©connect√© pour d'autres raisons que la d√©faillance du serveur: erreurs d'administration, d√©faillances de service.  Nous voulons retirer l'√©quilibreur cass√© sous la charge et dans ces cas, et nous avons besoin d'un autre m√©canisme. <br><br>  Par cons√©quent, en √©tendant le sch√©ma pr√©c√©dent, nous avons impl√©ment√© un battement de c≈ìur entre ExaBGP et HAProxy.  Il s'agit d'une impl√©mentation logicielle de l'interaction entre ExaBGP et HAProxy, lorsque ExaBGP utilise des scripts personnalis√©s pour v√©rifier l'√©tat des applications. <br><br>  Pour ce faire, dans la configuration ExaBGP, vous devez configurer un v√©rificateur d'int√©grit√© capable de v√©rifier l'√©tat de HAProxy.  Dans notre cas, nous avons configur√© le backend de sant√© dans HAProxy, et du c√¥t√© d'ExaBGP, nous v√©rifions avec une simple demande GET.  Si l'annonce cesse de se produire, alors HAProxy ne fonctionne probablement pas et il n'est pas n√©cessaire de l'annoncer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w0/ac/5c/w0ac5cvqsvjtki2cqzcgtgyk4x4.jpeg"></div><br>  <i>Bilan de sant√© HAProxy</i> <br><br><h2>  HAProxy Peers: synchronisation de session </h2><br>  La prochaine chose √† faire √©tait de synchroniser les sessions.  Lorsque vous travaillez avec des √©quilibreurs distribu√©s, il est difficile d'organiser le stockage des informations sur les sessions client.  Mais HAProxy est l'un des rares √©quilibreurs qui peut le faire en raison de la fonctionnalit√© Peers - la possibilit√© de transf√©rer des tables de session entre diff√©rents processus HAProxy. <br><br>  Il existe diff√©rentes m√©thodes d'√©quilibrage: simples, telles que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©p√©tition altern√©e</a> et avanc√©es, lorsqu'une session client est m√©moris√©e et chaque fois qu'elle arrive sur le m√™me serveur qu'auparavant.  Nous voulions mettre en ≈ìuvre la deuxi√®me option. <br><br>  HAProxy utilise des stick-tables pour enregistrer les sessions client pour ce m√©canisme.  Ils enregistrent l'adresse IP source du client, l'adresse cible s√©lectionn√©e (backend) et certaines informations de service.  En r√®gle g√©n√©rale, les tables Stick sont utilis√©es pour enregistrer la paire source-IP + destination-IP, ce qui est particuli√®rement utile pour les applications qui ne peuvent pas transmettre le contexte de session de l'utilisateur lors du basculement vers un autre √©quilibreur, par exemple, en mode d'√©quilibrage RoundRobin. <br><br>  Si la stick-table apprend √† se d√©placer entre diff√©rents processus HAProxy (entre lesquels l'√©quilibrage se produit), nos √©quilibreurs pourront travailler avec un seul pool de stick-tables.  Cela permettra de basculer de mani√®re transparente sur le r√©seau client lorsque l'un des √©quilibreurs tombe, le travail avec les sessions client continuera sur les m√™mes backends que ceux pr√©c√©demment s√©lectionn√©s. <br><br>  Pour un fonctionnement correct, l'adresse IP source de l'√©quilibreur √† partir duquel la session est √©tablie doit √™tre r√©solue.  Dans notre cas, il s'agit d'une adresse dynamique sur l'interface de bouclage. <br><br>  Le bon fonctionnement des pairs n'est atteint que dans certaines conditions.  Autrement dit, les d√©lais d'expiration TCP doivent √™tre suffisamment importants ou le commutateur doit √™tre suffisamment rapide pour que la session TCP n'ait pas le temps de se rompre.  Cependant, cela permet une commutation transparente. <br><br>  Chez IaaS, nous avons un service bas√© sur la m√™me technologie.  Il s'agit d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Load Balancer en tant que service pour OpenStack</a> appel√© Octavia.  Il est bas√© sur la base de deux processus HAProxy, il comprenait √† l'origine le support des pairs.  Ils ont fait leurs preuves dans ce service. <br><br>  L'image montre sch√©matiquement le mouvement des pairs-tables entre trois instances HAProxy, une configuration est sugg√©r√©e, comment cela peut √™tre configur√©: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ov/ol/wr/ovolwrmp-gzrvyybotjjagtb-re.jpeg"></div><br>  <i>HAProxy Peers (synchronisation de session)</i> <br><br>  Si vous impl√©mentez le m√™me sch√©ma, son travail doit √™tre soigneusement test√©.  Pas le fait que cela fonctionnera de la m√™me mani√®re dans 100% des cas.  Mais au moins, vous ne perdrez pas les tables de b√¢ton lorsque vous devez vous souvenir de l'IP source du client. <br><br><h2>  Limiter le nombre de demandes simultan√©es du m√™me client </h2><br>  Tous les services qui sont dans le domaine public, y compris nos API, peuvent faire l'objet d'avalanches de demandes.  Les raisons peuvent √™tre compl√®tement diff√©rentes, des erreurs des utilisateurs aux attaques cibl√©es.  Nous sommes p√©riodiquement DDoS aux adresses IP.  Les clients font souvent des erreurs dans leurs scripts, ils font de nous des mini-DDoS. <br><br>  D'une mani√®re ou d'une autre, une protection suppl√©mentaire doit √™tre fournie.  La solution √©vidente est de limiter le nombre de requ√™tes API et de ne pas perdre de temps CPU √† traiter les requ√™tes malveillantes. <br><br>  Pour mettre en ≈ìuvre de telles restrictions, nous utilisons des limites de taux, organis√©es sur la base de HAProxy, en utilisant les m√™mes stick-tables.  Les limites sont configur√©es assez simplement et vous permettent de limiter l'utilisateur par le nombre de requ√™tes √† l'API.  L'algorithme se souvient de l'adresse IP source √† partir de laquelle les demandes sont effectu√©es et limite le nombre de demandes simultan√©es d'un utilisateur.  Bien s√ªr, nous avons calcul√© le profil de charge API moyen pour chaque service et fix√© la limite ‚âà 10 fois cette valeur.  Jusqu'√† pr√©sent, nous continuons de suivre de pr√®s la situation, nous gardons le doigt sur le pouls. <br><br>  √Ä quoi cela ressemble-t-il dans la pratique?  Nous avons des clients qui utilisent constamment nos API de mise √† l'√©chelle automatique.  Ils cr√©ent environ deux ou trois cents machines virtuelles plus pr√®s du matin et les suppriment plus pr√®s du soir.  Pour OpenStack, cr√©ez une machine virtuelle, √©galement avec des services PaaS, au moins 1 000 demandes d'API, car l'interaction entre les services s'effectue √©galement via l'API. <br><br>  Un tel lancement de t√¢che entra√Æne une charge assez importante.  Nous avons estim√© cette charge, collect√© les pics quotidiens, les multipli√© par dix, ce qui est devenu notre limite de taux.  Nous gardons le doigt sur le pouls.  Nous voyons souvent des robots, des scanners, qui essaient de nous regarder, avons-nous des scripts CGA qui peuvent √™tre ex√©cut√©s, nous les coupons activement. <br><br><h2>  Comment mettre √† jour la base de code discr√®tement pour les utilisateurs </h2><br>  Nous impl√©mentons √©galement la tol√©rance aux pannes au niveau des processus de d√©ploiement de code.  Il y a des plantages lors des d√©ploiements, mais leur impact sur la disponibilit√© des services peut √™tre minimis√©. <br><br>  Nous mettons constamment √† jour nos services et devons assurer le processus de mise √† jour de la base de code sans effet pour les utilisateurs.  Nous avons r√©ussi √† r√©soudre ce probl√®me en utilisant les capacit√©s de gestion HAProxy et la mise en ≈ìuvre de Graceful Shutdown dans nos services. <br><br>  Pour r√©soudre ce probl√®me, il √©tait n√©cessaire de fournir un contr√¥le de l'√©quilibreur et l'arr√™t ¬´correct¬ª des services: <br><br><ul><li>  Dans le cas de HAProxy, le contr√¥le se fait via le fichier stats, qui est essentiellement un socket et est d√©fini dans la configuration HAProxy.  Vous pouvez lui envoyer des commandes via stdio.  Mais notre principal outil de contr√¥le de configuration est ansible, il a donc un module int√©gr√© pour g√©rer HAProxy.  Que nous utilisons activement. </li><li>  La plupart de nos services API et moteur prennent en charge des technologies d'arr√™t gracieuses: √† l'arr√™t, ils attendent la fin de la t√¢che en cours, que ce soit une requ√™te http ou une sorte de t√¢che utilitaire.  La m√™me chose se produit avec le travailleur.  Il conna√Æt toutes les t√¢ches qu'il accomplit et se termine lorsqu'il a tout accompli avec succ√®s. </li></ul><br>  Gr√¢ce √† ces deux points, l'algorithme s√ªr de notre d√©ploiement est le suivant. <br><br><ol><li>  Le d√©veloppeur construit un nouveau package de code (nous avons RPM), teste dans l'environnement de d√©veloppement, teste √† l'√©tape et le laisse dans le r√©f√©rentiel de l'√©tape. </li><li>  Le d√©veloppeur place la t√¢che sur le d√©ploiement avec la description la plus d√©taill√©e des ¬´artefacts¬ª: la version du nouveau package, une description de la nouvelle fonctionnalit√© et d'autres d√©tails sur le d√©ploiement, si n√©cessaire. </li><li>  L'administrateur syst√®me d√©marre la mise √† niveau.  Lance le playbook Ansible, qui √† son tour effectue les op√©rations suivantes: <br><ul><li>  Il prend un package du r√©f√©rentiel d'√©tape, met √† jour la version du package dans le r√©f√©rentiel de produit avec lui. </li><li>  Fait une liste de backends du service mis √† jour. </li><li>  D√©sactive le premier service mis √† jour dans HAProxy et attend la fin de ses processus.  Gr√¢ce √† un arr√™t progressif, nous sommes convaincus que toutes les demandes actuelles des clients se termineront avec succ√®s. </li><li>  Une fois l'API, les travailleurs et HAProxy compl√®tement arr√™t√©s, le code est mis √† jour. </li><li>  Ansible lance ses services. </li><li>  Pour chaque service, il tire certains ¬´stylos¬ª qui effectuent des tests unitaires pour un certain nombre de tests cl√©s pr√©d√©finis.  Une v√©rification de base du nouveau code se produit. </li><li>  Si aucune erreur n'a √©t√© trouv√©e √† l'√©tape pr√©c√©dente, le backend est activ√©. </li><li>  Acc√©dez au backend suivant. </li></ul></li><li>  Apr√®s la mise √† jour de tous les backends, des tests fonctionnels sont lanc√©s.  S'ils ne suffisent pas, le d√©veloppeur examine toutes les nouvelles fonctionnalit√©s qu'il a faites. </li></ol><br>  Sur ce d√©ploiement est termin√©. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-b/km/dt/-bkmdt98ituj53jetxiaay4uf4c.jpeg"></div><br>  <i>Cycle de mise √† jour du service</i> <br><br>  Ce sch√©ma ne fonctionnerait pas si nous n'avions pas une seule r√®gle.  Nous prenons en charge les anciennes et les nouvelles versions en combat.  √Ä l'avance, au stade du d√©veloppement logiciel, il est pr√©vu que m√™me s'il y a des changements dans la base de donn√©es des services, ils ne casseront pas le code pr√©c√©dent.  En cons√©quence, la base de code est progressivement mise √† jour. <br><br><h2>  Conclusion </h2><br>  Partageant mes propres r√©flexions sur l'architecture WEB tol√©rante aux pannes, je veux encore une fois noter ses points cl√©s: <br><br><ul><li>  tol√©rance aux pannes physiques; </li><li>  tol√©rance aux pannes du r√©seau (√©quilibreurs, BGP); </li><li>     . </li></ul><br>   uptime! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr474180/">https://habr.com/ru/post/fr474180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr474166/index.html">Index de couverture pour GiST</a></li>
<li><a href="../fr474170/index.html">Design Confession - 15 novembre, Moscou, DI Telegraph</a></li>
<li><a href="../fr474172/index.html">Une amende de 30 mille euros pour l'utilisation ill√©gale de cookies</a></li>
<li><a href="../fr474176/index.html">11 vid√©os du premier jour du DevFest 2019 √† Kaliningrad</a></li>
<li><a href="../fr474178/index.html">IVR sur Webhook</a></li>
<li><a href="../fr474184/index.html">Nous relevons le d√©fi de Callum Macrae √† 100%</a></li>
<li><a href="../fr474186/index.html">R√©futer les mythes: de v√©ritables pratiques informatiques en Arm√©nie</a></li>
<li><a href="../fr474192/index.html">Pourquoi suis-je pass√© de UX √† PM puis √† Lead PM et qu'est-ce qui a chang√©?</a></li>
<li><a href="../fr474194/index.html">√âquipe Compass</a></li>
<li><a href="../fr474196/index.html">Les 10 √©tapes les plus importantes du d√©veloppement de l'IA aujourd'hui</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>