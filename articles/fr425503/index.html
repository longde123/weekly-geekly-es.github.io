<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙇🏿 🎚️ 🚄 Évier Cassandra pour le streaming structuré Spark 🤹🏻 🎬 👩🏼‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il y a quelques mois, j'ai commencé à étudier Spark, et à un moment donné, j'ai été confronté au problème de l'enregistrement des calculs de Structure...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Évier Cassandra pour le streaming structuré Spark</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Il y a quelques mois, j'ai commencé à étudier Spark, et à un moment donné, j'ai été confronté au problème de l'enregistrement des calculs de Structured Streaming dans la base de données Cassandra. <br><br>  Dans cet article, je donne un exemple simple de création et d'utilisation de Cassandra Sink pour Spark Structured Streaming.  J'espère que le message sera utile à ceux qui ont récemment commencé à travailler avec Spark Structured Streaming et se demandent comment télécharger les résultats des calculs dans la base de données. <br><br>  L'idée de l'application est très simple - pour recevoir et analyser des messages de Kafka, effectuer des transformations simples dans une paire et enregistrer les résultats dans cassandra. <br><a name="habracut"></a><br><h3>  Avantages du streaming structuré </h3><br>  Vous pouvez en savoir plus sur Structured Streaming dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> .  En bref, Structured Streaming est un moteur de traitement d'informations de streaming bien évolutif basé sur le moteur Spark SQL.  Il vous permet d'utiliser Dataset / DataFrame pour agréger des données, calculer des fonctions de fenêtre, des connexions, etc. C'est-à-dire que Structured Streaming vous permet d'utiliser le bon vieux SQL pour travailler avec des flux de données. <br><br><h3>  Quel est le problème? </h3><br>  La version stable de Spark Structured Streaming a été publiée en 2017.  Autrement dit, il s'agit d'une API assez nouvelle qui implémente les fonctionnalités de base, mais certaines choses devront être faites par nous-mêmes.  Par exemple, Structured Streaming a des fonctions standard pour écrire la sortie dans un fichier, une tuile, une console ou une mémoire, mais pour enregistrer les données dans la base de données, vous devez utiliser le récepteur <i>foreach</i> disponible dans Structured Streaming et implémenter l'interface <i>ForeachWriter</i> .  <b>Depuis Spark 2.3.1, cette fonctionnalité ne peut être implémentée que dans Scala et Java</b> . <br><br>  Je suppose que le lecteur sait déjà comment le streaming structuré fonctionne en termes généraux, sait comment mettre en œuvre les transformations nécessaires et est maintenant prêt à télécharger les résultats dans la base de données.  Si certaines des étapes ci-dessus ne sont pas claires, la documentation officielle peut servir de bon point de départ pour l'apprentissage du streaming structuré.  Dans cet article, je voudrais me concentrer sur la dernière étape lorsque vous devez enregistrer les résultats dans une base de données. <br><br>  Ci-dessous, je vais décrire un exemple d'implémentation du récepteur Cassandra pour le streaming structuré et expliquer comment l'exécuter dans un cluster.  Le code complet est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Lorsque j'ai rencontré le problème ci-dessus pour la première fois, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce projet</a> s'est avéré très utile.  Cependant, cela peut sembler un peu compliqué si le lecteur vient de commencer à travailler avec le streaming structuré et cherche un exemple simple de la façon de télécharger des données sur cassandra.  En outre, le projet est écrit pour fonctionner en mode local et nécessite certaines modifications pour s'exécuter dans le cluster. <br><br>  Je veux également donner des exemples de la façon de sauvegarder des données sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MongoDB</a> et toute autre base de données en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">JDBC</a> . <br><br><h3>  Solution simple </h3><br>  Pour télécharger des données vers un système externe, vous devez utiliser le récepteur <i>foreach</i> .  En savoir plus à ce sujet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  En bref, l'interface <i>ForeachWriter</i> doit être implémentée.  Autrement dit, il est nécessaire de déterminer comment ouvrir la connexion, comment traiter chaque élément de données et comment fermer la connexion à la fin du traitement.  Le code source est le suivant: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  La définition de <i>CassandraDriver</i> et la structure du tableau de sortie que je décrirai plus tard, mais pour l'instant, examinons de plus près comment fonctionne le code ci-dessus.  Pour me connecter à Kasandra depuis Spark, je crée un objet <i>CassandraDriver</i> qui donne accès à <i>CassandraConnector</i> , un connecteur développé par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DataStax</a> .  Le CassandraConnector est responsable de l'ouverture et de la fermeture de la connexion à la base de données, donc j'affiche simplement les messages de débogage dans les méthodes <i>open</i> et <i>close</i> de la classe <i>CassandraSinkForeach</i> . <br><br>  Le code ci-dessus est appelé depuis l'application principale comme suit: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> est créé pour chaque ligne de données, de sorte que chaque nœud de travail insère sa partie des lignes dans la base de données.  Autrement dit, chaque nœud de travail exécute <i>val cassandraDriver = new CassandraDriver ();</i>  Voici à quoi ressemble CassandraDriver: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Examinons de plus près l'objet <i>étincelle</i> .  Le code de <i>SparkSessionBuilder est</i> le suivant: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Sur chaque nœud de travail, <i>SparkSessionBuilder</i> fournit un accès à la <i>SparkSession</i> qui a été créée sur le pilote.  Pour rendre un tel accès possible, il est nécessaire de sérialiser <i>SparkSessionBuilder</i> et d'utiliser la valeur <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">transitoire</a> paresseuse</i> , ce qui permet au système de sérialisation d'ignorer <i>les</i> objets <i>conf</i> et <i>spark</i> lorsque le programme est initialisé et jusqu'à ce que les objets soient accessibles.  Ainsi, lorsque le programme <i>buildSparkSession démarre, il est</i> sérialisé et envoyé à chaque nœud de travail, mais <i>les</i> objets <i>conf</i> et <i>spark</i> ne sont autorisés que lorsque le nœud de travail y accède. <br><br>  Examinons maintenant le code d'application principal: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Lorsque l'application est envoyée pour exécution, <i>buildSparkSession est</i> sérialisé et envoyé aux nœuds de travail, cependant, <i>les</i> objets <i>conf</i> et <i>spark</i> restent non résolus.  Ensuite, le pilote crée un objet étincelle à l'intérieur de <i>KafkaToCassandra</i> et répartit le travail entre les nœuds de travail.  Chaque nœud de travail lit les données de Kafka, effectue des transformations simples sur la partie reçue des enregistrements et lorsque le nœud de travail est prêt à écrire les résultats dans la base de données, il autorise <i>les</i> objets <i>conf</i> et <i>spark</i> , obtenant ainsi l'accès à la <i>SparkSession</i> créée sur le pilote. <br><br><h3>  Comment construire et exécuter l'application? </h3><br>  Lorsque j'ai déménagé de PySpark à Scala, il m'a fallu un certain temps pour comprendre comment créer l'application.  Par conséquent, j'ai inclus Maven <i>pom.xml</i> dans mon projet.  Le lecteur peut créer l'application à l'aide de Maven en exécutant la commande de <i>package mvn</i> .  Après que l'application peut être envoyée pour exécution en utilisant <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Pour créer et exécuter l'application, il est nécessaire de remplacer les noms de mes machines AWS par les vôtres (c'est-à-dire remplacer tout ce qui ressemble à ec2-xx-xxx-xx-xx.compute-1.amazonaws.com). <br><br>  Le Spark et le Streaming Structuré en particulier est un nouveau sujet pour moi, donc je serai très reconnaissant aux lecteurs pour leurs commentaires, discussions et corrections. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr425503/">https://habr.com/ru/post/fr425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr425489/index.html">Automatisation: menace de robot exagérée</a></li>
<li><a href="../fr425493/index.html">Configuration de MikroTik hAP mini pour IPTV Beeline</a></li>
<li><a href="../fr425497/index.html">Meetup Tutu PHP # 2: diffusion d'événements en direct</a></li>
<li><a href="../fr425499/index.html">HyperX Impact DDR4 - SO-DIMM qui pourrait! Ou pourquoi dans un ordinateur portable 64 Go de mémoire avec une fréquence de 3200 MHz?</a></li>
<li><a href="../fr425501/index.html">Tests A / B sur Android de A à Z</a></li>
<li><a href="../fr425505/index.html">Analyse du processus de démarrage du noyau Linux</a></li>
<li><a href="../fr425507/index.html">Parsim Wikipedia pour les tâches PNL en 4 équipes</a></li>
<li><a href="../fr425511/index.html">Fonctionnalités non évidentes de l'application Rotativa pour la génération de PDF dans l'application ASP.NET MVC</a></li>
<li><a href="../fr425515/index.html">Apple bloque la réparation indépendante des nouveaux modèles de MacBook</a></li>
<li><a href="../fr425517/index.html">Comment Yandex a créé une prévision des précipitations mondiales à l'aide de radars et de satellites</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>