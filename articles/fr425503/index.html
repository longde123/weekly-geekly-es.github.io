<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôáüèø üéöÔ∏è üöÑ √âvier Cassandra pour le streaming structur√© Spark ü§πüèª üé¨ üë©üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il y a quelques mois, j'ai commenc√© √† √©tudier Spark, et √† un moment donn√©, j'ai √©t√© confront√© au probl√®me de l'enregistrement des calculs de Structure...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>√âvier Cassandra pour le streaming structur√© Spark</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Il y a quelques mois, j'ai commenc√© √† √©tudier Spark, et √† un moment donn√©, j'ai √©t√© confront√© au probl√®me de l'enregistrement des calculs de Structured Streaming dans la base de donn√©es Cassandra. <br><br>  Dans cet article, je donne un exemple simple de cr√©ation et d'utilisation de Cassandra Sink pour Spark Structured Streaming.  J'esp√®re que le message sera utile √† ceux qui ont r√©cemment commenc√© √† travailler avec Spark Structured Streaming et se demandent comment t√©l√©charger les r√©sultats des calculs dans la base de donn√©es. <br><br>  L'id√©e de l'application est tr√®s simple - pour recevoir et analyser des messages de Kafka, effectuer des transformations simples dans une paire et enregistrer les r√©sultats dans cassandra. <br><a name="habracut"></a><br><h3>  Avantages du streaming structur√© </h3><br>  Vous pouvez en savoir plus sur Structured Streaming dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> .  En bref, Structured Streaming est un moteur de traitement d'informations de streaming bien √©volutif bas√© sur le moteur Spark SQL.  Il vous permet d'utiliser Dataset / DataFrame pour agr√©ger des donn√©es, calculer des fonctions de fen√™tre, des connexions, etc. C'est-√†-dire que Structured Streaming vous permet d'utiliser le bon vieux SQL pour travailler avec des flux de donn√©es. <br><br><h3>  Quel est le probl√®me? </h3><br>  La version stable de Spark Structured Streaming a √©t√© publi√©e en 2017.  Autrement dit, il s'agit d'une API assez nouvelle qui impl√©mente les fonctionnalit√©s de base, mais certaines choses devront √™tre faites par nous-m√™mes.  Par exemple, Structured Streaming a des fonctions standard pour √©crire la sortie dans un fichier, une tuile, une console ou une m√©moire, mais pour enregistrer les donn√©es dans la base de donn√©es, vous devez utiliser le r√©cepteur <i>foreach</i> disponible dans Structured Streaming et impl√©menter l'interface <i>ForeachWriter</i> .  <b>Depuis Spark 2.3.1, cette fonctionnalit√© ne peut √™tre impl√©ment√©e que dans Scala et Java</b> . <br><br>  Je suppose que le lecteur sait d√©j√† comment le streaming structur√© fonctionne en termes g√©n√©raux, sait comment mettre en ≈ìuvre les transformations n√©cessaires et est maintenant pr√™t √† t√©l√©charger les r√©sultats dans la base de donn√©es.  Si certaines des √©tapes ci-dessus ne sont pas claires, la documentation officielle peut servir de bon point de d√©part pour l'apprentissage du streaming structur√©.  Dans cet article, je voudrais me concentrer sur la derni√®re √©tape lorsque vous devez enregistrer les r√©sultats dans une base de donn√©es. <br><br>  Ci-dessous, je vais d√©crire un exemple d'impl√©mentation du r√©cepteur Cassandra pour le streaming structur√© et expliquer comment l'ex√©cuter dans un cluster.  Le code complet est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  Lorsque j'ai rencontr√© le probl√®me ci-dessus pour la premi√®re fois, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce projet</a> s'est av√©r√© tr√®s utile.  Cependant, cela peut sembler un peu compliqu√© si le lecteur vient de commencer √† travailler avec le streaming structur√© et cherche un exemple simple de la fa√ßon de t√©l√©charger des donn√©es sur cassandra.  En outre, le projet est √©crit pour fonctionner en mode local et n√©cessite certaines modifications pour s'ex√©cuter dans le cluster. <br><br>  Je veux √©galement donner des exemples de la fa√ßon de sauvegarder des donn√©es sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MongoDB</a> et toute autre base de donn√©es en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">JDBC</a> . <br><br><h3>  Solution simple </h3><br>  Pour t√©l√©charger des donn√©es vers un syst√®me externe, vous devez utiliser le r√©cepteur <i>foreach</i> .  En savoir plus √† ce sujet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  En bref, l'interface <i>ForeachWriter</i> doit √™tre impl√©ment√©e.  Autrement dit, il est n√©cessaire de d√©terminer comment ouvrir la connexion, comment traiter chaque √©l√©ment de donn√©es et comment fermer la connexion √† la fin du traitement.  Le code source est le suivant: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  La d√©finition de <i>CassandraDriver</i> et la structure du tableau de sortie que je d√©crirai plus tard, mais pour l'instant, examinons de plus pr√®s comment fonctionne le code ci-dessus.  Pour me connecter √† Kasandra depuis Spark, je cr√©e un objet <i>CassandraDriver</i> qui donne acc√®s √† <i>CassandraConnector</i> , un connecteur d√©velopp√© par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DataStax</a> .  Le CassandraConnector est responsable de l'ouverture et de la fermeture de la connexion √† la base de donn√©es, donc j'affiche simplement les messages de d√©bogage dans les m√©thodes <i>open</i> et <i>close</i> de la classe <i>CassandraSinkForeach</i> . <br><br>  Le code ci-dessus est appel√© depuis l'application principale comme suit: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> est cr√©√© pour chaque ligne de donn√©es, de sorte que chaque n≈ìud de travail ins√®re sa partie des lignes dans la base de donn√©es.  Autrement dit, chaque n≈ìud de travail ex√©cute <i>val cassandraDriver = new CassandraDriver ();</i>  Voici √† quoi ressemble CassandraDriver: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Examinons de plus pr√®s l'objet <i>√©tincelle</i> .  Le code de <i>SparkSessionBuilder est</i> le suivant: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Sur chaque n≈ìud de travail, <i>SparkSessionBuilder</i> fournit un acc√®s √† la <i>SparkSession</i> qui a √©t√© cr√©√©e sur le pilote.  Pour rendre un tel acc√®s possible, il est n√©cessaire de s√©rialiser <i>SparkSessionBuilder</i> et d'utiliser la valeur <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">transitoire</a> paresseuse</i> , ce qui permet au syst√®me de s√©rialisation d'ignorer <i>les</i> objets <i>conf</i> et <i>spark</i> lorsque le programme est initialis√© et jusqu'√† ce que les objets soient accessibles.  Ainsi, lorsque le programme <i>buildSparkSession d√©marre, il est</i> s√©rialis√© et envoy√© √† chaque n≈ìud de travail, mais <i>les</i> objets <i>conf</i> et <i>spark</i> ne sont autoris√©s que lorsque le n≈ìud de travail y acc√®de. <br><br>  Examinons maintenant le code d'application principal: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Lorsque l'application est envoy√©e pour ex√©cution, <i>buildSparkSession est</i> s√©rialis√© et envoy√© aux n≈ìuds de travail, cependant, <i>les</i> objets <i>conf</i> et <i>spark</i> restent non r√©solus.  Ensuite, le pilote cr√©e un objet √©tincelle √† l'int√©rieur de <i>KafkaToCassandra</i> et r√©partit le travail entre les n≈ìuds de travail.  Chaque n≈ìud de travail lit les donn√©es de Kafka, effectue des transformations simples sur la partie re√ßue des enregistrements et lorsque le n≈ìud de travail est pr√™t √† √©crire les r√©sultats dans la base de donn√©es, il autorise <i>les</i> objets <i>conf</i> et <i>spark</i> , obtenant ainsi l'acc√®s √† la <i>SparkSession</i> cr√©√©e sur le pilote. <br><br><h3>  Comment construire et ex√©cuter l'application? </h3><br>  Lorsque j'ai d√©m√©nag√© de PySpark √† Scala, il m'a fallu un certain temps pour comprendre comment cr√©er l'application.  Par cons√©quent, j'ai inclus Maven <i>pom.xml</i> dans mon projet.  Le lecteur peut cr√©er l'application √† l'aide de Maven en ex√©cutant la commande de <i>package mvn</i> .  Apr√®s que l'application peut √™tre envoy√©e pour ex√©cution en utilisant <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Pour cr√©er et ex√©cuter l'application, il est n√©cessaire de remplacer les noms de mes machines AWS par les v√¥tres (c'est-√†-dire remplacer tout ce qui ressemble √† ec2-xx-xxx-xx-xx.compute-1.amazonaws.com). <br><br>  Le Spark et le Streaming Structur√© en particulier est un nouveau sujet pour moi, donc je serai tr√®s reconnaissant aux lecteurs pour leurs commentaires, discussions et corrections. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr425503/">https://habr.com/ru/post/fr425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr425489/index.html">Automatisation: menace de robot exag√©r√©e</a></li>
<li><a href="../fr425493/index.html">Configuration de MikroTik hAP mini pour IPTV Beeline</a></li>
<li><a href="../fr425497/index.html">Meetup Tutu PHP # 2: diffusion d'√©v√©nements en direct</a></li>
<li><a href="../fr425499/index.html">HyperX Impact DDR4 - SO-DIMM qui pourrait! Ou pourquoi dans un ordinateur portable 64 Go de m√©moire avec une fr√©quence de 3200 MHz?</a></li>
<li><a href="../fr425501/index.html">Tests A / B sur Android de A √† Z</a></li>
<li><a href="../fr425505/index.html">Analyse du processus de d√©marrage du noyau Linux</a></li>
<li><a href="../fr425507/index.html">Parsim Wikipedia pour les t√¢ches PNL en 4 √©quipes</a></li>
<li><a href="../fr425511/index.html">Fonctionnalit√©s non √©videntes de l'application Rotativa pour la g√©n√©ration de PDF dans l'application ASP.NET MVC</a></li>
<li><a href="../fr425515/index.html">Apple bloque la r√©paration ind√©pendante des nouveaux mod√®les de MacBook</a></li>
<li><a href="../fr425517/index.html">Comment Yandex a cr√©√© une pr√©vision des pr√©cipitations mondiales √† l'aide de radars et de satellites</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>