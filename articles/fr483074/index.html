<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßôüèª üéè üë¥üèª TensorRT 6.xxx - inf√©rence hautes performances pour les mod√®les d'apprentissage en profondeur (d√©tection et segmentation d'objets) üçè üçå üë®üèæ‚Äçüî¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√áa fait mal seulement pour la premi√®re fois! 

 Bonjour √† tous! Chers amis, dans cet article, je veux partager mon exp√©rience avec TensorRT, RetinaNet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>TensorRT 6.xxx - inf√©rence hautes performances pour les mod√®les d'apprentissage en profondeur (d√©tection et segmentation d'objets)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/483074/"><img src="https://habrastorage.org/webt/qr/je/yu/qrjeyup390z-iv5uyduultmmxcs.png" alt="image"><br>  <i>√áa fait mal seulement pour la premi√®re fois!</i> <br><br>  Bonjour √† tous!  Chers amis, dans cet article, je veux partager mon exp√©rience avec TensorRT, RetinaNet bas√© sur le r√©f√©rentiel <a href="https://github.com/aidonchuk/retinanet-examples">github.com/aidonchuk/retinanet-examples</a> (il s'agit d'une fourchette du navet officiel <a href="https://github.com/NVIDIA/retinanet-examples">nvidia</a> , qui nous permettra de commencer √† utiliser des mod√®les optimis√©s en production d√®s que possible).  <a href="https://ods.ai/">En parcourant les canaux de la</a> communaut√© <a href="https://ods.ai/">ods.ai</a> , je rencontre des questions sur l'utilisation de TensorRT, et la plupart du temps, les questions sont r√©p√©t√©es, j'ai donc d√©cid√© d'√©crire <i>un</i> guide <i>aussi complet que possible</i> pour l'utilisation de l'inf√©rence rapide bas√©e sur TensorRT, RetinaNet, Unet et Docker. <br><a name="habracut"></a><br>  <b>Description de la t√¢che</b> <br><br>  Je propose de d√©finir la t√¢che de cette fa√ßon: nous devons marquer le jeu de donn√©es, former le r√©seau RetinaNet / Unet sur Pytorch1.3 +, convertir les poids re√ßus en ONNX, puis les convertir en moteur TensorRT et ex√©cuter tout cela dans Docker, de pr√©f√©rence sur Ubuntu 18 et extr√™mement de pr√©f√©rence sur l'architecture ARM (Jetson) *, minimisant ainsi le d√©ploiement manuel de l'environnement.  En cons√©quence, nous pr√©parerons un conteneur non seulement pour l'exportation et la formation de RetinaNet / Unet, mais aussi pour le d√©veloppement complet et la formation de la classification, de la segmentation avec toutes les liaisons n√©cessaires. <br><cut></cut><br>  <b>√âtape 1. Pr√©paration de l'environnement</b> <br><br>  Il est important de noter ici que r√©cemment j'ai compl√®tement abandonn√© l'utilisation et le d√©ploiement d'au moins certaines biblioth√®ques sur la machine de bureau, ainsi que sur devbox.  La seule chose que vous devez cr√©er et installer est l'environnement virtuel python et cuda 10.2 (vous pouvez vous limiter √† un seul pilote nvidia) de deb. <br><br>  Supposons que vous ayez un Ubuntu 18. fra√Æchement install√©. Installez cuda 10.2 (deb), je ne m'attarderai pas sur le processus d'installation en d√©tail, la documentation officielle est assez suffisante. <br><br>  Maintenant, installez docker, le guide d'installation de docker est facile √† trouver, voici un exemple <a href="https://www.digitalocean.com/community/tutorials/docker-ubuntu-18-04-1-ru">www.digitalocean.com/community/tutorials/docker-ubuntu-18-04-1-en</a> , la version 19+ est d√©j√† disponible - mettez-la.  Eh bien, n'oubliez pas de permettre d'utiliser docker sans sudo, ce sera plus pratique.  Apr√®s tout, nous aimons ceci: <br><br><pre><code class="bash hljs">distribution=$(. /etc/os-release;<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$ID</span></span><span class="hljs-variable"><span class="hljs-variable">$VERSION_ID</span></span>) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/<span class="hljs-variable"><span class="hljs-variable">$distribution</span></span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker</code> </pre> <br>  Et vous n'avez m√™me pas besoin de consulter le r√©f√©rentiel officiel <a href="https://github.com/NVIDIA/nvidia-docker">github.com/NVIDIA/nvidia-docker</a> . <br><br>  Maintenant, faites git clone <a href="https://github.com/aidonchuk/retinanet-examples">github.com/aidonchuk/retinanet-examples</a> . <br><br>  Cela reste juste un peu, afin de commencer √† utiliser Docker avec nvidia-image, nous devons nous inscrire dans NGC Cloud et vous connecter.  Nous allons ici <a href="https://ngc.nvidia.com/">ngc.nvidia.com</a> , enregistrez-vous et apr√®s √™tre <a href="https://ngc.nvidia.com/">entr√©</a> dans le NGC Cloud, appuyez sur SETUP dans le coin sup√©rieur gauche de l'√©cran ou suivez ce lien <a href="https://ngc.nvidia.com/setup/api-key">ngc.nvidia.com/setup/api-key</a> .  Cliquez sur "g√©n√©rer une cl√©".  Je recommande de l'enregistrer, sinon la prochaine fois que vous le visiterez, vous devrez le r√©g√©n√©rer et, en cons√©quence, le d√©ployer sur une nouvelle brouette, r√©p√©ter cette op√©ration. <br><br>  Ex√©cuter: <br><br><pre> <code class="bash hljs">docker login nvcr.io Username: <span class="hljs-variable"><span class="hljs-variable">$oauthtoken</span></span> Password: &lt;Your Key&gt; -  </code> </pre><br>  Le nom d'utilisateur suffit de copier.  Eh bien, consid√©rez, l'environnement est d√©ploy√©! <br><cut></cut><br>  <b>√âtape 2. Assemblage du conteneur Docker</b> <br><br>  √Ä la deuxi√®me √©tape de notre travail, nous assemblerons le docker et nous nous familiariserons avec son int√©rieur. <br>  Allons dans le dossier racine relatif au projet retina-examples et ex√©cutons <br><br><pre> <code class="bash hljs">docker build --build-arg USER=<span class="hljs-variable"><span class="hljs-variable">$USER</span></span> --build-arg UID=<span class="hljs-variable"><span class="hljs-variable">$UID</span></span> --build-arg GID=<span class="hljs-variable"><span class="hljs-variable">$GID</span></span> --build-arg PW=alex -t retinanet:latest retinanet/</code> </pre><br>  Nous collectons le docker en y jetant l'utilisateur actuel - cela est tr√®s utile si vous √©crivez quelque chose sur un VOLUME mont√© avec les droits de l'utilisateur actuel, sinon il y aura racine et douleur. <br><br>  Pendant que docker s'en va, explorons le Dockerfile: <br><br><pre> <code class="powershell hljs">FROM nvcr.io/nvidia/pytorch:<span class="hljs-number"><span class="hljs-number">19.10</span></span><span class="hljs-literal"><span class="hljs-literal">-py3</span></span> ARG USER=alex ARG UID=<span class="hljs-number"><span class="hljs-number">1000</span></span> ARG GID=<span class="hljs-number"><span class="hljs-number">1000</span></span> ARG PW=alex RUN useradd <span class="hljs-literal"><span class="hljs-literal">-m</span></span> <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} -<span class="hljs-literal"><span class="hljs-literal">-uid</span></span>=<span class="hljs-variable"><span class="hljs-variable">$</span></span>{UID} &amp;&amp; echo <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$</span></span></span><span class="hljs-string">{USER}:</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$</span></span></span><span class="hljs-string">{PW}"</span></span> | chpasswd RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> update &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install curl &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install wget &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install git &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install automake &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> sudo &amp;&amp; adduser <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} sudo RUN pip install git+https://github.com/bonlime/pytorch<span class="hljs-literal"><span class="hljs-literal">-tools</span></span>.git@master COPY . retinanet/ RUN pip install -<span class="hljs-literal"><span class="hljs-literal">-no</span></span><span class="hljs-literal"><span class="hljs-literal">-cache</span></span><span class="hljs-literal"><span class="hljs-literal">-dir</span></span> <span class="hljs-literal"><span class="hljs-literal">-e</span></span> retinanet/ RUN pip install /workspace/retinanet/extras/tensorrt<span class="hljs-literal"><span class="hljs-literal">-6</span></span>.<span class="hljs-number"><span class="hljs-number">0.1</span></span>.<span class="hljs-number"><span class="hljs-number">5</span></span><span class="hljs-literal"><span class="hljs-literal">-cp36</span></span><span class="hljs-literal"><span class="hljs-literal">-none</span></span><span class="hljs-literal"><span class="hljs-literal">-linux_x86_64</span></span>.whl RUN pip install tensorboardx RUN pip install albumentations RUN pip install setproctitle RUN pip install paramiko RUN pip install flask RUN pip install mem_top RUN pip install arrow RUN pip install pycuda RUN pip install torchvision RUN pip install pretrainedmodels RUN pip install efficientnet<span class="hljs-literal"><span class="hljs-literal">-pytorch</span></span> RUN pip install git+https://github.com/qubvel/segmentation_models.pytorch RUN pip install pytorch_toolbelt RUN chown <span class="hljs-literal"><span class="hljs-literal">-R</span></span> <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER}:<span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} retinanet/ RUN cd /workspace/retinanet/extras/cppapi &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake <span class="hljs-literal"><span class="hljs-literal">-DCMAKE_CUDA_FLAGS</span></span>=<span class="hljs-string"><span class="hljs-string">"--expt-extended-lambda -std=c++14"</span></span> .. &amp;&amp; make &amp;&amp; cd /workspace RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> openssh<span class="hljs-literal"><span class="hljs-literal">-server</span></span> &amp;&amp; apt install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> tmux &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install bison flex &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-cache</span></span> search pcre &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install net<span class="hljs-literal"><span class="hljs-literal">-tools</span></span> &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nmap RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install libpcre3 libpcre3<span class="hljs-literal"><span class="hljs-literal">-dev</span></span> &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install iputils<span class="hljs-literal"><span class="hljs-literal">-ping</span></span> RUN mkdir /var/run/sshd RUN echo <span class="hljs-string"><span class="hljs-string">'root:pass'</span></span> | chpasswd RUN sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/'</span></span> /etc/ssh/sshd_config RUN sed <span class="hljs-string"><span class="hljs-string">'s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g'</span></span> <span class="hljs-literal"><span class="hljs-literal">-i</span></span> /etc/pam.d/sshd ENV NOTVISIBLE <span class="hljs-string"><span class="hljs-string">"in users profile"</span></span> RUN echo <span class="hljs-string"><span class="hljs-string">"export VISIBLE=now"</span></span> &gt;&gt; /etc/profile CMD [<span class="hljs-string"><span class="hljs-string">"/usr/sbin/sshd"</span></span>, <span class="hljs-string"><span class="hljs-string">"-D"</span></span>]</code> </pre><br>  Comme vous pouvez le voir dans le texte, nous prenons tous nos favoris, compilons le r√©tinanet, distribuons les outils de base pour la commodit√© de travailler avec Ubuntu et configurons le serveur openssh.  La premi√®re ligne est juste l'h√©ritage de l'image nvidia, pour laquelle nous avons fait une connexion dans NGC Cloud et qui contient Pytorch1.3, TensorRT6.xxx et un tas de biblioth√®ques qui nous permettent de compiler le code source cpp pour notre d√©tecteur. <br><cut></cut><br>  <b>√âtape 3. D√©marrage et d√©bogage du conteneur Docker</b> <br><br>  Passons au cas principal de l'utilisation du conteneur et de l'environnement de d√©veloppement, pour commencer, ex√©cutez nvidia docker.  Ex√©cuter: <br><br><pre> <code class="bash hljs">docker run --gpus all --net=host -v /home/&lt;your_user_name&gt;:/workspace/mounted_vol -d -P --rm --ipc=host -it retinanet:latest</code> </pre> <br>  Maintenant, le conteneur est disponible sur ssh &lt;curr_user_name&gt; @localhost.  Apr√®s un lancement r√©ussi, ouvrez le projet dans PyCharm.  Ensuite, ouvrez <br><br><pre> <code class="bash hljs">Settings-&gt;Project Interpreter-&gt;Add-&gt;Ssh Interpreter</code> </pre> <br>  <i>√âtape 1</i> <br><img src="https://habrastorage.org/webt/g0/qc/e4/g0qce4xw2pe0arglt4b4iu4jfle.png" alt="image"><br><br>  <i>√âtape 2</i> <br><img src="https://habrastorage.org/webt/nf/m_/cu/nfm_cuj84kymlgofajbo-go8dwy.png" alt="image"><br><br>  <i>√âtape 3</i> <br><img src="https://habrastorage.org/webt/w6/nn/eg/w6nnegihsdou1fhoy75p7g9evhi.png" alt="image"><br><br>  Nous s√©lectionnons tout comme dans les captures d'√©cran, <br><br><pre> <code class="bash hljs">Interpreter -&gt; /opt/conda/bin/python</code> </pre> <br>  - ce sera ln en Python3.6 et <br><br><pre> <code class="bash hljs">Sync folder -&gt; /workspace/retinanet</code> </pre> <br>  Nous pressons la ligne d'arriv√©e, nous attendons l'indexation, et c'est tout, l'environnement est pr√™t √† l'emploi! <br><br>  <i><b>IMPORTANT !!!</b></i>  Imm√©diatement apr√®s l'indexation, extrayez les fichiers compil√©s pour Retinanet √† partir de Docker.  Dans le menu contextuel √† la racine du projet, s√©lectionnez <br><br><pre> <code class="bash hljs">Deployment-&gt;Download</code> </pre> <br>  Un fichier et deux dossiers de construction, retinanet.egg-info et _so appara√Ætront <br><br><img src="https://habrastorage.org/webt/ti/uo/pp/tiuoppum1j_wx2yfxlkl8i-gdle.png" alt="image"><br><br>  Si votre projet ressemble √† ceci, alors l'environnement voit tous les fichiers n√©cessaires et nous sommes pr√™ts √† apprendre RetinaNet. <br><br>  <b>√âtape 4. Marquage des donn√©es et apprentissage du d√©tecteur</b> <br><br>  Pour le balisage, j'utilise principalement <a href="https://supervise.ly/">supervise.ly</a> - un outil agr√©able et pratique, la derni√®re fois, un tas de jambages ont √©t√© fix√©s et il est devenu beaucoup mieux se comporter. <br><br>  Supposons que vous ayez marqu√© un ensemble de donn√©es et que vous l'ayez t√©l√©charg√©, mais cela ne fonctionnera pas imm√©diatement pour le mettre dans notre RetinaNet, car il est dans son propre format et pour cela, nous devons le convertir en COCO.  L'outil de conversion est dans: <br><br><pre> <code class="bash hljs">markup_utils/supervisly_to_coco.py</code> </pre> <br>  Veuillez noter que la cat√©gorie dans le script est un exemple et que vous devez ins√©rer la v√¥tre (vous n'avez pas besoin d'ajouter la cat√©gorie d'arri√®re-plan) <br><br><pre> <code class="json hljs">categories = [{'id': <span class="hljs-number"><span class="hljs-number">1</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">1</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">2</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">2</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">3</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">3</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">4</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">4</span></span>'}]</code> </pre> <br>  Pour une raison quelconque, les auteurs du r√©f√©rentiel d'origine ont d√©cid√© que vous ne formeriez rien sauf COCO / VOC pour la d√©tection, j'ai donc d√ª modifier l√©g√®rement le fichier source <br><br><pre> <code class="bash hljs">retinanet/dataset.py</code> </pre> <br>  En ajoutant tutda vos augmentations pr√©f√©r√©es <a href="https://albumentations.readthedocs.io/en/latest/">albumentations.readthedocs.io/en/latest</a> et d√©coupez les cat√©gories c√¢bl√©es de COCO.  Il est √©galement possible de saupoudrer de grandes zones de d√©tection si vous recherchez de petits objets dans de grandes images, vous avez un petit ensemble de donn√©es =), et rien ne fonctionne, mais plus encore une autre fois. <br><br>  En g√©n√©ral, la boucle du train est √©galement faible, au d√©part, elle n'a pas enregistr√© les points de contr√¥le, elle a utilis√© un horrible planificateur, etc.  Mais maintenant, tout ce que vous avez √† faire est de s√©lectionner la colonne vert√©brale et d'ex√©cuter <br><br><pre> <code class="bash hljs">/opt/conda/bin/python retinanet/main.py</code> </pre> <br>  avec param√®tres: <br><br><pre> <code class="bash hljs">train retinanet_rn34fpn.pth --backbone ResNet34FPN --classes 12 --val-iters 10 --images /workspace/mounted_vol/dataset/train/images --annotations /workspace/mounted_vol/dataset/train_12_class.json --val-images /workspace/mounted_vol/dataset/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>/images_small --val-annotations /workspace/mounted_vol/dataset/val_10_class_cropped.json --jitter 256 512 --max-size 512 --batch 32</code> </pre><br>  Dans la console, vous verrez: <br><br><pre> <code class="plaintext hljs">Initializing model... model: RetinaNet backbone: ResNet18FPN classes: 2, anchors: 9 Selected optimization level O0: Pure FP32 training. Defaults for this optimization level are: enabled : True opt_level : O0 cast_model_type : torch.float32 patch_torch_functions : False keep_batchnorm_fp32 : None master_weights : False loss_scale : 1.0 Processing user overrides (additional kwargs that are not None)... After processing overrides, optimization options are: enabled : True opt_level : O0 cast_model_type : torch.float32 patch_torch_functions : False keep_batchnorm_fp32 : None master_weights : False loss_scale : 128.0 Preparing dataset... loader: pytorch resize: [1024, 1280], max: 1280 device: 4 gpus batch: 4, precision: mixed Training model for 20000 iterations... [ 1/20000] focal loss: 0.95619, box loss: 0.51584, 4.042s/4-batch (fw: 0.698s, bw: 0.459s), 1.0 im/s, lr: 0.0001 [ 12/20000] focal loss: 0.76191, box loss: 0.31794, 0.187s/4-batch (fw: 0.055s, bw: 0.133s), 21.4 im/s, lr: 0.0001 [ 24/20000] focal loss: 0.65036, box loss: 0.30269, 0.173s/4-batch (fw: 0.045s, bw: 0.128s), 23.1 im/s, lr: 0.0001 [ 36/20000] focal loss: 0.46425, box loss: 0.23141, 0.178s/4-batch (fw: 0.047s, bw: 0.131s), 22.4 im/s, lr: 0.0001 [ 48/20000] focal loss: 0.45115, box loss: 0.23505, 0.180s/4-batch (fw: 0.047s, bw: 0.133s), 22.2 im/s, lr: 0.0001 [ 59/20000] focal loss: 0.38958, box loss: 0.25373, 0.184s/4-batch (fw: 0.049s, bw: 0.134s), 21.8 im/s, lr: 0.0001 [ 71/20000] focal loss: 0.37733, box loss: 0.23988, 0.174s/4-batch (fw: 0.049s, bw: 0.125s), 22.9 im/s, lr: 0.0001 [ 83/20000] focal loss: 0.39514, box loss: 0.23878, 0.181s/4-batch (fw: 0.048s, bw: 0.133s), 22.1 im/s, lr: 0.0001 [ 94/20000] focal loss: 0.39947, box loss: 0.23817, 0.185s/4-batch (fw: 0.050s, bw: 0.134s), 21.6 im/s, lr: 0.0001 [ 105/20000] focal loss: 0.37343, box loss: 0.20238, 0.182s/4-batch (fw: 0.048s, bw: 0.134s), 22.0 im/s, lr: 0.0001 [ 116/20000] focal loss: 0.19689, box loss: 0.17371, 0.183s/4-batch (fw: 0.050s, bw: 0.132s), 21.8 im/s, lr: 0.0001 [ 128/20000] focal loss: 0.20368, box loss: 0.16538, 0.178s/4-batch (fw: 0.046s, bw: 0.131s), 22.5 im/s, lr: 0.0001 [ 140/20000] focal loss: 0.22763, box loss: 0.15772, 0.176s/4-batch (fw: 0.050s, bw: 0.126s), 22.7 im/s, lr: 0.0001 [ 148/20000] focal loss: 0.21997, box loss: 0.18400, 0.585s/4-batch (fw: 0.047s, bw: 0.144s), 6.8 im/s, lr: 0.0001 Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.52674 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.91450 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.35172 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61881 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.58824 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000 Saving model: 148</code> </pre> <br>  Pour √©tudier l'ensemble des param√®tres, regardez <br><br><pre> <code class="bash hljs">retinanet/main.py</code> </pre> <br>  En g√©n√©ral, ils sont standard pour la d√©tection et ils ont une description.  Ex√©cutez la formation et attendez les r√©sultats.  Un exemple d'inf√©rence peut √™tre trouv√© dans: <br><br><pre> <code class="bash hljs">retinanet/infer_example.py</code> </pre> <br>  ou ex√©cutez la commande: <br><br><pre> <code class="bash hljs">/opt/conda/bin/python retinanet/main.py infer retinanet_rn34fpn.pth --images /workspace/mounted_vol/dataset/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>/images --annotations /workspace/mounted_vol/dataset/val.json --output result.json --resize 256 --max-size 512 --batch 32</code> </pre><br>  Focal Loss et plusieurs dorsales sont d√©j√† int√©gr√©s dans le r√©f√©rentiel, et leur <br><br><pre> <code class="bash hljs">retinanet/backbones/*.py</code> </pre> <br>  Les auteurs donnent quelques caract√©ristiques dans la plaque signal√©tique: <br><br><img src="https://habrastorage.org/webt/z2/rj/3c/z2rj3cuo4rvxgnqfdx-rntclfm0.png" alt="image"><br><br>  Il existe √©galement un r√©seau principal ResNeXt50_32x4dFPN et ResNeXt101_32x8dFPN, provenant de torchvision. <br>  J'esp√®re que nous avons un peu compris la d√©tection, mais vous devez absolument lire la documentation officielle pour <i><b>comprendre les modes d'exportation et de journalisation</b></i> . <br><br>  <b>√âtape 5. Exportation et inf√©rence de mod√®les Unet avec l'encodeur Resnet</b> <br><br>  Comme vous l'avez probablement remarqu√©, les biblioth√®ques de segmentation ont √©t√© install√©es dans le Dockerfile, et en particulier la merveilleuse biblioth√®que <a href="https://github.com/qubvel/segmentation_models.pytorch">github.com/qubvel/segmentation_models.pytorch</a> .  Dans le package Yunet, vous pouvez trouver des exemples d'inf√©rence et d'exportation de points de contr√¥le Pytorch dans le moteur TensorRT. <br><br>  Le principal probl√®me lors de l'exportation de mod√®les de type Unet d'ONNX vers TensoRT est la n√©cessit√© de d√©finir une taille d'Upsample fixe ou d'utiliser ConvTranspose2D: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.onnx.symbolic_opset9 <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> onnx_symbolic <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">upsample_nearest2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(g, input, output_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Currently, TRT 5.1/6.0 ONNX Parser does not support all ONNX ops # needed to support dynamic upsampling ONNX forumlation # Here we hardcode scale=2 as a temporary workaround scales = g.op("Constant", value_t=torch.tensor([1., 1., 2., 2.])) return g.op("Upsample", input, scales, mode_s="nearest") onnx_symbolic.upsample_nearest2d = upsample_nearest2d</span></span></code> </pre><br>  En utilisant cette conversion, vous pouvez le faire automatiquement lors de l'exportation vers ONNX, mais d√©j√† dans la version 7 de TensorRT, ce probl√®me a √©t√© r√©solu, et nous avons d√ª attendre tr√®s peu. <br><br>  <b>Conclusion</b> <br><br>  Lorsque j'ai commenc√© √† utiliser Docker, j'avais des doutes quant √† ses performances pour mes t√¢ches.  Dans l'une de mes unit√©s, il y a maintenant beaucoup de trafic r√©seau cr√©√© par plusieurs cam√©ras. <br><br><img src="https://habrastorage.org/webt/3p/b9/1q/3pb91qmq_vcanxwqlskw3aervtu.png" alt="image"><br><br>  Divers tests sur Internet ont r√©v√©l√© un surco√ªt relativement important pour l'interaction r√©seau et l'enregistrement sur VOLUME, plus un GIL terrible et inconnu, et depuis la prise de vue d'une image, le travail d'un pilote et la transmission d'une image sur un r√©seau sont des op√©rations atomiques en mode <i>temps r√©el dur</i> , des retards en ligne sont tr√®s critiques pour moi. <br><br>  Mais rien ne s'est pass√© =) <br><br>  PS Il reste √† ajouter votre boucle de train pr√©f√©r√©e pour la segmentation et la production! <br><br>  <b>Merci</b> <br><br>  Gr√¢ce √† la communaut√© <a href="http://ods.ai/">ods.ai</a> , il est impossible de se d√©velopper sans elle!  Un grand merci √† <a href="https://habr.com/ru/users/n01z3/" class="user_link">n01z3</a> , DL, qui m'a souhait√© de rejoindre DL, pour ses pr√©cieux conseils et son extraordinaire professionnalisme! <br><br>  <i>Utilisez des mod√®les optimis√©s en production!</i> <br><br><img src="https://habrastorage.org/webt/xv/x3/yv/xvx3yvszd_twqjbrtk3rjaxrxpk.png">  Aurorai, llc </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr483074/">https://habr.com/ru/post/fr483074/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr483056/index.html">Champs logiques dans les bases de donn√©es, existe-t-il un antidote?</a></li>
<li><a href="../fr483058/index.html">R√©sultats du sondage sur les vacances</a></li>
<li><a href="../fr483064/index.html">Vue pour le plus petit aka petit blog sur tous les canons</a></li>
<li><a href="../fr483066/index.html">Les 5 livres traditionnels de Bill Gates</a></li>
<li><a href="../fr483068/index.html">R√©tro-ing√©nierie du populaire BattlEye anti-triche</a></li>
<li><a href="../fr483076/index.html">Meilleurs langages de programmation pour le d√©veloppement d'applications Android en 2020</a></li>
<li><a href="../fr483078/index.html">Apprentissage par renforcement profond: comment apprendre aux araign√©es √† marcher</a></li>
<li><a href="../fr483082/index.html">La chasse aux vuln√©rabilit√©s est 7% plus efficace</a></li>
<li><a href="../fr483084/index.html">Cam√©ra avec fonction de suivi</a></li>
<li><a href="../fr483086/index.html">R√©sultats de 2019: quels actifs se sont r√©v√©l√©s √™tre les plus rentables pour les investisseurs russes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>