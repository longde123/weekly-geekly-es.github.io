<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöÖ üí¨ üèÇüèΩ Python vs. Scala f√ºr Apache Spark - erwarteter Benchmark mit unerwartetem Ergebnis üíä üß† üôÖüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Apache Spark ist heute vielleicht die beliebteste Plattform f√ºr die Analyse gro√üer Datenmengen. Ein wesentlicher Beitrag zu seiner Popularit√§t wird du...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python vs. Scala f√ºr Apache Spark - erwarteter Benchmark mit unerwartetem Ergebnis</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/443324/"><p><img src="https://habrastorage.org/webt/uk/hc/yv/ukhcyvudckoey9ttm1libhqkyv8.jpeg"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Spark ist</a> heute vielleicht die beliebteste Plattform f√ºr die Analyse gro√üer Datenmengen.  Ein wesentlicher Beitrag zu seiner Popularit√§t wird durch die M√∂glichkeit geleistet, es unter Python zu verwenden.  Gleichzeitig sind sich alle einig, dass im Rahmen der Standard-API die Leistung von Python- und Scala / Java-Code vergleichbar ist, es jedoch keine einheitliche Sichtweise in Bezug auf benutzerdefinierte Funktionen (User Defined Function, UDF) gibt.  Versuchen wir am Beispiel der Aufgabe, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SNA Hackathon 2019-</a> L√∂sung zu √ºberpr√ºfen, herauszufinden, wie sich die Gemeinkosten in diesem Fall erh√∂hen. </p><a name="habracut"></a><br><p>  Im Rahmen des Wettbewerbs l√∂sen die Teilnehmer das Problem, den Newsfeed eines sozialen Netzwerks zu sortieren und L√∂sungen in Form einer Reihe sortierter Listen hochzuladen.  Um die Qualit√§t der erhaltenen L√∂sung zu √ºberpr√ºfen, wird zuerst f√ºr jede der geladenen Listen die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ROC-AUC</a> berechnet und dann der Durchschnittswert angezeigt.  Bitte beachten Sie, dass Sie nicht eine gemeinsame ROC-AUC berechnen m√ºssen, sondern eine pers√∂nliche f√ºr jeden Benutzer. Es gibt kein fertiges Design zur L√∂sung dieses Problems, daher m√ºssen Sie eine spezielle Funktion schreiben.  Ein guter Grund, die beiden Ans√§tze in der Praxis zu vergleichen. </p><br><p> Als Vergleichsplattform werden wir einen Cloud-Container mit vier Kernen und Spark verwenden, der im lokalen Modus gestartet wird, und wir werden √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Zeppelin damit arbeiten</a> .  Um die Funktionalit√§t zu vergleichen, spiegeln wir denselben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> in PySpark und Scala Spark.  [hier] Beginnen wir mit dem Laden der Daten. </p><br><pre><code class="python hljs">data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) \ .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) \ .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) \ .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><p>  Bei Verwendung der Standard-API ist die fast vollst√§ndige Identit√§t des Codes bis zum Schl√ºsselwort <code>val</code> bemerkenswert.  Die Betriebszeit unterscheidet sich nicht wesentlich.  Versuchen wir nun, die ben√∂tigte UDF zu ermitteln. </p><br><pre> <code class="python hljs">parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)], ArrayType(IntegerType())) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores)) auc_udf = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, auc, DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, (x : <span class="hljs-type"><span class="hljs-type">String</span></span>) =&gt; x.slice(<span class="hljs-number"><span class="hljs-number">1</span></span>,x.size - <span class="hljs-number"><span class="hljs-number">1</span></span>).split(<span class="hljs-string"><span class="hljs-string">","</span></span>).map(_.trim.toInt)) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AucAccumulator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">height: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, area: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, negatives: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">val</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">auc_udf</span></span></span><span class="hljs-class"> </span></span>= sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, (byScore: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>], gt: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> byLabel = gt.toSet <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> accumulator = byScore.foldLeft(<span class="hljs-type"><span class="hljs-type">AucAccumulator</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>))((accumulated, current) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (byLabel.contains(current)) { accumulated.copy(height = accumulated.height + <span class="hljs-number"><span class="hljs-number">1</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { accumulated.copy(area = accumulated.area + accumulated.height, negatives = accumulated.negatives + <span class="hljs-number"><span class="hljs-number">1</span></span>) } }) (accumulator.area).toDouble / (accumulator.negatives * accumulator.height) })</code> </pre> <br><p>  Bei der Implementierung einer bestimmten Funktion ist klar, dass Python <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">pr√§ziser</a> ist, vor allem aufgrund der M√∂glichkeit, die integrierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scikit-Lernfunktion zu</a> verwenden.  Es gibt jedoch unangenehme Momente - Sie m√ºssen den Typ des R√ºckgabewerts explizit angeben, w√§hrend er in Scala automatisch ermittelt wird.  Lassen Sie uns die Operation ausf√ºhren: </p><br><pre> <code class="python hljs">toValidate.select(auc_udf(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><pre> <code class="scala hljs">toValidate.select(auc_udf(parse($<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse($<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p>  Der Code sieht fast identisch aus, aber die Ergebnisse sind entmutigend. </p><br><p><img src="https://habrastorage.org/webt/vc/jw/y_/vcjwy_zwvfbkb_jlw6mimlogdwi.png"></p><br><p>  Die Implementierung in PySpark hat eineinhalb Minuten statt zwei Sekunden in Scala geklappt, dh <strong>Python war 45-mal langsamer</strong> .  W√§hrend der Ausf√ºhrung werden oben 4 aktive Python-Prozesse angezeigt, die mit voller Geschwindigkeit ausgef√ºhrt werden. Dies deutet darauf hin, dass die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">globale Interpreter-Sperre</a> hier keine Probleme verursacht.  Aber!  Vielleicht liegt das Problem in der internen Implementierung von scikit-learn - versuchen wir, den Python-Code buchst√§blich zu reproduzieren, ohne auf Standardbibliotheken zur√ºckzugreifen. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) auc_udf_modified = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc_modified"</span></span>, auc, DoubleType()) toValidate.select(auc_udf_modified(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/cx/o2/io/cxo2ioxdl18a6djwsmgr6dravhy.png"></p><br><p>  Das Experiment zeigt interessante Ergebnisse.  Einerseits wurde mit diesem Ansatz die Produktivit√§t gesteigert, andererseits verschwand der Lakonismus.  Die erhaltenen Ergebnisse k√∂nnen darauf hinweisen, dass bei der Arbeit in Python mit zus√§tzlichen C ++ - Modulen ein erheblicher Aufwand f√ºr den Wechsel zwischen Kontexten auftritt.  Nat√ºrlich gibt es einen √§hnlichen Overhead bei der Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JNI</a> in Java / Scala, aber ich musste mich bei der Verwendung nicht 45-mal mit Beispielen f√ºr eine Verschlechterung befassen. </p><br><p>  F√ºr eine detailliertere Analyse werden wir zwei zus√§tzliche Experimente durchf√ºhren: Verwenden von reinem Python ohne Spark, um den Beitrag des Paketaufrufs zu messen, und mit einer erh√∂hten Datengr√∂√üe in Spark, um den Overhead zu amortisieren und einen genaueren Vergleich zu erhalten. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sklearn_auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores))</code> </pre> <br><p><img src="https://habrastorage.org/webt/bk/ul/m1/bkulm1xarplv2-f4ilyg5dtxixw.png"></p><br><p>  Das Experiment mit lokalem Python und Pandas best√§tigte die Annahme eines erheblichen Overheads bei Verwendung zus√§tzlicher Pakete - bei Verwendung von Scikit-Learn verringert sich die Geschwindigkeit um mehr als das 20-fache.  20 ist jedoch nicht 45 - versuchen wir, die Daten aufzublasen und die Spark-Leistung erneut zu vergleichen. </p><br><pre> <code class="python hljs">k4 = toValidate.union(toValidate) k8 = k4.union(k4) m1 = k8.union(k8) m2 = m1.union(m1) m4 = m2.union(m2).repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() m4.count()</code> </pre> <br><p><img src="https://habrastorage.org/webt/zp/sf/po/zpsfpoabev7w3_xjcn0vqobwqf4.png"></p><br><p>  Der neue Vergleich zeigt den Geschwindigkeitsvorteil einer Scala-Implementierung gegen√ºber Python um das 7- bis 8-fache - 7 Sekunden gegen√ºber 55. Schlie√ülich versuchen wir "die schnellste in Python" - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">numpy</a> , um die Summe des Arrays zu berechnen: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy numpy_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"numpy_sum"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: float(numpy.sum(x)), DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> my_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"my_sum"</span></span>, (x: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; x.map(_.toDouble).sum)</code> </pre> <br><p><img src="https://habrastorage.org/webt/og/tr/gx/ogtrgx-kkuzx4kmkh4pd5cbyd-k.png"></p><br><p>  Wieder eine deutliche Verlangsamung - 5 Sekunden Scala gegen√ºber 80 Sekunden Python.  Zusammenfassend k√∂nnen wir folgende Schlussfolgerungen ziehen: </p><br><ul><li>  W√§hrend PySpark im Rahmen der Standard-API arbeitet, kann es in seiner Geschwindigkeit wirklich mit Scala vergleichbar sein. </li><li>  Wenn eine bestimmte Logik in Form von benutzerdefinierten Funktionen angezeigt wird, nimmt die PySpark-Leistung sp√ºrbar ab.  Wenn bei ausreichenden Informationen die Verarbeitungszeit eines Datenblocks mehrere Sekunden √ºberschreitet, ist die Python-Implementierung 5-10 langsamer, da bei der Interpretation von Python Daten zwischen Prozessen verschoben und Ressourcen verschwendet werden m√ºssen. </li><li>  Wenn die Verwendung zus√§tzlicher Funktionen in C ++ - Modulen implementiert wird, entstehen zus√§tzliche Aufrufkosten, und der Unterschied zwischen Python und Scala erh√∂ht sich um das 10- bis 50-fache. </li></ul><br><p>  Trotz des Charmes von Python erscheint die Verwendung in Verbindung mit Spark daher nicht immer gerechtfertigt.  Wenn es nicht so viele Daten gibt, die den Python-Overhead signifikant machen, sollten Sie sich √ºberlegen, ob hier Spark ben√∂tigt wird.  Wenn viele Daten vorhanden sind, die Verarbeitung jedoch im Rahmen der Standard-Spark-SQL-API erfolgt, wird Python hier ben√∂tigt? </p><br><p>  Wenn viele Daten vorhanden sind und h√§ufig Aufgaben ausgef√ºhrt werden m√ºssen, die √ºber die Grenzen der SQL-API hinausgehen, m√ºssen Sie den Cluster zeitweise vergr√∂√üern, um bei Verwendung von PySpark dieselbe Menge an Arbeit auszuf√ºhren.  F√ºr Odnoklassniki w√ºrden sich beispielsweise die Investitionskosten f√ºr den Spark-Cluster um viele hundert Millionen Rubel erh√∂hen.  Wenn Sie versuchen, die erweiterten Funktionen der Python-√ñkosystembibliotheken zu nutzen, besteht das Risiko einer Verlangsamung nicht nur zuweilen, sondern um eine Gr√∂√üenordnung. </p><br><p>  Eine gewisse Beschleunigung kann unter Verwendung der relativ neuen Funktionalit√§t von vektorisierten Funktionen erhalten werden.  In diesem Fall wird nicht eine einzelne Zeile dem UDF-Eingang zugef√ºhrt, sondern ein Paket aus mehreren Zeilen in Form eines Pandas-Datenrahmens.  Die Entwicklung dieser Funktionalit√§t ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">jedoch noch nicht abgeschlossen</a> , und selbst in diesem Fall wird der Unterschied <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erheblich sein</a> . </p><br><p>  Eine Alternative w√§re die Aufrechterhaltung eines umfangreichen Teams von Dateningenieuren, die in der Lage sind, die Anforderungen von Datenwissenschaftlern mit zus√§tzlichen Funktionen schnell zu erf√ºllen.  Oder um in die Scala-Welt einzutauchen, da es nicht so schwierig ist: Viele der erforderlichen Tools sind bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorhanden. Es</a> erscheinen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schulungsprogramme</a> , die √ºber PySpark hinausgehen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443324/">https://habr.com/ru/post/de443324/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443314/index.html">Was ist Herzlichkeit? [√úbersetzung des Artikels]</a></li>
<li><a href="../de443316/index.html">ReactJS-Test: Wie tief das Kaninchenloch ist</a></li>
<li><a href="../de443318/index.html">Schreiben eines Wasm Loader f√ºr Ghidra. Teil 1: Problemstellung und Einrichtung der Umgebung</a></li>
<li><a href="../de443320/index.html">Elektronisches Dokumentenmanagementsystem "Vizier"</a></li>
<li><a href="../de443322/index.html">GitLab 11.8 ver√∂ffentlicht mit SAST f√ºr JavaScript, GitLab Pages f√ºr Untergruppen und Bug Tracking</a></li>
<li><a href="../de443326/index.html">Python & Arduino. Einfach, schnell und sch√∂n</a></li>
<li><a href="../de443330/index.html">Sicherheitswoche 11: RSA 2019 und eine bessere Zukunft</a></li>
<li><a href="../de443332/index.html">K√ºchenroboter</a></li>
<li><a href="../de443334/index.html">Gelbes Gesicht</a></li>
<li><a href="../de443336/index.html">Arbeiten Sie mit der Kamera in Flutter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>