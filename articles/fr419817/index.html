<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüè≠ üö¨ üåå Lancement du cluster RabbitMQ dans Kubernetes üëö üö∏ üéª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans le cas de l'organisation de microservices de l'application, un travail important repose sur les m√©canismes de communication d'int√©gration des mic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lancement du cluster RabbitMQ dans Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  Dans le cas de l'organisation de microservices de l'application, un travail important repose sur les m√©canismes de communication d'int√©gration des microservices.  De plus, cette int√©gration doit √™tre tol√©rante aux pannes, avec un haut degr√© de disponibilit√©. <br><br>  Dans nos solutions, nous utilisons l'int√©gration avec Kafka, gRPC et RabbitMQ. <br><br>  Dans cet article, nous partagerons notre exp√©rience du clustering RabbitMQ, dont les n≈ìuds sont h√©berg√©s sur Kubernetes. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="image"><br><br>  Avant RabbitMQ version 3.7, le clustering dans K8S n'√©tait pas une t√¢che tr√®s triviale, avec de nombreux hacks et des solutions pas tr√®s belles.  Dans la version 3.6, un plugin d'autocluster de RabbitMQ Community a √©t√© utilis√©.  Et dans 3.7 Kubernetes Peer Discovery Backend est apparu.  Il est int√©gr√© par le plug-in dans la livraison de base de RabbitMQ et ne n√©cessite pas d'assemblage et d'installation s√©par√©s. <br><br>  Nous d√©crirons la configuration finale dans son ensemble, tout en commentant ce qui se passe. <br><a name="habracut"></a><br><h2>  En th√©orie </h2><br>  Le plugin a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©f√©rentiel sur le github</a> , dans lequel il y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un exemple d'utilisation de base</a> . <br>  Cet exemple n'est pas destin√© √† la production, ce qui est clairement indiqu√© dans sa description, et de plus, certains des param√®tres qu'il contient sont d√©finis contrairement √† la logique d'utilisation dans le prod.  De plus, dans l'exemple, la persistance du stockage n'est pas du tout mentionn√©e, donc dans toute situation d'urgence, notre cluster se transformera en zilch. <br><br><h2>  En pratique </h2><br>  Nous allons maintenant vous dire √† quoi vous avez fait face et comment installer et configurer RabbitMQ. <br><br>  D√©crivons les configurations de toutes les parties de RabbitMQ en tant que service dans les K8.  Nous pr√©ciserons imm√©diatement que nous avons install√© RabbitMQ dans les K8 en tant que StatefulSet.  Sur chaque n≈ìud du cluster K8s, une instance de RabbitMQ fonctionnera toujours (un n≈ìud dans la configuration de cluster classique).  Nous installerons √©galement le panneau de contr√¥le RabbitMQ dans les K8 et donnerons acc√®s √† ce panneau en dehors du cluster. <br><br><h3>  Droits et r√¥les: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Les droits d'acc√®s pour RabbitMQ sont enti√®rement tir√©s de l'exemple, aucune modification n'y est requise.  Nous cr√©ons un ServiceAccount pour notre cluster et lui accordons des autorisations de lecture aux points de terminaison K8. <br><br><h3>  Stockage persistant: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Ici, nous avons pris le cas le plus simple comme le stockage persistant - hostPath (un dossier normal sur chaque n≈ìud K8s), mais vous pouvez utiliser n'importe lequel des nombreux types de volumes persistants pris en charge par K8s. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Cr√©ez une revendication de volume sur le volume cr√©√© √† l'√©tape pr√©c√©dente.  Cette revendication sera ensuite utilis√©e dans StatefulSet en tant que magasin de donn√©es persistantes. <br><br><h3>  Services: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Nous cr√©ons un service interne sans t√™te √† travers lequel le plugin Peer Discovery fonctionnera. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Pour que les applications des K8 fonctionnent avec notre cluster, nous cr√©ons un service d'√©quilibrage. <br><br>  Puisque nous avons besoin d'acc√©der au cluster RabbitMQ en dehors des K8, nous parcourons NodePort.  RabbitMQ sera disponible lors de l'acc√®s √† n'importe quel n≈ìud du cluster K8 sur les ports 31673 et 30673. Dans le travail r√©el, cela n'est pas vraiment n√©cessaire.  La question de la commodit√© de l'utilisation du panneau d'administration RabbitMQ. <br><br>  Lors de la cr√©ation d'un service avec le type NodePort dans K8s, un service avec le type ClusterIP est √©galement implicitement cr√©√© pour le servir.  Par cons√©quent, les applications des K8 qui doivent fonctionner avec notre RabbitMQ pourront acc√©der au cluster sur <i>amqp: // rabbitmq: 5672</i> <br><br><h3>  Configuration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Nous cr√©ons des fichiers de configuration RabbitMQ.  La magie principale. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Ajoutez les plugins n√©cessaires √† ceux autoris√©s pour le t√©l√©chargement.  Nous pouvons maintenant utiliser la d√©tection automatique des pairs dans le K8S. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Nous exposons le plugin n√©cessaire en tant que backend pour la d√©couverte des pairs. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Sp√©cifiez l'adresse et le port par lesquels vous pouvez atteindre kubernetes apiserver.  Ici, vous pouvez sp√©cifier directement l'adresse IP, mais ce sera plus beau de le faire. <br><br>  Dans l'espace de noms par d√©faut, un service est g√©n√©ralement cr√©√© avec le nom kubernetes menant √† k8-apiserver.  Dans diff√©rentes options d'installation de K8S, l'espace de noms, le nom du service et le port peuvent √™tre diff√©rents.  Si quelque chose dans une installation particuli√®re est diff√©rent, vous devez le corriger en cons√©quence. <br><br>  Par exemple, nous sommes confront√©s au fait que dans certains clusters, le service est sur le port 443 et dans certains sur 6443. Il sera possible de comprendre que quelque chose ne va pas dans les journaux de d√©marrage de RabbitMQ, le temps de connexion √† l'adresse sp√©cifi√©e ici y est clairement mis en √©vidence. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  Par d√©faut, l'exemple a sp√©cifi√© le type d'adresse du n≈ìud de cluster RabbitMQ par adresse IP.  Mais lorsque vous red√©marrez le pod, il obtient √† chaque fois une nouvelle adresse IP.  Surprise!  La grappe se meurt dans l'agonie. <br><br>  Modifiez l'adressage en nom d'h√¥te.  StatefulSet nous garantit l'invariabilit√© du nom d'h√¥te dans le cycle de vie de l'ensemble StatefulSet, ce qui nous convient parfaitement. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Puisque lorsque nous perdons l'un des n≈ìuds, nous supposons qu'il se r√©tablira t√¥t ou tard, nous d√©sactivons l'auto-suppression par un cluster de n≈ìuds inaccessibles.  Dans ce cas, d√®s que le n≈ìud revient en ligne, il entre dans le cluster sans perdre son √©tat pr√©c√©dent. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Ce param√®tre d√©termine les actions du cluster en cas de perte de quorum.  Ici, il vous suffit de lire la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation sur ce sujet</a> et de comprendre par vous-m√™me ce qui se rapproche le plus d'un cas d'utilisation sp√©cifique. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  D√©terminez la s√©lection de l'assistant pour les nouvelles files d'attente.  Avec ce param√®tre, l'Assistant s√©lectionnera le n≈ìud avec le moins de files d'attente, afin que les files d'attente soient r√©parties uniform√©ment sur les n≈ìuds du cluster. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Nous nommons le service K8 sans t√™te (cr√©√© par nous plus t√¥t) √† travers lequel les n≈ìuds RabbitMQ communiqueront entre eux. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Un nom important pour l'adressage dans un cluster est le nom d'h√¥te.  Le FQDN du foyer K8s est form√© comme un nom court (rabbitmq-0, rabbitmq-1) + suffixe (partie de domaine).  Ici, nous indiquons ce suffixe.  Dans K8S, il ressemble √† <b>. &lt;Nom du service&gt;. &lt;Nom de l'espace de noms&gt; .svc.cluster.local</b> <br><br>  kube-dns r√©sout les noms de la forme rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local en l'adresse IP d'un pod sp√©cifique sans aucune configuration suppl√©mentaire, ce qui rend toute la magie du regroupement par nom d'h√¥te possible. <br><br><h3>  Configuration de StatefulSet RabbitMQ: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  En fait, StatefulSet lui-m√™me.  Nous notons des points int√©ressants. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Nous √©crivons le nom du service sans t√™te via lequel les pods communiquent dans StatefulSet. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  D√©finissez le nombre de r√©pliques dans le cluster.  Nous l'avons √©gal au nombre de n≈ìuds de travail K8. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Lorsque l'un des n≈ìuds K8 tombe, le jeu avec √©tat cherche √† conserver le nombre d'instances dans l'ensemble, par cons√©quent, il cr√©e plusieurs foyers sur le m√™me n≈ìud K8.  Ce comportement est totalement ind√©sirable et, en principe, inutile.  Par cons√©quent, nous prescrivons une r√®gle anti-affinit√© pour les ensembles de foyers √† partir d'un √©tat.  Nous rendons la r√®gle difficile (obligatoire) afin que le planificateur de kube ne puisse pas la casser lors de la planification des pods. <br><br>  L'essence est simple: il est interdit au planificateur de placer (dans l'espace de noms) plus d'un pod avec la <i>balise app: rabbitmq</i> sur chaque n≈ìud.  Nous distinguons les <i>n≈ìuds</i> par la valeur de l'√©tiquette <i>kubernetes.io/hostname</i> .  Maintenant, si pour une raison quelconque, le nombre de n≈ìuds K8S actifs est inf√©rieur au nombre requis de r√©pliques dans StatefulSet, de nouvelles r√©pliques ne seront pas cr√©√©es jusqu'√† ce qu'un n≈ìud libre r√©apparaisse. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Nous enregistrons ServiceAccount, sous lequel nos pods fonctionnent. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  L'image de RabbitMQ est compl√®tement standard et est prise √† partir du hub docker; elle ne n√©cessite aucune reconstruction ni r√©vision de fichier. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Les donn√©es persistantes de RabbitMQ sont stock√©es dans / var / lib / rabbitmq / mnesia.  Ici, nous montons notre revendication de volume persistant dans ce dossier afin que lors du red√©marrage des foyers / n≈ìuds ou m√™me de l'ensemble StatefulSet, les donn√©es (√† la fois le service, y compris sur le cluster assembl√© et les donn√©es utilisateur) soient saines et saines.  Il existe des exemples o√π le dossier / var / lib / rabbitmq / est rendu persistant.  Nous sommes arriv√©s √† la conclusion que ce n'√©tait pas la meilleure id√©e, car en m√™me temps toutes les informations d√©finies par les configurations Rabbit commencent √† √™tre m√©moris√©es.  Autrement dit, pour modifier quelque chose dans le fichier de configuration, vous devez nettoyer le stockage persistant, ce qui est tr√®s g√™nant en fonctionnement. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Avec cet ensemble de variables d'environnement, nous demandons d'abord √† RabbitMQ d'utiliser le nom FQDN comme identifiant pour les membres du cluster, et deuxi√®mement, nous d√©finissons le format de ce nom.  Le format a √©t√© d√©crit pr√©c√©demment lors de l'analyse de la configuration. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  Nom du service sans t√™te pour la communication entre les membres du cluster. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  Le contenu du cookie Erlang doit √™tre le m√™me sur tous les n≈ìuds du cluster, vous devez enregistrer votre propre valeur.  Un n≈ìud avec un cookie diff√©rent ne peut pas entrer dans le cluster. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  D√©finissez le volume mapp√© √† partir de la revendication de volume persistant pr√©c√©demment cr√©√©e. <br><br>  C'est l√† que nous en avons termin√© avec la configuration des K8.  Le r√©sultat est un cluster RabbitMQ, qui r√©partit uniform√©ment les files d'attente entre les n≈ìuds et r√©siste aux probl√®mes de l'environnement d'ex√©cution. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="image"><br><br>  Si l'un des n≈ìuds du cluster n'est pas disponible, les files d'attente qu'il contient cesseront d'√™tre accessibles, tout le reste continuera de fonctionner.  D√®s que le n≈ìud sera de nouveau op√©rationnel, il reviendra au cluster et les files d'attente pour lesquelles il √©tait ma√Ætre redeviendront op√©rationnelles, en conservant toutes les donn√©es qui y sont contenues (si le stockage persistant ne s'est pas cass√©, bien s√ªr).  Tous ces processus sont enti√®rement automatiques et ne n√©cessitent aucune intervention. <br><br><h2>  Bonus: personnaliser HA </h2><br>  L'un des projets √©tait une nuance.  Les exigences sonnaient une mise en miroir compl√®te de toutes les donn√©es contenues dans le cluster.  Cela est n√©cessaire pour que dans une situation o√π au moins un n≈ìud de cluster est op√©rationnel, tout continue √† fonctionner du point de vue de l'application.  Ce moment n'a rien √† voir avec les K8, nous le d√©crivons simplement comme un mini mode d'emploi. <br><br>  Pour activer la haute disponibilit√© compl√®te, vous devez cr√©er une strat√©gie dans le tableau de bord RabbitMQ sur l'onglet <i>Admin -&gt; Strat√©gies</i> .  Le nom est arbitraire, le mod√®le est vide (toutes les files d'attente), dans les d√©finitions, ajoutez deux param√®tres: <i>ha-mode: all</i> , <i>ha-sync-mode: automatic</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="image"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="image"><br><br>  Apr√®s cela, toutes les files d'attente cr√©√©es dans le cluster seront en mode haute disponibilit√©: si le n≈ìud ma√Ætre n'est pas disponible, l'un des esclaves sera automatiquement s√©lectionn√© par le nouvel assistant.  Et les donn√©es entrant dans la file d'attente seront refl√©t√©es sur tous les n≈ìuds du cluster.  Qui, en fait, devait recevoir. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="image"><br><br>  En savoir plus sur HA dans RabbitMQ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> <br><br><h2>  Litt√©rature utile: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©p√¥t RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Exemple de configuration pour le d√©ploiement de RabbitMQ dans K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Description des principes de la formation de clusters, du m√©canisme de d√©couverte par les pairs et des plugins correspondants</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Discussion √©pique sur la configuration appropri√©e de la d√©couverte bas√©e sur le nom d'h√¥te</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Guide de mise en cluster RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Description des probl√®mes et solutions de regroupement des cerveaux divis√©s</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Files d'attente √† haute disponibilit√© chez RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Configurer les politiques</a> </li></ul><br>  Bonne chance! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr419817/">https://habr.com/ru/post/fr419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr419805/index.html">The Super Tiny Compiler - maintenant en russe</a></li>
<li><a href="../fr419807/index.html">Glaucome - comment ne pas devenir aveugle: parlons de traitement ...</a></li>
<li><a href="../fr419811/index.html">L'√©volution des √©crans flexibles</a></li>
<li><a href="../fr419813/index.html">Webinaires Skillbox: s√©lection du vendredi</a></li>
<li><a href="../fr419815/index.html">Secrets de tol√©rance aux fautes de notre front office</a></li>
<li><a href="../fr419819/index.html">Biomarqueurs du vieillissement. Panel de fragilit√©. 2e partie</a></li>
<li><a href="../fr419823/index.html">Duo insolite - phrases de passe et images mn√©moniques</a></li>
<li><a href="../fr419825/index.html">Test des performances de plusieurs types de disques dans un environnement virtuel</a></li>
<li><a href="../fr419829/index.html">Le chiffrement de cl√© par d√©faut d'OpenSSH est pire que rien</a></li>
<li><a href="../fr419831/index.html">Fonctionnement de JS: √©l√©ments personnalis√©s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>