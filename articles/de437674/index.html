<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘‚ ğŸ§™ğŸ» ğŸ’– 3D-Objekterkennungsmethoden fÃ¼r unbemannte Fahrzeuge. Yandex-Bericht ğŸ“ ğŸ¤¸ğŸ» ğŸ‘©ğŸ¿â€âš•ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unbemannte Autos kÃ¶nnen nicht ohne VerstÃ¤ndnis auskommen, was sich in der NÃ¤he befindet und wo genau. Im Dezember letzten Jahres hielt der Entwickler ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>3D-Objekterkennungsmethoden fÃ¼r unbemannte Fahrzeuge. Yandex-Bericht</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437674/">  Unbemannte Autos kÃ¶nnen nicht ohne VerstÃ¤ndnis auskommen, was sich in der NÃ¤he befindet und wo genau.  Im Dezember letzten Jahres hielt der Entwickler Victor Otliga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">vitonka</a> einen Vortrag Ã¼ber die Erkennung von 3D-Objekten am <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data-Christmas-Baum</a> .  Victor arbeitet in Richtung unbemannter Fahrzeuge Yandex, in der Gruppe, die die Verkehrssituation behandelt (und unterrichtet auch am ShAD).  Er erklÃ¤rte, wie wir das Problem der Erkennung anderer Verkehrsteilnehmer in einer dreidimensionalen Punktwolke lÃ¶sen, wie sich dieses Problem von der Erkennung von Objekten in einem Bild unterscheidet und wie wir von der gemeinsamen Nutzung verschiedener Sensortypen profitieren kÃ¶nnen. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Hallo allerseits!  Mein Name ist Victor Otliga, ich arbeite im Yandex-BÃ¼ro in Minsk und entwickle unbemannte Fahrzeuge.  Heute werde ich Ã¼ber eine ziemlich wichtige Aufgabe fÃ¼r Drohnen sprechen - die Erkennung von 3D-Objekten um uns herum. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/ky/wn/hu/kywnhuprs-iq34tx4rgohnvwqm8.jpeg"><br><br>  Um zu fahren, muss man verstehen, was sich in der NÃ¤he befindet.  Ich werde Ihnen kurz sagen, welche Sensoren und Sensoren in unbemannten Fahrzeugen verwendet werden und welche wir verwenden.  Ich werde Ihnen sagen, was die Aufgabe der Erkennung von 3D-Objekten ist und wie Sie die QualitÃ¤t der Erkennung messen kÃ¶nnen.  Dann werde ich Ihnen sagen, woran diese QualitÃ¤t gemessen werden kann.  Und dann werde ich einen kurzen Ãœberblick Ã¼ber gute moderne Algorithmen geben, einschlieÃŸlich derer, auf denen unsere LÃ¶sungen basieren.  Und am Ende - kleine Ergebnisse, ein Vergleich dieser Algorithmen, einschlieÃŸlich unserer. <br><br><img src="https://habrastorage.org/webt/pk/sf/vo/pksfvoxtqwlyf6k-wido8ixektg.jpeg"><br><br>  So sieht unser funktionierender Prototyp eines unbemannten Autos jetzt aus.  Ein solches Taxi kann von jedem ohne Fahrer in der russischen Stadt Innopolis sowie in Skolkovo gemietet werden.  Und wenn Sie genau hinschauen, befindet sich oben ein groÃŸer WÃ¼rfel.  Was ist da drin? <br><br><img src="https://habrastorage.org/webt/ei/3c/mq/ei3cmqe3l4kjfzjju-s1ndavkwi.jpeg"><br><br>  In einem einfachen Satz von Sensoren.  Es gibt eine GNSS- und GSM-Antenne, um festzustellen, wo sich das Auto befindet, und um mit der AuÃŸenwelt zu kommunizieren.  Wo ohne so einen klassischen Sensor wie eine Kamera.  Aber heute werden wir uns fÃ¼r Lidars interessieren. <br><br><img src="https://habrastorage.org/webt/cm/s6/pd/cms6pdxey7d4ykgktlc8lvyd8fc.jpeg"><br><br><img src="https://habrastorage.org/webt/un/ia/bg/uniabgh6yomlr2rourl7ynj3coc.jpeg"><br><br>  Lidar erzeugt ungefÃ¤hr eine solche Punktwolke um sich herum, die drei Koordinaten hat.  Und du musst mit ihnen arbeiten.  Ich werde Ihnen sagen, wie Sie mithilfe eines Kamerabilds und einer Lidarwolke Objekte erkennen kÃ¶nnen. <br><br><img src="https://habrastorage.org/webt/yx/rt/si/yxrtsiwsnt_gcdgjbz42pp8lqvc.jpeg"><br><br>  Was ist die Herausforderung?  Das Bild von der Kamera wird in den Eingang eingegeben, die Kamera wird mit dem Lidar synchronisiert.  Es wÃ¤re seltsam, das Bild von der Kamera vor einer Sekunde zu verwenden, die Lidarwolke aus einem ganz anderen Moment zu nehmen und zu versuchen, Objekte darauf zu erkennen. <br><br><img src="https://habrastorage.org/webt/xa/g6/aj/xag6ajjiigpo5m737kklxiwkmb0.jpeg"><br><br>  Wir synchronisieren irgendwie Kameras und Lidars, dies ist eine separate schwierige Aufgabe, aber wir bewÃ¤ltigen sie erfolgreich.  Solche Daten werden in die Eingabe eingegeben, und am Ende mÃ¶chten wir KÃ¤stchen erhalten, BegrenzungskÃ¤stchen, die das Objekt einschrÃ¤nken: FuÃŸgÃ¤nger, Radfahrer, Autos und andere Verkehrsteilnehmer und nicht nur. <br><br>  Die Aufgabe wurde gestellt.  Wie werden wir es bewerten? <br><br><img src="https://habrastorage.org/webt/ew/hm/wd/ewhmwdgektceevdbmgu838n5wui.jpeg"><br><br>  Das Problem der 2D-Erkennung von Objekten in einem Bild wurde umfassend untersucht. <br><br><img src="https://habrastorage.org/webt/-c/ri/cu/-cricuguziob4idgyr_u-ihfq-e.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sie kÃ¶nnen Standardmetriken oder deren Analoga verwenden.  Es gibt einen Jacquard-Koeffizienten oder Schnittpunkt Ã¼ber der Vereinigung, einen wunderbaren Koeffizienten, der zeigt, wie gut wir ein Objekt erkannt haben.  Wir kÃ¶nnen eine Box nehmen, in der sich, wie wir annehmen, das Objekt befindet, und eine Box, in der es sich tatsÃ¤chlich befindet.  ZÃ¤hlen Sie diese Metrik.  Es gibt Standardschwellen - sagen wir, fÃ¼r Autos nehmen sie oft eine Schwelle von 0,7.  Wenn dieser Wert grÃ¶ÃŸer als 0,7 ist, glauben wir, dass wir das Objekt erfolgreich erkannt haben und dass das Objekt dort ist.  Wir sind groÃŸartig, wir kÃ¶nnen noch weiter gehen. <br><br>  Um ein Objekt zu erkennen und zu verstehen, dass es sich irgendwo befindet, mÃ¶chten wir auÃŸerdem das Vertrauen haben, dass wir das Objekt dort wirklich sehen, und es auch messen.  Sie kÃ¶nnen einfach messen und die durchschnittliche Genauigkeit berÃ¼cksichtigen.  Sie kÃ¶nnen die PrÃ¤zisionsrÃ¼ckrufkurve und den Bereich darunter nehmen und sagen: Je grÃ¶ÃŸer sie ist, desto besser. <br><br><img src="https://habrastorage.org/webt/p7/jk/z5/p7jkz5xnyyhehx99e73nijtb798.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Um die QualitÃ¤t der 3D-Erkennung zu messen, nehmen sie normalerweise einen Datensatz und teilen ihn in mehrere Teile auf, da Objekte nahe oder weiter entfernt sein kÃ¶nnen und teilweise durch etwas anderes verdeckt werden kÃ¶nnen.  Daher ist die Validierungsstichprobe hÃ¤ufig in drei Teile unterteilt.  Objekte, die leicht zu erkennen, von mittlerer KomplexitÃ¤t und komplex, entfernt oder stark verdeckt sind.  Und sie messen getrennt in drei Teilen.  Und in den Ergebnissen des Vergleichs werden wir auch eine solche Partition nehmen. <br><br><img src="https://habrastorage.org/webt/6v/d9/tx/6vd9tx38777pdvhxt4iij_rc4xu.jpeg"><br><br>  Sie kÃ¶nnen die QualitÃ¤t wie in 3D messen, ein Analogon der Schnittmenge Ã¼ber der Vereinigung, jedoch nicht das VerhÃ¤ltnis der FlÃ¤chen, sondern beispielsweise die Volumina.  Aber einem unbemannten Auto ist es in der Regel egal, was in der Z-Koordinate vor sich geht. Wir kÃ¶nnen eine Vogelperspektive von oben betrachten und eine Art Metrik verwenden, als ob wir alles in 2D betrachten wÃ¼rden.  Der Mensch wird mehr oder weniger in 2D navigiert, und ein unbemanntes Fahrzeug ist dasselbe.  Wie hoch die Box ist, ist nicht sehr wichtig. <br><br><img src="https://habrastorage.org/webt/gs/gg/js/gsggjsnlbhi0qd_ppdpnvqvlncm.jpeg"><br><br>  Was ist zu messen? <br><br><img src="https://habrastorage.org/webt/-p/vq/qn/-pvqqnyi_yovi_bit9x6lergyva.jpeg"><br><br>  Wahrscheinlich hat jeder, der zumindest irgendwie vor der Aufgabe stand, die Lidarwolke in 3D zu erkennen, von einem Datensatz wie KITTI gehÃ¶rt. <br><br><img src="https://habrastorage.org/webt/bp/bz/wn/bpbzwnd-1o69xdoonrm1k81vndo.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  In einigen StÃ¤dten in Deutschland wurde ein Datensatz aufgezeichnet, ein mit Sensoren ausgestattetes Auto ging, es hatte GPS-Sensoren, Kameras und Lidars.  Dann wurden ungefÃ¤hr 8000 Szenen markiert und in zwei Teile geteilt.  Ein Teil ist das Training, an dem jeder trainieren kann, und der zweite Teil ist die Validierung, um die Ergebnisse zu messen.  Die KITTI-Validierungsprobe gilt als QualitÃ¤tsmaÃŸ.  Erstens gibt es auf der KITTI-Datensatzseite eine Rangliste. Dort kÃ¶nnen Sie Ihre Entscheidung und Ihre Ergebnisse im Validierungsdatensatz senden und mit den Entscheidungen anderer Marktteilnehmer oder Forscher vergleichen.  Aber auch dieser Datensatz ist Ã¶ffentlich verfÃ¼gbar. Sie kÃ¶nnen ihn herunterladen, niemandem mitteilen, Ihren eigenen Ã¼berprÃ¼fen, mit Mitbewerbern vergleichen, aber nicht Ã¶ffentlich hochladen. <br><br><img src="https://habrastorage.org/webt/12/mu/8_/12mu8_z9hlu8-zuvhlm1bofiko4.jpeg"><br><br>  Externe DatensÃ¤tze sind gut, Sie mÃ¼ssen Ihre Zeit und Ressourcen nicht dafÃ¼r aufwenden, aber in der Regel kann ein Auto, das nach Deutschland gereist ist, mit vÃ¶llig anderen Sensoren ausgestattet werden.  Und es ist immer gut, einen eigenen internen Datensatz zu haben.  DarÃ¼ber hinaus ist es schwieriger, ein externes Dataset auf Kosten anderer zu erweitern, aber es ist einfacher, Ihr eigenes zu verwalten.  Deshalb nutzen wir den wunderbaren Yandex.Tolok-Service. <br><br><img src="https://habrastorage.org/webt/i4/ha/ns/i4hansnkczrkhpeq9_liyn8t9bk.jpeg"><br><br>  Wir haben unser spezielles Aufgabensystem fertiggestellt.  FÃ¼r den Benutzer, der beim Markup helfen und eine Belohnung dafÃ¼r erhalten mÃ¶chte, geben wir ein Bild von der Kamera aus, geben eine Lidarwolke aus, die Sie drehen, vergrÃ¶ÃŸern, verkleinern kÃ¶nnen, und bitten ihn, KÃ¤stchen zu platzieren, die unsere Begrenzungsrahmen begrenzen, damit ein Auto oder ein FuÃŸgÃ¤nger in sie eindringt oder etwas anderes.  Daher sammeln wir interne Proben fÃ¼r den persÃ¶nlichen Gebrauch. <br><br>  Angenommen, wir haben entschieden, welche Aufgabe wir lÃ¶sen und wie wir davon ausgehen, dass wir es gut oder schlecht gemacht haben.  Wir haben die Daten irgendwohin gebracht. <br><br>  Was sind die Algorithmen?  Beginnen wir mit 2D.  Die Aufgabe der 2D-Erkennung ist sehr bekannt und untersucht. <br><br><img src="https://habrastorage.org/webt/6x/vq/ta/6xvqtadjyarjzn9h9v0xz6cqrny.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sicherlich kennen viele Menschen den SSD-Algorithmus, der zu den modernsten Methoden zur Erkennung von 2D-Objekten gehÃ¶rt, und im Prinzip kÃ¶nnen wir davon ausgehen, dass das Problem der Erkennung von Objekten im Bild in gewisser Weise recht gut gelÃ¶st ist.  Wenn Ã¼berhaupt, kÃ¶nnen wir diese Ergebnisse als zusÃ¤tzliche Informationen verwenden. <br><br>  Unsere Lidarwolke hat jedoch ihre eigenen Eigenschaften, die sie stark vom Bild unterscheiden.  Erstens ist es sehr spÃ¤rlich.  Wenn das Bild eine dichte Struktur hat, die Pixel nah sind, alles dicht ist, dann ist die Wolke sehr dÃ¼nn, es gibt nicht so viele Punkte und es hat keine regulÃ¤re Struktur.  Rein physisch gibt es dort viel mehr Punkte als in der Ferne, und je weiter Sie gehen, desto weniger Punkte gibt es, desto weniger Genauigkeit gibt es, desto schwieriger ist es, etwas zu bestimmen. <br><br>  Nun, die Punkte aus der Wolke kommen im Prinzip in einer unverstÃ¤ndlichen Reihenfolge.  Niemand garantiert, dass ein Punkt immer frÃ¼her als der andere ist.  Sie kommen in relativ zufÃ¤lliger Reihenfolge.  Sie kÃ¶nnen sich irgendwie darauf einigen, sie zu sortieren oder im Voraus neu zu ordnen und erst dann Modelle an die Eingabe zu senden. Dies ist jedoch recht unpraktisch. Sie mÃ¼ssen Zeit aufwenden, um sie zu Ã¤ndern, und so weiter. <br><br>  Wir mÃ¶chten ein System entwickeln, das fÃ¼r unsere Probleme unverÃ¤nderlich ist und all diese Probleme lÃ¶st.  GlÃ¼cklicherweise hat CVPR im vergangenen Jahr ein solches System vorgestellt.  Es gab eine solche Architektur - PointNet.  Wie arbeitet sie? <br><br><img src="https://habrastorage.org/webt/ny/ay/np/nyaynp7lrsoopcumwsbjmm9ynik.jpeg"><br><br>  Am Eingang kommt eine Wolke von n Punkten mit jeweils drei Koordinaten an.  Dann wird jeder Punkt irgendwie durch eine spezielle kleine Transformation standardisiert.  Weiterhin wird es durch ein vollstÃ¤ndig verbundenes Netzwerk gefahren, um diese Punkte mit Zeichen anzureichern.  Dann findet wieder die Transformation statt und am Ende wird sie zusÃ¤tzlich angereichert.  Irgendwann werden n Punkte erhalten, aber jeder hat ungefÃ¤hr 1024 Merkmale, sie sind irgendwie standardisiert.  Bisher haben wir das Problem der Invarianz von Verschiebungen, Wendungen usw. noch nicht gelÃ¶st.  Hier wird vorgeschlagen, Max-Pooling durchzufÃ¼hren, das Maximum unter den Punkten auf jedem Kanal zu nehmen und einen Vektor mit 1024 Zeichen zu erhalten, der ein Deskriptor unserer Cloud ist und Informationen Ã¼ber die gesamte Cloud enthÃ¤lt.  Und dann kÃ¶nnen Sie mit diesem Deskriptor viele verschiedene Dinge tun. <br><br><img src="https://habrastorage.org/webt/qj/lr/hc/qjlrhclpvd2smfl3q2j28bft0sw.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sie kÃ¶nnen es beispielsweise auf die Deskriptoren einzelner Punkte kleben und das Segmentierungsproblem lÃ¶sen, damit jeder Punkt bestimmt, zu welchem â€‹â€‹Objekt er gehÃ¶rt.  Es ist nur eine StraÃŸe oder eine Person oder ein Auto.  Und hier sind die Ergebnisse aus dem Artikel. <br><br><img src="https://habrastorage.org/webt/kh/-o/cw/kh-ocwqrkgelcs958zio30nc7ek.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  MÃ¶glicherweise stellen Sie fest, dass dieser Algorithmus sehr gute Arbeit leistet.  Insbesondere gefÃ¤llt mir die kleine Tabelle, in der einige Daten Ã¼ber die Arbeitsplatte weggeworfen wurden, sehr gut, und er hat trotzdem festgestellt, wo sich die Beine befinden und wo sich die Arbeitsplatte befindet.  Insbesondere dieser Algorithmus kann als Baustein zum Aufbau weiterer Systeme verwendet werden. <br><br>  Ein Ansatz, der dies verwendet, ist der Frustum PointNets-Ansatz oder der Ansatz der abgeschnittenen Pyramide.  Die Idee ist ungefÃ¤hr so: Lassen Sie uns Objekte in 2D erkennen, wir sind gut darin. <br><br><img src="https://habrastorage.org/webt/lh/xb/uv/lhxbuvbemlftwpvufw7hbpmmhog.jpeg"><br><br>  Wenn wir dann wissen, wie die Kamera funktioniert, kÃ¶nnen wir abschÃ¤tzen, in welchem â€‹â€‹Bereich das fÃ¼r uns interessante Objekt, die Maschine, liegen kann.  Schneiden Sie zum Projizieren nur diesen Bereich aus und lÃ¶sen Sie bereits darauf das Problem, ein interessantes Objekt zu finden, beispielsweise eine Maschine.  Dies ist viel einfacher als die Suche nach einer beliebigen Anzahl von Autos in der Cloud.  Die Suche nach einem Auto genau in derselben Cloud scheint viel klarer und effizienter zu sein. <br><br><img src="https://habrastorage.org/webt/lt/t-/0v/ltt-0v8iobuju4yfoj-ips2uka8.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Die Architektur sieht ungefÃ¤hr so â€‹â€‹aus.  Zuerst wÃ¤hlen wir irgendwie die Regionen aus, die uns interessieren, in jeder Region, die wir segmentieren, und dann lÃ¶sen wir das Problem, einen Begrenzungsrahmen zu finden, der das fÃ¼r uns interessante Objekt begrenzt. <br><br><img src="https://habrastorage.org/webt/2w/lj/8g/2wlj8gt2m9vyiljomkkz0tjvd_y.jpeg"><br><br>  Der Ansatz hat sich bewÃ¤hrt.  Auf den Bildern sieht man, dass es ganz gut funktioniert, aber auch Nachteile hat.  Der Ansatz ist zweistufig, daher kann er langsam sein.  Wir mÃ¼ssen zuerst Netzwerke anwenden und 2D-Objekte erkennen, dann das Problem der Segmentierung und Zuordnung des Begrenzungsrahmens auf einem Teil der Wolke ausschneiden und dann lÃ¶sen, damit es etwas langsam arbeiten kann. <br><br>  Ein anderer Ansatz.  Warum verwandeln wir unsere Wolke nicht in eine Art Struktur, die wie ein Bild aussieht?  Die Idee ist folgende: Schauen wir es uns von oben an und probieren Sie unsere Lidarwolke.  Wir bekommen RaumwÃ¼rfel. <br><br><img src="https://habrastorage.org/webt/sc/7l/3h/sc7l3h0ewwcfmiuejsxnl7ozx5c.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  In jedem WÃ¼rfel haben wir einige Punkte.  Wir kÃ¶nnen einige Features auf ihnen zÃ¤hlen, aber wir kÃ¶nnen PointNet verwenden, das fÃ¼r jedes StÃ¼ck Platz eine Art Deskriptor zÃ¤hlt.  Wir werden ein Voxel bekommen, jedes Voxel hat eine charakteristische Beschreibung und es wird mehr oder weniger wie eine dichte Struktur aussehen, wie ein Bild.  Wir kÃ¶nnen bereits verschiedene Architekturen erstellen, zum Beispiel eine SSD-Ã¤hnliche Architektur zum Erkennen von Objekten. <br><br><img src="https://habrastorage.org/webt/x_/tb/c_/x_tbc_hsv_yuahyllkmlxijsowe.jpeg"><br><br>  Letzterer Ansatz war einer der ersten AnsÃ¤tze zum Kombinieren von Daten von mehreren Sensoren.  Es wÃ¤re eine SÃ¼nde, nur Lidar-Daten zu verwenden, wenn wir auch Kameradaten haben.  Einer dieser AnsÃ¤tze wird als Multi-View 3D-Objekterkennungsnetzwerk bezeichnet.  Seine Idee ist folgende: Drei KanÃ¤le mit Eingabedaten in die Eingabe eines groÃŸen Netzwerks einspeisen. <br><br><img src="https://habrastorage.org/webt/tz/xv/ty/tzxvty86li7jozjwozhqlun8r9a.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Dies ist ein Bild von der Kamera und in zwei Versionen eine Lidarwolke: von oben mit einer Vogelperspektive und einer Art Vorderansicht, was wir vor uns sehen.  Wir Ã¼bermitteln dies dem Eingang des Neurons, und es konfiguriert alles in sich selbst und gibt uns das Endergebnis - das Objekt. <br><br>  Ich mÃ¶chte diese Modelle vergleichen.  Im KITTI-Datensatz wird auf Validierungslaufwerken die QualitÃ¤t als Prozentsatz der durchschnittlichen Genauigkeit bewertet. <br><br><img src="https://habrastorage.org/webt/pt/ny/8x/ptny8xzcmojphtluvg5pxm1-mlq.jpeg"><br><br>  Sie werden vielleicht feststellen, dass F-PointNet recht gut und schnell genug funktioniert und alle anderen in verschiedenen Bereichen schlÃ¤gt - zumindest laut den Autoren. <br><br>  Unser Ansatz basiert auf mehr oder weniger allen Ideen, die ich aufgelistet habe.  Wenn Sie vergleichen, erhalten Sie ungefÃ¤hr das folgende Bild.  Wenn wir nicht den ersten Platz einnehmen, dann zumindest den zweiten.  DarÃ¼ber hinaus brechen wir bei den Objekten, die schwer zu erkennen sind, in die FÃ¼hrer aus.  Und vor allem ist unser Ansatz schnell genug.  Dies bedeutet, dass es fÃ¼r Echtzeitsysteme bereits recht gut geeignet ist, und es ist besonders wichtig, dass ein unbemanntes Fahrzeug Ã¼berwacht, was auf der StraÃŸe passiert, und alle diese Objekte hervorhebt. <br><br><img src="https://habrastorage.org/webt/ad/dx/bk/addxbko462x7r4doq22akwwkpik.jpeg"><br><br>  Fazit - ein Beispiel fÃ¼r unseren Detektor: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Es ist ersichtlich, dass die Situation kompliziert ist: Einige der Objekte sind geschlossen, andere fÃ¼r die Kamera nicht sichtbar.  FuÃŸgÃ¤nger, Radfahrer.  Aber der Detektor kommt gut genug zurecht.  Vielen Dank! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437674/">https://habr.com/ru/post/de437674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437660/index.html">Aktualisierung des Lebenszeitprofils in Visual Studio 2019 Vorschau 2</a></li>
<li><a href="../de437664/index.html">Zusammengesetzte RÃ¼ckgewinnung</a></li>
<li><a href="../de437666/index.html">AnkÃ¼ndigung der F # 4.6-Vorschau</a></li>
<li><a href="../de437670/index.html">MSVC-Backend-Updates in Visual Studio 2019 Vorschau 2: Neue Optimierungen, OpenMP- und Build-Durchsatzverbesserungen</a></li>
<li><a href="../de437672/index.html">cyberd: Berechnung des Wissens aus web3</a></li>
<li><a href="../de437676/index.html">UniversitÃ¤ten und Corporate Accelerators als Hebel fÃ¼r den Start eines B2B-Startups in den USA</a></li>
<li><a href="../de437680/index.html">Meine DIY-Sammlung auf Youtube</a></li>
<li><a href="../de437682/index.html">Schreiben eines weiteren Kubernetes-Template-Tools</a></li>
<li><a href="../de437684/index.html">Oberster Algorithmus - Voreingenommenes Kompendium</a></li>
<li><a href="../de437686/index.html">Learning go: Schreiben eines P2P-Messenger mit End-to-End-VerschlÃ¼sselung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>