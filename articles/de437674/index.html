<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👂 🧙🏻 💖 3D-Objekterkennungsmethoden für unbemannte Fahrzeuge. Yandex-Bericht 📁 🤸🏻 👩🏿‍⚕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unbemannte Autos können nicht ohne Verständnis auskommen, was sich in der Nähe befindet und wo genau. Im Dezember letzten Jahres hielt der Entwickler ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>3D-Objekterkennungsmethoden für unbemannte Fahrzeuge. Yandex-Bericht</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437674/">  Unbemannte Autos können nicht ohne Verständnis auskommen, was sich in der Nähe befindet und wo genau.  Im Dezember letzten Jahres hielt der Entwickler Victor Otliga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">vitonka</a> einen Vortrag über die Erkennung von 3D-Objekten am <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data-Christmas-Baum</a> .  Victor arbeitet in Richtung unbemannter Fahrzeuge Yandex, in der Gruppe, die die Verkehrssituation behandelt (und unterrichtet auch am ShAD).  Er erklärte, wie wir das Problem der Erkennung anderer Verkehrsteilnehmer in einer dreidimensionalen Punktwolke lösen, wie sich dieses Problem von der Erkennung von Objekten in einem Bild unterscheidet und wie wir von der gemeinsamen Nutzung verschiedener Sensortypen profitieren können. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Hallo allerseits!  Mein Name ist Victor Otliga, ich arbeite im Yandex-Büro in Minsk und entwickle unbemannte Fahrzeuge.  Heute werde ich über eine ziemlich wichtige Aufgabe für Drohnen sprechen - die Erkennung von 3D-Objekten um uns herum. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/ky/wn/hu/kywnhuprs-iq34tx4rgohnvwqm8.jpeg"><br><br>  Um zu fahren, muss man verstehen, was sich in der Nähe befindet.  Ich werde Ihnen kurz sagen, welche Sensoren und Sensoren in unbemannten Fahrzeugen verwendet werden und welche wir verwenden.  Ich werde Ihnen sagen, was die Aufgabe der Erkennung von 3D-Objekten ist und wie Sie die Qualität der Erkennung messen können.  Dann werde ich Ihnen sagen, woran diese Qualität gemessen werden kann.  Und dann werde ich einen kurzen Überblick über gute moderne Algorithmen geben, einschließlich derer, auf denen unsere Lösungen basieren.  Und am Ende - kleine Ergebnisse, ein Vergleich dieser Algorithmen, einschließlich unserer. <br><br><img src="https://habrastorage.org/webt/pk/sf/vo/pksfvoxtqwlyf6k-wido8ixektg.jpeg"><br><br>  So sieht unser funktionierender Prototyp eines unbemannten Autos jetzt aus.  Ein solches Taxi kann von jedem ohne Fahrer in der russischen Stadt Innopolis sowie in Skolkovo gemietet werden.  Und wenn Sie genau hinschauen, befindet sich oben ein großer Würfel.  Was ist da drin? <br><br><img src="https://habrastorage.org/webt/ei/3c/mq/ei3cmqe3l4kjfzjju-s1ndavkwi.jpeg"><br><br>  In einem einfachen Satz von Sensoren.  Es gibt eine GNSS- und GSM-Antenne, um festzustellen, wo sich das Auto befindet, und um mit der Außenwelt zu kommunizieren.  Wo ohne so einen klassischen Sensor wie eine Kamera.  Aber heute werden wir uns für Lidars interessieren. <br><br><img src="https://habrastorage.org/webt/cm/s6/pd/cms6pdxey7d4ykgktlc8lvyd8fc.jpeg"><br><br><img src="https://habrastorage.org/webt/un/ia/bg/uniabgh6yomlr2rourl7ynj3coc.jpeg"><br><br>  Lidar erzeugt ungefähr eine solche Punktwolke um sich herum, die drei Koordinaten hat.  Und du musst mit ihnen arbeiten.  Ich werde Ihnen sagen, wie Sie mithilfe eines Kamerabilds und einer Lidarwolke Objekte erkennen können. <br><br><img src="https://habrastorage.org/webt/yx/rt/si/yxrtsiwsnt_gcdgjbz42pp8lqvc.jpeg"><br><br>  Was ist die Herausforderung?  Das Bild von der Kamera wird in den Eingang eingegeben, die Kamera wird mit dem Lidar synchronisiert.  Es wäre seltsam, das Bild von der Kamera vor einer Sekunde zu verwenden, die Lidarwolke aus einem ganz anderen Moment zu nehmen und zu versuchen, Objekte darauf zu erkennen. <br><br><img src="https://habrastorage.org/webt/xa/g6/aj/xag6ajjiigpo5m737kklxiwkmb0.jpeg"><br><br>  Wir synchronisieren irgendwie Kameras und Lidars, dies ist eine separate schwierige Aufgabe, aber wir bewältigen sie erfolgreich.  Solche Daten werden in die Eingabe eingegeben, und am Ende möchten wir Kästchen erhalten, Begrenzungskästchen, die das Objekt einschränken: Fußgänger, Radfahrer, Autos und andere Verkehrsteilnehmer und nicht nur. <br><br>  Die Aufgabe wurde gestellt.  Wie werden wir es bewerten? <br><br><img src="https://habrastorage.org/webt/ew/hm/wd/ewhmwdgektceevdbmgu838n5wui.jpeg"><br><br>  Das Problem der 2D-Erkennung von Objekten in einem Bild wurde umfassend untersucht. <br><br><img src="https://habrastorage.org/webt/-c/ri/cu/-cricuguziob4idgyr_u-ihfq-e.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sie können Standardmetriken oder deren Analoga verwenden.  Es gibt einen Jacquard-Koeffizienten oder Schnittpunkt über der Vereinigung, einen wunderbaren Koeffizienten, der zeigt, wie gut wir ein Objekt erkannt haben.  Wir können eine Box nehmen, in der sich, wie wir annehmen, das Objekt befindet, und eine Box, in der es sich tatsächlich befindet.  Zählen Sie diese Metrik.  Es gibt Standardschwellen - sagen wir, für Autos nehmen sie oft eine Schwelle von 0,7.  Wenn dieser Wert größer als 0,7 ist, glauben wir, dass wir das Objekt erfolgreich erkannt haben und dass das Objekt dort ist.  Wir sind großartig, wir können noch weiter gehen. <br><br>  Um ein Objekt zu erkennen und zu verstehen, dass es sich irgendwo befindet, möchten wir außerdem das Vertrauen haben, dass wir das Objekt dort wirklich sehen, und es auch messen.  Sie können einfach messen und die durchschnittliche Genauigkeit berücksichtigen.  Sie können die Präzisionsrückrufkurve und den Bereich darunter nehmen und sagen: Je größer sie ist, desto besser. <br><br><img src="https://habrastorage.org/webt/p7/jk/z5/p7jkz5xnyyhehx99e73nijtb798.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Um die Qualität der 3D-Erkennung zu messen, nehmen sie normalerweise einen Datensatz und teilen ihn in mehrere Teile auf, da Objekte nahe oder weiter entfernt sein können und teilweise durch etwas anderes verdeckt werden können.  Daher ist die Validierungsstichprobe häufig in drei Teile unterteilt.  Objekte, die leicht zu erkennen, von mittlerer Komplexität und komplex, entfernt oder stark verdeckt sind.  Und sie messen getrennt in drei Teilen.  Und in den Ergebnissen des Vergleichs werden wir auch eine solche Partition nehmen. <br><br><img src="https://habrastorage.org/webt/6v/d9/tx/6vd9tx38777pdvhxt4iij_rc4xu.jpeg"><br><br>  Sie können die Qualität wie in 3D messen, ein Analogon der Schnittmenge über der Vereinigung, jedoch nicht das Verhältnis der Flächen, sondern beispielsweise die Volumina.  Aber einem unbemannten Auto ist es in der Regel egal, was in der Z-Koordinate vor sich geht. Wir können eine Vogelperspektive von oben betrachten und eine Art Metrik verwenden, als ob wir alles in 2D betrachten würden.  Der Mensch wird mehr oder weniger in 2D navigiert, und ein unbemanntes Fahrzeug ist dasselbe.  Wie hoch die Box ist, ist nicht sehr wichtig. <br><br><img src="https://habrastorage.org/webt/gs/gg/js/gsggjsnlbhi0qd_ppdpnvqvlncm.jpeg"><br><br>  Was ist zu messen? <br><br><img src="https://habrastorage.org/webt/-p/vq/qn/-pvqqnyi_yovi_bit9x6lergyva.jpeg"><br><br>  Wahrscheinlich hat jeder, der zumindest irgendwie vor der Aufgabe stand, die Lidarwolke in 3D zu erkennen, von einem Datensatz wie KITTI gehört. <br><br><img src="https://habrastorage.org/webt/bp/bz/wn/bpbzwnd-1o69xdoonrm1k81vndo.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  In einigen Städten in Deutschland wurde ein Datensatz aufgezeichnet, ein mit Sensoren ausgestattetes Auto ging, es hatte GPS-Sensoren, Kameras und Lidars.  Dann wurden ungefähr 8000 Szenen markiert und in zwei Teile geteilt.  Ein Teil ist das Training, an dem jeder trainieren kann, und der zweite Teil ist die Validierung, um die Ergebnisse zu messen.  Die KITTI-Validierungsprobe gilt als Qualitätsmaß.  Erstens gibt es auf der KITTI-Datensatzseite eine Rangliste. Dort können Sie Ihre Entscheidung und Ihre Ergebnisse im Validierungsdatensatz senden und mit den Entscheidungen anderer Marktteilnehmer oder Forscher vergleichen.  Aber auch dieser Datensatz ist öffentlich verfügbar. Sie können ihn herunterladen, niemandem mitteilen, Ihren eigenen überprüfen, mit Mitbewerbern vergleichen, aber nicht öffentlich hochladen. <br><br><img src="https://habrastorage.org/webt/12/mu/8_/12mu8_z9hlu8-zuvhlm1bofiko4.jpeg"><br><br>  Externe Datensätze sind gut, Sie müssen Ihre Zeit und Ressourcen nicht dafür aufwenden, aber in der Regel kann ein Auto, das nach Deutschland gereist ist, mit völlig anderen Sensoren ausgestattet werden.  Und es ist immer gut, einen eigenen internen Datensatz zu haben.  Darüber hinaus ist es schwieriger, ein externes Dataset auf Kosten anderer zu erweitern, aber es ist einfacher, Ihr eigenes zu verwalten.  Deshalb nutzen wir den wunderbaren Yandex.Tolok-Service. <br><br><img src="https://habrastorage.org/webt/i4/ha/ns/i4hansnkczrkhpeq9_liyn8t9bk.jpeg"><br><br>  Wir haben unser spezielles Aufgabensystem fertiggestellt.  Für den Benutzer, der beim Markup helfen und eine Belohnung dafür erhalten möchte, geben wir ein Bild von der Kamera aus, geben eine Lidarwolke aus, die Sie drehen, vergrößern, verkleinern können, und bitten ihn, Kästchen zu platzieren, die unsere Begrenzungsrahmen begrenzen, damit ein Auto oder ein Fußgänger in sie eindringt oder etwas anderes.  Daher sammeln wir interne Proben für den persönlichen Gebrauch. <br><br>  Angenommen, wir haben entschieden, welche Aufgabe wir lösen und wie wir davon ausgehen, dass wir es gut oder schlecht gemacht haben.  Wir haben die Daten irgendwohin gebracht. <br><br>  Was sind die Algorithmen?  Beginnen wir mit 2D.  Die Aufgabe der 2D-Erkennung ist sehr bekannt und untersucht. <br><br><img src="https://habrastorage.org/webt/6x/vq/ta/6xvqtadjyarjzn9h9v0xz6cqrny.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sicherlich kennen viele Menschen den SSD-Algorithmus, der zu den modernsten Methoden zur Erkennung von 2D-Objekten gehört, und im Prinzip können wir davon ausgehen, dass das Problem der Erkennung von Objekten im Bild in gewisser Weise recht gut gelöst ist.  Wenn überhaupt, können wir diese Ergebnisse als zusätzliche Informationen verwenden. <br><br>  Unsere Lidarwolke hat jedoch ihre eigenen Eigenschaften, die sie stark vom Bild unterscheiden.  Erstens ist es sehr spärlich.  Wenn das Bild eine dichte Struktur hat, die Pixel nah sind, alles dicht ist, dann ist die Wolke sehr dünn, es gibt nicht so viele Punkte und es hat keine reguläre Struktur.  Rein physisch gibt es dort viel mehr Punkte als in der Ferne, und je weiter Sie gehen, desto weniger Punkte gibt es, desto weniger Genauigkeit gibt es, desto schwieriger ist es, etwas zu bestimmen. <br><br>  Nun, die Punkte aus der Wolke kommen im Prinzip in einer unverständlichen Reihenfolge.  Niemand garantiert, dass ein Punkt immer früher als der andere ist.  Sie kommen in relativ zufälliger Reihenfolge.  Sie können sich irgendwie darauf einigen, sie zu sortieren oder im Voraus neu zu ordnen und erst dann Modelle an die Eingabe zu senden. Dies ist jedoch recht unpraktisch. Sie müssen Zeit aufwenden, um sie zu ändern, und so weiter. <br><br>  Wir möchten ein System entwickeln, das für unsere Probleme unveränderlich ist und all diese Probleme löst.  Glücklicherweise hat CVPR im vergangenen Jahr ein solches System vorgestellt.  Es gab eine solche Architektur - PointNet.  Wie arbeitet sie? <br><br><img src="https://habrastorage.org/webt/ny/ay/np/nyaynp7lrsoopcumwsbjmm9ynik.jpeg"><br><br>  Am Eingang kommt eine Wolke von n Punkten mit jeweils drei Koordinaten an.  Dann wird jeder Punkt irgendwie durch eine spezielle kleine Transformation standardisiert.  Weiterhin wird es durch ein vollständig verbundenes Netzwerk gefahren, um diese Punkte mit Zeichen anzureichern.  Dann findet wieder die Transformation statt und am Ende wird sie zusätzlich angereichert.  Irgendwann werden n Punkte erhalten, aber jeder hat ungefähr 1024 Merkmale, sie sind irgendwie standardisiert.  Bisher haben wir das Problem der Invarianz von Verschiebungen, Wendungen usw. noch nicht gelöst.  Hier wird vorgeschlagen, Max-Pooling durchzuführen, das Maximum unter den Punkten auf jedem Kanal zu nehmen und einen Vektor mit 1024 Zeichen zu erhalten, der ein Deskriptor unserer Cloud ist und Informationen über die gesamte Cloud enthält.  Und dann können Sie mit diesem Deskriptor viele verschiedene Dinge tun. <br><br><img src="https://habrastorage.org/webt/qj/lr/hc/qjlrhclpvd2smfl3q2j28bft0sw.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Sie können es beispielsweise auf die Deskriptoren einzelner Punkte kleben und das Segmentierungsproblem lösen, damit jeder Punkt bestimmt, zu welchem ​​Objekt er gehört.  Es ist nur eine Straße oder eine Person oder ein Auto.  Und hier sind die Ergebnisse aus dem Artikel. <br><br><img src="https://habrastorage.org/webt/kh/-o/cw/kh-ocwqrkgelcs958zio30nc7ek.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Möglicherweise stellen Sie fest, dass dieser Algorithmus sehr gute Arbeit leistet.  Insbesondere gefällt mir die kleine Tabelle, in der einige Daten über die Arbeitsplatte weggeworfen wurden, sehr gut, und er hat trotzdem festgestellt, wo sich die Beine befinden und wo sich die Arbeitsplatte befindet.  Insbesondere dieser Algorithmus kann als Baustein zum Aufbau weiterer Systeme verwendet werden. <br><br>  Ein Ansatz, der dies verwendet, ist der Frustum PointNets-Ansatz oder der Ansatz der abgeschnittenen Pyramide.  Die Idee ist ungefähr so: Lassen Sie uns Objekte in 2D erkennen, wir sind gut darin. <br><br><img src="https://habrastorage.org/webt/lh/xb/uv/lhxbuvbemlftwpvufw7hbpmmhog.jpeg"><br><br>  Wenn wir dann wissen, wie die Kamera funktioniert, können wir abschätzen, in welchem ​​Bereich das für uns interessante Objekt, die Maschine, liegen kann.  Schneiden Sie zum Projizieren nur diesen Bereich aus und lösen Sie bereits darauf das Problem, ein interessantes Objekt zu finden, beispielsweise eine Maschine.  Dies ist viel einfacher als die Suche nach einer beliebigen Anzahl von Autos in der Cloud.  Die Suche nach einem Auto genau in derselben Cloud scheint viel klarer und effizienter zu sein. <br><br><img src="https://habrastorage.org/webt/lt/t-/0v/ltt-0v8iobuju4yfoj-ips2uka8.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Die Architektur sieht ungefähr so ​​aus.  Zuerst wählen wir irgendwie die Regionen aus, die uns interessieren, in jeder Region, die wir segmentieren, und dann lösen wir das Problem, einen Begrenzungsrahmen zu finden, der das für uns interessante Objekt begrenzt. <br><br><img src="https://habrastorage.org/webt/2w/lj/8g/2wlj8gt2m9vyiljomkkz0tjvd_y.jpeg"><br><br>  Der Ansatz hat sich bewährt.  Auf den Bildern sieht man, dass es ganz gut funktioniert, aber auch Nachteile hat.  Der Ansatz ist zweistufig, daher kann er langsam sein.  Wir müssen zuerst Netzwerke anwenden und 2D-Objekte erkennen, dann das Problem der Segmentierung und Zuordnung des Begrenzungsrahmens auf einem Teil der Wolke ausschneiden und dann lösen, damit es etwas langsam arbeiten kann. <br><br>  Ein anderer Ansatz.  Warum verwandeln wir unsere Wolke nicht in eine Art Struktur, die wie ein Bild aussieht?  Die Idee ist folgende: Schauen wir es uns von oben an und probieren Sie unsere Lidarwolke.  Wir bekommen Raumwürfel. <br><br><img src="https://habrastorage.org/webt/sc/7l/3h/sc7l3h0ewwcfmiuejsxnl7ozx5c.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  In jedem Würfel haben wir einige Punkte.  Wir können einige Features auf ihnen zählen, aber wir können PointNet verwenden, das für jedes Stück Platz eine Art Deskriptor zählt.  Wir werden ein Voxel bekommen, jedes Voxel hat eine charakteristische Beschreibung und es wird mehr oder weniger wie eine dichte Struktur aussehen, wie ein Bild.  Wir können bereits verschiedene Architekturen erstellen, zum Beispiel eine SSD-ähnliche Architektur zum Erkennen von Objekten. <br><br><img src="https://habrastorage.org/webt/x_/tb/c_/x_tbc_hsv_yuahyllkmlxijsowe.jpeg"><br><br>  Letzterer Ansatz war einer der ersten Ansätze zum Kombinieren von Daten von mehreren Sensoren.  Es wäre eine Sünde, nur Lidar-Daten zu verwenden, wenn wir auch Kameradaten haben.  Einer dieser Ansätze wird als Multi-View 3D-Objekterkennungsnetzwerk bezeichnet.  Seine Idee ist folgende: Drei Kanäle mit Eingabedaten in die Eingabe eines großen Netzwerks einspeisen. <br><br><img src="https://habrastorage.org/webt/tz/xv/ty/tzxvty86li7jozjwozhqlun8r9a.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></sub></sup> </h5><br>  Dies ist ein Bild von der Kamera und in zwei Versionen eine Lidarwolke: von oben mit einer Vogelperspektive und einer Art Vorderansicht, was wir vor uns sehen.  Wir übermitteln dies dem Eingang des Neurons, und es konfiguriert alles in sich selbst und gibt uns das Endergebnis - das Objekt. <br><br>  Ich möchte diese Modelle vergleichen.  Im KITTI-Datensatz wird auf Validierungslaufwerken die Qualität als Prozentsatz der durchschnittlichen Genauigkeit bewertet. <br><br><img src="https://habrastorage.org/webt/pt/ny/8x/ptny8xzcmojphtluvg5pxm1-mlq.jpeg"><br><br>  Sie werden vielleicht feststellen, dass F-PointNet recht gut und schnell genug funktioniert und alle anderen in verschiedenen Bereichen schlägt - zumindest laut den Autoren. <br><br>  Unser Ansatz basiert auf mehr oder weniger allen Ideen, die ich aufgelistet habe.  Wenn Sie vergleichen, erhalten Sie ungefähr das folgende Bild.  Wenn wir nicht den ersten Platz einnehmen, dann zumindest den zweiten.  Darüber hinaus brechen wir bei den Objekten, die schwer zu erkennen sind, in die Führer aus.  Und vor allem ist unser Ansatz schnell genug.  Dies bedeutet, dass es für Echtzeitsysteme bereits recht gut geeignet ist, und es ist besonders wichtig, dass ein unbemanntes Fahrzeug überwacht, was auf der Straße passiert, und alle diese Objekte hervorhebt. <br><br><img src="https://habrastorage.org/webt/ad/dx/bk/addxbko462x7r4doq22akwwkpik.jpeg"><br><br>  Fazit - ein Beispiel für unseren Detektor: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Es ist ersichtlich, dass die Situation kompliziert ist: Einige der Objekte sind geschlossen, andere für die Kamera nicht sichtbar.  Fußgänger, Radfahrer.  Aber der Detektor kommt gut genug zurecht.  Vielen Dank! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437674/">https://habr.com/ru/post/de437674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437660/index.html">Aktualisierung des Lebenszeitprofils in Visual Studio 2019 Vorschau 2</a></li>
<li><a href="../de437664/index.html">Zusammengesetzte Rückgewinnung</a></li>
<li><a href="../de437666/index.html">Ankündigung der F # 4.6-Vorschau</a></li>
<li><a href="../de437670/index.html">MSVC-Backend-Updates in Visual Studio 2019 Vorschau 2: Neue Optimierungen, OpenMP- und Build-Durchsatzverbesserungen</a></li>
<li><a href="../de437672/index.html">cyberd: Berechnung des Wissens aus web3</a></li>
<li><a href="../de437676/index.html">Universitäten und Corporate Accelerators als Hebel für den Start eines B2B-Startups in den USA</a></li>
<li><a href="../de437680/index.html">Meine DIY-Sammlung auf Youtube</a></li>
<li><a href="../de437682/index.html">Schreiben eines weiteren Kubernetes-Template-Tools</a></li>
<li><a href="../de437684/index.html">Oberster Algorithmus - Voreingenommenes Kompendium</a></li>
<li><a href="../de437686/index.html">Learning go: Schreiben eines P2P-Messenger mit End-to-End-Verschlüsselung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>