<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëãüèª ü§∑üèΩ ü§æüèæ Como o armazenamento do S3 DataLine funciona üö± üë©üèº‚Äçüî¨ üå¨Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! 

 N√£o √© segredo que grandes quantidades de dados est√£o envolvidas no trabalho de aplicativos modernos e seu fluxo est√° em constante crescim...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como o armazenamento do S3 DataLine funciona</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dataline/blog/423853/"><img src="https://habrastorage.org/webt/r2/ri/yr/r2riyrsimmtddysut6vvaatcqtw.png"><br><br>  Ol√° Habr! <br><br>  N√£o √© segredo que grandes quantidades de dados est√£o envolvidas no trabalho de aplicativos modernos e seu fluxo est√° em constante crescimento.  Esses dados precisam ser armazenados e processados, geralmente em um grande n√∫mero de m√°quinas, e isso n√£o √© uma tarefa f√°cil.  Para resolv√™-lo, existem armazenamentos de objetos na nuvem.  Geralmente eles s√£o uma implementa√ß√£o da tecnologia de armazenamento definido por software. <br><br>  No in√≠cio de 2018, lan√ßamos (e lan√ßamos!) Nosso pr√≥prio armazenamento 100% compat√≠vel com S3, baseado no Cloudian HyperStore.  Como se viu, a rede tem muito poucas publica√ß√µes em russo sobre o pr√≥prio Cloudian e muito menos sobre a aplica√ß√£o real dessa solu√ß√£o. <br><br>  Hoje, com base na experi√™ncia do DataLine, vou falar sobre a arquitetura e a estrutura interna do software Cloudian, incluindo a implementa√ß√£o do Cloudian SDS com base em v√°rias solu√ß√µes arquitet√¥nicas Apache Cassandra.  Separadamente, consideramos o mais interessante em qualquer armazenamento SDS - a l√≥gica de garantir toler√¢ncia a falhas e distribui√ß√£o de objetos. <br><br>  Se voc√™ estiver construindo seu armazenamento S3 ou estiver ocupado mantendo-o, este artigo ser√° √∫til para voc√™. <br><a name="habracut"></a><br>  Antes de tudo, explicarei por que nossa escolha caiu no Cloudian.  √â simples: existem muito poucas op√ß√µes dignas nesse nicho.  Por exemplo, alguns anos atr√°s, quando est√°vamos pensando em construir, havia apenas tr√™s op√ß√µes: <br><br><ul><li>  Gateway CEHP + RADOS; <br></li><li>  Minio <br></li><li>  Cloudian HyperStore. <br></li></ul><br>  Para n√≥s, como provedor de servi√ßos, os fatores decisivos foram: um alto n√≠vel de correspond√™ncia entre a API de armazenamento e o Amazon S3 original, a disponibilidade do faturamento interno, a escalabilidade com suporte multirregionalidade e a presen√ßa de uma terceira linha de suporte ao fornecedor.  Cloudian tem tudo isso. <br><br>  E sim, o mais (certamente!) O mais importante √© que o DataLine e o Cloudian t√™m cores corporativas semelhantes.  Voc√™ deve admitir que n√£o conseguimos resistir a tanta beleza. <br><br><img src="https://habrastorage.org/webt/cj/mh/_u/cjmh_uot3g8v4spn8vgm4fvvngo.png"><br><br>  Infelizmente, o Cloudian n√£o √© o software mais comum e praticamente n√£o h√° informa√ß√µes sobre ele no RuNet.  Hoje vamos corrigir essa injusti√ßa e conversar com voc√™ sobre os recursos da arquitetura HyperStore, examinar seus componentes mais importantes e lidar com as principais nuances da arquitetura.  Vamos come√ßar com o mais b√°sico, a saber: o que √© o Cloudian sob o cap√¥? <br><br><h1>  Como o armazenamento Cloudian HyperStore funciona </h1><br>  Vamos dar uma olhada no diagrama e ver como a solu√ß√£o Cloudian funciona. <br><br><img src="https://habrastorage.org/webt/8m/_n/0x/8m_n0xtx0vtlx50himh-wxrkjfm.jpeg"><br>  <i>O principal esquema de armazenamento de componentes.</i> <br><br>  Como podemos ver, o sistema consiste em v√°rios componentes principais: <br><br><ul><li>  <b>Cloudian Management Control</b> - <i>console de gerenciamento</i> ; </li><li>  <b>Servi√ßo Administrativo</b> - <i>m√≥dulo de administra√ß√£o interna</i> ; </li><li>  <b>Servi√ßo S3</b> - o <i>m√≥dulo respons√°vel pelo suporte ao protocolo S3</i> ; </li><li>  <b>Servi√ßo HyperStore</b> - o <i>servi√ßo de armazenamento real</i> ; </li><li>  <b>Apache Cassandra</b> - um <i>reposit√≥rio centralizado de dados de servi√ßo</i> ; </li><li>  <b>Redis</b> - <i>para os dados lidos com mais frequ√™ncia</i> . </li></ul><br>  De maior interesse para n√≥s ser√° o trabalho dos principais servi√ßos, S3 Service e HyperStore Service, e consideraremos cuidadosamente o trabalho deles.  Mas, primeiro, faz sentido descobrir como a distribui√ß√£o dos servi√ßos no cluster √© organizada e qual √© a toler√¢ncia a falhas e a confiabilidade do armazenamento de dados desta solu√ß√£o como um todo. <br><br><img src="https://habrastorage.org/webt/hr/vq/su/hrvqsuhmqgexmgetlc72lsfwhqu.jpeg"><br><br><br>  Por <i>servi√ßos comuns</i> no diagrama acima, entendemos os <b>servi√ßos S3, HyperStore, CMC e Apache Cassandra</b> .  √Ä primeira vista, tudo √© bonito e arrumado.  Por√©m, ap√≥s uma an√°lise mais detalhada, verifica-se que apenas uma falha de um n√≥ √© efetivamente resolvida.  E a perda simult√¢nea de dois n√≥s ao mesmo tempo pode ser fatal para a disponibilidade do cluster - o Redis QoS (no n√≥ 2) possui apenas 1 escravo (no n√≥ 3).  A mesma imagem com o risco de perder o gerenciamento de cluster - o Puppet Server √© apenas em dois n√≥s (1 e 2).  No entanto, a probabilidade de falha de dois n√≥s ao mesmo tempo √© muito baixa e voc√™ pode conviver com ele. <br><br>  No entanto, para aumentar a confiabilidade do armazenamento, usamos 4 n√≥s no DataLine em vez dos tr√™s m√≠nimos.  A seguinte distribui√ß√£o de recursos √© obtida: <br><br><img src="https://habrastorage.org/webt/x0/ue/f2/x0uef2dubkivpngycrxidcjvx1u.png"><br><br>  Mais uma nuance chama sua aten√ß√£o imediatamente - as <b>credenciais Redis</b> n√£o <b>s√£o</b> colocadas em todos os n√≥s (como poderia ser assumido no esquema oficial acima), mas apenas em tr√™s deles.  Nesse caso, as <b>credenciais Redis s√£o</b> usadas para todas as solicita√ß√µes recebidas.  Acontece que, devido √† necessidade de ir para os Redis de outra pessoa, h√° algum desequil√≠brio no desempenho do quarto n√≥. <br><br>  Para n√≥s, isso ainda n√£o √© significativo.  Durante o teste de estresse, n√£o foram observados desvios significativos na velocidade de resposta dos n√≥s, mas em grandes grupos de dezenas de n√≥s em funcionamento, faz sentido corrigir essa nuance. <br><br>  √â assim que o esquema de migra√ß√£o em 6 n√≥s se parece: <br><br><img src="https://habrastorage.org/webt/yc/df/1l/ycdf1lkqic3oh2rb6iu-yhyoiyo.jpeg"><br><br>  <i>O diagrama mostra como a migra√ß√£o de servi√ßo √© implementada no evento de uma falha do n√≥.</i>  <i>Somente a falha de um servidor de cada fun√ß√£o √© levada em considera√ß√£o.</i>  <i>Se ambos os servidores ca√≠rem, ser√° necess√°ria interven√ß√£o manual.</i> <br><br>  Aqui tamb√©m o neg√≥cio n√£o ficou isento de sutilezas.  A migra√ß√£o de fun√ß√£o usa o Puppet.  Portanto, se voc√™ o perder ou acidentalmente quebr√°-lo, o failover autom√°tico pode n√£o funcionar.  Pelo mesmo motivo, voc√™ n√£o deve editar manualmente o manifesto do Puppet interno.  Isso n√£o √© totalmente seguro, as altera√ß√µes podem ser repentinamente desgastadas, pois os manifestos s√£o editados usando o painel de administra√ß√£o do cluster. <br><br>  Do ponto de vista da seguran√ßa dos dados, tudo √© muito mais interessante.  Os metadados do objeto s√£o armazenados no Apache Cassandra e cada registro √© replicado para 3 de 4 n√≥s.  O fator de replica√ß√£o 3 tamb√©m √© usado para armazenar dados, mas voc√™ pode configurar um maior.  Isso garante a seguran√ßa dos dados, mesmo em caso de falha simult√¢nea de 2 dos 4 n√≥s.  E se voc√™ tiver tempo para reequilibrar o cluster, n√£o poder√° perder nada com um n√≥ restante.  O principal √© ter espa√ßo suficiente. <br><br><img src="https://habrastorage.org/webt/do/oo/ip/doooipch3gctjx0ruteyepc3n2w.jpeg"><br><br>  <i>√â o que acontece quando dois n√≥s falham.</i>  <i>O diagrama mostra claramente que, mesmo nessa situa√ß√£o, os dados permanecem seguros</i> <br><br>  Ao mesmo tempo, a disponibilidade de dados e armazenamento depender√° da estrat√©gia de garantir consist√™ncia.  Para dados, metadados, leitura e grava√ß√£o, eles s√£o configurados separadamente. <br><br>  As op√ß√µes v√°lidas s√£o pelo menos um n√≥, quorum ou todos os n√≥s. <br>  Essa configura√ß√£o determina quantos n√≥s devem confirmar a grava√ß√£o / leitura para que a solicita√ß√£o seja considerada bem-sucedida.  Usamos o quorum como um compromisso razo√°vel entre o tempo necess√°rio para processar uma solicita√ß√£o e a confiabilidade da escrita / inconsist√™ncia da leitura.  Ou seja, dos tr√™s n√≥s envolvidos na opera√ß√£o, para uma opera√ß√£o sem erros, basta obter uma resposta consistente de 2.  Portanto, para manter-se √† tona no caso de falha de mais de um n√≥, voc√™ precisar√° mudar para uma √∫nica estrat√©gia de grava√ß√£o / leitura. <br><br><h2>  Processamento de consultas no Cloudian </h2><br>  Abaixo, consideraremos dois esquemas para processar solicita√ß√µes recebidas no Cloudian HyperStore, PUT e GET.  Essa √© a principal tarefa do S3 Service e HyperStore. <br><br>  Vamos come√ßar como a solicita√ß√£o de grava√ß√£o √© processada: <br><br><img src="https://habrastorage.org/webt/ig/ae/g7/igaeg7v86nw9e1rgxt6xbdlhxb8.jpeg"><br><br>  Certamente, voc√™ observou que cada solicita√ß√£o gera muitas verifica√ß√µes e recupera√ß√µes de dados, pelo menos seis ocorr√™ncias de componente para componente.  √â a partir daqui que os atrasos na grava√ß√£o e o alto consumo de tempo da CPU aparecem quando se trabalha com arquivos pequenos. <br><br>  Arquivos grandes s√£o transmitidos por peda√ßos.  Peda√ßos separados n√£o s√£o considerados pedidos separados e algumas verifica√ß√µes n√£o s√£o realizadas. <br><br>  O n√≥ que recebeu a solicita√ß√£o inicial determina ainda de maneira independente onde e o que gravar, mesmo que n√£o seja gravado diretamente nele.  Isso permite ocultar a organiza√ß√£o interna do cluster do cliente final e usar balanceadores de carga externos.  Tudo isso afeta positivamente a facilidade de manuten√ß√£o e a toler√¢ncia a falhas do armazenamento. <br><br><img src="https://habrastorage.org/webt/ss/tr/zj/sstrzjuve-nm6oyz2mj0yitmnts.jpeg"><br><br>  Como voc√™ pode ver, a l√≥gica de leitura n√£o √© muito diferente da escrita.  Nele, √© observada a mesma alta sensibilidade do desempenho para o tamanho dos objetos processados.  Portanto, devido a economias significativas no trabalho com metadados, √© muito mais f√°cil extrair um objeto picado do que muitos objetos separados do mesmo volume total. <br><br><h2>  Armazenamento e duplica√ß√£o de dados </h2><br>  Como voc√™ pode ver nos diagramas acima, o Cloudian suporta v√°rios esquemas de armazenamento e duplica√ß√£o de dados: <br><br>  <b>Replica√ß√£o</b> - usando a replica√ß√£o, √© poss√≠vel manter um n√∫mero personalizado de c√≥pias de cada objeto de dados no sistema e armazenar cada c√≥pia em n√≥s diferentes.  Por exemplo, usando a replica√ß√£o 3X, 3 c√≥pias de cada objeto s√£o criadas e cada c√≥pia ‚Äúfica‚Äù em seu pr√≥prio n√≥. <br><br>  <b>Codifica√ß√£o de apagamento</b> - Com a codifica√ß√£o de apagamento, cada objeto √© codificado em uma quantidade personalizada (conhecida como n√∫mero K) de fragmentos de dados mais uma quantidade personalizada de c√≥digo de redund√¢ncia (n√∫mero M).  Cada fragmento K + M de um objeto √© √∫nico e cada fragmento √© armazenado em seu pr√≥prio n√≥.  Um objeto pode ser decodificado usando qualquer K fragmento.  Em outras palavras, o objeto permanece leg√≠vel, mesmo se os n√≥s M estiverem inacess√≠veis. <br><br>  Por exemplo, na codifica√ß√£o de apagamento, de acordo com a f√≥rmula 4 + 2 (4 fragmentos de dados mais 2 fragmentos de c√≥digo de redund√¢ncia), cada objeto √© dividido em 6 fragmentos exclusivos armazenados em seis n√≥s diferentes, e esse objeto pode ser restaurado e lido se houver 4 de 6 fragmentos dispon√≠veis . <br><br>  A vantagem do Erasure Coding, em compara√ß√£o com a replica√ß√£o, √© economizar espa√ßo, embora ao custo de um aumento significativo na carga do processador, piorando a velocidade de resposta e a necessidade de procedimentos em segundo plano para controlar a consist√™ncia dos objetos.  De qualquer forma, os metadados s√£o armazenados separadamente dos dados (no Apache Cassandra), o que aumenta a flexibilidade e a confiabilidade da solu√ß√£o. <br><br><h2>  Brevemente sobre outras fun√ß√µes do HyperStore </h2><br>  Como escrevi no come√ßo deste artigo, v√°rias ferramentas √∫teis s√£o incorporadas ao HyperStore.  Entre eles est√£o: <br><br><ul><li>  Faturamento flex√≠vel com suporte para alterar o pre√ßo de um recurso, dependendo do volume e plano tarif√°rio; <br></li><li>  Monitoramento incorporado <br></li><li>  A capacidade de limitar o uso de recursos para usu√°rios e grupos de usu√°rios; <br></li><li>  Configura√ß√µes de QoS e procedimentos internos para equilibrar o uso de recursos entre n√≥s, bem como procedimentos regulares para rebalancear entre n√≥s e discos nos n√≥s ou ao inserir novos n√≥s em um cluster. <br></li></ul><br>  No entanto, o Cloudian HyperStore ainda n√£o √© perfeito.  Por exemplo, por algum motivo, voc√™ n√£o pode transferir uma conta existente para outro grupo ou atribuir v√°rios grupos a um registro.  N√£o √© poss√≠vel gerar relat√≥rios de cobran√ßa intermedi√°rios - voc√™ receber√° todos os relat√≥rios somente ap√≥s o encerramento do per√≠odo do relat√≥rio.  Portanto, nem clientes nem podemos descobrir quanto a conta cresceu em tempo real. <br><br><h1>  L√≥gica Cloudian HyperStore </h1><br>  Agora vamos nos aprofundar ainda mais e examinar o mais interessante em qualquer armazenamento SDS - a l√≥gica da distribui√ß√£o de objetos por n√≥s.  No caso do armazenamento Cloudian, os metadados s√£o armazenados separadamente dos pr√≥prios dados.  Para metadados, o Cassandra √© usado, para dados, a solu√ß√£o propriet√°ria HyperStore. <br><br>  Infelizmente, at√© o momento n√£o h√° tradu√ß√£o oficial da documenta√ß√£o do Cloudian para o russo na Internet; portanto, postarei minha tradu√ß√£o das partes mais interessantes desta documenta√ß√£o. <br><br><h2>  O papel do Apache Cassandra no HyperStore </h2><br>  No HyperStore, o Cassandra √© usado para armazenar metadados do objeto, informa√ß√µes da conta do usu√°rio e dados de uso do servi√ßo.  Em uma implanta√ß√£o t√≠pica em cada HyperStore, os dados do Cassandra s√£o armazenados na mesma unidade do sistema operacional.  O sistema tamb√©m suporta dados do Cassandra em uma unidade dedicada em cada n√≥.  Os dados do Cassandra n√£o s√£o armazenados nos discos de dados do HyperStore.  Quando os vNodes s√£o atribu√≠dos ao host, eles s√£o distribu√≠dos apenas para os n√≥s de armazenamento HyperStore.  vNodes n√£o s√£o alocados para a unidade em que os dados do Cassandra est√£o armazenados. <br>  Dentro do cluster, os metadados no Cassandra s√£o replicados de acordo com a pol√≠tica (estrat√©gia) do seu reposit√≥rio.  O Cassandra Data Replication usa vNodes desta maneira: <br><br><ul><li>  Ao criar um novo objeto Cassandra (chave prim√°ria e seus valores correspondentes), ele √© um hash e o hash √© usado para associar o objeto a um vNode espec√≠fico.  O sistema verifica a qual host esse vNode est√° atribu√≠do e, em seguida, a primeira r√©plica do objeto Cassandra √© armazenada na unidade Cassandra nesse host. <br></li><li>  Por exemplo, suponha que um host receba 96 vNodes distribu√≠dos em v√°rios discos de dados HyperStore.  Os objetos Cassandra cujos valores de hash se enquadram nos intervalos de token de qualquer um desses 96 vNodes ser√£o gravados na unidade Cassandra neste host. <br></li><li>  R√©plicas adicionais do objeto Cassandra (o n√∫mero de r√©plicas depende da sua configura√ß√£o) s√£o associadas aos vNodes com o seguinte n√∫mero de sequ√™ncia e armazenadas no n√≥ ao qual esses vNodes s√£o atribu√≠dos, desde que os vNodes sejam ignorados, se necess√°rio, para que cada r√©plica do objeto Cassandra seja armazenada em outro m√°quina host. <br></li></ul><br><h2>  Como o armazenamento HyperStore funciona </h2><br>  O posicionamento e a replica√ß√£o dos objetos S3 em um cluster HyperStore s√£o baseados em um esquema de armazenamento em cache consistente que usa espa√ßo de token inteiro no intervalo de 0 a 2 <sup>127</sup> -1.  Os tokens inteiros s√£o atribu√≠dos aos n√≥s do HyperStore.  Para cada objeto S3, um hash √© calculado √† medida que √© carregado no armazenamento.  O objeto √© armazenado no n√≥ ao qual foi atribu√≠do o valor mais baixo do token, maior ou igual ao valor do hash do objeto.  A replica√ß√£o tamb√©m √© implementada armazenando o objeto nos n√≥s aos quais os tokens foram atribu√≠dos, que possuem um valor m√≠nimo. <br><br>  Em um armazenamento baseado em hash consistente "cl√°ssico", um token √© atribu√≠do a um n√≥ f√≠sico.  O sistema Cloudian HyperStore usa e estende a funcionalidade do ‚Äún√≥ virtual‚Äù (vNode) introduzido no Cassandra na vers√£o 1.2 - um grande n√∫mero de tokens √© designado a cada host f√≠sico (m√°ximo de 256).  De fato, o cluster de armazenamento consiste em um n√∫mero muito grande de "n√≥s virtuais" com um grande n√∫mero de n√≥s virtuais (tokens) em cada host f√≠sico. <br><br>  O sistema HyperStore atribui um conjunto separado de tokens (n√≥s virtuais) a cada disco em cada host f√≠sico.  Cada disco no host √© respons√°vel por seu pr√≥prio conjunto de r√©plicas de objetos.  Uma falha no disco afeta apenas r√©plicas de objetos que est√£o nele.  Outras unidades no host continuar√£o a operar e executar suas responsabilidades de armazenamento de dados. <br><br>  Damos um exemplo e consideramos um cluster de 6 hosts HyperStore, cada um com 4 discos de armazenamento S3.  Suponha que 32 tokens sejam atribu√≠dos a cada host f√≠sico e haja um espa√ßo de token simplificado de 0 a 960, e o valor de 192 tokens neste sistema (6 hosts de 32 tokens cada) seja 0, 5, 10, 15, 20 e assim por diante at√© 955. <br><br>  O diagrama abaixo mostra uma poss√≠vel distribui√ß√£o de tokens por todo o cluster.  32 tokens de cada host s√£o distribu√≠dos igualmente por 4 discos (8 tokens por disco) e os pr√≥prios tokens s√£o distribu√≠dos aleatoriamente pelo cluster. <br><br><img src="https://habrastorage.org/webt/wa/w2/9c/waw29ckv34avc3fdmqvfq4-a40a.jpeg"><br><br>  Agora, suponha que voc√™ configurou o HyperStore para replicar 3X objetos S3.  Vamos concordar que o objeto S3 √© carregado no sistema e o algoritmo de hash aplicado a seu nome nos fornece o valor de 322 hash (neste espa√ßo de hash simplificado).  O diagrama abaixo mostra como tr√™s inst√¢ncias ou r√©plicas de um objeto ser√£o armazenadas em um cluster: <br><br><ul><li>  Com seu valor de hash de nome 322, a primeira r√©plica do objeto √© armazenada no token 325, porque  esse √© o menor valor do token que √© maior ou igual ao valor do hash do objeto.  325 tokens (destacados em vermelho no diagrama) s√£o atribu√≠dos √† hyperstore2: Disk2.  Assim, a primeira r√©plica do objeto √© armazenada l√°. <br></li></ul><br><ul><li>  A segunda r√©plica √© armazenada no disco ao qual est√° atribu√≠do o pr√≥ximo token (330, destacado em laranja), ou seja, na hyperstore4: Disk2. <br></li><li>  A terceira r√©plica √© salva no disco, ao qual √© atribu√≠do o pr√≥ximo token ap√≥s 330 - 335 (amarelo), na hyperstore3: Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/xy/0w/k-/xy0wk-hrqt3lgppbyeohdpllsnq.jpeg"><br><blockquote>  <b>Acrescentarei um coment√°rio:</b> do ponto de vista pr√°tico, essa otimiza√ß√£o (a distribui√ß√£o de tokens n√£o apenas entre n√≥s f√≠sicos, mas tamb√©m entre discos individuais) √© necess√°ria n√£o apenas para garantir a acessibilidade, mas tamb√©m para garantir a distribui√ß√£o uniforme de dados entre os discos.  Nesse caso, a matriz RAID n√£o √© usada, toda a l√≥gica da aloca√ß√£o de dados nos discos √© controlada pelo pr√≥prio HyperStore.  Por um lado, √© conveniente e controlado; se um disco for perdido, tudo ser√° reequilibrado por si pr√≥prio.  Por outro lado, eu pessoalmente confio em mais bons controladores RAID - afinal, sua l√≥gica foi otimizada por muitos anos.  Mas essas s√£o todas as minhas prefer√™ncias pessoais, sobre batentes e problemas reais, que nunca pegamos no HyperStore, se seguirmos as recomenda√ß√µes do fornecedor ao instalar o software em servidores f√≠sicos.  Mas a tentativa de usar virtualiza√ß√£o e discos virtuais em cima da mesma lua no sistema de armazenamento falhou. Ao sobrecarregar o sistema de armazenamento durante o teste de carga, o HyperStore enlouqueceu e dispersou os dados de maneira totalmente desigual, obstruindo alguns discos e n√£o tocando em outros. </blockquote><h2>  Conduzir dispositivo dentro de um cluster </h2><br>  Lembre-se de que cada host possui 32 tokens, e os tokens de cada host s√£o distribu√≠dos igualmente entre seus discos.  Vamos dar uma olhada em hyperstore2: Disk2 (no diagrama abaixo).  Vemos que os tokens 325, 425, 370 e assim por diante s√£o atribu√≠dos a este disco. <br><br>  Como o cluster est√° configurado para replica√ß√£o 3X, o seguinte ser√° armazenado na hyperstore2: Disco2: <br><br>  De acordo com o token de disco 325: <br><ul><li>  As primeiras r√©plicas de objetos com um valor de hash de 320 (exclusivamente) a 325 (inclusive); </li><li>  Segundas r√©plicas de objetos com um valor de hash de 315 (exclusivamente) a 320 (inclusive); </li><li>  Terceiras r√©plicas de objetos com um valor de hash de 310 (exclusivamente) a 315 (inclusive). </li></ul><br>  De acordo com o token de disco 425: <br><ul><li>  As primeiras r√©plicas de objetos com um valor de hash de 420 (exclusivamente) a 425 (inclusive); </li><li>  Segundas r√©plicas de objetos com um valor de hash de 415 (exclusivamente) a 420 (inclusive); </li><li>  Terceiras r√©plicas de objetos com um valor de hash de 410 (exclusivamente) a 415 (inclusive). </li></ul><br>  E assim por diante <br><br>  Conforme observado anteriormente, ao colocar a segunda e a terceira r√©plicas, o HyperStore pode, em alguns casos, transmitir tokens para n√£o armazenar mais de uma c√≥pia do objeto em um n√≥ f√≠sico.  Isso elimina o uso de hyperstore2: disk2 como armazenamento para segunda ou terceira r√©plicas do mesmo objeto. <br><br><img src="https://habrastorage.org/webt/hl/rf/dq/hlrfdqwtqhiuwvdrrplv0mxt8dk.jpeg"><br><br>  Se o disco 2 travar nos discos 1, 3 e 4, os dados continuar√£o sendo armazenados e os objetos no disco 2 ser√£o armazenados no cluster, porque  foram replicados para outros hosts. <br><blockquote>  <b>Coment√°rio:</b> como resultado, a distribui√ß√£o de r√©plicas e / ou fragmentos de objetos no cluster HyperStore √© baseada no design do Cassandra, desenvolvido para as necessidades de armazenamento de arquivos.  ,    ,       ,     ,   ¬´¬ª  .          .                     .  ,      :        ,    ,     . </blockquote><h2>      </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora vamos ver como o HyperStore funciona em v√°rios data centers e regi√µes. No nosso caso, o modo multi-DPC difere do modo multi-regional usando um ou mais espa√ßos de token. No primeiro caso, o espa√ßo do token √© uniforme. No segundo, cada regi√£o ter√° um espa√ßo de token independente com (potencialmente) suas pr√≥prias configura√ß√µes para o n√≠vel de consist√™ncia, capacidade e configura√ß√µes de armazenamento. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para entender como isso funciona, voltemos novamente √† tradu√ß√£o da documenta√ß√£o, se√ß√£o </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúImplanta√ß√µes de v√°rios centros de dados‚Äù.</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Considere a implanta√ß√£o do HyperStore em dois data centers. Chame-os de DC1 e DC2. Cada centro de dados possui 3 n√≥s f√≠sicos. Como nos exemplos anteriores, cada n√≥ f√≠sico possui quatro discos, 32 tokens (vNodes) s√£o atribu√≠dos a cada host e assumimos um espa√ßo de token simplificado de 0 a 960. De acordo com esse cen√°rio, com v√°rios datacenters, o espa√ßo de token √© dividido em 192 tokens - 32 tokens para cada um dos 6 hosts f√≠sicos. Os tokens s√£o distribu√≠dos pelos hosts absolutamente aleatoriamente. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suponha tamb√©m que a replica√ß√£o de objetos S3 nesse caso esteja configurada em duas r√©plicas em cada data center. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vejamos como um objeto hipot√©tico S3 com um valor de hash 942 ser√° replicado em 2 data centers:</font></font><br><br><ul><li>     vNode 945 (     ),    DC2,  hyperstore5:Disk3. <br></li><li>     vNode 950 (  ) DC2,  hyperstore6:Disk4. <br></li><li>  vNode 955   DC2,      ,   vNode . <br></li><li>     vNode 0 () ‚Äî  DC1, hyperstore2:Disk3.  ,       (955)       (0). <br></li><li>  vNode (5)   DC2,      ,   vNode . <br></li><li>       vNode 10 () ‚Äî  DC1, hyperstore3:Disk3. <br></li></ul><br><img src="https://habrastorage.org/webt/uu/vl/ak/uuvlakjhabsho_u_swahrrf8cz4.png"><br><blockquote> <b>:</b>        ,      ,   ,  ,           .     ,      . </blockquote>           Cloudian.   ,      ,            . ,    ,   ,            ,       . <br>       S3   DataLine,         ,          ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt423853/">https://habr.com/ru/post/pt423853/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt423839/index.html">teste sha256 por toothOK para rede neural</a></li>
<li><a href="../pt423843/index.html">Se voc√™ est√° em Kazan ou Novosibirsk e deseja projetar microchips, como em Cupertino</a></li>
<li><a href="../pt423845/index.html">Preservativo corporativo</a></li>
<li><a href="../pt423847/index.html">Reconhecimento de cores e luzes com APDS-9960</a></li>
<li><a href="../pt423851/index.html">Apresentando o novo plugin do Grafana - painel Statusmap</a></li>
<li><a href="../pt423855/index.html">Nebulosa Zyxel - facilidade de gerenciamento como base para economia</a></li>
<li><a href="../pt423857/index.html">6 desafios que voc√™ encontrar√° ao aprender a programar sozinho</a></li>
<li><a href="../pt423861/index.html">Lanternas solares - precisamos de mais brilho</a></li>
<li><a href="../pt423863/index.html">Confronto nos PHDays 8 - Vis√£o SOC</a></li>
<li><a href="../pt423865/index.html">Roskomnadzor relatado publicamente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>