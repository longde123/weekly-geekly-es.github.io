<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∞ üëçüèº üëèüèª Vis√£o de m√°quina para o varejo. Como ler etiquetas de pre√ßo em uma loja üßòüèæ üôåüèΩ üè•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Atualmente, a vis√£o de m√°quina √© um assunto muito quente. Para resolver o problema de reconhecer tags de loja usando redes neurais, escolhemos a estru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Vis√£o de m√°quina para o varejo. Como ler etiquetas de pre√ßo em uma loja</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sap/blog/415657/"> Atualmente, a vis√£o de m√°quina √© um assunto muito quente.  Para resolver o problema de reconhecer tags de loja usando redes neurais, escolhemos a estrutura TensorFlow. <br><br>  O artigo discutir√° exatamente como us√°-lo para localizar e identificar v√°rios objetos no mesmo pre√ßo da loja, al√©m de reconhecer seu conte√∫do.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma tarefa semelhante de reconhecimento de pre√ßos da IKEA j√° foi resolvida no Habr√©</a> usando as ferramentas cl√°ssicas de processamento de imagens dispon√≠veis na biblioteca OpenCV. <br><br>  Separadamente, gostaria de observar que a solu√ß√£o pode funcionar tanto na plataforma SAP HANA em conjunto com o Tensorflow Serving, quanto na plataforma em nuvem SAP. <br><br>  A tarefa de reconhecer o pre√ßo dos produtos √© relevante para os compradores que desejam "atrapalhar" os pre√ßos entre si e escolher uma loja para compras e para os varejistas - eles querem aprender sobre os pre√ßos dos concorrentes em tempo real. <br><br>  Letra suficiente - v√° para a t√©cnica! <br><a name="habracut"></a><br>  <b>Toolkit</b> <br><br>  Para a detec√ß√£o e classifica√ß√£o de imagens, usamos redes neurais convolucionais implementadas na biblioteca TensorFlow e dispon√≠veis para controle atrav√©s da API de detec√ß√£o de objetos. <br>  A API de detec√ß√£o de objetos do TensorFlow √© um meta-quadro de c√≥digo aberto baseado no TensorFlow que simplifica a cria√ß√£o, o treinamento e a implanta√ß√£o de modelos para a detec√ß√£o de objetos. <br><br>  Ap√≥s a detec√ß√£o do objeto desejado, o reconhecimento de texto foi realizado usando o Tesseract, uma biblioteca para reconhecimento de caracteres.  Desde 2006, o Tesseract √© considerado uma das bibliotecas de OCR mais precisas dispon√≠veis em c√≥digo aberto. <br><br>  √â poss√≠vel que voc√™ fa√ßa uma pergunta - por que nem todo o processamento √© feito no TF?  A resposta √© muito simples - exigiria muito mais tempo para implementa√ß√£o, mas n√£o havia muito disso.  Era mais f√°cil sacrificar a velocidade de processamento e montar um prot√≥tipo acabado do que se preocupar com um OCR caseiro. <br><br>  <b>Cria√ß√£o e prepara√ß√£o de um conjunto de dados</b> <br><br>  Para come√ßar, era necess√°rio coletar materiais para o trabalho.  Visitamos 3 lojas e tiramos cerca de 400 fotos de diferentes pre√ßos em uma c√¢mera de celular no modo autom√°tico <br><br>  <i>Fotos de exemplo:</i> <br><br><img src="https://habrastorage.org/webt/bp/re/ql/bpreqlti9nccwxxkuu-ubgg5e7s.png"><br>  <i>Fig.</i>  <i>1. Exemplo de uma imagem de etiqueta de pre√ßo</i> <br><br><img src="https://habrastorage.org/webt/4u/cn/9f/4ucn9fztbym8gbhpwum28rgmigg.png"><br>  <i>Fig.</i>  <i>2. Exemplo de imagem de etiqueta de pre√ßo</i> <br><br>  Depois disso, voc√™ precisa processar e marcar todas as fotos dos pre√ßos.  No processo de coleta de imagens, tentamos coletar imagens de alta qualidade (sem artefatos): pre√ßos de aproximadamente o mesmo formato, sem desfocagem, rota√ß√µes significativas etc.  Isso foi feito para facilitar a compara√ß√£o do conte√∫do do pre√ßo real e de sua imagem digital.  No entanto, se treinarmos a rede neural apenas nas imagens de alta qualidade dispon√≠veis, isso naturalmente levar√° ao fato de que a confian√ßa do modelo na identifica√ß√£o de exemplos distorcidos cair√° significativamente.  Para treinar a rede neural a ser resistente a tais situa√ß√µes, utilizamos o procedimento conhecido para expandir o conjunto de treinamento com vers√µes distorcidas de imagens (aumento).  Para complementar a amostra de treinamento, aplicamos algoritmos da biblioteca Imgaug: turnos, pequenas curvas, desfoque gaussiano, ru√≠do.  Imagens distorcidas foram adicionadas √† amostra, o que aumentou em cerca de 5 vezes (de 300 para 1.500 imagens). <br><br>  Para marcar a imagem e selecionar objetos, foi utilizado o programa LabelImg, dispon√≠vel em dom√≠nio p√∫blico.  Permite selecionar os objetos necess√°rios na imagem com um ret√¢ngulo e atribuir cada classe √† caixa delimitadora.  Todas as coordenadas e etiquetas dos quadros criados para cada foto s√£o salvas em um arquivo XML separado. <br><br>  Os seguintes objetos se destacaram em cada foto: pre√ßo do produto, pre√ßo do produto, nome do produto e c√≥digo de barras do produto no pre√ßo.  Em alguns exemplos de imagens, onde era justificado logicamente, as √°reas foram marcadas com sobreposi√ß√£o. <br><br><img src="https://habrastorage.org/webt/bp/ib/k-/bpibk-12lzq1m0jpaf4pxs0omak.png"><br>  <i>Fig.</i>  <i>3. Um exemplo de uma fotografia de um par de etiquetas de pre√ßo marcadas em LabelImg.</i>  <i>√Åreas com descri√ß√£o do produto, pre√ßo e c√≥digo de barras s√£o destacadas.</i> <br><br><img src="https://habrastorage.org/webt/yl/nx/sl/ylnxslebkaqu778rxyomixqdvby.png"><br>  <i>Fig.</i>  <i>4. Um exemplo de uma fotografia de um pre√ßo marcado em LabelImg.</i>  <i>√Åreas com descri√ß√£o do produto, pre√ßo e c√≥digo de barras s√£o destacadas.</i> <br><br>  Depois que todas as fotos forem processadas e marcadas, preparamos o conjunto de dados com a separa√ß√£o de todas as fotos e os arquivos de tags em uma amostra de treinamento e teste.  Geralmente, leve 80% da amostra de treinamento para 20% da amostra de teste e misture aleatoriamente. <br><br>  Em seguida, na m√°quina onde o modelo ser√° treinado, √© necess√°rio instalar todas as bibliotecas necess√°rias.  Primeiro, instalamos a biblioteca de aprendizado de m√°quina TensorFlow.  Dependendo do tipo do seu sistema e voc√™ precisa instalar uma biblioteca adicional para computa√ß√£o na GPU.  Em seguida, instale a biblioteca da API de detec√ß√£o de objetos do Tensorflow e bibliotecas adicionais para trabalhar com imagens e gr√°ficos.  Abaixo est√° uma lista de bibliotecas que usamos em nosso trabalho: <br><br>  <i>GPU TensorFlow v1.5, CUDA v9.0, cuDNN v7.0</i> <i><br></i>  <i>Protobuf 3+, Python-tk, Pillow 1.0, lxml, tf Slim, notebook Jupyter, Matplotlib</i> <i><br></i>  <i>Fluxo de tensor, Cython, Cocoapi;</i>  <i>Opencv-python;</i>  <i>Pandas</i> <br><br>  Quando todas as etapas da instala√ß√£o estiverem conclu√≠das, voc√™ poder√° prosseguir com a prepara√ß√£o dos dados e a configura√ß√£o dos par√¢metros de aprendizado. <br><br>  <b>Modelo de treinamento</b> <br><br>  Para resolver nosso problema, usamos duas vers√µes da rede neural pr√©-treinada MobileNet V2 e Faster-RCNN V2 no conjunto de dados coco como extratores de propriedades de imagem.  Os modelos foram treinados em 4 novas classes: pre√ßo, descri√ß√£o do produto, pre√ßo e c√≥digo de barras.  Como principal, escolhemos o MobileNet V2, que √© um modelo relativamente simples que nos permite oferecer qualidade aceit√°vel a uma velocidade agrad√°vel.  O MobileNet V2 permite implementar o reconhecimento de imagens mesmo em um dispositivo m√≥vel. <br><br>  Primeiro, voc√™ precisa informar √† biblioteca da API de detec√ß√£o de objeto do Tensorflow o n√∫mero de r√≥tulos e os nomes desses r√≥tulos. <br><br>  A √∫ltima coisa a fazer antes do treinamento √© criar um mapa de atalho e editar o arquivo de configura√ß√£o.  O mapa de etiquetas informa o modelo e mapeia os nomes das classes para os n√∫meros de identifica√ß√£o de classe para cada objeto. <br><br><img src="https://habrastorage.org/webt/xp/4x/xu/xp4xxucl6kfbkukwqwkmqrahwbk.png"><br><br>  Por fim, voc√™ precisa configurar as fontes de aprendizado para a Detec√ß√£o de Objetos para determinar qual modelo e quais par√¢metros ser√£o usados ‚Äã‚Äãno treinamento.  Este √© o √∫ltimo passo antes de iniciar o treinamento. <br><br><img src="https://habrastorage.org/webt/ci/of/zi/ciofzi9v6s53lb-tusznaisw5ss.png"><br><br>  O procedimento de treinamento √© iniciado pelo comando: <br><br><pre><code class="hljs powershell">python train.py -<span class="hljs-literal"><span class="hljs-literal">-logtostderr</span></span> -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/mobilenet.config</code> </pre> <br>  Se tudo estiver configurado corretamente, o TensorFlow inicializar√° a reciclagem da rede neural.  A inicializa√ß√£o pode levar at√© 30 segundos antes do in√≠cio do treinamento.  √Ä medida que a rede neural √© treinada novamente a cada etapa, o valor da fun√ß√£o de erro do algoritmo (perda) ser√° exibido.  Para o MobileNet V2, o valor inicial da fun√ß√£o de perda √© de aproximadamente 20. O modelo deve ser treinado at√© que a fun√ß√£o de perda diminua para um valor de aproximadamente 2. Para visualizar o processo de aprendizado da rede neural, voc√™ pode usar o conveniente utilit√°rio TensorBoard. <br><br><pre> <code class="hljs pgsql">: tensorboard <span class="hljs-comment"><span class="hljs-comment">--logdir=training</span></span></code> </pre> <br>  O comando inicializa a interface da web na m√°quina local, que estar√° dispon√≠vel no localhost: 6006.  Ap√≥s a parada, o procedimento de treinamento pode ser retomado posteriormente, usando pontos de verifica√ß√£o salvos a cada 5 minutos. <br><br>  <b>Reconhecimento de etiquetas de pre√ßo e seus elementos</b> <br><br>  Quando o treinamento √© conclu√≠do, o √∫ltimo passo √© criar um gr√°fico de rede neural.  Isso √© feito pelo comando do console, onde, sob asteriscos, voc√™ deve especificar o maior n√∫mero de arquivos cpkt existentes no diret√≥rio de treinamento. <br><br><pre> <code class="hljs powershell">python export_inference_graph.py -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span> image_tensor -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span> training/faster_rcnn_inception_v2.config -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span> training/model.ckpt-**** -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span> inference_graph</code> </pre> <br>  Ap√≥s este procedimento, o classificador de detec√ß√£o de objetos est√° pronto para opera√ß√£o.  Para verificar o reconhecimento de imagem, basta executar um script que acompanha a biblioteca Tensorflow Object Detection, indicando o modelo que foi treinado anteriormente e fotos para reconhecimento.  Um exemplo padr√£o de script Python √© fornecido <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br>  Em nosso exemplo, leva cerca de 1,5 segundos para reconhecer uma foto usando o modelo ssd mobilenet em um laptop simples. <br><br><img src="https://habrastorage.org/webt/nd/uw/ev/nduwevuhylmr-xtfs9wcr6nmvwa.png"><br>  <i>Fig.</i>  <i>5. O resultado do reconhecimento de imagens com etiquetas de pre√ßo na amostra de teste</i> <br><br><img src="https://habrastorage.org/webt/or/-4/w-/or-4w-dq93_tjn2oj4y4f0icpag.png"><br>  <i>Fig.</i>  <i>6. O resultado do reconhecimento de imagens com etiquetas de pre√ßo na amostra de teste</i> <br><br>  Quando estamos convencidos de que os pre√ßos s√£o detectados normalmente, √© necess√°rio ensinar o modelo a ler informa√ß√µes de elementos individuais: o pre√ßo dos produtos, o nome dos produtos e um c√≥digo de barras.  Para isso, existem bibliotecas dispon√≠veis no Python para reconhecer caracteres e c√≥digos de barras nas fotografias - Pyzbar e Tesseract. <br><br>  Antes de come√ßar a reconhecer caracteres e c√≥digos de barras em uma foto, √© necess√°rio recort√°-la nos elementos de que precisamos - para aumentar a velocidade e n√£o reconhecer informa√ß√µes desnecess√°rias que n√£o est√£o inclu√≠das no pre√ßo.  Tamb√©m √© necess√°rio "retirar" as coordenadas dos objetos que o modelo reconheceu junto com suas classes. <br><br><img src="https://habrastorage.org/webt/hc/dk/g6/hcdkg6ngmc1l8no5paqiwm9ewga.png"><br><br>  Em seguida, usamos essas coordenadas para cortar nossa foto em partes para reconhecer apenas a √°rea necess√°ria. <br><br><img src="https://habrastorage.org/webt/tq/2c/q7/tq2cq7ri8ecarbn2i0qmgssexug.png"><br><img src="https://habrastorage.org/webt/hg/vi/ss/hgvissbwiljgcity6q51rt0wgg8.png"><br><img src="https://habrastorage.org/webt/db/gb/lt/dbgbltacl6uwrhswvqlw196h7rm.png"><br>  <i>Fig.</i>  <i>7. Um exemplo de partes destacadas do pre√ßo</i> <br><br>  Em seguida, transferimos todas as √°reas de recorte para as bibliotecas: o nome e o pre√ßo do produto s√£o transferidos para o tesseract e o c√≥digo de barras para o pyzbar, e obtemos o resultado do reconhecimento. <br><br><img src="https://habrastorage.org/webt/pn/oo/sc/pnooscbtqlhzdbbrkza9wzim_0a.png"><br><img src="https://habrastorage.org/webt/ib/gk/wb/ibgkwblzbbfmxzwie-5p4zr-4-0.png"><br>  <i>Fig.</i>  <i>8. Um exemplo de conte√∫do reconhecido √© a √°rea do pre√ßo.</i> <br><br>  Nesse momento, o reconhecimento de texto e c√≥digo de barras pode causar problemas se a imagem original estiver em baixa resolu√ß√£o ou emba√ßada.  Se o pre√ßo puder ser reconhecido normalmente devido aos grandes n√∫meros na etiqueta de pre√ßo, o nome e o c√≥digo de barras do produto ser√£o mal definidos ou nem um pouco definidos.  Para fazer isso, √© recomend√°vel n√£o usar fotos pequenas para reconhecimento e tamb√©m enviar imagens sem ru√≠do e distor√ß√£o forte - por exemplo, sem a falta de foco adequado. <br><br>  Exemplo de reconhecimento de imagem incorreta: <br><br><img src="https://habrastorage.org/webt/gs/gr/qm/gsgrqms2iy2x3j0ipckqhxvd3x4.png"><br><img src="https://habrastorage.org/webt/kc/od/bn/kcodbnaech2u4gr_2qp4j1qorlk.png"><br><img src="https://habrastorage.org/webt/sg/tv/ve/sgtvvedm0i1ssvdhuzmypadxb-0.png"><br><img src="https://habrastorage.org/webt/d7/cf/kj/d7cfkjdswbwqelwrbs1hcd_3yrc.png"><br>  <i>Fig.</i>  <i>9. Um exemplo de partes destacadas de um pre√ßo borrado e conte√∫do reconhecido</i> <br><br>  Neste exemplo, voc√™ pode ver que, se o pre√ßo dos produtos foi reconhecido mais ou menos corretamente na imagem de baixa qualidade, a biblioteca n√£o conseguiu lidar com o nome dos produtos.  E o c√≥digo de barras n√£o est√° sujeito a reconhecimento. <br><br>  O mesmo texto em boa qualidade. <br><br><img src="https://habrastorage.org/webt/yr/q0/kv/yrq0kvrbtwcyj7sbg_jknpwfgyg.png"><br><img src="https://habrastorage.org/webt/wh/cj/2m/whcj2mkjm-lafollpostyctv_lg.png"><br>  <i>Fig.</i>  <i>10. Exemplo de partes destacadas do pre√ßo e conte√∫do reconhecido</i> <br><br>  <b>Conclus√µes</b> <br><br>  No final, conseguimos obter um modelo de qualidade aceit√°vel com uma baixa porcentagem de erros e uma alta porcentagem de detec√ß√£o de objetos relevantes.  O RC2N mais r√°pido V2 tem melhor qualidade de reconhecimento que o MobileNet SSD V2, mas tem uma ordem de magnitude inferior √† velocidade, o que √© uma limita√ß√£o significativa. <br><br>  A precis√£o obtida do reconhecimento de pre√ßo em uma amostra atrasada de 50 imagens √© de 100%, ou seja, todos os pre√ßos foram identificados com sucesso em todas as fotos.  A precis√£o do reconhecimento de √°reas com c√≥digo de barras e pre√ßo foi de 90%.  A precis√£o do reconhecimento da √°rea de texto √© de 85%.  A precis√£o da leitura dos pre√ßos foi de cerca de 95%, e o texto - 80-85%.  Al√©m disso, como um experimento, apresentamos o resultado do reconhecimento de pre√ßo, que √© completamente diferente dos pre√ßos na amostra de treinamento. <br><br><img src="https://habrastorage.org/webt/_c/8d/ez/_c8dezdcd7wqlpkjycogphibwau.png"><br>  <i>Fig.</i>  <i>11. Um exemplo de reconhecimento de pre√ßos at√≠picos que n√£o est√£o no conjunto de treinamento.</i> <br><br>  Como voc√™ pode ver, mesmo com pre√ßos significativamente diferentes dos pre√ßos de treinamento, os modelos n√£o apresentam erros, mas objetos significativos podem ser reconhecidos no pre√ßo. <br><br>  <b>O que mais poderia ser feito?</b> <br><br>  1) Um artigo interessante sobre o aumento autom√°tico foi lan√ßado recentemente, cuja abordagem pode ser usada <br>  2) O modelo treinado acabado pode e deve ser substancialmente comprimido <br>  3) Exemplos de publica√ß√£o de servi√ßos conclu√≠dos no SCP e TFS <br><br>  <i>Na prepara√ß√£o do prot√≥tipo e deste artigo, foram utilizados os seguintes materiais:</i> <br><br>  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trazendo o Machine Learning (TensorFlow) para a empresa com o SAP HANA</a> <br>  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SAP Leonardo ML Foundation - Traga seu pr√≥prio modelo (BYOM)</a> <br>  3. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Reposit√≥rio GitHub de Detec√ß√£o de Objeto TensorFlow</a> <br>  4. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Artigo IKEA Cheque Reconhecimento</a> <br>  5. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Artigo sobre os benef√≠cios da MobileNet</a> <br>  6. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Artigo de Detec√ß√£o de Objeto TensorFlow</a> <br><br>  <i>O artigo foi preparado por:</i> <i><br></i>  <i>Sergey Abdurakipov, Dmitry Buslov, Alexey Khristenko</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt415657/">https://habr.com/ru/post/pt415657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt415645/index.html">Um olhar sobre o Highload ++ da Sib√©ria</a></li>
<li><a href="../pt415647/index.html">A partir de 1¬∫ de julho, os servi√ßos de Internet s√£o necess√°rios para armazenar mensagens de usu√°rios russos por 6 meses</a></li>
<li><a href="../pt415649/index.html">5G vs Wi-Fi: Expectativa e Realidade</a></li>
<li><a href="../pt415651/index.html">Mol√©culas org√¢nicas complexas descobertas no sat√©lite de Saturno</a></li>
<li><a href="../pt415655/index.html">Iniciante ou experiente? Como contratar um desenvolvedor m√≥vel para iOS que realmente saiba como</a></li>
<li><a href="../pt415659/index.html">A pr√°tica de trabalhar com threads no Node.js. 10.5.0</a></li>
<li><a href="../pt415661/index.html">Os relat√≥rios de desempenho dos funcion√°rios s√£o uma perda de tempo</a></li>
<li><a href="../pt415663/index.html">P√°ginas da hist√≥ria da Intel. 1101 - o primeiro MOS com um obturador de silicone</a></li>
<li><a href="../pt415665/index.html">Fornecendo trabalho r√°pido no site como parte do pipeline de desenvolvimento</a></li>
<li><a href="../pt415667/index.html">A equa√ß√£o de Drake n√£o funciona - e aqui est√° como corrigi-la</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>