<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🏫 👩🏿‍🚒 🤔 使用Apache Beam SDK和Python创建基于GCP的数据流模板，用于将数据从Pub / Sub传输到BigQuery 👩🏼‍🤝‍👨🏻 👶🏿 🧙🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="目前，我从事流式传输（和转换）数据的任务。 在某些圈子里 
 这样的过程被称为ETL ，即 提取，转换和加载信息。 


 整个过程包括以下Google Cloud Platform服务的参与： 


- 发布/子服务实时数据流 
- 数据流 -用于转换数据的服务（可以 实时和批处理模式下工作） 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用Apache Beam SDK和Python创建基于GCP的数据流模板，用于将数据从Pub / Sub传输到BigQuery</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441892/"><p><img src="https://habrastorage.org/webt/is/qf/7j/isqf7j7patfl6lgirlqfrfbz-k8.png" alt="图片"></p><br><p> 目前，我从事流式传输（和转换）数据的任务。 在某些圈子里 <br> 这样的过程被称为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ETL</a> ，即 提取，转换和加载信息。 </p><br><p> 整个过程包括以下<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Google Cloud Platform</a>服务的参与： </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">发布/子</a>服务实时数据流 </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">数据流</a> -用于转换数据的服务（可以 <br> 实时和批处理模式下工作） </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">BigQuery-</a>一种以表格形式存储数据的服务 <br>  （支持SQL） </li></ul><a name="habracut"></a><br><h5 id="0-tekuschee-polozhenie-del">  0.当前状态 </h5><br><p> 目前，上述服务有一个有效的流式传输版本，但是在 <br> 作为模板， <a href="">使用标准</a>模板之一。 </p><br><p> 问题在于该模板提供了1对1的数据传输，即 在 <br> 在Pub / Sub的入口处，我们有一个JSON格式的字符串，在输出处，我们有一个包含字段的BigQuery表， <br> 对应于输入JSON顶层对象的键。 </p><br><h5 id="1-postanovka-zadachi">  1.问题陈述 </h5><br><p>创建一个数据流模板，该模板可让您在输出中获取一个或多个表 <br> 根据给定的条件。 例如，我们要为每个表创建一个单独的表 <br> 特定输入JSON密钥的值。 有必要考虑以下事实： <br> 输入JSON对象可以包含嵌套JSON作为值，即 是必要的 <br> 能够使用<code>RECORD</code>类型的字段创建BigQuery表以存储<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">嵌套</a> <br> 数据。 </p><br><h5 id="2-podgotovka-k-resheniyu">  2.决定的准备 </h5><br><p> 要创建数据流模板，请使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Apache Beam</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">SDK</a> ，依次使用 <br> 支持Java和Python作为编程语言。 我必须说 <br> 仅支持Python 2.7.x版本，这让我有些惊讶。 而且，支持 <br>  Java稍宽一些，因为 例如，对于Python，某些功能不可用，更多 <br> 少量的内置<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">连接器</a> 。 顺便说一句，您可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">编写</a>自己的连接器。 </p><br><p> 但是，由于我不熟悉Java，因此我使用了Python。 </p><br><p> 在开始创建模板之前，必须具有以下条件： </p><br><ol><li> 输入JSON格式，它不应及时更改 </li><li>  BigQuery表的架构或将要流式传输数据的架构 </li><li> 输出数据流将流入的表数 </li></ol><br><p> 请注意，在创建模板并基于该模板启动Dataflow Job之后，可以将这些参数设置为 <br> 仅通过创建新模板进行更改。 </p><br><p> 让我们谈谈这些限制。 它们全都来自这样一个事实，即不可能 <br> 创建一个可以将任何字符串作为输入的动态模板，对其进行解析 <br> 根据内部逻辑，然后用动态填充动态创建的表 <br> 由电路创建。 这种可能性很可能存在，但是在数据中 <br> 我没有成功实施这样的计划。 据我了解的整体 <br> 该管道是在运行时执行之前构建的，因此无法将其更改为 <br> 飞。 也许有人会分享他们的决定。 </p><br><h5 id="3-reshenie">  3.决定 </h5><br><p> 为了更全面地了解该过程，有必要提供所谓的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">管道图</a> <br> 从Apache Beam文档中获得。 </p><br><p><img src="https://habrastorage.org/webt/yq/yi/3z/yqyi3zjiwpmjf4i6qp7x4znqv2c.png" alt="图片"></p><br><p> 在我们的例子中（我们将使用划分成几个表）： </p><br><ul><li> 输入-数据来自Dataflow Job中的PubSub </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">转换</a> ＃1-将数据从字符串转换为Python字典，我们得到输出 <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">PCollection</a> ＃1 </li><li> 转换＃2-标记数据，以根据单独的表进一步分离为 <br> 输出是PCollection＃2（实际上是PCollection元组） </li><li> 转换＃3-使用方案将PCollection＃2中的数据写入表 <br> 桌子 </li></ul><br><p> 在编写自己的模板的过程中， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">这些</a>示例为我带来了积极的启发。 </p><br><div class="spoiler">  <b class="spoiler_title">带有注释的模板代码（左注释与以前的作者相同）：</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding=utf-8 from __future__ import absolute_import import logging import json import os import apache_beam as beam from apache_beam.pvalue import TaggedOutput from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions from apache_beam.options.pipeline_options import StandardOptions from apache_beam.io.gcp.bigquery import parse_table_schema_from_json #  GCP  gcp_project = '' #  Pub/Sub  topic_name = '' # Pub/Sub    'projects/_GCP_/topics/_' input_topic = 'projects/%s/topics/%s' % (gcp_project, topic_name) #  BigQuery  bq_dataset = 'segment_eu_test' #       schema_dir = './' class TransformToBigQuery(beam.DoFn): #          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #   yield body class TagDataWithReqType(beam.DoFn): #      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element def run(): #       _.json   schema_dir,  #         ()  schema_dct = {} for schema_file in os.listdir(schema_dir): filename_list = schema_file.split('.') if filename_list[-1] == 'json': with open('%s/%s' % (schema_dir, schema_file)) as f: schema_json = f.read() schema_dct[filename_list[0]] = json.dumps({'fields': json.loads(schema_json)}) # We use the save_main_session option because one or more DoFn's in this # workflow rely on global context (eg, a module imported at module level). pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = True pipeline_options.view_as(StandardOptions).streaming = True # Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic) # Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery()) # Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()). with_outputs(*schema_dct.keys(), main='default') # Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), ) # Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), ) result = p.run() result.wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) logger = logging.getLogger(__name__) run()</span></span></code> </pre> </div></div><br><p> 现在，我们将遍历代码并给出解释，但是首先值得一提的是 <br> 编写此模板的困难在于根据“数据流”进行思考，并且 <br> 不是特定的消息。 还需要了解，Pub / Sub通过消息和 <br> 正是从他们那里，我们将收到标记流的信息。 </p><br><pre> <code class="python hljs">pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> pipeline_options.view_as(StandardOptions).streaming = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span></code> </pre> <br><p> 因为  Apache Beam Pub / Sub IO连接器仅在必要的流模式下使用 <br> 添加PipelineOptions（）（尽管实际上不使用选项）；否则，创建一个模板 <br> 属于例外。 必须提到有关启动模板的选项。 他们可以是 <br> 静态的，所谓的“运行时”。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">这是</a>有关此主题的文档<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">的</a>链接。 这些选项允许您创建模板而不预先指定参数，但是在从模板启动Dataflow Job时传递它们，但是我仍然无法实现它，可能是由于此连接器不支持<code>RuntimeValueProvider</code> 。 </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic)</span></span></code> </pre> <br><p> 注释清楚显示一切，我们阅读了该主题的主题。 值得补充的是，您可以参加直播 <br> 来自主题和订阅（订阅）。 如果将主题指定为输入，则 <br> 将自动创建对该主题的临时订阅。 语法也很漂亮 <br> 清晰的输入数据流<code>beam.io.ReadFromPubSub(input_topic)</code>发送到我们 <br> 管道<code>p</code> 。 </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery())</span></span></code> </pre> <br><p> 这是Transform＃1发生的地方，我们的输入从python字符串转换为 <br>  python dict，在输出中得到PCollection＃1。  <code>&gt;&gt;</code>出现在语法中。 开 <br> 实际上，引号中的文字是信息流的名称（必须唯一）以及注释， <br> 它将添加到GCP Dataflow Web界面中图形上的块中。 让我们更详细地考虑 <br> 重写的类<code>TransformToBigQuery</code> 。 </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TransformToBigQuery</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #  ,      python dict yield body</span></span></code> </pre> <br><p>  <code>element</code>变量将包含来自PubSub订阅的一条消息。 如从 <br> 代码，在我们的情况下，它应该是有效的JSON。 在教室里一定要 <br> 重新定义了<code>process</code>方法，其中应进行必要的转换 <br> 输入线以获得与电路匹配的输出 <br> 将数据加载到的表。 因为 在这种情况下，我们的流程是 <br> 连续的，对于Apache Beam来说是<code>unbounded</code> ，您必须使用 <br> 对于最终数据流， <code>yield</code>而不是<code>return</code> 。 如果是最终流程，您可以 <br>  （和必要）另外配置<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>windowing</code></a>和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><code>triggers</code></a> </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()).with_outputs(*schema_dct.keys(), main='default')</span></span></code> </pre> <br><p> 此代码将PCollection＃1引导到Transform＃2，在此进行标记 <br>  （分离）数据流。 在变量<code>schema_dct</code>在这种情况下，是一个字典，其中的键是不带扩展名的方案文件的名称，它将是标记，值是方案的有效JSON。 <br> 此标记的BigQuery表。 应该注意的是，该方案应准确地发送给 <br> 查看<code>{'fields': }</code> ，其中<code></code>是JSON形式的BigQuery表的架构（您可以 <br> 从网络界面导出）。 </p><br><p>  <code>main='default'</code>是它们将使用的线程标签的名称 <br> 所有不受标记条件约束的消息。 考虑上课 <br>  <code>TagDataWithReqType</code> 。 </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TagDataWithReqType</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element</span></span></code> </pre> <br><p> 如您所见， <code>process</code>类在这里也被覆盖。  <code>types</code>变量包含名称 <br> 标签，并且它们必须将数字和名称与字典键的数字和名称匹配 <br>  <code>schema_dct</code> 。 尽管<code>process</code>方法具有接受参数的能力，但我从未 <br> 我能够通过他们。 我还没有找到原因。 </p><br><p> 在输出中，我们在标签数量中得到了一个线程元组，即我们的数量 <br> 预定义标签+无法标记的默认线程。 </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), )</span></span></code> </pre> <br><p> 转换＃...（实际上，它不在图中，这是一个“分支”）-我们编写了默认流 <br> 到默认表。 </p><br><p>  <code>tagged_stream.default</code>使用具有<code>default</code>标签的流，替代语法为<code>tagged_stream['default']</code> </p><br><p>  <code>schema=parse_table_schema_from_json(schema_dct.get('default'))</code> -在此定义方案 <br> 表。 请注意，带有有效BigQuery表架构的<code>default.json</code>文件 <br> 必须在当前<code>schema_dir = './'</code>目录中。 </p><br><p> 流将转到名为<code>default</code>的表。 </p><br><p> 如果不存在具有该名称的表（在该项目的给定数据集中），则该表 <br> 默认设置将根据方案自动创建 <br> <code>create_disposition=BigQueryDisposition.CREATE_IF_NEEDED</code> </p> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), )</span></span></code> </pre> <br><p> 转换＃3，对于那些从一开始就读并拥有这篇文章的人来说，一切都应该清楚 <br>  python语法。 我们用循环将流元组分开，并使用以下命令将每个流写入其自己的表中： <br> 他的计划。 应该<code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code>流名称必须是唯一的- <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> 。 </p><br><p> 现在应该清楚它是如何工作的，您可以创建一个模板。 为此，您需要 <br> 在控制台中运行（如果可用，请不要忘记激活venv）或从IDE中运行： </p><br><pre> <code class="bash hljs">python _.py / --runner DataflowRunner / --project dreamdata-test / --staging_location gs://STORAGE_NAME/STAGING_DIR / --temp_location gs://STORAGE_NAME/TEMP_DIR / --template_location gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> </pre> <br><p> 同时，应组织对Google帐户的访问，例如通过导出 <br>  <code>GOOGLE_APPLICATION_CREDENTIALS</code>环境<code>GOOGLE_APPLICATION_CREDENTIALS</code>或其他<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">方式</a> 。 </p><br><p> 关于<code>--runner</code>的几句话。 在这种情况下， <code>DataflowRunner</code>表示该代码 <br> 将作为数据流作业的模板运行。 仍然可以指定 <br>  <code>DirectRunner</code> ，如果没有<code>--runner</code>和code选项，将默认使用它 <br> 将用作数据流作业，但在本地，这对于调试非常方便。 </p><br><p> 如果没有发生错误，则<code>gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code>将是 <br> 创建的模板。 值得一提的是，在<code>gs://STORAGE_NAME/STAGING_DIR</code>也将被写入 <br> 成功运行基于以下条件创建的Datafow作业所需的服务文件： <br> 模板，则无需删除它们。 </p><br><p> 接下来，您需要使用此模板手动或通过任何方式创建一个数据流作业 <br> 以另一种方式（例如CI）。 </p><br><h5 id="4-vyvody">  4.结论 </h5><br><p> 因此，我们设法使用以下方法将流从PubSub流到BigQuery <br> 必要的数据转换，以进一步存储，转换和 <br> 数据使用情况。 </p><br><h2 id="osnovnye-ssylki"> 主要连结 </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Apache Beam SDK</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwjHvcGT5svgAhV7wsQBHSDWDEoQFjAAegQIABAC%26url%3D">Google数据流</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">谷歌bigquery</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">詹姆斯·摩尔（James Moore）在媒体上的文章</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Apache Beam的Python代码示例</a> </li></ul><br><p> 在本文中，可能会出现错误甚至错误，我将非常感谢您的建设性 <br> 批评。 最后，我想补充一点，实际上，这里并没有全部使用 <br>  Apache Beam SDK的功能，但这不是目的。 </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN441892/">https://habr.com/ru/post/zh-CN441892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN441878/index.html">你随便你，但我做到了</a></li>
<li><a href="../zh-CN441882/index.html">最小的VMware NSX。 第3部分。配置DHCP</a></li>
<li><a href="../zh-CN441886/index.html">在过去的12年中，我从未展示过简历</a></li>
<li><a href="../zh-CN441888/index.html">来自扩音器的SIP家用费率</a></li>
<li><a href="../zh-CN441890/index.html">您需要了解的有关iOS App Extensions的所有信息</a></li>
<li><a href="../zh-CN441896/index.html">学习对抗策略，技巧和常识（ATT @ CK）。 企业策略。 第9部分</a></li>
<li><a href="../zh-CN441898/index.html">Sketch + Node.js：为许多平台和品牌生成图标</a></li>
<li><a href="../zh-CN441900/index.html">萨蒂亚·纳德拉（Satya Nadella）谈到了与五角大楼的合作</a></li>
<li><a href="../zh-CN441902/index.html">技术如何创造新的现实</a></li>
<li><a href="../zh-CN441904/index.html">在Thinkpad T430S上安装IPS显示器</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>