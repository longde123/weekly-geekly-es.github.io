<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙆 🤺 👨‍💻 3 cerita kecelakaan Kubernetes dalam produksi: anti-afinitas, shutdown anggun, webhook 🚊 🙁 📛</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Catatan perev. : Kami menyajikan pilihan mini post-mortem tentang masalah fatal yang dihadapi oleh insinyur dari berbagai perusahaan saat mengoperasik...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>3 cerita kecelakaan Kubernetes dalam produksi: anti-afinitas, shutdown anggun, webhook</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/475026/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/y7/_c/ra/y7_cracjhm0ke_mtazml1fhzprk.jpeg"></div><br>  <i><b>Catatan</b></i>  <i><b>perev.</b></i>  <i>: Kami menyajikan pilihan mini post-mortem tentang masalah fatal yang dihadapi oleh insinyur dari berbagai perusahaan saat mengoperasikan infrastruktur berdasarkan Kubernetes.</i>  <i>Setiap catatan berbicara tentang masalah itu sendiri, penyebab dan konsekuensinya, dan, tentu saja, tentang solusi yang membantu menghindari situasi serupa di masa depan.</i> <i><br><br></i>  <i>Seperti yang Anda ketahui, belajar dari pengalaman orang lain lebih murah, dan karenanya - biarkan cerita ini membantu Anda bersiap untuk kemungkinan kejutan.</i>  <i>Ngomong-ngomong, sejumlah besar tautan yang diperbarui secara berkala ke "cerita kegagalan" dipublikasikan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs ini</a> (menurut data dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori Git ini</a> ).</i> <a name="habracut"></a><br><br><h2>  1  Bagaimana panik kernel menabrak sebuah situs </h2><br>  <i>Asli: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Cahaya Bulan</a> .</i> <br><br>  Antara 18 dan 22 Januari, situs web Moonlight dan API mengalami kegagalan fungsi intermiten.  Semuanya dimulai dengan kesalahan API acak dan diakhiri dengan penghentian total.  Masalah diselesaikan dan aplikasi kembali normal. <br><br><h3>  Informasi umum </h3><br>  Moonlight menggunakan perangkat lunak yang dikenal sebagai Kubernetes.  Kubernetes menjalankan aplikasi pada grup server.  Server-server ini disebut node.  Salinan aplikasi yang berjalan pada node disebut pod.  Kubernetes memiliki penjadwal yang secara dinamis menentukan pod mana yang node harus bekerja. <br><br><h3>  Garis waktu </h3><br>  Kesalahan pertama pada hari Jumat terkait dengan masalah koneksi ke database Redis.  Moonlight API menggunakan Redis untuk memverifikasi sesi untuk setiap permintaan yang diautentikasi.  Alat pemantauan kami Kubernetes telah memberi tahu bahwa beberapa node dan pod tidak merespons.  Pada saat yang sama, Google Cloud melaporkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tidak berfungsinya layanan jaringan</a> , dan kami memutuskan bahwa mereka adalah penyebab masalah kami. <br><br>  Ketika lalu lintas pada akhir pekan menurun, kesalahan tampaknya diselesaikan secara massal.  Namun, pada Selasa pagi, situs Moonlight jatuh, dan lalu lintas eksternal tidak mencapai cluster sama sekali.  Kami menemukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">orang lain di Twitter</a> dengan gejala yang sama dan memutuskan bahwa hosting Google mengalami kegagalan jaringan.  Kami menghubungi dukungan Google Cloud, yang segera merujuk masalah ini ke tim dukungan teknis. <br><br>  Tim dukungan teknis Google mengungkapkan beberapa pola dalam perilaku node di kluster Kubernet kami.  Pemanfaatan CPU dari masing-masing node mencapai 100%, setelah itu kepanikan kernel terjadi di mesin virtual dan macet. <br><br><h3>  Alasan </h3><br>  Siklus yang menyebabkan kegagalan adalah sebagai berikut: <br><br><ul><li>  Penjadwal Kubernetes meng-host beberapa pod dengan konsumsi CPU tinggi pada node yang sama. </li><li>  Pod memakan semua sumber daya CPU pada node. </li><li>  Selanjutnya muncul panik kernel, yang menyebabkan periode downtime di mana node tidak menanggapi scheduler. </li><li>  Penjadwal memindahkan semua pod jatuh ke node baru, dan proses itu diulangi, memperburuk situasi umum. </li></ul><br>  Awalnya, kesalahan terjadi di pod Redis, tetapi pada akhirnya semua pod yang bekerja dengan traffic turun, yang menyebabkan shutdown total.  Keterlambatan eksponensial selama perencanaan ulang menyebabkan periode waktu henti yang lebih lama. <br><br><h3>  Solusi </h3><br>  Kami dapat memulihkan situs dengan menambahkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">aturan anti-afinitas</a> ke semua Penyebaran utama.  Mereka secara otomatis mendistribusikan pods melalui node, meningkatkan toleransi kesalahan dan kinerja. <br><br>  Kubernetes sendiri dirancang sebagai sistem host yang toleran terhadap kesalahan.  Moonlight menggunakan tiga node pada server yang berbeda untuk stabilitas, dan kami menjalankan tiga salinan dari setiap aplikasi yang melayani lalu lintas.  Idenya adalah memiliki satu salinan pada setiap simpul.  Dalam hal ini, bahkan kegagalan dua node tidak akan menyebabkan downtime.  Namun, Kubernetes kadang-kadang menempatkan ketiga pod dengan situs pada node yang sama, sehingga menciptakan hambatan dalam sistem.  Pada saat yang sama, aplikasi lain yang menuntut daya prosesor (yaitu, rendering sisi-server) berada pada node yang sama, dan bukan pada yang terpisah. <br><br>  Cluster Kubernetes yang dikonfigurasikan dengan benar dan berfungsi dengan benar diperlukan untuk mengatasi beban CPU yang lama dan menempatkan pod sedemikian rupa untuk memaksimalkan penggunaan sumber daya yang tersedia.  Kami terus bekerja dengan dukungan Google Cloud untuk mengidentifikasi dan mengatasi penyebab utama panik kernel pada server. <br><br><h3>  Kesimpulan </h3><br>  Aturan anti-afinitas memungkinkan Anda membuat aplikasi yang berfungsi dengan lalu lintas eksternal lebih toleran terhadap kesalahan.  Jika Anda memiliki layanan serupa di Kubernetes, pertimbangkan untuk menambahkannya. <br><br>  Kami terus bekerja dengan orang-orang dari Google untuk menemukan dan menghilangkan penyebab kegagalan pada kernel OS pada node. <br><br><h2>  2  Rahasia "kotor" dari titik akhir Kubernetes dan Ingress </h2><br>  <i>Asli: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Phil Pearl of Ravelin</a> .</i> <br><br><h3>  Keanggunan berlebihan </h3><br>  Kami di Ravelin bermigrasi ke Kubernetes (di GKE).  Prosesnya sangat sukses.  Anggaran gangguan pod kami penuh seperti biasa, statefuls benar-benar megah <i>(kata-kata yang sulit diterjemahkan: "set statefuls kami sangat megah" - kira-kira. Terjemahan)</i> , Dan penggantian node yang berjalan seperti jarum jam. <br><br>  Bagian terakhir dari teka-teki adalah memindahkan lapisan API dari mesin virtual lama ke kluster Kubernetes.  Untuk melakukan ini, kita perlu mengonfigurasi Ingress agar API dapat diakses dari dunia luar. <br><br>  Awalnya, tugas itu tampak sederhana.  Kami hanya mendefinisikan pengontrol Ingress, mengubah Terraform untuk mendapatkan sejumlah alamat IP, dan Google menangani hampir semua hal lainnya.  Dan semua ini akan bekerja seperti sulap.  Kelas! <br><br>  Namun, seiring waktu, mereka mulai memperhatikan bahwa tes integrasi secara berkala menerima kesalahan 502. Dari sini, perjalanan kami dimulai.  Namun, saya akan menghemat waktu Anda dan langsung menuju kesimpulan. <br><br><h3>  Shutdown yang anggun </h3><br>  Semua orang berbicara tentang shutdown anggun ("anggun", shutdown bertahap).  Tapi Anda benar-benar tidak harus bergantung padanya di Kubernetes.  Atau setidaknya itu bukan shutdown anggun yang Anda <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://golang.org/pkg/net/">serap dengan susu ibu Anda</a> .  Di dunia Kubernetes, tingkat "keanggunan" ini tidak perlu dan mengancam dengan masalah serius. <br><br><h3>  Dunia yang sempurna </h3><br>  Berikut ini cara tampilan mayoritas pod dihapus dari layanan atau penyeimbang beban di Kubernetes: <br><br><ol><li>  Pengontrol replikasi memutuskan untuk menghapus pod. </li><li>  Pod titik akhir dihapus dari penyeimbang layanan atau beban.  Lalu lintas baru ke pod tidak lagi tiba. </li><li>  Pengait pre-stop disebut, atau pod menerima sinyal SIGTERM. </li><li>  Pod "anggun" terputus.  Itu berhenti menerima koneksi masuk. </li><li>  Putus "anggun" selesai, dan pod hancur setelah semua koneksi yang ada dihentikan atau dihentikan. </li></ol><br>  Sayangnya, kenyataannya sangat berbeda. <br><br><h3>  Dunia nyata </h3><br>  Sebagian besar dokumentasi mengisyaratkan bahwa semuanya terjadi sedikit berbeda, tetapi mereka tidak secara eksplisit menulis tentang ini di mana pun.  Masalah utama adalah bahwa langkah 3 tidak mengikuti langkah 2. Mereka terjadi secara bersamaan.  Dalam layanan biasa, penghapusan titik akhir sangat cepat sehingga kemungkinan menghadapi masalah sangat rendah.  Namun, dengan Ingresss, semuanya berbeda: mereka biasanya merespons lebih lambat, sehingga masalahnya menjadi jelas.  Pod bisa mendapatkan SIGTERM jauh sebelum perubahan pada titik akhir masuk ke Ingress. <br><br>  Akibatnya, shutdown yang anggun sama sekali tidak seperti yang diminta pod.  Dia akan menerima koneksi baru dan harus terus memprosesnya, jika tidak klien akan mulai menerima kesalahan ke-500 dan seluruh cerita indah tentang penyebaran yang tidak rumit dan penskalaan akan mulai berantakan. <br><br>  Inilah yang sebenarnya terjadi: <br><br><ol><li>  Pengontrol replikasi memutuskan untuk menghapus pod. </li><li>  Pod titik akhir dihapus dari penyeimbang layanan atau beban.  Dalam kasus Ingress, ini bisa memakan waktu, dan lalu lintas baru akan terus mengalir ke pod. </li><li>  Pengait pre-stop disebut, atau pod menerima sinyal SIGTERM. </li><li>  Untuk sebagian besar, pod harus mengabaikan ini, terus bekerja dan memelihara koneksi baru.  Jika memungkinkan, ia harus memberi petunjuk kepada pelanggan bahwa alangkah baiknya untuk pindah ke tempat lain.  Misalnya, dalam hal HTTP, mungkin mengirim <code>Connection: close</code> di header respons. </li><li>  Pod keluar hanya ketika periode tunggu "elegan" berakhir dan terbunuh oleh SIGKILL. </li><li>  Pastikan periode ini lebih lama dari waktu yang diperlukan untuk memprogram ulang penyeimbang beban. </li></ol><br>  Jika ini adalah kode pihak ketiga dan Anda tidak dapat mengubah perilakunya, maka hal terbaik yang dapat Anda lakukan adalah menambahkan kait pre-stop yang hanya akan tidur selama periode "elegan", sehingga pod akan terus bekerja seolah-olah tidak ada terjadi <br><br><h2>  Nomor 3  Bagaimana webhook sederhana menyebabkan kegagalan cluster </h2><br>  <i>Asli: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Jetstack</a> .</i> <br><br>  Jetstack menawarkan platform multi-tenant kepada pelanggannya di Kubernetes.  Terkadang ada persyaratan khusus yang tidak dapat kami penuhi dengan konfigurasi standar Kubernetes.  Untuk mengimplementasikannya, baru-baru ini kami mulai menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Open Policy Agent</a> <i>(kami menulis tentang proyek secara lebih rinci dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ulasan ini</a> - kira-kira Terjemahkan.)</i> <br><br>  Artikel ini menjelaskan kegagalan yang disebabkan oleh integrasi ini yang tidak dikonfigurasi dengan benar. <br><br><h3>  Insiden </h3><br>  Kami terlibat dalam memperbarui wizard untuk klaster dev, di mana berbagai tim menguji aplikasi mereka selama hari kerja.  Itu adalah cluster regional di zona eropa-barat1 di Google Kubernetes Engine (GKE). <br><br>  Perintah diperingatkan bahwa pembaruan sedang berlangsung, dengan tidak ada downtime yang diharapkan.  Sebelumnya pada hari itu, kami sudah melakukan pembaruan yang mirip dengan lingkungan pra-produksi lain. <br><br>  Kami memulai peningkatan menggunakan pipa GKE Terraform kami.  Pembaruan panduan tidak selesai sampai batas waktu Terraform berakhir, yang kami atur selama 20 menit.  Ini adalah panggilan bangun pertama yang terjadi kesalahan, meskipun di konsol GKE cluster masih terdaftar sebagai "peningkatan". <br><br>  Mulai ulang pipa menyebabkan kesalahan berikut <br><br><pre> <code class="bash hljs">google_container_cluster.cluster: Error waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> updating GKE master version: All cluster resources were brought up, but the cluster API is reporting that: component <span class="hljs-string"><span class="hljs-string">"kube-apiserver"</span></span> from endpoint <span class="hljs-string"><span class="hljs-string">"gke-..."</span></span> is unhealthy</code> </pre> <br>  Kali ini, koneksi dengan server API mulai terputus secara berkala dan tim tidak dapat menggunakan aplikasi mereka. <br><br>  Sementara kami mencoba memahami apa yang sedang terjadi, semua node mulai dihancurkan dan diciptakan kembali dalam siklus yang tak berujung.  Hal ini menyebabkan penolakan layanan tanpa pandang bulu untuk semua pelanggan kami. <br><br><h3>  Kami menetapkan akar penyebab kegagalan </h3><br>  Dengan dukungan Google, kami dapat menentukan urutan peristiwa yang menyebabkan kegagalan: <br><br><ol><li>  GKE menyelesaikan upgrade pada satu instance wizard dan mulai menerima semua lalu lintas ke server API saat wizard berikutnya diperbarui. </li><li>  Selama pemutakhiran instance kedua wizard, server API tidak dapat menjalankan <a href="">PostStartHook</a> untuk <a href="">mendaftarkan CA.</a> </li><li>  Selama pelaksanaan kait ini, server API mencoba memperbarui ConfigMap yang disebut <code>extension-apiserver-authentication</code> di <code>kube-system</code> .  Itu tidak mungkin untuk melakukan ini karena backend untuk webhook memeriksa Agen Kebijakan Terbuka (OPA) yang kami konfigurasi tidak merespons. </li><li>  Agar wisaya lulus pemeriksaan kesehatan, operasi ini harus selesai dengan sukses.  Karena ini tidak terjadi, master kedua memasuki siklus darurat dan menghentikan pembaruan. </li></ol><br>  Hasilnya adalah crash API berkala, karena kubelet tidak dapat melaporkan kesehatan node.  Pada gilirannya, ini mengarah pada fakta bahwa mekanisme untuk pemulihan otomatis node GKE <i>(perbaikan otomatis node)</i> mulai me-restart node.  Fitur ini dijelaskan secara rinci dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> : <br><br><blockquote>  <i>Status yang tidak sehat dapat berarti: Dalam waktu tertentu (sekitar 10 menit), simpul tidak memberikan status apa pun sama sekali.</i> </blockquote><br><h3>  Solusi </h3><br>  Ketika kami mengetahui bahwa sumber daya <code>ValidatingAdmissionWebhook</code> menyebabkan akses intermiten ke server API, kami menghapusnya dan mengembalikan cluster agar berfungsi. <br><br>  Sejak itu, <code>ValidatingAdmissionWebhook</code> untuk OPA telah dikonfigurasikan untuk memantau hanya ruang nama di mana kebijakan tersebut berlaku dan yang dapat diakses oleh tim pengembangan.  Kami juga membatasi webhook untuk <code>Ingress</code> dan <code>Service</code> , satu-satunya yang sesuai dengan kebijakan kami. <br><br>  Sejak kami pertama kali menyebarkan OPA, dokumentasi telah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">diperbarui</a> untuk mencerminkan perubahan ini. <br><br>  Kami juga menambahkan tes liveness untuk memastikan bahwa OPA restart jika itu tidak tersedia (dan membuat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">amandemen yang sesuai</a> untuk dokumentasi). <br><br>  Kami juga mempertimbangkan untuk menonaktifkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mekanisme</a> pemulihan otomatis untuk node GKE, tetapi masih memutuskan untuk meninggalkan ide ini. <br><br><h3>  Ringkasan </h3><br>  Jika kami mengaktifkan peringatan waktu respons server API, pada awalnya kami akan dapat melihat peningkatan global untuk semua permintaan <code>CREATE</code> dan <code>UPDATE</code> setelah menggunakan webhook untuk OPA. <br><br>  Ini menggarisbawahi pentingnya menyiapkan tes untuk semua beban kerja.  Melihat ke belakang, kita dapat mengatakan bahwa penyebaran OPA sangat sederhana sehingga kita bahkan tidak terlibat dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">grafik Helm</a> (meskipun seharusnya).  Grafik membuat sejumlah penyesuaian di luar pengaturan dasar yang dijelaskan dalam manual, termasuk pengaturan livenessProbe untuk wadah dengan pengontrol masuk. <br><br>  Kami bukan yang pertama kali menghadapi masalah ini: masalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hulu</a> tetap terbuka.  Fungsionalitas dalam hal ini jelas dapat ditingkatkan (dan kami akan menindaklanjutinya). <br><br><h2>  PS dari penerjemah </h2><br>  Baca juga di blog kami: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagaimana prioritas pod di Kubernetes menyebabkan downtime di Grafana Labs</a> ;" </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dari kehidupan dengan Kubernetes: Bagaimana orang-orang Spanyol tidak mengeluh tentang server HTTP</a> "; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">6 bug sistem yang menghibur dalam pengoperasian Kubernetes [dan solusi mereka]</a> ”; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">6 cerita praktis dari kehidupan sehari-hari SRE kami</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id475026/">https://habr.com/ru/post/id475026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id475016/index.html">QA mitap di Redmadrobot 22 November</a></li>
<li><a href="../id475018/index.html">Perubahan kolom Radiotehnika S-30</a></li>
<li><a href="../id475020/index.html">Bagaimana teknologi modern secara bertahap menggantikan menara api</a></li>
<li><a href="../id475022/index.html">Schizophrenia Arsitektur Facebook Libra</a></li>
<li><a href="../id475024/index.html">Berlari adalah olahraga yang ideal untuk pekerja jarak jauh. Bagian 1: jalan menuju balapan pertama sejauh seratus kilometer</a></li>
<li><a href="../id475028/index.html">Pengamatan tentang penerapan ML dalam bisnis pada saham ŽijemeIT</a></li>
<li><a href="../id475032/index.html">Gartner Hype Cycle 2019: tanya jawab</a></li>
<li><a href="../id475034/index.html">Grafik di browser untuk Arduino dan STM32</a></li>
<li><a href="../id475036/index.html">Migrasi Cassandra ke Kubernetes: fitur dan solusi</a></li>
<li><a href="../id475038/index.html">Set pertama "Matematika Terapan dan Ilmu Komputer" di St. Petersburg HSE: siapa mereka dan bagaimana cara bekerja dengannya?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>