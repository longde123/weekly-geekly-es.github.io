<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äç‚úàÔ∏è üëºüèø üïã Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit kubeadm. Teil 1/3 üè© üëã ü§öüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 2/3 hier 
 Teil 3/3 hier 


 Hallo allerseits! In diesem Artikel m√∂chte ich die Informationen optimieren und die Erfahrungen beim Erstellen und V...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit kubeadm. Teil 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Teil 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> <br>  <strong>Teil 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> </p><br><p>  Hallo allerseits!  In diesem Artikel m√∂chte ich die Informationen optimieren und die Erfahrungen beim Erstellen und Verwenden des internen Kubernetes-Clusters teilen. </p><br><p>  In den letzten Jahren hat diese Container-Orchestrierungstechnologie einen gro√üen Schritt nach vorne gemacht und ist f√ºr Tausende von Unternehmen zu einer Art Unternehmensstandard geworden.  Einige verwenden es in der Produktion, andere testen es nur an Projekten, aber die Leidenschaften, egal wie Sie es sagen, leuchten ernst.  Wenn Sie es noch nie benutzt haben, ist es Zeit, mit dem Dating zu beginnen. </p><br><h3 id="0-vstuplenie">  0. Einleitung </h3><br><p>  Kubernetes ist eine skalierbare Orchestrierungstechnologie, die mit der Installation auf einem einzelnen Knoten beginnen und die Gr√∂√üe gro√üer HA-Cluster erreichen kann, die auf mehreren hundert Knoten im Inneren basieren.  Die meisten g√§ngigen Cloud-Anbieter bieten verschiedene Arten von Kubernetes-Implementierungen an - nehmen und verwenden.  Die Situationen sind jedoch anders, und es gibt Unternehmen, die die Clouds nicht nutzen, und sie m√∂chten alle Vorteile moderner Orchestrierungstechnologien nutzen.  Und hier kommt die Installation von Kubernetes auf Bare Metal. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Einleitung </h3><br><p> In diesem Beispiel erstellen wir einen Kubernetes HA-Cluster mit der Topologie f√ºr mehrere Master, mit einem externen Cluster usw. als Basisschicht und einem MetalLB-Load-Balancer im Inneren.  Auf allen Arbeitsknoten werden wir GlusterFS als einfachen internen verteilten Clusterspeicher bereitstellen.  Wir werden auch versuchen, mehrere Testprojekte mithilfe unserer pers√∂nlichen Docker-Registrierung darin bereitzustellen. </p><br><p>  Im Allgemeinen gibt es mehrere M√∂glichkeiten, einen Kubernetes-HA-Cluster zu erstellen: den schwierigen und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">detaillierten</a> Pfad, der im beliebten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokument kubernetes-the-hard-way beschrieben</a> ist, oder den einfacheren Weg mit dem Dienstprogramm <strong>kubeadm</strong> . </p><br><p>  Kubeadm ist ein Tool, das von der Kubernetes-Community speziell entwickelt wurde, um die Installation von Kubernetes zu vereinfachen und den Prozess zu vereinfachen.  Zuvor wurde Kubeadm nur zum Erstellen kleiner Testcluster mit einem Masterknoten empfohlen, um loszulegen.  Im letzten Jahr wurde jedoch viel verbessert, und jetzt k√∂nnen wir damit HA-Cluster mit mehreren Masterknoten erstellen.  Laut Community-News von Kubernetes wird Kubeadm in Zukunft als Tool f√ºr die Installation von Kubernetes empfohlen. </p><br><p>  Die Kubeadm-Dokumentation bietet zwei grundlegende M√∂glichkeiten zum Implementieren eines Clusters mit Stack- und externen etcd-Topologien.  Ich werde den zweiten Pfad mit externen etcd-Knoten aufgrund der Fehlertoleranz des HA-Clusters w√§hlen. </p><br><p>  Hier ist ein Diagramm aus der Kubeadm-Dokumentation, das diesen Pfad beschreibt: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Ich werde es ein wenig √§ndern.  Zun√§chst werde ich ein Paar HAProxy-Server als Load Balancer f√ºr das Heartbeat-Paket verwenden, das die virtuelle IP-Adresse gemeinsam nutzt.  Heartbeat und HAProxy verwenden eine kleine Menge an Systemressourcen, daher werde ich sie auf einem Paar von etcd-Knoten platzieren, um die Anzahl der Server f√ºr unseren Cluster geringf√ºgig zu reduzieren. </p><br><p>  F√ºr dieses Kubernetes-Clusterschema sind acht Knoten erforderlich.  Drei Server f√ºr einen externen Cluster usw. (LB-Dienste verwenden auch einige davon), zwei f√ºr Knoten der Steuerebene (Hauptknoten) und drei f√ºr Arbeitsknoten.  Es kann sich entweder um Bare-Metal- oder einen VM-Server handeln.  In diesem Fall spielt es keine Rolle.  Sie k√∂nnen das Schema einfach √§ndern, indem Sie weitere Masterknoten hinzuf√ºgen und HAProxy mit Heartbeat auf separaten Knoten platzieren, wenn viele freie Server vorhanden sind.  Obwohl meine Option f√ºr die erste Implementierung des HA-Clusters f√ºr die Augen ausreicht. </p><br><p>  Wenn Sie m√∂chten, f√ºgen Sie einen kleinen Server mit dem <strong>installierten</strong> Dienstprogramm <strong>kubectl hinzu</strong> , um diesen Cluster zu verwalten, oder verwenden Sie dazu Ihren eigenen Linux-Desktop. </p><br><p>  Das Diagramm f√ºr dieses Beispiel sieht ungef√§hr so ‚Äã‚Äãaus: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Anforderungen </h3><br><p>  Sie ben√∂tigen zwei Kubernetes-Masterknoten mit den empfohlenen Mindestsystemanforderungen: 2 CPUs und 2 GB RAM gem√§√ü der <strong>kubeadm-</strong> Dokumentation.  F√ºr funktionierende Knoten empfehle ich die Verwendung leistungsf√§higerer Server, da alle unsere Anwendungsdienste auf diesen ausgef√ºhrt werden.  Und f√ºr Etcd + LB k√∂nnen wir auch Server mit zwei CPUs und mindestens 2 GB RAM verwenden. </p><br><p>  W√§hlen Sie ein √∂ffentliches oder privates Netzwerk f√ºr diesen Cluster aus.  IP-Adressen spielen keine Rolle;  Es ist wichtig, dass alle Server f√ºr einander und nat√ºrlich f√ºr Sie zug√§nglich sind.  Sp√§ter werden wir im Kubernetes-Cluster ein Overlay-Netzwerk einrichten. </p><br><p>  Die Mindestanforderungen f√ºr dieses Beispiel sind: </p><br><ul><li>  2 Server mit 2 Prozessoren und 2 GB RAM f√ºr den Masterknoten </li><li>  3 Server mit 4 Prozessoren und 4-8 GB RAM f√ºr Arbeitsknoten </li><li>  3 Server mit 2 Prozessoren und 2 GB RAM f√ºr Etcd und HAProxy </li><li>  192.168.0.0/24 - das Subnetz. </li></ul><br><p>  192.168.0.1 - Virtuelle HAProxy-IP-Adresse, 192.168.0.2 - 4 Haupt-IP-Adressen von Etcd- und HAProxy-Knoten, 192.168.0.5 - 6 Haupt-IP-Adressen des Kubernetes-Masterknotens, 192.168.0.7 - 9 Haupt-IP-Adressen der Kubernetes-Arbeitsknoten . </p><br><p>  Die Debian 9-Datenbank ist auf allen Servern installiert. </p><br><blockquote>  Denken Sie auch daran, dass die Systemanforderungen davon abh√§ngen, wie gro√ü und leistungsf√§hig der Cluster ist.  Weitere Informationen finden Sie in der Dokumentation zu Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Konfigurieren Sie HAProxy und Heartbeat. </h3><br><p>  Wir haben mehr als einen Kubernetes-Masterknoten. Daher m√ºssen Sie einen HAProxy-Load-Balancer vor sich konfigurieren, um den Datenverkehr zu verteilen.  Dies ist ein Paar HAProxy-Server mit einer gemeinsam genutzten virtuellen IP-Adresse.  Die Fehlertoleranz wird mit dem Heartbeat-Paket geliefert.  F√ºr die Bereitstellung verwenden wir die ersten beiden etcd-Server. </p><br><p>  Installieren und konfigurieren Sie HAProxy mit Heartbeat auf dem ersten und zweiten etcd-Server (in diesem Beispiel 192.168.0.2‚Äì3): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Speichern Sie die urspr√ºngliche Konfiguration und erstellen Sie eine neue: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  F√ºgen Sie diese Konfigurationsoptionen f√ºr beide HAProxy hinzu: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Wie Sie sehen k√∂nnen, teilen sich beide HAProxy-Dienste die IP-Adresse - 192.168.0.1.  Diese virtuelle IP-Adresse wird zwischen den Servern verschoben, daher sind wir etwas gerissen und <strong>aktivieren den</strong> Parameter <strong>net.ipv4.ip_nonlocal_bind</strong> , um die Bindung von Systemdiensten an eine nicht lokale IP-Adresse zu erm√∂glichen. </p><br><p>  F√ºgen Sie diese Funktion zur Datei <strong>/etc/sysctl.conf hinzu</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Auf beiden Servern ausf√ºhren: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  F√ºhren Sie HAProxy auch auf beiden Servern aus: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Stellen Sie sicher, dass HAProxy auf beiden Servern ausgef√ºhrt wird und die virtuelle IP-Adresse √ºberwacht: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Haube!  Installieren Sie nun Heartbeat und konfigurieren Sie diese virtuelle IP. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Es ist Zeit, mehrere Konfigurationsdateien daf√ºr zu erstellen: F√ºr den ersten und den zweiten Server sind sie im Grunde gleich. </p><br><p>  Erstellen Sie zuerst die Datei <strong>/etc/ha.d/authkeys</strong> . In dieser Datei speichert Heartbeat Daten zur gegenseitigen Authentifizierung.  Die Datei muss auf beiden Servern gleich sein: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  Diese Datei sollte nur f√ºr root zug√§nglich sein: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Erstellen Sie nun die Hauptkonfigurationsdatei f√ºr Heartbeat auf beiden Servern: F√ºr jeden Server ist dies etwas anders. </p><br><p>  Erstellen Sie <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>etcd1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>etcd2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Rufen Sie die "Knoten" -Parameter f√ºr diese Konfiguration ab, indem Sie uname -n auf beiden Etcd-Servern ausf√ºhren.  Verwenden Sie auch den Namen Ihrer Netzwerkkarte anstelle von ens18. </p><br><p>  Schlie√ülich m√ºssen Sie die Datei <strong>/etc/ha.d/haresources</strong> auf diesen Servern erstellen.  F√ºr beide Server muss die Datei identisch sein.  In dieser Datei legen wir unsere gemeinsame IP-Adresse fest und bestimmen, welcher Knoten der Standardmaster ist: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Wenn alles fertig ist, starten Sie die Heartbeat-Dienste auf beiden Servern und stellen Sie sicher, dass wir diese deklarierte virtuelle IP auf dem <strong>Knoten</strong> etcd1 erhalten haben: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Sie k√∂nnen √ºberpr√ºfen, ob HAProxy ordnungsgem√§√ü funktioniert, indem Sie <strong>nc</strong> unter 192.168.0.1 6443 ausf√ºhren. Sie m√ºssen eine Zeit√ºberschreitung festgestellt haben, da die Kubernetes-API auf der Serverseite noch nicht √ºberwacht.  Dies bedeutet jedoch, dass HAProxy und Heartbeat korrekt konfiguriert sind. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Vorbereitung der Knoten f√ºr Kubernetes </h3><br><p>  Der n√§chste Schritt besteht darin, alle Kubernetes-Knoten vorzubereiten.  Sie m√ºssen Docker mit einigen zus√§tzlichen Paketen installieren, das Kubernetes-Repository hinzuf√ºgen und die Pakete <strong>kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl daraus</strong> installieren.  Diese Einstellung ist f√ºr alle Kubernetes-Knoten (Master, Worker usw.) gleich. </p><br><blockquote>  Der Hauptvorteil von <strong>Kubeadm</strong> besteht darin, dass keine zus√§tzliche Software ben√∂tigt wird.  Installieren Sie <strong>kubeadm</strong> auf allen Hosts - und verwenden Sie es.  Generieren Sie mindestens CA-Zertifikate. </blockquote><p>  Installieren Sie Docker auf allen Knoten: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Installieren Sie danach Kubernetes-Pakete auf allen Knoten: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : Befehl zum Laden des Clusters. </li><li>  <strong><code>kubelet</code></strong> : Eine Komponente, die auf allen Computern im Cluster ausgef√ºhrt wird und Aktionen wie das Starten von Herden und Containern ausf√ºhrt. </li><li>  <strong><code>kubectl</code></strong> : Verwenden Sie die Befehlszeile, um mit dem Cluster zu kommunizieren. </li><li>  <strong>kubectl</strong> - nach Belieben;  Ich installiere es oft auf allen Knoten, um einige Kubernetes-Befehle zum Debuggen auszuf√ºhren. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Vergessen Sie nach der <strong>Installation von kubeadm</strong> und anderen Paketen nicht, den Swap zu deaktivieren. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  Wiederholen Sie die Installation auf den verbleibenden Knoten.  Softwarepakete sind f√ºr alle Knoten im Cluster gleich, und nur die folgende Konfiguration bestimmt die Rollen, die sie sp√§ter erhalten. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Konfigurieren Sie den HA Etcd-Cluster </h3><br><p>  Nachdem wir die Vorbereitungen abgeschlossen haben, werden wir den Kubernetes-Cluster konfigurieren.  Der erste Baustein ist der HA Etcd-Cluster, der ebenfalls mit dem kubeadm-Tool konfiguriert wird. </p><br><blockquote>  Bevor wir beginnen, stellen Sie sicher, dass alle etcd-Knoten √ºber die Ports 2379 und 2380 kommunizieren. Au√üerdem m√ºssen Sie den SSH-Zugriff zwischen ihnen konfigurieren, um <strong>scp verwenden zu k√∂nnen</strong> . </blockquote><p>  Beginnen wir mit dem ersten etcd-Knoten und kopieren dann einfach alle erforderlichen Zertifikate und Konfigurationsdateien auf die anderen Server. </p><br><p>  Auf allen <strong>etcd-</strong> Knoten m√ºssen Sie eine neue <strong>systemd-</strong> Konfigurationsdatei f√ºr die <strong>Kubelet-</strong> Einheit mit einer h√∂heren Priorit√§t hinzuf√ºgen: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Dann gehen wir √ºber ssh zum ersten <strong>etcd-</strong> Knoten - wir werden ihn verwenden, um alle erforderlichen <strong>kubeadm-</strong> Konfigurationen f√ºr jeden <strong>etcd-</strong> Knoten zu generieren und sie dann zu kopieren. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Erstellen Sie nun die <strong>Hauptzertifizierungsstelle</strong> mit <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Dieser Befehl erstellt zwei <strong>ca.crt- und ca.key-Dateien</strong> im <strong>Verzeichnis</strong> <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Jetzt werden wir Zertifikate f√ºr alle <strong>etcd-</strong> Knoten generieren: </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Kopieren Sie dann die Zertifikate und Konfigurationen von kubeadm auf die Knoten <strong>etcd2</strong> und <strong>etcd3</strong> . </p><br><blockquote>  Generieren Sie zuerst ein Paar <strong>SSH-</strong> Schl√ºssel auf <strong>etcd1</strong> und f√ºgen Sie den √∂ffentlichen Teil zu den <strong>Knoten etcd2</strong> und <strong>3 hinzu</strong> .  In diesem Beispiel werden alle Befehle im Auftrag eines Benutzers ausgef√ºhrt, der alle Rechte im System besitzt. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Stellen Sie vor dem Starten des etcd-Clusters sicher, dass die Dateien auf allen Knoten vorhanden sind: </p><br><p>  Liste der erforderlichen Dateien auf <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ ca.key ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  F√ºr den Knoten <strong>etcd2</strong> lautet dies: </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Und der letzte Knoten ist <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Wenn alle Zertifikate und Konfigurationen vorhanden sind, erstellen wir Manifeste.  F√ºhren Sie auf jedem Knoten den Befehl <strong>kubeadm aus</strong> , um ein statisches Manifest f√ºr den Cluster <strong>etcd</strong> zu generieren: </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Jetzt ist der Cluster <strong>etcd</strong> - theoretisch - konfiguriert und <strong>fehlerfrei</strong> .  √úberpr√ºfen Sie dies, indem Sie den folgenden Befehl auf dem <strong>Knoten</strong> etcd1 ausf√ºhren: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  Der <strong>etcd-</strong> Cluster ist gestiegen, fahren Sie fort. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Konfigurieren von Master- und Arbeitsknoten </h3><br><p>  Konfigurieren Sie die <strong>Masterknoten</strong> unseres Clusters - kopieren Sie diese Dateien vom ersten <strong>etcd-</strong> Knoten auf den ersten <strong>Masterknoten</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Gehen Sie dann ssh zum <strong>Masterknoten master1</strong> und erstellen Sie die <strong>Datei kubeadm-config.yaml</strong> mit dem folgenden Inhalt: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  Verschieben Sie die zuvor kopierten Zertifikate und den Schl√ºssel wie in der Beschreibung der Einstellung in das entsprechende Verzeichnis auf dem <strong>Knoten</strong> master1. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Gehen Sie wie folgt vor, um den ersten Masterknoten zu erstellen: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Wenn alle vorherigen Schritte korrekt ausgef√ºhrt wurden, wird Folgendes angezeigt: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Kopieren Sie diese <strong>kubeadm-</strong> Initialisierungsausgabe in eine beliebige Textdatei. Wir werden dieses Token in Zukunft verwenden, wenn wir den zweiten Master und die Arbeitsknoten an unseren Cluster anh√§ngen. </p><br><p>  Ich habe bereits gesagt, dass der Kubernetes-Cluster eine Art Overlay-Netzwerk f√ºr Herde und andere Dienste verwenden wird. Daher m√ºssen Sie an dieser Stelle eine Art CNI-Plugin installieren.  Ich empfehle das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weave CNI</a> Plugin.  Die Erfahrung hat gezeigt: Es ist n√ºtzlicher und weniger problematisch, aber Sie k√∂nnen ein anderes w√§hlen, zum Beispiel Calico. </p><br><p>  Installieren des Weave-Netzwerk-Plugins auf dem ersten Masterknoten: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Warten Sie einen Moment und geben Sie dann den folgenden Befehl ein, um zu √ºberpr√ºfen, ob die Komponentenherde gestartet werden: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  Es wird empfohlen, neue Knoten der Steuerebene erst nach der Initialisierung des ersten Knotens anzuh√§ngen. </li></ul><br><p>  Gehen Sie wie folgt vor, um den Clusterstatus zu √ºberpr√ºfen: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  Gro√üartig!  Der erste Hauptknoten stieg.  Jetzt ist es fertig und wir werden die Erstellung des Kubernetes-Clusters abschlie√üen - wir werden einen zweiten Masterknoten und Arbeitsknoten hinzuf√ºgen. <br>  Um einen zweiten <strong>Masterknoten</strong> hinzuzuf√ºgen, erstellen Sie einen <strong>SSH-</strong> Schl√ºssel auf <strong>Master1</strong> und f√ºgen Sie den √∂ffentlichen Teil zu <strong>Master2 hinzu</strong> .  F√ºhren Sie eine Testanmeldung durch und kopieren Sie dann einige Dateien vom ersten Masterknoten auf den zweiten: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  Verschieben Sie auf dem zweiten Masterknoten die zuvor kopierten Zertifikate und Schl√ºssel in die entsprechenden Verzeichnisse: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Verbinden Sie den zweiten Masterknoten mit dem Cluster.  Dazu ben√∂tigen Sie die Ausgabe des Verbindungsbefehls, der zuvor von <strong><code>kubeadm init</code></strong> auf dem ersten Knoten an uns <strong><code>kubeadm init</code></strong> . </p><br><p>  F√ºhren Sie den <strong>Masterknoten master2 aus</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  Sie m√ºssen das <strong><code>--experimental-control-plane</code></strong> hinzuf√ºgen.  Es automatisiert das Anh√§ngen von Stammdaten an einen Cluster.  Ohne dieses Flag wird einfach der √ºbliche Arbeitsknoten hinzugef√ºgt. </li></ul><br><p>  Warten Sie etwas, bis der Knoten dem Cluster beitritt, und √ºberpr√ºfen Sie den neuen Status des Clusters: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Stellen Sie au√üerdem sicher, dass alle Pods von allen Masterknoten normal gestartet werden: </p><br><pre> <code class="plaintext hljs">master1# kubectl ‚Äî kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  Gro√üartig!  Wir sind fast fertig mit der Kubernetes-Cluster-Konfiguration.  Als letztes m√ºssen Sie die drei Arbeitsknoten hinzuf√ºgen, die wir zuvor vorbereitet haben. </p><br><p>  Geben Sie die Arbeitsknoten ein und f√ºhren Sie den Befehl kubeadm join ohne das <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  √úberpr√ºfen Sie den Clusterstatus erneut: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Wie Sie sehen k√∂nnen, haben wir einen vollst√§ndig konfigurierten Kubernetes HA-Cluster mit zwei Master- und drei Arbeitsknoten.  Es basiert auf dem HA etcd-Cluster mit einem ausfallsicheren Load Balancer vor den Masterknoten.  Klingt ziemlich gut f√ºr mich. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Konfigurieren der Remote-Clusterverwaltung </h3><br><p>  Eine weitere Aktion, die in diesem ersten Teil des Artikels noch ber√ºcksichtigt werden muss, ist das Einrichten des Remote-Dienstprogramms <strong>kubectl</strong> zum Verwalten des Clusters.  Bisher haben wir alle Befehle vom <strong>Masterknoten master1 ausgef√ºhrt</strong> , dies ist jedoch nur zum ersten Mal geeignet - bei der Konfiguration des Clusters.  Es w√§re sch√∂n, einen externen Steuerknoten zu konfigurieren.  Sie k√∂nnen hierf√ºr einen Laptop oder einen anderen Server verwenden. </p><br><p>  Melden Sie sich bei diesem Server an und f√ºhren Sie Folgendes aus: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, jetzt f√ºhren wir einen Test in unserem Cluster durch und √ºberpr√ºfen, wie es funktioniert. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  Gl√ºckwunsch!  Sie haben gerade Kubernetes bereitgestellt.  Und das bedeutet, dass Ihr neuer HA-Cluster bereit ist.  Tats√§chlich ist das Einrichten eines Kubernetes-Clusters mit <strong>kubeadm</strong> recht einfach und schnell. </p><br><p>  Im n√§chsten Teil des Artikels werden wir internen Speicher hinzuf√ºgen, indem wir GlusterFS auf allen Arbeitsknoten einrichten, einen internen Load Balancer f√ºr unseren Kubernetes-Cluster einrichten, bestimmte Stresstests ausf√ºhren, einige Knoten trennen und den Cluster auf Stabilit√§t √ºberpr√ºfen. </p><br><h3 id="posleslovie">  Nachwort </h3><br><p>  Ja, wenn Sie an diesem Beispiel arbeiten, werden Sie auf eine Reihe von Problemen sto√üen.  Kein Grund zur Sorge: Um die √Ñnderungen r√ºckg√§ngig zu machen und die Knoten in ihren urspr√ºnglichen Zustand zur√ºckzusetzen, f√ºhren Sie einfach <strong>kubeadm reset</strong> aus. Die zuvor von <strong>kubeadm</strong> vorgenommenen √Ñnderungen werden zur√ºckgesetzt und Sie k√∂nnen sie erneut konfigurieren.  Vergessen Sie auch nicht, den Status der Docker-Container auf den Clusterknoten zu √ºberpr√ºfen. Stellen Sie sicher, dass alle fehlerfrei gestartet werden und funktionieren.  Verwenden Sie den <strong>Befehl docker logs containerid</strong> , um weitere Informationen zu besch√§digten Containern zu erhalten. </p><br><p>  Das ist alles f√ºr heute.  Viel Gl√ºck </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439562/">https://habr.com/ru/post/de439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439550/index.html">Hackquest 2018. Ergebnisse & Zuschreibungen. Tag 1-3</a></li>
<li><a href="../de439552/index.html">Sch√§dliche Chrome-Erweiterungen</a></li>
<li><a href="../de439556/index.html">TDMS Fairway. PMBOK-Methoden und russische Designorganisationen</a></li>
<li><a href="../de439558/index.html">Neues altes Telefon. Das PSTN-Telefon neu erfinden</a></li>
<li><a href="../de439560/index.html">Ethereum-Blockchain-Adapter f√ºr die IRIS-Datenplattform von InterSystems</a></li>
<li><a href="../de439564/index.html">Praktische Anwendung der AST-Baumtransformation am Beispiel von Putout</a></li>
<li><a href="../de439566/index.html">Warum SRE-Dokumentation wichtig ist. Teil 3</a></li>
<li><a href="../de439568/index.html">QLC-basierte SSDs - ein Festplattenkiller? Nicht wirklich</a></li>
<li><a href="../de439570/index.html">IPython-Magie zum Bearbeiten von Jupyter-Zellen-Tags</a></li>
<li><a href="../de439572/index.html">Computergest√ºtztes Design elektronischer Ger√§te</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>