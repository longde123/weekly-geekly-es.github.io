<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍✈️ 👼🏿 🕋 Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit kubeadm. Teil 1/3 🏩 👋 🤚🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 2/3 hier 
 Teil 3/3 hier 


 Hallo allerseits! In diesem Artikel möchte ich die Informationen optimieren und die Erfahrungen beim Erstellen und V...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit kubeadm. Teil 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Teil 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> <br>  <strong>Teil 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> </p><br><p>  Hallo allerseits!  In diesem Artikel möchte ich die Informationen optimieren und die Erfahrungen beim Erstellen und Verwenden des internen Kubernetes-Clusters teilen. </p><br><p>  In den letzten Jahren hat diese Container-Orchestrierungstechnologie einen großen Schritt nach vorne gemacht und ist für Tausende von Unternehmen zu einer Art Unternehmensstandard geworden.  Einige verwenden es in der Produktion, andere testen es nur an Projekten, aber die Leidenschaften, egal wie Sie es sagen, leuchten ernst.  Wenn Sie es noch nie benutzt haben, ist es Zeit, mit dem Dating zu beginnen. </p><br><h3 id="0-vstuplenie">  0. Einleitung </h3><br><p>  Kubernetes ist eine skalierbare Orchestrierungstechnologie, die mit der Installation auf einem einzelnen Knoten beginnen und die Größe großer HA-Cluster erreichen kann, die auf mehreren hundert Knoten im Inneren basieren.  Die meisten gängigen Cloud-Anbieter bieten verschiedene Arten von Kubernetes-Implementierungen an - nehmen und verwenden.  Die Situationen sind jedoch anders, und es gibt Unternehmen, die die Clouds nicht nutzen, und sie möchten alle Vorteile moderner Orchestrierungstechnologien nutzen.  Und hier kommt die Installation von Kubernetes auf Bare Metal. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Einleitung </h3><br><p> In diesem Beispiel erstellen wir einen Kubernetes HA-Cluster mit der Topologie für mehrere Master, mit einem externen Cluster usw. als Basisschicht und einem MetalLB-Load-Balancer im Inneren.  Auf allen Arbeitsknoten werden wir GlusterFS als einfachen internen verteilten Clusterspeicher bereitstellen.  Wir werden auch versuchen, mehrere Testprojekte mithilfe unserer persönlichen Docker-Registrierung darin bereitzustellen. </p><br><p>  Im Allgemeinen gibt es mehrere Möglichkeiten, einen Kubernetes-HA-Cluster zu erstellen: den schwierigen und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">detaillierten</a> Pfad, der im beliebten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokument kubernetes-the-hard-way beschrieben</a> ist, oder den einfacheren Weg mit dem Dienstprogramm <strong>kubeadm</strong> . </p><br><p>  Kubeadm ist ein Tool, das von der Kubernetes-Community speziell entwickelt wurde, um die Installation von Kubernetes zu vereinfachen und den Prozess zu vereinfachen.  Zuvor wurde Kubeadm nur zum Erstellen kleiner Testcluster mit einem Masterknoten empfohlen, um loszulegen.  Im letzten Jahr wurde jedoch viel verbessert, und jetzt können wir damit HA-Cluster mit mehreren Masterknoten erstellen.  Laut Community-News von Kubernetes wird Kubeadm in Zukunft als Tool für die Installation von Kubernetes empfohlen. </p><br><p>  Die Kubeadm-Dokumentation bietet zwei grundlegende Möglichkeiten zum Implementieren eines Clusters mit Stack- und externen etcd-Topologien.  Ich werde den zweiten Pfad mit externen etcd-Knoten aufgrund der Fehlertoleranz des HA-Clusters wählen. </p><br><p>  Hier ist ein Diagramm aus der Kubeadm-Dokumentation, das diesen Pfad beschreibt: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Ich werde es ein wenig ändern.  Zunächst werde ich ein Paar HAProxy-Server als Load Balancer für das Heartbeat-Paket verwenden, das die virtuelle IP-Adresse gemeinsam nutzt.  Heartbeat und HAProxy verwenden eine kleine Menge an Systemressourcen, daher werde ich sie auf einem Paar von etcd-Knoten platzieren, um die Anzahl der Server für unseren Cluster geringfügig zu reduzieren. </p><br><p>  Für dieses Kubernetes-Clusterschema sind acht Knoten erforderlich.  Drei Server für einen externen Cluster usw. (LB-Dienste verwenden auch einige davon), zwei für Knoten der Steuerebene (Hauptknoten) und drei für Arbeitsknoten.  Es kann sich entweder um Bare-Metal- oder einen VM-Server handeln.  In diesem Fall spielt es keine Rolle.  Sie können das Schema einfach ändern, indem Sie weitere Masterknoten hinzufügen und HAProxy mit Heartbeat auf separaten Knoten platzieren, wenn viele freie Server vorhanden sind.  Obwohl meine Option für die erste Implementierung des HA-Clusters für die Augen ausreicht. </p><br><p>  Wenn Sie möchten, fügen Sie einen kleinen Server mit dem <strong>installierten</strong> Dienstprogramm <strong>kubectl hinzu</strong> , um diesen Cluster zu verwalten, oder verwenden Sie dazu Ihren eigenen Linux-Desktop. </p><br><p>  Das Diagramm für dieses Beispiel sieht ungefähr so ​​aus: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Anforderungen </h3><br><p>  Sie benötigen zwei Kubernetes-Masterknoten mit den empfohlenen Mindestsystemanforderungen: 2 CPUs und 2 GB RAM gemäß der <strong>kubeadm-</strong> Dokumentation.  Für funktionierende Knoten empfehle ich die Verwendung leistungsfähigerer Server, da alle unsere Anwendungsdienste auf diesen ausgeführt werden.  Und für Etcd + LB können wir auch Server mit zwei CPUs und mindestens 2 GB RAM verwenden. </p><br><p>  Wählen Sie ein öffentliches oder privates Netzwerk für diesen Cluster aus.  IP-Adressen spielen keine Rolle;  Es ist wichtig, dass alle Server für einander und natürlich für Sie zugänglich sind.  Später werden wir im Kubernetes-Cluster ein Overlay-Netzwerk einrichten. </p><br><p>  Die Mindestanforderungen für dieses Beispiel sind: </p><br><ul><li>  2 Server mit 2 Prozessoren und 2 GB RAM für den Masterknoten </li><li>  3 Server mit 4 Prozessoren und 4-8 GB RAM für Arbeitsknoten </li><li>  3 Server mit 2 Prozessoren und 2 GB RAM für Etcd und HAProxy </li><li>  192.168.0.0/24 - das Subnetz. </li></ul><br><p>  192.168.0.1 - Virtuelle HAProxy-IP-Adresse, 192.168.0.2 - 4 Haupt-IP-Adressen von Etcd- und HAProxy-Knoten, 192.168.0.5 - 6 Haupt-IP-Adressen des Kubernetes-Masterknotens, 192.168.0.7 - 9 Haupt-IP-Adressen der Kubernetes-Arbeitsknoten . </p><br><p>  Die Debian 9-Datenbank ist auf allen Servern installiert. </p><br><blockquote>  Denken Sie auch daran, dass die Systemanforderungen davon abhängen, wie groß und leistungsfähig der Cluster ist.  Weitere Informationen finden Sie in der Dokumentation zu Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Konfigurieren Sie HAProxy und Heartbeat. </h3><br><p>  Wir haben mehr als einen Kubernetes-Masterknoten. Daher müssen Sie einen HAProxy-Load-Balancer vor sich konfigurieren, um den Datenverkehr zu verteilen.  Dies ist ein Paar HAProxy-Server mit einer gemeinsam genutzten virtuellen IP-Adresse.  Die Fehlertoleranz wird mit dem Heartbeat-Paket geliefert.  Für die Bereitstellung verwenden wir die ersten beiden etcd-Server. </p><br><p>  Installieren und konfigurieren Sie HAProxy mit Heartbeat auf dem ersten und zweiten etcd-Server (in diesem Beispiel 192.168.0.2–3): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Speichern Sie die ursprüngliche Konfiguration und erstellen Sie eine neue: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  Fügen Sie diese Konfigurationsoptionen für beide HAProxy hinzu: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Wie Sie sehen können, teilen sich beide HAProxy-Dienste die IP-Adresse - 192.168.0.1.  Diese virtuelle IP-Adresse wird zwischen den Servern verschoben, daher sind wir etwas gerissen und <strong>aktivieren den</strong> Parameter <strong>net.ipv4.ip_nonlocal_bind</strong> , um die Bindung von Systemdiensten an eine nicht lokale IP-Adresse zu ermöglichen. </p><br><p>  Fügen Sie diese Funktion zur Datei <strong>/etc/sysctl.conf hinzu</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Auf beiden Servern ausführen: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  Führen Sie HAProxy auch auf beiden Servern aus: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Stellen Sie sicher, dass HAProxy auf beiden Servern ausgeführt wird und die virtuelle IP-Adresse überwacht: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Haube!  Installieren Sie nun Heartbeat und konfigurieren Sie diese virtuelle IP. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Es ist Zeit, mehrere Konfigurationsdateien dafür zu erstellen: Für den ersten und den zweiten Server sind sie im Grunde gleich. </p><br><p>  Erstellen Sie zuerst die Datei <strong>/etc/ha.d/authkeys</strong> . In dieser Datei speichert Heartbeat Daten zur gegenseitigen Authentifizierung.  Die Datei muss auf beiden Servern gleich sein: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  Diese Datei sollte nur für root zugänglich sein: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Erstellen Sie nun die Hauptkonfigurationsdatei für Heartbeat auf beiden Servern: Für jeden Server ist dies etwas anders. </p><br><p>  Erstellen Sie <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>etcd1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>etcd2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Rufen Sie die "Knoten" -Parameter für diese Konfiguration ab, indem Sie uname -n auf beiden Etcd-Servern ausführen.  Verwenden Sie auch den Namen Ihrer Netzwerkkarte anstelle von ens18. </p><br><p>  Schließlich müssen Sie die Datei <strong>/etc/ha.d/haresources</strong> auf diesen Servern erstellen.  Für beide Server muss die Datei identisch sein.  In dieser Datei legen wir unsere gemeinsame IP-Adresse fest und bestimmen, welcher Knoten der Standardmaster ist: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Wenn alles fertig ist, starten Sie die Heartbeat-Dienste auf beiden Servern und stellen Sie sicher, dass wir diese deklarierte virtuelle IP auf dem <strong>Knoten</strong> etcd1 erhalten haben: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Sie können überprüfen, ob HAProxy ordnungsgemäß funktioniert, indem Sie <strong>nc</strong> unter 192.168.0.1 6443 ausführen. Sie müssen eine Zeitüberschreitung festgestellt haben, da die Kubernetes-API auf der Serverseite noch nicht überwacht.  Dies bedeutet jedoch, dass HAProxy und Heartbeat korrekt konfiguriert sind. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Vorbereitung der Knoten für Kubernetes </h3><br><p>  Der nächste Schritt besteht darin, alle Kubernetes-Knoten vorzubereiten.  Sie müssen Docker mit einigen zusätzlichen Paketen installieren, das Kubernetes-Repository hinzufügen und die Pakete <strong>kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl daraus</strong> installieren.  Diese Einstellung ist für alle Kubernetes-Knoten (Master, Worker usw.) gleich. </p><br><blockquote>  Der Hauptvorteil von <strong>Kubeadm</strong> besteht darin, dass keine zusätzliche Software benötigt wird.  Installieren Sie <strong>kubeadm</strong> auf allen Hosts - und verwenden Sie es.  Generieren Sie mindestens CA-Zertifikate. </blockquote><p>  Installieren Sie Docker auf allen Knoten: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Installieren Sie danach Kubernetes-Pakete auf allen Knoten: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : Befehl zum Laden des Clusters. </li><li>  <strong><code>kubelet</code></strong> : Eine Komponente, die auf allen Computern im Cluster ausgeführt wird und Aktionen wie das Starten von Herden und Containern ausführt. </li><li>  <strong><code>kubectl</code></strong> : Verwenden Sie die Befehlszeile, um mit dem Cluster zu kommunizieren. </li><li>  <strong>kubectl</strong> - nach Belieben;  Ich installiere es oft auf allen Knoten, um einige Kubernetes-Befehle zum Debuggen auszuführen. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Vergessen Sie nach der <strong>Installation von kubeadm</strong> und anderen Paketen nicht, den Swap zu deaktivieren. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  Wiederholen Sie die Installation auf den verbleibenden Knoten.  Softwarepakete sind für alle Knoten im Cluster gleich, und nur die folgende Konfiguration bestimmt die Rollen, die sie später erhalten. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Konfigurieren Sie den HA Etcd-Cluster </h3><br><p>  Nachdem wir die Vorbereitungen abgeschlossen haben, werden wir den Kubernetes-Cluster konfigurieren.  Der erste Baustein ist der HA Etcd-Cluster, der ebenfalls mit dem kubeadm-Tool konfiguriert wird. </p><br><blockquote>  Bevor wir beginnen, stellen Sie sicher, dass alle etcd-Knoten über die Ports 2379 und 2380 kommunizieren. Außerdem müssen Sie den SSH-Zugriff zwischen ihnen konfigurieren, um <strong>scp verwenden zu können</strong> . </blockquote><p>  Beginnen wir mit dem ersten etcd-Knoten und kopieren dann einfach alle erforderlichen Zertifikate und Konfigurationsdateien auf die anderen Server. </p><br><p>  Auf allen <strong>etcd-</strong> Knoten müssen Sie eine neue <strong>systemd-</strong> Konfigurationsdatei für die <strong>Kubelet-</strong> Einheit mit einer höheren Priorität hinzufügen: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Dann gehen wir über ssh zum ersten <strong>etcd-</strong> Knoten - wir werden ihn verwenden, um alle erforderlichen <strong>kubeadm-</strong> Konfigurationen für jeden <strong>etcd-</strong> Knoten zu generieren und sie dann zu kopieren. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Erstellen Sie nun die <strong>Hauptzertifizierungsstelle</strong> mit <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Dieser Befehl erstellt zwei <strong>ca.crt- und ca.key-Dateien</strong> im <strong>Verzeichnis</strong> <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Jetzt werden wir Zertifikate für alle <strong>etcd-</strong> Knoten generieren: </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Kopieren Sie dann die Zertifikate und Konfigurationen von kubeadm auf die Knoten <strong>etcd2</strong> und <strong>etcd3</strong> . </p><br><blockquote>  Generieren Sie zuerst ein Paar <strong>SSH-</strong> Schlüssel auf <strong>etcd1</strong> und fügen Sie den öffentlichen Teil zu den <strong>Knoten etcd2</strong> und <strong>3 hinzu</strong> .  In diesem Beispiel werden alle Befehle im Auftrag eines Benutzers ausgeführt, der alle Rechte im System besitzt. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Stellen Sie vor dem Starten des etcd-Clusters sicher, dass die Dateien auf allen Knoten vorhanden sind: </p><br><p>  Liste der erforderlichen Dateien auf <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Für den Knoten <strong>etcd2</strong> lautet dies: </p><br><pre> <code class="plaintext hljs">/root └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Und der letzte Knoten ist <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key</code> </pre> <br><p>  Wenn alle Zertifikate und Konfigurationen vorhanden sind, erstellen wir Manifeste.  Führen Sie auf jedem Knoten den Befehl <strong>kubeadm aus</strong> , um ein statisches Manifest für den Cluster <strong>etcd</strong> zu generieren: </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Jetzt ist der Cluster <strong>etcd</strong> - theoretisch - konfiguriert und <strong>fehlerfrei</strong> .  Überprüfen Sie dies, indem Sie den folgenden Befehl auf dem <strong>Knoten</strong> etcd1 ausführen: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  Der <strong>etcd-</strong> Cluster ist gestiegen, fahren Sie fort. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Konfigurieren von Master- und Arbeitsknoten </h3><br><p>  Konfigurieren Sie die <strong>Masterknoten</strong> unseres Clusters - kopieren Sie diese Dateien vom ersten <strong>etcd-</strong> Knoten auf den ersten <strong>Masterknoten</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Gehen Sie dann ssh zum <strong>Masterknoten master1</strong> und erstellen Sie die <strong>Datei kubeadm-config.yaml</strong> mit dem folgenden Inhalt: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  Verschieben Sie die zuvor kopierten Zertifikate und den Schlüssel wie in der Beschreibung der Einstellung in das entsprechende Verzeichnis auf dem <strong>Knoten</strong> master1. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Gehen Sie wie folgt vor, um den ersten Masterknoten zu erstellen: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Wenn alle vorherigen Schritte korrekt ausgeführt wurden, wird Folgendes angezeigt: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Kopieren Sie diese <strong>kubeadm-</strong> Initialisierungsausgabe in eine beliebige Textdatei. Wir werden dieses Token in Zukunft verwenden, wenn wir den zweiten Master und die Arbeitsknoten an unseren Cluster anhängen. </p><br><p>  Ich habe bereits gesagt, dass der Kubernetes-Cluster eine Art Overlay-Netzwerk für Herde und andere Dienste verwenden wird. Daher müssen Sie an dieser Stelle eine Art CNI-Plugin installieren.  Ich empfehle das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weave CNI</a> Plugin.  Die Erfahrung hat gezeigt: Es ist nützlicher und weniger problematisch, aber Sie können ein anderes wählen, zum Beispiel Calico. </p><br><p>  Installieren des Weave-Netzwerk-Plugins auf dem ersten Masterknoten: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Warten Sie einen Moment und geben Sie dann den folgenden Befehl ein, um zu überprüfen, ob die Komponentenherde gestartet werden: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  Es wird empfohlen, neue Knoten der Steuerebene erst nach der Initialisierung des ersten Knotens anzuhängen. </li></ul><br><p>  Gehen Sie wie folgt vor, um den Clusterstatus zu überprüfen: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  Großartig!  Der erste Hauptknoten stieg.  Jetzt ist es fertig und wir werden die Erstellung des Kubernetes-Clusters abschließen - wir werden einen zweiten Masterknoten und Arbeitsknoten hinzufügen. <br>  Um einen zweiten <strong>Masterknoten</strong> hinzuzufügen, erstellen Sie einen <strong>SSH-</strong> Schlüssel auf <strong>Master1</strong> und fügen Sie den öffentlichen Teil zu <strong>Master2 hinzu</strong> .  Führen Sie eine Testanmeldung durch und kopieren Sie dann einige Dateien vom ersten Masterknoten auf den zweiten: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  Verschieben Sie auf dem zweiten Masterknoten die zuvor kopierten Zertifikate und Schlüssel in die entsprechenden Verzeichnisse: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Verbinden Sie den zweiten Masterknoten mit dem Cluster.  Dazu benötigen Sie die Ausgabe des Verbindungsbefehls, der zuvor von <strong><code>kubeadm init</code></strong> auf dem ersten Knoten an uns <strong><code>kubeadm init</code></strong> . </p><br><p>  Führen Sie den <strong>Masterknoten master2 aus</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  Sie müssen das <strong><code>--experimental-control-plane</code></strong> hinzufügen.  Es automatisiert das Anhängen von Stammdaten an einen Cluster.  Ohne dieses Flag wird einfach der übliche Arbeitsknoten hinzugefügt. </li></ul><br><p>  Warten Sie etwas, bis der Knoten dem Cluster beitritt, und überprüfen Sie den neuen Status des Clusters: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Stellen Sie außerdem sicher, dass alle Pods von allen Masterknoten normal gestartet werden: </p><br><pre> <code class="plaintext hljs">master1# kubectl — kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  Großartig!  Wir sind fast fertig mit der Kubernetes-Cluster-Konfiguration.  Als letztes müssen Sie die drei Arbeitsknoten hinzufügen, die wir zuvor vorbereitet haben. </p><br><p>  Geben Sie die Arbeitsknoten ein und führen Sie den Befehl kubeadm join ohne das <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Überprüfen Sie den Clusterstatus erneut: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Wie Sie sehen können, haben wir einen vollständig konfigurierten Kubernetes HA-Cluster mit zwei Master- und drei Arbeitsknoten.  Es basiert auf dem HA etcd-Cluster mit einem ausfallsicheren Load Balancer vor den Masterknoten.  Klingt ziemlich gut für mich. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Konfigurieren der Remote-Clusterverwaltung </h3><br><p>  Eine weitere Aktion, die in diesem ersten Teil des Artikels noch berücksichtigt werden muss, ist das Einrichten des Remote-Dienstprogramms <strong>kubectl</strong> zum Verwalten des Clusters.  Bisher haben wir alle Befehle vom <strong>Masterknoten master1 ausgeführt</strong> , dies ist jedoch nur zum ersten Mal geeignet - bei der Konfiguration des Clusters.  Es wäre schön, einen externen Steuerknoten zu konfigurieren.  Sie können hierfür einen Laptop oder einen anderen Server verwenden. </p><br><p>  Melden Sie sich bei diesem Server an und führen Sie Folgendes aus: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, jetzt führen wir einen Test in unserem Cluster durch und überprüfen, wie es funktioniert. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  Glückwunsch!  Sie haben gerade Kubernetes bereitgestellt.  Und das bedeutet, dass Ihr neuer HA-Cluster bereit ist.  Tatsächlich ist das Einrichten eines Kubernetes-Clusters mit <strong>kubeadm</strong> recht einfach und schnell. </p><br><p>  Im nächsten Teil des Artikels werden wir internen Speicher hinzufügen, indem wir GlusterFS auf allen Arbeitsknoten einrichten, einen internen Load Balancer für unseren Kubernetes-Cluster einrichten, bestimmte Stresstests ausführen, einige Knoten trennen und den Cluster auf Stabilität überprüfen. </p><br><h3 id="posleslovie">  Nachwort </h3><br><p>  Ja, wenn Sie an diesem Beispiel arbeiten, werden Sie auf eine Reihe von Problemen stoßen.  Kein Grund zur Sorge: Um die Änderungen rückgängig zu machen und die Knoten in ihren ursprünglichen Zustand zurückzusetzen, führen Sie einfach <strong>kubeadm reset</strong> aus. Die zuvor von <strong>kubeadm</strong> vorgenommenen Änderungen werden zurückgesetzt und Sie können sie erneut konfigurieren.  Vergessen Sie auch nicht, den Status der Docker-Container auf den Clusterknoten zu überprüfen. Stellen Sie sicher, dass alle fehlerfrei gestartet werden und funktionieren.  Verwenden Sie den <strong>Befehl docker logs containerid</strong> , um weitere Informationen zu beschädigten Containern zu erhalten. </p><br><p>  Das ist alles für heute.  Viel Glück </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439562/">https://habr.com/ru/post/de439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439550/index.html">Hackquest 2018. Ergebnisse & Zuschreibungen. Tag 1-3</a></li>
<li><a href="../de439552/index.html">Schädliche Chrome-Erweiterungen</a></li>
<li><a href="../de439556/index.html">TDMS Fairway. PMBOK-Methoden und russische Designorganisationen</a></li>
<li><a href="../de439558/index.html">Neues altes Telefon. Das PSTN-Telefon neu erfinden</a></li>
<li><a href="../de439560/index.html">Ethereum-Blockchain-Adapter für die IRIS-Datenplattform von InterSystems</a></li>
<li><a href="../de439564/index.html">Praktische Anwendung der AST-Baumtransformation am Beispiel von Putout</a></li>
<li><a href="../de439566/index.html">Warum SRE-Dokumentation wichtig ist. Teil 3</a></li>
<li><a href="../de439568/index.html">QLC-basierte SSDs - ein Festplattenkiller? Nicht wirklich</a></li>
<li><a href="../de439570/index.html">IPython-Magie zum Bearbeiten von Jupyter-Zellen-Tags</a></li>
<li><a href="../de439572/index.html">Computergestütztes Design elektronischer Geräte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>