<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîá üåë ‚åõÔ∏è C√≥mo crear un juego AI: una gu√≠a para principiantes üö¥üèΩ ‚úãüèø üö≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Encontr√© material interesante sobre inteligencia artificial en juegos. Con una explicaci√≥n de las cosas b√°sicas sobre IA usando ejemplos simples, y de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo crear un juego AI: una gu√≠a para principiantes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/428892/"><img src="https://habrastorage.org/webt/hp/x-/0n/hpx-0n-frdakrflfvlpdt-6hp1e.png"><br><br>  Encontr√© material interesante sobre inteligencia artificial en juegos.  Con una explicaci√≥n de las cosas b√°sicas sobre IA usando ejemplos simples, y dentro hay muchas herramientas y m√©todos √∫tiles para su conveniente desarrollo y dise√±o.  C√≥mo, d√≥nde y cu√°ndo usarlos, tambi√©n est√° ah√≠. <br><br>  La mayor√≠a de los ejemplos est√°n escritos en pseudoc√≥digo, por lo que no se requieren conocimientos profundos de programaci√≥n.  Debajo del corte 35 hojas de texto con im√°genes y gifs, as√≠ que prep√°rate. <br><br>  UPD  Lo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">siento</a> , pero <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PatientZero</a> ya hizo la traducci√≥n de este art√≠culo sobre Habr√©.  Puede leer su versi√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , pero por alguna raz√≥n el art√≠culo me pas√≥ (utilic√© la b√∫squeda, pero algo sali√≥ mal).  Y como estoy escribiendo un blog de desarrollo de juegos, decid√≠ dejar mi opci√≥n de traducci√≥n para los suscriptores (algunos momentos son diferentes para m√≠, algunos se pierden intencionalmente por el consejo de los desarrolladores). <br><a name="habracut"></a><br><h2>  ¬øQu√© es la IA? </h2><br>  Game AI se centra en las acciones que debe realizar un objeto en funci√≥n de las condiciones en las que se encuentra.  Esto generalmente se llama la gesti√≥n de "agentes inteligentes", donde el agente es un personaje del juego, un veh√≠culo, un bot y, a veces, algo m√°s abstracto: un grupo completo de entidades o incluso una civilizaci√≥n.  En cada caso, es algo que debe ver su entorno, tomar decisiones sobre la base y actuar de acuerdo con ellas.  Esto se llama el ciclo Sentido / Pensar / Actuar: <br><br><ul><li>  Sentido: el agente encuentra o recibe informaci√≥n sobre cosas en su entorno que pueden afectar su comportamiento (amenazas cercanas, elementos para recolectar, lugares interesantes para investigar). </li><li>  Piensa: el agente decide c√≥mo reaccionar (considera si es seguro recolectar objetos o si debe luchar / esconderse primero). </li><li>  Actuar: el agente realiza acciones para implementar la decisi√≥n anterior (comienza a moverse hacia el oponente u objeto). </li><li>  ... ahora la situaci√≥n ha cambiado debido a las acciones de los personajes, por lo que el ciclo se repite con nuevos datos. </li></ul><br>  La IA tiende a centrarse en la parte sensorial del bucle.  Por ejemplo, los autom√≥viles aut√≥nomos toman fotos de la carretera, las combinan con datos de radar y lidar e interpretan.  Por lo general, esto se hace mediante el aprendizaje autom√°tico, que procesa los datos entrantes y les da significado, extrayendo informaci√≥n sem√°ntica como "hay otro autom√≥vil 20 yardas por delante de usted".  Estos son los llamados problemas de clasificaci√≥n. <br><br>  Los juegos no necesitan un sistema complejo para extraer informaci√≥n, ya que la mayor√≠a de los datos ya son una parte integral de la misma.  No es necesario ejecutar algoritmos de reconocimiento de im√°genes para determinar si hay un enemigo por delante: el juego ya conoce y transfiere informaci√≥n directamente en el proceso de toma de decisiones.  Por lo tanto, parte del ciclo de detecci√≥n es a menudo mucho m√°s simple que pensar y actuar. <br><br><h2>  Limitaciones de la IA del juego </h2><br>  La IA tiene una serie de restricciones que deben observarse: <br><br><ul><li>  La IA no necesita ser entrenada de antemano, como si fuera un algoritmo de aprendizaje autom√°tico.  No tiene sentido escribir una red neuronal durante el desarrollo para observar decenas de miles de jugadores y aprender la mejor manera de jugar contra ellos.  Por qu√©  Porque el juego no se lanz√≥, pero no hay jugadores. </li><li>  El juego debe entretener y desafiar, por lo tanto, los agentes no deben encontrar el mejor enfoque contra las personas. </li><li>  Los agentes deben parecer realistas para que los jugadores sientan que est√°n jugando contra personas reales.  AlphaGo destac√≥ a los humanos, pero los pasos dados estaban lejos de la comprensi√≥n tradicional del juego.  Si el juego imita a un oponente humano, no deber√≠a existir ese sentimiento.  El algoritmo necesita ser cambiado para que tome decisiones plausibles, no ideales. </li><li>  La IA deber√≠a funcionar en tiempo real.  Esto significa que el algoritmo no puede monopolizar el uso del procesador durante mucho tiempo para la toma de decisiones.  Incluso 10 milisegundos para esto es demasiado largo, porque la mayor√≠a de los juegos solo tienen de 16 a 33 milisegundos para completar todo el procesamiento y pasar al siguiente cuadro del gr√°fico. </li><li>  Idealmente, al menos parte del sistema est√° basado en datos para que los no codificadores puedan hacer cambios y los ajustes sean m√°s r√°pidos. </li></ul><br>  Considere los enfoques de IA que abarcan todo el ciclo Sentido / Pensar / Actuar. <br><br><h3>  Toma de decisiones b√°sicas </h3><br>  Comencemos con el juego m√°s simple: Pong.  Objetivo: mover la plataforma (paleta) para que la pelota rebote en lugar de volar.  Es como el tenis, en el que pierdes si no golpeas la pelota.  Aqu√≠, la IA tiene una tarea relativamente f√°cil: decidir en qu√© direcci√≥n mover la plataforma. <br><br><img src="https://habrastorage.org/webt/1l/6g/p8/1l6gp88aolep77ohq7dqwkw8oky.jpeg"><br><br><h3>  Declaraciones condicionales </h3><br>  Para AI, Pong tiene la soluci√≥n m√°s obvia: siempre trate de colocar la plataforma debajo de la pelota. <br><br>  Un algoritmo simple para esto, escrito en pseudoc√≥digo: <br><br>  <i>cada cuadro / actualizaci√≥n mientras se ejecuta el juego:</i> <i><br></i>  <i>si la pelota est√° a la izquierda de la pala:</i> <i><br></i>  <i>mover la paleta hacia la izquierda</i> <i><br></i>  <i>si la pelota est√° a la derecha de la pala:</i> <i><br></i>  <i>mover la paleta hacia la derecha</i> <br><br>  Si la plataforma se mueve a la velocidad de la pelota, entonces este es el algoritmo perfecto para la IA en Pong.  No es necesario complicar nada si no hay tantos datos y posibles acciones para el agente. <br><br>  Este enfoque es tan simple que casi no se nota todo el ciclo Sentido / Pensar / Actuar.  Pero √©l es: <br><br><ul><li>  La parte de sentido est√° en dos declaraciones if.  El juego sabe d√≥nde est√° la pelota y d√≥nde est√° la plataforma, por lo que la IA recurre a ella para obtener esta informaci√≥n. </li><li>  La parte Think tambi√©n viene en dos declaraciones if.  Incorporan dos soluciones, que en este caso son mutuamente excluyentes.  Como resultado, se selecciona una de las tres acciones: mover la plataforma hacia la izquierda, mover hacia la derecha o no hacer nada si ya est√° posicionada correctamente. </li><li>  La parte Act est√° en las declaraciones Move Paddle Left y Move Paddle Right.  Dependiendo del dise√±o del juego, pueden mover la plataforma al instante o a cierta velocidad. </li></ul><br>  Tales enfoques se denominan reactivos: hay un conjunto simple de reglas (en este caso, si las declaraciones en el c√≥digo) responden al estado actual del mundo y act√∫an. <br><br><h3>  √Årbol de decisiones </h3><br>  El ejemplo de Pong es en realidad igual al concepto formal de IA llamado √°rbol de decisi√≥n.  El algoritmo lo pasa para llegar a una "hoja", una decisi√≥n sobre qu√© acci√≥n tomar. <br><br>  Hagamos un diagrama de bloques del √°rbol de decisi√≥n para el algoritmo de nuestra plataforma: <br><br><img src="https://habrastorage.org/webt/yu/u8/nd/yuu8ndgkxfb0mj1qyfhht-vhrnw.png"><br><br>  Cada parte del √°rbol se llama nodo; la IA usa la teor√≠a de grafos para describir tales estructuras.  Hay dos tipos de nodos: <br><br><ul><li>  Nodos de decisi√≥n: elegir entre dos alternativas en funci√≥n de verificar una determinada condici√≥n en la que cada alternativa se presenta como un nodo separado. </li><li>  Nodos finales: una acci√≥n a realizar que representa la decisi√≥n final. </li></ul><br>  El algoritmo comienza con el primer nodo (la "ra√≠z" del √°rbol).  Decide a qu√© nodo hijo ir, o realiza una acci√≥n almacenada en el nodo y completa. <br><br>  ¬øCu√°l es la ventaja si el √°rbol de decisi√≥n hace el mismo trabajo que las declaraciones if en la secci√≥n anterior?  Aqu√≠ hay un sistema com√∫n donde cada soluci√≥n tiene solo una condici√≥n y dos resultados posibles.  Esto permite al desarrollador crear IA a partir de los datos que representan las decisiones en el √°rbol, evitando su codificaci√≥n.  Imagina en forma de tabla: <br><br><img src="https://habrastorage.org/webt/dt/mx/zg/dtmxzgddk1mo585bhzy7-x_espw.png"><br><br>  En el lado del c√≥digo, obtienes un sistema para leer cadenas.  Cree un nodo para cada uno de ellos, conecte la l√≥gica de decisi√≥n basada en la segunda columna y los nodos secundarios basados ‚Äã‚Äãen la tercera y cuarta columna.  Todav√≠a necesitas programar las condiciones y acciones, pero ahora la estructura del juego ser√° m√°s complicada.  En √©l, agrega decisiones y acciones adicionales, y luego configura toda la IA simplemente editando un archivo de texto con una definici√≥n de √°rbol.  A continuaci√≥n, transfiera el archivo al dise√±ador del juego, que puede cambiar el comportamiento sin volver a compilar el juego y cambiar el c√≥digo. <br><br>  Los √°rboles de decisi√≥n son muy √∫tiles cuando se crean autom√°ticamente en funci√≥n de un gran conjunto de ejemplos (por ejemplo, utilizando el algoritmo ID3).  Esto los convierte en una herramienta eficaz y de alto rendimiento para clasificar situaciones basadas en los datos recibidos.  Sin embargo, vamos m√°s all√° de un sistema simple para que los agentes seleccionen acciones. <br><br><h3>  Escenarios </h3><br>  Desmontamos un sistema de √°rbol de decisi√≥n que utilizaba condiciones y acciones pre-creadas.  El dise√±ador de IA puede organizar el √°rbol de la manera que quiera, pero a√∫n tiene que confiar en el codificador que lo program√≥ todo.  ¬øQu√© pasar√≠a si pudi√©ramos darle al dise√±ador herramientas para crear nuestras propias condiciones o acciones? <br><br>  Para evitar que el programador escriba c√≥digo para las condiciones Is Ball Left Of Paddle y Is Ball Right Of Paddle, puede crear un sistema en el que el dise√±ador registre las condiciones para verificar estos valores.  Entonces los datos del √°rbol de decisi√≥n se ver√°n as√≠: <br><br><img src="https://habrastorage.org/webt/o9/nw/pe/o9nwpet07f-6xi7crzt5u34-orm.png"><br><br>  En esencia, esto es lo mismo que en la primera tabla, pero las soluciones dentro de s√≠ mismas tienen su propio c√≥digo, un poco similar a la parte condicional de la instrucci√≥n if.  En el lado del c√≥digo, esto se leer√≠a en la segunda columna para los nodos de decisi√≥n, pero en lugar de buscar una condici√≥n espec√≠fica para cumplir (Is Ball Left Of Paddle), eval√∫a la expresi√≥n condicional y devuelve verdadero o falso, respectivamente.  Esto se hace usando el lenguaje de script Lua o Angelscript.  Al usarlos, el desarrollador puede tomar objetos en su juego (pelota y paleta) y crear variables que estar√°n disponibles en el gui√≥n (bola.posici√≥n).  Adem√°s, el lenguaje de secuencias de comandos es m√°s simple que C ++.  No requiere una etapa de compilaci√≥n completa, por lo tanto, es ideal para el ajuste r√°pido de la l√≥gica del juego y permite a los "no codificadores" crear las funciones necesarias por s√≠ mismos. <br><br>  En el ejemplo anterior, el lenguaje de secuencias de comandos se usa solo para evaluar una expresi√≥n condicional, pero tambi√©n se puede usar para acciones.  Por ejemplo, los datos de Move Paddle Right pueden convertirse en una declaraci√≥n de script (ball.position.x + = 10).  Para que la acci√≥n tambi√©n se defina en el script, sin la necesidad de programar Move Paddle Right. <br><br>  Puede ir a√∫n m√°s lejos y escribir un √°rbol de decisi√≥n completo en un lenguaje de script.  Este ser√° un c√≥digo en forma de declaraciones condicionales codificadas (codificadas), pero se ubicar√°n en archivos de script externos, es decir, se pueden cambiar sin recompilar todo el programa.  A menudo, puedes cambiar el archivo de script directamente durante el juego para probar r√°pidamente diferentes reacciones de IA. <br><br><h3>  Respuesta al evento </h3><br>  Los ejemplos anteriores son perfectos para Pong.  Continuamente ejecutan el ciclo Sense / Think / Act y act√∫an sobre la base del √∫ltimo estado del mundo.  Pero en los juegos m√°s complejos, debes responder a eventos individuales y no evaluar todo a la vez.  Pong ya es un ejemplo infructuoso.  Elige otro. <br><br>  Imagine un tirador donde los enemigos est√°n inm√≥viles hasta que encuentran al jugador, despu√©s de lo cual act√∫an dependiendo de su "especializaci√≥n": alguien correr√° para "aplastarse", alguien atacar√° desde lejos.  Este sigue siendo el sistema de respuesta b√°sico: "si se nota al jugador, entonces haga algo", pero se puede dividir l√≥gicamente en el evento Player Seen (se nota al jugador) y la reacci√≥n (seleccione la respuesta y ejec√∫tela). <br><br>  Esto nos lleva de vuelta al ciclo Sentido / Pensar / Actuar.  Podemos codificar la parte de detecci√≥n, que cada cuadro verificar√° para ver si la IA del jugador es visible.  Si no, no pasa nada, pero si se ve, entonces se genera el evento Player Seen.  El c√≥digo tendr√° una secci√≥n separada que dice: ‚Äúcuando ocurra el evento Player Seen, hazlo‚Äù, ¬ød√≥nde est√° la respuesta que necesitas para referirte a las partes Think and Act?  Por lo tanto, configurar√° reacciones al evento Player Seen: ChargeAndAttack para el personaje "creciente" y HideAndSnipe para el francotirador.  Estas relaciones se pueden crear en el archivo de datos para una edici√≥n r√°pida sin tener que volver a compilar.  Y aqu√≠ tambi√©n puedes usar el lenguaje de secuencias de comandos. <br><br><h2>  Tomando decisiones dif√≠ciles </h2><br>  Aunque los sistemas de reacci√≥n simples son muy efectivos, hay muchas situaciones en las que no son suficientes.  A veces es necesario tomar varias decisiones basadas en lo que el agente est√° haciendo en este momento, pero es dif√≠cil imaginar esto como una condici√≥n.  A veces hay demasiadas condiciones para representarlas efectivamente en un √°rbol de decisiones o script.  A veces es necesario evaluar previamente c√≥mo cambiar√° la situaci√≥n antes de decidir el siguiente paso.  Resolver estos problemas requiere enfoques m√°s sofisticados. <br><br><h3>  M√°quina de estados finitos </h3><br>  La m√°quina de estados finitos o FSM (m√°quina de estados) es una forma de decir que nuestro agente se encuentra actualmente en uno de varios estados posibles y que puede moverse de un estado a otro.  Hay un cierto n√∫mero de tales estados, de ah√≠ el nombre.  El mejor ejemplo de la vida es un sem√°foro.  En diferentes lugares, diferentes secuencias de luces, pero el principio es el mismo: cada estado representa algo (pararse, ir, etc.).  Un sem√°foro est√° solo en un estado en un momento dado y se mueve de uno a otro seg√∫n reglas simples. <br><br>  Con los NPC en los juegos, una historia similar.  Por ejemplo, tenga cuidado con las siguientes condiciones: <br><br><ul><li>  Patrullando </li><li>  Atacando </li><li>  Huyendo </li></ul><br>  Y tales condiciones para cambiar su condici√≥n: <br><br><ul><li>  Si el guardia ve al enemigo, ataca. </li><li>  Si el guardia ataca, pero ya no ve al enemigo, vuelve a patrullar. </li><li>  Si el guardia ataca, pero est√° gravemente herido, huye. </li></ul><br>  Tambi√©n puede escribir sentencias if con una variable de estado de guardia y varias comprobaciones: ¬øhay un enemigo cerca, cu√°l es el nivel de salud del NPC, etc. Agreguemos algunos estados m√°s: <br><br><ul><li>  Inacci√≥n (inactivo): entre patrullas. </li><li>  Buscar (Buscando) - cuando el enemigo notado desapareci√≥. </li><li>  Pide ayuda (Encontrar ayuda): cuando se ve al enemigo, pero es demasiado fuerte para luchar solo con √©l. </li></ul><br>  La elecci√≥n para cada uno de ellos es limitada: por ejemplo, un guardia no ir√° a buscar un enemigo oculto si tiene mala salud. <br><br>  Al final, la enorme lista de "si &lt;x e y, pero no z&gt;, entonces &lt;p&gt;" puede volverse demasiado engorrosa, por lo que deber√≠amos formalizar un m√©todo que nos permita tener en cuenta los estados y las transiciones entre estados.  Para hacer esto, tenemos en cuenta todos los estados, y en cada estado, enumeramos todas las transiciones a otros estados, junto con las condiciones necesarias para ellos. <br><br><img src="https://habrastorage.org/webt/ut/25/j2/ut25j2aky0lx_ajk_rf2eeilgei.png"><br><br>  Esta tabla de transici√≥n de estado es una forma integral de representar FSM.  Dibujemos un diagrama y obtengamos una visi√≥n general completa de c√≥mo cambia el comportamiento de los NPC. <br><br><img src="https://habrastorage.org/webt/fg/7u/so/fg7uso5gla8wi4-fry0qne_bfvy.png"><br><br>  El cuadro refleja la esencia de la toma de decisiones para este agente en funci√≥n de la situaci√≥n actual.  Adem√°s, cada flecha muestra una transici√≥n entre estados si la condici√≥n al lado es verdadera. <br><br>  En cada actualizaci√≥n, verificamos el estado actual del agente, miramos la lista de transiciones y, si se cumplen las condiciones para la transici√≥n, adquiere un nuevo estado.  Por ejemplo, cada cuadro verifica si el temporizador de 10 segundos ha expirado, y si es as√≠, el guardia cambia de Ralent√≠ a Patrullaje.  De la misma manera, el estado de ataque verifica la salud del agente; si es bajo, pasa al estado de Huida. <br><br>  Esto es manejar las transiciones de estado, pero ¬øqu√© pasa con el comportamiento asociado con los estados mismos?  Con respecto a la implementaci√≥n del comportamiento real para un estado en particular, generalmente hay dos tipos de "ganchos" donde asignamos acciones al FSM: <br><br><ul><li>  Acciones que realizamos peri√≥dicamente para el estado actual. </li><li>  Las acciones que tomamos al pasar de un estado a otro. </li></ul><br>  Ejemplos para el primer tipo.  Estado de patrullaje Cada cuadro mover√° al agente a lo largo de la ruta de patrullaje.  Estado de ataque cada cuadro intentar√° iniciar un ataque o entrar en un estado cuando sea posible. <br><br>  Para el segundo tipo, considere la transici√≥n ‚Äúsi el enemigo es visible y el enemigo es demasiado fuerte, entonces vaya al estado Encontrar Ayuda.  El agente debe elegir a d√≥nde ir para obtener ayuda y guardar esta informaci√≥n para que el estado de B√∫squeda de ayuda sepa a d√≥nde ir.  Tan pronto como se encuentra ayuda, el agente vuelve al estado de ataque.  En este punto, querr√° contarle al aliado sobre la amenaza, por lo que puede ocurrir la acci√≥n NotifyFriendOfThreat. <br><br>  Y nuevamente, podemos ver este sistema a trav√©s del prisma del ciclo Sentido / Pensar / Actuar.  El sentido se traduce en datos utilizados por la l√≥gica de transici√≥n.  Pensar: transiciones disponibles en cada estado.  Y la Ley se lleva a cabo mediante acciones realizadas peri√≥dicamente dentro del estado o en transiciones entre estados. <br><br>  A veces, el sondeo continuo de las condiciones de transici√≥n puede ser costoso.  Por ejemplo, si cada agente realizar√° c√°lculos complejos en cada cuadro para determinar si ve enemigos y comprende si es posible pasar de Patrullar a Atacar, esto requerir√° mucho tiempo del procesador. <br><br>  Los cambios importantes en el estado del mundo pueden considerarse eventos que se procesar√°n a medida que ocurran.  En lugar de que FSM verifique la condici√≥n de transici√≥n "¬øpuede mi agente ver al jugador?" En cada cuadro, puede configurar un sistema separado para realizar verificaciones con menos frecuencia (por ejemplo, 5 veces por segundo).  Y el resultado es dar al jugador visto cuando pasa el cheque. <br><br>  Esto se pasa al FSM, que ahora tiene que entrar en la condici√≥n recibida del evento Player Seen y reaccionar en consecuencia.  El comportamiento resultante es el mismo, excepto por un retraso casi imperceptible antes de responder.  Pero el rendimiento mejor√≥ como resultado de la separaci√≥n de parte de Sense en una parte separada del programa. <br><br><h3>  M√°quina jer√°rquica de estados finitos </h3><br>  Sin embargo, trabajar con FSM grandes no siempre es conveniente.  Si queremos expandir el estado del ataque, reemplaz√°ndolo con MeleeAttacking (cuerpo a cuerpo) y RangedAttacking (a distancia) separados, tendremos que cambiar las transiciones de todos los dem√°s estados que conducen al estado de Ataque (actual y futuro). <br><br>  Seguramente not√≥ que en nuestro ejemplo hay muchas transiciones duplicadas.  La mayor√≠a de las transiciones en el estado inactivo son id√©nticas a las transiciones en el estado de patrullaje.  Ser√≠a bueno no repetir, especialmente si agregamos m√°s estados similares.  Tiene sentido agrupar a Idling and Patrolling bajo la etiqueta com√∫n "no combate", donde solo hay un conjunto com√∫n de transiciones para combatir estados.  Si presentamos esta etiqueta como un estado, entonces Idling and Patrolling se convertir√°n en subestados.  Un ejemplo de uso de una tabla de conversi√≥n separada para un nuevo subestado que no es de combate: <br><br>  <i>Las principales condiciones:</i> <br><img src="https://habrastorage.org/webt/jk/6j/u6/jk6ju6k3dtxhbou06sqelhixykm.png"><br><br>  <i>Estado fuera de combate:</i> <br><img src="https://habrastorage.org/webt/b4/yn/ae/b4ynaedk42xhvikbzqjsbh9itgc.png"><br><br>  Y en forma de gr√°fico: <br><br><img src="https://habrastorage.org/webt/ni/li/dv/nilidvj4kqtrime1pgyzv11gh10.png"><br><br>  Este es el mismo sistema, pero con un nuevo estado de no combate, que incluye Idling and Patrolling.  Con cada estado que contiene FSM con subestados (y estos subestados, a su vez, contienen sus propios FSM, y as√≠ sucesivamente, todo lo que necesite), obtenemos una M√°quina de estado finito jer√°rquico o HFSM (m√°quina de estado jer√°rquico).  Habiendo agrupado un estado que no es de combate, cortamos un mont√≥n de transiciones redundantes.  Podemos hacer lo mismo para cualquier estado nuevo con transiciones comunes.  Por ejemplo, si en el futuro extendemos el estado de ataque a los estados de ataque cuerpo a cuerpo y ataque de misiles, ser√°n subestaciones que se cruzan entre s√≠ en funci√≥n de la distancia al enemigo y la presencia de municiones.  Como resultado, los modelos de comportamiento complejos y los submodelos de comportamiento pueden representarse con un m√≠nimo de transiciones duplicadas. <br><br><h3>   </h3><br>  HFSM      .   ,   ,            .        ,  .           .    ,    ,            . ,      25%,  ,      ,     ,    ‚Äî        .            25%  10%,     . <br><br>       ,    ¬´   ¬ª,     ,            .    . <br><br>     ,           :    ¬´¬ª ,     ,   ,  .     : <br><br><ul><li>       : Succeeded (  ), Failed (  )  Running (        ). </li><li>         .    Decorator,      .   Succeed,      . </li><li> ,  ,   Running    . </li></ul><br>             .  HFSM        : <br><br><img src="https://habrastorage.org/webt/1i/e5/4i/1ie54izd-h-ybfpwrb-yowau724.png"><br><br>           Idling/Patrolling   Attacking   .   ,    ,     Fleeing,   ,      ‚Äî Patrolling, Idling, Attacking   . <br><br><img src="https://habrastorage.org/webt/iw/je/uu/iwjeuuax51z5qxgmg5h4zcbm4tg.png"><br><br>    ‚Äî     ,            .     ,     ‚Äî         ,     ?   ,    ‚Äî  ,      Idling   10    ,      ,    ? <br><br>     . ,        .        ,        . <br><br><h3> Utility-based system </h3><br>       . ,          ,        .  ,         ,           . <br><br> Utility-based system (,   )     .  ,      ,      ,     .   ‚Äî   ,         . <br><br>         ,            .    FSM,   ,        ,  .  ,         ( ,    ).       ,      . <br><br>       ‚Äî ,  0 ( )  100 ( ).      ,     .      : <br><br><img src="https://habrastorage.org/webt/ty/an/id/tyanidrfvaoee_ekrdretfmyqhg.png"><br><br>     ‚Äî       .       .   ,    ,    ,   Fleeing,  FindingHelp    .   FindingHelp   .  ,       50,      .          . <br><br>         ,      .          . ,  Fleeing     ,    ,   Attacking   ,    . -   Fleeing    Attacking   ,   ,         .          ,        ,     FSM. <br><br>        .            .  The Sims,     ,     ‚Äî    ¬´¬ª,    .   ,        ,     EatFood     ,     ,   ,    EatFood  . <br><br>         ,  Utility-based system        ,      .             .  ,       Utility    ,  ,    . <br><br><h2>    </h2><br>       ,      ,  ,    .            ?    ,    ,     ,      ,     ?   . <br><br><h3>  Gesti√≥n </h3><br>     ,      ,    ,        .        ,   ,     . .   Sense/Think/Act,   ,   Think  ,   Act     .      ,      ,        .        ‚Äî ,     .  ,    ,          .   : <br><br> <i>desired_travel = destination_position ‚Äì agent_position</i> <br><br>  2D-.     (-2,-2),   -  -   (30, 20),     ,    ‚Äî (32, 22). ,      ‚Äî      5   ,            (4.12, 2.83).            8 . <br><br>      .       ,     ,       5 / (   ),   .      ,         . <br><br>      ‚Äî ,   ,   ,        .        .     steering behaviours,      : Seek (), Flee (), Arrival ()  . .    ,         ,           ,       . <br><br>      . Seek  Arrival ‚Äî       . Obstacle Avoidance ( )  Separation ()   ,       . Alignment ()  Cohesion ()     .    steering behaviours            . ,   Arrival, Separation  Obstacle Avoidance,        .          . <br><br>    ,      ‚Äî  ,      -  Arrival  Obstacle Avoidance.    ,  ,     .  :     ,          . <br><br>        ,    ,   -   . <br><br><h3>   </h3><br> Steering behaviours         (   ),       ‚Äî        .      pathfinding ( ),            . <br><br>   ‚Äî                .  -     ,           ,     .    .         ,          ( ,     ).  ,     Breadth-First Search  BFS (   ).         ( breadth, ¬´¬ª).      ,  ,      ‚Äî         ,       ,       . <br><br><img src="https://habrastorage.org/webt/ik/gg/_n/ikgg_n7oyigrgvo1av12jtsaga4.gif"><br><br>      ,     .     (, pathfinding) ‚Äî  ,   ,    . <br><br> ,        ,   steering behaviours,     ‚Äî   1   2,    2   3   .   ‚Äî     ,    ‚Äî         . -        . <br><br>   BFS    ‚Äî       ¬´¬ª ,   ¬´¬ª.        A* (A star).   ,     - (  ,       ),         ,      ,     .     ,     ‚Äî ¬´¬ª      (    )   ,        (    ). <br><br><img src="https://habrastorage.org/webt/3k/jb/al/3kjbal-iagj4ovxicix2swrbsxq.gif"><br><br>    ,        ,    ,    .    ,    BFS,        ‚Äî        . <br><br><h3>    </h3><br>  Pero la mayor√≠a de los juegos no se presentan en la grilla, y a menudo no se puede hacer sin comprometer el realismo.  Se necesitan compromisos.  ¬øDe qu√© tama√±o deben ser los cuadrados?  Demasiado grande, y no podr√°n imaginar correctamente corredores o giros peque√±os, demasiado peque√±os, habr√° demasiados cuadrados para buscar, lo que al final llevar√° mucho tiempo. <br><br>  Lo primero que hay que entender es que la cuadr√≠cula nos da un gr√°fico de nodos conectados.  Los algoritmos A * y BFS realmente funcionan en gr√°ficos y no se preocupan por nuestra cuadr√≠cula en absoluto.  Podr√≠amos colocar los nodos en cualquier parte del mundo del juego: si hay una conexi√≥n entre dos nodos conectados, as√≠ como entre los puntos de inicio y finalizaci√≥n y al menos uno de los nodos, el algoritmo funcionar√° igual de bien que antes.  Esto a menudo se llama el sistema de puntos de referencia, ya que cada nodo representa una posici√≥n significativa en el mundo, que puede ser parte de cualquier n√∫mero de caminos hipot√©ticos. <br><br><img src="https://habrastorage.org/webt/k7/ab/gq/k7abgqdwnx7efuqf8pqw0p6nbqm.png"><br>  <i>Ejemplo 1: un nudo en cada cuadrado.</i>  <i>La b√∫squeda comienza desde el nodo donde se encuentra el agente, y termina en el nodo del cuadrado deseado.</i> <br><br><img src="https://habrastorage.org/webt/m6/lx/5a/m6lx5a5wleqbvxthzcoajeat_us.png"><br>  <i>Ejemplo 2: un conjunto m√°s peque√±o de nodos (puntos de referencia).</i>  <i>La b√∫squeda comienza al cuadrado con el agente, pasa por el n√∫mero requerido de nodos y luego contin√∫a hasta el destino.</i> <br><br>  Este es un sistema completamente flexible y potente.  Pero necesita cierta precauci√≥n al decidir d√≥nde y c√≥mo colocar el punto de referencia; de lo contrario, los agentes simplemente no podr√°n ver el punto m√°s cercano y no podr√°n iniciar el camino.  Ser√≠a m√°s f√°cil si pudi√©ramos establecer autom√°ticamente puntos de referencia basados ‚Äã‚Äãen la geometr√≠a del mundo. <br><br>  Luego aparece una malla de navegaci√≥n o malla de navegaci√≥n.  Esta suele ser una malla 2D de tri√°ngulos que se superpone a la geometr√≠a del mundo, donde sea que el agente pueda caminar.  Cada uno de los tri√°ngulos en la cuadr√≠cula se convierte en un nodo en el gr√°fico y tiene hasta tres tri√°ngulos adyacentes que se convierten en nodos adyacentes en el gr√°fico. <br><br>  Esta imagen es un ejemplo del motor de Unity: analiz√≥ la geometr√≠a en el mundo y cre√≥ navmesh (azul claro en la captura de pantalla).  Cada pol√≠gono en navmesh es un √°rea en la que un agente puede pararse o moverse de un pol√≠gono a otro pol√≠gono.  En este ejemplo, los pol√≠gonos son m√°s peque√±os que los pisos en los que est√°n ubicados, hechos para tener en cuenta las dimensiones del agente, que ir√°n m√°s all√° de su posici√≥n nominal. <br><br><img src="https://habrastorage.org/webt/-y/ip/ic/-yipico3akqxyv8hhubbe0rzkgk.png"><br><br>  Podemos buscar la ruta a trav√©s de esta cuadr√≠cula, nuevamente usando el algoritmo A *.  Esto nos dar√° una ruta casi perfecta en el mundo que tiene en cuenta toda la geometr√≠a y no requiere nodos y puntos de referencia adicionales. <br><br>  Pathfinding es un tema demasiado extenso sobre cu√°l secci√≥n del art√≠culo no es suficiente.  Si desea estudiarlo con m√°s detalle, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sitio de Amit Patel</a> lo ayudar√°. <br><br><h2>  Planificacion </h2><br>  Con la b√∫squeda de ruta nos aseguramos de que a veces no basta con elegir una direcci√≥n y moverse, debemos elegir una ruta y hacer varios giros para llegar al destino deseado.  Podemos resumir esta idea: lograr el objetivo no es solo el siguiente paso, sino una secuencia completa, donde a veces es necesario mirar hacia adelante unos pocos pasos para descubrir cu√°l deber√≠a ser el primero.  Esto se llama planificaci√≥n.  Pathfinding se puede considerar como una de varias adiciones de planificaci√≥n.  Desde la perspectiva de nuestro ciclo Sentido / Pensar / Actuar, aqu√≠ es donde la parte Pensar planea varias partes Actuar para el futuro. <br><br>  Veamos el ejemplo del juego de mesa Magic: The Gathering.  Primero vamos con un juego de cartas en la mano: <br><br><ul><li>  Pantano: da 1 man√° negro (mapa de la tierra). </li><li>  Bosque: da 1 man√° verde (mapa de la tierra). </li><li>  Mago fugitivo: Requiere 1 man√° azul para invocar. </li><li>  M√≠stico √©lfico: requiere 1 man√° verde para invocar. </li></ul><br>  Ignoramos las tres cartas restantes para que sea m√°s f√°cil.  Seg√∫n las reglas, un jugador puede jugar 1 carta de tierra por turno, puede "tocar" esta carta para extraer man√° de ella y luego usar hechizos (incluidas las criaturas de invocaci√≥n) de acuerdo con la cantidad de man√°.  En esta situaci√≥n, el jugador humano sabe que necesitas jugar Forest, "tocar" 1 man√° verde y luego llamar a Elvish Mystic.  Pero, ¬øc√≥mo adivinas el juego AI? <br><br><h3>  Planificaci√≥n f√°cil </h3><br>  El enfoque trivial es probar cada acci√≥n por turnos hasta que haya otras adecuadas.  Mirando las cartas, la IA ve lo que Swamp puede jugar.  Y lo juega.  ¬øQuedan otras acciones este turno?  No puede invocar ni a Elvish Mystic ni a Fugitive Wizard, ya que su invocaci√≥n requiere respectivamente man√° verde y azul, y Swamp solo da man√° negro.  Y no podr√° jugar Forest, porque ya ha jugado Swamp.  Por lo tanto, el juego AI sigui√≥ las reglas, pero lo hizo mal.  Se puede mejorar. <br><br>  La planificaci√≥n puede encontrar una lista de acciones que llevan el juego al estado deseado.  As√≠ como cada cuadrado en el camino ten√≠a vecinos (en la b√∫squeda de caminos), cada acci√≥n en el plan tambi√©n tiene vecinos o sucesores.  Podemos buscar estas acciones y acciones posteriores hasta que alcancemos el estado deseado. <br><br>  En nuestro ejemplo, el resultado deseado es "convocar a una criatura, si es posible".  Al comienzo del movimiento, solo vemos dos acciones posibles permitidas por las reglas del juego: <br><br>  <i>1. Jugar Swamp (resultado: Swamp en el juego)</i> <i><br></i>  <i>2. Jugar Forest (resultado: Forest en el juego)</i> <br><br>  Cada acci√≥n tomada puede conducir a acciones adicionales y cerrar otras, nuevamente, dependiendo de las reglas del juego.  Imagina que jugamos a Swamp: esto eliminar√° Swamp como el siguiente paso (ya lo jugamos), tambi√©n eliminar√° Forest (porque seg√∫n las reglas puedes jugar un mapa de la tierra por turno).  Despu√©s de eso, la IA agrega como el siguiente paso: obtener 1 man√° negro, porque no hay otras opciones.  Si va m√°s all√° y elige Tap the Swamp, recibir√° 1 unidad de man√° negro y no puede hacer nada con √©l. <br><br>  <i>1. Jugar Swamp (resultado: Swamp en el juego)</i> <i><br></i>  <i>1.1 Pantano ‚ÄúTap‚Äù (resultado: Pantano ‚Äútap‚Äù, +1 unidad de man√° negro)</i> <i><br></i>  <i>No hay acciones disponibles - FIN</i> <i><br></i>  <i>2. Jugar Forest (resultado: Forest en el juego)</i> <br><br>  La lista de acciones fue corta, estamos en un punto muerto.  Repita el proceso para el siguiente paso.  Jugamos a Forest, abrimos la acci√≥n "get 1 green mana", que a su vez abrir√° la tercera acci√≥n: la llamada de Elvish Mystic. <br><br>  <i>1. Jugar Swamp (resultado: Swamp en el juego)</i> <i><br></i>  <i>1.1 Pantano ‚ÄúTap‚Äù (resultado: Pantano ‚Äútap‚Äù, +1 unidad de man√° negro)</i> <i><br></i>  <i>No hay acciones disponibles - FIN</i> <i><br></i>  <i>2. Jugar Forest (resultado: Forest en el juego)</i> <i><br></i>  <i>2.1 Bosque "Tap" (resultado: Bosque "tap", +1 unidad de man√° verde)</i> <i><br></i>  <i>2.1.1 Invocar m√≠stico √©lfico (resultado: m√≠stico √©lfico en el juego, -1 unidad de man√° verde)</i> <i><br></i>  <i>No hay acciones disponibles - FIN</i> <br><br>  Finalmente, examinamos todas las acciones posibles y encontramos un plan que llama a la criatura. <br><br>  Este es un ejemplo muy simplificado.  Es aconsejable elegir el mejor plan posible, y no uno que cumpla con algunos criterios.  Como regla general, puede evaluar los planes potenciales en funci√≥n del resultado final o los beneficios totales de su implementaci√≥n.  Puedes sumarte 1 punto por jugar un mapa de la tierra y 3 puntos por desafiar a una criatura.  Jugar a Swamp ser√≠a un plan que da 1 punto.  Y para jugar Forest ‚Üí Tap the Forest ‚Üí invoca a Elvish Mystic: inmediatamente dar√° 4 puntos. <br><br>  As√≠ es como funciona la planificaci√≥n en Magic: The Gathering, pero por la misma l√≥gica se aplica en otras situaciones.  Por ejemplo, mueva un pe√≥n para dejar espacio para que el alfil se mueva en el ajedrez.  O c√∫brete detr√°s de una pared para disparar con seguridad a XCOM as√≠.  En general, entiendes el punto. <br><br><h3>  Planificaci√≥n mejorada </h3><br>  A veces hay demasiadas acciones potenciales para considerar todas las opciones posibles.  Volviendo al ejemplo con Magic: The Gathering: digamos que en el juego y en tus manos hay varias cartas de tierra y criaturas: la cantidad de combinaciones posibles de movimientos puede estar en las decenas.  Hay varias soluciones al problema. <br><br>  La primera forma es encadenar hacia atr√°s.  En lugar de clasificar todas las combinaciones, es mejor comenzar con el resultado final e intentar encontrar una ruta directa.  En lugar del camino desde la ra√≠z del √°rbol hasta una hoja espec√≠fica, nos movemos en la direcci√≥n opuesta, desde la hoja hasta la ra√≠z.  Este m√©todo es m√°s simple y r√°pido. <br><br>  Si el oponente tiene 1 unidad de salud, puedes encontrar un plan para "infligir 1 o m√°s unidades de da√±o".  Para lograr esto, se deben cumplir una serie de condiciones: <br><br>  1. El da√±o puede ser causado por un hechizo, debe estar en la mano. <br>  2. Para lanzar un hechizo, necesitas man√°. <br>  3. Para obtener man√°, debes jugar una carta de tierra. <br>  4. Para jugar una carta de la tierra, debes tenerla en tu mano. <br><br>  Otra forma es la mejor b√∫squeda primero.  En lugar de recorrer todos los caminos, elegimos el m√°s adecuado.  Muy a menudo, este m√©todo ofrece un plan √≥ptimo sin costos de b√∫squeda innecesarios.  A * es la forma de la mejor primera b√∫squeda: explorando las rutas m√°s prometedoras desde el principio, ya puede encontrar la mejor manera sin tener que consultar otras opciones. <br><br>  Una opci√≥n interesante y cada vez m√°s popular para la mejor b√∫squeda es Monte Carlo Tree Search.  En lugar de adivinar qu√© planes son mejores que otros al elegir cada acci√≥n posterior, el algoritmo selecciona sucesores aleatorios en cada paso hasta llegar al final (cuando el plan condujo a la victoria o la derrota).  Luego, el resultado final se utiliza para aumentar o disminuir la clasificaci√≥n de peso de las opciones anteriores.  Repitiendo este proceso varias veces seguidas, el algoritmo da una buena estimaci√≥n de qu√© pr√≥ximo paso es mejor, incluso si la situaci√≥n cambia (si el oponente toma medidas para evitar que el jugador). <br><br>  La historia sobre la planificaci√≥n en los juegos no funcionar√° sin la Planificaci√≥n de Acci√≥n Orientada a Objetivos o GOAP (planificaci√≥n de acci√≥n orientada a objetivos).  Este es un m√©todo ampliamente utilizado y discutido, pero adem√°s de algunos detalles distintivos, es esencialmente el m√©todo de encadenamiento hacia atr√°s del que hablamos anteriormente.  Si la tarea era "destruir al jugador", y el jugador est√° detr√°s de la cobertura, el plan puede ser este: destruir con una granada ‚Üí obtenerlo ‚Üí soltarlo. <br><br>  Por lo general, hay varios objetivos, cada uno con su propia prioridad.  Si el objetivo con la prioridad m√°s alta no se puede completar (ninguna combinaci√≥n de acciones crea un plan para "destruir al jugador" porque el jugador no es visible), la IA volver√° a los objetivos con una prioridad m√°s baja. <br><br><h2>  Entrenamiento y adaptaci√≥n </h2><br>  Ya dijimos que la IA de los juegos generalmente no usa el aprendizaje autom√°tico porque no es adecuada para administrar agentes en tiempo real.  Pero esto no significa que no pueda pedir prestado nada de esta √°rea.  Queremos tal adversario en un tirador del que podamos aprender algo.  Por ejemplo, descubra las mejores posiciones en el mapa.  O un adversario en un juego de lucha que bloquear√≠a los trucos combinados de uso frecuente del jugador, motivando a otros a usar.  Por lo tanto, el aprendizaje autom√°tico en tales situaciones puede ser muy √∫til. <br><br><h3>  Estad√≠sticas y probabilidades </h3><br>  Antes de pasar a ejemplos complejos, calcularemos hasta d√≥nde podemos llegar tomando algunas medidas simples y us√°ndolas para tomar decisiones.  Por ejemplo, una estrategia en tiempo real: ¬øc√≥mo podemos determinar si un jugador puede lanzar un ataque en los primeros minutos de un juego y qu√© defensa preparar contra esto?  Podemos estudiar la experiencia pasada del jugador para comprender cu√°l podr√≠a ser la reacci√≥n futura.  Para empezar, no tenemos esos datos iniciales, pero podemos recopilarlos: cada vez que la IA juega contra una persona, √©l puede registrar el momento del primer ataque.  Despu√©s de varias sesiones, obtendremos el tiempo promedio durante el cual el jugador atacar√° en el futuro. <br><br>  Los valores promedio tienen un problema: si un jugador "decide" 20 veces y juega lentamente 20 veces, los valores necesarios estar√°n en alg√∫n punto intermedio, y esto no nos dar√° nada √∫til.  Una soluci√≥n es limitar la entrada: puede considerar las √∫ltimas 20 piezas. <br><br>  Se utiliza un enfoque similar para evaluar la probabilidad de ciertas acciones, suponiendo que las preferencias pasadas del jugador ser√°n las mismas en el futuro.  Si un jugador nos ataca cinco veces con una bola de fuego, dos veces con un rayo y una vez con un combate cuerpo a cuerpo, es obvio que prefiere una bola de fuego.  Extrapolamos y vemos la probabilidad de usar varias armas: bola de fuego = 62.5%, rayo = 25% y cuerpo a cuerpo = 12.5%.  Nuestro juego AI necesita prepararse para la protecci√≥n contra incendios. <br><br>  Otro m√©todo interesante es utilizar el clasificador Bayesiano ingenuo (clasificador bayesiano ingenuo) para estudiar grandes vol√∫menes de datos de entrada y clasificar la situaci√≥n para que la IA responda de la manera correcta.  Los clasificadores bayesianos son m√°s conocidos por usar filtros de correo no deseado.  All√≠, investigan palabras, las comparan con el lugar donde aparecieron estas palabras antes (en correo no deseado o no) y sacan conclusiones sobre las cartas entrantes.  Podemos hacer lo mismo incluso con menos entrada.  Sobre la base de toda la informaci√≥n √∫til que ve la IA (por ejemplo, qu√© unidades enemigas se crean, qu√© hechizos usan o qu√© tecnolog√≠as exploraron), y el resultado final (guerra o paz, "aplastar" o defender, etc.) - Seleccionaremos el comportamiento de IA deseado. <br><br>  Todos estos m√©todos de entrenamiento son suficientes, pero es recomendable usarlos en base a los datos de las pruebas.  AI aprender√° c√≥mo adaptarse a las diversas estrategias que han utilizado tus evaluadores de juego.  Una IA que se adapta a un jugador despu√©s de un lanzamiento puede volverse demasiado predecible, o viceversa, demasiado complejo para ganar. <br><br><h3>  Adaptaci√≥n Basada en el Valor </h3><br>  Dado el contenido de nuestro mundo de juego y las reglas, podemos cambiar el conjunto de valores que afectan la toma de decisiones, y no solo usar los datos de entrada.  Hacemos esto: <br><br><ul><li>  Deje que la IA recopile datos sobre el estado del mundo y los eventos clave durante el juego (como se indic√≥ anteriormente). </li><li>  Cambiemos algunos valores importantes basados ‚Äã‚Äãen estos datos. </li><li>  Nos damos cuenta de nuestras decisiones basadas en el procesamiento o la evaluaci√≥n de estos valores. </li></ul><br>  Por ejemplo, un agente tiene varias salas para elegir un tirador en primera persona en el mapa.  Cada habitaci√≥n tiene su propio valor, que determina qu√© tan deseable es visitarla.  La IA elige aleatoriamente a qu√© habitaci√≥n ir en funci√≥n del valor del valor.  Luego, el agente recuerda en qu√© habitaci√≥n fue asesinado y reduce su valor (la probabilidad de que regrese all√≠).  De manera similar para la situaci√≥n inversa: si el agente destruye a muchos oponentes, entonces el valor de la habitaci√≥n aumenta. <br><br><h3>  Modelo de Markov </h3><br>  ¬øQu√© pasa si usamos los datos recopilados para el pron√≥stico?  Si recordamos cada habitaci√≥n en la que vemos al jugador durante un cierto per√≠odo de tiempo, predeciremos a qu√© habitaci√≥n puede entrar el jugador.  Al rastrear y registrar el movimiento del jugador en las salas (valores), podemos predecirlos. <br><br>  Tomemos tres habitaciones: rojo, verde y azul.  Adem√°s de las observaciones que registramos al ver una sesi√≥n de juego: <br><br><img src="https://habrastorage.org/webt/cz/qm/-k/czqm-khdlmnybf2c4amu6v_yrc0.png"><br><br>  El n√∫mero de observaciones para cada habitaci√≥n es casi igual: todav√≠a no sabemos d√≥nde hacer un buen lugar para una emboscada.  La recopilaci√≥n de estad√≠sticas tambi√©n se complica por la reaparici√≥n de jugadores que aparecen de manera uniforme en todo el mapa.  Pero los datos de la siguiente habitaci√≥n, que ingresan despu√©s de aparecer en el mapa, ya son √∫tiles. <br><br>  Se puede ver que la sala verde se adapta a los jugadores: la mayor√≠a de las personas de rojo van a ella, el 50% de las cuales permanece all√≠ y m√°s.  La sala azul, por el contrario, no es popular, casi nunca se visita, y si lo es, no se demora. <br><br>  Pero los datos nos dicen algo m√°s importante: cuando el jugador est√° en la habitaci√≥n azul, la siguiente habitaci√≥n en la que probablemente lo veremos ser√° roja, no verde.  A pesar de que la sala verde es m√°s popular que la roja, la situaci√≥n cambia si el jugador est√° en azul.  El siguiente estado (es decir, la sala en la que entrar√° el jugador) depende del estado anterior (es decir, la sala en la que se encuentra el jugador ahora).  Debido al estudio de las dependencias, haremos pron√≥sticos con mayor precisi√≥n que si simplemente calcul√°ramos las observaciones independientemente el uno del otro. <br><br>  La predicci√≥n de un estado futuro basado en datos de estados pasados ‚Äã‚Äãse llama modelo de Markov, y tales ejemplos (con habitaciones) se llaman cadenas de Markov.  Dado que los modelos representan la probabilidad de cambios entre estados sucesivos, se muestran visualmente como FSM con una probabilidad cercana a cada transici√≥n.  Anteriormente, utilizamos FSM para representar el estado de comportamiento en el que se encontraba el agente, pero este concepto se aplica a cualquier estado, independientemente de si est√° relacionado con el agente o no.  En este caso, los estados representan la habitaci√≥n ocupada por el agente: <br><br><img src="https://habrastorage.org/webt/xj/jn/zn/xjjnznthergzfs89ixqdghp7bjq.png"><br><br>  Esta es una versi√≥n simple de la representaci√≥n de la probabilidad relativa de cambios en los estados, dando a la IA alguna oportunidad de predecir el siguiente estado.  Puedes predecir algunos pasos hacia adelante. <br><br>  Si el jugador est√° en la sala verde, hay un 50% de posibilidades de que permanezca all√≠ durante la pr√≥xima observaci√≥n.  Pero, ¬øcu√°l es la probabilidad de que √©l siga all√≠ incluso despu√©s?  No solo existe la posibilidad de que el jugador permanezca en la sala verde despu√©s de dos observaciones, sino tambi√©n la posibilidad de que se vaya y regrese.  Aqu√≠ est√° la nueva tabla con los nuevos datos: <br><br><img src="https://habrastorage.org/webt/te/wc/ya/tewcyajzsv_ys-9bro4mjdhtpte.png"><br><br>  Muestra que la probabilidad de ver a un jugador en la sala verde despu√©s de dos observaciones ser√° del 51% - 21%, que vendr√° de la sala roja, 5% de ellos, que el jugador visitar√° la sala azul entre ellos, y 25%, que el jugador no saldr√° de la sala verde. <br><br>  Una tabla es solo una herramienta visual: un procedimiento solo requiere una multiplicaci√≥n de probabilidades en cada paso.  Esto significa que puede mirar hacia el futuro con una enmienda: suponemos que la posibilidad de ingresar a una habitaci√≥n depende completamente de la habitaci√≥n actual.  Esto se llama la propiedad de Markov: el estado futuro depende solo del presente.  Pero esto no es completamente exacto.  Los jugadores pueden tomar decisiones dependiendo de otros factores: nivel de salud o cantidad de municiones.  Como no fijamos estos valores, nuestros pron√≥sticos ser√°n menos precisos. <br><br><h3>  N-gramos </h3><br>         - ?   !      ,     ,    -. <br><br>      ‚Äî    (, Kick, Punch  Block)         . ,    Kick, Kick, Punch,    SuperDeathFist,           ,    . <br><br><img src="https://habrastorage.org/webt/2m/ji/l4/2mjil4fdhjrro3em8vde-zcxhak.png"><br> (  ,     SuperDeathFist.) <br><br>    ,    Kick,    Kick,   ,     Punch.     - SuperDeathFist   ,   . <br><br>     N- (N-grams),  N ‚Äî   .      3- (),  :       .   5-        . <br><br>      N-.   N   ,     . , 2- ()   Kick, Kick  Kick, Punch,     Kick, Kick, Punch,       SuperDeathFist. <br><br>   ,          ,       .        Kick, Punch  Block,    10-,    60   . <br><br>       ‚Äî   ¬´ / ¬ª  ,         . 3-    N-      ,    (   N-)    ,    ‚Äî .         Kick  Kick   Kick  Punch.        , ,  ,       .     ,          ,  -  . <br><br><h2>  Conclusi√≥n </h2><br>            .    ,          . <br><br>           . ,  ,     .   ,     : <br><br><ul><li>   ,    ,      </li><li>   / (minimax  alpha-beta pruning) </li><li>   (,      ) </li><li>        </li><li>     ( ,        ) </li><li>   (   ) </li><li>   ( ,  anytime,  timeslicing) </li></ul><br> -  : <br><br> 1.  GameDev.net  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">      </a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> . <br> 2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AiGameDev.com</a>             . <br> 3. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">The GDC Vault</a>       GDC AI,     . <br> 4.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Game Programmers Guild</a> . <br> 5.  ,     ,    YouTube- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI and Games</a>        . <br><br>   : <br><br> 1.   Game AI Pro     , ,         . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=http://go.gamedev.net/%3Fid%3D13722X707581%26xs%3D1%26isjs%3D1%26url%3Dhttps%253A%252F%252Famzn.to%252F2KGoB8n%26xguid%3Df8ad586e5984991508efff4754027dbd%26xuuid%3D305451ecead59d76ca830fded0aab276%26xsessid%3D6ccb8b9fa3f10b478b65f7ed703a447b%26xcreo%3D0%26xed%3D0%26sref%3Dhttps%253A%252F%252Fwww.gamedev.net%252Farticles%252Fprogramming%252Fartificial-intelligence%252Fthe-total-beginners-guide-to-game-ai-r4942%252F%253Fdo%253Dedit%2526d%253D1%2526id%253D4942%2526csrfKey%253D7015c6d2c5c643e87baa74f8e5d2c094%26pref%3D">Game AI Pro: Collected Wisdom of Game AI Professionals</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=http://go.gamedev.net/%3Fid%3D13722X707581%26xs%3D1%26isjs%3D1%26url%3Dhttps%253A%252F%252Famzn.to%252F2KFKyoe%26xguid%3Df8ad586e5984991508efff4754027dbd%26xuuid%3D305451ecead59d76ca830fded0aab276%26xsessid%3D6ccb8b9fa3f10b478b65f7ed703a447b%26xcreo%3D0%26xed%3D0%26sref%3Dhttps%253A%252F%252Fwww.gamedev.net%252Farticles%252Fprogramming%252Fartificial-intelligence%252Fthe-total-beginners-guide-to-game-ai-r4942%252F%253Fdo%253Dedit%2526d%253D1%2526id%253D4942%2526csrfKey%253D7015c6d2c5c643e87baa74f8e5d2c094%26pref%3D">Game AI Pro 2: Collected Wisdom of Game AI Professionals</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Game AI Pro 3: Collected Wisdom of Game AI Professionals</a> <br><br> 2.  AI Game Programming Wisdom ‚Äî   Game AI Pro.     ,      . <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Game Programming Wisdom 1</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Game Programming Wisdom 2</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Game Programming Wisdom 3</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Game Programming Wisdom 4</a> <br><br> 3. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Artificial Intelligence: A Modern Approach</a> ‚Äî              .       ‚Äî     . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428892/">https://habr.com/ru/post/es428892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428878/index.html">Pig Flight u optimizaci√≥n de int√©rpretes de bytecode</a></li>
<li><a href="../es428880/index.html">Nuevos m√©todos de autenticaci√≥n: ¬øuna amenaza para la privacidad?</a></li>
<li><a href="../es428882/index.html">Mobile Yandex. Blitz: analizamos tareas</a></li>
<li><a href="../es428888/index.html">qml: poder y simplicidad</a></li>
<li><a href="../es428890/index.html">Toda la verdad sobre RTOS. Art√≠culo # 18. Grupos de banderas de eventos: servicios auxiliares y estructuras de datos</a></li>
<li><a href="../es428894/index.html">IVA en compras nacionales</a></li>
<li><a href="../es428896/index.html">Censura hentai redes neuronales</a></li>
<li><a href="../es428898/index.html">Aspectos problem√°ticos de la programaci√≥n en C ++</a></li>
<li><a href="../es428900/index.html">Los robots con ruedas comienzan a entregar productos a los residentes de los Estados Unidos y Gran Breta√±a</a></li>
<li><a href="../es428902/index.html">Etiquetas inal√°mbricas NFC</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>