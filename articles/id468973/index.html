<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¥‡ ğŸ•ºğŸ¼ ğŸ…°ï¸ Jaringan saraf untuk mengklasifikasikan gambar satelit menggunakan Tensorflow in Python ğŸš¶ğŸ¿ ğŸ”« â˜•ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ini adalah instruksi langkah demi langkah untuk klasifikasi citra multispektral dari satelit Landsat 5. Saat ini, di sejumlah area, pembelajaran menda...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jaringan saraf untuk mengklasifikasikan gambar satelit menggunakan Tensorflow in Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/468973/"><img src="https://habrastorage.org/webt/uq/qc/uz/uqqcuz2dhxwps1cueikg6ceigja.jpeg"><br><br>  Ini adalah instruksi langkah demi langkah untuk klasifikasi citra multispektral dari satelit Landsat 5. Saat ini, di sejumlah area, pembelajaran mendalam mendominasi sebagai alat untuk memecahkan masalah yang kompleks, termasuk yang geospasial.  Saya harap Anda terbiasa dengan dataset satelit, khususnya, Landsat 5 TM.  Jika Anda sedikit terbiasa dengan algoritma pembelajaran mesin, ini akan membantu Anda dengan cepat mempelajari manual ini.  Dan bagi mereka yang tidak mengerti, itu akan cukup untuk mengetahui bahwa, pada kenyataannya, pembelajaran mesin terdiri dalam membangun hubungan antara beberapa karakteristik (seperangkat atribut X) dari suatu objek dengan properti lainnya (nilai atau label, variabel target Y).  Kami memberi makan model dengan banyak objek yang karakteristik dan nilai indikator target / kelas objek (data berlabel) diketahui dan melatihnya sehingga dapat memprediksi nilai variabel target Y untuk data baru (tidak ditandai). <br><a name="habracut"></a><br>  Apa masalah utama dengan citra satelit? <br><br>  Dua atau lebih kelas objek (misalnya, bangunan, tanah kosong, dan lubang pondasi) dalam citra satelit dapat memiliki karakteristik spektral nilai yang sama, oleh karena itu, dalam dua puluh tahun terakhir, klasifikasi mereka merupakan tugas yang sulit. <br><br>  Karena itu, dimungkinkan untuk menggunakan model pembelajaran mesin klasik dengan dan tanpa guru, tetapi kualitasnya akan jauh dari ideal.  Mereka selalu memiliki kekurangan yang sama.  Pertimbangkan sebuah contoh: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/15b/16c/5b7/15b16c5b76906f81f27374daa8558806.jpg"><br><br>  Jika Anda menggunakan garis vertikal sebagai classifier dan memindahkannya di sepanjang sumbu X, maka mengklasifikasikan gambar rumah tidak akan mudah.  Data didistribusikan sehingga tidak mungkin untuk memisahkan mereka ke dalam kelas menggunakan satu garis vertikal (dalam kasus seperti itu dikatakan bahwa "objek dari kelas yang berbeda tidak dapat dipisahkan secara linear").  Tetapi ini tidak berarti bahwa rumah tidak dapat diklasifikasikan sama sekali! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cc4/ea7/7a0/cc4ea77a0f9f4713ea6a49b7885d31c2.gif"><br><br>  Mari kita gunakan garis merah untuk memisahkan dua kelas.  Dalam hal ini, penggolong mengidentifikasi sebagian besar rumah, tetapi satu rumah tidak ditugaskan ke kelasnya, dan tiga pohon lagi secara keliru ditugaskan ke "rumah".  Agar tidak ketinggalan satu rumah, Anda dapat menggunakan classifier dalam bentuk garis biru.  Maka semuanya akan dibahas di rumah, yaitu, kita katakan bahwa metrik recall (kepenuhan) tinggi.  Namun, tidak semua nilai yang diklasifikasikan ternyata rumah, yaitu, pada saat yang sama kami mendapat nilai rendah dari metrik presisi.  Jika kita menggunakan garis hijau, maka semua gambar yang diklasifikasikan sebagai rumah akan benar-benar rumah, yaitu, penggolong akan menunjukkan akurasi yang tinggi.  Dalam hal ini, kepenuhannya akan lebih sedikit, karena ketiga rumah tersebut akan dilupakan.  Dalam kebanyakan kasus, kita harus menemukan kompromi antara akurasi dan kelengkapan. <br><br>  Masalah rumah dan pohon ini mirip dengan masalah bangunan, tanah kosong, dan lubang.  Prioritas metrik klasifikasi citra satelit dapat bervariasi tergantung pada tugas.  Misalnya, jika Anda perlu memastikan bahwa semua wilayah yang dibangun diklasifikasikan sebagai bangunan tanpa kecuali, dan Anda siap untuk memasang piksel kelas lain dengan tanda tangan serupa, yang juga akan diklasifikasikan sebagai bangunan, maka Anda akan memerlukan model dengan kelengkapan tinggi.  Dan jika lebih penting bagi Anda untuk mengklasifikasikan bangunan, tanpa menambahkan piksel dari kelas lain, dan Anda siap untuk meninggalkan klasifikasi wilayah campuran, maka pilihlah classifier dengan akurasi tinggi.  Dalam kasus rumah dan pohon, model yang biasa akan menggunakan garis merah, menjaga keseimbangan antara akurasi dan kelengkapan. <br><br><h2>  Data yang digunakan </h2><br>  Sebagai tanda, kita akan menggunakan nilai enam rentang (pita 2 - pita 7) gambar dari Landsat 5 TM, dan mencoba memprediksi kelas pengembangan biner.  Untuk pelatihan dan pengujian, data multispektral (gambar dan lapisan dengan kelas bangunan biner) dengan Landsat 5 untuk 2011 untuk Bangalore akan digunakan.  Dan untuk prediksi akan digunakan data Landsat 5 multispektral yang diperoleh pada tahun 2005 di Hyderabad. <br>  Karena kami menggunakan data yang ditandai untuk mengajar, ini disebut mengajar dengan guru. <br><br><img src="https://habrastorage.org/webt/c4/nx/71/c4nx71fzbix6rhp4fb1-ofdupjm.jpeg"><br><br>  <i>Data pelatihan multispektral dan lapisan biner terkait dengan pengembangan.</i> <br><br>  Untuk membuat jaringan saraf, kita akan menggunakan Python - perpustakaan Google Tensorflow.  Kami juga membutuhkan perpustakaan ini: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>pyrsgis</i></a> - untuk membaca dan menulis GeoTIFF. <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>scikit-learn</i></a> - untuk preprocessing data dan penilaian akurasi. <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><i>numpy</i></a> - untuk operasi dasar dengan array. <br></li></ol><br>  Dan sekarang, tanpa basa-basi lagi, mari kita menulis kode. <br><br>  Masukkan ketiga file dalam direktori, tulis jalur dan nama file input dalam skrip, dan kemudian baca file GeoTIFF. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyrsgis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> raster os.chdir(<span class="hljs-string"><span class="hljs-string">"E:\\yourDirectoryName"</span></span>) mxBangalore = <span class="hljs-string"><span class="hljs-string">'l5_Bangalore2011_raw.tif'</span></span> builtupBangalore = <span class="hljs-string"><span class="hljs-string">'l5_Bangalore2011_builtup.tif'</span></span> mxHyderabad = <span class="hljs-string"><span class="hljs-string">'l5_Hyderabad2011_raw.tif'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Read the rasters as array ds1, featuresBangalore = raster.read(mxBangalore, bands='all') ds2, labelBangalore = raster.read(builtupBangalore, bands=1) ds3, featuresHyderabad = raster.read(mxHyderabad, bands='all')</span></span></code> </pre> <br>  Modul <code>raster</code> dari paket <code>pyrsgis</code> membaca data geolokasi GeoTIFF dan nilai nomor digital (DN) sebagai array NumPy yang terpisah.  Jika Anda tertarik pada detail, baca di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Sekarang kami menampilkan ukuran data yang dibaca. <br><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Bangalore multispectral image shape: "</span></span>, featuresBangalore.shape) print(<span class="hljs-string"><span class="hljs-string">"Bangalore binary built-up image shape: "</span></span>, labelBangalore.shape) print(<span class="hljs-string"><span class="hljs-string">"Hyderabad multispectral image shape: "</span></span>, featuresHyderabad.shape)</code> </pre> <br>  Hasil: <br><br><pre> <code class="python hljs">Bangalore multispectral image shape: <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">2054</span></span>, <span class="hljs-number"><span class="hljs-number">2044</span></span> Bangalore binary built-up image shape: <span class="hljs-number"><span class="hljs-number">2054</span></span>, <span class="hljs-number"><span class="hljs-number">2044</span></span> Hyderabad multispectral image shape: <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">1318</span></span>, <span class="hljs-number"><span class="hljs-number">1056</span></span></code> </pre> <br>  Seperti yang Anda lihat, gambar Bangalore memiliki jumlah baris dan kolom yang sama seperti pada layer biner (sesuai dengan bangunan).  Jumlah lapisan dalam gambar multispektral di Bangalore dan Hyderabad juga bersamaan.  Model akan belajar menentukan piksel mana yang termasuk dalam bangunan dan mana yang tidak, berdasarkan nilai yang sesuai untuk semua 6 spektrum.  Oleh karena itu, gambar multispektral harus memiliki jumlah fitur (rentang) yang sama yang tercantum dalam urutan yang sama. <br><br>  Sekarang kita mengubah array menjadi dua dimensi, di mana setiap baris mewakili piksel yang terpisah, karena ini diperlukan untuk operasi sebagian besar algoritma pembelajaran mesin.  Kami akan melakukan ini menggunakan modul <code>convert</code> dari paket <code>pyrsgis</code> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d69/176/b91/d69176b91e9758291e1faae6c25486e5.jpg"><br>  <i>Skema restrukturisasi data.</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyrsgis.convert <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> changeDimension featuresBangalore = changeDimension(featuresBangalore) labelBangalore = changeDimension (labelBangalore) featuresHyderabad = changeDimension(featuresHyderabad) nBands = featuresBangalore.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] labelBangalore = (labelBangalore == <span class="hljs-number"><span class="hljs-number">1</span></span>).astype(int) print(<span class="hljs-string"><span class="hljs-string">"Bangalore multispectral image shape: "</span></span>, featuresBangalore.shape) print(<span class="hljs-string"><span class="hljs-string">"Bangalore binary built-up image shape: "</span></span>, labelBangalore.shape) print(<span class="hljs-string"><span class="hljs-string">"Hyderabad multispectral image shape: "</span></span>, featuresHyderabad.shape)</code> </pre> <br>  Hasil: <br><br><pre> <code class="python hljs">Bangalore multispectral image shape: <span class="hljs-number"><span class="hljs-number">4198376</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span> Bangalore binary built-up image shape: <span class="hljs-number"><span class="hljs-number">4198376</span></span> Hyderabad multispectral image shape: <span class="hljs-number"><span class="hljs-number">1391808</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span></code> </pre> <br>  Di baris ketujuh, kami mengekstraksi semua piksel dengan nilai 1. Ini membantu menghindari masalah dengan piksel tanpa informasi (NoData), yang seringkali memiliki nilai sangat tinggi atau rendah. <br>  Sekarang kita akan membagi data menjadi sampel pelatihan dan validasi.  Ini diperlukan agar model tidak melihat data uji dan berfungsi dengan baik dengan informasi baru.  Jika tidak, model akan dilatih ulang dan hanya akan berfungsi dengan baik pada data pelatihan. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split xTrain, xTest, yTrain, yTest = train_test_split(featuresBangalore, labelBangalore, test_size=<span class="hljs-number"><span class="hljs-number">0.4</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>) print(xTrain.shape) print(yTrain.shape) print(xTest.shape) print(yTest.shape)</code> </pre> <br>  Hasil: <br><br><pre> <code class="python hljs">(<span class="hljs-number"><span class="hljs-number">2519025</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) (<span class="hljs-number"><span class="hljs-number">2519025</span></span>,) (<span class="hljs-number"><span class="hljs-number">1679351</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) (<span class="hljs-number"><span class="hljs-number">1679351</span></span>,) test_size=<span class="hljs-number"><span class="hljs-number">0.4</span></span></code> </pre> <br>  berarti bahwa data dibagi menjadi pelatihan dan validasi dalam rasio 60/40. <br>  Banyak algoritma pembelajaran mesin, termasuk jaringan saraf, membutuhkan data yang dinormalisasi.  Ini berarti bahwa mereka harus didistribusikan dalam kisaran yang diberikan (dalam hal ini, dari 0 hingga 1).  Oleh karena itu, untuk memenuhi persyaratan ini, kami menormalkan gejala.  Ini dapat dilakukan dengan mengekstraksi nilai minimum dan kemudian membaginya dengan spread (perbedaan antara nilai maksimum dan minimum).  Karena dataset Landsat adalah delapan-bit, nilai minimum dan maksimum akan menjadi 0 dan 255 (2 <sup>â¸</sup> = 256 nilai). <br><br>  Perhatikan bahwa untuk normalisasi selalu lebih baik untuk menghitung nilai minimum dan maksimum berdasarkan data.  Untuk menyederhanakan tugas, kami akan mematuhi rentang delapan-bit secara default. <br><br>  Tahap pra-pemrosesan lainnya adalah transformasi matriks atribut dari dua dimensi menjadi tiga dimensi, sehingga model menganggap setiap baris sebagai piksel terpisah (objek pembelajaran terpisah). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/965/d0f/aa8/965d0faa8e8c7b4787c10823cf038d20.jpg"><br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Normalise the data xTrain = xTrain / 255.0 xTest = xTest / 255.0 featuresHyderabad = featuresHyderabad / 255.0 # Reshape the data xTrain = xTrain.reshape((xTrain.shape[0], 1, xTrain.shape[1])) xTest = xTest.reshape((xTest.shape[0], 1, xTest.shape[1])) featuresHyderabad = featuresHyderabad.reshape((featuresHyderabad.shape[0], 1, featuresHyderabad.shape[1])) # Print the shape of reshaped data print(xTrain.shape, xTest.shape, featuresHyderabad.shape)</span></span></code> </pre> <br>  Hasil: <br><br><pre> <code class="python hljs">(<span class="hljs-number"><span class="hljs-number">2519025</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) (<span class="hljs-number"><span class="hljs-number">1679351</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>) (<span class="hljs-number"><span class="hljs-number">1391808</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)</code> </pre> <br>  Semuanya siap, mari kumpulkan model kita dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">keras</a> .  Untuk memulai, mari kita gunakan model sekuensial, menambahkan lapisan satu demi satu.  Kami akan memiliki satu layer input dengan jumlah node yang sama dengan jumlah rentang ( <code>nBands</code> ) - dalam kasus kami ada 6. Kami juga akan menggunakan satu layer tersembunyi dengan 14 node dan <code>ReLu</code> aktivasi <code>ReLu</code> .  Lapisan terakhir terdiri dari dua node untuk mendefinisikan kelas bangunan biner dengan <code>softmax</code> aktivasi <code>softmax</code> , yang cocok untuk menampilkan hasil yang dikategorikan.  Baca lebih lanjut tentang fungsi aktivasi di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-comment"><span class="hljs-comment"># Define the parameters of the model model = keras.Sequential([ keras.layers.Flatten(input_shape=(1, nBands)), keras.layers.Dense(14, activation='relu'), keras.layers.Dense(2, activation='softmax')]) # Define the accuracy metrics and parameters model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]) # Run the model model.fit(xTrain, yTrain, epochs=2)</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/d8a/9cd/5ec/d8a9cd5ec53c57d671d77b0c46bfb17e.png"><br>  <i>Arsitektur jaringan saraf</i> <br><br>  Seperti disebutkan dalam baris 10, kami menetapkan <code>adam</code> sebagai pengoptimal model (ada beberapa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">lainnya</a> ).  Dalam hal ini, kita akan menggunakan cross entropy sebagai fungsi kerugian (id. <code>categorical-sparse-crossentropy</code> - lebih lanjut tentang ini ditulis di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> ).  Untuk menilai kualitas model, kami akan menggunakan metrik <code>accuracy</code> . <br><br>  Akhirnya, kita akan mulai melatih model kita untuk dua era (atau iterasi) pada <code>xTrain</code> dan <code>yTrain</code> .  Ini akan memakan waktu, tergantung pada ukuran data dan kekuatan pemrosesan.  Inilah yang akan Anda lihat setelah kompilasi: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dfa/3fc/c47/dfa3fcc47572b2fd20c5252ba4da1e50.png"><br><br>  Mari kita memprediksi nilai untuk data validasi yang kami simpan secara terpisah dan menghitung berbagai metrik akurasi. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> confusion_matrix, precision_score, recall_score <span class="hljs-comment"><span class="hljs-comment"># Predict for test data yTestPredicted = model.predict(xTest) yTestPredicted = yTestPredicted[:,1] # Calculate and display the error metrics yTestPredicted = (yTestPredicted&gt;0.5).astype(int) cMatrix = confusion_matrix(yTest, yTestPredicted) pScore = precision_score(yTest, yTestPredicted) rScore = recall_score(yTest, yTestPredicted) print("Confusion matrix: for 14 nodes\n", cMatrix) print("\nP-Score: %.3f, R-Score: %.3f" % (pScore, rScore))</span></span></code> </pre> <br>  Fungsi <code>softmax</code> menghasilkan kolom terpisah untuk nilai probabilitas untuk setiap kelas.  Kami hanya menggunakan nilai untuk kelas pertama ("ada bangunan"), seperti yang dapat dilihat dari baris keenam kode di atas.  Mengevaluasi pekerjaan model analisis geospasial tidak begitu sederhana, tidak seperti masalah klasik pembelajaran mesin.  Tidaklah adil untuk mengandalkan kesalahan total umum.  Kunci untuk model yang sukses adalah tata ruang.  Dengan demikian, matriks kebingungan, akurasi dan kelengkapan dapat memberikan gagasan yang lebih benar tentang kualitas model. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/399/3d8/bf3/3993d8bf36a5f4165b60f39e36e5c566.jpg"><br>  <i>Jadi konsol menampilkan matriks kesalahan, akurasi dan kelengkapan.</i> <br><br>  Seperti yang dapat Anda lihat dari matriks kebingungan, ada ribuan piksel yang terkait dengan bangunan, tetapi diklasifikasikan secara berbeda, dan sebaliknya.  Namun, bagian mereka dari total volume data tidak terlalu besar.  Keakuratan dan kelengkapan data uji melebihi ambang batas 0,8. <br><br>  Anda dapat menghabiskan lebih banyak waktu dan melakukan beberapa iterasi untuk menemukan jumlah optimal lapisan tersembunyi, jumlah node di setiap lapisan tersembunyi, serta jumlah era untuk mencapai akurasi yang diinginkan.  Sesuai kebutuhan, indeks penginderaan jauh seperti NDBI atau NDWI dapat digunakan sebagai fitur.  Saat mencapai akurasi yang diinginkan, gunakan model untuk memprediksi pengembangan berdasarkan data baru dan ekspor hasilnya ke GeoTIFF.  Untuk tugas-tugas seperti itu, Anda dapat menggunakan model serupa dengan perubahan kecil. <br><br><pre> <code class="python hljs">predicted = model.predict(feature2005) predicted = predicted[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#Export raster prediction = np.reshape(predicted, (ds.RasterYSize, ds.RasterXSize)) outFile = 'Hyderabad_2011_BuiltupNN_predicted.tif' raster.export(prediction, ds3, filename=outFile, dtype='float')</span></span></code> </pre> <br>  Harap perhatikan bahwa kami mengekspor GeoTIFF dengan nilai probabilitas yang diprediksi, dan tidak dengan versi yang dipisah-pindahkan ambangnya.  Kemudian di lingkungan GIS, kita dapat mengatur nilai ambang batas dari tipe float, seperti yang ditunjukkan pada gambar di bawah ini. <br><br><img src="https://habrastorage.org/webt/sk/ka/kz/skkakzxokrbkvrgulcvm0vkuwua.jpeg"><br>  <i>Lapisan bawaan Hyderabad diprediksi oleh model berdasarkan data multispektral.</i> <br><br>  Akurasi model telah diukur dengan presisi dan daya ingat.  Anda juga dapat melakukan pemeriksaan tradisional (misalnya, menggunakan koefisien kappa) pada layer yang baru diprediksi.  Selain kesulitan yang disebutkan di atas dengan klasifikasi citra satelit, batasan jelas lainnya termasuk ketidakmungkinan perkiraan berdasarkan gambar yang diambil pada waktu yang berbeda dalam setahun dan di wilayah yang berbeda, karena mereka akan memiliki tanda tangan spektral yang berbeda. <br><br>  Model yang dijelaskan dalam artikel ini memiliki arsitektur paling sederhana untuk jaringan saraf.  Hasil yang lebih baik dapat dicapai dengan model yang lebih kompleks, termasuk jaringan saraf convolutional.  Keuntungan utama dari klasifikasi tersebut adalah skalabilitasnya (penerapan) setelah pelatihan model. <br><br>  Data yang digunakan dan semua kode ada di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id468973/">https://habr.com/ru/post/id468973/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id468961/index.html">Bagaimana cara menghilangkan rutinitas dalam hidup seharga $ 560? Atau bagaimana hidup, bukan hidup</a></li>
<li><a href="../id468963/index.html">Cadangkan, bagian atas permintaan pembaca: ikhtisar UrBackup, BackupPC, AMANDA</a></li>
<li><a href="../id468967/index.html">"Teknologi" untuk mendapatkan persamaan dinamika TAU. Dan mengapa Sistem Identifikasi menyebalkan, dan "fisika jujur" berkuasa</a></li>
<li><a href="../id468969/index.html">Buat pengguna Google dari PowerShell via API</a></li>
<li><a href="../id468971/index.html">Menulis di Java untuk Nintendo DS</a></li>
<li><a href="../id468989/index.html">Pasar UEBA Meninggal - UEBA Langsung Lama</a></li>
<li><a href="../id468991/index.html">Karakter sprite modular dan animasinya</a></li>
<li><a href="../id468993/index.html">Oculus Quest terhubung ke PC dan melihat tangan</a></li>
<li><a href="../id468995/index.html">Kebijakan keterbukaan: bagaimana pengguna memengaruhi proyek</a></li>
<li><a href="../id468997/index.html">Mentoring - bonus yang harus dimiliki atau bagus?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>