<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💻 😲 ⏱️ Test de charge d'un jeu avec quelques centaines de milliers d'utilisateurs virtuels 🐺 😮 💪🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 Je travaille pour une société de jeux qui développe des jeux en ligne. Actuellement, tous nos jeux sont divisés en plusieurs «marché...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Test de charge d'un jeu avec quelques centaines de milliers d'utilisateurs virtuels</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445368/">  Bonjour, Habr! <br><br>  Je travaille pour une société de jeux qui développe des jeux en ligne.  Actuellement, tous nos jeux sont divisés en plusieurs «marchés» (un «marché» par pays) et dans chaque «marché» il y a une douzaine de mondes entre lesquels les joueurs sont répartis lors de l'inscription (enfin, ou parfois ils peuvent le choisir eux-mêmes).  Chaque monde possède une base de données et un ou plusieurs serveurs Web / d'applications.  Ainsi, la charge est divisée et distribuée à travers les mondes / serveurs presque également, et en conséquence, nous obtenons le maximum en ligne de joueurs 6K-8K (c'est le maximum, la plupart du temps plusieurs fois moins) et 200-300 demandes par heure de grande écoute par monde. <br><br>  Une telle structure avec la division des joueurs en marchés et mondes devient obsolète; les joueurs veulent quelque chose de mondial.  Lors des derniers matchs, nous avons cessé de diviser les gens par pays et laissé seulement un / deux marchés (Amérique et Europe), mais toujours avec de nombreux mondes dans chacun.  La prochaine étape sera le développement de jeux avec une nouvelle architecture et l'unification de tous les joueurs dans un seul monde avec <b>une seule base de données</b> . <br><br>  Aujourd’hui, je voulais parler un peu de la façon dont j’étais chargé de vérifier si l’ensemble (et 50 à 200 000 utilisateurs à la fois) de l’un de nos jeux populaires «envoyait» jouer au prochain jeu basé sur la nouvelle architecture et si l'ensemble du système, en particulier la base de données ( <b>PostgreSQL 11</b> ), peut pratiquement supporter une telle charge et, s'il ne le peut pas, savoir où est notre maximum.  Je vais vous parler un peu des problèmes qui sont survenus et des décisions à prendre pour préparer à tester autant d'utilisateurs, du processus lui-même et des résultats. <br><a name="habracut"></a><br><h2>  Intro </h2><br>  Dans le passé, chez <b>InnoGames GmbH,</b> chaque équipe de jeu a créé un projet de jeu à leur goût et à leur couleur, en utilisant souvent différentes technologies, langages de programmation et bases de données.  De plus, nous avons de nombreux systèmes externes chargés des paiements, de l'envoi de notifications push, du marketing et plus encore.  Pour travailler avec ces systèmes, les développeurs ont également créé leurs interfaces uniques du mieux qu'ils pouvaient. <br><br>  Actuellement, dans le secteur des jeux mobiles, beaucoup d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">argent</a> et, par conséquent, beaucoup de concurrence.  Il est très important ici de le récupérer de chaque dollar dépensé en marketing et un peu plus d'en haut, donc toutes les sociétés de jeux «clôturent» très souvent les jeux même au stade des tests fermés, si elles ne répondent pas aux attentes analytiques.  En conséquence, perdre du temps sur l'invention de la roue suivante n'est pas rentable, il a donc été décidé de créer une plate-forme unifiée qui fournira aux développeurs une solution prête à l'emploi pour l'intégration avec tous les systèmes externes, une base de données avec réplication et toutes les meilleures pratiques.  Tout ce dont les développeurs ont besoin, c'est de développer et de «mettre» un bon jeu en plus et de ne pas perdre de temps sur un développement non lié au jeu lui-même. <br><br>  Cette plateforme s'appelle <b>GameStarter</b> : <br><br><img src="https://habrastorage.org/webt/fz/go/g3/fzgog3jsz4rzjqi0zvbwzysz-po.jpeg" alt="image"><br><br>  Donc, au fait.  Tous les futurs jeux InnoGames seront construits sur cette plate-forme, qui dispose de deux bases de données - maître et jeu (PostgreSQL 11).  Master stocke des informations de base sur les joueurs (login, mot de passe, etc.) et ne participe, principalement, qu'au processus de connexion / enregistrement dans le jeu lui-même.  Jeu - la base de données du jeu lui-même, où, en conséquence, toutes les données et entités du jeu sont stockées, ce qui est le cœur du jeu, où toute la charge ira. <br>  Ainsi, la question s'est posée de savoir si toute cette structure pouvait supporter un nombre d'utilisateurs potentiel égal au maximum en ligne de l'un de nos jeux les plus populaires. <br><br><h2>  Défi </h2><br>  La tâche elle-même était la suivante: vérifier si la base de données (PostgreSQL 11), avec la réplication activée, peut supporter toute la charge que nous avons actuellement dans le jeu le plus chargé, ayant à sa disposition tout l'hyperviseur PowerEdge M630 (HV). <br>  Je précise que la tâche pour le moment n'était <b>que de vérifier</b> , en utilisant les configurations de base de données existantes, que nous avons formées en tenant compte des meilleures pratiques et de notre propre expérience. <br><br>  Je dirai tout de suite la base de données, et l'ensemble du système s'est bien montré, à l'exception de quelques points.  Mais ce projet de jeu particulier était au stade de prototype et à l'avenir, avec la complication des mécanismes de jeu, les demandes à la base de données deviendront plus compliquées et la charge elle-même pourrait augmenter considérablement et sa nature pourrait changer.  Pour éviter cela, il est nécessaire de tester de manière itérative le projet avec chaque étape plus ou moins importante.  L'automatisation de la possibilité d'exécuter ce type de tests avec quelques centaines de milliers d'utilisateurs est devenue la tâche principale à ce stade. <br><br><h2>  Le profil </h2><br>  Comme tout test de charge, tout commence par un profil de charge. <br>  Notre valeur potentielle CCU60 (CCU est le nombre maximum d'utilisateurs pendant une certaine période de temps, dans ce cas 60 minutes) est supposée être de <b>250 000</b> utilisateurs.  Le nombre d'utilisateurs virtuels (VU) compétitifs est inférieur à celui du CCU60 et les analystes ont suggéré qu'il peut être divisé en deux en toute sécurité.  Arrondissez et acceptez <b>150 000</b> VU compétitifs. <br><br>  Le nombre total de requêtes par seconde provient d'un jeu plutôt chargé: <br><br><img src="https://habrastorage.org/webt/lv/te/69/lvte69rifceelurn7r3t7trbgs4.png"><br><br>  Ainsi, notre charge cible est de ~ <b>20 000 requêtes / s</b> à <b>150 000</b> VU. <br><br><h2>  La structure </h2><br><h3>  Caractéristiques du «stand» </h3><br>  Dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> précédent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">,</a> j'ai déjà parlé de l'automatisation de l'ensemble du processus de test de charge.  De plus, je vais peut-être me répéter un peu, mais je vais vous dire quelques points plus en détail. <br><br><img src="https://habrastorage.org/webt/zh/hz/eo/zhhzeorw5_gboyuocu9ajmb3ulo.png"><br><br>  Dans le diagramme, les carrés bleus sont nos hyperviseurs (HV), un nuage composé de nombreux serveurs (Dell M620 - M640).  Sur chaque HV, une dizaine de machines virtuelles (VM) sont lancées via KVM (web / app et db dans le mix).  Lors de la création d'une nouvelle machine virtuelle, l'équilibrage et la recherche dans l'ensemble des paramètres d'un HV approprié se produisent et on ne sait pas initialement sur quel serveur il sera. <br><br><h4>  Base de données (Game DB): </h4><br>  Mais pour notre objectif db1, nous avons réservé un <b>targer_hypervisor</b> HV <b>séparé</b> basé sur le M630. <br><br>  Brève caractéristiques de targer_hypervisor: <br><br>  Dell M_630 <br>  Nom du modèle: Intel® Xeon® CPU E5-2680 v3 @ 2.50GHz <br>  Processeur (s): 48 <br>  Fil (s) par noyau: 2 <br>  Noyau (s) par socket: 12 <br>  Prise (s): 2 <br>  RAM: 128 Go <br>  Debian GNU / Linux 9 (stretch) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br><br><div class="spoiler">  <b class="spoiler_title">Spécifications détaillées</b> <div class="spoiler_text">  Debian GNU / Linux 9 (stretch) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br>  lscpu <br>  Architecture: x86_64 <br>  Mode (s) opérationnel (s) du processeur: 32 bits, 64 bits <br>  Ordre des octets: Little Endian <br>  Processeur (s): 48 <br>  Liste des processeurs en ligne: 0-47 <br>  Fil (s) par noyau: 2 <br>  Noyau (s) par socket: 12 <br>  Prise (s): 2 <br>  Nœud (s) NUMA: 2 <br>  ID du fournisseur: GenuineIntel <br>  Famille de CPU: 6 <br>  Modèle: 63 <br>  Nom du modèle: Intel® Xeon® CPU E5-2680 v3 @ 2.50GHz <br>  Étape: 2 <br>  CPU MHz: 1309.356 <br>  CPU max MHz: 3300,0000 <br>  CPU min MHz: 1200,0000 <br>  BogoMIPS: 4988.42 <br>  Virtualisation: VT-x <br>  Cache L1d: 32 Ko <br>  Cache L1i: 32 Ko <br>  Cache L2: 256 Ko <br>  Cache L3: 30720 Ko <br>  NUMA node0 CPU (s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42 44,46 <br>  Processeur (s) NUMA node1: 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43 , 45,47 <br>  Drapeaux: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant qtsopmopcopts bts bts smx is TM2 SSSE3 SDBG fma CX16 xtpr PDCM PCID dca sse4_1 sse4_2 x2apic movbe POPCNT tsc_deadline_timer aes xsave AVX F16C rdrand lahf_lm abm EPB invpcid_single SSBD CCRI ibpb stibp kaiser tpr_shadow vnmi FlexPriority ept VPID fsgsbase tsc_adjust BMI1 AVX2 EPEOA bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts flush_l1d <br><br>  / usr / bin / qemu-system-x86_64 --version <br>  Émulateur QEMU version 2.8.1 (Debian 1: 2.8 + dfsg-6 + deb9u5) <br>  Copyright © 2003-2016 Fabrice Bellard et les développeurs du projet QEMU <br></div></div><br>  Brève caractéristiques de db1: <br>  Architecture: x86_64 <br>  Processeur (s): 48 <br>  RAM: 64 Go <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.144-3.1 (2019-02-19) x86_64 GNU / Linux <br>  Debian GNU / Linux 9 (stretch) <br>  psql (PostgreSQL) 11.2 (Debian 11.2-1.pgdg90 + 1) <br><br><div class="spoiler">  <b class="spoiler_title">Configuration PostgreSQL avec quelques explications</b> <div class="spoiler_text">  seq_page_cost = 1.0 <br>  random_page_cost = 1.1 # Nous avons un SSD <br>  inclure «/etc/postgresql/11/main/extension.conf» <br>  log_line_prefix = '% t [% p-% l]% q% u @% h' <br>  log_checkpoints = on <br>  log_lock_waits = on <br>  log_statement = ddl <br>  log_min_duration_statement = 100 <br>  log_temp_files = 0 <br>  autovacuum_max_workers = 5 <br>  autovacuum_naptime = 10s <br>  autovacuum_vacuum_cost_delay = 20ms <br>  vacuum_cost_limit = 2000 <br>  maintenance_work_mem = 128 Mo <br>  synchronous_commit = off <br>  checkpoint_timeout = 30min <br>  listen_addresses = '*' <br>  work_mem = 32 Mo <br>  effective_cache_size = 26214 Mo # 50% de la mémoire disponible <br>  shared_buffers = 16384 Mo # 25% de la mémoire disponible <br>  max_wal_size = 15 Go <br>  min_wal_size = 80 Mo <br>  wal_level = hot_standby <br>  max_wal_senders = 10 <br>  wal_compression = on <br>  archive_mode = on <br>  archive_command = '/ bin / true' <br>  archive_timeout = 1800 <br>  hot_standby = on <br>  wal_log_hints = on <br>  hot_standby_feedback = on <br></div></div><br>  <b>hot_standby_feedback</b> est <b>désactivé par</b> défaut, nous l'avons allumé, mais il a dû être désactivé plus tard pour effectuer un test réussi.  J'expliquerai plus tard pourquoi. <br><br>  Les principales tables actives de la base de données (construction, production, game_entity, building, core_inventory_player_resource, survivor) sont préremplies de données (environ 80 Go) à l'aide d'un script bash. <br><br><div class="spoiler">  <b class="spoiler_title">db-fill-script.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash --clean TRUNCATE TABLE production CASCADE; TRUNCATE TABLE construction CASCADE; TRUNCATE TABLE building CASCADE; TRUNCATE TABLE grid CASCADE; TRUNCATE TABLE core_inventory_player_resource CASCADE; TRUNCATE TABLE survivor CASCADE; TRUNCATE TABLE city CASCADE; TRUNCATE TABLE game_entity CASCADE; TRUNCATE TABLE player CASCADE; TRUNCATE TABLE core_player CASCADE; TRUNCATE TABLE core_client_device CASCADE; --core_client_device INSERT INTO core_client_device (id, creation_date, modification_date, device_model, device_name, locale, platform, user_agent, os_type, os_version, network_type, device_type) SELECT (1000000000+generate_series(0,999999)) AS id, now(), now(), 'device model', 'device name', 'en_DK', 'ios', 'ios user agent', 'android', '8.1', 'wlan', 'browser'; --core_player INSERT INTO core_player (id, guest, name, nickname, premium_points, soft_deleted, session_id, tracking_device_data_id) SELECT (1000000000+generate_series(0,999999)) AS id, true, 'guest0000000000000000000', null, 100, false, '00000000-0000-0000-0000-000000000000', (1000000000+generate_series(0,999999)) ; --player INSERT INTO player (id, creation_date, modification_date, core_player_id) SELECT (1000000000+generate_series(0,999999)) , now(), now(), (1000000000+generate_series(0,999999)) ; --city INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1000000000+generate_series(0,999999)) , 'city', now(), now(); INSERT INTO city (id, game_design, player_id) SELECT (1000000000+generate_series(0,999999)) , 'city.default', (1000000000+generate_series(0,999999)) ; --survivor INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1001000000+generate_series(0,999999)) , 'survivor', now(), now(); INSERT INTO survivor (id, game_design, owning_entity_id, type) SELECT (1001000000+generate_series(0,999999)) , 'survivor.prod_1', (1000000000+generate_series(0,999999)) , 'survivor'; --core_inventory_player_resource INSERT INTO core_inventory_player_resource (id, creation_date, modification_date, amount, player_id, resource_key) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 1000, (1000000000+generate_series(0,1999999)/2) , CONCAT('resource_', (1000000000+generate_series(0,1999999)) % 2); --grid DROP INDEX grid_area_idx; INSERT INTO grid (id, creation_date, modification_date, area, city_id) SELECT (1000000000+generate_series(0,19999999)) , NOW(), NOW(), BOX '0,0,4,4', (1000000000+generate_series(0,19999999)/20) ; create index on grid using gist (area box_ops); --building INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1002000000+generate_series(0,99999999)) , 'building', now(), now(); INSERT INTO building (id, game_design, owning_entity_id, x, y, rotation, type) SELECT (1002000000+generate_series(0,99999999)) , 'building.building_prod_1', (1000000000+generate_series(0,99999999)/100) , 0, 0, 'DEGREES_0', 'building'; --construction INSERT INTO construction (id, creation_date, modification_date, definition, entity_id, start) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 'construction.building_prod_1-construction', (1002000000+generate_series(0,1999999)*50) , NOW(); --production INSERT INTO production (id, creation_date, modification_date, active, definition, entity_id, start_time) SELECT (1000000000+generate_series(0,49999999)) , NOW(), NOW(), true, 'production.building_prod_1_production_1', (1002000000+generate_series(0,49999999)*2) , NOW();</span></span></code> </pre> <br></div></div><br>  Réplication: <br><br><pre> <code class="plaintext hljs">SELECT * FROM pg_stat_replication; pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_lsn | write_lsn | flush_lsn | replay_lsn | write_lag | flush_lag | replay_lag | sync_priority | sync_state -----+----------+---------+---------------------+--------------+---------------------+-------------+-------------------------------+--------------+-----------+------------+------------+------------+------------+-----------------+-----------------+-----------------+---------------+------------ 759 | 17035 | repmgr | xl1db2 | xxxx | xl1db2 | 51142 | 2019-01-27 08:56:44.581758+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000393 | 00:00:00.001159 | 00:00:00.001313 | 0 | async 977 | 17035 | repmgr | xl1db3 |xxxxx | xl1db3 | 42888 | 2019-01-27 08:57:03.232969+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000373 | 00:00:00.000798 | 00:00:00.000919 | 0 | async</code> </pre><br><h4>  Serveur d'application </h4><br>  Ensuite, sur des HV productifs (prod_hypervisors) de différentes configurations et capacités, 15 serveurs d'applications ont été lancés: 8 cœurs, 4 Go.  La principale chose que l'on peut dire: openjdk 11.0.1 2018-10-16, printemps, interaction avec la base de données via <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">hikari</a> (hikari.maximum-pool-size: 50) <br><br><h4>  Environnement de test de stress </h4><br>  L'ensemble de l'environnement de test de charge se compose d'un serveur principal <b>admin.loadtest</b> et de plusieurs serveurs <b>generatorN.loadtest</b> (dans ce cas, il y en avait 14). <br><br>  <b>generatorN.loadtest</b> - VM «brute» Debian Linux 9, avec Java 8. installé 32 noyaux / 32 gigaoctets.  Ils sont situés sur des HV non productifs afin de ne pas tuer accidentellement les performances des VM importantes. <br><br>  <b>admin.loadtest</b> - <b>Machine virtuelle</b> Debian Linux 9, 16 cœurs / 16 concerts, il exécute Jenkins, JLTC et d'autres logiciels supplémentaires sans importance. <br><br>  JLTC - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">centre de test de charge jmeter</a> .  Un système en Py / Django qui contrôle et automatise le lancement des tests, ainsi que l'analyse des résultats. <br><br><h3>  Schéma de lancement du test </h3><br><img src="https://habrastorage.org/webt/pb/f_/th/pbf_thx7mwuois96bffvbeehtxk.png"><br><br>  Le processus d'exécution du test ressemble à ceci: <br><br><ul><li>  Le test est lancé depuis <b>Jenkins</b> .  Sélectionnez le Job requis, puis vous devez entrer les paramètres de test souhaités: <ul><li>  <b>DURATION</b> - durée du test </li><li>  <b>RAMPUP</b> - temps «d'échauffement» </li><li>  <b>THREAD_COUNT_TOTAL</b> - le nombre souhaité d'utilisateurs virtuels (VU) ou de threads </li><li>  <b>TARGET_RESPONSE_TIME</b> est un paramètre important, afin de ne pas surcharger l'ensemble du système à l'aide de celui-ci, nous définissons le temps de réponse souhaité.En conséquence, le test maintiendra la charge à un niveau auquel le temps de réponse de l'ensemble du système ne sera pas supérieur à celui spécifié. </li></ul></li><li>  Lancement </li><li>  Jenkins clone le plan de test de Gitlab, l'envoie à JLTC. </li><li>  JLTC fonctionne un peu avec un plan de test (par exemple, insère un écrivain simple CSV). </li><li>  JLTC calcule le nombre requis de serveurs Jmeter pour exécuter le nombre souhaité de VU (THREAD_COUNT_TOTAL). </li><li>  JLTC se connecte à chaque générateur de loadgeneratorN et démarre le serveur jmeter. </li></ul><br>  Pendant le test, le <b>client JMeter</b> génère un fichier CSV avec les résultats.  Ainsi, pendant le test, la quantité de données et la taille de ce fichier augmentent à un rythme <b>insensé</b> , et il ne peut pas être utilisé pour l'analyse après le test - <b>Daemon a été</b> inventé (comme une expérience), qui l'analyse <i>«à la volée»</i> . <br><br><h3>  Plan de test </h3><br>  Vous pouvez télécharger le plan de test <a href="">ici</a> . <br><br>  Après l'enregistrement / la connexion, les utilisateurs travaillent dans le module <b>Comportement</b> , qui se compose de plusieurs <b>contrôleurs de débit</b> qui spécifient la probabilité d'une fonction de jeu particulière.  Dans chaque contrôleur de débit, il existe un <b>contrôleur de module</b> , qui fait référence au module correspondant qui implémente la fonction. <br><br><img src="https://habrastorage.org/webt/t0/ws/qp/t0wsqpbkgo-w6dt55gvxd6aw9pm.png"><br><br><h4>  Hors sujet </h4><br>  Pendant le développement du script, nous avons essayé d'utiliser Groovy au maximum, et grâce à notre programmeur Java, j'ai découvert quelques astuces pour moi-même (peut-être que cela sera utile pour quelqu'un): <br><br><ul><li>  Vous pouvez déclarer une fonction quelque part au début du plan de test, puis l'utiliser dans d'autres pré, post-processeurs et échantillonneurs.  Plus de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bonté groovy: transformez les méthodes en fermetures</a> : <br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//     - def sum(Integer x, Integer y) { return x + y } vars.putObject('sum', this.&amp;sum) //      closure.   . //     sampler`       def sum= vars.getObject('sum'); println sum(2, 2);</span></span></code> </pre> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">groovy.json.JsonSlurper</a> est un excellent analyseur JSON rapide.  Avec groovy, il vous permet d'analyser avec élégance les données et de les traiter: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> groovy.json.JsonSlurper def canBuild = vars.getObject(canBuild); <span class="hljs-comment"><span class="hljs-comment">// ""       def content = jsonSlurper.parseText(response).content def buildings = content[0].buildings //         //               def constructableBuildingDefs = buildings .collect { k,v -&gt; v } .grep{ it.definitions .grep { it2 -&gt; it2['@type'] == 'type.googleapis.com/ConstructionDefinitionDTO'} .grep { it2 -&gt; canBuild(it2) } //   .size() &gt; 0 } if (!constructableBuildingDefs) { return; } Collections.shuffle(constructableBuildingDefs) //       </span></span></code> </pre></li></ul><br><h3>  VU / Threads </h3><br>  Lorsqu'un utilisateur entre le nombre souhaité de VU à l'aide du paramètre THREAD_COUNT_TOTAL lors de la configuration du travail dans Jenkins, il est nécessaire de démarrer le nombre requis de serveurs Jmeter et de répartir le nombre final de VU entre eux.  Cette partie appartient au JLTC dans la partie appelée <b>contrôleur / provision</b> . <br><br>  Essentiellement, l'algorithme est le suivant: <br><br><ul><li>  Nous divisons le nombre souhaité de <b>threads VU_num</b> en 200-300 threads et en fonction de la taille plus ou moins adéquate <b>-Xmsm -Xmxm, nous</b> déterminons la valeur de mémoire requise pour un <i>serveur jmeter</i> <b>required_memory_for_jri</b> (JRI - j'appelle l'instance distante Jmeter, au lieu de Jmeter-server). </li><li>  De threads_num et required_memory_for_jri nous trouvons le nombre total de jmeter-server: <b>target_amount_jri</b> et la valeur totale de la mémoire <b>requise</b> : <b>required_memory_total</b> . </li><li>  Nous trions tous les générateurs de loadgeneratorN un par un et démarrons le nombre maximum de serveurs jmeter en fonction de la mémoire disponible.  Tant que le nombre d'instances de current_amount_jri en cours d'exécution n'est <b>pas égal à</b> target_amount_jri. </li><li>  (Si le nombre de générateurs et la mémoire totale ne suffisent pas, ajoutez-en un nouveau dans le pool) </li><li>  Nous nous connectons à chaque générateur, en utilisant <b>netstat, nous nous</b> souvenons de tous les ports occupés, et nous exécutons sur des ports aléatoires (qui sont inoccupés) le nombre requis de serveurs jmeter: <br><br><pre> <code class="python hljs"> netstat_cmd= <span class="hljs-string"><span class="hljs-string">'netstat -tulpn | grep LISTEN'</span></span> stdin, stdout, stderr = ssh.exec_command(cmd1) used_ports = [] netstat_output = str(stdout.readlines()) ports = re.findall(<span class="hljs-string"><span class="hljs-string">'\d+\.\d+\.\d+\.\d+\:(\d+)'</span></span>, netstat_output) ports_ipv6 = re.findall(<span class="hljs-string"><span class="hljs-string">'\:\:\:(\d+)'</span></span>, netstat_output) p.wait() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports: used_ports.append(int(port)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports_ipv6: used_ports.append(int(port)) ssh.close() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, possible_jris_on_host + <span class="hljs-number"><span class="hljs-number">1</span></span>): port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> used_ports: port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># ...  Jmeter-    </span></span></code> </pre></li><li>  Nous collectons tous les serveurs jmeter en cours d'exécution en une seule fois au format adresse: port, par exemple <b>générateur13: 15576, générateur9: 14015, générateur11: 19152, générateur14: 12125, générateur2: 17602</b> </li><li>  La liste résultante et threads_per_host sont envoyés au client JMeter lorsque le test démarre: <br><br><pre> <code class="bash hljs">REMOTE_TESTING_FLAG=<span class="hljs-string"><span class="hljs-string">" -R </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$REMOTE_HOSTS_STRING</span></span></span><span class="hljs-string">"</span></span> java -jar -Xms7g -Xmx7g -Xss228k <span class="hljs-variable"><span class="hljs-variable">$JMETER_DIR</span></span>/bin/ApacheJMeter.jar -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -n -t <span class="hljs-variable"><span class="hljs-variable">$TEST_PLAN</span></span> -j <span class="hljs-variable"><span class="hljs-variable">$WORKSPACE</span></span>/loadtest.log -GTHREAD_COUNT=<span class="hljs-variable"><span class="hljs-variable">$THREADS_PER_HOST</span></span> <span class="hljs-variable"><span class="hljs-variable">$OTHER_VARS</span></span> <span class="hljs-variable"><span class="hljs-variable">$REMOTE_TESTING_FLAG</span></span> -Jjmeter.save.saveservice.default_delimiter=,</code> </pre></li></ul><br>  Dans notre cas, le test a eu lieu simultanément à partir de 300 serveurs Jmeter, 500 threads chacun, le format de lancement d'un serveur Jmeter avec des paramètres Java ressemblait à ceci: <br><br><pre> <code class="bash hljs">nohup java -server -Xms1200m -Xmx1200m -Xss228k -XX:+DisableExplicitGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -Djava.net.preferIPv6Addresses=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">false</span></span> -jar <span class="hljs-string"><span class="hljs-string">"/tmp/jmeter-JwKse5nY/bin/ApacheJMeter.jar"</span></span> -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> <span class="hljs-string"><span class="hljs-string">"-Djava.rmi.server.hostname=generator12.loadtest.ig.local"</span></span> -Duser.dir=/tmp/jmeter-JwKse5nY/bin/ -Dserver_port=13114 -s -Jpoll=49 &gt; /dev/null 2&gt;&amp;1</code> </pre> <br><h3>  50ms </h3><br>  La tâche consiste à déterminer dans quelle mesure notre base de données peut supporter, plutôt que de la surcharger et l'ensemble du système dans un état critique.  Avec autant de serveurs Jmeter, vous devez en quelque sorte maintenir la charge à un certain niveau et ne pas tuer tout le système.  Le paramètre <b>TARGET_RESPONSE_TIME</b> spécifié lors du démarrage du test en est responsable.  Nous avons convenu que <b>50 ms</b> est le temps de réponse optimal dont le système devrait être responsable. <br><br>  Dans JMeter, par défaut, il existe de nombreux temporisateurs différents qui vous permettent de contrôler le débit, mais on ne sait pas où l'obtenir dans notre cas.  Mais il y a <b>JSR223-Timer</b> avec lequel vous pouvez trouver quelque chose en utilisant le <b>temps de réponse du</b> système <b>actuel</b> .  Le temporisateur lui-même est dans le bloc de <b>comportement</b> principal: <br><br><img src="https://habrastorage.org/webt/uv/o8/rb/uvo8rb9ph7mr1xhxkzxccsfa06a.png"><br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      = 0 vars.put('samples', '20'); vars.putObject('respAvg', ${TARGET_RESPONSE_TIME}.0); vars.putObject('sleep', 0.0); //  JSR223-Timer           "" double sleep = vars.getObject('sleep'); double respAvg = vars.getObject('respAvg'); double previous = sleep; double target = ${TARGET_RESPONSE_TIME}; if (respAvg &lt; target) { sleep /= 1.5; } if (respAvg &gt; target) { sleep *= 1.1; } sleep = Math.max(10, sleep); //      sleep = Math.min(20000, sleep); vars.putObject('sleep', sleep); return (int)sleep;</span></span></code> </pre><br><h3>  Analyse des résultats (démon) </h3><br>  En plus des graphiques dans Grafana, il est également nécessaire d'avoir des résultats de test agrégés afin que les tests puissent ensuite être comparés dans JLTC. <br><br>  Un tel test génère 16 000 à 20 000 requêtes par seconde, il est facile de calculer qu'en 4 heures, il génère un fichier CSV de quelques centaines de Go, il était donc nécessaire de créer un travail qui analyse les données toutes les minutes, les envoie à la base de données et nettoie le fichier principal. <br><br><img src="https://habrastorage.org/webt/pb/mj/kc/pbmjkcwgjcxupnihgzpzmqd0nvq.png"><br><br>  L'algorithme est le suivant: <br><br><ul><li>  Nous lisons les données du fichier CSV <b>result.jtl</b> généré par le client jmeter, l'enregistrons et nettoyons le fichier (vous devez le nettoyer correctement, sinon le fichier vide aura le même FD avec la même taille): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(jmeter_results_file, <span class="hljs-string"><span class="hljs-string">'r+'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: rows = f.readlines() f.seek(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.truncate(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.writelines(rows[<span class="hljs-number"><span class="hljs-number">-1</span></span>])</code> </pre></li><li>  Nous écrivons les données lues dans le fichier temporaire <b>temp_result.jtl</b> : <br><br><pre> <code class="python hljs">rows_num = len(rows) open(temp_result_filename, <span class="hljs-string"><span class="hljs-string">'w'</span></span>).writelines(rows[<span class="hljs-number"><span class="hljs-number">0</span></span>:rows_num]) <span class="hljs-comment"><span class="hljs-comment"># avoid last line</span></span></code> </pre> </li><li>  Nous lisons le fichier <b>temp_result.jtl</b> .  Nous distribuons les données lues "en quelques minutes": <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> r <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f.readlines(): row = r.split(<span class="hljs-string"><span class="hljs-string">','</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) == <span class="hljs-number"><span class="hljs-number">13</span></span>: ts_c = int(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) dt_c = datetime.datetime.fromtimestamp(ts_c/<span class="hljs-number"><span class="hljs-number">1000</span></span>) minutes_data.setdefault(dt_c.strftime(<span class="hljs-string"><span class="hljs-string">'%Y_%m_%d_%H_%M'</span></span>), []).append(r)</code> </pre></li><li>  Les données pour chaque minute de <b>minutes_data sont</b> écrites dans le fichier correspondant dans le dossier <b>to_parse /</b> .  (ainsi, pour le moment, chaque minute du test a son propre fichier de données, puis lors de l'agrégation <b>,</b> peu importe l'ordre dans lequel les données sont entrées dans chaque fichier): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> key, value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> minutes_data.iteritems(): <span class="hljs-comment"><span class="hljs-comment">#      timestamp (key) temp_ts_file = os.path.join(temp_to_parse_path, key) open(temp_ts_file, 'a+').writelines(value)</span></span></code> </pre></li><li>  En cours de route, nous analysons les fichiers dans le dossier to_parse et si l'un d'eux n'a pas changé en une minute, ce fichier est un candidat pour l'analyse des données, l'agrégation et l'envoi à la base de données JLTC: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(temp_to_parse_path): data_file = os.path.join(temp_to_parse_path, filename) file_mod_time = os.stat(data_file).st_mtime last_time = (time.time() - file_mod_time) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> last_time &gt; <span class="hljs-number"><span class="hljs-number">60</span></span>: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON] File {} was not modified since 1min, adding to parse list.'</span></span>.format(data_file)) files_to_parse.append(data_file)</code> </pre></li><li>  S'il existe de tels fichiers (un ou plusieurs), alors nous les envoyons analysés à la fonction <b>parse_csv_data</b> (chaque fichier en parallèle): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> f <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> files_to_parse: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON THREAD] Parse {}.'</span></span>.format(f)) t = threading.Thread( target=parse_csv_data, args=( f, jmeter_results_file_fields, test, data_resolution)) t.start() threads.append(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> threads: t.join()</code> </pre></li></ul><br>  Le démon lui-même dans cron.d démarre chaque minute: <br><br>  le démon démarre chaque minute avec cron.d: <br><br><pre> <code class="bash hljs">* * * * * root sleep 21 &amp;&amp; /usr/bin/python /var/lib/jltc/manage.py daemon</code> </pre> <br>  Ainsi, le fichier avec les résultats ne gonfle pas à des tailles inconcevables, mais est analysé <i>à la volée</i> et effacé. <br><br><h2>  Résultats </h2><br><h3>  L'appli </h3><br>  Nos 150 000 joueurs virtuels: <br><br><img src="https://habrastorage.org/webt/sv/ex/ep/svexepi9ikzy0unpxur96mty5q8.png"><br><br>  Le test tente de «faire correspondre» le temps de réponse de 50 ms, de sorte que la charge elle-même saute constamment dans la région entre 16 000 et 18 000 requêtes / c: <br><br><img src="https://habrastorage.org/webt/-z/98/oi/-z98oi-_8a41hkqbrmz2rvzmryk.png"><br><br>  Charge du serveur d'applications (15 applications).  Deux serveurs sont «malchanceux» d'être sur le M620 plus lent: <br><br><img src="https://habrastorage.org/webt/nc/xy/et/ncxyetqyk_a8mbhjocndd-yxgkm.png"><br><br>  Temps de réponse de la base de données (pour les serveurs d'applications): <br><br><img src="https://habrastorage.org/webt/zr/a-/gw/zra-gwtq_vqhtfhlrjzzy1osisg.png"><br><br><h3>  Base de données </h3><br>  Utilisation du processeur sur db1 (VM): <br><br><img src="https://habrastorage.org/webt/ej/hn/in/ejhnin0jo_7rrzhlj7pqko6udnq.png"><br><br>  Utilisation du processeur sur l'hyperviseur: <br><br><img src="https://habrastorage.org/webt/uw/ik/tz/uwiktzvcdzydlsjaoexqfbr3tay.png"><br><br>  La charge sur la machine virtuelle est plus faible, car elle estime qu'elle dispose de 48 cœurs réels, en fait, il y a 24 cœurs <b>hyperthreading</b> sur l'hyperviseur. <br><br>  Un <b>maximum de ~ 250 000 requêtes / s</b> va à la base de données, composée de (83% de sélections, 3% - insertions, 11,6% - mises à jour (90% HOT), 1,6% suppressions): <br><br><img src="https://habrastorage.org/webt/lx/lu/bl/lxlublobhm4nm3c45g9jikcstc4.png"><br><br><img src="https://habrastorage.org/webt/18/jw/gp/18jwgpmebrkyot3ngvarsl_3ysu.png"><br><br>  Avec une valeur par défaut <b>autovacuum_vacuum_scale_factor</b> = 0,2, le nombre de tuples morts a augmenté très rapidement avec le test (avec l'augmentation de la taille des tables), ce qui a conduit plusieurs fois à de courts problèmes de performances de la base de données qui ont ruiné plusieurs fois l'ensemble du test.  J'ai dû «apprivoiser» cette croissance pour certaines tables en attribuant des valeurs personnelles à ce paramètre autovacuum_vacuum_scale_factor: <br><br><div class="spoiler">  <b class="spoiler_title">ALTER TABLE ... SET (autovacuum_vacuum_scale_factor = ...)</b> <div class="spoiler_text">  ALTER TABLE construction SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE production SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE core_inventory_player_resource SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE survivor SET (autovacuum_vacuum_scale_factor = 0,01); <br>  ALTER TABLE survivor SET (autovacuum_analyze_scale_factor = 0.01); <br></div></div><br><img src="https://habrastorage.org/webt/0s/ja/1e/0sja1e1m3-2gitbmhh8j24t2ktu.png"><br><br>  Idéalement, row_fetched devrait être proche de row_returned, ce que nous observons heureusement: <br><br><img src="https://habrastorage.org/webt/e4/k6/zo/e4k6zobp25h5asrmxhmficoea3a.png"><br><br><h4>  hot_standby_feedback </h4><br>  Le problème <b>venait du</b> paramètre <b>hot_standby_feedback</b> , qui peut considérablement affecter les performances du serveur <b>principal</b> si ses serveurs de <b>secours</b> n'ont pas le temps d'appliquer les modifications des fichiers WAL.  La documentation (https://postgrespro.ru/docs/postgrespro/11/runtime-config-replication) indique qu'elle «détermine si le serveur de secours à chaud notifiera le maître ou l'esclave supérieur des demandes qu'il exécute actuellement».  Par défaut, il est désactivé, mais il a été activé dans notre configuration.  Ce qui a entraîné de tristes conséquences, s'il y a 2 serveurs de secours et que le décalage de réplication pendant le chargement est différent de zéro (pour diverses raisons), vous pouvez observer une telle image, ce qui peut conduire à l'effondrement de l'ensemble du test: <br><br><img src="https://habrastorage.org/webt/vl/1l/jd/vl1ljdockxrjlfsoiybq751vo9y.png"><br><br><img src="https://habrastorage.org/webt/qh/4s/m_/qh4sm_hpnunb2w0axzhk90lmf6u.png"><br><br>  Cela est dû au fait que lorsque hot_standby_feedback est activé, VACUUM ne souhaite pas supprimer les tuples «morts» si les serveurs de secours sont en retard dans leur ID de transaction pour éviter les conflits de réplication.  Article détaillé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ce que fait vraiment hot_standby_feedback dans PostgreSQL</a> : <br><br><pre> <code class="plaintext hljs">xl1_game=# VACUUM VERBOSE core_inventory_player_resource; INFO: vacuuming "public.core_inventory_player_resource" INFO: scanned index "core_inventory_player_resource_pkey" to remove 62869 row versions DETAIL: CPU: user: 1.37 s, system: 0.58 s, elapsed: 4.20 s ………... INFO: "core_inventory_player_resource": found 13682 removable, 7257082 nonremovable row versions in 71842 out of 650753 pages &lt;b&gt;DETAIL: 3427824 dead row versions cannot be removed yet, oldest xmin: 3810193429&lt;/b&gt; There were 1920498 unused item pointers. Skipped 8 pages due to buffer pins, 520953 frozen pages. 0 pages are entirely empty. CPU: user: 4.55 s, system: 1.46 s, elapsed: 11.74 s.</code> </pre><br>  Un si grand nombre de tuples morts mène à l'image ci-dessus.  Voici deux tests, avec hot_standby_feedback activé et désactivé: <br><br><img src="https://habrastorage.org/webt/8f/od/n6/8fodn60lohgzsu-twheqysldjzy.png"><br><br>  Et c'est notre retard de réplication pendant le test, avec lequel il sera nécessaire de faire quelque chose à l'avenir: <br><br><img src="https://habrastorage.org/webt/et/s0/7h/ets07hkl8skkv5wexxjrgi-3x7m.png"><br><br><h2>  Conclusion </h2><br>  Ce test, heureusement (ou malheureusement pour le contenu de l'article) a montré qu'à ce stade du prototype du jeu, il est tout à fait possible d'absorber la charge souhaitée de la part des utilisateurs, ce qui est suffisant pour donner le feu vert pour la poursuite du prototypage et du développement.  Dans les étapes ultérieures du développement, il est nécessaire de suivre les règles de base (pour garder la simplicité des requêtes exécutées, pour éviter une surabondance d'index, ainsi que des lectures non indexées, etc.) et surtout, tester le projet à chaque étape importante du développement pour trouver et résoudre les problèmes comme peut être plus tôt.  Peut-être bientôt, j'écrirai un article car nous avons déjà résolu des problèmes spécifiques. <br><br>  Bonne chance à tous! <br><br>  Notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> au cas où;) </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr445368/">https://habr.com/ru/post/fr445368/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr445356/index.html">Aperçu de la section mobile à DUMP-2019: maximum appliqué et utile dans le travail quotidien</a></li>
<li><a href="../fr445358/index.html">Organisation du système d'événements dans Unity - à travers les yeux d'un game designer</a></li>
<li><a href="../fr445360/index.html">5 tâches typiques pour les interviews JavaScript: analyse et solutions</a></li>
<li><a href="../fr445362/index.html">Le livre "Distributed Systems. Modèles de conception</a></li>
<li><a href="../fr445366/index.html">Comment accélérer le cryptage selon GOST 28147-89 sur le processeur Baikal-T1 en raison du bloc SIMD</a></li>
<li><a href="../fr445370/index.html">Analyse TSDB dans Prométhée 2</a></li>
<li><a href="../fr445372/index.html">Vision industrielle vs intuition humaine: algorithmes pour perturber le fonctionnement des programmes de reconnaissance d'objets</a></li>
<li><a href="../fr445378/index.html">Labyrinthes: classification, génération, recherche de solutions</a></li>
<li><a href="../fr445380/index.html">Le PHP moderne est beau et productif</a></li>
<li><a href="../fr445384/index.html">Mission Chang'e-4 - équipement scientifique sur le module d'atterrissage et le satellite répéteur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>