<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🚒 🎂 😊 Auf der Suche nach freien Parkplätzen mit Python 👩🏻‍🤝‍👨🏿 🎅🏿 💘</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ich lebe in einer guten Stadt. Aber wie bei vielen anderen wird die Suche nach einem Parkplatz immer zum Test. Freie Plätze sind schnell besetzt, und ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Auf der Suche nach freien Parkplätzen mit Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451164/"><img src="https://habrastorage.org/webt/vz/x5/od/vzx5odyqel0ow-z2qolfdo1htd4.gif" alt="Bild"><br><br>  Ich lebe in einer guten Stadt.  Aber wie bei vielen anderen wird die Suche nach einem Parkplatz immer zum Test.  Freie Plätze sind schnell besetzt, und selbst wenn Sie eigene haben, wird es für Freunde schwierig sein, Sie anzurufen, da sie nirgendwo parken können. <br><br>  Also habe ich beschlossen, die Kamera aus dem Fenster zu richten und Deep Learning zu verwenden, damit mein Computer mir sagt, wann der Platz verfügbar ist: <br><br><img src="https://habrastorage.org/webt/lx/md/gy/lxmdgyxkvnwtwc5nsqccy83mp34.gif" alt="Bild"><br><br>  Es mag kompliziert klingen, aber tatsächlich ist das Schreiben eines funktionierenden Prototyps mit tiefem Lernen schnell und einfach.  Alle notwendigen Komponenten sind bereits vorhanden - Sie müssen nur wissen, wo Sie sie finden und wie Sie sie zusammensetzen können. <br><br>  Also lasst uns Spaß haben und ein genaues kostenloses Parkbenachrichtigungssystem mit Python und Deep Learning schreiben <a name="habracut"></a><br><br><h3>  Aufgabe zerlegen </h3><br>  Wenn wir eine schwierige Aufgabe haben, die wir mit maschinellem Lernen lösen möchten, besteht der erste Schritt darin, sie in eine Folge einfacher Aufgaben aufzuteilen.  Dann können wir verschiedene Werkzeuge verwenden, um jedes von ihnen zu lösen.  Durch die Kombination mehrerer einfacher Lösungen erhalten wir ein System, das zu etwas Komplexem fähig ist. <br><br>  So habe ich meine Aufgabe gebrochen: <br><br><img src="https://habrastorage.org/webt/q7/gi/hi/q7gihifth7-k9mad7fhgbj4itcc.jpeg" alt="Bild"><br><br>  Der auf das Fenster gerichtete Videostream von der Webcam wird in die Förderereingabe eingegeben: <br><br><img src="https://habrastorage.org/webt/aa/wk/ig/aawkigsexhbk5s4slqmvksvofcm.gif" alt="Bild"><br><br>  Über die Pipeline übertragen wir jedes Bild des Videos einzeln. <br><br>  Der erste Schritt besteht darin, alle möglichen Parkplätze im Rahmen zu erkennen.  Bevor wir nach unbesetzten Plätzen suchen können, müssen wir natürlich verstehen, in welchen Teilen des Bildes sich Parkplätze befinden. <br><br>  Dann müssen Sie auf jedem Rahmen alle Autos finden.  Auf diese Weise können wir die Bewegung jeder Maschine von Bild zu Bild verfolgen. <br><br>  Der dritte Schritt besteht darin, festzustellen, welche Plätze von Maschinen belegt sind und welche nicht.  Kombinieren Sie dazu die Ergebnisse der ersten beiden Schritte. <br><br>  Schließlich sollte das Programm eine Warnung senden, wenn der Parkplatz frei wird.  Dies wird durch Änderungen der Position der Maschinen zwischen den Bildern des Videos bestimmt. <br><br>  Jeder dieser Schritte kann auf unterschiedliche Weise mit unterschiedlichen Technologien ausgeführt werden.  Es gibt keinen einzigen richtigen oder falschen Weg, um diesen Förderer zusammenzusetzen, verschiedene Ansätze haben ihre Vor- und Nachteile.  Lassen Sie uns jeden Schritt genauer behandeln. <br><br><h3>  Wir erkennen Parkplätze </h3><br>  Folgendes sieht unsere Kamera: <br><br><img src="https://habrastorage.org/webt/2u/zl/xt/2uzlxtgxbn6jvfkhfy0e523ow88.png" alt="Bild"><br><br>  Wir müssen dieses Bild irgendwie scannen und eine Liste der Parkplätze erhalten: <br><br><img src="https://habrastorage.org/webt/m-/bq/xb/m-bqxb9ybcjc44blvsuzhnw6xyk.png" alt="Bild"><br><br>  Die Lösung „in der Stirn“ wäre, die Positionen aller Parkplätze einfach manuell fest zu codieren, anstatt sie automatisch zu erkennen.  In diesem Fall müssen wir den gesamten Vorgang erneut ausführen, wenn wir die Kamera bewegen oder nach Parkplätzen in einer anderen Straße suchen möchten.  Es klingt so lala, also suchen wir nach einer automatischen Methode, um Parkplätze zu erkennen. <br><br>  Alternativ können Sie im Bild nach Parkuhren suchen und davon ausgehen, dass sich neben jeder Parkuhr ein Parkplatz befindet: <br><br><img src="https://habrastorage.org/webt/qi/g8/cj/qig8cjwmp7dmduejcddjk6tnoiw.png" alt="Bild"><br><br>  Bei diesem Ansatz ist jedoch nicht alles so reibungslos.  Erstens hat nicht jeder Parkplatz eine Parkuhr, und tatsächlich sind wir mehr daran interessiert, Parkplätze zu finden, für die Sie nicht bezahlen müssen.  Zweitens sagt uns die Position der Parkuhr nichts darüber aus, wo sich der Parkplatz befindet, sondern lässt uns nur eine Annahme treffen. <br><br>  Eine andere Idee besteht darin, ein Objekterkennungsmodell zu erstellen, das nach auf der Straße gezeichneten Parkmarkierungen sucht: <br><br><img src="https://habrastorage.org/webt/bo/vv/nu/bovvnu6rsl-zimlr1gtpp1a_egm.png" alt="Bild"><br><br>  Aber dieser Ansatz ist so lala.  Erstens sind in meiner Stadt alle diese Markierungen sehr klein und aus der Ferne schwer zu erkennen, so dass es schwierig sein wird, sie mit einem Computer zu erkennen.  Zweitens ist die Straße voller allerlei anderer Linien und Markierungen.  Es wird schwierig sein, Parkmarken von Fahrspurtrennern und Fußgängerüberwegen zu trennen. <br><br>  Wenn Sie auf ein Problem stoßen, das auf den ersten Blick schwierig erscheint, nehmen Sie sich ein paar Minuten Zeit, um einen anderen Lösungsansatz zu finden, mit dem Sie einige technische Probleme umgehen können.  Was gibt es einen Parkplatz?  Dies ist nur ein Ort, an dem ein Auto lange Zeit geparkt ist.  Vielleicht müssen wir Parkplätze überhaupt nicht erkennen.  Warum erkennen wir nicht einfach die Autos, die lange still stehen und gehen nicht davon aus, dass sie auf dem Parkplatz stehen? <br><br>  Mit anderen Worten, Parkplätze befinden sich dort, wo Autos lange stehen: <br><br><img src="https://habrastorage.org/webt/b8/tb/ua/b8tbuafyf4uci3jy61jnjlwanqa.png" alt="Bild"><br><br>  Wenn wir also die Autos erkennen und herausfinden können, welche sich nicht zwischen den Rahmen bewegen, können wir erraten, wo sich die Parkplätze befinden.  So einfach ist das - gehen Sie zur Maschinenerkennung! <br><br><h3>  Autos erkennen </h3><br>  Das Erkennen von Autos auf einem Videorahmen ist eine klassische Objekterkennungsaufgabe.  Es gibt viele Ansätze des maschinellen Lernens, die wir zur Erkennung verwenden könnten.  Hier sind einige davon in der Reihenfolge von der "alten Schule" zur "neuen Schule": <br><br><ul><li>  Sie können den Detektor basierend auf HOG (Histogramm orientierter Gradienten, Histogramme gerichteter Gradienten) trainieren und ihn durch das gesamte Bild führen, um alle Autos zu finden.  Dieser alte Ansatz, bei dem kein tiefes Lernen verwendet wird, funktioniert relativ schnell, kommt jedoch mit Maschinen, die sich auf unterschiedliche Weise befinden, nicht sehr gut zurecht. </li><li>  Sie können den CNN-basierten Detektor (Convolutional Neural Network, ein Convolutional Neural Network) trainieren und ihn durch das gesamte Bild führen, bis wir alle Maschinen gefunden haben.  Dieser Ansatz funktioniert genau, aber nicht so effizient, da wir das Bild mehrmals mit CNN scannen müssen, um alle Maschinen zu finden.  Und obwohl wir Maschinen auf unterschiedliche Weise finden können, benötigen wir viel mehr Trainingsdaten als für einen HOG-Detektor. </li><li>  Sie können einen neuen Ansatz mit Deep Learning wie Mask R-CNN, Faster R-CNN oder YOLO verwenden, der die Genauigkeit von CNN und eine Reihe technischer Tricks kombiniert, die die Erkennungsgeschwindigkeit erheblich erhöhen.  Solche Modelle funktionieren relativ schnell (auf der GPU), wenn wir viele Daten haben, um das Modell zu trainieren. </li></ul><br>  Im allgemeinen Fall benötigen wir die einfachste Lösung, die ordnungsgemäß funktioniert und die geringste Menge an Trainingsdaten erfordert.  Dies muss nicht der neueste und schnellste Algorithmus sein.  Insbesondere in unserem Fall ist Mask R-CNN jedoch eine vernünftige Wahl, obwohl es recht neu und schnell ist. <br><br>  Die R-CNN-Architektur der Maske ist so konzipiert, dass sie Objekte im gesamten Bild erkennt, Ressourcen effektiv verbraucht und nicht den Schiebefensteransatz verwendet.  Mit anderen Worten, es funktioniert ziemlich schnell.  Mit einer modernen GPU können wir Objekte in Videos in hoher Auflösung mit einer Geschwindigkeit von mehreren Bildern pro Sekunde erkennen.  Für unser Projekt sollte dies ausreichen. <br><br>  Darüber hinaus bietet Mask R-CNN viele Informationen zu jedem erkannten Objekt.  Die meisten Erkennungsalgorithmen geben für jedes Objekt nur einen Begrenzungsrahmen zurück.  Die Maske R-CNN gibt uns jedoch nicht nur die Position jedes Objekts, sondern auch dessen Umriss (Maske): <br><br><img src="https://habrastorage.org/webt/n2/b0/hp/n2b0hpwgwpkn6ahfhqetvbhq1rg.png" alt="Bild"><br><br>  Um Mask R-CNN zu trainieren, benötigen wir viele Bilder von Objekten, die wir erkennen möchten.  Wir könnten nach draußen gehen, Fotos von Autos machen und sie auf Fotos markieren, was mehrere Arbeitstage erfordern würde.  Glücklicherweise gehören Autos zu den Objekten, die Menschen häufig erkennen möchten. Daher existieren bereits mehrere öffentliche Datensätze mit Bildern von Autos. <br><br>  Eines davon ist das beliebte SOCO- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dataset</a> (kurz für Common Objects In Context), dessen Bilder mit Objektmasken versehen sind.  Dieser Datensatz enthält über 12.000 Bilder mit bereits gekennzeichneten Maschinen.  Hier ist ein Beispielbild aus dem Datensatz: <br><br><img src="https://habrastorage.org/webt/dv/lz/7l/dvlz7ltgwmudog9-b2f6i7tlmhe.jpeg" alt="Bild"><br><br>  Solche Daten eignen sich hervorragend zum Trainieren eines auf Mask R-CNN basierenden Modells. <br><br>  Aber halt die Pferde, es gibt noch bessere Neuigkeiten!  Wir sind nicht die ersten, die ihr Modell anhand des COCO-Datensatzes trainieren wollten - viele Menschen haben dies bereits vor uns getan und ihre Ergebnisse geteilt.  Anstatt unser Modell zu trainieren, können wir daher ein fertiges Modell nehmen, das bereits Autos erkennt.  Für unser Projekt werden wir das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Open-Source-Modell von Matterport verwenden.</a> <br><br>  Wenn wir dem Eingang dieses Modells ein Bild von der Kamera geben, erhalten wir dies bereits „out of the box“: <br><br><img src="https://habrastorage.org/webt/vy/kq/50/vykq50pcxhyt_vkmfzmxk_fgl5g.png" alt="Bild"><br><br>  Das Modell erkannte nicht nur Autos, sondern auch Objekte wie Ampeln und Personen.  Es ist lustig, dass sie den Baum als Zimmerpflanze erkannte. <br><br>  Für jedes erkannte Objekt gibt das Mask R-CNN-Modell 4 Dinge zurück: <br><br><ul><li>  Art des erkannten Objekts (Ganzzahl).  Das vorgefertigte COCO-Modell kann 80 verschiedene gemeinsame Objekte wie Autos und Lastwagen erkennen.  Eine vollständige Liste finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier.</a> </li><li>  Der Grad des Vertrauens in die Erkennungsergebnisse.  Je höher die Zahl, desto stärker ist das Modell von der Erkennung des Objekts überzeugt. </li><li>  Ein Begrenzungsrahmen für ein Objekt in Form von XY-Koordinaten von Pixeln im Bild. </li><li>  Eine „Maske“, die anzeigt, welche Pixel innerhalb des Begrenzungsrahmens Teil des Objekts sind.  Mithilfe der Maskendaten können Sie den Umriss des Objekts ermitteln. </li></ul><br>  Unten finden Sie den Python-Code zum Erkennen des Begrenzungsrahmens für Maschinen mithilfe der vorab trainierten Modelle Mask R-CNN und OpenCV: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-comment"><span class="hljs-comment"># ,     Mask-RCNN. class MaskRCNNConfig(mrcnn.config.Config): NAME = "coco_pretrained_model_config" IMAGES_PER_GPU = 1 GPU_COUNT = 1 NUM_CLASSES = 1 + 80 #   COCO  80  + 1  . DETECTION_MIN_CONFIDENCE = 0.6 #    ,    . def get_car_boxes(boxes, class_ids): car_boxes = [] for i, box in enumerate(boxes): #     ,   . if class_ids[i] in [3, 8, 6]: car_boxes.append(box) return np.array(car_boxes) #   . ROOT_DIR = Path(".") #       . MODEL_DIR = ROOT_DIR / "logs" #       . COCO_MODEL_PATH = ROOT_DIR / "mask_rcnn_coco.h5" #   COCO  . if not COCO_MODEL_PATH.exists(): mrcnn.utils.download_trained_weights(COCO_MODEL_PATH) #     . IMAGE_DIR = ROOT_DIR / "images" #      —   0,    ,   . VIDEO_SOURCE = "test_images/parking.mp4" #   Mask-RCNN   . model = MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=MaskRCNNConfig()) #   . model.load_weights(COCO_MODEL_PATH, by_name=True) #   . parked_car_boxes = None #  ,     . video_capture = cv2.VideoCapture(VIDEO_SOURCE) #      . while video_capture.isOpened(): success, frame = video_capture.read() if not success: break #      BGR ( OpenCV)  RGB. rgb_image = frame[:, :, ::-1] #    Mask R-CNN   . results = model.detect([rgb_image], verbose=0) # Mask R-CNN ,       . #     ,     . r = results[0] #  r    : # - r['rois'] —      ; # - r['class_ids'] —  () ; # - r['scores'] —  ; # - r['masks'] —   (    ). #      . car_boxes = get_car_boxes(r['rois'], r['class_ids']) print("Cars found in frame of video:") #     . for box in car_boxes: print("Car:", box) y1, x1, y2, x2 = box #  . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1) #    . cv2.imshow('Video', frame) #  'q',  . if cv2.waitKey(1) &amp; 0xFF == ord('q'): break #    . video_capture.release() cv2.destroyAllWindows()</span></span></code> </pre> <br>  Nach dem Ausführen dieses Skripts wird auf dem Bildschirm ein Bild mit einem Rahmen um jeden erkannten Computer angezeigt: <br><br><img src="https://habrastorage.org/webt/_p/il/0r/_pil0reoz3gj7dtqboav_rgerl8.jpeg" alt="Bild"><br><br>  Außerdem werden die Koordinaten jeder Maschine in der Konsole angezeigt: <br><br><pre> <code class="python hljs">Cars found <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> frame of video: Car: [<span class="hljs-number"><span class="hljs-number">492</span></span> <span class="hljs-number"><span class="hljs-number">871</span></span> <span class="hljs-number"><span class="hljs-number">551</span></span> <span class="hljs-number"><span class="hljs-number">961</span></span>] Car: [<span class="hljs-number"><span class="hljs-number">450</span></span> <span class="hljs-number"><span class="hljs-number">819</span></span> <span class="hljs-number"><span class="hljs-number">509</span></span> <span class="hljs-number"><span class="hljs-number">913</span></span>] Car: [<span class="hljs-number"><span class="hljs-number">411</span></span> <span class="hljs-number"><span class="hljs-number">774</span></span> <span class="hljs-number"><span class="hljs-number">470</span></span> <span class="hljs-number"><span class="hljs-number">856</span></span>]</code> </pre><br>  So haben wir gelernt, Autos im Bild zu erkennen. <br><br><h3>  Wir erkennen leere Parkplätze </h3><br>  Wir kennen die Pixelkoordinaten jeder Maschine.  Wenn wir mehrere aufeinanderfolgende Frames durchsehen, können wir leicht feststellen, welches der Autos sich nicht bewegt hat, und davon ausgehen, dass es Parkplätze gibt.  Aber wie kann man verstehen, dass das Auto den Parkplatz verlassen hat? <br><br>  Das Problem ist, dass sich die Rahmen der Maschinen teilweise überlappen: <br><br><img src="https://habrastorage.org/webt/7t/vi/4q/7tvi4q1rgvkfkaljrsp8sjathr0.jpeg" alt="Bild"><br><br>  Wenn Sie sich also vorstellen, dass jeder Rahmen einen Parkplatz darstellt, kann sich herausstellen, dass er teilweise von der Maschine belegt ist, obwohl er tatsächlich leer ist.  Wir müssen einen Weg finden, um den Schnittgrad zweier Objekte zu messen, um nur nach den „leersten“ Frames zu suchen. <br><br>  Wir werden ein Maß namens Intersection Over Union (Verhältnis von Schnittfläche zu Gesamtfläche) oder IoU verwenden.  IoU kann ermittelt werden, indem die Anzahl der Pixel berechnet wird, an denen sich zwei Objekte schneiden, und durch die Anzahl der von diesen Objekten belegten Pixel dividiert wird: <br><br><img src="https://habrastorage.org/webt/zs/c0/sz/zsc0szsct8xjwkx5eo-6ieynfuc.png" alt="Bild"><br><br>  So können wir verstehen, wie sich der sehr begrenzende Rahmen des Autos mit dem Rahmen des Parkplatzes schneidet.  Auf diese Weise können Sie leicht feststellen, ob das Parken kostenlos ist.  Wenn der IoU-Wert niedrig ist, wie z. B. 0,15, nimmt das Auto einen kleinen Teil des Parkplatzes ein.  Und wenn es hoch ist, wie 0,6, bedeutet dies, dass das Auto den größten Teil des Platzes einnimmt und Sie dort nicht parken können. <br><br>  Da IoU in der Bildverarbeitung häufig verwendet wird, ist es sehr wahrscheinlich, dass die entsprechenden Bibliotheken diese Maßnahme implementieren.  In unserer Bibliothek Mask R-CNN ist sie als Funktion mrcnn.utils.compute_overlaps () implementiert. <br><br>  Wenn wir eine Liste von Begrenzungsrahmen für Parkplätze haben, können Sie eine Überprüfung auf das Vorhandensein von Autos in diesem Rahmen hinzufügen, indem Sie eine oder zwei ganze Codezeilen hinzufügen: <br><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#      . car_boxes = get_car_boxes(r['rois'], r['class_ids']) # ,        . overlaps = mrcnn.utils.compute_overlaps(car_boxes, parking_areas) print(overlaps)</span></span></code> </pre><br>  Das Ergebnis sollte ungefähr so ​​aussehen: <br><br><pre> <code class="python hljs">[ [<span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">0.07040032</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] [<span class="hljs-number"><span class="hljs-number">0.07040032</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">0.07673165</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] [<span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.02332112</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] ]</code> </pre><br>  In diesem zweidimensionalen Array spiegelt jede Zeile einen Rahmen des Parkplatzes wider.  Und jede Spalte gibt an, wie stark sich jeder Ort mit einer der erkannten Maschinen schneidet.  Ein Ergebnis von 1,0 bedeutet, dass der gesamte Platz vollständig vom Auto belegt ist, und ein niedriger Wert wie 0,02 zeigt an, dass das Auto ein wenig an seinen Platz geklettert ist, aber Sie können trotzdem darauf parken. <br><br>  Um nicht belegte Plätze zu finden, müssen Sie nur jede Zeile in diesem Array überprüfen.  Wenn alle Zahlen nahe Null sind, ist der Platz höchstwahrscheinlich frei! <br><br>  Beachten Sie jedoch, dass die Objekterkennung bei Echtzeitvideos nicht immer perfekt funktioniert.  Obwohl das auf Mask R-CNN basierende Modell ziemlich genau ist, kann es von Zeit zu Zeit ein oder zwei Autos in einem Frame des Videos vermissen.  Bevor Sie behaupten, dass der Platz frei ist, müssen Sie daher sicherstellen, dass dies auch für die nächsten 5 bis 10 nächsten Videobilder der Fall ist.  Auf diese Weise können wir Situationen vermeiden, in denen das System fälschlicherweise einen leeren Platz aufgrund eines Fehlers in einem Bild des Videos markiert.  Sobald wir sicherstellen, dass der Platz für mehrere Frames frei bleibt, können Sie eine Nachricht senden! <br><br><h3>  SMS senden </h3><br>  Der letzte Teil unseres Förderers sendet SMS-Benachrichtigungen, wenn ein freier Parkplatz angezeigt wird. <br><br>  Das Senden einer Nachricht von Python ist sehr einfach, wenn Sie Twilio verwenden.  Twilio ist eine beliebte API, mit der Sie SMS aus nahezu jeder Programmiersprache mit nur wenigen Codezeilen senden können.  Wenn Sie einen anderen Dienst bevorzugen, können Sie ihn natürlich nutzen.  Ich habe nichts mit Twilio zu tun, es ist nur das erste, was mir in den Sinn kommt. <br><br>  Um Twilio zu verwenden, melden Sie sich für ein Testkonto an, erstellen Sie eine Twilio-Telefonnummer und erhalten Sie Informationen zur Kontoauthentifizierung.  Installieren Sie dann die Client-Bibliothek: <br><br><pre> <code class="python hljs">$ pip3 install twilio</code> </pre><br>  Verwenden Sie danach den folgenden Code, um die Nachricht zu senden: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> twilio.rest <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Client <span class="hljs-comment"><span class="hljs-comment">#   Twilio. twilio_account_sid = ' Twilio SID' twilio_auth_token = '   Twilio' twilio_source_phone_number = '   Twilio' #    Twilio. client = Client(twilio_account_sid, twilio_auth_token) #  SMS. message = client.messages.create( body=" ", from_=twilio_source_phone_number, to=" ,   " )</span></span></code> </pre><br>  Kopieren Sie diesen Code einfach dorthin, um die Möglichkeit zum Senden von Nachrichten an unser Skript hinzuzufügen.  Sie müssen jedoch sicherstellen, dass die Nachricht nicht in jedem Frame gesendet wird, in dem Sie den freien Speicherplatz sehen können.  Daher haben wir ein Flag, das im installierten Zustand das Senden von Nachrichten für einige Zeit oder bis ein anderer Ort frei ist, nicht zulässt. <br><br><h3>  Alles zusammenfügen </h3><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> twilio.rest <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Client <span class="hljs-comment"><span class="hljs-comment"># ,     Mask-RCNN. class MaskRCNNConfig(mrcnn.config.Config): NAME = "coco_pretrained_model_config" IMAGES_PER_GPU = 1 GPU_COUNT = 1 NUM_CLASSES = 1 + 80 #   COCO  80  + 1  . DETECTION_MIN_CONFIDENCE = 0.6 #    ,    . def get_car_boxes(boxes, class_ids): car_boxes = [] for i, box in enumerate(boxes): #     ,   . if class_ids[i] in [3, 8, 6]: car_boxes.append(box) return np.array(car_boxes) #  Twilio. twilio_account_sid = ' Twilio SID' twilio_auth_token = '   Twilio' twilio_phone_number = '   Twilio' destination_phone_number = ',   ' client = Client(twilio_account_sid, twilio_auth_token) #   . ROOT_DIR = Path(".") #       . MODEL_DIR = ROOT_DIR / "logs" #       . COCO_MODEL_PATH = ROOT_DIR / "mask_rcnn_coco.h5" #   COCO  . if not COCO_MODEL_PATH.exists(): mrcnn.utils.download_trained_weights(COCO_MODEL_PATH) #     . IMAGE_DIR = ROOT_DIR / "images" #      —   0,   ,   . VIDEO_SOURCE = "test_images/parking.mp4" #   Mask-RCNN   . model = MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=MaskRCNNConfig()) #   . model.load_weights(COCO_MODEL_PATH, by_name=True) #   . parked_car_boxes = None #  ,     . video_capture = cv2.VideoCapture(VIDEO_SOURCE) #         . free_space_frames = 0 #    SMS? sms_sent = False #      . while video_capture.isOpened(): success, frame = video_capture.read() if not success: break #      BGR  RGB. rgb_image = frame[:, :, ::-1] #    Mask R-CNN   . results = model.detect([rgb_image], verbose=0) # Mask R-CNN ,       . #     ,     . r = results[0] #  r    : # - r['rois'] —      ; # - r['class_ids'] —  () ; # - r['scores'] —  ; # - r['masks'] —   (    ). if parked_car_boxes is None: #     — ,       . #            . parked_car_boxes = get_car_boxes(r['rois'], r['class_ids']) else: #   ,  . ,   . #     . car_boxes = get_car_boxes(r['rois'], r['class_ids']) # ,         . overlaps = mrcnn.utils.compute_overlaps(parked_car_boxes, car_boxes) # ,    ,      . free_space = False #        . for parking_area, overlap_areas in zip(parked_car_boxes, overlaps): #        #    (, ). max_IoU_overlap = np.max(overlap_areas) #         . y1, x1, y2, x2 = parking_area # ,   ,   IoU. if max_IoU_overlap &lt; 0.15: #  !     . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3) # ,        . free_space = True else: #     —   . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 1) #   IoU  . font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(frame, f"{max_IoU_overlap:0.2}", (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255)) #       ,   . #   ,  ,     #      . if free_space: free_space_frames += 1 else: #   ,  . free_space_frames = 0 #       ,  ,   . if free_space_frames &gt; 10: #   SPACE AVAILABLE!!  . font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(frame, f"SPACE AVAILABLE!", (10, 150), font, 3.0, (0, 255, 0), 2, cv2.FILLED) #  ,     . if not sms_sent: print("SENDING SMS!!!") message = client.messages.create( body="Parking space open - go go go!", from_=twilio_phone_number, to=destination_phone_number ) sms_sent = True #    . cv2.imshow('Video', frame) #  'q',  . if cv2.waitKey(1) &amp; 0xFF == ord('q'): break #  'q',  . video_capture.release() cv2.destroyAllWindows()</span></span></code> </pre><br>  Um diesen Code auszuführen, müssen Sie zuerst Python 3.6+, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Matterport Mask R-CNN</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenCV</a> installieren. <br><br>  Ich habe den Code speziell so einfach wie möglich geschrieben.  Wenn er zum Beispiel im ersten Frame ein Auto sieht, kommt er zu dem Schluss, dass alle geparkt sind.  Versuchen Sie, damit zu experimentieren, und prüfen Sie, ob Sie die Zuverlässigkeit verbessern können. <br><br>  Durch einfaches Ändern der Bezeichner der Objekte, nach denen das Modell sucht, können Sie den Code in etwas völlig anderes verwandeln.  Stellen Sie sich zum Beispiel vor, Sie arbeiten in einem Skigebiet.  Nach einigen Änderungen können Sie dieses Skript in ein System verwandeln, das Snowboarder, die von einer Rampe springen, automatisch erkennt und Videos mit coolen Sprüngen aufzeichnet.  Wenn Sie in einem Naturschutzgebiet arbeiten, können Sie auch ein System erstellen, das Zebras zählt.  Sie sind nur durch Ihre Vorstellungskraft begrenzt. <br><br>  Weitere solche Artikel finden Sie im Telegrammkanal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neuron</a> (@neurondata) <br><br>  Link zur alternativen Übersetzung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tproger.ru/translations/parking-searching/</a> <br><br>  Alles Wissen.  Experimentieren Sie! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451164/">https://habr.com/ru/post/de451164/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451152/index.html">Der Widerstand in der Gate-Schaltung oder wie man es richtig macht</a></li>
<li><a href="../de451154/index.html">Lokales autonomes Datenerfassungssystem (Fortsetzung)</a></li>
<li><a href="../de451158/index.html">Stromkreise. Schaltungstypen</a></li>
<li><a href="../de451160/index.html">Apache Kafka und Streaming mit Spark Streaming</a></li>
<li><a href="../de451162/index.html">Fehlerkorrektur - Physikalische Konstanten in der gegenwärtigen und neuen Version des Internationalen Einheitensystems (SI)</a></li>
<li><a href="../de451166/index.html">Was bieten die neuen Repositories für AI- und MO-Systeme?</a></li>
<li><a href="../de451170/index.html">Jeff Bezos kündigte Pläne an, den Mond zu erobern</a></li>
<li><a href="../de451172/index.html">Julia: Funktionen und Strukturen als Funktionen</a></li>
<li><a href="../de451174/index.html">Anpassung von Programmen für ZX Spectrum an TR-DOS mit modernen Mitteln. Teil 1</a></li>
<li><a href="../de451176/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 458 (23.04.2019 - 09.04.2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>