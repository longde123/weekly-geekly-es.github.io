<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è∏Ô∏è üëâ üèÑ Generieren von Bildern aus Text mit AttnGAN üë©üèª‚Äç‚úàÔ∏è üíß üßñüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich pr√§sentiere Ihnen die √úbersetzung des Artikels " AttnGAN: Feink√∂rniger Text zur Bilderzeugung mit aufmerksamen generativen kontradikto...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Generieren von Bildern aus Text mit AttnGAN</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/424957/">  Hallo Habr!  Ich pr√§sentiere Ihnen die √úbersetzung des Artikels " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AttnGAN: Feink√∂rniger Text zur Bilderzeugung mit aufmerksamen generativen kontradiktorischen Netzwerken</a> " von Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang und Xiaodong He. <br><br>  In dieser Ver√∂ffentlichung m√∂chte ich √ºber meine Experimente mit der AttnGAN-Architektur sprechen, um Bilder aus einer Textbeschreibung zu generieren.  Diese Architektur wurde bereits nach der Ver√∂ffentlichung des Originalartikels Anfang 2018 auf Habr√© erw√§hnt, und ich war an der Frage interessiert, wie schwierig es sein wird, ein solches Modell selbst zu trainieren. <br><br><h3>  Architekturbeschreibung </h3><br>  F√ºr diejenigen, die mit AttnGAN und dem klassischen GAN nicht vertraut sind, werde ich kurz die Essenz beschreiben.  Das klassische GAN besteht aus mindestens zwei neuronalen Netzen - einem Generator und einem Diskriminator.  Die Aufgabe des Generators besteht darin, einige Daten (Bilder, Text, Audio, Video usw.) zu generieren, die den realen Daten aus dem Datensatz ‚Äû√§hnlich‚Äú sind.  Die Aufgabe des Diskriminators besteht darin, die erzeugten Daten auszuwerten, sie mit realen zu vergleichen und sie abzulehnen.  Das abgelehnte Ergebnis der Arbeit des Generators regt ihn an, das beste Ergebnis zu erzielen, um den Diskriminator zu ‚Äût√§uschen‚Äú, der wiederum lernt, F√§lschungen besser zu erkennen. <br><br>  Es gibt sehr viele Modifikationen von GAN, und die Autoren von AttnGAN gingen die Frage der Architektur ziemlich erfinderisch an.  Das Modell besteht aus 9 neuronalen Netzen, die f√ºr die Interaktion fein abgestimmt sind.  Es sieht ungef√§hr so ‚Äã‚Äãaus: <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/tk/eq/nq/tkeqnqzoqcw9dt9giju0rdsh4zg.png"><br><br>  Text- und Bildcodierer (Text- / Bildcodierer) konvertieren die urspr√ºngliche Textbeschreibung und reale Bilder in eine Art interne Darstellung.  In diesem Fall wird der Text charakteristischerweise als eine Folge einzelner W√∂rter betrachtet, deren Darstellung zusammen mit der Darstellung des Bildes verarbeitet wird, wodurch Sie einzelne W√∂rter mit einzelnen Teilen des Bildes vergleichen k√∂nnen.  Auf diese Weise wird der von den Autoren des DAMSM-Artikels aufgerufene Aufmerksamkeitsmechanismus implementiert. <br><br>  Fca - Erstellt eine pr√§zise Darstellung der Gesamtszene im Bild, basierend auf der gesamten Textbeschreibung.  Der Wert von C am Ausgang wird mit einem Vektor aus der Normalverteilung von Z verkettet, der die Variabilit√§t der Szene bestimmt.  Diese Informationen bilden die Grundlage f√ºr den Betrieb des Generators. <br><br>  Der Generator ist das gr√∂√üte Netzwerk, das aus drei Ebenen besteht.  Jede Ebene erzeugt Bilder mit zunehmender Aufl√∂sung von 64 * 64 bis 256 * 256 Pixel, und das Ergebnis der Arbeit auf jeder Ebene wird mithilfe von Fattn-Aufmerksamkeitsnetzwerken angepasst, die Informationen √ºber die korrekte Position einzelner Objekte in der Szene enthalten.  Zus√§tzlich werden die Ergebnisse auf jeder Ebene von drei separat arbeitenden Diskriminatoren √ºberpr√ºft, die den Realismus des Bildes und seine √úbereinstimmung mit der allgemeinen Idee der Szene bewerten. <br><br><h3>  Schulung </h3><br>  Um die Architektur zu testen, habe ich den Standard-CUB-Datensatz mit Fotos und Textbeschreibungen von V√∂geln verwendet. <br><br>  Das gesamte Modell wird in zwei Schritten trainiert.  Die erste Phase ist das Pre-Training von DAMSM-Netzwerken, das aus einem Text- und Bildcodierer besteht.  In dieser Phase wird, wie oben beschrieben, eine ‚ÄûAufmerksamkeitskarte‚Äú erstellt, die folgenderma√üen aussieht: <br><br><img src="https://habrastorage.org/webt/of/w7/6f/ofw76fxdvl6bbuohon_okmsgfro.png"><br><br>  Wie aus der Abbildung ersichtlich, gelingt es DAMSM, die Beziehung zwischen einzelnen W√∂rtern aus der Textbeschreibung und den Bildelementen sehr genau zu erfassen. F√ºr das Modell ist es besonders einfach, Farben zu erkennen.  Ich muss sagen, dass das System keine zus√§tzlichen Informationen dar√ºber hat, was "rot", "gelb" oder "Fl√ºgel", "Schnabel" ist.  Es gibt nur eine Reihe von Texten und Bildern. <br><br>  Das DAMSM-Training findet problemlos statt. Die Trainingszeit f√ºr diesen Datensatz betr√§gt 150-200 Epochen, was mehreren Stunden auf einer Hochleistungs-GPU entspricht. <br><br>  Die zweite und Hauptstufe ist das Training des Generators mit dem DAMSM-Modell. <br>  Der Generator auf jeder Ebene erzeugt ein Bild mit h√∂herer Aufl√∂sung - es sieht so aus: <br><br><img src="https://habrastorage.org/webt/xz/go/cw/xzgocw0eswwfeku7kxogp2jhuqy.png"><br><br>  Das Generatortraining dauert viel l√§nger und ist nicht immer so stabil. Die empfohlene Trainingszeit f√ºr diesen Datensatz betr√§gt 300-600 Epochen, was ungef√§hr 4-8 Tagen auf einer Hochleistungs-GPU entspricht. <br><br>  Das Hauptproblem beim Training des Generators ist meiner Meinung nach das Fehlen ausreichend guter Metriken, die es uns erm√∂glichen w√ºrden, die Qualit√§t des Trainings formeller zu bewerten.  Ich habe mehrere Implementierungen des Inception-Scores studiert, der theoretisch als universelle Metrik f√ºr solche Aufgaben positioniert ist - aber sie schienen mir nicht √ºberzeugend genug zu sein.  Wenn Sie sich entscheiden, einen solchen Generator zu trainieren, m√ºssen Sie den Trainingsfortschritt anhand der Zwischenergebnisse st√§ndig visuell √ºberwachen.  Diese Regel gilt jedoch f√ºr solche Aufgaben. Eine visuelle Kontrolle ist immer erforderlich. <br><br><h3>  Ergebnisse </h3><br>  Nun der lustige Teil.  Mit dem trainierten Modell werden wir versuchen, Bilder zu erzeugen, wir werden mit einfachen S√§tzen beginnen: <br><br><img src="https://habrastorage.org/webt/7g/ri/x-/7grix-945iwxoysnibzph4yjd0w.png"><br><br>  Versuchen wir komplexere Beschreibungen: <br><br><img src="https://habrastorage.org/webt/8n/kp/eu/8nkpeuqwf4wiqk_c6fn8bynmxiq.png"><br><br>  Alle Textbeschreibungen sind erfunden, ich habe absichtlich keine Phrasen aus dem Datensatz f√ºr Tests verwendet.  Nat√ºrlich wurden nicht alle diese Bilder beim ersten Versuch erhalten.  Das Modell ist falsch, die Autoren selbst sprechen dar√ºber.  Mit zunehmendem Beschreibungstext und den anzuzeigenden Elementen wird es immer schwieriger, den Realismus der gesamten Szene aufrechtzuerhalten.  Wenn Sie jedoch in der Produktion etwas √Ñhnliches verwenden m√∂chten, z. B. Bilder bestimmter Objekte f√ºr einen Designer erstellen m√∂chten, k√∂nnen Sie das System trainieren und an Ihre Anforderungen anpassen, was sehr streng sein kann. <br><br>  F√ºr jede Textbeschreibung k√∂nnen Sie viele Bildoptionen (einschlie√ülich unrealistischer) generieren, sodass immer eine gro√üe Auswahl zur Verf√ºgung steht. <br><br><h3>  Technische Details </h3><br>  In dieser Arbeit verwendete ich eine GPU mit geringem Stromverbrauch f√ºr das Prototyping und einen Google Cloud-Server, auf dem Tesla K80 w√§hrend der Schulungsphase installiert war. <br><br>  Der Quellcode wurde aus dem Repository der Autoren des Artikels entnommen und einer gr√ºndlichen √úberarbeitung unterzogen.  Das System wurde in Python 3.6 mit Pytorch 0.4.1 getestet <br><br>  Vielen Dank f√ºr Ihre Aufmerksamkeit! <br><br>  <i>Originalartikel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AttnGAN</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feink√∂rniger Text zur Bilderzeugung mit aufmerksamen generativen gegnerischen Netzwerken</a> , 2018 - Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de424957/">https://habr.com/ru/post/de424957/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de424945/index.html">Smartphone f√§hrt ein Spielzeugauto.</a></li>
<li><a href="../de424947/index.html">Gestenerkennung mit APDS-9960</a></li>
<li><a href="../de424949/index.html">PHP Digest Nr. 140 (17. - 30. September 2018)</a></li>
<li><a href="../de424951/index.html">Hurra! Es war keine Paranoia</a></li>
<li><a href="../de424955/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 332 (24. - 30. September 2018)</a></li>
<li><a href="../de424961/index.html">MTA-STS f√ºr Postfix</a></li>
<li><a href="../de424963/index.html">Zuckerberg-Finanzierung: Gemeinsam Werkzeuge f√ºr die Wissenschaft bauen</a></li>
<li><a href="../de424965/index.html">Reagieren Sie auf die Anwendungsentwicklung mit ReasonReact</a></li>
<li><a href="../de424967/index.html">JavaScript-Verkn√ºpfungen f√ºr Anf√§nger</a></li>
<li><a href="../de424969/index.html">Node.js-Handbuch, Teil 9: Arbeiten mit dem Dateisystem</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>