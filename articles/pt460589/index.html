<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíî ‚ö†Ô∏è üê¢ Escrevendo uma rede neural simples usando matem√°tica e numpy üíÉüèæ üêô üèåÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Por que o pr√≥ximo artigo sobre como escrever redes neurais do zero? Infelizmente, n√£o consegui encontrar artigos em que a teoria e o c√≥digo fossem des...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Escrevendo uma rede neural simples usando matem√°tica e numpy</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460589/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/5_/h5/0t/5_h50teeaqngyx0ccp8cqjqfumm.jpeg" width="350"></div><br>  Por que o pr√≥ximo artigo sobre como escrever redes neurais do zero?  Infelizmente, n√£o consegui encontrar artigos em que a teoria e o c√≥digo fossem descritos do zero para um modelo totalmente funcional.  Eu imediatamente aviso que haver√° muita matem√°tica.  Suponho que o leitor esteja familiarizado com os conceitos b√°sicos de √°lgebra linear, derivadas parciais e, pelo menos parcialmente, com a teoria das probabilidades, al√©m de Python e Numpy.  Lidaremos com uma rede neural totalmente conectada e com o MNIST. <br><a name="habracut"></a><br><h2>  Matem√°tica  Parte 1 (simples) </h2><br>  O que √© uma camada totalmente conectada (camada FC)?  Geralmente eles dizem algo como "Uma camada totalmente conectada √© uma camada, cada neur√¥nio conectado a todos os neur√¥nios da camada anterior".  N√£o est√° claro o que s√£o os neur√¥nios, como eles est√£o conectados, especialmente no c√≥digo.  Agora vou tentar analisar isso com um exemplo.  Que haja uma camada de 100 neur√¥nios.  Eu sei que ainda n√£o expliquei o que √©, mas vamos imaginar que existem 100 neur√¥nios e eles t√™m uma entrada para onde os dados s√£o enviados e uma sa√≠da de onde eles fornecem os dados.  E uma imagem em preto e branco de 28x28 pixels √© alimentada na entrada - apenas 784 valores, se voc√™ a esticar em um vetor.  Uma imagem pode ser chamada de camada de entrada.  Ent√£o, para que cada um dos 100 neur√¥nios se conecte a cada "neur√¥nio" ou, se quiser, o valor da camada anterior (ou seja, a figura), √© necess√°rio que cada um dos 100 neur√¥nios aceite 784 valores da figura original.  Por exemplo, para cada um dos 100 neur√¥nios, basta multiplicar 784 valores da imagem por cerca de 784 n√∫meros e adicion√°-los, como resultado, um n√∫mero sai.  Ou seja, este √© um neur√¥nio: <br><p><math> </math> $$ display $$ \ text {Sa√≠da do neur√¥nio} = \ text {algum n√∫mero} _ {1} \ cdot \ text {valor da imagem} _1 ~ + \\ + ~ ... ~ + ~ \ text {some- esse n√∫mero} _ {784} \ cdot \ text {valor da imagem} _ {784} $$ display $$ </p><br>  Acontece que cada neur√¥nio tem 784 n√∫meros e todos esses n√∫meros: (n√∫mero de neur√¥nios nessa camada) x (n√∫mero de neur√¥nios na camada anterior) = <math> </math> $ inline $ 100 \ times784 $ inline $   = 78.400 d√≠gitos.  Esses n√∫meros s√£o comumente chamados de pesos da camada.  Cada neur√¥nio fornecer√° seu n√∫mero e, como resultado, obteremos um vetor 100-dimensional e, de fato, podemos escrever que esse vetor 100-dimensional √© obtido multiplicando o vetor 784-dimensional (nossa imagem original) por uma matriz de peso do tamanho <math> </math> $ inline $ 100 \ times784 $ inline $   : <br><p><math> </math> exibi√ß√£o $$ $$ \ boldsymbol {x} ^ {100} = W_ {100 \ times784} \ cdot \ boldsymbol {x} ^ {784} $$ exibi√ß√£o $$ </p><br><br>  Al√©m disso, os 100 n√∫meros resultantes s√£o repassados ‚Äã‚Äãpara a fun√ß√£o de ativa√ß√£o - alguma fun√ß√£o n√£o linear - que afeta cada n√∫mero separadamente.  Por exemplo, sigm√≥ide, tangente hiperb√≥lica, ReLU e outros.  A fun√ß√£o de ativa√ß√£o √© necessariamente n√£o linear, caso contr√°rio, a rede neural aprender√° apenas transforma√ß√µes simples. <br><br><img src="https://habrastorage.org/webt/0j/rl/ba/0jrlbaqv0486mryhqj32u8et0cw.png"><br><br>  Em seguida, os dados resultantes s√£o novamente alimentados a uma camada totalmente conectada, mas com um n√∫mero diferente de neur√¥nios e novamente √† fun√ß√£o de ativa√ß√£o.  Isso acontece v√°rias vezes.  A √∫ltima camada da rede √© a camada que produz a resposta.  Nesse caso, a resposta s√£o informa√ß√µes sobre o n√∫mero na imagem. <br><br><img src="https://habrastorage.org/webt/9f/73/q-/9f73q-feve3kb5k9u5fxvmz4kxk.png"><br><br>  Durante o treinamento da rede, √© necess√°rio que saibamos qual figura √© mostrada na figura.  Ou seja, que o conjunto de dados est√° marcado.  Ent√£o voc√™ pode usar outro elemento - a fun√ß√£o de erro.  Ela observa a resposta da rede neural e a compara com a resposta real.  Gra√ßas a isso, a rede neural est√° aprendendo. <br><br><h2>  Declara√ß√£o geral do problema </h2><br>  Todo o conjunto de dados √© um tensor grande (chamaremos um conjunto de dados multidimensionais de tensor) <math> </math> $ inline $ \ boldsymbol {X} = \ left [\ boldsymbol {x} _1, \ boldsymbol {x} _2, \ ldots, \ boldsymbol {x} _n \ right] $ inline $   onde <math> </math> $ inline $ \ boldsymbol {x} _i $ inline $   - i-√©simo objeto, por exemplo, uma imagem, que tamb√©m √© um tensor.  Para cada objeto existe <math> </math> $ inline $ y_i $ inline $   - a resposta correta no i-√©simo objeto.  Nesse caso, uma rede neural pode ser representada como uma fun√ß√£o que recebe um objeto como entrada e fornece algumas respostas: <br><p><math> </math> $$ display $$ F (\ boldsymbol {x} _i) = \ hat {y} _i $$ display $$ </p><br>  Agora vamos dar uma olhada mais de perto na fun√ß√£o <math> </math> $ inline $ F (\ boldsymbol {x} _i) $ inline $   .  Como a rede neural consiste em camadas, cada camada individual √© uma fun√ß√£o.  E isso significa <br><p><math> </math> $$ display $$ F (\ boldsymbol {x} _i) = f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i)))) = \ hat {y} _i $$ display $ $ </p><br>  Ou seja, na primeira fun√ß√£o - a primeira camada - uma imagem √© apresentada na forma de algum tensor.  Fun√ß√£o <math> </math> $ inline $ f_1 $ inline $   d√° alguma resposta - tamb√©m um tensor, mas de uma dimens√£o diferente.  Este tensor ser√° chamado de representa√ß√£o interna.  Agora essa representa√ß√£o interna √© alimentada na entrada da fun√ß√£o <math> </math> $ inline $ f_2 $ inline $   , que fornece sua representa√ß√£o interna.  E assim por diante, at√© a fun√ß√£o <math> </math> $ inline $ f_k $ inline $   - √∫ltima camada - n√£o dar√° uma resposta <math> </math> $ inline $ \ hat {y} _i $ inline $   . <br><br>  Agora, a tarefa √© treinar a rede - fazer a resposta da rede corresponder √† resposta correta.  Primeiro, voc√™ precisa medir o qu√£o errada √© a rede neural.  Medir isso √© uma fun√ß√£o de erro. <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   .  E impomos restri√ß√µes: <br><br>  1 <math> </math> $ inline $ \ hat {y} _i \ xrightarrow {} y_i \ Rightarrow L (\ hat {y} _i, y_i) \ xrightarrow {} 0 $ inline $ <br>  2) <math> </math> $ inline $ \ existe ~ dL (\ hat {y} _i, y_i) $ inline $ <br>  3) <math> </math> $ inline $ L (\ hat {y} _i, y_i) \ geq 0 $ inline $ <br><br>  A restri√ß√£o 2 √© imposta a todas as fun√ß√µes das camadas <math> </math> $ inline $ f_j $ inline $   - sejam todos diferenci√°veis. <br><br>  Al√©m disso, de fato (eu n√£o mencionei isso), algumas dessas fun√ß√µes dependem dos par√¢metros - os pesos da rede neural - <math> </math> $ inline $ f_j (\ boldsymbol {x} _i | \ boldsymbol {\ omega} _j) $ inline $   .  E a id√©ia toda √© pegar esses pesos para que <math> </math> $ inline $ \ hat {y} _i $ inline $   coincidiu com <math> </math> $ inline $ y_i $ inline $   em todos os objetos de um conjunto de dados.  Noto que nem todas as fun√ß√µes t√™m pesos. <br><br>  Ent√£o, onde paramos?  Todas as fun√ß√µes da rede neural s√£o diferenci√°veis, a fun√ß√£o de erro tamb√©m √© diferenci√°vel.  Lembre-se de uma das propriedades do gradiente - mostre a dire√ß√£o do crescimento da fun√ß√£o.  Usamos isso, restri√ß√µes 1 e 3, o fato de que <br><p><math> </math> $$ display $$ L (F (\ boldsymbol {x} _i)) = L (f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i))))) = L (\ hat {y} _i) $$ exibir $$ </p><br>  e o fato de eu poder considerar derivadas parciais e derivadas de uma fun√ß√£o complexa.  Agora h√° tudo o que voc√™ precisa para calcular <br><p><math> </math> $$ display $$ \ frac {\ L parcial (F (\ boldsymbol {x} _i))} {\ parcial \ boldsymbol {\ omega_j}} $$ display $$ </p><br>  para qualquer i e j.  Essa derivada parcial mostra a dire√ß√£o na qual mudar <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   aumentar <math> </math> $ inline $ L $ inline $   .  Para reduzir, voc√™ precisa dar um passo para o lado <math> </math> $ inline $ - \ frac {\ L parcial (F (\ boldsymbol {x} _i))} {\ parcial \ boldsymbol {\ omega_j}} $ inline $   nada complicado. <br><br>  Isso significa que o processo de aprendizado de rede est√° estruturado da seguinte maneira: v√°rias vezes em um ciclo, percorremos todo o conjunto de dados (isso √© chamado de era), para cada objeto do conjunto de dados que consideramos <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   (isso √© chamado de encaminhamento) e considere a derivada parcial <math> </math> $ inline $ \ parcial L $ inline $   para todos os pesos <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   , atualize os pesos (isso √© chamado de passagem para tr√°s). <br><br>  Noto que ainda n√£o introduzi fun√ß√µes e camadas espec√≠ficas.  Se, nesta fase, n√£o est√° claro o que fazer com tudo isso, proponho continuar lendo - haver√° mais matem√°tica, mas agora ela vai com exemplos. <br><br><h2>  Matem√°tica  Parte 2 (dif√≠cil) </h2><br><h3>  Fun√ß√£o de erro </h3><br>  Iniciarei do final e derivarei a fun√ß√£o de erro para o problema de classifica√ß√£o.  Para o problema de regress√£o, a deriva√ß√£o da fun√ß√£o de erro est√° bem descrita no livro ‚ÄúDeep Learning.  Imers√£o no mundo das redes neurais ". <br><br>  Por uma quest√£o de simplicidade, existe uma rede neural (NN) que separa as fotos de gatos das fotos de c√£es e h√° um conjunto de fotos de gatos e c√£es para os quais existe uma resposta correta <math> </math> $ inline $ y_ {true} $ inline $   . <br><p><math> </math> $$ display $$ NN (imagem | \ Omega) = y_ {pred} $$ display $$ </p><br>  Tudo o que farei a seguir √© muito semelhante ao m√©todo da m√°xima verossimilhan√ßa.  Portanto, a principal tarefa √© encontrar a fun√ß√£o de probabilidade.  Se omitirmos os detalhes, uma fun√ß√£o que compara a previs√£o da rede neural e a resposta correta e, se eles coincidem, gera um grande valor, se n√£o, vice-versa.  A probabilidade de uma resposta correta vem √† mente com os par√¢metros fornecidos: <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) $$ display $$ </p><br>  E agora vamos fazer uma finta, que, ao que parece, n√£o segue de nenhum lugar.  Deixe a rede neural dar uma resposta na forma de um vetor bidimensional, cuja soma dos valores √© 1. O primeiro elemento desse vetor pode ser chamado de medida de confian√ßa de que o gato est√° na foto e o segundo elemento, a medida de confian√ßa de que o cachorro est√° na foto.  Sim, √© quase probabilidade! <br><p><math> </math> $$ display $$ NN (imagem | \ Omega) = \ left [\ begin {matrix} p_0 \\ p_1 \\\ final {matrix} \ right] $$ display $$ </p><br>  Agora a fun√ß√£o de probabilidade pode ser reescrita como: <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) = p_ \ Omega (y_ {pred}) ^ t_ {0} * (1 - p_ \ Omega (y_ {pred})) ^ t_ {1} = \\ p_0 ^ {t_0} * p_1 ^ {t_1} $$ display $$ </p><br>  Onde <math> </math> $ inline $ t_0, t_1 $ inline $   r√≥tulos da classe correta, por exemplo, se <math> </math> $ inline $ y_ {true} = cat $ inline $   ent√£o <math> </math> $ inline $ t_0 == 1, t_1 == 0 $ inline $   se <math> </math> $ inline $ y_ {true} = c√£o $ inline $   ent√£o <math> </math> $ inline $ t_0 == 0, t_1 == 1 $ inline $   .  Assim, a probabilidade de uma classe que deveria ter sido prevista por uma rede neural (mas n√£o necessariamente prevista por ela) √© sempre considerada.  Agora isso pode ser generalizado para qualquer n√∫mero de classes (por exemplo, m classes): <br><p><math> </math> $$ display $$ p (y_ {pred} = y_ {true} | \ Omega) = \ prod_0 ^ m p_i ^ {t_i} $$ display $$ </p><br>  No entanto, em qualquer conjunto de dados, existem muitos objetos (por exemplo, N objetos).  Quero que a rede neural d√™ a resposta correta em cada um ou na maioria dos objetos.  E para isso, voc√™ precisa multiplicar os resultados da f√≥rmula acima para cada objeto do conjunto de dados. <br><p><math> </math> $$ display $$ MaximumLikelyhood = \ prod_ {j = 0} ^ N \ prod_ {i = 0} ^ m p_ {i, j} ^ {t_ {i, j}} $$ display $$ </p><br>  Para obter bons resultados, essa fun√ß√£o precisa ser maximizada.  Mas, primeiro, √© mais √≠ngreme minimizar, porque temos uma descida gradiente estoc√°stica e todos os p√£es para ele - basta atribuir um sinal de menos e, segundo, √© dif√≠cil trabalhar com um trabalho enorme - √© o logaritmo. <br><p><math> </math> $$ display $$ CrossEntropyLoss = - \ soma \ limites_ {j = 0} ^ {N} \ soma \ limites_ {i = 0} ^ {m} t_ {i, j} \ cdot \ log (p_ {i, j }) $$ display $$ </p><br>  √ìtimo!  O resultado foi entropia cruzada ou, no caso bin√°rio, perda de log.  Essa fun√ß√£o √© f√°cil de contar e ainda mais f√°cil de diferenciar: <br><p><math> </math> $$ display $$ \ frac {\ parcial CrossEntropyLoss} {\ parcial p_j} = - \ frac {\ boldsymbol {t_j}} {\ boldsymbol {p_ {j}}} $$ display $$ </p><br>  Voc√™ precisa se diferenciar no algoritmo de retropropaga√ß√£o.  Noto que a fun√ß√£o de erro n√£o altera a dimens√£o do vetor.  Se, como no caso do MNIST, a sa√≠da for um vetor 10-dimensional de respostas, ao calcular a derivada, obteremos um vetor 10-dimensional de derivadas.  Outra coisa interessante √© que apenas um elemento da derivada n√£o ser√° zero, no qual <math> </math> $ inline $ t_ {i, j} \ neq 0 $ inline $   , ou seja, com a resposta correta.  E quanto menor a probabilidade de uma resposta correta prevista por uma rede neural em um determinado objeto, mais a fun√ß√£o de erro estar√° nela. <br><br><h3>  Recursos de ativa√ß√£o </h3><br>  Na sa√≠da de cada camada totalmente conectada de uma rede neural, uma fun√ß√£o de ativa√ß√£o n√£o linear deve estar presente.  Sem ele, √© imposs√≠vel treinar uma rede neural significativa.  No futuro, uma camada totalmente conectada de uma rede neural √© simplesmente uma multiplica√ß√£o dos dados de entrada por uma matriz de peso.  Na √°lgebra linear, isso √© chamado de mapa linear - uma fun√ß√£o linear.  A combina√ß√£o de fun√ß√µes lineares tamb√©m √© uma fun√ß√£o linear.  Mas isso significa que essa fun√ß√£o pode apenas aproximar fun√ß√µes lineares.  Infelizmente, n√£o √© por isso que s√£o necess√°rias redes neurais. <br><br><h4>  Softmax </h4><br>  Normalmente, essa fun√ß√£o √© usada na √∫ltima camada da rede, pois transforma o vetor da √∫ltima camada em um vetor de ‚Äúprobabilidades‚Äù: cada elemento do vetor fica de 0 a 1 e sua soma √© 1. Ele n√£o altera a dimens√£o do vetor. <br><p><math> </math> $$ display $$ Softmax_i = \ frac {e ^ {x_i}} {\ sum \ limits_ {j} e ^ {x_j}} $$ display $$ </p><br>  Agora vamos para a pesquisa derivada.  Desde <math> </math> $ inline $ \ boldsymbol {x} $ inline $   √â um vetor, e todos os seus elementos est√£o sempre presentes no denominador; ent√£o, ao tomar a derivada, obtemos o Jacobiano: <br><p><math> </math> $$ display $$ J_ {Softmax} = \ begin {cases} x_i - x_i \ cdot x_j, i = j \\ - x_i \ cdot x_j, i \ neq j \ end {cases} $$ display $$ </p><br>  Agora sobre retropropaga√ß√£o.  O vetor de derivadas vem da camada anterior (geralmente essa √© uma fun√ß√£o de erro) <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   .  Caso <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   veio de uma fun√ß√£o de erro no mnist, <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   - vetor 10-dimensional.  Ent√£o o jacobiano tem uma dimens√£o de 10x10.  Para obter <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   , que vai al√©m da camada anterior (n√£o esque√ßa que passamos do final ao in√≠cio da rede quando o erro se propaga de volta), precisamos multiplicar <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   em <math> </math> $ inline $ J_ {Softmax} $ inline $   (linha por coluna): <br><p><math> </math> $$ display $$ dz_ {new} = \ boldsymbol {dz} \ times J_ {Softmax} $$ display $$ </p><br>  Na sa√≠da, obtemos um vetor 10-dimensional de derivadas <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   . <br><br><h4>  Relu </h4><br><p><math> </math> $$ display $$ ReLU (x) = \ begin {cases} x, x&gt; 0 \\ 0, x &lt;0 \ end {cases} $$ display $$ </p><br>  O ReLU come√ßou a ser utilizado em massa ap√≥s 2011, quando foi publicado o artigo "Redes Neurais do Retificador Escasso Profundo".  No entanto, essa fun√ß√£o era conhecida anteriormente.  O conceito de "poder de ativa√ß√£o" √© aplic√°vel ao ReLU (para obter mais detalhes, consulte o livro "Deep Learning. Imers√£o no mundo das redes neurais").  Mas o principal recurso que torna a ReLU mais atraente do que outras fun√ß√µes de ativa√ß√£o √© seu simples c√°lculo derivativo: <br><p><math> </math> $$ display $$ d (ReLU (x)) = \ begin {cases} 1, x&gt; 0 \\ 0, x &lt;0 \ end {cases} $$ display $$ </p><br>  Assim, ReLU √© computacionalmente mais eficiente do que outras fun√ß√µes de ativa√ß√£o (sigm√≥ide, tangente hiperb√≥lica, etc.). <br><br><h3>  Camada totalmente conectada </h3><br>  Agora √© a hora de discutir uma camada totalmente conectada.  O mais importante de todos os outros, porque √© nessa camada que est√£o localizados todos os pesos, que devem ser ajustados para que a rede neural funcione bem.  Uma camada totalmente conectada √© simplesmente uma matriz de peso: <br><p><math> </math> $$ exibi√ß√£o $$ W = | w_ {i, j} | $$ exibi√ß√£o $$ </p><br>  Uma nova representa√ß√£o interna √© obtida quando a matriz de pesos √© multiplicada pela coluna de entrada: <br><p><math> </math> $$ display $$ \ boldsymbol {x} _ {new} = W \ cdot \ boldsymbol {x} $$ display $$ </p><br>  Onde <math> </math> $ inline $ \ boldsymbol {x} $ inline $   tem tamanho <math> </math> $ inline $ input \ _shape $ inline $   e <math> </math> $ inline $ x_ {new} $ inline $   - <math> </math> $ inline $ output \ _shape $ inline $   .  Por exemplo <math> </math> $ inline $ \ boldsymbol {x} $ inline $   - vetor 784 dimensional, e <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   √â um vetor 100-dimensional, ent√£o a matriz W tem um tamanho de 100x784.  Acontece que nessa camada h√° 100x784 = 78.400 pesos. <br><br>  Com a propaga√ß√£o reversa do erro, √© preciso levar a derivada em rela√ß√£o a cada peso dessa matriz.  Simplifique o problema e use apenas a derivada com rela√ß√£o a <math> </math> $ inline $ w_ {1,1} $ inline $   .  Ao multiplicar a matriz e o vetor, o primeiro elemento do novo vetor <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   √© igual a <math> </math> $ inline $ x_ {new ~ 1} = w_ {1,1} \ cdot x_1 + ... + w_ {1.784} \ cdot x_ {784} $ inline $   e o derivado <math> </math> $ inline $ x_ {new ~ 1} $ inline $   por <math> </math> $ inline $ w_ {1,1} $ inline $   vai ser simples <math> </math> $ inline $ x_1 $ inline $   , voc√™ s√≥ precisa obter a derivada do valor acima.  Da mesma forma acontece para todos os outros pesos.  Mas este n√£o √© um algoritmo de propaga√ß√£o de erro, desde que seja apenas uma matriz de derivadas.  Voc√™ precisa se lembrar que da pr√≥xima camada a esta (o erro vai do fim ao come√ßo) vem um vetor de gradiente 100-dimensional <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   .  Primeiro elemento deste vetor <math> </math> $ inline $ dz_1 $ inline $   ser√° multiplicado por todos os elementos da matriz de derivativos que "participaram" da cria√ß√£o <math> </math> $ inline $ x_ {new ~ 1} $ inline $   , ou seja, em <math> </math> $ inline $ x_1, x_2, ..., x_ {784} $ inline $   .  Da mesma forma, o resto dos elementos.  Se voc√™ traduzir isso para o idioma da √°lgebra linear, ser√° escrito assim: <br><p><math> </math> exibi√ß√£o $$ $$ \ frac {\ L parcial} {\ parcial W} = (d \ boldsymbol {z}, ~ dW) = \ left (\ begin {matrix} dz_ {1} \ cdot \ boldsymbol {x} \ \ ... \\ dz_ {100} \ cdot \ boldsymbol {x} \ end {matrix} \ right) _ {100} $$ display $$ </p><br>  A sa√≠da √© uma matriz 100x784. <br><img src="https://habrastorage.org/webt/1m/8_/hl/1m8_hljpr28gm3dikkgpsk4zss8.png"><br><br>  Agora voc√™ precisa entender o que transferir para a camada anterior.  Para isso e para uma melhor compreens√£o do que aconteceu agora, quero anotar o que aconteceu ao obter derivativos nessa camada em uma linguagem um pouco diferente, para me afastar das especificidades do ‚Äúo que √© multiplicado‚Äù pelas fun√ß√µes (novamente). <br><br>  Quando eu queria ajustar os pesos, queria usar a derivada da fun√ß√£o de erro para esses pesos: <math> </math> $ inline $ \ frac {\ L parcial \ {W parcial W} $ inline $   .  Foi mostrado acima como obter derivadas de fun√ß√µes de erro e fun√ß√µes de ativa√ß√£o.  Portanto, podemos considerar esse caso (em <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   todas as derivadas da fun√ß√£o de erro e das fun√ß√µes de ativa√ß√£o j√° est√£o paradas): <br><p><math> </math> $$ display $$ \ frac {\ L parcial} {\ W parcial} = d \ boldsymbol {z} \ cdot \ frac {\ parcial \ boldsymbol {x} _ {novo} (W)} {\ W parcial} $ $ display $$ </p><br>  Isso pode ser feito, porque voc√™ pode considerar <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   em fun√ß√£o de W: <math> </math> $ inline $ \ boldsymbol {x} _ {new} = W \ cdot \ boldsymbol {x} $ inline $   . <br>  Voc√™ pode substituir isso na f√≥rmula acima: <br><br><p><math> </math> exibi√ß√£o $$ $$ \ frac {\ L parcial \ {W parcial} = d \ boldsymbol {z} \ cdot \ frac {\ parcial W \ cdot \ boldsymbol {x}} {\ parcial W} = d \ boldsymbol { z} \ cdot E \ cdot \ boldsymbol {x} $$ display $$ </p><br>  Onde E √© uma matriz que consiste em unidades (N√ÉO √© uma matriz de unidades). <br><br>  Agora, quando voc√™ precisar obter a derivada da camada anterior (mesmo que por simplicidade de c√°lculos, ela tamb√©m seja uma camada totalmente conectada, mas, no caso geral, n√£o altera nada), √© necess√°rio considerar <math> </math> $ inline $ \ boldsymbol {x} $ inline $   em fun√ß√£o da camada anterior <math> </math> $ inline $ \ boldsymbol {x} (W_ {old}) $ inline $   : <br><p><math> </math> exibi√ß√£o $$ $$ \ begin {reunido} \ frac {\ L parcial \ {L} parcial} = d \ s√≠mbolo de negrito {z} \ cdot \ frac {\ s√≠mbolo de negrito parcial {x} _ {novo} (W )} {\ W_ parcial {antigo}} = d \ boldsymbol {z} \ cdot \ frac {\ W \ cdot parcial \ boldsymbol {x} (W_ {old})} {\ W_ parcial {old}} = \\ = d \ boldsymbol {z} \ cdot \ frac {\ W parcial \ cdot W_ {antigo} \ cdot \ boldsymbol {x} _ {antigo}} {\ W parcial {antigo}} = d \ boldsymbol {z} \ cdot W \ cdot E \ cdot \ boldsymbol {x} _ {antigo} = \\ = d \ boldsymbol {z} _ {novo} \ cdot E \ cdot \ boldsymbol {x} _ {antigo} \ end {reunido} $$ exibir $$ </p><br>  Exatamente <math> </math> $ inline $ d \ boldsymbol {z} _ {new} = d \ boldsymbol {z} \ cdot W $ inline $   e voc√™ precisa enviar para a camada anterior. <br><br><h2>  C√≥digo </h2><br><blockquote>  Este artigo tem como objetivo principal explicar a matem√°tica das redes neurais.  Dedicarei muito pouco tempo ao c√≥digo. </blockquote><br>  Este √© um exemplo de implementa√ß√£o da fun√ß√£o de erro: <br><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropy</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, y_true, y_hat)</span></span></span><span class="hljs-function">:</span></span> self.y_hat = y_hat self.y_true = y_true self.loss = -np.sum(self.y_true * np.log(y_hat)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.loss <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> dz = -self.y_true / self.y_hat <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dz</code> </pre> <br>  A classe possui m√©todos para passagem direta e reversa.  No momento do passe direto, a inst√¢ncia da classe armazena os dados dentro da camada e, no momento do passe de retorno, os usa para calcular o gradiente.  As demais camadas s√£o constru√≠das da mesma maneira.  Gra√ßas a isso, torna-se poss√≠vel escrever um neural totalmente conectado neste estilo: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MnistNet</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.d1_layer = Dense(<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) self.a1_layer = ReLu() self.drop1_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>) self.d2_layer = Dense(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.a2_layer = ReLu() self.drop2_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) self.d3_layer = Dense(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) self.a3_layer = Softmax() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, train=True)</span></span></span><span class="hljs-function">:</span></span> ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dz, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.01</span></span></span></span><span class="hljs-function"><span class="hljs-params">, mini_batch=True, update=False, len_mini_batch=None)</span></span></span><span class="hljs-function">:</span></span> ...</code> </pre><br>  O c√≥digo completo pode ser encontrado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br>  Tamb√©m aconselho a estudar este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo sobre Habr√©</a> . <br><br><h2>  Conclus√£o </h2><br>  Espero ter sido capaz de explicar e mostrar que a matem√°tica bastante simples est√° por tr√°s das redes neurais e que isso n√£o √© nada assustador.  No entanto, para uma compreens√£o mais profunda, vale a pena tentar escrever sua pr√≥pria ‚Äúbicicleta‚Äù.  Corre√ß√µes e sugest√µes s√£o felizes em ler nos coment√°rios. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt460589/">https://habr.com/ru/post/pt460589/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt460567/index.html">React Native: fa√ßa uma lista arrast√°vel e desliz√°vel</a></li>
<li><a href="../pt460569/index.html">Software de grava√ß√£o com a funcionalidade dos utilit√°rios cliente-servidor Windows, parte 01</a></li>
<li><a href="../pt460573/index.html">O Google afirma que "reCAPTCHA" n√£o abuse dos dados do usu√°rio. Vale a pena acreditar?</a></li>
<li><a href="../pt460577/index.html">Viva o rei: mundo cruel da hierarquia em uma matilha de c√£es vadios</a></li>
<li><a href="../pt460587/index.html">M√≥dulo sem fio para sensor capacitivo de umidade do solo no nRF52832</a></li>
<li><a href="../pt460591/index.html">Obten√ß√£o de raiz em um roteador Tenda Nova MW6</a></li>
<li><a href="../pt460593/index.html">"Universal" na equipe de desenvolvimento: benef√≠cio ou dano?</a></li>
<li><a href="../pt460597/index.html">Como diagnosticar problemas de integra√ß√£o do SDK. A experi√™ncia da equipe de desenvolvimento do Yandex Mobile Ads SDK</a></li>
<li><a href="../pt460599/index.html">Not√≠cias do mundo do OpenStreetMap no 468 (02/07/2019 - 08.07.2019)</a></li>
<li><a href="../pt460603/index.html">V2G. Carros el√©tricos ajudar√£o a equilibrar a produ√ß√£o e o consumo de eletricidade</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>