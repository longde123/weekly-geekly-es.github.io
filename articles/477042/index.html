<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèª ‚úäüèº üë®üèæ‚Äçüè≠ Optimizaci√≥n de estrategia de blackjack de Monte Carlo üë©üèø‚ÄçüöÄ ‚ÜïÔ∏è ‚èØÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La traducci√≥n del art√≠culo fue preparada espec√≠ficamente para estudiantes del curso de Machine Learning . 



 El entrenamiento reforzado tom√≥ el mund...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimizaci√≥n de estrategia de blackjack de Monte Carlo</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>La traducci√≥n del art√≠culo fue preparada espec√≠ficamente para estudiantes del curso de <a href="https://otus.pw/Zkti/">Machine Learning</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  El entrenamiento reforzado tom√≥ el mundo de la Inteligencia Artificial.  A partir de AlphaGo y <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> , un n√∫mero cada vez mayor de actividades que antes eran dominadas por humanos ahora son conquistadas por agentes de IA basados ‚Äã‚Äãen entrenamiento de refuerzo.  En resumen, estos logros dependen de la optimizaci√≥n de las acciones del agente en un entorno particular para lograr la m√°xima recompensa.  En los √∫ltimos art√≠culos de <a href="https://medium.com/gradientcrescent">GradientCrescent,</a> hemos analizado varios aspectos fundamentales del aprendizaje reforzado, desde los conceptos b√°sicos de los sistemas de <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">bandidos</a> y los enfoques basados ‚Äã‚Äãen <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">pol√≠ticas</a> hasta la optimizaci√≥n del comportamiento basado en recompensas en los <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">entornos de Markov</a> .  Todos estos enfoques requieren un conocimiento completo de nuestro entorno.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">La programaci√≥n din√°mica</a> , por ejemplo, requiere que tengamos una distribuci√≥n de probabilidad completa de todas las posibles transiciones de estado.  Sin embargo, en realidad, encontramos que la mayor√≠a de los sistemas no se pueden interpretar completamente, y que las distribuciones de probabilidad no se pueden obtener expl√≠citamente debido a la complejidad, la incertidumbre inherente o las limitaciones en las capacidades computacionales.  Como analog√≠a, considere la tarea del meteor√≥logo: la cantidad de factores involucrados en el pron√≥stico del tiempo puede ser tan grande que es imposible calcular con precisi√≥n la probabilidad. <a name="habracut"></a><br><br>  Para tales casos, los m√©todos de ense√±anza como Monte Carlo son la soluci√≥n.  El t√©rmino Monte Carlo se usa com√∫nmente para describir cualquier enfoque para la estimaci√≥n de muestreo aleatorio.  En otras palabras, no predecimos el conocimiento sobre nuestro entorno, sino que aprendemos de la experiencia al pasar por secuencias ejemplares de estados, acciones y recompensas obtenidas como resultado de la interacci√≥n con el entorno.  Estos m√©todos funcionan observando directamente las recompensas devueltas por el modelo durante la operaci√≥n normal para juzgar el valor promedio de sus condiciones.  Curiosamente, incluso sin ning√∫n conocimiento de la din√°mica del entorno (que debe considerarse como la distribuci√≥n de probabilidad de las transiciones de estado), todav√≠a podemos obtener un comportamiento √≥ptimo para maximizar las recompensas. <br><br>  Como ejemplo, considere el resultado de un lanzamiento de 12 dados.  Considerando estos lanzamientos como un solo estado, podemos promediar estos resultados para acercarnos al verdadero resultado predicho.  Cuanto m√°s grande sea la muestra, m√°s exactamente nos acercaremos al resultado real esperado. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>La cantidad promedio esperada en 12 dados para 60 tiros es 41.57</i> <br><br>  Este tipo de evaluaci√≥n basada en muestreo puede parecer familiar para el lector, ya que dicho muestreo tambi√©n se realiza para sistemas k-bandit.  En lugar de comparar diferentes bandidos, los m√©todos de Monte Carlo se utilizan para comparar diferentes pol√≠ticas en entornos de Markov, determinando el valor del estado a medida que se sigue una determinada pol√≠tica hasta que se completa el trabajo. <br><br><h3>  Estimaci√≥n de Monte Carlo del valor del estado </h3><br>  En el contexto del aprendizaje por refuerzo, los m√©todos de Monte Carlo son una forma de evaluar la importancia del estado de un modelo promediando los resultados de la muestra.  Debido a la necesidad de un estado terminal, los m√©todos de Monte Carlo son inherentemente aplicables a entornos epis√≥dicos.  Debido a esta limitaci√≥n, los m√©todos de Monte Carlo generalmente se consideran "aut√≥nomos" en los que todas las actualizaciones se realizan despu√©s de alcanzar el estado terminal.  Se puede dar una analog√≠a simple con la b√∫squeda de una salida de un laberinto: un enfoque aut√≥nomo obligar√≠a al agente a llegar al final antes de usar la experiencia intermedia obtenida para intentar reducir el tiempo que lleva atravesar el laberinto.  Por otro lado, con el enfoque en l√≠nea, el agente cambiar√° constantemente su comportamiento durante el paso del laberinto, tal vez notar√° que los corredores verdes conducen a callejones sin salida y decidir√° evitarlos, por ejemplo.  Discutiremos los enfoques en l√≠nea en uno de los siguientes art√≠culos. <br><br>  El m√©todo de Monte Carlo se puede formular de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Para comprender mejor c√≥mo funciona el m√©todo Monte Carlo, considere el siguiente diagrama de transici√≥n de estado.  La recompensa por cada transici√≥n de estado se muestra en negro, se le aplica un factor de descuento de 0.5.  Dejemos de lado el valor real del estado y centr√©monos en calcular los resultados de un lanzamiento. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Diagrama de transici√≥n de estado.</i>  <i>El n√∫mero de estado se muestra en rojo, el resultado es negro.</i> <br>  Dado que el estado terminal devuelve un resultado igual a 0, calculemos el resultado de cada estado, comenzando con el estado terminal (G5).  Tenga en cuenta que hemos establecido el factor de descuento en 0.5, lo que conducir√° a una ponderaci√≥n hacia los estados posteriores. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  O m√°s generalmente: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Para evitar almacenar todos los resultados en la lista, podemos realizar el procedimiento de actualizaci√≥n gradual del valor de estado en el m√©todo de Monte Carlo, utilizando una ecuaci√≥n que tenga algunas similitudes con el descenso de gradiente tradicional: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Procedimiento de actualizaci√≥n incremental de Monte Carlo.</i>  <i>S es el estado, V es su valor, G es su resultado y A es el par√°metro del valor del paso.</i> <br><br>  Como parte del entrenamiento de refuerzo, los m√©todos de Monte Carlo pueden incluso clasificarse como Primera visita o Cada visita.  En resumen, la diferencia entre los dos es cu√°ntas veces se puede visitar un pasaje de un estado antes de la actualizaci√≥n de Monte Carlo.  El m√©todo de la primera visita de Monte Carlo estima el valor de todos los estados como el valor promedio de los resultados despu√©s de visitas individuales a cada estado antes de la finalizaci√≥n, mientras que el m√©todo de cada visita de Monte Carlo promedia los resultados despu√©s de n visitas hasta la finalizaci√≥n.  Utilizaremos la primera visita de Montecarlo a lo largo de este art√≠culo debido a su relativa simplicidad. <br><br><h3>  Gesti√≥n de pol√≠ticas de Monte Carlo </h3><br>  Si el modelo no puede proporcionar la pol√≠tica, Monte Carlo se puede utilizar para evaluar los valores de acci√≥n del estado.  Esto es m√°s √∫til que solo el significado de los estados, ya que la idea del significado de cada acci√≥n <i>(q)</i> en un estado dado le permite al agente formular autom√°ticamente una pol√≠tica a partir de observaciones en un entorno desconocido. <br><br>  M√°s formalmente, podemos usar Monte Carlo para estimar <i>q (s, a, pi)</i> , el resultado esperado al comenzar desde el estado s, la acci√≥n a y la pol√≠tica posterior <i>Pi</i> .  Los m√©todos de Monte Carlo siguen siendo los mismos, excepto que existe una dimensi√≥n adicional de las acciones tomadas para un determinado estado.  Se cree que se visita <i>un</i> par estado-acci√≥n <i>(s, a)</i> durante el paso si alguna vez se visita el estado <i>s</i> y se realiza la acci√≥n <i>a</i> en √©l.  Del mismo modo, la evaluaci√≥n de las acciones de valor puede llevarse a cabo utilizando los enfoques "Primera visita" y "Cada visita". <br><br>  Al igual que en la programaci√≥n din√°mica, podemos usar una pol√≠tica de iteraci√≥n generalizada (GPI) para formar una pol√≠tica a partir de la observaci√≥n de valores de acci√≥n estatal. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  Al alternar los pasos de evaluaci√≥n de pol√≠ticas y mejora de pol√≠ticas, e incluir investigaciones para asegurar que se visiten todas las acciones posibles, podemos lograr la pol√≠tica √≥ptima para cada condici√≥n.  Para el GPI de Monte Carlo, esta rotaci√≥n generalmente se realiza despu√©s del final de cada pase. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>Monte Carlo GPI</i> <br><br><h3>  Estrategia de blackjack </h3><br>  Para comprender mejor c√≥mo funciona el m√©todo de Monte Carlo en la pr√°ctica en la tarea de evaluar varios valores de estado, hagamos una demostraci√≥n paso a paso del juego de blackjack.  Para comenzar, determinemos las reglas y condiciones de nuestro juego: <br><br><ul><li>  Jugaremos solo contra el crupier, no habr√° otros jugadores.  Esto nos permitir√° considerar las manos del distribuidor como parte del medio ambiente. </li><li>  El valor de las tarjetas con n√∫meros iguales a los valores nominales.  El valor de las cartas ilustradas: Jack, King y Queen es 10. El valor del as puede ser 1 u 11 dependiendo de la elecci√≥n del jugador. </li><li>  Ambas partes reciben dos cartas.  Dos cartas de jugador est√°n boca arriba, una de las cartas del crupier tambi√©n est√° boca arriba. </li><li>  El objetivo del juego es que la cantidad de cartas en la mano es &lt;= 21.  Un valor mayor que 21 es una quiebra, si ambas partes tienen un valor de 21, entonces el juego se juega en empate. </li><li>  Despu√©s de que el jugador haya visto sus cartas y la primera carta del crupier, el jugador puede elegir tomarle una nueva carta ("todav√≠a") o no ("suficiente") hasta que est√© satisfecho con la suma de los valores de la carta en su mano. </li><li>  Luego, el crupier muestra su segunda carta: si la cantidad resultante es inferior a 17, est√° obligado a tomar cartas hasta llegar a 17 puntos, despu√©s de lo cual ya no toma la carta. </li></ul><br>  Veamos c√≥mo funciona el m√©todo Monte Carlo con estas reglas. <br><br><h4>  Ronda 1. </h4><br>  Ganas un total de 19. Pero intentas atrapar la suerte por la cola, arriesgarte, obt√©n 3 y ve a la quiebra.  Cuando quebraste, el crupier solo ten√≠a una tarjeta abierta con una suma de 10. Esto se puede representar de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Si vamos a la quiebra, nuestra recompensa por la ronda es -1.  Establezcamos este valor como el resultado de retorno del pen√∫ltimo estado, usando el siguiente formato [Cantidad de agente, cantidad de distribuidor, as?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Bueno, ahora no tenemos suerte.  Pasemos a otra ronda. <br><br><h4>  Ronda 2. </h4><br>  Escribe un total de 19. Esta vez decide detenerse.  El crupier marca 13, toma una tarjeta y se arruina.  El pen√∫ltimo estado se puede describir de la siguiente manera. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Describamos las condiciones y recompensas que recibimos en esta ronda: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Con el final del pasaje, ahora podemos actualizar los valores de todos nuestros estados en esta ronda utilizando los resultados calculados.  Tomando un factor de descuento de 1, simplemente distribuimos nuestra nueva recompensa manual, como se hizo con las transiciones de estado anteriores.  Como el estado <i>V (19, 10, no)</i> devolvi√≥ anteriormente -1, calculamos el valor de retorno esperado y lo asignamos a nuestro estado: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Valores del estado final para la demostraci√≥n utilizando el blackjack como ejemplo</i> . <br><br><h3>  Implementaci√≥n </h3><br>  Escribamos un juego de blackjack usando el m√©todo de la primera visita de Monte Carlo para descubrir todos los valores de estado posibles (o varias combinaciones disponibles) en el juego usando Python.  Nuestro enfoque se basar√° en el <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">enfoque de Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Como de costumbre, puede encontrar todo el c√≥digo del art√≠culo en <a href="https://github.com/EXJUSTICE/GradientCrescent)">nuestro GitHub</a> . <br><br>  Para simplificar la implementaci√≥n, utilizaremos el gimnasio de OpenAI.  Piense en el entorno como una interfaz para iniciar el blackjack con una cantidad m√≠nima de c√≥digo, esto nos permitir√° centrarnos en implementar el aprendizaje reforzado.  Convenientemente, toda la informaci√≥n recopilada sobre los estados, acciones y recompensas se almacena en las variables de <i>"observaci√≥n"</i> , que se acumulan durante las sesiones de juego actuales. <br><br>  Comencemos importando todas las bibliotecas que necesitamos para obtener y recopilar nuestros resultados. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Luego, inicialicemos nuestro entorno de <i>gimnasio</i> y definamos una pol√≠tica que coordine las acciones de nuestro agente.  De hecho, continuaremos tomando la tarjeta hasta que la cantidad en la mano alcance 19 o m√°s, despu√©s de lo cual nos detendremos. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Definamos un m√©todo para generar datos de pase utilizando nuestra pol√≠tica.  Almacenaremos informaci√≥n sobre el estado, las acciones tomadas y la remuneraci√≥n por la acci√≥n. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Finalmente, definamos la funci√≥n de predicci√≥n de Monte Carlo primera visita.  Primero, inicializamos un diccionario vac√≠o para almacenar los valores de estado actuales y un diccionario que almacena el n√∫mero de registros para cada estado en diferentes pases. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Para cada pase, llamamos a nuestro m√©todo <i>generate_episode</i> para obtener informaci√≥n sobre los valores del estado y las recompensas recibidas despu√©s de que ocurra el estado.  Tambi√©n inicializamos la variable para almacenar nuestros resultados incrementales.  Luego obtenemos la recompensa y el valor del estado actual para cada estado visitado durante el pase, y aumentamos nuestros rendimientos variables por el valor de la recompensa por paso. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) ‚Äî 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns ‚Äî value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Perm√≠tame recordarle que, dado que estamos implementando la primera visita de Montecarlo, visitamos un estado en un solo paso.  Por lo tanto, hacemos una verificaci√≥n condicional en el diccionario de estado para ver si el estado ha sido visitado.  Si se cumple esta condici√≥n, podemos calcular el nuevo valor utilizando el procedimiento definido previamente para actualizar los valores de estado utilizando el m√©todo de Monte Carlo y aumentar el n√∫mero de observaciones para este estado en 1. Luego, repetimos el proceso para el pr√≥ximo paso para obtener finalmente el valor promedio del resultado . <br><br>  ¬°Ejecutemos lo que tenemos y miremos los resultados! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Conclusi√≥n de una muestra que muestra los valores de estado de varias combinaciones en las manos en el blackjack.</i> <br><br>  Podemos continuar haciendo observaciones de Monte Carlo para 5000 pases y construir una distribuci√≥n de valores de estado que describa los valores de cualquier combinaci√≥n en manos del jugador y el crupier. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualizaci√≥n de los valores de estado de varias combinaciones en el blackjack.</i> <br><br>  Entonces, resumamos lo que aprendimos. <br><br><ul><li>  Los m√©todos de aprendizaje basados ‚Äã‚Äãen muestreo nos permiten evaluar los valores de estado y acci√≥n-estado sin ninguna din√°mica de transici√≥n, simplemente por muestreo. </li><li>  Los enfoques de Monte Carlo se basan en un muestreo aleatorio del modelo, observando las recompensas devueltas por el modelo y recolectando informaci√≥n durante la operaci√≥n normal para determinar el valor promedio de sus estados. </li><li>  Usando los m√©todos de Monte Carlo, es posible una pol√≠tica de iteraci√≥n generalizada. </li><li>  El valor de todas las combinaciones posibles en manos del jugador y del crupier en el blackjack se puede estimar utilizando m√∫ltiples simulaciones de Monte Carlo, allanando el camino para estrategias optimizadas. </li></ul><br>  Esto concluye la introducci√≥n al m√©todo de Monte Carlo.  En nuestro pr√≥ximo art√≠culo, pasaremos a los m√©todos de ense√±anza de la forma Aprendizaje de diferencia temporal. <br><br><h3>  Fuentes: </h3><br>  Sutton y col.  al, aprendizaje de refuerzo <br>  White et.  al, Fundamentos del aprendizaje por refuerzo, Universidad de Alberta <br>  Silva et.  al, aprendizaje por refuerzo, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Universidad de Northeaster</a> <br><br>  Eso es todo.  ¬°Nos vemos en el <a href="https://otus.pw/Zkti/">curso</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/477042/">https://habr.com/ru/post/477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../477022/index.html">¬øC√≥mo podemos ayudarte? Como nos pueden ayudar</a></li>
<li><a href="../477026/index.html">S√©ptimo Hackathon anual de JetBrains</a></li>
<li><a href="../477032/index.html">De blockchain a DAG: deshacerse de los intermediarios</a></li>
<li><a href="../477038/index.html">El mejor lenguaje de programaci√≥n para principiantes.</a></li>
<li><a href="../477040/index.html">Gartner chart 2019: ¬øde qu√© se tratan todas estas palabras de moda?</a></li>
<li><a href="../477044/index.html">Automatizaci√≥n de pruebas End-2-End de un sistema de informaci√≥n integrado. Parte 2. T√©cnica</a></li>
<li><a href="../477046/index.html">.Net Meetup en Raiffeisenbank 28/11 + Broadcast</a></li>
<li><a href="../477048/index.html">¬øPor qu√© una empresa con una capitalizaci√≥n de $ 55 mil millones pens√≥ en abandonar el intercambio?</a></li>
<li><a href="../477050/index.html">Black Friday 2019 para video vigilancia y nubes.</a></li>
<li><a href="../477052/index.html">Reactor, WebFlux, Kotlin Coroutines o Asynchrony con un ejemplo simple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>