<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôèüèª üë®‚Äçüé® üôÑ Apache Kafka et le streaming avec Spark Streaming üë®üèæ ‚ö™Ô∏è üõÄüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Aujourd'hui, nous allons construire un syst√®me qui utilisera Apark Kafka pour traiter les flux de messages √† l'aide de Spark Streaming ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apache Kafka et le streaming avec Spark Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/451160/">  Bonjour, Habr!  Aujourd'hui, nous allons construire un syst√®me qui utilisera Apark Kafka pour traiter les flux de messages √† l'aide de Spark Streaming et √©crire le r√©sultat du traitement dans la base de donn√©es cloud AWS RDS. <br><br>  Imaginez qu'un certain √©tablissement de cr√©dit nous ait confi√© la t√¢che de traiter √† la vol√©e les transactions entrantes dans toutes ses succursales.  Cela peut √™tre fait afin de calculer rapidement la position de change ouverte pour le Tr√©sor, les limites ou les r√©sultats financiers des transactions, etc. <br><br>  Comment mettre en ≈ìuvre ce cas sans utiliser de magie et de sorts magiques - nous lisons sous la coupe!  C'est parti! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">(Source de l'image)</a> <br><a name="habracut"></a><br><h2>  Pr√©sentation </h2><br>  Bien s√ªr, le traitement d'un grand tableau de donn√©es en temps r√©el offre de nombreuses possibilit√©s d'utilisation dans les syst√®mes modernes.  L'une des combinaisons les plus populaires pour cela est le tandem Apache Kafka et Spark Streaming, o√π Kafka cr√©e un flux de paquets de messages entrants, et Spark Streaming traite ces paquets √† un intervalle de temps sp√©cifi√©. <br><br>  Pour augmenter la tol√©rance aux pannes de l'application, nous utiliserons des points de contr√¥le - points de contr√¥le.  En utilisant ce m√©canisme, lorsque le module Spark Streaming a besoin de r√©cup√©rer des donn√©es perdues, il n'a qu'√† retourner au dernier point de contr√¥le et reprendre ses calculs. <br><br><h2>  Architecture du syst√®me en cours de d√©veloppement </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Composants utilis√©s: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>Apache Kafka</b></a> est un syst√®me de messagerie distribu√© avec publication et abonnement.  Convient √† la fois pour la consommation de messages hors ligne et en ligne.  Pour √©viter la perte de donn√©es, les messages Kafka sont stock√©s sur disque et r√©pliqu√©s dans le cluster.  Le syst√®me Kafka est construit au-dessus du service de synchronisation ZooKeeper; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>Apache Spark Streaming</b></a> - Composant Spark pour le traitement des donn√©es en streaming.  Le module Spark Streaming est construit √† l'aide de l'architecture micro-batch, lorsque le flux de donn√©es est interpr√©t√© comme une s√©quence continue de petits paquets de donn√©es.  Spark Streaming re√ßoit des donn√©es de diverses sources et les combine en petits paquets.  De nouveaux packages sont cr√©√©s √† intervalles r√©guliers.  Au d√©but de chaque intervalle de temps, un nouveau paquet est cr√©√© et toutes les donn√©es re√ßues pendant cet intervalle sont incluses dans le paquet.  √Ä la fin de l'intervalle, la croissance des paquets s'arr√™te.  La taille de l'intervalle est d√©termin√©e par un param√®tre appel√© intervalle de lot; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>Apache Spark SQL</b></a> - Combine le traitement relationnel avec la programmation fonctionnelle Spark.  Les donn√©es structur√©es font r√©f√©rence aux donn√©es qui ont un sch√©ma, c'est-√†-dire un ensemble unique de champs pour tous les enregistrements.  Spark SQL prend en charge la saisie √† partir d'une vari√©t√© de sources de donn√©es structur√©es et, gr√¢ce √† la disponibilit√© des informations sur le sch√©ma, il ne peut r√©cup√©rer efficacement que les champs d'enregistrement requis et fournit √©galement des API DataFrame; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>AWS RDS</b></a> est une base de donn√©es relationnelle bas√©e sur le cloud relativement peu co√ªteuse, un service Web qui simplifie la configuration, le fonctionnement et la mise √† l'√©chelle, et est directement administr√© par Amazon. </li></ul><br><h2>  Installer et d√©marrer le serveur Kafka </h2><br>  Avant d'utiliser directement Kafka, vous devez vous assurer que Java est disponible, car  JVM est utilis√© pour le travail: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Cr√©ez un nouvel utilisateur pour travailler avec Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  Ensuite, t√©l√©chargez la distribution sur le site officiel d'Apache Kafka: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  D√©compressez l'archive t√©l√©charg√©e: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  L'√©tape suivante est facultative.  Le fait est que les param√®tres par d√©faut ne permettent pas d'utiliser pleinement toutes les fonctionnalit√©s d'Apache Kafka.  Par exemple, supprimez un sujet, une cat√©gorie, un groupe dans lequel les messages peuvent √™tre publi√©s.  Pour changer cela, modifiez le fichier de configuration: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  Ajoutez ce qui suit √† la fin du fichier: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Avant de d√©marrer le serveur Kafka, vous devez d√©marrer le serveur ZooKeeper, nous utiliserons le script auxiliaire fourni avec la distribution Kafka: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  Apr√®s le d√©marrage r√©ussi de ZooKeeper, dans un terminal s√©par√©, nous lan√ßons le serveur Kafka: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Cr√©ez un nouveau sujet appel√© Transaction: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Assurez-vous que la rubrique avec le bon nombre de partitions et de r√©plication a √©t√© cr√©√©e: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Nous manquerons les moments de tester le producteur et le consommateur pour le sujet nouvellement cr√©√©.  Pour plus de d√©tails sur la fa√ßon de tester l'envoi et la r√©ception de messages, consultez la documentation officielle - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Envoyer des messages</a> .  Eh bien, nous passons √† l'√©criture d'un producteur en Python √† l'aide de l'API KafkaProducer. <br><br><h2>  √âcriture du producteur </h2><br>  Le producteur g√©n√©rera des donn√©es al√©atoires - 100 messages par seconde.  Par donn√©es al√©atoires, nous entendons un dictionnaire compos√© de trois champs: <br><br><ul><li>  <b>Succursale</b> - nom du point de vente de l'√©tablissement de cr√©dit; </li><li>  <b>Devise</b> - <b>devise de</b> transaction; </li><li>  <b>Montant</b> - montant de la transaction.  Le montant sera un nombre positif s'il s'agit d'un achat de devises par la Banque et n√©gatif s'il s'agit d'une vente. </li></ul><br>  Le code du producteur est le suivant: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  Ensuite, en utilisant la m√©thode d'envoi, nous envoyons un message au serveur, dans la rubrique dont nous avons besoin, au format JSON: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  Lors de l'ex√©cution du script, nous recevons les messages suivants dans le terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  Cela signifie que tout fonctionne comme nous le voulions - le producteur g√©n√®re et envoie des messages au sujet dont nous avons besoin. <br><br>  L'√©tape suivante consiste √† installer Spark et √† traiter ce flux de messages. <br><br><h2>  Installer Apache Spark </h2><br>  <b>Apache Spark</b> est une plate-forme informatique en grappe polyvalente et hautes performances. <br><br>  En termes de performances, Spark surpasse les impl√©mentations populaires du mod√®le MapReduce, fournissant simultan√©ment la prise en charge d'un plus large √©ventail de types de calculs, y compris les requ√™tes interactives et le traitement des flux.  La vitesse joue un r√¥le important dans le traitement de grandes quantit√©s de donn√©es, car c'est la vitesse qui vous permet de travailler de mani√®re interactive sans passer des minutes ou des heures √† attendre.  L'une des plus grandes forces de Spark √† une vitesse aussi √©lev√©e est sa capacit√© √† effectuer des calculs en m√©moire. <br><br>  Ce framework est √©crit en Scala, vous devez donc d'abord l'installer: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  T√©l√©chargez la distribution Spark sur le site officiel: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  D√©compressez l'archive: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  Ajoutez le chemin d'acc√®s √† Spark dans le fichier bash: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  Ajoutez les lignes suivantes via l'√©diteur: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  Ex√©cutez la commande ci-dessous apr√®s avoir apport√© des modifications √† bashrc: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  D√©ploiement d'AWS PostgreSQL </h2><br>  Il reste √† d√©ployer la base de donn√©es, o√π nous t√©l√©chargerons les informations trait√©es √† partir des flux.  Pour cela, nous utiliserons le service AWS RDS. <br><br>  Acc√©dez √† la console AWS -&gt; AWS RDS -&gt; Bases de donn√©es -&gt; Cr√©er une base de donn√©es: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  S√©lectionnez PostgreSQL et cliquez sur le bouton Suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Parce que  Cet exemple est compris uniquement √† des fins p√©dagogiques, nous utiliserons un serveur gratuit ¬´au minimum¬ª (Free Tier): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  Ensuite, cochez le bloc Free Tier, et apr√®s cela, on nous proposera automatiquement une instance de la classe t2.micro - bien que faible, elle est gratuite et tout √† fait adapt√©e √† notre t√¢che: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Des choses tr√®s importantes suivent: le nom de l'instance de base de donn√©es, le nom de l'utilisateur principal et son mot de passe.  Nommons l'instance: myHabrTest, l'utilisateur principal: <b>habr</b> , le mot de passe: <b>habr12345</b> et cliquez sur le bouton Suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  La page suivante contient les param√®tres responsables de la disponibilit√© de notre serveur de base de donn√©es de l'ext√©rieur (accessibilit√© publique) et de la disponibilit√© des ports: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Cr√©ons une nouvelle configuration pour le groupe de s√©curit√© VPC, qui nous permettra d'acc√©der √† notre serveur de base de donn√©es de l'ext√©rieur via le port 5432 (PostgreSQL). <br><br>  Dans une fen√™tre de navigateur distincte, acc√©dez √† la console AWS dans le tableau de bord VPC -&gt; Groupes de s√©curit√© -&gt; Cr√©er un groupe de s√©curit√©: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  D√©finissez le nom du groupe S√©curit√© - PostgreSQL, une description, indiquez √† quel VPC ce groupe doit √™tre associ√© et cliquez sur le bouton Cr√©er: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  Remplissez le groupe de r√®gles entrantes nouvellement cr√©√© pour le port 5432, comme indiqu√© dans l'image ci-dessous.  Vous n'avez pas besoin de sp√©cifier un port manuel, mais s√©lectionnez PostgreSQL dans la liste d√©roulante Type. <br><br>  √Ä strictement parler, la valeur :: / 0 signifie la disponibilit√© du trafic entrant pour un serveur de partout dans le monde, ce qui n'est canoniquement pas tout √† fait vrai, mais pour l'analyse de l'exemple, utilisons cette approche: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  Nous revenons √† la page du navigateur, o√π nous avons ¬´Configurer les param√®tres avanc√©s¬ª ouvert et s√©lectionnez dans la section Groupes de s√©curit√© VPC -&gt; Choisissez les groupes de s√©curit√© VPC existants -&gt; PostgreSQL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  Ensuite, dans la section Options de la base de donn√©es -&gt; Nom de la base de donn√©es -&gt; d√©finissez le nom - <b>habrDB</b> . <br><br>  Nous pouvons laisser le reste des param√®tres, √† l'exception de la d√©sactivation de la sauvegarde (p√©riode de r√©tention de la sauvegarde - 0 jour), de la surveillance et de Performance Insights, par d√©faut.  Cliquez sur le bouton <b>Cr√©er une base de donn√©es</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Gestionnaire de flux </h2><br>  La derni√®re √©tape sera le d√©veloppement de travaux Spark, qui traiteront toutes les deux secondes les nouvelles donn√©es provenant de Kafka et entreront le r√©sultat dans la base de donn√©es. <br><br>  Comme indiqu√© ci-dessus, les points de contr√¥le sont le principal m√©canisme de SparkStreaming qui doit √™tre configur√© pour fournir une tol√©rance aux pannes.  Nous utiliserons des points de contr√¥le et, en cas de chute d'une proc√©dure, le module Spark Streaming n'aura qu'√† retourner au dernier point de contr√¥le et reprendre ses calculs pour restaurer les donn√©es perdues. <br><br>  Vous pouvez activer le point d'arr√™t en d√©finissant le r√©pertoire dans un syst√®me de fichiers fiable et tol√©rant aux pannes (par exemple, HDFS, S3, etc.), dans lequel les informations sur le point d'arr√™t seront enregistr√©es.  Cela se fait en utilisant, par exemple: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  Dans notre exemple, nous utiliserons l'approche suivante, √† savoir, si checkpointDirectory existe, le contexte sera recr√©√© √† partir des donn√©es du point de contr√¥le.  Si le r√©pertoire n'existe pas (c'est-√†-dire qu'il est ex√©cut√© pour la premi√®re fois), la fonction functionToCreateContext est appel√©e pour cr√©er un nouveau contexte et configurer DStreams: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Cr√©ez un objet DirectStream pour vous connecter √† la rubrique "transaction" √† l'aide de la m√©thode createDirectStream de la biblioth√®que KafkaUtils: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  Analyse des donn√©es entrantes au format JSON: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  En utilisant Spark SQL, nous faisons un regroupement simple et imprimons le r√©sultat sur la console: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Obtenir le texte de la requ√™te et l'ex√©cuter via Spark SQL: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  Et puis nous enregistrons les donn√©es agr√©g√©es re√ßues dans une table dans AWS RDS.  Pour enregistrer les r√©sultats d'agr√©gation dans une table de base de donn√©es, nous utiliserons la m√©thode d'√©criture de l'objet DataFrame: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  Quelques mots sur la configuration d'une connexion √† AWS RDS.  Nous avons cr√©√© l'utilisateur et le mot de passe pour celui-ci √† l'√©tape ¬´D√©ploiement d'AWS PostgreSQL¬ª.  Pour l'URL du serveur de base de donn√©es, utilisez Endpoint, qui s'affiche dans la section Connectivit√© et s√©curit√©: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  Afin de connecter correctement Spark et Kafka, vous devez ex√©cuter le travail via smark-submit en utilisant l' <b>artefact spark-streaming-kafka-0-8_2.11</b> .  De plus, nous appliquons √©galement l'artefact pour interagir avec la base de donn√©es PostgreSQL, nous les transf√©rerons via --packages. <br><br>  Pour la flexibilit√© du script, nous supprimons √©galement le nom du serveur de messages et le sujet √† partir duquel nous voulons recevoir des donn√©es comme param√®tres d'entr√©e. <br><br>  Il est donc temps de d√©marrer et de tester le syst√®me: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  Tout a fonctionn√©!  Comme vous pouvez le voir dans l'image ci-dessous, pendant le fonctionnement de l'application, de nouveaux r√©sultats d'agr√©gation sont affich√©s toutes les 2 secondes, car nous avons d√©fini l'intervalle de traitement par lots √† 2 secondes lorsque nous avons cr√©√© l'objet StreamingContext: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  Ensuite, nous effectuons une simple requ√™te dans la base de donn√©es pour v√©rifier les enregistrements dans la table <b>transaction_flow</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Conclusion </h2><br>  Cet article a examin√© un exemple de traitement d'informations en continu √† l'aide de Spark Streaming en conjonction avec Apache Kafka et PostgreSQL.  Avec la croissance des donn√©es provenant de diverses sources, il est difficile de surestimer la valeur pratique de Spark Streaming pour cr√©er des applications de streaming et des applications fonctionnant en temps r√©el. <br><br>  Vous pouvez trouver le code source complet dans mon r√©f√©rentiel sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> . <br><br>  Je suis pr√™t √† discuter de cet article avec plaisir, j'attends vos commentaires avec impatience et j'esp√®re √©galement des critiques constructives de tous les lecteurs concern√©s. <br><br>  Je vous souhaite du succ√®s! <br><br>  <b>PS</b> Il √©tait initialement pr√©vu d'utiliser une base de donn√©es PostgreSQL locale, mais √©tant donn√© mon amour pour AWS, j'ai d√©cid√© de mettre la base de donn√©es dans le cloud.  Dans le prochain article sur ce sujet, je montrerai comment impl√©menter l'ensemble du syst√®me d√©crit ci-dessus dans AWS √† l'aide d'AWS Kinesis et d'AWS EMR.  Suivez l'actualit√©! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr451160/">https://habr.com/ru/post/fr451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr451148/index.html">Programmation orient√©e objet en langages graphiques</a></li>
<li><a href="../fr451150/index.html">Attrape-moi si tu peux. Version du gestionnaire</a></li>
<li><a href="../fr451152/index.html">La r√©sistance dans le circuit de grille ou comment le faire correctement</a></li>
<li><a href="../fr451154/index.html">Syst√®me local autonome d'acquisition de donn√©es (suite)</a></li>
<li><a href="../fr451158/index.html">Circuits √©lectriques. Types de circuits</a></li>
<li><a href="../fr451162/index.html">Correction d'erreurs - Constantes physiques dans les versions actuelles et nouvelles du syst√®me international d'unit√©s (SI)</a></li>
<li><a href="../fr451164/index.html">Vous cherchez un espace de stationnement gratuit avec Python</a></li>
<li><a href="../fr451166/index.html">Qu'offriront les nouveaux r√©f√©rentiels pour les syst√®mes AI et MO?</a></li>
<li><a href="../fr451170/index.html">Jeff Bezos a annonc√© son intention de conqu√©rir la lune</a></li>
<li><a href="../fr451172/index.html">Julia: fonctions et structures en tant que fonctions</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>