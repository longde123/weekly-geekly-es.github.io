<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ò üòÆ ü§¶ RL Intuitivo (Aprendizagem por Refor√ßo): Introdu√ß√£o ao Crit√©rio de Atores de Vantagem (A2C) üë©‚Äçüç≥ ü§§ üë®‚Äçüë®‚Äçüëß‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Trago a sua aten√ß√£o uma tradu√ß√£o do artigo de Rudy Gilman e Katherine Wang RL intuitiva: Introdu√ß√£o ao Advantage-Actor-Critic (A2C) . 

 Os ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RL Intuitivo (Aprendizagem por Refor√ßo): Introdu√ß√£o ao Crit√©rio de Atores de Vantagem (A2C)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442522/"><p>  Ol√° Habr!  Trago a sua aten√ß√£o uma tradu√ß√£o do artigo de Rudy Gilman e Katherine Wang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RL intuitiva: Introdu√ß√£o ao Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Os Especialistas em Aprendizado Refor√ßado (RL) produziram muitos tutoriais excelentes.  A maioria, no entanto, descreve RL em termos de equa√ß√µes matem√°ticas e diagramas abstratos.  Gostamos de pensar sobre o assunto de uma perspectiva diferente.  A pr√≥pria RL √© inspirada na maneira como os animais aprendem; ent√£o, por que n√£o converter o mecanismo subjacente da RL novamente em fen√¥menos naturais que ele pretende simular?  As pessoas aprendem melhor atrav√©s de hist√≥rias. </p><br><p>  Esta √© a hist√≥ria do modelo Actor Advantage Critic (A2C).  O modelo sujeito-cr√≠tico √© uma forma popular do modelo Gradiente de Pol√≠tica, que por si s√≥ √© um algoritmo tradicional de RL.  Se voc√™ entende A2C, entende RL profunda. </p><br><a name="habracut"></a><p>  Depois de obter uma compreens√£o intuitiva do A2C, verifique: </p><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nossa simples implementa√ß√£o</a> do c√≥digo A2C (para treinamento) ou nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vers√£o</a> industrial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do PyTorch com</a> base no modelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI TensorFlow Baselines</a> ; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma introdu√ß√£o ao RL por Barto &amp; Sutton</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o curso can√¥nico de David Silver</a> , uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">revis√£o por Yusi Lee</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Denny Brits no reposit√≥rio do GitHub</a> para imers√£o profunda no RL; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O incr√≠vel curso fast.ai</a> para cobertura intuitiva e pr√°tica de aprendizado profundo em geral, implementado no PyTorch; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arthur Giuliani</a> RL Tutoriais implementados no TensorFlow. </li></ul><p>  Ilustra√ß√µes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">@embermarke</a> </p><br><p>  Em RL, o agente, a raposa Klyukovka, percorre estados cercados por a√ß√µes, tentando maximizar as recompensas ao longo do caminho. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C recebe entradas de status - entradas de sensor no caso de Klukovka - e gera duas sa√≠das: <br>  1) Uma avalia√ß√£o de quanto a remunera√ß√£o ser√° recebida, a partir do momento do estado atual, com exce√ß√£o da remunera√ß√£o atual (existente). <br>  2) Uma recomenda√ß√£o sobre que a√ß√£o tomar (pol√≠tica). <br><br>  Cr√≠tico: uau, que vale maravilhoso!  Ser√° um dia frut√≠fero para forragear!  Aposto que hoje colecionarei 20 pontos antes do p√¥r do sol. <br>  "Assunto": essas flores est√£o lindas, sinto um desejo por "A". </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Os modelos Deep RL s√£o m√°quinas de mapeamento de entrada e sa√≠da, como qualquer outro modelo de classifica√ß√£o ou regress√£o.  Em vez de categorizar imagens ou texto, os modelos RL profundos levam estados a a√ß√µes e / ou estados a valores de estado.  A2C faz as duas coisas. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  Esse conjunto de recompensa-a√ß√£o-estado √© uma observa√ß√£o.  Ela escrever√° essa linha de dados em seu di√°rio, mas ainda n√£o pensar√° nisso.  Ela o preencher√° quando parar para pensar. <br><br>  Alguns autores associam a recompensa 1 √† etapa 1 do tempo, outros a associam √† etapa 2, mas todos t√™m em mente o mesmo conceito: a recompensa est√° relacionada ao estado e a a√ß√£o imediatamente o precede. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Enganchar repete o processo novamente.  Primeiro, ela percebe seu entorno e desenvolve uma fun√ß√£o V (S) e uma recomenda√ß√£o de a√ß√£o. <br><br>  Cr√≠tico: Este vale parece bastante padr√£o.  V (S) = 19. <br>  Assunto: As op√ß√µes de a√ß√£o s√£o muito semelhantes.  Eu acho que vou apenas na pista "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Ent√£o ele age. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  Recebe uma recompensa de +20!  E registra a observa√ß√£o. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  Ela repete o processo novamente. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  Depois de coletar tr√™s observa√ß√µes, Klyukovka p√°ra para pensar. <br><br>  Outras fam√≠lias de modelos esperam at√© o final do dia (Monte Carlo), enquanto outras pensam ap√≥s cada etapa (uma etapa). <br>  Antes que ela possa estabelecer sua cr√≠tica interna, Klukovka precisa calcular quantos pontos ela realmente receber√° em cada estado. <br><br>  Mas primeiro! <br>  Vejamos como a prima de Klukovka, Lis Monte Carlo, calcula o verdadeiro significado de cada estado. <br><br>  Os modelos de Monte Carlo n√£o refletem sua experi√™ncia at√© o final do jogo e, como o valor do √∫ltimo estado √© zero, √© muito simples encontrar o verdadeiro valor desse estado anterior como a soma das recompensas recebidas ap√≥s esse momento. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  De fato, esta √© apenas uma amostra de alta dispers√£o V (S).  O agente poderia facilmente seguir uma trajet√≥ria diferente do mesmo estado, recebendo assim uma recompensa agregada diferente. </p><br><p>  Mas Klyukovka vai, para e reflete muitas vezes at√© o dia chegar ao fim.  Ela quer saber quantos pontos ela realmente receber√° de cada estado at√© o final do jogo, porque faltam v√°rias horas para o final do jogo. <br><br>  √â a√≠ que ela faz algo realmente inteligente - a raposa Klyukovka estima quantos pontos receber√° pelo √∫ltimo estado neste conjunto.  Felizmente, ela tem uma avalia√ß√£o correta de sua condi√ß√£o - sua cr√≠tica. <br>  Com essa avalia√ß√£o, Klyukovka pode calcular os valores "corretos" dos estados anteriores exatamente como a raposa de Monte Carlo. <br><br>  Lis Monte Carlo avalia as marcas de alvo, realizando a trajet√≥ria e acrescentando recompensas a partir de cada estado.  A2C corta essa trajet√≥ria e a substitui por uma avalia√ß√£o de seu cr√≠tico.  Essa carga inicial reduz a varia√ß√£o da pontua√ß√£o e permite que o A2C seja executado continuamente, embora introduzindo um pequeno vi√©s. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  As recompensas s√£o muitas vezes reduzidas para refletir o fato de que a remunera√ß√£o √© agora melhor do que no futuro.  Por uma quest√£o de simplicidade, Klukovka n√£o reduz suas recompensas. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p>  O Klukovka agora pode percorrer cada linha de dados e comparar suas estimativas de valores de estado com seus valores reais.  Ela usa a diferen√ßa entre esses n√∫meros para aperfei√ßoar suas habilidades de previs√£o.  A cada tr√™s etapas ao longo do dia, Klyukovka coleta uma experi√™ncia valiosa que vale a pena considerar. <br><br>  ‚ÄúClassifiquei mal os estados 1 e 2. O que fiz de errado?  Sim!  A pr√≥xima vez que eu ver penas como essas, aumentarei V (S). <br><br>  Pode parecer louco que Klukovka seja capaz de usar sua classifica√ß√£o V (S) como base para compar√°-la com outras previs√µes.  Mas os animais (incluindo n√≥s) fazem isso o tempo todo!  Se voc√™ acha que as coisas est√£o indo bem, n√£o precisa treinar novamente as a√ß√µes que o levaram a esse estado. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  Cortando nossas sa√≠das calculadas e substituindo-as por uma estimativa de carga inicial, substitu√≠mos a grande varia√ß√£o de Monte Carlo por uma pequena inclina√ß√£o.  Os modelos de RL normalmente sofrem de alta dispers√£o (representando todos os caminhos poss√≠veis), e essa substitui√ß√£o geralmente vale a pena. </p><br><p>  Klukovka repete esse processo o dia todo, coletando tr√™s observa√ß√µes de recompensa de a√ß√£o-estado e refletindo sobre elas. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Cada conjunto de tr√™s observa√ß√µes √© uma pequena s√©rie autocorrelacionada de dados de treinamento rotulados.  Para reduzir essa autocorrela√ß√£o, muitos A2Cs treinam muitos agentes em paralelo, somando sua experi√™ncia antes de envi√°-la para uma rede neural comum. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  O dia est√° finalmente chegando ao fim.  Apenas dois passos restantes. <br>  Como dissemos anteriormente, as recomenda√ß√µes das a√ß√µes de Klukovka s√£o expressas em porcentagem de confian√ßa sobre suas capacidades.  Em vez de apenas escolher a op√ß√£o mais confi√°vel, Klukovka escolhe essa distribui√ß√£o de a√ß√µes.  Isso garante que ela nem sempre aceite a√ß√µes seguras, mas potencialmente med√≠ocres. </p><br><p>  Eu poderia me arrepender, mas ... √Äs vezes, explorando coisas desconhecidas, voc√™ pode chegar a novas descobertas emocionantes ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  Para incentivar ainda mais a pesquisa, um valor chamado entropia √© subtra√≠do da fun√ß√£o de perda.  Entropia significa o "escopo" da distribui√ß√£o de a√ß√µes. <br>  - Parece que o jogo valeu a pena! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  Ou n√£o? <br><br>  √Äs vezes, o agente est√° em um estado em que todas as a√ß√µes levam a resultados negativos.  A2C, no entanto, lida bem com situa√ß√µes ruins. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  Quando o sol se p√¥s, Klyukovka refletiu sobre o √∫ltimo conjunto de solu√ß√µes. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  Conversamos sobre como Klyukovka cria seu cr√≠tico interno.  Mas como ela ajusta seu "sujeito" interno?  Como ela aprende a fazer escolhas t√£o requintadas? </p><br><p>  A pol√≠tica gradiente-raposa de mente simples examinaria a renda real ap√≥s a a√ß√£o e ajustaria sua pol√≠tica para aumentar a probabilidade de uma boa renda: - Parece que minha pol√≠tica nesse estado levou a uma perda de 20 pontos, acho que no futuro √© melhor fazer "C" menos prov√°vel. <br><br>  - Mas espera!  √â injusto culpar a a√ß√£o "C".  Esse estado tinha um valor estimado de -100, portanto, escolher "C" e terminar com -20 foi na verdade uma melhoria relativa de 80!  Eu tenho que tornar "C" mais prov√°vel no futuro. <br><br>  Em vez de ajustar sua pol√≠tica em resposta √† receita total que recebeu ao selecionar a a√ß√£o C, ela ajusta sua a√ß√£o √†s receitas relativas da a√ß√£o C. Isso √© chamado de ‚Äúvantagem‚Äù. </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  O que chamamos de vantagem √© simplesmente um erro.  Como vantagem, Klukovka o usa para tornar mais surpreendentemente boas atividades surpreendentemente boas.  Por engano, ela usa a mesma quantia para pressionar seu cr√≠tico interno para melhorar sua avalia√ß√£o do valor do status. <br><br>  O sujeito tira proveito de: <br>  - "Uau, isso funcionou melhor do que eu pensava, a a√ß√£o C deve ser uma boa id√©ia." <br>  O cr√≠tico usa o erro: <br>  ‚ÄúMas por que fiquei surpresa?  Provavelmente n√£o deveria ter avaliado essa condi√ß√£o t√£o negativamente. " <br><br>  Agora podemos mostrar como as perdas totais s√£o calculadas - minimizamos essa fun√ß√£o para melhorar nosso modelo. <br>  ‚ÄúPerda total = perda de a√ß√£o + perda de valor - entropia‚Äù <br><br>  Observe que, para calcular os gradientes de tr√™s tipos qualitativamente diferentes, consideramos os valores "atrav√©s de um".  Isso √© eficaz, mas pode dificultar a converg√™ncia. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Como todos os animais, √† medida que Klyukovka envelhece, ele aprimora sua capacidade de prever os valores dos estados, ganha mais confian√ßa em suas a√ß√µes e menos frequentemente se surpreende com os pr√™mios. </p><br><p>  Os agentes de RL, como Klukovka, n√£o apenas geram todos os dados necess√°rios, simplesmente interagindo com o ambiente, mas tamb√©m avaliam os pr√≥prios r√≥tulos de destino.  √â isso mesmo, os modelos RL atualizam as notas anteriores para corresponder melhor √†s notas novas e melhoradas. <br><br>  Como o Dr. David Silver, chefe do grupo de RL do Google Deepmind, diz: AI = DL + RL.  Quando um agente como Klyukovka pode definir sua pr√≥pria intelig√™ncia, as possibilidades s√£o infinitas ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt442522/">https://habr.com/ru/post/pt442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt442512/index.html">Customiza√ß√£o do Django ORM no exemplo do ZomboDB</a></li>
<li><a href="../pt442514/index.html">Sistemas distribu√≠dos. Padr√µes de design. Resenha</a></li>
<li><a href="../pt442516/index.html">Guia do Pandas para an√°lise de big data</a></li>
<li><a href="../pt442518/index.html">As 10 melhores t√©cnicas de hackers na Web em 2018</a></li>
<li><a href="../pt442520/index.html">Case. Economizando 300 000 p. por m√™s em publicidade contextual</a></li>
<li><a href="../pt442524/index.html">Como aumentar a seguran√ßa nos sistemas de identifica√ß√£o pessoal e controle de acesso</a></li>
<li><a href="../pt442526/index.html">A hist√≥ria dos toca-fitas sovi√©ticos (parte dois): o boom do Walkmen, um gadget para a KGB e gravadores</a></li>
<li><a href="../pt442528/index.html">Como fazer o jogo funcionar a 60fps</a></li>
<li><a href="../pt442530/index.html">Wireshark 3.0.0: revis√£o de inova√ß√µes</a></li>
<li><a href="../pt442532/index.html">Gravadores de v√≠deo para vigil√¢ncia por v√≠deo - gratuitamente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>