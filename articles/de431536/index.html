<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘ƒğŸ» ğŸš¬ ğŸ¥š Ceph. Katastrophenanatomie ğŸ‘©ğŸ½â€ğŸš€ ğŸ‘¨ğŸ¾â€ğŸ”¬ ğŸš’</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ceph ist ein Objektspeicher, mit dem ein Failovercluster erstellt werden kann. Trotzdem passieren Fehler. Jeder, der mit Ceph arbeitet, kennt die Lege...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph. Katastrophenanatomie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/431536/">  Ceph ist ein Objektspeicher, mit dem ein Failovercluster erstellt werden kann.  Trotzdem passieren Fehler.  Jeder, der mit Ceph arbeitet, kennt die Legende Ã¼ber CloudMouse oder Rosreestr.  Leider ist es nicht Ã¼blich, negative Erfahrungen mit uns zu teilen, die Ursachen von Fehlern werden meistens vertuscht und erlauben zukÃ¼nftigen Generationen nicht, aus den Fehlern anderer zu lernen. <br><br>  Nun, lassen Sie uns einen Testcluster einrichten, der jedoch dem realen nahe kommt, und die Katastrophe anhand von Knochen analysieren.  Wir werden alle LeistungseinbuÃŸen messen, Speicherlecks finden und den Service-Wiederherstellungsprozess analysieren.  Und all dies unter der FÃ¼hrung von Artemy Kapitula, der fast ein Jahr lang Fallstricke studierte, fÃ¼hrte dazu, dass die Clusterleistung bei Null versagte und die Latenz nicht auf unanstÃ¤ndige Werte sprang.  Und ich habe eine rote Grafik, die viel besser ist. <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>  Als nÃ¤chstes finden Sie eine Video- und Textversion eines der besten Berichte von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DevOpsConf Russia</a> 2018. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_fWYUl2QsoI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  <strong>Ãœber den Sprecher:</strong> Artemy Kapitula Systemarchitekt RCNTEC.  Das Unternehmen bietet IP-TelefonielÃ¶sungen an (Zusammenarbeit, Organisation eines Remote-BÃ¼ros, softwaredefinierte Speichersysteme und Energieverwaltungs- / Verteilungssysteme).  Das Unternehmen ist hauptsÃ¤chlich im Unternehmensbereich tÃ¤tig und daher auf dem DevOps-Markt nicht sehr bekannt.  Dennoch wurden einige Erfahrungen mit Ceph gesammelt, das in vielen Projekten als Grundelement der Speicherinfrastruktur verwendet wird. <br><br>  <strong>Ceph ist ein softwaredefiniertes Repository mit vielen Softwarekomponenten.</strong> <br><img src="https://habrastorage.org/webt/dw/ow/hm/dwowhmqvjfugd0u-ljhhz3fy2ji.png"><br><br>  Im Diagramm: <br><br><ul><li>  Die obere Ebene ist das interne Clusternetzwerk, Ã¼ber das der Cluster selbst kommuniziert. </li><li>  Die untere Ebene - eigentlich Ceph - ist eine Reihe von internen Ceph-DÃ¤monen (MON, MDS und OSD), die Daten speichern. </li></ul><br>  In der Regel werden alle Daten repliziert. Im Diagramm habe ich absichtlich drei Gruppen mit jeweils drei OSDs ausgewÃ¤hlt, und jede dieser Gruppen enthÃ¤lt normalerweise eine Datenreplik.  Infolgedessen werden Daten in drei Kopien gespeichert. <br><br>  Ein Ã¼bergeordnetes Clusternetzwerk ist das Netzwerk, Ã¼ber das Ceph-Clients auf Daten zugreifen.  Ãœber sie kommunizieren Clients mit dem Monitor, mit MDS (wer benÃ¶tigt es) und mit OSD.  Jeder Client arbeitet mit jedem OSD und mit jedem Monitor unabhÃ¤ngig.  Daher weist das <strong>System keinen einzigen Fehlerpunkt auf</strong> , was sehr erfreulich ist. <br><br><h2>  Kunden <br></h2><br>  â— S3-Kunden <br><br>  S3 ist eine API fÃ¼r HTTP.  S3-Clients arbeiten Ã¼ber HTTP und stellen eine Verbindung zu RGW-Komponenten (Ceph Rados Gateway) her.  Sie kommunizieren fast immer mit einer Komponente Ã¼ber ein dediziertes Netzwerk.  Dieses Netzwerk (ich habe es S3-Netzwerk genannt) verwendet nur HTTP, Ausnahmen sind selten. <br><br>  â— Hypervisor mit virtuellen Maschinen <br><br>  Diese Kundengruppe wird hÃ¤ufig verwendet.  Sie arbeiten mit Monitoren und mit OSD, von denen sie allgemeine Informationen Ã¼ber den Clusterstatus und die Datenverteilung erhalten.  FÃ¼r Daten gehen diese Clients Ã¼ber das Ã¶ffentliche Cluster-Netzwerk direkt zu OSD-Daemons. <br><br>  â— RBD-Clients <br><br>  Es gibt auch physische BR-Metall-Hosts, bei denen es sich normalerweise um Linux handelt.  Sie sind RBD-Clients und erhalten Zugriff auf Images, die in einem Ceph-Cluster gespeichert sind (Images der virtuellen Maschine). <br><br>  â— CephFS-Clients <br><br>  Die vierte Gruppe von Clients, die noch nicht viele haben, aber von wachsendem Interesse sind, sind CephFS-Cluster-Dateisystem-Clients.  Das CephFS-Clustersystem kann gleichzeitig von vielen Knoten bereitgestellt werden, und alle Knoten erhalten Zugriff auf dieselben Daten, wobei sie mit jedem OSD arbeiten.  Das heiÃŸt, es gibt keine Gateways als solche (Samba, NFS und andere).  Das Problem ist, dass ein solcher Client nur Linux und eine ziemlich moderne Version sein kann. <br><img src="https://habrastorage.org/webt/fw/nm/xc/fwnmxcaiig0yy6tkofrljqri3ck.png"><br><br>  Unser Unternehmen arbeitet auf dem Unternehmensmarkt, und dort wird der Ball von ESXi, HyperV und anderen regiert.  Dementsprechend muss der Ceph-Cluster, der irgendwie im Unternehmenssektor verwendet wird, die entsprechenden Techniken unterstÃ¼tzen.  Dies war uns bei Ceph nicht genug, daher mussten wir den Ceph-Cluster mit unseren Komponenten verfeinern und erweitern und tatsÃ¤chlich etwas mehr als Ceph aufbauen, unsere eigene Plattform zum Speichern von Daten. <br><br>  DarÃ¼ber hinaus arbeiten Kunden im Unternehmenssektor nicht unter Linux, aber die meisten von ihnen, Windows, gelegentlich Mac OS, kÃ¶nnen nicht selbst zum Ceph-Cluster wechseln.  Sie mÃ¼ssen durch eine Art Gateway laufen, die in diesem Fall zu EngpÃ¤ssen werden. <br><br>  Wir mussten alle diese Komponenten hinzufÃ¼gen und erhielten einen etwas breiteren Cluster. <br><img src="https://habrastorage.org/webt/p2/tg/j2/p2tgj2qtpzrzsnst5bophkclywi.png"><br><br>  Wir haben zwei zentrale Komponenten: die <strong>SCSI-Gateways-Gruppe</strong> , die Ã¼ber FibreChannel oder iSCSI den Zugriff auf Daten in einem Ceph-Cluster ermÃ¶glicht.  Diese Komponenten werden verwendet, um HyperV und ESXi mit einem Ceph-Cluster zu verbinden.  PROXMOX-Kunden arbeiten immer noch auf ihre eigene Art und Weise - Ã¼ber RBD. <br><br>  Wir lassen Dateiclients nicht direkt in das Clusternetzwerk, da ihnen mehrere fehlertolerante Gateways zugewiesen sind.  Jedes Gateway bietet Zugriff auf das Dateiclustersystem Ã¼ber NFS, AFP oder SMB.  Dementsprechend erhÃ¤lt fast jeder Client, sei es Linux, FreeBSD oder nicht nur ein Client, Server (OS X, Windows), Zugriff auf CephFS. <br><br>  Um all dies zu bewÃ¤ltigen, mussten wir tatsÃ¤chlich unser eigenes Ceph-Orchester und alle unsere Komponenten entwickeln, die dort zahlreich sind.  Aber jetzt darÃ¼ber zu sprechen macht keinen Sinn, da dies unsere Entwicklung ist.  Die meisten werden sich wahrscheinlich fÃ¼r den "nackten" Ceph selbst interessieren. <br><br>  Ceph wird hÃ¤ufig verwendet, und gelegentlich treten Fehler auf.  Sicher kennt jeder, der mit Ceph arbeitet, die Legende Ã¼ber CloudMouse.  Dies ist eine schreckliche urbane Legende, aber dort ist nicht alles so schlimm, wie es scheint.  Es gibt ein neues MÃ¤rchen Ã¼ber Rosreestr.  Ceph drehte sich Ã¼berall und Ã¼berall versagte es.  Irgendwo endete es tÃ¶dlich, irgendwo gelang es, die Konsequenzen schnell zu beseitigen. <br><br>  Leider ist es fÃ¼r uns nicht Ã¼blich, negative Erfahrungen auszutauschen, jeder versucht, die relevanten Informationen zu verbergen.  AuslÃ¤ndische Unternehmen sind etwas offener, insbesondere DigitalOcean (ein bekannter Anbieter, der virtuelle Maschinen vertreibt) erlitt fast einen Tag lang einen Ceph-Ausfall. Es war der 1. April - ein wunderbarer Tag!  Sie haben einige der Berichte verÃ¶ffentlicht, ein kurzes Protokoll unten. <br><img src="https://habrastorage.org/webt/qo/sb/ds/qosbdsczlkvzh-zqsfvid86er5u.png"><br><br>  Die Probleme begannen um 7 Uhr morgens, um 11 Uhr verstanden sie, was geschah, und begannen, den Fehler zu beseitigen.  Zu diesem Zweck haben sie zwei Befehle zugewiesen: Einer lief aus irgendeinem Grund um die Server herum und installierte dort Speicher, und der zweite startete aus irgendeinem Grund manuell einen Server nach dem anderen und Ã¼berwachte sorgfÃ¤ltig alle Server.  Warum?  Wir sind alle daran gewÃ¶hnt, alles mit einem Klick einzuschalten. <br><br>  <em>Was passiert grundsÃ¤tzlich in einem verteilten System, wenn es effektiv aufgebaut ist und fast an der Grenze seiner FÃ¤higkeiten arbeitet?</em> <br><br>  Um diese Frage zu beantworten, mÃ¼ssen wir uns ansehen, wie der Ceph-Cluster funktioniert und wie der Fehler auftritt. <br><img src="https://habrastorage.org/webt/ln/ks/rd/lnksrda1mb-lmfbymyacym1f8aw.png"><br><br><h2>  Ceph-Fehlerszenario <br></h2><br>  Zuerst funktioniert der Cluster gut, alles lÃ¤uft gut.  Dann passiert etwas, wonach die OSD-DÃ¤monen, in denen die Daten gespeichert sind, den Kontakt zu den zentralen Komponenten des Clusters (Monitore) verlieren.  Zu diesem Zeitpunkt tritt eine ZeitÃ¼berschreitung auf und der gesamte Cluster erhÃ¤lt einen Einsatz.  Der Cluster bleibt eine Weile stehen, bis er erkennt, dass etwas mit ihm nicht stimmt, und korrigiert danach sein internes Wissen.  Danach wird der Kundenservice bis zu einem gewissen Grad wiederhergestellt, und der Cluster arbeitet wieder in einem herabgesetzten Modus.  Und das Lustige ist, dass es schneller funktioniert als im normalen Modus - das ist eine erstaunliche Tatsache. <br><br>  Dann beseitigen wir den Fehler.  Angenommen, wir haben den Strom verloren, das Rack wurde komplett abgeschnitten.  Elektriker kamen angerannt, sie alle restauriert, sie versorgten die Server mit Strom, die Server wurden eingeschaltet und dann <strong>beginnt der SpaÃŸ</strong> . <br><br><blockquote>  Jeder ist daran gewÃ¶hnt, dass bei einem Serverausfall alles schlecht wird und beim Einschalten des Servers alles gut wird.  Hier ist alles vÃ¶llig falsch. <br></blockquote><br>  Der Cluster stoppt praktisch, fÃ¼hrt die primÃ¤re Synchronisation durch und beginnt dann eine reibungslose, langsame Wiederherstellung, wobei er allmÃ¤hlich in den normalen Modus zurÃ¼ckkehrt. <br><img src="https://habrastorage.org/webt/ml/r_/i3/mlr_i3llw-lsdaybp4vbedxeuhi.png"><br><br>  Oben sehen Sie eine grafische Darstellung der Ceph-Clusterleistung, wenn sich ein Fehler entwickelt.  Bitte beachten Sie, dass hier genau die Intervalle, Ã¼ber die wir gesprochen haben, sehr deutlich nachvollzogen werden: <br><br><ul><li>  Normalbetrieb bis ca. 70 Sekunden; </li><li>  Ausfall fÃ¼r eine Minute bis ca. 130 Sekunden; </li><li>  Ein Plateau, das merklich hÃ¶her als der normale Betrieb ist, ist die Arbeit von degradierten Clustern; </li><li>  Dann schalten wir den fehlenden Knoten ein - dies ist ein Trainingscluster, es gibt nur 3 Server und 15 SSDs.  Wir starten den Server irgendwo um 260 Sekunden. </li><li>  Der Server wurde eingeschaltet und trat in den Cluster ein - IOPS'y fiel. </li></ul><br>  Versuchen wir herauszufinden, was dort wirklich passiert ist.  Das erste, was uns interessiert, ist ein Eintauchen ganz am Anfang des Diagramms. <br><br><h3>  OSD-Fehler <br></h3><br>  Stellen Sie sich ein Beispiel fÃ¼r einen Cluster mit drei Racks mit jeweils mehreren Knoten vor.  Wenn das linke Rack ausfÃ¤llt, pingen sich alle OSD-Daemons (keine Hosts!) In einem bestimmten Intervall mit Ceph-Nachrichten.  Wenn mehrere Nachrichten verloren gehen, wird eine Nachricht an den Monitor gesendet: "Ich, OSD so und so, kann OSD so und so nicht erreichen." <br><img src="https://habrastorage.org/webt/zh/1s/ge/zh1sge1ljlclxjmgfygxpyyyc8i.png"><br><br>  In diesem Fall werden Nachrichten normalerweise nach Hosts gruppiert. Wenn also zwei Nachrichten von verschiedenen OSDs auf demselben Host eintreffen, werden sie zu einer Nachricht zusammengefasst.  Wenn OSD 11 und OSD 12 melden, dass sie OSD 1 nicht erreichen kÃ¶nnen, wird dies als Host 11 interpretiert, der Ã¼ber OSD 1 beschwert ist. Wenn OSD 21 und OSD 22 gemeldet wurden, wird dies als Host 21 interpretiert, der mit OSD 1 unzufrieden ist Danach berÃ¼cksichtigt der Monitor, dass sich OSD 1 im Status "Down" befindet, und benachrichtigt alle Mitglieder des Clusters (durch Ã„ndern der OSD-Zuordnung). Die Arbeit wird im herabgesetzten Modus fortgesetzt. <br><img src="https://habrastorage.org/webt/uu/-c/1w/uu-c1wnwflbqk6ueyumhohtlvjy.png"><br><br>  Hier ist also unser Cluster und das ausgefallene Rack (Host 5 und Host 6).  Wir schalten Host 5 und Host 6 ein, als die Stromversorgung erschien, und ... <br><br><h3>  Cephs inneres Verhalten <br></h3><br>  Und jetzt ist der interessanteste Teil, dass wir mit der <strong>anfÃ¤nglichen Datensynchronisation beginnen</strong> .  Da es viele Replikate gibt, mÃ¼ssen diese synchron sein und dieselbe Version haben.  Beim Starten des OSD-Starts: <br><br><ul><li>  OSD liest die verfÃ¼gbaren Versionen und den verfÃ¼gbaren Verlauf (pg_log - um die aktuellen Versionen von Objekten zu ermitteln). </li><li>  Danach wird festgelegt, auf welchem â€‹â€‹OSD die neuesten Versionen von herabgesetzten Objekten (missing_loc) aktiviert sind und welche sich dahinter befinden. </li><li>  Wenn die RÃ¼ckwÃ¤rtsversionen gespeichert sind, ist eine Synchronisierung erforderlich, und neue Versionen kÃ¶nnen als Referenz zum Lesen und Schreiben von Daten verwendet werden. </li></ul><br>  Es wird eine Geschichte verwendet, die von allen OSDs gesammelt wird, und diese Geschichte kann ziemlich viel sein;  Der tatsÃ¤chliche Standort der Gruppe von Objekten im Cluster, in dem sich die entsprechenden Versionen befinden, wird bestimmt.  Wie viele Objekte sich im Cluster befinden, wie viele DatensÃ¤tze erhalten werden, wenn der Cluster lange Zeit im herabgesetzten Modus gestanden hat, ist die Geschichte lang. <br><br>  <strong>Zum Vergleich: Die</strong> typische GrÃ¶ÃŸe eines Objekts bei der Arbeit mit einem RBD-Bild betrÃ¤gt 4 MB.  Wenn wir in LÃ¶schcode arbeiten - 1 MB.  Wenn wir eine 10-TB-Festplatte haben, erhalten wir eine Million Megabyte-Objekte auf der Festplatte.  Wenn wir 10 Festplatten auf dem Server haben, gibt es bereits 10 Millionen Objekte. Wenn 32 Festplatten vorhanden sind (wir bauen einen effizienten Cluster auf, wir haben eine enge Zuordnung), mÃ¼ssen 32 Millionen Objekte im Speicher gehalten werden.  DarÃ¼ber hinaus werden Informationen zu jedem Objekt in mehreren Kopien gespeichert, da jede Kopie angibt, dass sie an dieser Stelle in dieser Version und in dieser - in dieser - liegt. <br><br>  Es stellt sich heraus, dass sich eine groÃŸe Datenmenge im RAM befindet: <br><br><ul><li>  Je mehr Objekte vorhanden sind, desto grÃ¶ÃŸer ist die Historie von missing_loc. </li><li>  je mehr PG - desto mehr pg_log und OSD-Map; </li></ul><br>  AuÃŸerdem: <br><br><ul><li>  je grÃ¶ÃŸer die FestplattengrÃ¶ÃŸe ist; </li><li>  je hÃ¶her die Dichte (die Anzahl der Festplatten in jedem Server); </li><li>  Je hÃ¶her die Belastung des Clusters und desto schneller Ihr Cluster. </li><li>  je lÃ¤nger das OSD inaktiv ist (im Offline-Status); </li></ul><br>  Mit anderen Worten, je <strong>steiler der von uns erstellte Cluster ist und je lÃ¤nger der Teil des Clusters nicht reagiert, desto mehr RAM wird beim Start benÃ¶tigt</strong> . <br><br><h2>  Extreme Optimierungen sind die Wurzel allen Ãœbels <br></h2><br><blockquote>  <em>"... und der schwarze OOM kommt nachts zu den bÃ¶sen Jungs und MÃ¤dchen und tÃ¶tet alle Prozesse links und rechts ab."</em> <br><br>  Stadt Sysadmin Legende <br></blockquote><br>  RAM benÃ¶tigt also viel, der Speicherverbrauch steigt (wir haben sofort mit einem Drittel des Clusters begonnen), und das System kann theoretisch in SWAP integriert werden, wenn Sie es natÃ¼rlich erstellt haben.  Ich denke, es gibt viele Leute, die SWAP fÃ¼r schlecht halten und es nicht schaffen: â€Warum?  Wir haben viel GedÃ¤chtnis! â€œ  Dies ist jedoch der falsche Ansatz. <br><br>  Wenn die SWAP-Datei nicht im Voraus erstellt wurde, da entschieden wurde, dass Linux effizienter arbeitet, wird es frÃ¼her oder spÃ¤ter zu einem Speicherkiller (OOM-Killer) kommen. Und nicht zu der Tatsache, dass derjenige getÃ¶tet wird, der den gesamten Speicher verschlungen hat, nicht derjenige, der zuerst Pech hatte.  Wir wissen, was ein optimistischer Ort ist - wir bitten um eine Erinnerung, sie versprechen es uns, wir sagen: "Jetzt gib uns eine", als Antwort: "Aber nein!"  - und aus dem GedÃ¤chtnis Killer. <br><br>  Dies ist ein regulÃ¤rer Linux-Job, sofern er nicht im Bereich des virtuellen Speichers konfiguriert ist. <br><br>  Der Prozess wird aus dem GedÃ¤chtnis Killer und fÃ¤llt schnell und rÃ¼cksichtslos aus.  DarÃ¼ber hinaus wissen keine anderen Prozesse, die er starb, nicht.  Er hatte keine Zeit, irgendjemanden Ã¼ber irgendetwas zu informieren, sie kÃ¼ndigten ihn einfach. <br><br>  Dann wird der Prozess natÃ¼rlich neu gestartet - wir haben systemd, es startet bei Bedarf auch OSDs, die gefallen sind.  Gefallene OSDs beginnen und ... eine Kettenreaktion beginnt. <br><img src="https://habrastorage.org/webt/9p/s8/4z/9ps84zkjtmuamxyllkcgffsgxkq.png"><br><br>  In unserem Fall haben wir OSD 8 und OSD 9 gestartet, sie haben angefangen, alles zu zerstÃ¶ren, aber kein GlÃ¼ck, OSD 0 und OSD 5. Ein Killer ohne Speicher flog zu ihnen und beendete sie.  Sie starteten neu - sie lasen ihre Daten, begannen den Rest zu synchronisieren und zu zerstÃ¶ren.  Drei weitere PechvÃ¶gel (OSD 9, OSD 4 und OSD 7).  Diese drei starteten neu, Ã¼bten Druck auf den gesamten Cluster aus, die nÃ¤chste Packung hatte Pech. <br><br>  <strong>Der Cluster beginnt buchstÃ¤blich vor unseren Augen auseinanderzufallen</strong> .  Der Abbau erfolgt sehr schnell, und dieses "sehr schnelle" wird normalerweise in Minuten ausgedrÃ¼ckt, maximal zehn Minuten.  Wenn Sie 30 Knoten (10 Knoten pro Rack) haben und das Rack aufgrund eines Stromausfalls herunterfahren, liegt nach 6 Minuten die HÃ¤lfte des Clusters. <br><br>  Wir bekommen also so etwas wie das Folgende. <br><img src="https://habrastorage.org/webt/1b/hq/bu/1bhqburpjt74vwnpbgqn5ehdhh0.png"><br><br>  Auf fast jedem Server ist ein OSD ausgefallen.  Und wenn dies auf jedem Server der Fall ist, dh in jeder FehlerdomÃ¤ne, <strong>die</strong> wir fÃ¼r das ausgefallene OSD haben, sind die <strong>meisten unserer Daten nicht zugÃ¤nglich</strong> .  Jede Anfrage ist blockiert - zum Schreiben, zum Lesen - es macht keinen Unterschied.  Das ist alles!  Wir sind aufgestanden. <br><br>  Was tun in einer solchen Situation?  Genauer gesagt, <strong>was musste getan werden</strong> ? <br><br>  <strong>Antwort:</strong> Starten Sie den Cluster nicht sofort, dh das gesamte Rack, sondern heben Sie vorsichtig jeweils einen DÃ¤mon auf. <br><br>  Das wussten wir aber nicht.  Wir haben sofort angefangen und bekommen, was wir haben.  In diesem Fall haben wir einen der vier Daemons (8, 9, 10, 11) gestartet. Der Speicherverbrauch wird um ca. 20% steigen.  In der Regel stehen wir vor einem solchen Sprung.  Dann beginnt der Speicherverbrauch zu sinken, da einige der Strukturen, die zum Speichern von Informationen darÃ¼ber verwendet wurden, wie sich der Cluster verschlechtert hat, verlassen werden.  Das heiÃŸt, ein Teil der Platzierungsgruppen ist in seinen normalen Zustand zurÃ¼ckgekehrt, und alles, was zur Aufrechterhaltung des verschlechterten Zustands erforderlich ist, wird freigegeben - <strong>theoretisch wird es freigegeben</strong> . <br><br>  Sehen wir uns ein Beispiel an.  Der C-Code links und rechts ist fast identisch, der Unterschied besteht nur in Konstanten. <br><img src="https://habrastorage.org/webt/sy/1j/u0/sy1ju0rfqjg507jxvk_4wax9_o4.png"><br><br>  Diese beiden Beispiele fordern vom System eine unterschiedliche Speichermenge an: <br><br><ul><li>  links - 2048 StÃ¼ck Ã  1 MB; </li><li>  rechts - 2097152 StÃ¼ck von 1 KByte. </li></ul><br>  Dann warten beide Beispiele darauf, dass wir sie oben fotografieren.  Und nach dem DrÃ¼cken der EINGABETASTE wird Speicher freigegeben - alles auÃŸer dem letzten StÃ¼ck.  Das ist sehr wichtig - das letzte StÃ¼ck bleibt.  Und wieder warten sie darauf, dass wir sie fotografieren. <br><br>  Unten ist, was tatsÃ¤chlich passiert ist. <br><img src="https://habrastorage.org/webt/zx/ah/ug/zxahugrdasantcktho7dbu-tnes.png"><br><br><ul><li>  Zuerst haben beide Prozesse gestartet und den Speicher aufgefressen.  Klingt nach der Wahrheit - 2 GB RSS. </li><li>  DrÃ¼cken Sie ENTER und lassen Sie sich Ã¼berraschen.  Das erste Programm, das in groÃŸen StÃ¼cken auffiel, gab Speicher zurÃ¼ck.  Das zweite Programm kehrte jedoch nicht zurÃ¼ck. </li></ul><br>  Die Antwort darauf liegt im Linux-Malloc. <br><br>  Wenn wir Speicher in groÃŸen BlÃ¶cken anfordern, wird er unter Verwendung des anonymen mmap-Mechanismus ausgegeben, der dem Adressraum des Prozessors zugewiesen wird, von wo aus der Speicher zu uns geschnitten wird.  Wenn wir free () ausfÃ¼hren, wird Speicher freigegeben und Seiten werden in den Seitencache (System) zurÃ¼ckgegeben. <br><br>  Wenn wir Speicher in kleinen StÃ¼cken zuweisen, machen wir sbrk ().  sbrk () verschiebt den Zeiger auf das Ende des Heaps. Theoretisch kann das verschobene Ende zurÃ¼ckgegeben werden, indem Speicherseiten an das System zurÃ¼ckgegeben werden, wenn kein Speicher verwendet wird. <br><br>  Schauen Sie sich nun die Abbildung an.  Wir hatten viele Aufzeichnungen in der Geschichte des Standorts degradierter Objekte, und dann kam die Benutzersitzung - ein langlebiges Objekt.  Wir haben synchronisiert und alle zusÃ¤tzlichen Strukturen sind verschwunden, aber das langlebige Objekt ist geblieben, und wir kÃ¶nnen sbrk () nicht zurÃ¼ckbewegen. <br><img src="https://habrastorage.org/webt/06/wf/eg/06wfegwyvu0ibae8xjlwizrwteo.png"><br><br>  Wir haben immer noch viel ungenutzten Speicherplatz, der mit SWAP frei werden kÃ¶nnte.  Aber wir sind schlau - wir haben SWAP deaktiviert. <br><br>  NatÃ¼rlich wird dann ein Teil des Speichers vom Anfang des Heaps verwendet, aber dies ist nur ein Teil, und ein sehr bedeutender Rest wird belegt bleiben. <br><br>  Was tun in einer solchen Situation?  Die Antwort ist unten. <br><br><h3>  Kontrollierter Start <br></h3><br><ul><li>  Wir starten einen OSD-Daemon. </li><li>  Wir warten, wÃ¤hrend es synchronisiert ist, wir Ã¼berprÃ¼fen die Speicherbudgets. </li><li>  Wenn wir verstehen, dass wir den Start des nÃ¤chsten DÃ¤mons Ã¼berleben werden, starten wir den nÃ¤chsten. </li><li>  Wenn nicht, starten Sie schnell den Daemon neu, der den meisten Speicherplatz beansprucht hat.  Er konnte fÃ¼r kurze Zeit ausfallen, er hat nicht viel Geschichte, fehlende Locs und andere Dinge, also wird er weniger Speicher essen, das Speicherbudget wird sich leicht erhÃ¶hen. </li><li>  Wir laufen um den Cluster herum, kontrollieren ihn und erhÃ¶hen schrittweise alles. </li><li>  Wir prÃ¼fen, ob es mÃ¶glich ist, mit dem nÃ¤chsten OSD fortzufahren. </li></ul><br>  DigitalOcean hat dies tatsÃ¤chlich erreicht: <br>  <em>"Unser Datacenter-Team fÃ¼hrt Speichererweiterungen durch, wÃ¤hrend ein anderes Team langsam weiterhin Knoten aufruft, wÃ¤hrend das Speicherbudget jedes Hosts manuell verwaltet wird."</em> <br><img src="https://habrastorage.org/webt/nr/yg/a1/nryga17av_ez5yj0mt3lm5grkk0.png"><br><br>  Kehren wir zu unserer Konfiguration und aktuellen Situation zurÃ¼ck.  Jetzt haben wir einen zusammengebrochenen Cluster nach einer Kettenreaktion von Out-of-Memory-Killer.  Wir verbieten den automatischen Neustart von OSD in der roten DomÃ¤ne und starten nacheinander Knoten aus den blauen DomÃ¤nen.  Weil <strong>unsere erste Aufgabe immer darin besteht, den Dienst wiederherzustellen</strong> , ohne zu verstehen, warum dies passiert ist.  Wir werden spÃ¤ter verstehen, wenn wir den Dienst wiederherstellen.  Im Betrieb ist dies immer der Fall. <br><br>  Wir bringen den Cluster in den Zielzustand, um den Dienst wiederherzustellen, und beginnen dann, ein OSD nach unserer Methodik nach dem anderen auszufÃ¼hren.  Wir schauen uns den ersten an, starten Sie bei Bedarf die anderen neu, um das Speicherbudget anzupassen, den nÃ¤chsten - 9, 10, 11 - und der Cluster scheint synchronisiert und bereit zu sein, mit der Wartung zu beginnen. <br><br>  Das Problem ist, wie die <strong>Schreibwartung in Ceph durchgefÃ¼hrt wird</strong> . <br><img src="https://habrastorage.org/webt/hl/rp/ek/hlrpekm0rvjgrjwl11zgdklwecc.png"><br><br>  Wir haben 3 Replikate: ein Master-OSD und zwei Slaves dafÃ¼r.  Wir werden klarstellen, dass der Master / Slave in jeder Platzierungsgruppe einen eigenen hat, aber jeder einen Master und zwei Slaves hat. <br><br>  Die Schreib- oder Leseoperation fÃ¤llt auf den Master.  Wenn der Master beim Lesen die richtige Version hat, gibt er sie dem Kunden.  Die Aufnahme ist etwas komplizierter, die Aufnahme muss auf allen Replikaten wiederholt werden.  Wenn der Client 64 KB in OSD 0 schreibt, gehen dementsprechend die gleichen 64 KB in unserem Beispiel an OSD 5 und OSD 8. <br><br>  Tatsache ist jedoch, dass unser OSD 8 stark beeintrÃ¤chtigt ist, da wir viele Prozesse neu gestartet haben. <br><img src="https://habrastorage.org/webt/es/_z/fr/es_zfrsvdaq8a7f_rgn7hcakpi4.png"><br><br>  Da in Ceph jede Ã„nderung ein Ãœbergang von Version zu Version ist, haben wir unter OSD 0 und OSD 5 eine neue Version, unter OSD 8 - die alte.  ,   ,    ( 64 )    OSD 8   â€”   4  ( ).     4   OSD 0,   OSD 8,  ,    .       ,      64 . <br><br>    â€”  . <br><img src="https://habrastorage.org/webt/ch/uc/l_/chucl_b0vhoi-jvuhl3xokm26qg.png"><br><br>   : <br><br><ul><li>    4   1 ,  1000 /  1 . </li><li>   4  ( )  22 ,  45 /. </li></ul><br> ,      ,       ,        ,         . <br><br>      â€”     . <br><img src="https://habrastorage.org/webt/it/0p/34/it0p34kbqfs3u9hvyhmextflvqc.png"><br><br>    4   22 ,  22 ,   1    4   .   45          SSD,       1  â€” <strong>   45 </strong> . <br><br>       ,    . <br><br><h2>    <br></h2><br><br><ul><li>   <strong> </strong> ,    â€” (45+1) / 2 = <strong>23 .</strong> </li><li>   <strong>75% </strong> ,  (45 * 3 + 1) / 4 = <strong>34 </strong> . </li><li>  90% â€”(45 * 9 + 1) / 10 = 41  â€”  40  ,   . </li></ul><br>     Ceph,      .                 ,     ,    ,     . <br><br>      Ceph       . <br><img src="https://habrastorage.org/webt/ng/jj/od/ngjjodzmfd4n6kes71g4n6pg7os.png"><br><br><ol><li>     â€”   :  , ,  ,  ,    . <br></li><li>  â€” latency.   latency  ,   .      100%    (    ,          ). Latency  60     ,       . <br></li></ol><br><img src="https://habrastorage.org/webt/z3/pb/ob/z3pbobkev0bfszscnwgprpop3xe.png"><br><br>       ,       .  10 ,   1 200 /,    300      ,    ,   .  10 SSD â€”   300   ,   â€” ,  - 300   . <br><br><blockquote>    ,     . <br></blockquote><br>  ,     .       900 / (  SSD).     2 500   128    ( , ESXi  HyperV     128 ).      degraded,   225   .     file store,   object store,         ( ),    110   ,     - . <br><br> SSD  110    â€” ! <br><br> <strong>   ?</strong> <br><br> <strong> 1:</strong>     â€” <b>   </b> . <br><img src="https://habrastorage.org/webt/ls/ib/rh/lsibrhcfnucjiox9f8gzxbk1cc8.png"><br><br>    :   ;   PG; <br>       . <br><br>    : <br><br><ul><li>    ,  45  â€”   . </li><li>     (     . ),   14 . </li><li>    ,  8  (  10% PG). </li></ul><br>   <strong>  ,  </strong> ,       , ,  ,     . <br><br> <strong> 2:</strong>   â€” <b>  </b> (order, objectsize)  . <br><br>     , , ,   4   2  1 .      ,     ,   .  : <br><br><ul><li>     ; </li><li>     (latency)     . </li></ul><br>     : <br><br><ul><li>    ; </li><li>     ; </li><li>   â€”        .     4 ,   . </li></ul><br>        (32  ) â€”      ! <br><br> <strong> 3:</strong>    â€”  <b> Ceph</b> . <br><br>     ,   -,  <strong> Ceph</strong> .                  ,      ,      .     . <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>     ,   â€” Latency.  â€”  ,  â€” . Latency      30% ,       ,      . <br><br>  Community     ,     preproduction .     ,     .      ,   . <br><br><h1>  Fazit <br></h1><br>      -  ,     .        ,   Ceph    - ,  ,    . <br><br> â— <strong>   -  </strong> . <br>     ,     .  ,  <strong>     </strong> .       .  ,         ,    production.  ,       ,     ,    DigitalOcean  ,   .   ,  ,    ,  . <br><br>   ,        ,        .    ,  : Â«    !  ?!Â»     ,  ,     .   ,      : ,   ,    down time. <br><br> â— <strong>    (OSD).</strong> <br>  ,       ,     â€”     , ,  -      ,   . <strong>     OSD â€”    â€”   </strong> .    ,     . <br><br> â— <strong>  .</strong> <br>        OSD       . <strong>   ,   </strong> .  ,     ,     ,   . <br><br> â— <strong>  RAM   OSD.</strong> <br><br> â— <strong>  SWAP.</strong> <br>   SWAP    Ceph' ,    Linux' .         . <br><br> â— <strong>    .</strong> <br>         100%,    10%. ,    ,      ,   . <br><br> â— <strong>        RBD      Rados Getway.</strong> <br>  ,         . <strong>   SWAP â€”    .</strong> ,    SWAP  â€”    , ,  ,    ,     . <br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Artikel ist eine Abschrift eines der besten Berichte von DevOpsConf Russia. </font><font style="vertical-align: inherit;">In KÃ¼rze werden wir das Video Ã¶ffnen und in einer Textversion verÃ¶ffentlichen, wie interessant Themen sind. </font><font style="vertical-align: inherit;">Abonnieren Sie hier auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Youtube</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> oder im </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Newsletter,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wenn Sie solche nÃ¼tzlichen Materialien nicht verpassen und Ã¼ber die Neuigkeiten von DevOps informiert werden mÃ¶chten.</font></font><br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de431536/">https://habr.com/ru/post/de431536/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de431526/index.html">Korrupter Einfluss: Wie die Stasi Ostdeutschland vor Videospielen verteidigte</a></li>
<li><a href="../de431528/index.html">Das mysteriÃ¶se mathematische Genie und der Schriftsteller fÃ¶rdern die LÃ¶sung des Permutationsproblems</a></li>
<li><a href="../de431530/index.html">Offene Lektion "Android Material Design: Update-Ãœbersicht"</a></li>
<li><a href="../de431532/index.html">Memristoren bestehend aus 2 nm dicken Teilen</a></li>
<li><a href="../de431534/index.html">ProblemidentitÃ¤ten unter Entwicklern</a></li>
<li><a href="../de431538/index.html">Fallrate & Waren und Mobio: schrittweise ErhÃ¶hung aller Indikatoren</a></li>
<li><a href="../de431540/index.html">Pakete und Paketmanager fÃ¼r k8s</a></li>
<li><a href="../de431542/index.html">Effektive Entwicklung und Pflege von Ansible-Rollen</a></li>
<li><a href="../de431544/index.html">Tragen Sie DevOps zu den Massen</a></li>
<li><a href="../de431546/index.html">Warum sagen wir OK?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>