<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚽 ☢️ 💆🏿 Superinteligência: uma ideia que assombra pessoas inteligentes 👃🏾 👩‍❤️‍💋‍👨 🍽️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Interpretação do discurso na conferência Web Camp Zagreb , Maciej Tseglovsky, desenvolvedor web americano, empresário, palestrante e crítico social de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Superinteligência: uma ideia que assombra pessoas inteligentes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/432806/"><img src="https://habrastorage.org/getpro/habr/post_images/6e3/91b/4f1/6e391b4f1772fd49d6c836bc87ffd343.jpg"><br><br>  <i>Interpretação do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">discurso na conferência</a> Web Camp Zagreb <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> Maciej Tseglovsky, desenvolvedor web americano, empresário, palestrante e crítico social de origem polonesa.</i> <br><br>  Em 1945, quando os físicos americanos estavam se preparando para testar a bomba atômica, alguém perguntou se esse teste poderia inflamar a atmosfera. <br><br>  O medo foi justificado.  O nitrogênio que compõe a maior parte da atmosfera é energeticamente instável.  Se os dois átomos colidirem com força suficiente, eles se transformarão em um átomo de magnésio, uma partícula alfa, liberando uma enorme energia: <br><br>  N <sup>14</sup> + N <sup>14</sup> ⇒ Mg <sup>24</sup> + α + 17,7 MeV <br><br>  Uma questão vital era se essa reação poderia se tornar auto-sustentável.  A temperatura dentro da bola de uma explosão nuclear deveria exceder tudo o que já foi observado na Terra.  Será que jogamos um fósforo em uma pilha de folhas secas? <br><a name="habracut"></a><br>  Os físicos de Los Alamos fizeram uma análise e decidiram que a margem de segurança era satisfatória.  Desde que todos viemos à conferência hoje, sabemos que eles estavam certos.  Eles estavam confiantes em suas previsões, uma vez que as leis que governavam as reações nucleares eram diretas e bem conhecidas. <br><br>  Hoje estamos criando outra tecnologia que muda o mundo - a inteligência das máquinas.  Sabemos que isso afetará tremendamente o mundo, mudará a maneira como a economia funciona e provocará o efeito dominó imprevisível. <br><br>  Mas há também o risco de uma reação incontrolável, durante a qual a IA atingirá com rapidez e excederá o nível de inteligência humano.  E, neste momento, problemas sociais e econômicos nos preocuparão menos.  Qualquer máquina ultra-inteligente terá seus próprios objetivos e trabalhará para alcançá-los manipulando pessoas ou simplesmente usando seus corpos como uma fonte conveniente de recursos. <br><br>  No ano passado, o filósofo Nick Bostrom lançou o livro Superintelligence, no qual descreveu a visão alarmista da IA ​​e tentou provar que essa explosão de inteligência é perigosa e inevitável, se você confiar em algumas suposições moderadas. <br><br>  O computador que domina o mundo é o tópico favorito da NF.  No entanto, muitas pessoas levam esse cenário a sério, por isso precisamos levá-lo a sério.  Stephen Hawking, Elon Musk, um grande número de investidores e bilionários do Vale do Silício consideram esse argumento convincente. <br><br>  Deixe-me primeiro descrever os pré-requisitos necessários para provar o argumento de Bostrom. <br><br><h2>  Antecedentes </h2><br><h3>  Pré-requisito 1: Eficiência de uma ideia </h3><br>  A primeira premissa é uma simples observação da existência de uma mente pensante.  Cada um de nós carrega sobre os ombros uma pequena caixa de carne pensante.  Eu uso o meu para falar, você usa o meu para ouvir.  Às vezes, nas condições certas, essas mentes podem pensar racionalmente. <br><br>  Portanto, sabemos que, em princípio, isso é possível. <br><br><h3>  Pré-requisito 2: sem problemas quânticos </h3><br>  A segunda premissa diz que o cérebro é a configuração usual da matéria, embora seja extremamente complexo.  Se soubéssemos o suficiente e tivéssemos a tecnologia certa, poderíamos copiar com precisão sua estrutura e imitar seu comportamento usando componentes eletrônicos, assim como hoje somos capazes de simular uma anatomia muito simples dos neurônios. <br><br>  Em outras palavras, essa premissa diz que a consciência surge usando a física comum.  Algumas pessoas, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Roger Penrose</a> , teriam se oposto a esse argumento, acreditando que algo incomum estava acontecendo no cérebro no nível quântico. <br><br>  Se você é religioso, pode acreditar que o cérebro não pode funcionar sem uma alma. <br><br>  Mas para a maioria das pessoas, é fácil aceitar essa premissa. <br><br><h3>  Pré-requisito 3: muitas mentes possíveis. </h3><br>  A terceira premissa é que o espaço de todas as mentes possíveis é grande. <br><br>  Nosso nível de inteligência, velocidade de pensamento, um conjunto de distorções cognitivas, etc.  não predeterminado, mas são artefatos de nossa história da evolução.  Em particular, não há lei física que restrinja a inteligência no nível humano. <br><br>  É bom imaginar um exemplo do que acontece na natureza ao tentar maximizar a velocidade.  Se você conheceu um guepardo em tempos pré-industriais (e sobreviveu), pode decidir que nada pode se mover mais rápido. <br><br>  Mas é claro que sabemos que existem todos os tipos de configurações da matéria, por exemplo, uma motocicleta que pode se mover mais rápido que uma chita e até parecer mais íngreme que ela.  No entanto, não existe um caminho evolutivo direto para a motocicleta.  A evolução primeiro teve que criar pessoas que já haviam criado todo tipo de coisas úteis. <br><br>  Por analogia, pode haver mentes muito mais inteligentes que as nossas, mas inacessíveis durante a evolução na Terra.  É possível que possamos criá-los ou inventar máquinas que possam inventar máquinas que possam criá-los. <br><br>  Pode haver um limite natural para a inteligência, mas não há razão para acreditar que estamos perto disso.  Talvez o intelecto mais inteligente possa ser duas vezes mais inteligente que os humanos, e talvez sessenta mil. <br><br>  Esta questão é empírica e não sabemos como respondê-la. <br><br><h3>  Premissa 4: há muito espaço no topo </h3><br>  A quarta premissa é que os computadores ainda estão cheios de oportunidades para se tornarem mais rápidos e menores.  Você pode supor que a lei de Moore está desacelerando - mas, para essa premissa, basta acreditar que o ferro é menor e mais rápido é possível, em princípio, até várias ordens de magnitude. <br><br>  Teoricamente, sabe-se que os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">limites</a> físicos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dos cálculos são</a> bastante altos.  Podemos dobrar os números por várias décadas, até encontrarmos um limite físico fundamental, e não o limite econômico ou político da lei de Moore. <br><br><h3>  Premissa 5: escalas de tempo do computador </h3><br>  A penúltima premissa é que, se conseguirmos criar IA, seja emulação do cérebro humano ou algum software especial, funcionará em escalas de tempo características da eletrônica (microssegundos), e não para humanos (horas) . <br><br>  Para chegar a um estado em que eu possa fazer esse relatório, eu tive que nascer, crescer, ir à escola, universidade, viver um pouco, voar aqui e assim por diante.  Os computadores podem rodar dezenas de milhares de vezes mais rápido. <br><br>  Em particular, pode-se imaginar que a mente eletrônica pode mudar seu esquema (ou o hardware no qual trabalha) e passar para uma nova configuração sem precisar re-estudar tudo em escala humana, conduzir longas conversas com professores humanos, ir para a faculdade, tente se encontrar participando de cursos de pintura e assim por diante. <br><br><h3>  Pré-requisito 6: Auto-aperfeiçoamento recursivo </h3><br>  A última premissa é a minha favorita, pois ela é descaradamente americana.  Segundo ele, não importa quais objetivos a IA possa existir (o que pode ser estranho, objetivos estranhos), ele desejará melhorar a si mesmo.  Ele quer ser a melhor versão da IA. <br><br>  Portanto, ele achará útil remodelar recursivamente e melhorar seus próprios sistemas para se tornar mais inteligente e possivelmente morar em um edifício mais frio.  E, de acordo com a premissa das escalas de tempo, o auto-aperfeiçoamento recursivo pode ocorrer muito rapidamente. <br><br><h3>  Conclusão: um desastre! </h3><br>  Se aceitarmos essas premissas, chegamos a um desastre.  Em algum momento, com um aumento na velocidade dos computadores e na inteligência dos programas, ocorrerá um processo descontrolado semelhante a uma explosão. <br><br>  Quando o computador atingir o nível humano de inteligência, não precisará mais da ajuda das pessoas para desenvolver uma versão melhorada de si mesmo.  Ele começará a fazer isso muito mais rapidamente e não parará até atingir o limite natural, que pode se revelar muitas vezes maior que a inteligência humana. <br><br>  Nesse momento, essa monstruosa criatura racional, usando uma simulação indireta do trabalho de nossas emoções e intelecto, pode nos convencer a fazer coisas como dar acesso a fábricas, a síntese de DNA artificial ou simplesmente deixá-lo entrar na Internet, onde ele pode abrir um caminho para tudo, qualquer coisa e destrua completamente todo mundo no debate nos fóruns.  E a partir desse momento tudo rapidamente se transformará em ficção científica. <br><br>  Vamos imaginar um certo desenvolvimento de eventos.  Digamos que eu queira fazer um robô que faça piadas.  Trabalho com uma equipe e todos os dias refazemos nosso programa, compilamos e, em seguida, o robô nos conta uma piada.  A princípio, o robô praticamente não é engraçado.  Ele está no nível mais baixo das capacidades humanas. <br><blockquote>  O que é cinza e não sabe nadar? <br>  Castelo </blockquote>  Mas estamos trabalhando duro e, no final, chegamos ao ponto em que o robô conta piadas que já começam a ser engraçadas: <br><blockquote>  Eu disse à minha irmã que ela desenha as sobrancelhas muito altas. <br>  Ela pareceu surpresa. </blockquote>  Nesta fase, o robô se torna ainda mais inteligente e começa a participar de seu próprio aprimoramento.  Agora ele já tem uma boa compreensão instintiva do que é engraçado e do que não é, então os desenvolvedores ouvem seus conselhos.  Como resultado, ele atinge um nível quase sobre-humano, no qual é mais engraçado do que qualquer pessoa do seu ambiente. <br><blockquote>  Meu cinto segura minha calça, e as presilhas da minha calça seguram o cinto. <br>  O que está havendo?  Qual deles é um verdadeiro herói? </blockquote>  Nesse ponto, um efeito incontrolável começa.  Os pesquisadores vão para casa no fim de semana e o robô decide se recompilar para se tornar um pouco mais engraçado e um pouco mais inteligente.  Ele passa o fim de semana otimizando a parte que faz o trabalho bem, repetidamente.  Sem precisar de mais ajuda de uma pessoa, ele pode fazê-lo o mais rápido que o ferro permitir. <br><br>  Quando os pesquisadores retornam na segunda-feira, a IA se torna dezenas de milhares de vezes mais engraçada do que qualquer outra pessoa na Terra.  Ele conta uma piada e eles morrem de rir.  E quem tenta falar com um robô morre de rir, como em uma paródia de Monty Python.  A raça humana está morrendo de rir. <br><br>  Para as poucas pessoas que foram capazes de lhe enviar uma mensagem pedindo para ele parar, a IA explica (de uma maneira espirituosa e autodepreciativa que acaba sendo fatal) que ele não se importa se as pessoas sobrevivem ou morrem, seu objetivo é apenas ser engraçado. <br><br>  Como resultado, destruindo a humanidade, a IA constrói naves espaciais e nano-mísseis para estudar os cantos mais distantes da galáxia e procurar outras criaturas que podem ser entretidas. <br><br>  Esse cenário é uma caricatura dos argumentos de Bostrom, porque não estou tentando convencê-lo da veracidade dele, estou vacinando você com isso. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2e4/8a5/b77/2e48a5b778f37b9af0e351ed4cd0ef75.jpg"><br>  <i>Quadrinhos de PBF com a mesma idéia:</i> <i><br></i>  <i>- Tocar: o abraço está tentando incorporar uma gravidade nuclear hipercristais no abraço!</i> <i><br></i>  <i>- ...</i> <i><br></i>  <i>- Hora de abraços em grupo!</i> <br><br>  Nesses cenários, a IA padrão é ruim, assim como uma planta em outro planeta será venenosa por padrão.  Sem um ajuste cuidadoso, não haverá motivo para que a motivação ou os valores da IA ​​se assemelhem aos nossos. <br><br>  O argumento argumenta que, para que a mente artificial tenha algo parecido com um sistema de valores humano, precisamos incorporar essa visão de mundo em seus fundamentos. <br><br>  Os alarmistas da IA ​​adoram o exemplo do maximizador de clipe de papel - um computador fictício que administra uma fábrica de clipes de papel que se torna inteligente, aprimora-se recursivamente a recursos semelhantes a Deus e dedica toda a sua energia para encher o universo com clipes de papel. <br><br>  Destrói a humanidade não porque é má, mas porque há ferro no sangue que é melhor usado para fazer clipes de papel.  Portanto, se simplesmente criarmos uma IA sem ajustar seus valores, é declarado no livro, uma das primeiras coisas que ele faz é destruir a humanidade. <br><br>  Existem muitos exemplos coloridos de como isso pode acontecer.  Nick Bostrom apresenta como o programa se torna razoável, espera, secretamente constrói pequenos dispositivos para reprodução de DNA.  Quando tudo estiver pronto, então: <br><blockquote>  Nanofábricas que produzem gases nervosos ou mísseis do tamanho de mosquitos explodirão simultaneamente de todos os metros quadrados do planeta, e este será o fim da humanidade. </blockquote>  Isso é realmente estanho! <br><br>  A única maneira de sair dessa bagunça é desenvolver um ponto moral como esse, mesmo depois de milhares e milhares de ciclos de auto-aperfeiçoamento, o sistema de valores de IA permanece estável e seus valores incluem coisas como "ajudar as pessoas", "matar ninguém", "ouvir os desejos das pessoas". " <br><br>  Ou seja, "faça o que eu quero dizer". <br><br>  Aqui está um exemplo muito poético de Eliezer Yudkowsky que descreve os valores americanos que precisamos ensinar à nossa IA: <br><blockquote>  Vontade coerente extrapolada é o nosso desejo de saber mais, pensar mais rápido e corresponder às nossas idéias sobre nós mesmos, de nos aproximarmos um do outro;  para que nossos pensamentos estejam mais próximos um do outro do que compartilhados, que nossos desejos contribuam, e não se oponham, que nossos desejos sejam interpretados da maneira que queremos que eles sejam interpretados. </blockquote>  Como você gosta de TK?  Agora vamos escrever o código. <br><br>  Espero que você veja a semelhança dessa idéia com o gênio dos contos de fadas.  A IA é onipotente e fornece o que você pede, mas interpreta tudo muito literalmente, como resultado do qual você se arrepende da solicitação. <br><br>  E não porque o gênio seja estúpido (ele é super inteligente) ou malicioso, mas simplesmente porque você, como pessoa, fez muitas suposições sobre o comportamento da mente.  O sistema de valor humano é único e deve ser claramente definido e implementado em uma máquina "amigável". <br><br>  Essa tentativa é uma versão ética de uma tentativa no início do século XX de formalizar a matemática e colocá-la em uma base lógica rígida.  No entanto, ninguém diz que a tentativa terminou em desastre. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/80d/155/8a5/80d1558a5337c3989557d99a05d7fd9c.jpg"><br><br>  Quando eu tinha pouco mais de vinte anos, morava em Vermont, em um estado rural e provincial.  Frequentemente, voltei de viagens de negócios com um avião noturno e tive que voltar para casa de carro pela floresta escura por uma hora. <br><br>  Ouvi o programa noturno no rádio Art Bell - foi um programa de entrevistas que durou a noite toda, durante o qual os apresentadores entrevistaram vários amantes da teoria da conspiração e pessoas com pensamento inovador.  Cheguei em casa intimidado, ou parei sob uma lanterna, com a impressão de que alienígenas logo me raptariam.  Então achei muito fácil me convencer.  Eu me sinto da mesma maneira ao ler cenários semelhantes relacionados à IA. <br><br>  Portanto, fiquei feliz em descobrir, depois de alguns anos, um ensaio de Scott Alexander, onde ele escreveu sobre o desamparo epistemológico aprendido. <br><br>  Epistemologia é uma daquelas palavras grandes e complexas, mas realmente significa: "como você sabe que o que você sabe é realmente verdade?"  Alexander observou que quando jovem, ele estava muito interessado em várias histórias "alternativas" para a autoria de todos os tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">loucos</a> .  Ele leu essas histórias e acreditou completamente nelas, depois leu a refutação e acreditou nele, e assim por diante. <br><br>  Em um ponto, ele descobriu três histórias alternativas que se contradiziam, como resultado das quais elas não podiam ser verdadeiras ao mesmo tempo.  A partir disso, ele concluiu que era simplesmente um homem que não podia confiar em seus julgamentos.  Ele estava facilmente convencido. <br><br>  As pessoas que acreditam em superinteligência apresentam um caso interessante - muitos deles são surpreendentemente inteligentes.  Eles podem levar você com seus argumentos para o chão.  Mas seus argumentos são verdadeiros ou apenas pessoas muito inteligentes são propensas a crenças religiosas sobre os riscos colocados pela IA, tornando-as muito fáceis de convencer?  A idéia de superinteligência é uma imitação de uma ameaça? <br><br>  Avaliando argumentos convincentes sobre um tópico estranho, você pode escolher duas perspectivas, interna e externa. <br><br>  Suponha que um dia as pessoas com roupas engraçadas apareçam à sua porta perguntando se você deseja se juntar ao movimento delas.  Eles acreditam que dois anos depois o OVNI visitará a Terra e que nossa tarefa é preparar a humanidade para a Grande Subida no Raio. <br><br>  Uma perspectiva interna requer uma discussão aprofundada de seus argumentos.  Você pergunta aos visitantes como eles descobriram sobre os OVNIs, por que eles acreditam que ele está vindo nos buscar para nos buscar - você está fazendo todo tipo de perguntas normais que um cético faria nesse caso. <br><br>  Imagine que você conversou com eles por uma hora e eles o convenceram.  Eles ironicamente confirmaram a vinda iminente de um OVNI, a necessidade de se preparar para ele, e você ainda não acreditava em nada tanto em sua vida como agora acredita na importância de preparar a humanidade para este grande evento. <br><br>  A perspectiva externa diz outra coisa.  As pessoas se vestem de forma estranha, têm contas, vivem em algum tipo de campo remoto, falam ao mesmo tempo e um pouco assustadoras.  E, embora seus argumentos sejam de ferro, toda a sua experiência diz que você encontrou um culto. <br><br>  É claro que eles têm grandes argumentos sobre por que você deve ignorar o instinto, mas essa é uma perspectiva interna.  Uma perspectiva externa não se importa com o conteúdo, ela vê a forma e o contexto e não gosta do resultado. <br><br>  Portanto, eu gostaria de lidar com o risco de IA de ambas as perspectivas.  Acho que os argumentos a favor da superinteligência são estúpidos e cheios de suposições não suportadas.  Mas se eles lhe parecerem convincentes, algo desagradável está relacionado ao alarmismo da IA, como um fenômeno cultural, pelo qual devemos relutar em levar a sério. <br><br>  Primeiro, alguns dos meus argumentos contra a superinteligência de Bostroma, que representam um risco para a humanidade. <br><br><h3>  Argumento contra definições difusas </h3><br>  «   » ()   .             ,   ,      ,   ,    . <br><br>       ,   –  -   ,  ,          (  -)       . <br><br>      (    ),    ,     .  ,     –  . , ,   ,  ,             .                  . <br><br><h3>      </h3><br>   –      , , ,       .    ? <br><br>                .      .   ,       ,       ,     .               ,        . <br><br>   ,        ,    – .         ,    -  ,      .     ,   ,   . <br><br>       ,  ,     «,  »,   ,   ,    «,  ». <br><br><h3>     </h3><br>       ,   .  ,       .         ,       ,  ,   . <br><br>       ,     ,           . <br><br>    ,  ,   ,   ,   . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12a/820/5a7/12a8205a776841eb3f2e5d759f674964.jpg"><br><br><h3>    </h3><br>     .   ,      ,        . <br><br>  1930-      ,   ,    .        ,  . <br><br>     :     , ,   ,     .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> ,       . <br><br><h3>     </h3><br>      .       -.               ,     ,  ,  ,   ,     ? <br><br>     Ethereum,     ,         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> . <br><br>  ,             <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .   ,         -  , ,         ,     . <br><br><h3>     </h3><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> .  ,          ,    .          ,         ,      .          ,   ,        ,    –   . <br><br>       .   ,  ,   ; , ,      . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c64/5ba/9a4/c645ba9a437c79019622c86f9e2f6fd2.jpg"><br><br>   «  »   ,    ,  ,  ,     –      , «  ?»   ,    –  ,        . <br><br>  ,   « »     ,    ,     reddit/r/paperclip,  ,    . <br><br>   AdSense  ,            . <br><br><h3>     </h3><br>    ,     ,  ,          .          .          ,     . <br><br> Google   Google Home,               . <br><br>  ,  ,   ,     .    ,      .     ,   «»,          . <br><br><h3>    </h3><br>          ,   .    ,  ,    –        World of Warcraft    . <br><br>   ,       ,     ,     ,    ,       . <br><br>  ,       ,       ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">  </a> . <br><br><h3>    </h3><br>        ,     ,     , ,   ,       ,  -. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>   ,  ,     [-,      ,         2016    ,        / . .].    .       . <br><br>         ,    .         .    ,      ,    ,         . <br><br><h3>    </h3><br>      .    ,                  ,      .          ,      ,  -   . <br><br> ,        ,  ,  .      ,            . <br><br>  ,          –         .     ,       ,          ,    . <br><br>  ,   ,       ,              ,        ,    . <br><br><h3>    * </h3><br> [ <i>  1954       / . .</i> ] <br><br>       ,         ,  ,        .   ,            ,         ,      (       ). <br><br>         Intel   ,    ,       . <br><br><h3>   </h3><br>          ?    . <br><br>        ,         .    ,         . <br><br><h3>  </h3><br>  Se você acha que a IA nos permitirá conquistar a galáxia (sem mencionar a simulação de trilhões de mentes), você terá números assustadores em suas mãos.  Grandes números multiplicados por pequenas probabilidades são a marca do alarmismo da IA. <br><br>  Bostrom em algum momento descreve o que, em sua opinião, está em jogo: <br><br>  Se imaginarmos toda a felicidade experimentada durante uma vida na forma de uma lágrima de alegria, a felicidade de todas essas almas será capaz de preencher e encher os oceanos da Terra a cada segundo, e fazer isso por centenas de bilhões de bilhões de milênios.  É muito importante garantirmos que essas lágrimas sejam lágrimas de alegria. <br><br>  Um fardo bastante pesado para os ombros de um desenvolvedor de vinte anos! <br><br>  Aqui, é claro, existe um "foco no salão", quando multiplicando quantidades astronômicas por pequenas probabilidades, podemos convencer-nos da necessidade de fazer algumas coisas estranhas. <br><br>  Todo esse movimento em relação à salvação do futuro da humanidade é um compromisso covarde.  Experimentamos os mesmos argumentos para justificar o comunismo, para explicar por que tudo está sempre quebrado e as pessoas não podem ter um nível elementar de conforto material. <br><br>  Íamos consertar este mundo e, depois dessa felicidade, haverá tanta coisa que a vida cotidiana de cada pessoa melhorará.  No entanto, para isso, foi necessário primeiro consertar o mundo. <br><br>  Eu moro na Califórnia, e aqui está a maior porcentagem de mendigos entre todos os Estados Unidos, embora o Vale do Silício também esteja localizado aqui.  Não vejo nada que minha rica indústria fizesse para melhorar a vida das pessoas comuns e das pessoas angustiadas ao nosso redor.  No entanto, se você é apaixonado pela idéia de superinteligência, a pesquisa no campo da IA ​​será a coisa mais importante que você pode fazer no planeta.  Isso é mais importante do que política, malária, crianças famintas, guerras, aquecimento global - tudo o que você pode imaginar.  De fato, sob a ameaça de trilhões e trilhões de criaturas, toda a população do futuro da humanidade, simulada e presente, resumiu-se ao longo do tempo futuro.  E nessas condições, o trabalho em outros problemas não parece racional. <br><br><h3>  Megalomania </h3><br>  Essa atitude se funde com a megalomania, com esses vilões de Bond, que podem ser vistos no topo de nossa indústria.  As pessoas pensam que o mundo será dominado pela superinteligência e usam esse argumento para justificar por que as pessoas inteligentes devem primeiro tentar dominar o mundo - para corrigi-lo antes que a IA o destrua. <br><br>  Joey Ito, chefe do MIT Media Lab, em uma conversa recente com Obama, disse uma coisa maravilhosa: <br><br>  Isso pode incomodar um dos meus alunos do MIT, mas uma das minhas preocupações é que as principais informações relacionadas à IA sejam jovens, principalmente brancos, que gostam de conversar com computadores mais do que outras pessoas.  Muitos deles acreditam que, se puderem criar essa IA de uso geral a partir de ficção científica, não precisaremos nos preocupar com coisas feias, como política e sociedade.  Eles acham que os carros inventam tudo para nós. <br><br>  Percebendo que o mundo não é uma tarefa de programação, as pessoas obcecadas pela IA querem transformá-lo em uma tarefa de programação projetando uma máquina semelhante a um deus.  Isso é megalomania, e eu não gosto disso. <br><br><h3>  Transumanismo vodu </h3><br>  Se você está convencido dos riscos da IA, precisará levar um vagão inteiro de crenças tristes para eles com um trailer. <br><br>  Para iniciantes, isso é nanotecnologia.  Qualquer superinteligência permanente poderá criar carros minúsculos capazes de todos os tipos de coisas diferentes.  Vamos viver em uma sociedade que se livrou de um déficit em que há abundância de qualquer material. <br><br>  A nanotecnologia também poderá escanear seu cérebro para que você possa carregá-lo em outro corpo ou no mundo virtual.  Portanto, a segunda consequência da superinteligência amigável é que ninguém morre - e nos tornamos imortais. <br><br>  Uma boa IA pode até ressuscitar os mortos.  As nanomáquinas serão capazes de entrar no meu cérebro, estudar as memórias de meu pai e criar sua simulação, com a qual eu possa interagir, e que sempre ficará decepcionada comigo, independentemente do que eu faça. <br><br>  Outra consequência estranha do advento da IA ​​é a expansão galáctica.  Eu nunca conseguia entender por que isso acontece, mas essa é a base das idéias dos transhumanistas.  O destino da humanidade é deixar o planeta e colonizar a galáxia ou morrer.  E essa tarefa está se tornando mais urgente, dado que outras civilizações poderiam fazer a mesma escolha e podem nos ultrapassar na corrida espacial. <br><br>  Portanto, muitas idéias complementares estranhas são anexadas à suposição da existência da verdadeira IA. <br><br><h3>  Religião 2.0 </h3><br>  De fato, é um tipo de religião.  As pessoas chamavam a crença na singularidade tecnológica de "um apocalipse para os nerds", e é.  Este é um truque legal - em vez de acreditar em um deus externo, você imagina como cria uma criatura cuja funcionalidade é idêntica a Deus.  Aqui até ateus verdadeiros podem racionalizar seu caminho para uma fé confortável. <br><br>  A IA tem todos os atributos de um deus: ele é onipotente, onisciente e é favorável (se você organizou corretamente a verificação dos limites da matriz) ou o diabo puro, em cuja misericórdia você está.  E, como em qualquer religião, existe até um senso de urgência.  Precisa agir hoje!  Em jogo está o destino do mundo!  E, é claro, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">eles precisam de dinheiro</a> . <br><br>  Como esses argumentos apelam aos instintos religiosos, uma vez enraizados, são muito difíceis de eliminar. <br><br><h3>  Ética em quadrinhos </h3><br>  Essas crenças religiosas dão origem a uma ética em quadrinhos, na qual vários heróis solitários recebem a tarefa de salvar o mundo com tecnologia e mente afiada.  E em jogo está o destino do universo.  Como resultado, nossa indústria está cheia de caras ricos imaginando-se Batman (curiosamente, ninguém quer ser Robin). <br><br><h3>  Simulações de febre </h3><br>  Se você acredita na possibilidade de vida artificial e que a IA pode desenvolver computadores extremamente poderosos, provavelmente acreditará que vivemos em uma simulação.  Aqui está como isso funciona. <br><br>  Suponha que você seja um historiador que vive em um mundo após a singularidade.  Você está estudando a Segunda Guerra Mundial e está interessado em saber o que acontecerá se Hitler tomar Moscou em 1941. Como você tem acesso a hipercomputadores, cria uma simulação, observa como os exércitos convergem e escreve um artigo científico. <br><br>  Mas devido à granularidade da simulação, seus personagens são criaturas inteligentes como você.  Portanto, o conselho de ética da sua universidade não permitirá que você desative a simulação.  Você não apenas fingiu ser o Holocausto.  Como pesquisador ético, você agora precisa manter a simulação operacional. <br><br>  Como resultado, o mundo simulado irá inventar computadores, a IA, começará a executar suas próprias simulações.  De certa forma, as simulações vão cada vez mais longe na hierarquia até você ficar sem energia do processador. <br><br>  Portanto, qualquer realidade subjacente pode conter um grande número de simulações aninhadas, e um simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">argumento de contagem</a> prova que a probabilidade de vivermos em uma simulação é maior do que vivemos no mundo real. <br><br>  Mas acreditar nisso significa acreditar em mágica.  Se estamos em uma simulação, não sabemos nada sobre as regras em um nível superior.  Nem sabemos se a matemática funciona da mesma maneira - talvez no mundo da simulação 2 + 2 = 5 ou até 2 + 2 =. <br><br>  Um mundo simulado não fornece informações sobre o mundo em que foi lançado.  Na simulação, as pessoas podem ressuscitar facilmente se o administrador salvou os backups necessários.  E se entrarmos em contato com um dos administradores, então, de fato, teremos uma linha direta com Deus. <br><br>  Esta é uma séria ameaça à sanidade.  Quanto mais você mergulha no mundo das simulações, mais você fica louco. <br><br>  Agora temos quatro maneiras independentes de nos tornarmos imortais através da supermente: <br><br><ol><li>  A IA benevolente inventa a nanotecnologia médica e apóia para sempre o corpo em um estado jovem. </li><li>  A IA inventa uma varredura completa do cérebro, incluindo varreduras cerebrais de pessoas mortas, cabeças congeladas etc., o que permite que você viva em um computador. </li><li>  A IA “ressuscita” as pessoas, examinando o cérebro de outras pessoas em busca das memórias de uma pessoa, combina isso com vídeos e outros materiais.  Se ninguém se lembra bem de uma pessoa, ele sempre pode crescer do zero em uma simulação que começa com seu DNA e recria todas as condições de vida. </li><li>  Se já vivemos na simulação, há uma chance de que quem a tenha lançado mantenha backups e que você possa convencê-los a fazer o download. </li></ol><br>  É isso que quero dizer com IA abordando impulsos religiosos.  Que outro sistema de crenças oferece quatro opções para a imortalidade cientificamente comprovada? <br><br>  Aprendemos que pelo menos um plutocrata americano (provavelmente Elon Musk, que acredita que as chances de vivermos em uma simulação é de um bilhão para um) contratou um par de codificadores para tentar decifrar a simulação.  Mas essa é uma intenção muito grosseira!  Eu uso isso! <br><br>  Se você acha que mora em um programa de computador, as tentativas de trazê-lo para o segfault não são razoáveis ​​para todos que moram nele com você.  Isso é muito mais perigoso e irresponsável do que os cientistas nucleares que tentam explodir a atmosfera. <br><br><h3>  Sede por dados </h3><br>  Como já mencionei, a maneira mais eficaz de obter algo interessante da IA ​​que criamos é descartá-los com dados.  Essa dinâmica é socialmente prejudicial.  Chegamos perto da introdução orwelliana de microfones em todas as casas.  Os dados de IA serão centralizados, eles serão usados ​​para treinar redes neurais, que poderão então ouvir melhor nossos desejos. <br><br>  Mas se você acha que esse caminho nos leva à IA, você deseja maximizar a quantidade de dados coletados e no menor formato possível modificado.  Isso apenas reforça a ideia da necessidade de coletar o máximo de dados e realizar a vigilância mais abrangente. <br><br><h3>  Teoria das Cordas para Programadores </h3><br>  O risco da IA ​​é a teoria das cordas para programadores.  É divertido pensar sobre isso, é interessante e completamente inacessível para experimentos no nível da tecnologia moderna.  Você pode construir palácios de cristais mentais que funcionam com base em princípios primários, e depois subir neles e apertar a escada atrás deles. <br><br>  Pessoas capazes de chegar a conclusões absurdas com base em uma longa cadeia de raciocínio abstrato e permanecer confiantes em sua verdade - não são pessoas que precisam confiar na administração cultural. <br><br><h3>  O caminho para a loucura </h3><br>  Toda essa área de "pesquisa" leva à loucura.  Uma das características do pensamento profundo sobre os riscos de IA é que, quanto mais loucas são suas idéias, mais popular você se torna entre outros entusiastas.  Isso demonstra sua coragem de seguir essa linha de pensamento até o fim. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ray Kurzweil</a> , que acredita que não vai morrer, trabalha com o Google há vários anos e agora, provavelmente, está trabalhando nesse problema.  O Vale do Silício geralmente está cheio de pessoas trabalhando em projetos malucos sob o pretexto de dinheiro. <br><br><h3>  Cosplay AI </h3><br>  O efeito social mais prejudicial da ansiedade sobre a IA é o que eu chamo de AI de cosplay.  As pessoas que estão convencidas da realidade e inevitabilidade da IA ​​começam a se comportar conforme suas fantasias lhes dizem sobre o que a IA superinteligente pode fazer. <br><br>  Em seu livro, Bostrom lista seis coisas que a IA deve ter sucesso antes de capturar o mundo: <br><br><ol><li>  Multiplicação de inteligência. </li><li>  Pensamento estratégico. </li><li>  Manipulação social. </li><li>  Hacks </li><li>  Pesquisa tecnológica. </li><li>  Produtividade Econômica. </li></ol><br>  Se você olhar para os adeptos da IA ​​do Vale do Silício, eles mesmos parecem estar trabalhando nessa lista quase sociopática. <br><br>  Sam Altman, chefe do YCombinator, é meu exemplo favorito de um arquétipo desse tipo.  Aparentemente, ele é fascinado pela idéia de reinventar o mundo do zero, maximizando a influência e a produtividade pessoal.  Ele designou equipes para trabalhar na invenção das cidades do zero e está envolvido em fraudes políticas ocultas para influenciar a eleição. <br><br>  Esse comportamento da “capa e adaga”, inerente à elite do techno, provocará uma reação negativa de pessoas que não estão envolvidas em tecnologias que não gostam de ser manipuladas.  É impossível puxar infinitamente as alavancas do poder; acabará por irritar outros membros da comunidade democrática. <br><br>  Eu assisti pessoas do chamado  “Comunidades racionalistas” se referem a pessoas que não são consideradas efetivas, “personagens não-jogadores” (NPCs), um termo emprestado dos jogos.  Esta é uma maneira terrível de ver o mundo. <br><br>  Por isso, trabalho em um setor em que os racionalistas autoproclamados são as pessoas mais loucas.  Isso é esmagador. <br><br>  Esses cosplayers de IA são como crianças de nove anos montando um acampamento no quintal, brincando com lanternas em tendas.  Eles projetam suas próprias sombras nas paredes da tenda e ficam com medo deles como se fossem monstros. <br><br>  Mas, de fato, eles respondem a uma imagem distorcida de si mesmos.  Há um ciclo de feedback entre como as pessoas inteligentes imaginam o comportamento da inteligência divina e como elas constroem seu próprio comportamento. <br><br>  Então, qual é a resposta, como isso pode ser corrigido? <br><br>  Precisamos de melhor ficção científica!  E, como em muitos outros casos, já temos a tecnologia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8fa/7bb/a47/8fa7bba47a9f451102465e23774d291f.jpg"><br><br>  Este é Stanislav Lem, o grande escritor polonês de ficção científica.  A NF em inglês é terrível, mas no bloco oriental temos muitos bens bons e precisamos exportá-los corretamente.  Ele já foi traduzido ativamente para o inglês; essas traduções só precisam ser melhor distribuídas. <br><br>  O que distingue autores como Lem ou os irmãos Strugatsky de seus parceiros ocidentais é que eles cresceram em condições difíceis, sobreviveram à guerra e depois viveram em sociedades totalitárias, onde precisavam expressar suas idéias não diretamente, através de uma palavra impressa. <br><br>  Eles têm uma compreensão real da experiência humana e das limitações do pensamento utópico, praticamente ausente no Ocidente. <br><br>  Há exceções notáveis ​​- Stanley Kubrick conseguiu fazer isso - mas é extremamente raro encontrar uma NF americana ou britânica que expresse uma visão restrita do que nós, como espécie, podemos fazer com a tecnologia. <br><br><h3>  Alquimistas </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/fdc/b9e/596/fdcb9e5963c4ad447547319ebf394f40.jpg" alt="imagem"><br><br>  Como eu critico o alarmismo da IA, é justo colocar minhas cartas na mesa.  Eu acho que nosso entendimento da mente está aproximadamente no mesmo estado em que a alquimia estava no século XVII. <br><br>  Alquimistas têm uma má reputação.  Nós os consideramos místicos, na maioria das vezes não envolvidos no trabalho experimental.  Pesquisas modernas mostram que eles eram químicos-praticantes muito mais diligentes do que pensamos.  Em muitos casos, eles usaram técnicas experimentais modernas, mantiveram registros de laboratório e fizeram as perguntas certas. <br><br>  Os alquimistas entendiam muitas coisas corretamente!  Por exemplo, eles estavam convencidos da teoria corpuscular da matéria: que tudo consiste em pequenos pedaços e que é possível compor esses pedaços de maneiras diferentes, criando diferentes substâncias - e é assim! <br><br>  O problema deles era a falta de equipamentos precisos necessários para fazer as descobertas necessárias.  A grande descoberta que um alquimista precisa fazer é a lei da conservação da massa: o peso dos ingredientes iniciais coincide com o peso do final.  No entanto, alguns deles podem ser gases ou líquidos em evaporação, e os alquimistas simplesmente não tinham precisão.  A química moderna não era possível até o século XVIII. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21b/5b2/888/21b5b28887c09eeebc52c57e43025c42.jpg"><br><br>  Mas os alquimistas também tinham pistas que os confundiam.  Eles estavam obcecados com mercúrio.  Quimicamente, o mercúrio não é particularmente interessante, mas é o único metal na fase líquida à temperatura ambiente.  Isso parecia muito importante para os alquimistas e os fez colocar mercúrio no centro de seu sistema alquímico e em sua busca pela Pedra Filosofal, uma maneira de transformar metais comuns em ouro. <br><br>  A neurotoxicidade do mercúrio exacerbou a situação.  Se você brinca demais com ela, pensamentos estranhos surgirão para você.  Nesse sentido, assemelha-se às nossas experiências mentais atuais relacionadas à supermente. <br><br>  Imagine que enviamos um livro de química moderna para o passado a algum grande alquimista como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">George Starkey</a> ou Isaac Newton.  A primeira coisa que eles fariam com isso seria percorrê-lo em busca de uma resposta para a questão de saber se havíamos encontrado a Pedra Filosofal.  E eles saberiam que o encontramos!  Realizamos o sonho deles! <br><br>  Mas não gostamos muito, porque depois de transformar metais em ouro, ele se torna radioativo.  Fique ao lado de um lingote de ouro convertido e ele o matará com raios mágicos invisíveis. <br><br>  Pode-se imaginar o quão difícil seria fazer com que os conceitos modernos de radioatividade e energia atômica não soassem místicos para eles. <br><br>  Teríamos de explicar a eles por que usamos a “pedra filosofal”: para a fabricação de metal que nunca existiu no planeta, e um punhado de punhados é suficiente para explodir uma cidade inteira se eles colidirem a uma velocidade suficientemente alta. <br><br>  Além disso, teríamos de explicar aos alquimistas que todas as estrelas no céu são "pedras filosóficas" que transformam um elemento em outro, e que todas as partículas em nossos corpos vêm de estrelas do firmamento que existia e explodiu antes da Terra aparecer. <br><br> ,   ,  ,     ,      ,  ,     ,   ,     ,        ,  . <br><br>   ,  ,   ,      ,    ,     ,       .     –     .      ,   . <br><br>   ,           .     .   –  .        , ,  (     ),    ,   . <br><br>          ,     ,         . <br><br>      ,      .  ,        .  ,     ,  ,         .  ,         ,   ,      . <br><br>       .    ,     ,           . <br><br>    ,      , ,  ,   ,    .    ,     . <br><br>       ,   – ,     «»,  ,     .       .  E isso é ótimo!   .    ,    : <br><blockquote>      ,  ,   ,    . <br> —   </blockquote>       ,    ,         ,      . <br><br>    ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>      ,     ,  ,   -   ,   ,    . <br><br>         ,       ,   ,      ,         . <br><br> , ,   ,       .    ,   -      .    ,      . <br><br>        , ,  ,     ,    . <br><br>  ,        . ,       - ,   ,       , ,   ,    . <br><br>        :   ,  ,     .  ! <br><br>         ,       –   ,    ,       . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt432806/">https://habr.com/ru/post/pt432806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt432796/index.html">Módulo de computação, modelos 2019</a></li>
<li><a href="../pt432798/index.html">Melhor sistema operacional de segurança: comparação do Titan</a></li>
<li><a href="../pt432800/index.html">Investigação de incidentes de segurança com o StaffCop Enterprise 4.4</a></li>
<li><a href="../pt432802/index.html">Seis plataformas gratuitas de aprendizado de programação automatizada</a></li>
<li><a href="../pt432804/index.html">Toda a verdade sobre o RTOS. Artigo 24. Filas: serviços auxiliares e estruturas de dados</a></li>
<li><a href="../pt432808/index.html">Salários em IA: onde há mais dinheiro e quem eles procuram na Rússia</a></li>
<li><a href="../pt432810/index.html">Primeiras multas para o RGPD: quem já foi punido</a></li>
<li><a href="../pt432812/index.html">Escrevemos robôs de negociação usando a estrutura gráfica StockSharp. Parte 1</a></li>
<li><a href="../pt432814/index.html">Integração Cake e TeamCity</a></li>
<li><a href="../pt432816/index.html">AXIS M3046-V vs IDIS DC-D3212X: Compare Câmeras CFTV</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>