<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèº üî∏ üë©üèª‚Äç‚öñÔ∏è Bereitstellen von Elasticsearch unter AWS mit Kubernetes in 10 Schritten üèê üßîüèº üçå</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kubernetes aka k8s ist ein Open Source-System zur Automatisierung der Bereitstellung, Skalierung und Verwaltung von Containeranwendungen. In diesem Ar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bereitstellen von Elasticsearch unter AWS mit Kubernetes in 10 Schritten</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/416643/"><p><img src="https://habrastorage.org/webt/rt/no/ud/rtnouda08zlh-unwdbcvqlb8fts.jpeg"></p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes</a> aka <code>k8s</code> ist ein Open Source-System zur Automatisierung der Bereitstellung, Skalierung und Verwaltung von Containeranwendungen.  In diesem Artikel werde ich Ihnen zeigen, wie Sie einen Kubernetes-Cluster einrichten und einen Elasticsearch-Cluster f√ºr AWS bereitstellen.  Diese Einstellungen funktionieren auch in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GCE</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure</a> . </p><a name="habracut"></a><br><h3 id="konfigurirovanie-kubernetes-na-aws">  Kubernetes unter AWS konfigurieren </h3><br><p>  <strong>Erhalten Sie zun√§chst</strong> Administratorzugriff auf die folgenden AWS-Services: <strong>S3, EC2, Route53, IAM</strong> und <strong>VPC</strong> . </p><br><p>  <strong>1. Installation:</strong> Ich werde die Installation der CLI f√ºr Linux zeigen.  Wenn Sie ein anderes Betriebssystem haben, folgen Sie den nachstehenden Links, um Installationsanweisungen f√ºr Ihr Betriebssystem zu erhalten. </p><br><p>  Stellen Sie zun√§chst die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS-CLI so ein</a> , dass √ºber die CLI auf AWS zugegriffen wird.  Wenn Sie bereits Python und Pip haben, f√ºhren Sie den folgenden Befehl aus: </p><br><pre> <code class="plaintext hljs">pip install awscli --upgrade --user</code> </pre> <br><p>  Anschlie√üend verwenden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kops</a> , ein Befehlszeilentool, das uns durch die Einrichtung des K8S-Clusters auf Produktionsebene f√ºhrt. <br>  Installieren Sie die <strong>Kops-</strong> Bin√§rdateien direkt von Github. </p><br><pre> <code class="plaintext hljs">wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/</code> </pre> <br><p>  Schlie√ülich verwenden wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubectl-</a> CLI, um den K8S-Cluster zu verwalten (wenn Sie Docker verwendet haben, √§hnelt dies der <strong>Docker-</strong> CLI).  Die neueste Version wird mit dem folgenden Befehl installiert: </p><br><pre> <code class="plaintext hljs">wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl</code> </pre> <br><p>  <strong>Hinweis:</strong> Sie k√∂nnen den Kubernetes-Cluster starten und den Anweisungen in diesem Artikel auf einem Heimcomputer mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Minikube folgen</a> . </p><br><p>  <strong>2.Erstellen von IAM-Benutzern: Um</strong> Cluster in AWS zu erstellen, erstellen wir einen separaten IAM-Benutzer f√ºr <code>kops</code> .  F√ºr <code>kops</code> ben√∂tigen <code>kops</code> ein API-Konto.  Erstellen Sie einen Benutzer und konfigurieren Sie ein Konto √ºber die Benutzeroberfl√§che der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS-Konsole</a> .  Der <code>kops</code> Benutzer ben√∂tigt die folgende IAM-Berechtigung: </p><br><ul><li>  AmazonEC2FullAccess </li><li>  AmazonRoute53FullAccess </li><li>  AmazonS3FullAccess </li><li>  IAMFullAccess </li><li>  AmazonVPCFullAccess </li></ul><br><p><img src="https://habrastorage.org/webt/93/ll/zh/93llzhkqarlmyjtluwo3xiqpjwa.jpeg"></p><br><p>  Alternativ k√∂nnen Sie dasselbe √ºber die CLI tun, indem Sie die folgenden Befehle anwenden: </p><br><pre> <code class="plaintext hljs">aws iam create-group --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops aws iam create-user --user-name kops aws iam add-user-to-group --user-name kops --group-name kops aws iam create-access-key --user-name kops</code> </pre> <br><p>  Beachten Sie die <code>SecretAccessKey</code> und <code>AccessKeyID</code> in <code>kops</code> . </p><br><p>  Konfigurieren Sie die AWS-CLI so, dass Ihr Konto mit <code>aws configure</code> . </p><br><p>  Stellen Sie sicher, dass sich der von Ihnen erstellte Benutzer in der <code>aws iam list-users</code> . </p><br><p>  Wir exportieren das AWS-Konto als die folgenden Umgebungsvariablen, damit die CLI- <code>kops</code> sie verwenden k√∂nnen. </p><br><pre> <code class="plaintext hljs">export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)</code> </pre> <br><blockquote>  <em>Wenn Sie Kops 1.6.2 oder h√∂her verwenden, ist das Einrichten eines DNS optional.</em>  <em>Sie k√∂nnen einen Klatschcluster erstellen.</em>  <em>Einzige Voraussetzung: Der Clustername muss mit <code>.k8s.local</code> .</em> </blockquote><br><h3 id="nastroyka-dns">  DNS-Setup </h3><br><p>  Wenn Sie Ihre Domain bereits √ºber AWS gehostet haben und planen, sie zu verwenden, m√ºssen Sie nichts tun.  Eine weitere Option: Wenn Sie eine Subdomain Ihrer Domain verwenden m√∂chten, erstellen Sie eine zweite √∂ffentliche Hostingzone f√ºr diese Subdomain.  In diesem Handbuch arbeiten wir mit einer privaten Hosting-Zone.  Stellen Sie die Zone unter einem beliebigen Namen ein.  Verwenden Sie diesen Namen, um Kubernetes-Cluster zu erstellen.  <a href="">Weitere</a> Informationen zum Einrichten von DNS finden Sie <a href="">hier</a> . </p><br><p>  <strong>3. Erstellen eines S3-</strong> Buckets: Um den Status und das Erscheinungsbild unseres K8S-Clusters zu speichern, m√ºssen Sie einen separaten S3-Bucket f√ºr <code>kops</code> .  Dieser Bucket wird zu einer Quelle zuverl√§ssiger Daten f√ºr den Konfigurationscluster. </p><br><pre> <code class="plaintext hljs">aws s3api create-bucket \ --bucket &lt;your-unique-bucket-name&gt; \ --region us-east-1</code> </pre> <br><p>  <strong>Hinweis:</strong> Wenn Sie Ihren Eimer in einem anderen Bereich als <code>us-east-1</code> in Betrieb nehmen <code>- region</code> wechseln Sie zus√§tzlich zur Einstellung <code>- region</code> zum gew√ºnschten Bereich und f√ºgen Sie <code>LocationConstraint</code> zum selben Bereich hinzu.  Das Folgende zeigt den Befehl zum Erstellen von Buckets in der Region <code>us-west-1</code> . </p><br><pre> <code class="plaintext hljs">aws s3api create-bucket \ --bucket &lt;your-unique-bucket-name&gt; \ --region us-west-1 \ --create-bucket-configuration LocationConstraint=us-west-1</code> </pre> <br><p>  Verwenden Sie den folgenden Befehl, um den Speicher f√ºr S3-Bucket-Versionen f√ºr die Wiederherstellung zu konfigurieren: </p><br><pre> <code class="plaintext hljs">aws s3api put-bucket-versioning \ --bucket &lt;your-unique-bucket-name&gt; \ --versioning-configuration Status=Enabled</code> </pre> <br><p>  <strong>4. Erstellen des ersten Kubernetes-Clusters:</strong> Jetzt k√∂nnen Sie Ihren ersten Cluster erstellen!  Richten Sie zun√§chst die Umgebungsvariablen ein, um den Prozess zu vereinfachen.  Wenn Sie die DNS-Konfiguration √ºbersprungen haben (nach Schritt 2), f√ºgen <code>.k8s.local</code> dem <code>NAME</code> Wert <code>.k8s.local</code> hinzu. </p><br><pre> <code class="plaintext hljs">export NAME=myfirstcluster.example.com export KOPS_STATE_STORE=s3://your-bucket-name</code> </pre> <br><p>  Vergessen Sie nicht zu verfolgen, welche regionalen Zonen Ihnen zur Verf√ºgung stehen.  In diesem Beispiel werden wir einen Cluster in der Region <strong>us-east-2</strong> bereitstellen. </p><br><pre> <code class="plaintext hljs">aws ec2 describe-availability-zones --region us-east-2</code> </pre> <br><p>  Wenn Sie eine √∂ffentliche Hostingzone verwenden, erstellen Sie einen Cluster mit dem folgenden Befehl: </p><br><pre> <code class="plaintext hljs">kops create cluster \ --zones us-east-2c \ --node-count 3 \ ${NAME}</code> </pre> <br><p>  Wenn Sie eine private Hosting-Zone verwenden, gehen Sie wie folgt vor: </p><br><pre> <code class="plaintext hljs">kops create cluster \ --zones us-east-2c \ --node-count 3 \ --dns private ${NAME}</code> </pre> <br><p>  Mit diesem Befehl erhalten Sie das K8S-Cluster-Konfigurationsprotokoll.  Der Start des Clusters dauert einige Zeit, da neue EC2-Maschinen f√ºr Minion-Masterknoten erstellt werden. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 test]$ kops create cluster \ &gt; --dns private \ &gt; --zones us-east-2c \ &gt; --node-count 3 \ &gt; ${NAME} --yes I0306 09:45:29.636834 20628 create_cluster.go:439] Inferred --cloud=aws from zone "us-east-2c" I0306 09:45:29.637030 20628 create_cluster.go:971] Using SSH public key: /home/ec2-user/.ssh/id_rsa.pub I0306 09:45:29.850021 20628 subnets.go:184] Assigned CIDR 172.20.32.0/19 to subnet us-east-2c I0306 09:45:31.118837 20628 dns.go:92] Private DNS: skipping DNS validation I0306 09:45:46.986963 20628 executor.go:91] Tasks: 73 done / 73 total; 0 can run I0306 09:45:46.987101 20628 dns.go:153] Pre-creating DNS records I0306 09:45:47.668392 20628 update_cluster.go:248] Exporting kubecfg for cluster kops has set your kubectl context to k8s.appbase Cluster is starting. It should be ready in a few minutes.</code> </pre> <br><p>  Voila!  Der K8s-Cluster sollte bereits funktionieren. </p><br><p>  <strong>5. Clusterpr√ºfung:</strong> Alle von <code>kops</code> erstellten <code>kops</code> befinden sich in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ASG (Auto Scaling Groups)</a> .  Im Fehlerfall werden die ASG-Instanzen √ºberpr√ºft und automatisch neu erstellt. </p><br><p>  F√ºhren Sie den folgenden Befehl aus, um die Clusterkonfiguration zu √§ndern: </p><br><pre> <code class="plaintext hljs">kops edit cluster ${NAME}</code> </pre> <br><p>  Jedes Mal, wenn Sie die Clusterkonfiguration √§ndern, m√ºssen Sie einen Cluster erstellen, indem Sie den folgenden Befehl ausf√ºhren: </p><br><pre> <code class="plaintext hljs">kops update cluster ${NAME} --yes</code> </pre> <br><p>  Sie werden so etwas sehen. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 examples]$ kops update cluster --yes Using cluster from kubectl context: k8s.appbase I0216 05:09:06.074467 2158 dns.go:92] Private DNS: skipping DNS validation I0216 05:09:07.699380 2158 executor.go:91] Tasks: 73 done / 73 total; 0 can run I0216 05:09:07.699486 2158 dns.go:153] Pre-creating DNS records I0216 05:09:07.961703 2158 update_cluster.go:248] Exporting kubecfg for cluster kops has set your kubectl context to k8s.appbase Cluster changes have been applied to the cloud.</code> </pre> <br><p>  √úberpr√ºfen Sie den Cluster. </p><br><pre> <code class="plaintext hljs">kops validate cluster</code> </pre> <br><p>  Stellen Sie sicher, dass der Cluster aktiv ist. </p><br><pre> <code class="plaintext hljs">Using cluster from kubectl context: k8s.appbase Validating cluster k8s.appbase INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-us-east-2c Master t2.large 1 1 us-east-2c nodes Node t2.medium 3 3 us-east-2c NODE STATUS NAME ROLE READY ip-172-20-44-33.us-east-2.compute.internal master True ip-172-20-52-48.us-east-2.compute.internal node True ip-172-20-62-30.us-east-2.compute.internal node True ip-172-20-64-53.us-east-2.compute.internal node True Your cluster k8s.appbase is ready</code> </pre> <br><p>  <strong>Schauen Sie sich Ihre neuen k8s an!</strong> </p><br><p>  Mit einem einfachen Aufruf der Kubernetes-API k√∂nnen Sie √ºberpr√ºfen, ob die API online ist und abh√∂rt.  Verwenden Sie <code>kubectl</code> , um die Knoten zu √ºberpr√ºfen. </p><br><pre> <code class="plaintext hljs">kubectl get nodes</code> </pre> <br><p>  Dadurch erhalten Sie Informationen zu Ihren Knoten und deren aktuellem Status. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 elasticsearch]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-172-20-44-33.us-east-2.compute.internal Ready master 1m v1.8.6 ip-172-20-52-48.us-east-2.compute.internal Ready node 3m v1.8.6 ip-172-20-62-30.us-east-2.compute.internal Ready node 2m v1.8.6 ip-172-20-64-53.us-east-2.compute.internal Ready node 4m v1.8.6</code> </pre> <br><p>  Ein Kubernetes-Sub ist eine Abstraktion, die eine Gruppe von einem oder mehreren Anwendungscontainern (z. B. Docker) und mehreren gemeinsam genutzten Ressourcen f√ºr diese Container darstellt.  Unter entfaltet sich auf dem Knoten.  Wenn Sie die Anwendung skalieren m√ºssen, f√ºgen Sie dem bereitgestellten K8S Knoten hinzu. </p><br><p>  So informieren Sie sich √ºber die verf√ºgbaren Pods: </p><br><pre> <code class="plaintext hljs">kubectl get pods</code> </pre> <br><p>  Dieser Befehl listet die verf√ºgbaren Herde im Cluster auf. </p><br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE es-5967f5d99c-5vcpb 1/1 Running 0 3h es-5967f5d99c-cqk88 1/1 Running 0 3h es-5967f5d99c-lp789 1/1 Running 0 3h</code> </pre> <br><h3 id="razvertyvanie-elasticsearch-v-klastere-k8s">  Elasticsearch-Bereitstellung in K8S-Cluster </h3><br><p><img src="https://habrastorage.org/webt/ot/1m/he/ot1mhep0o9nltuo2ueq0puexg7g.png"></p><br><p>  Wenn Sie mit Kubernetes nicht vertraut sind, empfehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ich das Online-Training von k8</a> . </p><br><p>  Im Moment haben wir im K8S-Cluster erstellt: den Hauptknoten und zwei Agentenknoten.  Die Rolle des Hauptknotens besteht darin, Bereitstellungsbefehle an Anwendungen zu √ºbertragen, die in den Pods der Knotenagenten ausgef√ºhrt werden. </p><br><p>  K8S-Anwendungsbereitstellungen sind deklarativ und werden √ºber JSON / YAML-Dateien konfiguriert.  W√§hlen Sie einen Controller basierend auf dem Typ der Anwendung oder des Systems, das Sie bereitstellen.  Da Elasticsearch eine Stateful-Anwendung ist, verwenden wir den StatefulSet-Controller. </p><br><p>  <strong>6. Bereitstellung √ºber StatefulSet.</strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>StatefulSet</strong></a> verwaltet Pods basierend auf der Spezifikation identischer Container.  Es verwaltet die Bereitstellung und Skalierung des Herdsatzes und stellt die Reihenfolge und Einzigartigkeit dieser Herde sicher.  Der <strong>StatefulSet-</strong> Controller erleichtert auch das <strong>Zuordnen</strong> der Anwendung zu einem dauerhaften Volume, was f√ºr Elasticsearch wichtig ist. </p><br><p>  Erstellen Sie eine Datei mit dem Namen <code>es-stateful set. yaml</code> .  Es enth√§lt die Elasticsearch-Spezifikation.  Sie k√∂nnen die Konfiguration jederzeit √§ndern.  Eine Liste der Umgebungsvariablen, die an Ihr Elasticsearch-Image √ºbergeben werden k√∂nnen, finden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie hier</a> . </p><br><p>  <strong>7. Services:</strong> <code>Service</code> Kubernetes - eine Abstraktion, die einen logischen Satz von <code></code> und den Zugriff darauf definiert.  Auf diese Weise kann die Containeranwendung eine andere Containeranwendung oder eine eigene Instanz in einem anderen Herd identifizieren. </p><br><p><img src="https://habrastorage.org/webt/di/wn/yq/diwnyqsurqxa0813hatxpfhfgco.png"></p><br><p>  <code>LoadBalancer</code> ist eine spezielle Art von Dienst, der Pods f√ºr externe Netzwerke bereitstellt und die Last verteilt.  Wir werden es verwenden, um eine externe IP-Adresse zu erstellen, √ºber die jeder den Elasticsearch-Cluster kontaktieren kann.  Wir werden diesen Service f√ºr ES-Knoten verwenden, um uns gegenseitig zu entdecken. </p><br><p>  Erstellen Sie eine Datei mit dem Namen <code>es-svc.yaml</code> .  Bearbeiten Sie es und geben Sie den Load Balancer-Dienst an. </p><br><pre> <code class="plaintext hljs">apiVersion: v1 #API Version of the resource kind: Service #Type of resource metadata: #Contains metadata of this resource. name: elasticsearch #Name of this resource labels: #Additional identifier to put on pods component: elasticsearch #puts component = elasticsearch spec: #Specifications of this resource type: LoadBalancer #type of service selector: #will distribute load on pods which component: elasticsearch #have label `component = elasticsearch` ports: #Port on which LoadBalancer will listen - name: http #Name given to port port: 9200 #Port number protocol: TCP #Protocol supported - name: transport #Name given to port port: 9300 #Port number protocol: TCP #Protocol supported</code> </pre> <br><p>  <strong>8. Erstellen einer Anwendung:</strong> Das ist alles, was wir brauchen.  Stellen Sie unseren Elasticsearch-Cluster mit den folgenden Befehlen auf K8S bereit. </p><br><pre> <code class="plaintext hljs">kubectl create -f es-statefulset.yaml kubectl create -f es-svc.yaml</code> </pre> <br><p>  'Erstellen' ist ein universeller Befehl zum Erstellen einer Ressource in K8S. </p><br><p>  Unser Elasticsearch-Cluster mit 3 Knoten (erinnern Sie sich an <code>replicas = 3</code> in der StatefulSet-Konfiguration?) Wird sofort gestartet. </p><br><p>  Wir k√∂nnen die Elasticsearch-Pods mit diesem Befehl √ºberpr√ºfen: </p><br><pre> <code class="plaintext hljs">kubectl get pods</code> </pre> <br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 test]$ kubectl get pods,svc,deployment NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 23m es-1 1/1 Running 0 17m es-2 1/1 Running 0 23m</code> </pre> <br><p>  <strong>9. Testen des Elasticsearch-Clusters:</strong> √úberpr√ºfen Sie, ob Elasticsearch richtig konfiguriert ist und funktioniert.  Rufen Sie die externe IP-Adresse ab, um eine Verbindung zu Elasticsearch herzustellen.  Es befindet sich im von uns erstellten <strong>LoadBalancer-</strong> Dienst.  Verwenden Sie den folgenden Befehl, um den <strong>LoadBalancer</strong> zu beschreiben: </p><br><pre> <code class="plaintext hljs">kubectl describe service elasticsearch</code> </pre> <br><pre> <code class="plaintext hljs">[ec2-user@ip-172-31-35-145 examples]$ kubectl describe service elasticsearch Name: elasticsearch Namespace: default Labels: component=elasticsearch Annotations: &lt;none&gt; Selector: component=elasticsearch Type: LoadBalancer IP: 100.70.114.146 LoadBalancer Ingress: http://a4d0c157d212811e898430af47d23da1-952261901.us-east-2.elb.amazonaws.com Port: http 9200/TCP TargetPort: 9200/TCP NodePort: http 31358/TCP Endpoints: 100.96.4.28:9200 Port: transport 9300/TCP TargetPort: 9300/TCP NodePort: transport 31767/TCP Endpoints: 100.96.4.28:9300 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 1m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 1m service-controller Ensured load balancer [ec2-user@ip-172-31-35-145 examples]$</code> </pre> <br><p>  Beachten Sie den Wert von <code>LoadBalancer Ingress</code> .  √ñffnen Sie einen Browser mit einer URI und der Suffixnummer des externen Elasticsearch-Ports: <code>9200</code> .  Sie werden dies sehen: </p><br><p><img src="https://habrastorage.org/webt/el/b_/zl/elb_zlrvyocf5b7phv5pd9zubri.jpeg"></p><br><p>  Sie k√∂nnen die Funktionalit√§t von Elasticsearch-Knoten √ºberpr√ºfen, indem Sie <code>9200/_cluster /health?pretty</code> hinzuf√ºgen: <code>9200/_cluster /health?pretty</code> zur externen IP-Adresse. </p><br><p><img src="https://habrastorage.org/webt/hh/ye/aa/hhyeaabyci4yvxnwqjkxacghek4.jpeg"></p><br><p>  <strong>10. Kubernetes-Heilungstests:</strong> StatefulSets kann die angegebene Anzahl von Replikaten speichern.  Auf diese Weise startet StatefulSet ein neues Sub, wenn ein Sub f√§llt. </p><br><p>  Wir werden es testen, indem wir einen Fehler simulieren (L√∂schen aller Pods, auf denen unsere ES-Instanzen ausgef√ºhrt werden), um festzustellen, ob unser ES-Cluster automatisch mit intakten Daten sichern kann. </p><br><p><img src="https://habrastorage.org/webt/l1/vh/nb/l1vhnbrmqhhumqzqi1c7uczvdaa.gif"></p><br><p>  Da StatefulSet jeweils einen Herd ausf√ºhrt, dauert es einige Zeit, alle Container wiederherzustellen. </p><br><p>  Wir sehen, dass uns nach der Wiederherstellung der Herde im Zustand vor dem ES-Ausfall ein indizierter Datensatz zur Verf√ºgung steht. </p><br><h3 id="rekomenduem-sleduyuschie-shagi">  Empfohlene n√§chste Schritte </h3><br><p>  Bevor Sie diese Einstellungen in der Produktion verwenden, beachten Sie bitte: </p><br><ol><li>  Konfigurieren Sie Backups.  Hilft bei der Wiederherstellung verlorener Daten.  Dieser Prozess ist am besten automatisiert. </li><li>  Autorisierungs-Setup.  Wir m√∂chten den Elasticsearch-Cluster sch√ºtzen.  Das Einrichten der Basisauthentifizierung oder -autorisierung basierend auf einem Medientoken bietet Sicherheit. </li><li>  TLS-Zertifikate.  Konfigurieren Sie LetsEncrypt / andere TLS-Anbieter von Zertifikaten f√ºr die Zuordnung pers√∂nlicher Dom√§nen f√ºr unseren ES-Cluster und sch√ºtzen Sie alle an ihn gesendeten Anforderungen. </li></ol><br><p>  Der Artikel handelt zwar nicht davon, wei√ü aber: Kubernetes kann das alles. </p><br><p>  Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stellen Sie Elasticsearch mit Kubernetes unter AWS in 10 Schritten bereit</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de416643/">https://habr.com/ru/post/de416643/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de416633/index.html">QUIC, TLS 1.3, DNS-over-HTTPS, dann √ºberall</a></li>
<li><a href="../de416635/index.html">Von rechts nach links. So drehen Sie die Site-Schnittstelle unter RTL</a></li>
<li><a href="../de416637/index.html">Musik aus Papier und Pappe: eine kurze Geschichte des Variophons und des ‚Äûgezeichneten Klangs‚Äú</a></li>
<li><a href="../de416639/index.html">Interview mit einem Pionier der Verj√ºngung</a></li>
<li><a href="../de416641/index.html">8 Phasen des Prozesses zur Entwicklung einer mobilen Anwendungsschnittstelle</a></li>
<li><a href="../de416645/index.html">MIS. Forschungsmuster</a></li>
<li><a href="../de416647/index.html">Tr√§umen Regierungsbeh√∂rden von elektrischen Risiken?</a></li>
<li><a href="../de416651/index.html">1M HTTP-RPS auf 1 CPU-Kern. DPDK anstelle von Nginx + Linux Kernel TCP / IP</a></li>
<li><a href="../de416653/index.html">Bibliothek sortieren</a></li>
<li><a href="../de416657/index.html">Zwei Drittel der verwendeten Speicherkarten enthalten personenbezogene Daten von Vorbesitzern</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>