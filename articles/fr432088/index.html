<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüè≠ ‚¨ÖÔ∏è üë©üèø‚Äçü§ù‚Äçüë®üèΩ Haute disponibilit√© MySQL sur GitHub üëÜüèø üßë ü§≥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="GitHub utilise MySQL comme son principal entrep√¥t de donn√©es pour tout ce qui n'est pas li√© √† git , donc la disponibilit√© de MySQL est la cl√© du fonct...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Haute disponibilit√© MySQL sur GitHub</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/432088/"><p> GitHub utilise MySQL comme son principal entrep√¥t de donn√©es pour tout ce qui n'est pas li√© √† <code>git</code> , donc la disponibilit√© de MySQL est la cl√© du fonctionnement normal de GitHub.  Le site lui-m√™me, l'API GitHub, le syst√®me d'authentification et de nombreuses autres fonctionnalit√©s n√©cessitent un acc√®s aux bases de donn√©es.  Nous utilisons plusieurs clusters MySQL pour g√©rer divers services et t√¢ches.  Ils sont configur√©s selon le sch√©ma classique avec un n≈ìud <em>principal</em> disponible pour l'enregistrement et ses r√©pliques.  <em>Les r√©pliques</em> (autres n≈ìuds de cluster) reproduisent de mani√®re asynchrone les modifications apport√©es au n≈ìud principal et fournissent un acc√®s en lecture. </p><br><p>  La disponibilit√© des sites h√¥tes est critique.  Sans le n≈ìud principal, le cluster ne prend pas en charge l'enregistrement, ce qui signifie que vous ne pouvez pas enregistrer les modifications n√©cessaires.  Corriger les transactions, enregistrer les probl√®mes, cr√©er de nouveaux utilisateurs, des r√©f√©rentiels, des avis et bien plus sera tout simplement impossible. </p><br><p>  Pour prendre en charge l'enregistrement, un n≈ìud accessible correspondant est requis - le n≈ìud principal du cluster.  Cependant, la capacit√© d'identifier ou de <em>d√©tecter</em> un tel n≈ìud est tout aussi importante. </p><br><p>  En cas de panne du n≈ìud principal actuel, il est important de s'assurer de l'apparition rapide d'un nouveau serveur pour le remplacer, ainsi que de pouvoir notifier rapidement tous les services de ce changement.  Le temps d'arr√™t total correspond au temps n√©cessaire pour d√©tecter une d√©faillance, basculer et notifier un nouveau n≈ìud principal. </p><br><p><img src="https://habrastorage.org/webt/m8/ah/po/m8ahpo0jhrucgwj3kb5u7hn9edo.jpeg"></p><a name="habracut"></a><br><p>  Cette publication d√©crit une solution pour assurer une haute disponibilit√© de MySQL sur GitHub et d√©couvrir le service principal, ce qui nous permet d'effectuer de mani√®re fiable des op√©rations couvrant plusieurs centres de donn√©es, de maintenir l'op√©rabilit√© lorsque certains de ces centres ne sont pas disponibles et de garantir un temps d'arr√™t minimum en cas de panne. </p><br><h3 id="celi-obespecheniya-vysokoy-dostupnosti">  Objectifs de haute disponibilit√© </h3><br><p>  La solution d√©crite dans cet article est une nouvelle version am√©lior√©e des pr√©c√©dentes solutions haute disponibilit√© (HA) impl√©ment√©es sur GitHub.  √Ä mesure que nous grandissons, nous devons adapter la strat√©gie MySQL HA pour changer.  Nous nous effor√ßons de suivre des approches similaires pour MySQL et d'autres services sur GitHub. </p><br><p>  Pour trouver la bonne solution pour la haute disponibilit√© et la d√©couverte de services, vous devez d'abord r√©pondre √† quelques questions sp√©cifiques.  En voici un exemple: </p><br><ul><li>  Quel temps d'arr√™t maximum n'est pas critique pour vous? </li><li>  Quelle est la fiabilit√© des outils de d√©tection de pannes?  Les faux positifs (traitement des d√©faillances pr√©matur√©es) sont-ils critiques pour vous? </li><li>  Quelle est la fiabilit√© du syst√®me de basculement?  O√π une panne peut-elle se produire? </li><li>  Quelle est l'efficacit√© de la solution dans plusieurs centres de donn√©es?  Quelle est l'efficacit√© de la solution dans les r√©seaux √† latence faible et √©lev√©e? </li><li>  La solution continuera-t-elle de fonctionner en cas de panne compl√®te du centre de donn√©es (DPC) ou d'isolement du r√©seau? </li><li>  Quel m√©canisme (le cas √©ch√©ant) emp√™che ou att√©nue les cons√©quences de l'√©mergence de deux serveurs principaux dans le cluster qui enregistrent ind√©pendamment? </li><li>  La perte de donn√©es est-elle critique pour vous?  Si oui, dans quelle mesure? </li></ul><br><p>  Afin de d√©montrer, consid√©rons d'abord la solution pr√©c√©dente et discutons des raisons pour lesquelles nous avons d√©cid√© de l'abandonner. </p><br><h3 id="otkaz-ot-ispolzovaniya-vip-i-dns-dlya-obnaruzheniya">  Refus d'utiliser VIP et DNS pour la d√©couverte </h3><br><p>  Dans le cadre de la solution pr√©c√©dente, nous avons utilis√©: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">orchestrateur</a> pour la d√©tection des pannes et le basculement; </li><li>  VIP et DNS pour la d√©couverte d'h√¥te. </li></ul><br><p>  Dans ce cas, les clients ont d√©couvert un n≈ìud d'enregistrement par son nom, par exemple, <code>mysql-writer-1.github.net</code> .  Le nom a √©t√© utilis√© pour d√©terminer l'adresse IP virtuelle (VIP) du n≈ìud principal. </p><br><p>  Ainsi, dans une situation normale, les clients devaient simplement r√©soudre le nom et se connecter √† l'adresse IP re√ßue, o√π le n≈ìud principal les attendait d√©j√†. </p><br><p>  Consid√©rez la topologie de r√©plication suivante qui couvre trois centres de donn√©es diff√©rents: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/8fa/e2e/4f3/8fae2e4f382f3060b5a14ed9d80a5f67.png" alt="image"></p><br><p>  En cas de panne du n≈ìud principal, un nouveau serveur doit √™tre affect√© √† sa place (une des r√©pliques). </p><br><p>  <code>orchestrator</code> d√©tecte une d√©faillance, s√©lectionne un nouveau n≈ìud ma√Ætre, puis attribue le nom / VIP.  En r√©alit√©, les clients ne connaissent pas l'identit√© du n≈ìud principal, ils ne connaissent que le nom, qui devrait d√©sormais pointer vers le nouveau n≈ìud.  Attention cependant √† cela. </p><br><p>  Les adresses VIP sont partag√©es, les serveurs de bases de donn√©es eux-m√™mes en font la demande et en sont propri√©taires.  Pour recevoir ou lib√©rer un VIP, le serveur doit envoyer une demande ARP.  Le serveur propri√©taire du VIP doit d'abord le lib√©rer avant que le nouveau ma√Ætre puisse acc√©der √† cette adresse.  Cette approche entra√Æne des cons√©quences ind√©sirables: </p><br><ul><li>  En mode normal, le syst√®me de basculement contactera d'abord le n≈ìud principal d√©faillant et lui demandera de lib√©rer le VIP, puis se tournera vers le nouveau serveur principal avec une demande d'affectation VIP.  Mais que faire si le premier n≈ìud principal n'est pas disponible ou refuse une demande de lib√©ration de l'adresse VIP?  √âtant donn√© que le serveur est actuellement en √©tat de d√©faillance, il est peu probable qu'il puisse r√©pondre √† une demande √† temps ou y r√©pondre du tout. <br><ol><li>  En cons√©quence, une situation peut survenir lorsque deux h√¥tes revendiquent leurs droits sur le m√™me VIP.  Diff√©rents clients peuvent se connecter √† n'importe lequel de ces serveurs en fonction du chemin r√©seau le plus court. </li><li>  Le bon fonctionnement dans cette situation d√©pend de l'interaction de deux serveurs ind√©pendants et une telle configuration n'est pas fiable. </li></ol></li><li>  M√™me si le premier n≈ìud principal r√©pond aux demandes, nous perdons un temps pr√©cieux: le basculement vers le nouveau serveur principal ne se produit pas lorsque nous contactons l'ancien. </li><li>  De plus, m√™me en cas de r√©affectation de VIP, il n'y a aucune garantie que les connexions client existantes sur l'ancien serveur seront d√©connect√©es.  Encore une fois, nous courons le risque d'√™tre dans une situation avec deux n≈ìuds principaux ind√©pendants. </li></ul><br><p>  Ici et l√†, dans notre environnement, les adresses VIP sont associ√©es √† un emplacement physique.  Ils sont affect√©s √† un commutateur ou un routeur.  Par cons√©quent, nous ne pouvons r√©affecter une adresse VIP qu'√† un serveur situ√© dans le m√™me environnement que l'h√¥te d'origine.  En particulier, dans certains cas, nous ne pourrons pas attribuer un serveur VIP dans un autre centre de donn√©es et nous devrons apporter des modifications au DNS. </p><br><ul><li>  La distribution des modifications au DNS prend plus de temps.  Les clients stockent les noms DNS pour une p√©riode pr√©d√©finie.  Le basculement impliquant plusieurs centres de donn√©es implique un temps d'arr√™t plus long, car il faut plus de temps pour fournir √† tous les clients des informations sur le nouveau n≈ìud principal. </li></ul><br><p>  Ces restrictions ont √©t√© suffisantes pour nous obliger √† commencer la recherche d'une nouvelle solution, mais nous avons √©galement d√ª tenir compte des √©l√©ments suivants: </p><br><ul><li>  Les n≈ìuds principaux ont transmis ind√©pendamment des paquets d'impulsions via le service <code>pt-heartbeat</code> pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mesurer le retard et la r√©gulation de charge</a> .  Le service a d√ª √™tre transf√©r√© au n≈ìud principal nouvellement nomm√©.  Si possible, il aurait d√ª √™tre d√©sactiv√© sur l'ancien serveur. </li><li>  De m√™me, les n≈ìuds principaux contr√¥laient ind√©pendamment le fonctionnement du <a href="">Pseudo-GTID</a> .  Il √©tait n√©cessaire de d√©marrer ce processus sur le nouveau n≈ìud principal et de pr√©f√©rence de s'arr√™ter sur l'ancien. </li><li>  Le nouveau n≈ìud ma√Ætre est devenu accessible en √©criture.  L'ancien n≈ìud (si possible) devrait avoir <code>read_only</code> (lecture seule). </li></ul><br><p>  Ces √©tapes suppl√©mentaires ont entra√Æn√© une augmentation des temps d'arr√™t globaux et ajout√© leurs propres points de d√©faillance et de probl√®mes. </p><br><p>  La solution a fonctionn√© et GitHub a r√©ussi √† g√©rer les √©checs MySQL en arri√®re-plan, mais nous voulions am√©liorer notre approche de HA comme suit: </p><br><ul><li>  garantir l'ind√©pendance vis-√†-vis de centres de donn√©es sp√©cifiques; </li><li>  garantir l'op√©rabilit√© en cas de panne du centre de donn√©es; </li><li>  Abandonnez les workflows collaboratifs peu fiables </li><li>  r√©duire les temps d'arr√™t totaux; </li><li>  Effectuez, dans la mesure du possible, le basculement sans perte. </li></ul><br><h3 id="ha-reshenie-github-orchestrator-consul-glb">  Solution GitHub HA: orchestrateur, Consul, GLB </h3><br><p>  Notre nouvelle strat√©gie, ainsi que les am√©liorations qui l'accompagnent, √©liminent la plupart des probl√®mes mentionn√©s ci-dessus ou att√©nuent leurs cons√©quences.  Notre syst√®me HA actuel se compose des √©l√©ments suivants: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">orchestrateur</a> pour la d√©tection des pannes et le basculement.  Nous utilisons le sch√©ma <a href="">orchestrateur / raft</a> avec plusieurs centres de donn√©es, comme indiqu√© dans la figure ci-dessous; </li><li>  Hashicorp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Consul</a> pour la d√©couverte de services; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GLB / HAProxy</a> comme couche proxy entre les clients et les n≈ìuds d'enregistrement.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le code source</a> du GLB Director est ouvert; </li><li>  <code>anycast</code> technologie <code>anycast</code> pour le routage r√©seau. </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/762/fb4/7a0/762fb47a0de253cce045889faa945228.png" alt="image"></p><br><p>  Le nouveau sch√©ma a permis d'abandonner compl√®tement les modifications apport√©es au VIP et au DNS.  D√©sormais, lors de l'introduction de nouveaux composants, nous pouvons les s√©parer et simplifier la t√¢che.  De plus, nous avons eu l'opportunit√© d'utiliser des solutions fiables et stables.  Une analyse d√©taill√©e de la nouvelle solution est donn√©e ci-dessous. </p><br><h3 id="normalnyy-potok">  D√©bit normal </h3><br><p>  Dans une situation normale, les applications se connectent aux n≈ìuds d'enregistrement via GLB / HAProxy. </p><br><p>  Les applications ne re√ßoivent pas l'identit√© du serveur principal.  Comme auparavant, ils n'utilisent que le nom.  Par exemple, le n≈ìud principal de <code>cluster1</code> serait <code>mysql-writer-1.github.net</code> .  Cependant, dans notre configuration actuelle, ce nom se r√©sout en l'adresse IP <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">anycast</a> . </p><br><p>  Gr√¢ce √† la technologie <code>anycast</code> , le nom est r√©solu √† la m√™me adresse IP n'importe o√π, mais le trafic est dirig√© diff√©remment, compte tenu de l'emplacement du client.  En particulier, plusieurs instances de GLB, notre √©quilibreur de charge hautement disponible, sont d√©ploy√©es dans chacun de nos centres de donn√©es.  Le trafic sur <code>mysql-writer-1.github.net</code> toujours achemin√© vers le cluster GLB du centre de donn√©es local.  Pour cette raison, tous les clients sont servis par des mandataires locaux. </p><br><p>  Nous ex√©cutons GLB sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HAProxy</a> .  Notre serveur HAProxy fournit <em>des pools d'√©criture</em> : un pour chaque cluster MySQL.  De plus, chaque pool ne poss√®de qu'un seul serveur (le n≈ìud <em>principal</em> du cluster).  Toutes les instances GLB / HAProxy dans tous les centres de donn√©es ont les m√™mes pools, et elles pointent toutes vers les m√™mes serveurs dans ces pools.  Ainsi, si l'application souhaite √©crire des donn√©es dans la base de donn√©es sur <code>mysql-writer-1.github.net</code> , peu importe le serveur GLB auquel elle se connecte.  Dans les deux cas, une redirection vers le n≈ìud de cluster principal r√©el <code>cluster1</code> sera effectu√©e. </p><br><p>  Pour les applications, la d√©couverte se termine sur GLB et la red√©couverte n'est pas n√©cessaire.  Ce GLB redirige le trafic au bon endroit. </p><br><p>  O√π le GLB obtient-il des informations sur les serveurs √† r√©pertorier?  Comment modifions-nous le GLB? </p><br><h3 id="obnaruzhenie-cherez-consul">  D√©couverte par le consul </h3><br><p>  Le service Consul est largement connu comme une solution de d√©couverte de services, et il prend √©galement en charge les fonctions DNS.  Cependant, dans notre cas, nous l'utilisons comme un stockage hautement accessible des valeurs cl√©s (KV). </p><br><p>  Dans le r√©f√©rentiel KV de Consul, nous enregistrons l'identit√© des n≈ìuds principaux du cluster.  Pour chaque cluster, il existe un ensemble d'enregistrements KV pointant vers les donn√©es du n≈ìud principal correspondant: ses adresses <code>fqdn</code> , port, ipv4 et ipv6. </p><br><p>  Chaque n≈ìud GLB / HAProxy lance un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mod√®le de consul</a> , un service qui suit les changements dans les donn√©es du consul (dans notre cas, les changements dans les donn√©es des n≈ìuds principaux).  Le <code>consul-template</code> cr√©e un fichier de configuration et peut recharger HAProxy lors de la modification des param√®tres. </p><br><p>  Pour cette raison, des informations sur la modification de l'identit√© du n≈ìud principal dans Consul sont disponibles pour chaque instance GLB / HAProxy.  Sur la base de ces informations, la configuration des instances est effectu√©e, les nouveaux n≈ìuds principaux sont indiqu√©s comme la seule entit√© dans le pool de serveurs de cluster.  Apr√®s cela, les instances sont recharg√©es pour que les modifications prennent effet. </p><br><p>  Nous avons d√©ploy√© des instances Consul dans chaque centre de donn√©es et chaque instance offre une haute disponibilit√©.  Cependant, ces instances sont ind√©pendantes les unes des autres.  Ils ne se r√©pliquent pas et n'√©changent aucune donn√©e. </p><br><p>  O√π Consul obtient-il des informations sur les changements et comment est-il distribu√© entre les centres de donn√©es? </p><br><h3 id="orchestratorraft">  orchestrateur / radeau </h3><br><p>  Nous utilisons le sch√©ma <code>orchestrator/raft</code> : <code>orchestrator</code> n≈ìuds d' <code>orchestrator</code> communiquent entre eux par le biais d'un consensus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">radeau</a> .  Dans chaque centre de donn√©es, nous avons un ou deux n≈ìuds d' <code>orchestrator</code> . </p><br><p>  <code>orchestrator</code> est charg√© de d√©tecter les pannes, le basculement MySQL et de transf√©rer les donn√©es modifi√©es du n≈ìud ma√Ætre vers Consul.  Le basculement est g√©r√© par un seul h√¥te <code>orchestrator/raft</code> , mais les <em>modifications</em> , nouvelles que le cluster est d√©sormais un nouveau ma√Ætre, sont propag√©es √† tous <code>orchestrator</code> n≈ìuds d' <code>orchestrator</code> √† l'aide du m√©canisme <code>raft</code> . </p><br><p>  Lorsque les n≈ìuds d' <code>orchestrator</code> re√ßoivent des informations sur une modification des donn√©es du n≈ìud principal, chacun d'eux contacte sa propre instance locale de Consul et lance un enregistrement KV.  Les centres de donn√©es avec plusieurs instances d' <code>orchestrator</code> recevront plusieurs enregistrements (identiques) dans Consul. </p><br><h3 id="obobschennoe-predstavlenie-vsego-potoka">  Vue g√©n√©ralis√©e de l'ensemble du flux </h3><br><p>  Si le n≈ìud ma√Ætre √©choue: </p><br><ul><li>  <code>orchestrator</code> n≈ìuds d' <code>orchestrator</code> d√©tectent les pannes; </li><li>  <code>orchestrator/raft</code> master lance la r√©cup√©ration.  Un nouveau n≈ìud ma√Ætre est attribu√©; </li><li>  le sch√©ma <code>orchestrator/raft</code> transf√®re les donn√©es sur le changement du n≈ìud principal √† tous les n≈ìuds du cluster de <code>raft</code> ; </li><li>  chaque instance d' <code>orchestrator/raft</code> re√ßoit une notification concernant un changement de n≈ìud et √©crit l'identit√© du nouveau n≈ìud ma√Ætre dans le stockage KV local dans Consul; </li><li>  sur chaque instance GLB / HAProxy, le service de <code>consul-template</code> est lanc√©, qui surveille les changements dans le r√©f√©rentiel KV dans Consul, reconfigure et red√©marre HAProxy; </li><li>  Le trafic client est redirig√© vers le nouveau n≈ìud ma√Ætre. </li></ul><br><p>  Pour chaque composante, les responsabilit√©s sont clairement r√©parties et l'ensemble de la structure est diversifi√© et simplifi√©.  <code>orchestrator</code> n'interagit pas avec les √©quilibreurs de charge.  Le consul n'a pas besoin d'informations sur l'origine des informations.  Les serveurs proxy fonctionnent uniquement avec Consul.  Les clients ne fonctionnent qu'avec des serveurs proxy. </p><br><p>  De plus: </p><br><ul><li>  Pas besoin d'apporter des modifications au DNS et de diffuser des informations √† leur sujet; </li><li>  TTL n'est pas utilis√©; </li><li>  le thread n'attend pas les r√©ponses de l'h√¥te dans un √©tat d'erreur.  En g√©n√©ral, il est ignor√©. </li></ul><br><h3 id="dopolnitelnaya-informaciya">  Information additionnelle </h3><br><p>  Pour stabiliser le flux, nous appliquons √©galement les m√©thodes suivantes: </p><br><ul><li>  Le param√®tre d' <code>hard-stop-after</code> fixe HAProxy est d√©fini sur une tr√®s petite valeur.  Lorsque HAProxy red√©marre avec le nouveau serveur dans le pool d'√©criture, le serveur met automatiquement fin √† toutes les connexions existantes √† l'ancien n≈ìud ma√Ætre. <br><ol><li>  La d√©finition du param√®tre d' <code>hard-stop-after</code> urgence vous permet de ne pas attendre d'actions des clients, en outre, les cons√©quences n√©gatives de l'occurrence possible de deux n≈ìuds principaux dans le cluster sont minimis√©es.  Il est important de comprendre qu'il n'y a pas de magie ici, et en tout cas, un <em>certain temps</em> s'√©coule avant que les anciens liens ne soient rompus.  Mais il y a un moment o√π nous pouvons cesser d'attendre des surprises d√©sagr√©ables. </li></ol></li><li>  Nous n'exigeons pas la disponibilit√© continue du service Consul.  En fait, nous avons besoin qu'il soit disponible uniquement pendant le basculement.  Si le service Consul ne r√©pond pas, alors GLB continue de travailler avec les derni√®res valeurs connues et ne prend pas de mesures drastiques. </li><li>  Le GLB est configur√© pour v√©rifier l'identit√© du n≈ìud ma√Ætre nouvellement affect√©.  Comme pour nos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pools MySQL contextuels</a> , une v√©rification est effectu√©e pour confirmer que le serveur est bien accessible en √©criture.  Si nous supprimons accidentellement l'identit√© du n≈ìud principal dans Consul, il n'y aura aucun probl√®me, un enregistrement vide sera ignor√©.  Si nous √©crivons par erreur le nom d'un autre serveur (pas le principal) dans Consul, alors dans ce cas, tout va bien: GLB ne le mettra pas √† jour et continuera de fonctionner avec le dernier √©tat valide. </li></ul><br><p>  Dans les sections suivantes, nous examinons les probl√®mes et analysons les objectifs de haute disponibilit√©. </p><br><h3 id="obnaruzhenie-sboev-s-pomoschyu-orchestratorraft">  D√©tection de crash avec orchestrateur / radeau </h3><br><p>  <code>orchestrator</code> adopte une <a href="">approche globale</a> de la d√©tection des pannes, ce qui garantit une grande fiabilit√© de l'outil.  Nous ne rencontrons pas de faux r√©sultats positifs, les pannes pr√©matur√©es ne sont pas effectu√©es, ce qui signifie que les temps d'arr√™t inutiles sont exclus. </p><br><p>  Les circuits d' <code>orchestrator/raft</code> √©galement face √† des situations d'isolement complet du r√©seau du centre de donn√©es (cl√¥ture du centre de donn√©es).  L'isolement du r√©seau du centre de donn√©es peut √™tre source de confusion: les serveurs √† l'int√©rieur du centre de donn√©es peuvent communiquer entre eux.  Comment comprendre qui est vraiment isol√© - les serveurs d' <em>un</em> centre de donn√©es <em>donn√©</em> ou de tous les <em>autres</em> centres de donn√©es? </p><br><p>  Dans le sch√©ma <code>orchestrator/raft</code> , le ma√Ætre de <code>orchestrator/raft</code> est le basculement.  Le n≈ìud devient le leader, qui re√ßoit le soutien de la majorit√© du groupe (quorum).  Nous avons d√©ploy√© le n≈ìud d' <code>orchestrator</code> de telle mani√®re qu'aucun centre de donn√©es unique ne peut fournir la majorit√©, tandis que n'importe quel centre de donn√©es <code>n-1</code> peut le fournir. </p><br><p>  Dans le cas d'une isolation compl√®te du r√©seau du centre de donn√©es, les n≈ìuds d' <code>orchestrator</code> de ce centre sont d√©connect√©s des n≈ìuds similaires d'autres centres de donn√©es.  Par cons√©quent, les n≈ìuds d' <code>orchestrator</code> d'un centre de donn√©es isol√© ne peuvent pas devenir les leaders d'un cluster de <code>raft</code> .  Si un tel n≈ìud √©tait le ma√Ætre, il perd ce statut.  Un nouvel h√¥te se verra attribuer l'un des n≈ìuds des autres centres de donn√©es.  Ce leader aura le soutien de tous les autres centres de donn√©es qui peuvent interagir les uns avec les autres. </p><br><p>  De cette fa√ßon, le ma√Ætre d' <code>orchestrator</code> sera toujours en dehors du centre de donn√©es isol√© du r√©seau.  Si le n≈ìud ma√Ætre se trouvait dans le centre de donn√©es isol√©, <code>orchestrator</code> lance un basculement pour le remplacer par le serveur de l'un des centres de donn√©es disponibles.  Nous att√©nuons l'impact de l'isolement des centres de donn√©es en d√©l√©guant des d√©cisions au quorum des centres de donn√©es disponibles. </p><br><h3 id="uskorennoe-opoveschenie">  Notification plus rapide </h3><br><p>  Le temps d'indisponibilit√© total peut √™tre encore r√©duit en acc√©l√©rant la notification d'un changement dans le n≈ìud principal.  Comment y parvenir? </p><br><p>  Lorsque l' <code>orchestrator</code> d√©marre le basculement, il consid√®re un groupe de serveurs, dont l'un peut √™tre affect√© comme serveur principal.  Compte tenu des r√®gles de r√©plication, des recommandations et des limites, il est en mesure de prendre une d√©cision √©clair√©e sur la meilleure ligne de conduite. </p><br><p>  Selon les signes suivants, il peut √©galement comprendre qu'un serveur accessible est <em>un candidat id√©al</em> pour une nomination comme principal: </p><br><ul><li>  rien n'emp√™che le serveur de devenir √©lev√© (et peut-√™tre que l'utilisateur recommande ce serveur); </li><li>  il est pr√©vu que le serveur pourra utiliser tous les autres serveurs comme r√©pliques. </li></ul><br><p>  Dans ce cas, <code>orchestrator</code> configure d'abord le serveur comme accessible en √©criture et annonce imm√©diatement une augmentation de son √©tat (dans notre cas, il √©crit l'enregistrement dans le r√©f√©rentiel KV dans Consul).   orchestrator     ,     . </p><br><p>  ,    ,    GLB   ,     ,     .   :    ! </p><br><h3 id="polusinhronnaya-replikaciya">   </h3><br><p>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> MySQL         ,           .       :  ,    ,   ,        . </p><br><p>     ,      .        ,    ,   .  ,    ,           ,    . </p><br><p>       : <code>500 </code> .                    .          (    ),          . </p><br><p>                   (   )    .           ,      . </p><br><p>       ,        <em> </em>     .            <em></em> ,      ,  <em></em>    .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,       <em> </em> , ,       . </p><br><h3 id="peredacha-paketov-pulsa">    </h3><br><p>  ,   /  <code>pt-heartbeat</code>  /  ,       .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,   <code>pt-heartbeat</code>     ,       <code>read_only</code> ,    . </p><br><p>      <code>pt-heartbeat</code>     ,     .       .               .     ,  <code>pt-heartbeat</code>              . </p><br><h3 id="delegirovanie-zadach-orchestrator">   orchestrator </h3><br><p>    orchestrator  : </p><br><ul><li>  Pseudo-GTID; </li><li>       ,    ; </li><li>         ( <code>read_only</code> ),   . </li></ul><br><p>    ,     . ,      ,      ,      .     <code>orchestrator</code>         . </p><br><h3 id="ogranicheniya-i-nedostatki">    </h3><br><p>  -   ,        ,         .     ,   -,         . </p><br><p>     ,       . </p><br><p> ,      ,     ,     -      .         .               <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">STONITH</a>    .    ,  <em> </em> ,       ,    ¬´¬ª -   .  ,       ,  . </p><br><p>    :  Consul    ,     . .  , ,      ,    ,      . </p><br><h3 id="rezultaty">  </h3><br><p>   orchestrator/GLB/Consul   : </p><br><ul><li>   ; </li><li>      ; </li><li>       ; </li><li>    ; </li><li>  ,      (    ); </li><li>    ; </li><li>    <code>10-13 </code>   . <br><ol><li>        <code>20 </code> ,      ‚Äî <code>25 </code> . </li></ol></li></ul><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  ¬´// ¬ª         ,   ,   .       .     ,    . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr432088/">https://habr.com/ru/post/fr432088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr432078/index.html">Trafic au bout du tunnel ou DNS dans le pentest</a></li>
<li><a href="../fr432080/index.html">Id√©es fausses des joueurs lors de l'√©valuation des risques. Contr√¥le du g√©n√©rateur de nombres al√©atoires en d√©veloppement</a></li>
<li><a href="../fr432082/index.html">Microsoft AI Chatbot lance une collection de v√™tements en Chine</a></li>
<li><a href="../fr432084/index.html">Comment nous avons organis√© une comp√©tition par √©quipes entre les travailleurs de la production (comme en URSS)</a></li>
<li><a href="../fr432086/index.html">Impression 3D √† l'√©cole internationale du nom de M.V. Lomonosov</a></li>
<li><a href="../fr432090/index.html">Magento Meetup Kharkiv No. 4 - rapports vid√©o</a></li>
<li><a href="../fr432092/index.html">Erreurs d√©sagr√©ables lors de l'√©criture des tests unitaires</a></li>
<li><a href="../fr432094/index.html">Hackathon en ligne conjoint d'OpenGift et de la plateforme Blockchain de cr√©dits</a></li>
<li><a href="../fr432096/index.html">Guide CMake complet. Deuxi√®me partie: Build System</a></li>
<li><a href="../fr432098/index.html">Pilotes automatiques dans le transport routier, comment interagir avec les sp√©ciaux. en transport?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>