<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåÜ üöà üéüÔ∏è Neuronale Netze und Deep Learning, Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen ‚õ≤Ô∏è üåë üôçüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Kapitel gebe ich eine einfache und meist visuelle Erkl√§rung des Universalit√§tstheorems. Um dem Material in diesem Kapitel zu folgen, m√ºssen ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und Deep Learning, Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  In diesem Kapitel gebe ich eine einfache und meist visuelle Erkl√§rung des Universalit√§tstheorems.  Um dem Material in diesem Kapitel zu folgen, m√ºssen Sie die vorherigen nicht lesen.  Es ist als eigenst√§ndiger Aufsatz aufgebaut.  Wenn Sie das grundlegendste Verst√§ndnis von NS haben, sollten Sie in der Lage sein, die Erkl√§rungen zu verstehen. <br><br><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  Eine der erstaunlichsten Tatsachen √ºber neuronale Netze ist, dass sie jede Funktion √ºberhaupt berechnen k√∂nnen.  Nehmen wir an, jemand gibt Ihnen eine komplexe und kurvenreiche Funktion f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  Und unabh√§ngig von dieser Funktion ist ein solches neuronales Netzwerk garantiert, dass f√ºr jede Eingabe x der Wert f (x) (oder eine nahe liegende Ann√§herung) die Ausgabe dieses Netzwerks ist, dh: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Dies funktioniert auch dann, wenn es sich um eine Funktion vieler Variablen f = f (x <sub>1</sub> , ..., x <sub>m</sub> ) und mit vielen Werten handelt.  Hier ist zum Beispiel ein Netzwerk, das eine Funktion mit m = 3 Eing√§ngen und n = 2 Ausg√§ngen berechnet: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Dieses Ergebnis legt nahe, dass neuronale Netze eine gewisse Universalit√§t aufweisen.  Egal welche Funktion wir berechnen m√∂chten, wir wissen, dass es ein neuronales Netzwerk gibt, das dies kann. <br><br>  Dar√ºber hinaus gilt der Universalit√§tstheorem auch dann, wenn wir das Netzwerk auf eine einzige Schicht zwischen eingehenden und ausgehenden Neuronen beschr√§nken - die sogenannten  in einer versteckten Schicht.  So k√∂nnen auch Netzwerke mit einer sehr einfachen Architektur extrem leistungsf√§hig sein. <br><br>  Der Universalit√§tstheorem ist Menschen, die neuronale Netze verwenden, gut bekannt.  Obwohl dies so ist, ist ein Verst√§ndnis dieser Tatsache nicht so weit verbreitet.  Und die meisten Erkl√§rungen daf√ºr sind technisch zu komplex.  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einer der ersten Arbeiten,</a> die dieses Ergebnis belegen, wurden beispielsweise der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hahn-Banach-Satz</a> , der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Riesz-Repr√§sentationssatz</a> und einige Fourier-Analysen verwendet.  Wenn Sie Mathematiker sind, ist es f√ºr Sie leicht, diese Beweise zu verstehen, aber f√ºr die meisten Menschen ist es nicht so einfach.  Schade, denn die Hauptgr√ºnde f√ºr die Universalit√§t sind einfach und sch√∂n. <br><br>  In diesem Kapitel gebe ich eine einfache und meist visuelle Erkl√§rung des Universalit√§tstheorems.  Wir werden Schritt f√ºr Schritt die zugrunde liegenden Ideen durchgehen.  Sie werden verstehen, warum neuronale Netze wirklich jede Funktion berechnen k√∂nnen.  Sie werden einige der Einschr√§nkungen dieses Ergebnisses verstehen.  Und Sie werden verstehen, wie das Ergebnis mit tiefem NS verbunden ist. <br><br>  Um dem Material in diesem Kapitel zu folgen, m√ºssen Sie die vorherigen nicht lesen.  Es ist als eigenst√§ndiger Aufsatz aufgebaut.  Wenn Sie das grundlegendste Verst√§ndnis von NS haben, sollten Sie in der Lage sein, die Erkl√§rungen zu verstehen.  Aber ich werde manchmal Links zu vorherigem Material bereitstellen, um Wissensl√ºcken zu schlie√üen. <br><br>  Universals√§tze finden sich oft in der Informatik, deshalb vergessen wir manchmal sogar, wie erstaunlich sie sind.  Aber es lohnt sich, sich daran zu erinnern: Die F√§higkeit, eine beliebige Funktion zu berechnen, ist wirklich erstaunlich.  Fast jeder Prozess, den Sie sich vorstellen k√∂nnen, kann auf die Berechnung einer Funktion reduziert werden.  Betrachten Sie die Aufgabe, den Namen einer Musikkomposition anhand einer kurzen Passage zu finden.  Dies kann als Funktionsberechnung angesehen werden.  Oder √ºberlegen Sie, ob Sie einen chinesischen Text ins Englische √ºbersetzen m√∂chten.  Und dies kann als Funktionsberechnung betrachtet werden (in der Tat viele Funktionen, da es viele akzeptable Optionen f√ºr die √úbersetzung eines einzelnen Textes gibt).  Oder betrachten Sie die Aufgabe, eine Beschreibung der Handlung des Films und der Qualit√§t des Schauspiels basierend auf der mp4-Datei zu erstellen.  Auch dies kann als Berechnung einer bestimmten Funktion angesehen werden (die Bemerkung zu den Text√ºbersetzungsoptionen ist auch hier richtig).  Universalit√§t bedeutet, dass NS im Prinzip alle diese und viele andere Aufgaben ausf√ºhren k√∂nnen. <br><br>  Nur aus der Tatsache, dass wir wissen, dass es NS gibt, die beispielsweise vom Chinesischen ins Englische √ºbersetzen k√∂nnen, folgt nat√ºrlich nicht, dass wir √ºber gute Techniken zum Erstellen oder sogar Erkennen eines solchen Netzwerks verf√ºgen.  Diese Einschr√§nkung gilt auch f√ºr traditionelle Universalit√§tstheoreme f√ºr Modelle wie Boolesche Schemata.  Wie wir bereits in diesem Buch gesehen haben, verf√ºgt der NS √ºber leistungsstarke Algorithmen zum Lernen von Funktionen.  Die Kombination aus Lernalgorithmen und Vielseitigkeit ist eine attraktive Mischung.  Bisher haben wir uns in dem Buch auf Trainingsalgorithmen konzentriert.  In diesem Kapitel konzentrieren wir uns auf die Vielseitigkeit und deren Bedeutung. <br><br><h2>  Zwei Tricks </h2><br>  Bevor ich erkl√§re, warum der Universalit√§tstheorem wahr ist, m√∂chte ich zwei Tricks erw√§hnen, die in der informellen Aussage ‚ÄûEin neuronales Netzwerk kann jede Funktion berechnen‚Äú enthalten sind. <br><br>  Erstens bedeutet dies nicht, dass das Netzwerk verwendet werden kann, um eine Funktion genau zu berechnen.  Wir k√∂nnen nur eine so gute Ann√§herung bekommen, wie wir brauchen.  Indem wir die Anzahl der versteckten Neuronen erh√∂hen, verbessern wir die Approximation.  Zum Beispiel habe ich zuvor ein Netzwerk dargestellt, das eine bestimmte Funktion f (x) unter Verwendung von drei versteckten Neuronen berechnet.  F√ºr die meisten Funktionen kann unter Verwendung von drei Neuronen nur eine Ann√§herung von geringer Qualit√§t erhalten werden.  Durch Erh√∂hen der Anzahl versteckter Neuronen (z. B. bis zu f√ºnf) k√∂nnen wir normalerweise eine verbesserte Ann√§herung erhalten: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Und um die Situation zu verbessern, indem die Anzahl der versteckten Neuronen weiter erh√∂ht wird. <br><br>  Nehmen wir zur Verdeutlichung dieser Aussage an, wir h√§tten eine Funktion f (x) erhalten, die wir mit der notwendigen Genauigkeit Œµ&gt; 0 berechnen wollen.  Es besteht die Garantie, dass bei Verwendung einer ausreichenden Anzahl versteckter Neuronen immer ein NS gefunden werden kann, dessen Ausgabe g (x) die Gleichung | g (x) - f (x) | &lt;Œµ f√ºr jedes x erf√ºllt.  Mit anderen Worten wird die Ann√§herung mit der gew√ºnschten Genauigkeit f√ºr jeden m√∂glichen Eingabewert erreicht. <br><br>  Der zweite Haken ist, dass Funktionen, die mit der beschriebenen Methode approximiert werden k√∂nnen, zu einer kontinuierlichen Klasse geh√∂ren.  Wenn die Funktion unterbrochen wird, dh pl√∂tzlich scharfe Spr√ºnge macht, ist es im allgemeinen Fall unm√∂glich, mit Hilfe von NS eine Ann√§herung vorzunehmen.  Dies ist nicht √ºberraschend, da unsere NS kontinuierliche Funktionen von Eingabedaten berechnen.  Selbst wenn die Funktion, die wir wirklich berechnen m√ºssen, diskontinuierlich ist, ist die Approximation oft ziemlich kontinuierlich.  Wenn ja, dann k√∂nnen wir NS verwenden.  In der Praxis ist diese Einschr√§nkung normalerweise nicht wichtig. <br><br>  Infolgedessen wird eine genauere Aussage des Universalit√§tstheorems sein, dass NS mit einer verborgenen Schicht verwendet werden kann, um jede kontinuierliche Funktion mit jeder gew√ºnschten Genauigkeit zu approximieren.  In diesem Kapitel beweisen wir eine etwas weniger strenge Version dieses Theorems, bei der zwei verborgene Schichten anstelle einer verwendet werden.  In Aufgaben werde ich kurz beschreiben, wie diese Erkl√§rung mit geringf√ºgigen √Ñnderungen an einen Beweis angepasst werden kann, der nur eine verborgene Ebene verwendet. <br><br><h2>  Vielseitigkeit mit einem Eingabe- und einem Ausgabewert </h2><br>  Um zu verstehen, warum der Universalit√§tstheorem wahr ist, verstehen wir zun√§chst, wie eine NS-Approximationsfunktion mit nur einem Eingabe- und einem Ausgabewert erstellt wird: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Es stellt sich heraus, dass dies die Essenz der Aufgabe der Universalit√§t ist.  Sobald wir diesen Sonderfall verstanden haben, wird es ziemlich einfach sein, ihn auf Funktionen mit vielen Eingabe- und Ausgabewerten zu erweitern. <br><br>  Um zu verstehen, wie ein Netzwerk zum Z√§hlen von f aufgebaut wird, beginnen wir mit einem Netzwerk, das eine einzelne verborgene Schicht mit zwei verborgenen Neuronen und eine Ausgangsschicht mit einem Ausgangsneuron enth√§lt: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Um uns vorzustellen, wie die Netzwerkkomponenten funktionieren, konzentrieren wir uns auf das obere versteckte Neuron.  Im Diagramm des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikels k√∂nnen</a> Sie das Gewicht interaktiv mit der Maus √§ndern, indem Sie auf ‚Äûw‚Äú klicken und sofort sehen, wie sich die vom oberen versteckten Neuron berechnete Funktion √§ndert: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Wie wir fr√ºher in diesem Buch erfahren haben, z√§hlt ein verstecktes Neuron œÉ (wx + b), wobei œÉ (z) ‚â° 1 / (1 + e <sup>‚àíz</sup> ) ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sigmoid ist</a> .  Bisher haben wir diese algebraische Form ziemlich oft verwendet.  Um die Universalit√§t zu beweisen, ist es jedoch besser, diese Algebra vollst√§ndig zu ignorieren und stattdessen die Form im Diagramm zu manipulieren und zu beobachten.  Dies hilft Ihnen nicht nur, besser zu f√ºhlen, was passiert, sondern gibt uns auch einen Beweis f√ºr die Universalit√§t, die f√ºr andere Aktivierungsfunktionen neben Sigmoid gilt. <br><br>  Genau genommen wird der von mir gew√§hlte visuelle Ansatz traditionell nicht als Beweis angesehen.  Ich glaube jedoch, dass der visuelle Ansatz mehr Einblick in die Wahrheit des Endergebnisses bietet als herk√∂mmliche Beweise.  Und nat√ºrlich ist ein solches Verst√§ndnis der eigentliche Zweck des Beweises.  In den von mir vorgeschlagenen Beweisen treten gelegentlich L√ºcken auf;  Ich werde vern√ºnftige, aber nicht immer strenge visuelle Beweise liefern.  Wenn Sie dies st√∂rt, betrachten Sie es als Ihre Aufgabe, diese L√ºcken zu schlie√üen.  Verlieren Sie jedoch nicht das Hauptziel aus den Augen: zu verstehen, warum der Universalit√§tstheorem wahr ist. <br><br>  Klicken Sie zun√§chst auf den Versatz b im Originaldiagramm und ziehen Sie ihn nach rechts, um ihn zu vergr√∂√üern.  Sie werden sehen, dass sich das Diagramm mit zunehmendem Versatz nach links bewegt, aber seine Form nicht √§ndert. <br><br>  Ziehen Sie es dann nach links, um den Versatz zu verringern.  Sie werden sehen, dass sich das Diagramm nach rechts bewegt, ohne die Form zu √§ndern. <br><br>  Gewicht auf 2-3 reduzieren.  Sie werden sehen, dass sich die Kurve mit abnehmendem Gewicht gerade richtet.  Damit die Kurve nicht vom Diagramm abweicht, m√ºssen Sie m√∂glicherweise den Versatz korrigieren. <br><br>  Erh√∂hen Sie schlie√ülich das Gewicht auf Werte gr√∂√üer als 100. Die Kurve wird steiler und n√§hert sich schlie√ülich dem Schritt.  Stellen Sie den Versatz so ein, dass sein Winkel im Bereich des Punktes x = 0,3 liegt.  Das folgende Video zeigt, was passieren soll: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Wir k√∂nnen unsere Analyse erheblich vereinfachen, indem wir das Gewicht erh√∂hen, sodass die Ausgabe wirklich eine gute Ann√§herung an die Schrittfunktion darstellt.  Unten habe ich die Ausgabe des oberen versteckten Neurons f√ºr das Gewicht w = 999 erstellt.  Dies ist ein statisches Bild: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Die Verwendung von Schrittfunktionen ist etwas einfacher als bei einem typischen Sigmoid.  Der Grund ist, dass Beitr√§ge aller versteckten Neuronen in der Ausgabeschicht addiert werden.  Die Summe einer Reihe von Schrittfunktionen ist leicht zu analysieren, es ist jedoch schwieriger, dar√ºber zu sprechen, was passiert, wenn eine Reihe von Kurven in Form eines Sigmoid hinzugef√ºgt wird.  Daher ist es viel einfacher anzunehmen, dass unsere versteckten Neuronen schrittweise Funktionen produzieren.  Genauer gesagt fixieren wir dazu das Gewicht w auf einen sehr gro√üen Wert und weisen dann die Position der Stufe durch den Versatz zu.  Nat√ºrlich ist die Arbeit mit einer Ausgabe als Schrittfunktion eine Ann√§herung, aber sie ist sehr gut, und bis jetzt werden wir die Funktion als echte Schrittfunktion behandeln.  Sp√§ter werde ich auf die Auswirkung von Abweichungen von dieser Ann√§herung zur√ºckkommen. <br><br>  Welcher Wert von x ist der Schritt?  Mit anderen Worten, wie h√§ngt die Position der Stufe von Gewicht und Verschiebung ab? <br><br>  Versuchen Sie zur Beantwortung der Frage, das Gewicht und den Versatz im interaktiven Diagramm zu √§ndern.  K√∂nnen Sie verstehen, wie die Position des Schritts von w und b abh√§ngt?  Wenn Sie ein wenig √ºben, k√∂nnen Sie sich selbst davon √ºberzeugen, dass seine Position proportional zu b und umgekehrt proportional zu w ist. <br><br>  Tats√§chlich liegt der Schritt bei s = ‚Äìb / w, wie zu sehen sein wird, wenn wir das Gewicht und die Verschiebung auf die folgenden Werte einstellen: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Unser Leben wird stark vereinfacht, wenn wir versteckte Neuronen mit einem einzigen Parameter s beschreiben, dh durch die Position des Schritts s = ‚àíb / w.  Im folgenden interaktiven Diagramm k√∂nnen Sie einfach s √§ndern: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Wie oben erw√§hnt, haben wir einem sehr gro√üen Wert speziell ein Gewicht w am Eingang zugewiesen - gro√ü genug, damit die Schrittfunktion eine gute Ann√§herung darstellt.  Und wir k√∂nnen das parametrisierte Neuron auf diese Weise leicht in seine √ºbliche Form zur√ºckversetzen, indem wir den Bias b = ‚àíws w√§hlen. <br><br>  Bisher haben wir uns nur auf die Ausgabe des √ºberlegenen versteckten Neurons konzentriert.  Betrachten wir das Verhalten des gesamten Netzwerks.  Angenommen, versteckte Neuronen berechnen die Schrittfunktionen, die durch die Parameter der Schritte s <sub>1</sub> (oberes Neuron) und s <sub>2</sub> (unteres Neuron) definiert sind.  Ihre jeweiligen Ausgangsgewichte sind w <sub>1</sub> und w <sub>2</sub> .  Hier ist unser Netzwerk: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  Rechts ist ein Diagramm der gewichteten Ausgabe w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 der</sub> verborgenen Schicht.  Hier sind a <sub>1</sub> und a <sub>2</sub> die Ausg√§nge der oberen bzw. unteren versteckten Neuronen.  Sie werden mit "a" bezeichnet, da sie oft als neuronale Aktivierungen bezeichnet werden. <br><br>  √úbrigens stellen wir fest, dass der Ausgang des gesamten Netzwerks œÉ ist (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), wobei b die Vorspannung des Ausgangsneurons ist.  Dies ist offensichtlich nicht dasselbe wie die gewichtete Ausgabe der verborgenen Ebene, deren Diagramm wir erstellen.  Im Moment konzentrieren wir uns jedoch auf die ausgeglichene Ausgabe der verborgenen Schicht und denken erst sp√§ter dar√ºber nach, wie sie sich auf die Ausgabe des gesamten Netzwerks bezieht. <br><br>  Versuchen Sie, den Schritt s <sub>1 des</sub> oberen versteckten Neurons im interaktiven Diagramm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im Originalartikel</a> zu erh√∂hen und zu verringern.  Sehen Sie, wie dies die gewichtete Ausgabe der verborgenen Ebene √§ndert.  Es ist besonders n√ºtzlich zu verstehen, was passiert, wenn s <sub>1</sub> s <sub>2</sub> √ºberschreitet.  Sie werden sehen, dass der Graph in diesen F√§llen seine Form √§ndert, wenn wir von einer Situation, in der das obere versteckte Neuron zuerst aktiviert wird, zu einer Situation √ºbergehen, in der das untere versteckte Neuron zuerst aktiviert wird. <br><br>  Versuchen Sie in √§hnlicher Weise, den Schritt s <sub>2 des</sub> unteren verborgenen Neurons zu manipulieren, und sehen Sie, wie dies die Gesamtleistung der verborgenen Neuronen ver√§ndert. <br><br>  Versuchen Sie, die Ausgangsgewichte zu reduzieren und zu erh√∂hen.  Beachten Sie, wie dies den Beitrag der entsprechenden versteckten Neuronen skaliert.  Was passiert, wenn eines der Gewichte gleich 0 ist? <br><br>  Versuchen Sie abschlie√üend, w <sub>1</sub> auf 0,8 und w <sub>2</sub> auf -0,8 einzustellen.  Das Ergebnis ist eine "Vorsprung" -Funktion mit einem Anfang bei s <sub>1</sub> , einem Ende bei s <sub>2</sub> und einer H√∂he von 0,8.  Eine gewichtete Ausgabe k√∂nnte beispielsweise folgenderma√üen aussehen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Nat√ºrlich kann der Vorsprung auf jede H√∂he skaliert werden.  Verwenden wir einen Parameter, h, der die H√∂he bezeichnet.  Der Einfachheit halber werde ich auch die Notation "s <sub>1</sub> = ..." und "w <sub>1</sub> = ..." entfernen. <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Versuchen Sie, den h-Wert zu erh√∂hen und zu verringern, um zu sehen, wie sich die H√∂he des Vorsprungs √§ndert.  Versuchen Sie, h negativ zu machen.  Versuchen Sie, die Punkte der Schritte zu √§ndern, um zu beobachten, wie sich dadurch die Form des Vorsprungs √§ndert. <br><br>  Sie werden sehen, dass wir unsere Neuronen nicht nur als grafische Grundelemente verwenden, sondern auch als Einheiten, die Programmierern vertrauter sind - so etwas wie eine Wenn-Dann-Sonst-Anweisung in der Programmierung: <br><br>  wenn Eingabe&gt; = Schrittbeginn: <br>  addiere 1 zur gewichteten Ausgabe <br>  sonst: <br>  Addiere 0 zur gewichteten Ausgabe <br><br>  Zum gr√∂√üten Teil werde ich mich an die grafische Notation halten.  Manchmal ist es jedoch hilfreich, zur Wenn-Dann-Sonst-Ansicht zu wechseln und dar√ºber nachzudenken, was in diesen Begriffen geschieht. <br><br>  Wir k√∂nnen unseren Protrusionstrick verwenden, indem wir zwei Teile versteckter Neuronen im selben Netzwerk zusammenkleben: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Hier lie√ü ich die Gewichte fallen, indem ich einfach die h-Werte f√ºr jedes Paar versteckter Neuronen aufschrieb.  Versuchen Sie, mit beiden h-Werten zu spielen, und sehen Sie, wie sich das Diagramm √§ndert.  Verschieben Sie die Registerkarten und √§ndern Sie die Punkte der Schritte. <br><br>  In einem allgemeineren Fall kann diese Idee verwendet werden, um eine beliebige Anzahl von Spitzen einer beliebigen H√∂he zu erhalten.  Insbesondere k√∂nnen wir das Intervall [0,1] in eine gro√üe Anzahl von (N) Teilintervallen unterteilen und N Paare versteckter Neuronen verwenden, um Peaks beliebiger H√∂he zu erhalten.  Mal sehen, wie das bei N = 5 funktioniert.  Dies sind bereits ziemlich viele Neuronen, daher bin ich etwas enger dargestellt.  Entschuldigung f√ºr das komplexe Diagramm - ich k√∂nnte die Komplexit√§t hinter zus√§tzlichen Abstraktionen verbergen, aber es scheint mir, dass es eine kleine Qual mit Komplexit√§t wert ist, um besser zu f√ºhlen, wie die neuronalen Netze funktionieren. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Sie sehen, wir haben f√ºnf Paare versteckter Neuronen.  Die Punkte der Schritte der entsprechenden Paare liegen bei 0,1 / 5, dann bei 1 / 5,2 / 5 usw. bis zu 4 / 5,5 / 5.  Diese Werte sind fest - wir erhalten f√ºnf Vorspr√ºnge gleicher Breite in der Grafik. <br><br>  Jedem Neuronenpaar ist ein Wert h zugeordnet.  Denken Sie daran, dass Ausgangsneuronenverbindungen die Gewichte h und ‚Äìh haben.  Im Originalartikel im Diagramm k√∂nnen Sie auf die h-Werte klicken und sie von links nach rechts verschieben.  Mit einer √Ñnderung der H√∂he √§ndert sich auch der Zeitplan.  Durch √Ñndern der Ausgabegewichte konstruieren wir die endg√ºltige Funktion! <br><br>  Im Diagramm k√∂nnen Sie weiterhin auf das Diagramm klicken und die H√∂he der Schritte nach oben oder unten ziehen.  Wenn Sie die H√∂he √§ndern, sehen Sie, wie sich die H√∂he des entsprechenden h √§ndert.  Die Ausgabegewichte + h und ‚Äìh √§ndern sich entsprechend.  Mit anderen Worten, wir manipulieren direkt eine Funktion, deren Grafik rechts angezeigt wird, und sehen diese √Ñnderungen in den Werten von h links.  Sie k√∂nnen auch die Maustaste an einem der Vorspr√ºnge gedr√ºckt halten und dann die Maus nach links oder rechts ziehen. Die Vorspr√ºnge werden dann an die aktuelle H√∂he angepasst. <br><br>  Es ist Zeit, die Arbeit zu erledigen. <br><br>  Erinnern Sie sich an die Funktion, die ich ganz am Anfang des Kapitels gezeichnet habe: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Dann habe ich das nicht erw√§hnt, aber tats√§chlich sieht es so aus: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>15</mn><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&nbsp;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mn>50</mn><mi>x</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>113</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Es ist f√ºr x-Werte von 0 bis 1 konstruiert und Werte entlang der y-Achse variieren von 0 bis 1. <br><br>  Offensichtlich ist diese Funktion nicht trivial.  Und Sie m√ºssen herausfinden, wie Sie es mithilfe neuronaler Netze berechnen k√∂nnen. <br><br>  In unseren obigen neuronalen Netzen haben wir eine gewichtete Kombination ‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> der Ausgabe versteckter Neuronen analysiert.  Wir wissen, wie wir diesen Wert ma√ügeblich kontrollieren k√∂nnen.  Wie bereits erw√§hnt, entspricht dieser Wert jedoch nicht der Netzwerkausgabe.  Die Ausgabe des Netzwerks ist œÉ (‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), wobei b die Verschiebung des Ausgangsneurons ist.  K√∂nnen wir direkt die Kontrolle √ºber die Netzwerkausgabe erlangen? <br><br>  Die L√∂sung besteht darin, ein neuronales Netzwerk zu entwickeln, in dem die gewichtete Ausgabe der verborgenen Schicht durch die Gleichung œÉ <sup>‚àí1</sup> ‚ãÖf (x) gegeben ist, wobei œÉ <sup>‚àí1</sup> die Umkehrfunktion von œÉ ist.  Das hei√üt, wir m√∂chten, dass die gewichtete Ausgabe der verborgenen Ebene folgenderma√üen aussieht: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn dies erfolgreich ist, ist die Ausgabe des gesamten Netzwerks eine gute Ann√§herung an f (x) (ich setze den Versatz des Ausgangsneurons auf 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann besteht Ihre Aufgabe darin, einen NS zu entwickeln, der sich der oben gezeigten Zielfunktion ann√§hert. Um besser zu verstehen, was passiert, empfehle ich Ihnen, dieses Problem zweimal zu l√∂sen. </font><font style="vertical-align: inherit;">Klicken </font><font style="vertical-align: inherit;">Sie zum ersten Mal im </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalartikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf das Diagramm und passen Sie die H√∂hen der verschiedenen Vorspr√ºnge direkt an. Es wird f√ºr Sie ziemlich einfach sein, eine gute Ann√§herung an die Zielfunktion zu erhalten. Der Approximationsgrad wird durch die durchschnittliche Abweichung, die Differenz zwischen der Zielfunktion und der vom Netzwerk berechneten Funktion gesch√§tzt. Ihre Aufgabe ist es, die durchschnittliche Abweichung auf einen Mindestwert zu bringen. Die Aufgabe gilt als erledigt, wenn die durchschnittliche Abweichung 0,40 nicht √ºberschreitet.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie erfolgreich sind, klicken Sie auf die Schaltfl√§che Zur√ºcksetzen, wodurch die Registerkarten zuf√§llig ge√§ndert werden. Ber√ºhren Sie beim zweiten Mal nicht das Diagramm, sondern √§ndern Sie die h-Werte auf der linken Seite des Diagramms, um die durchschnittliche Abweichung auf einen Wert von 0,40 oder weniger zu bringen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie haben also alle Elemente gefunden, die das Netzwerk ben√∂tigt, um die Funktion f (x) n√§herungsweise zu berechnen! Die Ann√§herung stellte sich als grob heraus, aber wir k√∂nnen das Ergebnis leicht verbessern, indem wir einfach die Anzahl der Paare versteckter Neuronen erh√∂hen, wodurch die Anzahl der Vorspr√ºnge erh√∂ht wird. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Insbesondere ist es einfach, alle gefundenen Daten mit der f√ºr NS verwendeten Parametrierung wieder in die Standardansicht umzuwandeln. Lassen Sie mich schnell daran erinnern, wie das funktioniert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der ersten Schicht haben alle Gewichte einen gro√üen konstanten Wert, zum Beispiel w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verschiebungen versteckter Neuronen werden durch b = ‚àíws berechnet. So wird beispielsweise f√ºr das zweite versteckte Neuron s = 0,2 zu b = ‚Äì1000 √ó 0,2 = ‚Äì200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die letzte Schicht der Skala wird durch die Werte von h bestimmt. So bedeutet beispielsweise der Wert, den Sie f√ºr das erste h w√§hlen, h = -0,2, dass die Ausgabegewichte der beiden oberen versteckten Neuronen -0,2 bzw. 0,2 betragen. Und so weiter f√ºr die gesamte Ebene der Ausgabegewichte. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schlie√ülich ist der Versatz des Ausgangsneurons 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und das war's: Wir haben eine vollst√§ndige Beschreibung des NS erhalten, die die anf√§ngliche Zielfunktion gut berechnet. Und wir verstehen es, die Qualit√§t der Approximation zu verbessern, indem wir die Anzahl der versteckten Neuronen verbessern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Au√üerdem ist in unserer urspr√ºnglichen Zielfunktion f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0,3sin (15x) + 0,05cos (50x) ist nichts Besonderes. </font><font style="vertical-align: inherit;">Ein √§hnliches Verfahren k√∂nnte f√ºr jede kontinuierliche Funktion in den Intervallen von [0,1] bis [0,1] angewendet werden. </font><font style="vertical-align: inherit;">Tats√§chlich verwenden wir unseren einschichtigen NS, um eine Nachschlagetabelle f√ºr eine Funktion zu erstellen. </font><font style="vertical-align: inherit;">Und wir k√∂nnen diese Idee als Grundlage nehmen, um einen allgemeinen Beweis der Universalit√§t zu erhalten.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Funktion vieler Parameter </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir erweitern unsere Ergebnisse auf den Fall einer Reihe von Eingabevariablen. Es klingt kompliziert, aber alle Ideen, die wir brauchen, k√∂nnen bereits f√ºr den Fall mit nur zwei eingehenden Variablen verstanden werden. Daher betrachten wir den Fall mit zwei eingehenden Variablen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schauen wir uns zun√§chst an, was passiert, wenn ein Neuron zwei Eing√§nge hat: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben Eing√§nge x und y mit den entsprechenden Gewichten w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und dem Offset b des Neurons. Wir setzen das Gewicht von w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf 0 und spielen mit dem ersten, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , und versetzen b, um zu sehen, wie sie die Ausgabe des Neurons beeinflussen: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie sehen k√∂nnen </font><font style="vertical-align: inherit;">, beeinflusst die Eingabe y </font><font style="vertical-align: inherit;">bei w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0 nicht die Ausgabe des Neurons. Alles geschieht so, als w√§re x die einzige Eingabe.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was wird Ihrer Meinung nach passieren, wenn wir das Gewicht von w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 </font><font style="vertical-align: inherit;">erh√∂hen </font><font style="vertical-align: inherit;">und w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0 belassen? Wenn Ihnen dies nicht sofort klar ist, denken Sie ein wenig √ºber dieses Problem nach. Dann schauen Sie sich das folgende Video an, das zeigt, was passieren wird:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie zuvor n√§hert sich die Ausgabe mit einer Erh√∂hung des Eingabegewichts der Form der Stufe an. Der Unterschied besteht darin, dass sich unsere Schrittfunktion jetzt in drei Dimensionen befindet. Nach wie vor k√∂nnen wir die Position der Schritte verschieben, indem wir den Versatz √§ndern. Der Winkel liegt am Punkt s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wiederholen wir das Diagramm so, dass der Parameter der Ort des Schritts ist: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir nehmen an, dass das Eingabegewicht von x von gro√üer Bedeutung ist - ich habe w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 verwendet - und das Gewicht w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. Die Zahl auf dem Neuron ist die Position des Schritts, und das x dar√ºber erinnert uns daran, dass wir den Schritt entlang der x-Achse bewegen. Nat√ºrlich ist es durchaus m√∂glich, eine Schrittfunktion entlang der y-Achse zu erhalten, wodurch das eingehende Gewicht f√ºr y gro√ü wird (zum Beispiel w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2)</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), und das Gewicht f√ºr x ist 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Zahl auf dem Neuron gibt wiederum die Position des Schritts an, und y dar√ºber erinnert uns daran, dass wir den Schritt entlang der y-Achse bewegen. Ich k√∂nnte die Gewichte f√ºr x und y direkt bestimmen, habe es aber nicht getan, da dies das Diagramm verunreinigen w√ºrde. Beachten Sie jedoch, dass der y-Marker anzeigt, dass das Gewicht f√ºr y gro√ü und f√ºr x 0 ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir k√∂nnen die soeben entworfenen Schrittfunktionen verwenden, um die dreidimensionale Vorsprungsfunktion zu berechnen. Dazu nehmen wir zwei Neuronen, von denen jedes eine Schrittfunktion entlang der x-Achse berechnet. Dann kombinieren wir diese Schrittfunktionen mit den Gewichten h und ‚Äìh, wobei h die gew√ºnschte Vorsprungsh√∂he ist. All dies ist im folgenden Diagramm zu sehen:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie, den Wert von h zu √§ndern. Sehen Sie, wie es sich auf Netzwerkgewichte bezieht. Und wie sie die H√∂he der Vorsprungsfunktion rechts ver√§ndert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie auch, den Punkt des Schritts zu √§ndern, dessen Wert im oberen versteckten Neuron auf 0,30 eingestellt ist. Sehen Sie, wie sich die Form des Vorsprungs √§ndert. Was passiert, wenn Sie es √ºber den 0,70-Punkt hinaus bewegen, der dem unteren versteckten Neuron zugeordnet ist? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben gelernt, wie man die Vorsprungsfunktion entlang der x-Achse aufbaut. Nat√ºrlich k√∂nnen wir die Vorsprungsfunktion leicht entlang der y-Achse ausf√ºhren, indem wir zwei Schrittfunktionen entlang der y-Achse verwenden. Denken Sie daran, dass wir dies tun k√∂nnen, indem wir am Eingang y gro√üe Gewichte machen und am Eingang x das Gewicht 0 setzen. Und so, was passiert:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es sieht fast identisch mit dem vorherigen Netzwerk aus! Die einzige sichtbare Ver√§nderung sind kleine y-Marker auf versteckten Neuronen. Sie erinnern uns daran, dass sie Schrittfunktionen f√ºr y und nicht f√ºr x erzeugen, sodass das Gewicht am Eingang y sehr gro√ü ist und am Eingang x Null ist und nicht umgekehrt. Nach wie vor habe ich beschlossen, es nicht direkt zu zeigen, um das Bild nicht zu √ºberladen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mal sehen, was passiert, wenn wir zwei Vorsprungsfunktionen hinzuf√ºgen, eine entlang der x-Achse, die andere entlang der y-Achse, beide mit der H√∂he h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um das Verbindungsdiagramm mit dem Gewicht Null zu vereinfachen, habe ich weggelassen. Bisher habe ich kleine x- und y-Marker auf versteckten Neuronen hinterlassen, um mich daran zu erinnern, in welche Richtungen die Protrusionsfunktionen berechnet werden. Sp√§ter werden wir sie ablehnen, da sie durch die eingehende Variable impliziert werden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie, den Parameter h zu √§ndern. </font><font style="vertical-align: inherit;">Wie Sie sehen k√∂nnen, √§ndern sich aus diesem Grund die Ausgangsgewichte sowie die Gewichte der beiden Vorsprungsfunktionen x und y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unsere </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellung ist ein </font><font style="vertical-align: inherit;">bisschen wie eine ‚ÄûTurmfunktion‚Äú: </font><font style="vertical-align: inherit;">Wenn wir solche Turmfunktionen erstellen k√∂nnen, k√∂nnen wir sie verwenden, um beliebige Funktionen zu approximieren, indem wir einfach T√ºrme unterschiedlicher H√∂he an verschiedenen Stellen hinzuf√ºgen: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich haben wir die Erstellung einer beliebigen Turmfunktion noch nicht erreicht. </font><font style="vertical-align: inherit;">Bisher haben wir so etwas wie einen zentralen Turm der H√∂he 2h mit einem Plateau der H√∂he h um ihn herum gebaut. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber wir k√∂nnen einen Turm zum Funktionieren bringen. </font><font style="vertical-align: inherit;">Denken Sie daran, dass wir zuvor gezeigt haben, wie Neuronen verwendet werden k√∂nnen, um die if-then-else-Anweisung zu implementieren:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Es war ein Neuron mit einem Eingang.  Und wir m√ºssen eine √§hnliche Idee auf die kombinierte Ausgabe versteckter Neuronen anwenden: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Wenn wir die richtige Schwelle w√§hlen - zum Beispiel 3h / 2, die zwischen der H√∂he des Plateaus und der H√∂he des zentralen Turms gedr√ºckt wird - k√∂nnen wir das Plateau auf Null dr√ºcken und nur einen Turm belassen. <br><br>  Stellen Sie sich vor, wie das geht?  Versuchen Sie, mit dem folgenden Netzwerk zu experimentieren.  Jetzt zeichnen wir die Ausgabe des gesamten Netzwerks und nicht nur die gewichtete Ausgabe der verborgenen Schicht.  Dies bedeutet, dass wir den Offset-Term zur gewichteten Ausgabe der verborgenen Ebene hinzuf√ºgen und das Sigmoid anwenden.  K√∂nnen Sie die Werte f√ºr h und b finden, f√ºr die Sie einen Turm erhalten?  Wenn Sie an dieser Stelle nicht weiterkommen, sind hier zwei Tipps: (1) Damit das ausgehende Neuron das Wenn-Dann-Sonst-Verhalten zeigt, m√ºssen die eingehenden Gewichte (alle h oder ‚Äìh) gro√ü sein.  (2) Der Wert von b bestimmt die Skala der Wenn-Dann-Sonst-Schwelle. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Mit Standardparametern √§hnelt die Ausgabe einer abgeflachten Version des vorherigen Diagramms mit einem Turm und einem Plateau.  Um das gew√ºnschte Verhalten zu erzielen, m√ºssen Sie den Wert von h erh√∂hen.  Dies gibt uns das Schwellenverhalten von Wenn-Dann-Sonst.  Zweitens muss man b ‚âà ‚àí3h / 2 w√§hlen, um den Schwellenwert korrekt einzustellen. <br><br>  So sieht es f√ºr h = 10 aus: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Selbst f√ºr relativ bescheidene Werte von h erhalten wir eine sch√∂ne Turmfunktion.  Und nat√ºrlich k√∂nnen wir ein beliebig sch√∂nes Ergebnis erzielen, indem wir h weiter erh√∂hen und die Vorspannung auf dem Niveau b = ‚Äì3h / 2 halten. <br><br>  Versuchen wir, zwei Netzwerke zusammenzukleben, um zwei verschiedene Turmfunktionen zu z√§hlen.  Um die jeweiligen Rollen der beiden Subnetze zu verdeutlichen, habe ich sie in separate Rechtecke eingef√ºgt: Jedes von ihnen berechnet die Turmfunktion mit der oben beschriebenen Technik.  Die Grafik rechts zeigt die gewichtete Ausgabe der zweiten verborgenen Schicht, dh die gewichtete Kombination von Turmfunktionen. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  Insbesondere ist zu sehen, dass Sie durch √Ñndern des Gewichts in der letzten Schicht die H√∂he der Ausgangst√ºrme √§ndern k√∂nnen. <br><br>  Mit derselben Idee k√∂nnen Sie so viele T√ºrme berechnen, wie Sie m√∂chten.  Wir k√∂nnen sie beliebig d√ºnn und gro√ü machen.  Infolgedessen garantieren wir, dass sich die gewichtete Ausgabe der zweiten verborgenen Schicht jeder gew√ºnschten Funktion zweier Variablen ann√§hert: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  Insbesondere wenn wir die gewichtete Ausgabe der zweiten verborgenen Schicht zwingen, sich gut œÉ <sup>‚àí1</sup> ‚ãÖf anzun√§hern, garantieren wir, dass die Ausgabe unseres Netzwerks eine gute Ann√§herung an die gew√ºnschte Funktion f ist. <br><br>  Was ist mit den Funktionen vieler Variablen? <br><br>  Versuchen wir drei Variablen zu nehmen: x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Kann das folgende Netzwerk verwendet werden, um die Turmfunktion in vier Dimensionen zu berechnen? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Hier bezeichnen x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> den Netzwerkeingang.  s <sub>1</sub> , t <sub>1</sub> usw. - Schrittpunkte f√ºr Neuronen - das hei√üt, alle Gewichte in der ersten Schicht sind gro√ü, und die Offsets werden so zugewiesen, dass die Punkte der Schritte s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... sind. Die Gewichte in der zweiten Schicht wechseln sich ab, + h, ‚àíh, wobei h eine sehr gro√üe Zahl ist.  Der Ausgangsoffset betr√§gt ‚àí5h / 2. <br><br>  Das Netzwerk berechnet unter drei Bedingungen eine Funktion gleich 1: x <sub>1</sub> liegt zwischen s <sub>1</sub> und t <sub>1</sub> ;  x <sub>2</sub> liegt zwischen s <sub>2</sub> und t <sub>2</sub> ;  x <sub>3</sub> liegt zwischen s <sub>3</sub> und t <sub>3</sub> .  Das Netzwerk ist an allen anderen Orten 0.  Dies ist ein Turm, in dem 1 einen kleinen Teil des Eingangsraums darstellt und 0 alles andere ist. <br><br>  Wenn wir viele solcher Netzwerke zusammenkleben, k√∂nnen wir so viele T√ºrme erhalten, wie wir m√∂chten, und eine beliebige Funktion von drei Variablen approximieren.  Die gleiche Idee funktioniert in m Dimensionen.  Nur der Ausgangsversatz (‚àím + 1/2) h wird ge√§ndert, um die gew√ºnschten Werte richtig zu dr√ºcken und das Plateau zu entfernen. <br><br>  Nun wissen wir, wie man NS verwendet, um die reale Funktion vieler Variablen zu approximieren.  Was ist mit den Vektorfunktionen f (x <sub>1</sub> , ..., x <sub>m</sub> ) ‚àà R <sup>n</sup> ?  Nat√ºrlich kann eine solche Funktion einfach als n separate reelle Funktionen f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ) usw. betrachtet werden.  Und dann kleben wir einfach alle Netzwerke zusammen.  Es ist also einfach, es herauszufinden. <br><br><h3>  Herausforderung </h3><br><ul><li>  Wir haben gesehen, wie man neuronale Netze mit zwei verborgenen Schichten verwendet, um eine beliebige Funktion zu approximieren.  K√∂nnen Sie beweisen, dass dies mit einer verborgenen Schicht m√∂glich ist?  Tipp - Versuchen Sie, nur mit zwei Ausgabevariablen zu arbeiten, und zeigen Sie Folgendes: (a) Es ist m√∂glich, die Funktionen der Schritte nicht nur entlang der x- oder y-Achse, sondern auch in einer beliebigen Richtung abzurufen.  (b) Addiert man viele Konstruktionen aus Schritt (a), ist es m√∂glich, die Funktion eines runden statt eines rechteckigen Turms zu approximieren;  ¬© Mit runden T√ºrmen kann eine beliebige Funktion angen√§hert werden.  Schritt ¬© wird mit dem in diesem Kapitel vorgestellten Material etwas einfacher. </li></ul><br><h2>  √úber sigmoidale Neuronen hinausgehen </h2><br>  Wir haben bewiesen, dass ein Netzwerk von Sigmoidneuronen jede Funktion berechnen kann.  Denken Sie daran, dass sich in einem Sigmoid-Neuron die Eing√§nge x <sub>1</sub> , x <sub>2</sub> , ... am Ausgang in œÉ (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b) verwandeln, wobei w <sub>j</sub> die Gewichte sind, b die Verschiebung ist, œÉ das Sigmoid ist. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  Was ist, wenn wir einen anderen Neuronentyp mit einer anderen Aktivierungsfunktion betrachten, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Das hei√üt, wir nehmen an, dass wenn ein Neuron x <sub>1</sub> , x <sub>2</sub> , ... Gewichte w <sub>1</sub> , w <sub>2</sub> , ... und Vorspannung b hat, s (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b) ausgegeben wird. <br><br>  Wir k√∂nnen diese Aktivierungsfunktion verwenden, um Schritt zu machen, genau wie im Fall des Sigmoid.  Versuchen Sie (im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel</a> ) im Diagramm, das Gewicht auf beispielsweise w = 100 anzuheben: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Wie im Fall des Sigmoid wird dadurch die Aktivierungsfunktion komprimiert, was zu einer sehr guten Ann√§herung an die Schrittfunktion f√ºhrt.  Wenn Sie den Versatz √§ndern, werden Sie feststellen, dass wir die Position des Schritts in eine beliebige √§ndern k√∂nnen.  Daher k√∂nnen wir dieselben Tricks wie zuvor verwenden, um jede gew√ºnschte Funktion zu berechnen. <br><br>  Welche Eigenschaften sollte s (z) haben, damit dies funktioniert?  Wir m√ºssen annehmen, dass s (z) gut definiert ist als z ‚Üí ‚àí‚àû und z ‚Üí ‚àû.  Diese Grenzwerte sind zwei Werte, die von unserer Schrittfunktion akzeptiert werden.  Wir m√ºssen auch davon ausgehen, dass diese Grenzen unterschiedlich sind.  Wenn sie sich nicht unterscheiden w√ºrden, w√ºrden die Schritte nicht funktionieren, es w√ºrde einfach einen flachen Zeitplan geben!  Wenn jedoch die Aktivierungsfunktion s (z) diese Eigenschaften erf√ºllt, sind die darauf basierenden Neuronen universell f√ºr Berechnungen geeignet. <br><br><h3>  Die Aufgaben </h3><br><ul><li>  Zu Beginn des Buches haben wir einen anderen Neuronentyp kennengelernt - ein begradigtes lineares Neuron oder eine gleichgerichtete lineare Einheit, ReLU.  Erkl√§ren Sie, warum solche Neuronen die f√ºr die Universalit√§t erforderlichen Bedingungen nicht erf√ºllen.  Finden Sie Beweise f√ºr die Vielseitigkeit, die zeigen, dass ReLUs universell f√ºr die Datenverarbeitung geeignet sind. </li><li>  Angenommen, wir betrachten lineare Neuronen mit der Aktivierungsfunktion s (z) = z.  Erkl√§ren Sie, warum lineare Neuronen die Bedingungen der Universalit√§t nicht erf√ºllen.  Zeigen Sie, dass solche Neuronen nicht f√ºr Universal Computing verwendet werden k√∂nnen. </li></ul><br><h2>  Schrittfunktion korrigieren </h2><br>  Vorl√§ufig gingen wir davon aus, dass unsere Neuronen genaue Schrittfunktionen erzeugen.  Dies ist eine gute Ann√§herung, aber nur eine Ann√§herung.  Tats√§chlich gibt es eine enge Fehlerl√ºcke, die in der folgenden Grafik dargestellt ist und in der sich die Funktionen √ºberhaupt nicht wie eine Schrittfunktion verhalten: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  In dieser Zeit des Scheiterns funktioniert meine Erkl√§rung der Universalit√§t nicht. <br><br>  Das Scheitern ist nicht so be√§ngstigend.  Durch Einstellen ausreichend gro√üer Eingabegewichte k√∂nnen diese L√ºcken beliebig klein gemacht werden.  Wir k√∂nnen sie viel kleiner als auf der Karte machen und f√ºr das Auge unsichtbar machen.  Vielleicht m√ºssen wir uns also keine Sorgen um dieses Problem machen. <br><br>  Trotzdem h√§tte ich gerne einen Weg, es zu l√∂sen. <br><br>  Es stellt sich heraus, dass es leicht zu l√∂sen ist.  Schauen wir uns diese L√∂sung zur Berechnung von NS-Funktionen mit nur einer Eingabe und Ausgabe an.  Dieselben Ideen werden funktionieren, um das Problem mit einer gro√üen Anzahl von Ein- und Ausg√§ngen zu l√∂sen. <br><br>  Angenommen, wir m√∂chten, dass unser Netzwerk eine Funktion f berechnet.  Nach wie vor versuchen wir dies, indem wir das Netzwerk so gestalten, dass die gewichtete Ausgabe der verborgenen Neuronenschicht œÉ <sup>‚àí1</sup> ‚ãÖf (x) ist: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Wenn wir dies mit der oben beschriebenen Technik tun, werden wir die verborgenen Neuronen zwingen, eine Folge von Vorsprungsfunktionen zu erzeugen: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Nat√ºrlich habe ich die Gr√∂√üe der Ausfallintervalle √ºbertrieben, damit es leichter zu sehen war.  Es sollte klar sein, dass wir, wenn wir alle diese Funktionen der Vorspr√ºnge addieren, √ºberall eine ziemlich gute Ann√§herung von œÉ <sup>‚àí1</sup> ‚ãÖf (x) erhalten, mit Ausnahme der Ausfallintervalle. <br><br>  Nehmen wir jedoch an, dass wir anstelle der gerade beschriebenen N√§herung eine Reihe versteckter Neuronen verwenden, um die N√§herung der H√§lfte unserer urspr√ºnglichen Zielfunktion zu berechnen, dh œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2.  Nat√ºrlich sieht es genauso aus wie eine skalierte Version des neuesten Diagramms: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Nehmen wir an, wir lassen einen weiteren Satz versteckter Neuronen die Ann√§herung an œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2 berechnen. An ihrer Basis werden die Vorspr√ºnge jedoch um die H√§lfte ihrer Breite verschoben: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Jetzt haben wir zwei verschiedene N√§herungen f√ºr œÉ - 1‚ãÖf (x) / 2.  Wenn wir diese beiden N√§herungen addieren, erhalten wir eine allgemeine N√§herung an œÉ - 1‚ãÖf (x).  Diese allgemeine Ann√§herung weist in kleinen Intervallen immer noch Ungenauigkeiten auf.  Das Problem wird jedoch geringer sein als zuvor - da die Punkte, die in die Intervalle des Versagens der ersten N√§herung fallen, nicht in die Intervalle des Versagens der zweiten N√§herung fallen.  Daher ist die Ann√§herung in diesen Intervallen ungef√§hr zweimal besser. <br><br>  Wir k√∂nnen die Situation verbessern, indem wir eine gro√üe Anzahl M √ºberlappender Approximationen der Funktion œÉ - 1‚ãÖf (x) / M hinzuf√ºgen.  Wenn alle ihre Ausfallintervalle eng genug sind, wird jeder Strom nur in einem von ihnen sein.  Wenn Sie eine ausreichend gro√üe Anzahl √ºberlappender N√§herungen von M verwenden, ist das Ergebnis eine ausgezeichnete allgemeine N√§herung. <br><br><h2>  Fazit </h2><br>  Die hier diskutierte Erkl√§rung der Universalit√§t kann definitiv nicht als praktische Beschreibung des Z√§hlens von Funktionen unter Verwendung neuronaler Netze bezeichnet werden!  In diesem Sinne ist es eher ein Beweis f√ºr die Vielseitigkeit von NAND-Logikgattern und mehr.  Daher habe ich im Grunde versucht, dieses Design klar und einfach zu befolgen, ohne seine Details zu optimieren.  Der Versuch, dieses Design zu optimieren, kann jedoch eine interessante und lehrreiche √úbung f√ºr Sie sein. <br><br>  Obwohl das erhaltene Ergebnis nicht direkt zum Erstellen von NS verwendet werden kann, ist es wichtig, da es die Frage nach der Berechenbarkeit einer bestimmten Funktion unter Verwendung von NS beseitigt.  Die Antwort auf eine solche Frage wird immer positiv sein.  Daher ist es richtig zu fragen, ob eine Funktion berechenbar ist, aber wie kann sie richtig berechnet werden? <br><br>  Unser universelles Design verwendet nur zwei versteckte Schichten, um eine beliebige Funktion zu berechnen.  Wie bereits erw√§hnt, ist es m√∂glich, dasselbe Ergebnis mit einer einzelnen verborgenen Ebene zu erzielen.  Vor diesem Hintergrund fragen Sie sich vielleicht, warum wir tiefe Netzwerke ben√∂tigen, dh Netzwerke mit einer gro√üen Anzahl versteckter Schichten.  K√∂nnen wir diese Netzwerke nicht einfach durch flache Netzwerke mit einer verborgenen Schicht ersetzen? <br><br>  Obwohl es im Prinzip m√∂glich ist, gibt es gute praktische Gr√ºnde f√ºr die Verwendung tiefer neuronaler Netze.  Wie in Kapitel 1 beschrieben, haben tiefe NS eine hierarchische Struktur, die es ihnen erm√∂glicht, sich gut anzupassen, um hierarchisches Wissen zu studieren, das zur L√∂sung realer Probleme n√ºtzlich ist.  Insbesondere bei der L√∂sung von Problemen wie der Mustererkennung ist es n√ºtzlich, ein System zu verwenden, das nicht nur einzelne Pixel, sondern auch immer komplexere Konzepte versteht: von R√§ndern √ºber einfache geometrische Formen und dar√ºber hinaus bis hin zu komplexen Szenen mit mehreren Objekten.  In sp√§teren Kapiteln werden wir Beweise daf√ºr sehen, dass tiefe NS besser mit dem Studium solcher Wissenshierarchien umgehen k√∂nnen als flache.  Zusammenfassend: Die Universalit√§t sagt uns, dass NS jede Funktion berechnen kann;  Empirische Erkenntnisse legen nahe, dass tiefe NS besser an die Untersuchung von Funktionen angepasst sind, die zur L√∂sung vieler realer Probleme n√ºtzlich sind. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461659/">https://habr.com/ru/post/de461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461649/index.html">Wie werde ich die Welt retten?</a></li>
<li><a href="../de461651/index.html">Frontend Weekly Digest (22. - 28. Juli 2019)</a></li>
<li><a href="../de461653/index.html">Software Defined Radio - wie funktioniert es? Teil 10</a></li>
<li><a href="../de461655/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 373 (22. - 28. Juli 2019)</a></li>
<li><a href="../de461657/index.html">Kauf eines roten Hutes: Wird es dem blauen Riesen helfen, f√ºr eine hybride Cloud-F√ºhrung zu k√§mpfen?</a></li>
<li><a href="../de461661/index.html">Komponentenbasiertes Entwicklungshandbuch</a></li>
<li><a href="../de461663/index.html">Die Geschichte, wie Linux Windows einbrachte</a></li>
<li><a href="../de461665/index.html">Zen2. Die Entwicklung der AM4-Plattform am Beispiel des Ryzen 7 3700x</a></li>
<li><a href="../de461669/index.html">PHP Digest Nr. 161 (15. - 29. Juli 2019)</a></li>
<li><a href="../de461673/index.html">8 Tipps f√ºr Programmieranf√§nger oder eine Retrospektive meiner Karriere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>