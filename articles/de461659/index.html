<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌆 🚈 🎟️ Neuronale Netze und Deep Learning, Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen können ⛲️ 🌑 🙍🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Kapitel gebe ich eine einfache und meist visuelle Erklärung des Universalitätstheorems. Um dem Material in diesem Kapitel zu folgen, müssen ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und Deep Learning, Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen können</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  In diesem Kapitel gebe ich eine einfache und meist visuelle Erklärung des Universalitätstheorems.  Um dem Material in diesem Kapitel zu folgen, müssen Sie die vorherigen nicht lesen.  Es ist als eigenständiger Aufsatz aufgebaut.  Wenn Sie das grundlegendste Verständnis von NS haben, sollten Sie in der Lage sein, die Erklärungen zu verstehen. <br><br><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum trägt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie wählt man Hyperparameter für neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen können</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Jüngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  Eine der erstaunlichsten Tatsachen über neuronale Netze ist, dass sie jede Funktion überhaupt berechnen können.  Nehmen wir an, jemand gibt Ihnen eine komplexe und kurvenreiche Funktion f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  Und unabhängig von dieser Funktion ist ein solches neuronales Netzwerk garantiert, dass für jede Eingabe x der Wert f (x) (oder eine nahe liegende Annäherung) die Ausgabe dieses Netzwerks ist, dh: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Dies funktioniert auch dann, wenn es sich um eine Funktion vieler Variablen f = f (x <sub>1</sub> , ..., x <sub>m</sub> ) und mit vielen Werten handelt.  Hier ist zum Beispiel ein Netzwerk, das eine Funktion mit m = 3 Eingängen und n = 2 Ausgängen berechnet: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Dieses Ergebnis legt nahe, dass neuronale Netze eine gewisse Universalität aufweisen.  Egal welche Funktion wir berechnen möchten, wir wissen, dass es ein neuronales Netzwerk gibt, das dies kann. <br><br>  Darüber hinaus gilt der Universalitätstheorem auch dann, wenn wir das Netzwerk auf eine einzige Schicht zwischen eingehenden und ausgehenden Neuronen beschränken - die sogenannten  in einer versteckten Schicht.  So können auch Netzwerke mit einer sehr einfachen Architektur extrem leistungsfähig sein. <br><br>  Der Universalitätstheorem ist Menschen, die neuronale Netze verwenden, gut bekannt.  Obwohl dies so ist, ist ein Verständnis dieser Tatsache nicht so weit verbreitet.  Und die meisten Erklärungen dafür sind technisch zu komplex.  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einer der ersten Arbeiten,</a> die dieses Ergebnis belegen, wurden beispielsweise der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hahn-Banach-Satz</a> , der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Riesz-Repräsentationssatz</a> und einige Fourier-Analysen verwendet.  Wenn Sie Mathematiker sind, ist es für Sie leicht, diese Beweise zu verstehen, aber für die meisten Menschen ist es nicht so einfach.  Schade, denn die Hauptgründe für die Universalität sind einfach und schön. <br><br>  In diesem Kapitel gebe ich eine einfache und meist visuelle Erklärung des Universalitätstheorems.  Wir werden Schritt für Schritt die zugrunde liegenden Ideen durchgehen.  Sie werden verstehen, warum neuronale Netze wirklich jede Funktion berechnen können.  Sie werden einige der Einschränkungen dieses Ergebnisses verstehen.  Und Sie werden verstehen, wie das Ergebnis mit tiefem NS verbunden ist. <br><br>  Um dem Material in diesem Kapitel zu folgen, müssen Sie die vorherigen nicht lesen.  Es ist als eigenständiger Aufsatz aufgebaut.  Wenn Sie das grundlegendste Verständnis von NS haben, sollten Sie in der Lage sein, die Erklärungen zu verstehen.  Aber ich werde manchmal Links zu vorherigem Material bereitstellen, um Wissenslücken zu schließen. <br><br>  Universalsätze finden sich oft in der Informatik, deshalb vergessen wir manchmal sogar, wie erstaunlich sie sind.  Aber es lohnt sich, sich daran zu erinnern: Die Fähigkeit, eine beliebige Funktion zu berechnen, ist wirklich erstaunlich.  Fast jeder Prozess, den Sie sich vorstellen können, kann auf die Berechnung einer Funktion reduziert werden.  Betrachten Sie die Aufgabe, den Namen einer Musikkomposition anhand einer kurzen Passage zu finden.  Dies kann als Funktionsberechnung angesehen werden.  Oder überlegen Sie, ob Sie einen chinesischen Text ins Englische übersetzen möchten.  Und dies kann als Funktionsberechnung betrachtet werden (in der Tat viele Funktionen, da es viele akzeptable Optionen für die Übersetzung eines einzelnen Textes gibt).  Oder betrachten Sie die Aufgabe, eine Beschreibung der Handlung des Films und der Qualität des Schauspiels basierend auf der mp4-Datei zu erstellen.  Auch dies kann als Berechnung einer bestimmten Funktion angesehen werden (die Bemerkung zu den Textübersetzungsoptionen ist auch hier richtig).  Universalität bedeutet, dass NS im Prinzip alle diese und viele andere Aufgaben ausführen können. <br><br>  Nur aus der Tatsache, dass wir wissen, dass es NS gibt, die beispielsweise vom Chinesischen ins Englische übersetzen können, folgt natürlich nicht, dass wir über gute Techniken zum Erstellen oder sogar Erkennen eines solchen Netzwerks verfügen.  Diese Einschränkung gilt auch für traditionelle Universalitätstheoreme für Modelle wie Boolesche Schemata.  Wie wir bereits in diesem Buch gesehen haben, verfügt der NS über leistungsstarke Algorithmen zum Lernen von Funktionen.  Die Kombination aus Lernalgorithmen und Vielseitigkeit ist eine attraktive Mischung.  Bisher haben wir uns in dem Buch auf Trainingsalgorithmen konzentriert.  In diesem Kapitel konzentrieren wir uns auf die Vielseitigkeit und deren Bedeutung. <br><br><h2>  Zwei Tricks </h2><br>  Bevor ich erkläre, warum der Universalitätstheorem wahr ist, möchte ich zwei Tricks erwähnen, die in der informellen Aussage „Ein neuronales Netzwerk kann jede Funktion berechnen“ enthalten sind. <br><br>  Erstens bedeutet dies nicht, dass das Netzwerk verwendet werden kann, um eine Funktion genau zu berechnen.  Wir können nur eine so gute Annäherung bekommen, wie wir brauchen.  Indem wir die Anzahl der versteckten Neuronen erhöhen, verbessern wir die Approximation.  Zum Beispiel habe ich zuvor ein Netzwerk dargestellt, das eine bestimmte Funktion f (x) unter Verwendung von drei versteckten Neuronen berechnet.  Für die meisten Funktionen kann unter Verwendung von drei Neuronen nur eine Annäherung von geringer Qualität erhalten werden.  Durch Erhöhen der Anzahl versteckter Neuronen (z. B. bis zu fünf) können wir normalerweise eine verbesserte Annäherung erhalten: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Und um die Situation zu verbessern, indem die Anzahl der versteckten Neuronen weiter erhöht wird. <br><br>  Nehmen wir zur Verdeutlichung dieser Aussage an, wir hätten eine Funktion f (x) erhalten, die wir mit der notwendigen Genauigkeit ε&gt; 0 berechnen wollen.  Es besteht die Garantie, dass bei Verwendung einer ausreichenden Anzahl versteckter Neuronen immer ein NS gefunden werden kann, dessen Ausgabe g (x) die Gleichung | g (x) - f (x) | &lt;ε für jedes x erfüllt.  Mit anderen Worten wird die Annäherung mit der gewünschten Genauigkeit für jeden möglichen Eingabewert erreicht. <br><br>  Der zweite Haken ist, dass Funktionen, die mit der beschriebenen Methode approximiert werden können, zu einer kontinuierlichen Klasse gehören.  Wenn die Funktion unterbrochen wird, dh plötzlich scharfe Sprünge macht, ist es im allgemeinen Fall unmöglich, mit Hilfe von NS eine Annäherung vorzunehmen.  Dies ist nicht überraschend, da unsere NS kontinuierliche Funktionen von Eingabedaten berechnen.  Selbst wenn die Funktion, die wir wirklich berechnen müssen, diskontinuierlich ist, ist die Approximation oft ziemlich kontinuierlich.  Wenn ja, dann können wir NS verwenden.  In der Praxis ist diese Einschränkung normalerweise nicht wichtig. <br><br>  Infolgedessen wird eine genauere Aussage des Universalitätstheorems sein, dass NS mit einer verborgenen Schicht verwendet werden kann, um jede kontinuierliche Funktion mit jeder gewünschten Genauigkeit zu approximieren.  In diesem Kapitel beweisen wir eine etwas weniger strenge Version dieses Theorems, bei der zwei verborgene Schichten anstelle einer verwendet werden.  In Aufgaben werde ich kurz beschreiben, wie diese Erklärung mit geringfügigen Änderungen an einen Beweis angepasst werden kann, der nur eine verborgene Ebene verwendet. <br><br><h2>  Vielseitigkeit mit einem Eingabe- und einem Ausgabewert </h2><br>  Um zu verstehen, warum der Universalitätstheorem wahr ist, verstehen wir zunächst, wie eine NS-Approximationsfunktion mit nur einem Eingabe- und einem Ausgabewert erstellt wird: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Es stellt sich heraus, dass dies die Essenz der Aufgabe der Universalität ist.  Sobald wir diesen Sonderfall verstanden haben, wird es ziemlich einfach sein, ihn auf Funktionen mit vielen Eingabe- und Ausgabewerten zu erweitern. <br><br>  Um zu verstehen, wie ein Netzwerk zum Zählen von f aufgebaut wird, beginnen wir mit einem Netzwerk, das eine einzelne verborgene Schicht mit zwei verborgenen Neuronen und eine Ausgangsschicht mit einem Ausgangsneuron enthält: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Um uns vorzustellen, wie die Netzwerkkomponenten funktionieren, konzentrieren wir uns auf das obere versteckte Neuron.  Im Diagramm des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikels können</a> Sie das Gewicht interaktiv mit der Maus ändern, indem Sie auf „w“ klicken und sofort sehen, wie sich die vom oberen versteckten Neuron berechnete Funktion ändert: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Wie wir früher in diesem Buch erfahren haben, zählt ein verstecktes Neuron σ (wx + b), wobei σ (z) ≡ 1 / (1 + e <sup>−z</sup> ) ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sigmoid ist</a> .  Bisher haben wir diese algebraische Form ziemlich oft verwendet.  Um die Universalität zu beweisen, ist es jedoch besser, diese Algebra vollständig zu ignorieren und stattdessen die Form im Diagramm zu manipulieren und zu beobachten.  Dies hilft Ihnen nicht nur, besser zu fühlen, was passiert, sondern gibt uns auch einen Beweis für die Universalität, die für andere Aktivierungsfunktionen neben Sigmoid gilt. <br><br>  Genau genommen wird der von mir gewählte visuelle Ansatz traditionell nicht als Beweis angesehen.  Ich glaube jedoch, dass der visuelle Ansatz mehr Einblick in die Wahrheit des Endergebnisses bietet als herkömmliche Beweise.  Und natürlich ist ein solches Verständnis der eigentliche Zweck des Beweises.  In den von mir vorgeschlagenen Beweisen treten gelegentlich Lücken auf;  Ich werde vernünftige, aber nicht immer strenge visuelle Beweise liefern.  Wenn Sie dies stört, betrachten Sie es als Ihre Aufgabe, diese Lücken zu schließen.  Verlieren Sie jedoch nicht das Hauptziel aus den Augen: zu verstehen, warum der Universalitätstheorem wahr ist. <br><br>  Klicken Sie zunächst auf den Versatz b im Originaldiagramm und ziehen Sie ihn nach rechts, um ihn zu vergrößern.  Sie werden sehen, dass sich das Diagramm mit zunehmendem Versatz nach links bewegt, aber seine Form nicht ändert. <br><br>  Ziehen Sie es dann nach links, um den Versatz zu verringern.  Sie werden sehen, dass sich das Diagramm nach rechts bewegt, ohne die Form zu ändern. <br><br>  Gewicht auf 2-3 reduzieren.  Sie werden sehen, dass sich die Kurve mit abnehmendem Gewicht gerade richtet.  Damit die Kurve nicht vom Diagramm abweicht, müssen Sie möglicherweise den Versatz korrigieren. <br><br>  Erhöhen Sie schließlich das Gewicht auf Werte größer als 100. Die Kurve wird steiler und nähert sich schließlich dem Schritt.  Stellen Sie den Versatz so ein, dass sein Winkel im Bereich des Punktes x = 0,3 liegt.  Das folgende Video zeigt, was passieren soll: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Wir können unsere Analyse erheblich vereinfachen, indem wir das Gewicht erhöhen, sodass die Ausgabe wirklich eine gute Annäherung an die Schrittfunktion darstellt.  Unten habe ich die Ausgabe des oberen versteckten Neurons für das Gewicht w = 999 erstellt.  Dies ist ein statisches Bild: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Die Verwendung von Schrittfunktionen ist etwas einfacher als bei einem typischen Sigmoid.  Der Grund ist, dass Beiträge aller versteckten Neuronen in der Ausgabeschicht addiert werden.  Die Summe einer Reihe von Schrittfunktionen ist leicht zu analysieren, es ist jedoch schwieriger, darüber zu sprechen, was passiert, wenn eine Reihe von Kurven in Form eines Sigmoid hinzugefügt wird.  Daher ist es viel einfacher anzunehmen, dass unsere versteckten Neuronen schrittweise Funktionen produzieren.  Genauer gesagt fixieren wir dazu das Gewicht w auf einen sehr großen Wert und weisen dann die Position der Stufe durch den Versatz zu.  Natürlich ist die Arbeit mit einer Ausgabe als Schrittfunktion eine Annäherung, aber sie ist sehr gut, und bis jetzt werden wir die Funktion als echte Schrittfunktion behandeln.  Später werde ich auf die Auswirkung von Abweichungen von dieser Annäherung zurückkommen. <br><br>  Welcher Wert von x ist der Schritt?  Mit anderen Worten, wie hängt die Position der Stufe von Gewicht und Verschiebung ab? <br><br>  Versuchen Sie zur Beantwortung der Frage, das Gewicht und den Versatz im interaktiven Diagramm zu ändern.  Können Sie verstehen, wie die Position des Schritts von w und b abhängt?  Wenn Sie ein wenig üben, können Sie sich selbst davon überzeugen, dass seine Position proportional zu b und umgekehrt proportional zu w ist. <br><br>  Tatsächlich liegt der Schritt bei s = –b / w, wie zu sehen sein wird, wenn wir das Gewicht und die Verschiebung auf die folgenden Werte einstellen: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Unser Leben wird stark vereinfacht, wenn wir versteckte Neuronen mit einem einzigen Parameter s beschreiben, dh durch die Position des Schritts s = −b / w.  Im folgenden interaktiven Diagramm können Sie einfach s ändern: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Wie oben erwähnt, haben wir einem sehr großen Wert speziell ein Gewicht w am Eingang zugewiesen - groß genug, damit die Schrittfunktion eine gute Annäherung darstellt.  Und wir können das parametrisierte Neuron auf diese Weise leicht in seine übliche Form zurückversetzen, indem wir den Bias b = −ws wählen. <br><br>  Bisher haben wir uns nur auf die Ausgabe des überlegenen versteckten Neurons konzentriert.  Betrachten wir das Verhalten des gesamten Netzwerks.  Angenommen, versteckte Neuronen berechnen die Schrittfunktionen, die durch die Parameter der Schritte s <sub>1</sub> (oberes Neuron) und s <sub>2</sub> (unteres Neuron) definiert sind.  Ihre jeweiligen Ausgangsgewichte sind w <sub>1</sub> und w <sub>2</sub> .  Hier ist unser Netzwerk: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  Rechts ist ein Diagramm der gewichteten Ausgabe w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 der</sub> verborgenen Schicht.  Hier sind a <sub>1</sub> und a <sub>2</sub> die Ausgänge der oberen bzw. unteren versteckten Neuronen.  Sie werden mit "a" bezeichnet, da sie oft als neuronale Aktivierungen bezeichnet werden. <br><br>  Übrigens stellen wir fest, dass der Ausgang des gesamten Netzwerks σ ist (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), wobei b die Vorspannung des Ausgangsneurons ist.  Dies ist offensichtlich nicht dasselbe wie die gewichtete Ausgabe der verborgenen Ebene, deren Diagramm wir erstellen.  Im Moment konzentrieren wir uns jedoch auf die ausgeglichene Ausgabe der verborgenen Schicht und denken erst später darüber nach, wie sie sich auf die Ausgabe des gesamten Netzwerks bezieht. <br><br>  Versuchen Sie, den Schritt s <sub>1 des</sub> oberen versteckten Neurons im interaktiven Diagramm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im Originalartikel</a> zu erhöhen und zu verringern.  Sehen Sie, wie dies die gewichtete Ausgabe der verborgenen Ebene ändert.  Es ist besonders nützlich zu verstehen, was passiert, wenn s <sub>1</sub> s <sub>2</sub> überschreitet.  Sie werden sehen, dass der Graph in diesen Fällen seine Form ändert, wenn wir von einer Situation, in der das obere versteckte Neuron zuerst aktiviert wird, zu einer Situation übergehen, in der das untere versteckte Neuron zuerst aktiviert wird. <br><br>  Versuchen Sie in ähnlicher Weise, den Schritt s <sub>2 des</sub> unteren verborgenen Neurons zu manipulieren, und sehen Sie, wie dies die Gesamtleistung der verborgenen Neuronen verändert. <br><br>  Versuchen Sie, die Ausgangsgewichte zu reduzieren und zu erhöhen.  Beachten Sie, wie dies den Beitrag der entsprechenden versteckten Neuronen skaliert.  Was passiert, wenn eines der Gewichte gleich 0 ist? <br><br>  Versuchen Sie abschließend, w <sub>1</sub> auf 0,8 und w <sub>2</sub> auf -0,8 einzustellen.  Das Ergebnis ist eine "Vorsprung" -Funktion mit einem Anfang bei s <sub>1</sub> , einem Ende bei s <sub>2</sub> und einer Höhe von 0,8.  Eine gewichtete Ausgabe könnte beispielsweise folgendermaßen aussehen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Natürlich kann der Vorsprung auf jede Höhe skaliert werden.  Verwenden wir einen Parameter, h, der die Höhe bezeichnet.  Der Einfachheit halber werde ich auch die Notation "s <sub>1</sub> = ..." und "w <sub>1</sub> = ..." entfernen. <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Versuchen Sie, den h-Wert zu erhöhen und zu verringern, um zu sehen, wie sich die Höhe des Vorsprungs ändert.  Versuchen Sie, h negativ zu machen.  Versuchen Sie, die Punkte der Schritte zu ändern, um zu beobachten, wie sich dadurch die Form des Vorsprungs ändert. <br><br>  Sie werden sehen, dass wir unsere Neuronen nicht nur als grafische Grundelemente verwenden, sondern auch als Einheiten, die Programmierern vertrauter sind - so etwas wie eine Wenn-Dann-Sonst-Anweisung in der Programmierung: <br><br>  wenn Eingabe&gt; = Schrittbeginn: <br>  addiere 1 zur gewichteten Ausgabe <br>  sonst: <br>  Addiere 0 zur gewichteten Ausgabe <br><br>  Zum größten Teil werde ich mich an die grafische Notation halten.  Manchmal ist es jedoch hilfreich, zur Wenn-Dann-Sonst-Ansicht zu wechseln und darüber nachzudenken, was in diesen Begriffen geschieht. <br><br>  Wir können unseren Protrusionstrick verwenden, indem wir zwei Teile versteckter Neuronen im selben Netzwerk zusammenkleben: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Hier ließ ich die Gewichte fallen, indem ich einfach die h-Werte für jedes Paar versteckter Neuronen aufschrieb.  Versuchen Sie, mit beiden h-Werten zu spielen, und sehen Sie, wie sich das Diagramm ändert.  Verschieben Sie die Registerkarten und ändern Sie die Punkte der Schritte. <br><br>  In einem allgemeineren Fall kann diese Idee verwendet werden, um eine beliebige Anzahl von Spitzen einer beliebigen Höhe zu erhalten.  Insbesondere können wir das Intervall [0,1] in eine große Anzahl von (N) Teilintervallen unterteilen und N Paare versteckter Neuronen verwenden, um Peaks beliebiger Höhe zu erhalten.  Mal sehen, wie das bei N = 5 funktioniert.  Dies sind bereits ziemlich viele Neuronen, daher bin ich etwas enger dargestellt.  Entschuldigung für das komplexe Diagramm - ich könnte die Komplexität hinter zusätzlichen Abstraktionen verbergen, aber es scheint mir, dass es eine kleine Qual mit Komplexität wert ist, um besser zu fühlen, wie die neuronalen Netze funktionieren. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Sie sehen, wir haben fünf Paare versteckter Neuronen.  Die Punkte der Schritte der entsprechenden Paare liegen bei 0,1 / 5, dann bei 1 / 5,2 / 5 usw. bis zu 4 / 5,5 / 5.  Diese Werte sind fest - wir erhalten fünf Vorsprünge gleicher Breite in der Grafik. <br><br>  Jedem Neuronenpaar ist ein Wert h zugeordnet.  Denken Sie daran, dass Ausgangsneuronenverbindungen die Gewichte h und –h haben.  Im Originalartikel im Diagramm können Sie auf die h-Werte klicken und sie von links nach rechts verschieben.  Mit einer Änderung der Höhe ändert sich auch der Zeitplan.  Durch Ändern der Ausgabegewichte konstruieren wir die endgültige Funktion! <br><br>  Im Diagramm können Sie weiterhin auf das Diagramm klicken und die Höhe der Schritte nach oben oder unten ziehen.  Wenn Sie die Höhe ändern, sehen Sie, wie sich die Höhe des entsprechenden h ändert.  Die Ausgabegewichte + h und –h ändern sich entsprechend.  Mit anderen Worten, wir manipulieren direkt eine Funktion, deren Grafik rechts angezeigt wird, und sehen diese Änderungen in den Werten von h links.  Sie können auch die Maustaste an einem der Vorsprünge gedrückt halten und dann die Maus nach links oder rechts ziehen. Die Vorsprünge werden dann an die aktuelle Höhe angepasst. <br><br>  Es ist Zeit, die Arbeit zu erledigen. <br><br>  Erinnern Sie sich an die Funktion, die ich ganz am Anfang des Kapitels gezeichnet habe: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Dann habe ich das nicht erwähnt, aber tatsächlich sieht es so aus: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhilnP6ZSgEJf91EF4oxAVp-cptLXA#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>15</mn><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&nbsp;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mn>50</mn><mi>x</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>113</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Es ist für x-Werte von 0 bis 1 konstruiert und Werte entlang der y-Achse variieren von 0 bis 1. <br><br>  Offensichtlich ist diese Funktion nicht trivial.  Und Sie müssen herausfinden, wie Sie es mithilfe neuronaler Netze berechnen können. <br><br>  In unseren obigen neuronalen Netzen haben wir eine gewichtete Kombination ∑ <sub>j</sub> w <sub>j</sub> a <sub>j</sub> der Ausgabe versteckter Neuronen analysiert.  Wir wissen, wie wir diesen Wert maßgeblich kontrollieren können.  Wie bereits erwähnt, entspricht dieser Wert jedoch nicht der Netzwerkausgabe.  Die Ausgabe des Netzwerks ist σ (∑ <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), wobei b die Verschiebung des Ausgangsneurons ist.  Können wir direkt die Kontrolle über die Netzwerkausgabe erlangen? <br><br>  Die Lösung besteht darin, ein neuronales Netzwerk zu entwickeln, in dem die gewichtete Ausgabe der verborgenen Schicht durch die Gleichung σ <sup>−1</sup> ⋅f (x) gegeben ist, wobei σ <sup>−1</sup> die Umkehrfunktion von σ ist.  Das heißt, wir möchten, dass die gewichtete Ausgabe der verborgenen Ebene folgendermaßen aussieht: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn dies erfolgreich ist, ist die Ausgabe des gesamten Netzwerks eine gute Annäherung an f (x) (ich setze den Versatz des Ausgangsneurons auf 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dann besteht Ihre Aufgabe darin, einen NS zu entwickeln, der sich der oben gezeigten Zielfunktion annähert. Um besser zu verstehen, was passiert, empfehle ich Ihnen, dieses Problem zweimal zu lösen. </font><font style="vertical-align: inherit;">Klicken </font><font style="vertical-align: inherit;">Sie zum ersten Mal im </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalartikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf das Diagramm und passen Sie die Höhen der verschiedenen Vorsprünge direkt an. Es wird für Sie ziemlich einfach sein, eine gute Annäherung an die Zielfunktion zu erhalten. Der Approximationsgrad wird durch die durchschnittliche Abweichung, die Differenz zwischen der Zielfunktion und der vom Netzwerk berechneten Funktion geschätzt. Ihre Aufgabe ist es, die durchschnittliche Abweichung auf einen Mindestwert zu bringen. Die Aufgabe gilt als erledigt, wenn die durchschnittliche Abweichung 0,40 nicht überschreitet.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie erfolgreich sind, klicken Sie auf die Schaltfläche Zurücksetzen, wodurch die Registerkarten zufällig geändert werden. Berühren Sie beim zweiten Mal nicht das Diagramm, sondern ändern Sie die h-Werte auf der linken Seite des Diagramms, um die durchschnittliche Abweichung auf einen Wert von 0,40 oder weniger zu bringen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie haben also alle Elemente gefunden, die das Netzwerk benötigt, um die Funktion f (x) näherungsweise zu berechnen! Die Annäherung stellte sich als grob heraus, aber wir können das Ergebnis leicht verbessern, indem wir einfach die Anzahl der Paare versteckter Neuronen erhöhen, wodurch die Anzahl der Vorsprünge erhöht wird. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Insbesondere ist es einfach, alle gefundenen Daten mit der für NS verwendeten Parametrierung wieder in die Standardansicht umzuwandeln. Lassen Sie mich schnell daran erinnern, wie das funktioniert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der ersten Schicht haben alle Gewichte einen großen konstanten Wert, zum Beispiel w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verschiebungen versteckter Neuronen werden durch b = −ws berechnet. So wird beispielsweise für das zweite versteckte Neuron s = 0,2 zu b = –1000 × 0,2 = –200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die letzte Schicht der Skala wird durch die Werte von h bestimmt. So bedeutet beispielsweise der Wert, den Sie für das erste h wählen, h = -0,2, dass die Ausgabegewichte der beiden oberen versteckten Neuronen -0,2 bzw. 0,2 betragen. Und so weiter für die gesamte Ebene der Ausgabegewichte. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schließlich ist der Versatz des Ausgangsneurons 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und das war's: Wir haben eine vollständige Beschreibung des NS erhalten, die die anfängliche Zielfunktion gut berechnet. Und wir verstehen es, die Qualität der Approximation zu verbessern, indem wir die Anzahl der versteckten Neuronen verbessern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Außerdem ist in unserer ursprünglichen Zielfunktion f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0,3sin (15x) + 0,05cos (50x) ist nichts Besonderes. </font><font style="vertical-align: inherit;">Ein ähnliches Verfahren könnte für jede kontinuierliche Funktion in den Intervallen von [0,1] bis [0,1] angewendet werden. </font><font style="vertical-align: inherit;">Tatsächlich verwenden wir unseren einschichtigen NS, um eine Nachschlagetabelle für eine Funktion zu erstellen. </font><font style="vertical-align: inherit;">Und wir können diese Idee als Grundlage nehmen, um einen allgemeinen Beweis der Universalität zu erhalten.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Funktion vieler Parameter </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir erweitern unsere Ergebnisse auf den Fall einer Reihe von Eingabevariablen. Es klingt kompliziert, aber alle Ideen, die wir brauchen, können bereits für den Fall mit nur zwei eingehenden Variablen verstanden werden. Daher betrachten wir den Fall mit zwei eingehenden Variablen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schauen wir uns zunächst an, was passiert, wenn ein Neuron zwei Eingänge hat: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben Eingänge x und y mit den entsprechenden Gewichten w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und dem Offset b des Neurons. Wir setzen das Gewicht von w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf 0 und spielen mit dem ersten, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , und versetzen b, um zu sehen, wie sie die Ausgabe des Neurons beeinflussen: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie sehen können </font><font style="vertical-align: inherit;">, beeinflusst die Eingabe y </font><font style="vertical-align: inherit;">bei w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0 nicht die Ausgabe des Neurons. Alles geschieht so, als wäre x die einzige Eingabe.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was wird Ihrer Meinung nach passieren, wenn wir das Gewicht von w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 </font><font style="vertical-align: inherit;">erhöhen </font><font style="vertical-align: inherit;">und w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0 belassen? Wenn Ihnen dies nicht sofort klar ist, denken Sie ein wenig über dieses Problem nach. Dann schauen Sie sich das folgende Video an, das zeigt, was passieren wird:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie zuvor nähert sich die Ausgabe mit einer Erhöhung des Eingabegewichts der Form der Stufe an. Der Unterschied besteht darin, dass sich unsere Schrittfunktion jetzt in drei Dimensionen befindet. Nach wie vor können wir die Position der Schritte verschieben, indem wir den Versatz ändern. Der Winkel liegt am Punkt s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ≡ - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wiederholen wir das Diagramm so, dass der Parameter der Ort des Schritts ist: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir nehmen an, dass das Eingabegewicht von x von großer Bedeutung ist - ich habe w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 verwendet - und das Gewicht w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. Die Zahl auf dem Neuron ist die Position des Schritts, und das x darüber erinnert uns daran, dass wir den Schritt entlang der x-Achse bewegen. Natürlich ist es durchaus möglich, eine Schrittfunktion entlang der y-Achse zu erhalten, wodurch das eingehende Gewicht für y groß wird (zum Beispiel w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2)</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), und das Gewicht für x ist 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Zahl auf dem Neuron gibt wiederum die Position des Schritts an, und y darüber erinnert uns daran, dass wir den Schritt entlang der y-Achse bewegen. Ich könnte die Gewichte für x und y direkt bestimmen, habe es aber nicht getan, da dies das Diagramm verunreinigen würde. Beachten Sie jedoch, dass der y-Marker anzeigt, dass das Gewicht für y groß und für x 0 ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir können die soeben entworfenen Schrittfunktionen verwenden, um die dreidimensionale Vorsprungsfunktion zu berechnen. Dazu nehmen wir zwei Neuronen, von denen jedes eine Schrittfunktion entlang der x-Achse berechnet. Dann kombinieren wir diese Schrittfunktionen mit den Gewichten h und –h, wobei h die gewünschte Vorsprungshöhe ist. All dies ist im folgenden Diagramm zu sehen:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie, den Wert von h zu ändern. Sehen Sie, wie es sich auf Netzwerkgewichte bezieht. Und wie sie die Höhe der Vorsprungsfunktion rechts verändert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie auch, den Punkt des Schritts zu ändern, dessen Wert im oberen versteckten Neuron auf 0,30 eingestellt ist. Sehen Sie, wie sich die Form des Vorsprungs ändert. Was passiert, wenn Sie es über den 0,70-Punkt hinaus bewegen, der dem unteren versteckten Neuron zugeordnet ist? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben gelernt, wie man die Vorsprungsfunktion entlang der x-Achse aufbaut. Natürlich können wir die Vorsprungsfunktion leicht entlang der y-Achse ausführen, indem wir zwei Schrittfunktionen entlang der y-Achse verwenden. Denken Sie daran, dass wir dies tun können, indem wir am Eingang y große Gewichte machen und am Eingang x das Gewicht 0 setzen. Und so, was passiert:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es sieht fast identisch mit dem vorherigen Netzwerk aus! Die einzige sichtbare Veränderung sind kleine y-Marker auf versteckten Neuronen. Sie erinnern uns daran, dass sie Schrittfunktionen für y und nicht für x erzeugen, sodass das Gewicht am Eingang y sehr groß ist und am Eingang x Null ist und nicht umgekehrt. Nach wie vor habe ich beschlossen, es nicht direkt zu zeigen, um das Bild nicht zu überladen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mal sehen, was passiert, wenn wir zwei Vorsprungsfunktionen hinzufügen, eine entlang der x-Achse, die andere entlang der y-Achse, beide mit der Höhe h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um das Verbindungsdiagramm mit dem Gewicht Null zu vereinfachen, habe ich weggelassen. Bisher habe ich kleine x- und y-Marker auf versteckten Neuronen hinterlassen, um mich daran zu erinnern, in welche Richtungen die Protrusionsfunktionen berechnet werden. Später werden wir sie ablehnen, da sie durch die eingehende Variable impliziert werden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Versuchen Sie, den Parameter h zu ändern. </font><font style="vertical-align: inherit;">Wie Sie sehen können, ändern sich aus diesem Grund die Ausgangsgewichte sowie die Gewichte der beiden Vorsprungsfunktionen x und y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unsere </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstellung ist ein </font><font style="vertical-align: inherit;">bisschen wie eine „Turmfunktion“: </font><font style="vertical-align: inherit;">Wenn wir solche Turmfunktionen erstellen können, können wir sie verwenden, um beliebige Funktionen zu approximieren, indem wir einfach Türme unterschiedlicher Höhe an verschiedenen Stellen hinzufügen: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Natürlich haben wir die Erstellung einer beliebigen Turmfunktion noch nicht erreicht. </font><font style="vertical-align: inherit;">Bisher haben wir so etwas wie einen zentralen Turm der Höhe 2h mit einem Plateau der Höhe h um ihn herum gebaut. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber wir können einen Turm zum Funktionieren bringen. </font><font style="vertical-align: inherit;">Denken Sie daran, dass wir zuvor gezeigt haben, wie Neuronen verwendet werden können, um die if-then-else-Anweisung zu implementieren:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Es war ein Neuron mit einem Eingang.  Und wir müssen eine ähnliche Idee auf die kombinierte Ausgabe versteckter Neuronen anwenden: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Wenn wir die richtige Schwelle wählen - zum Beispiel 3h / 2, die zwischen der Höhe des Plateaus und der Höhe des zentralen Turms gedrückt wird - können wir das Plateau auf Null drücken und nur einen Turm belassen. <br><br>  Stellen Sie sich vor, wie das geht?  Versuchen Sie, mit dem folgenden Netzwerk zu experimentieren.  Jetzt zeichnen wir die Ausgabe des gesamten Netzwerks und nicht nur die gewichtete Ausgabe der verborgenen Schicht.  Dies bedeutet, dass wir den Offset-Term zur gewichteten Ausgabe der verborgenen Ebene hinzufügen und das Sigmoid anwenden.  Können Sie die Werte für h und b finden, für die Sie einen Turm erhalten?  Wenn Sie an dieser Stelle nicht weiterkommen, sind hier zwei Tipps: (1) Damit das ausgehende Neuron das Wenn-Dann-Sonst-Verhalten zeigt, müssen die eingehenden Gewichte (alle h oder –h) groß sein.  (2) Der Wert von b bestimmt die Skala der Wenn-Dann-Sonst-Schwelle. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Mit Standardparametern ähnelt die Ausgabe einer abgeflachten Version des vorherigen Diagramms mit einem Turm und einem Plateau.  Um das gewünschte Verhalten zu erzielen, müssen Sie den Wert von h erhöhen.  Dies gibt uns das Schwellenverhalten von Wenn-Dann-Sonst.  Zweitens muss man b ≈ −3h / 2 wählen, um den Schwellenwert korrekt einzustellen. <br><br>  So sieht es für h = 10 aus: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Selbst für relativ bescheidene Werte von h erhalten wir eine schöne Turmfunktion.  Und natürlich können wir ein beliebig schönes Ergebnis erzielen, indem wir h weiter erhöhen und die Vorspannung auf dem Niveau b = –3h / 2 halten. <br><br>  Versuchen wir, zwei Netzwerke zusammenzukleben, um zwei verschiedene Turmfunktionen zu zählen.  Um die jeweiligen Rollen der beiden Subnetze zu verdeutlichen, habe ich sie in separate Rechtecke eingefügt: Jedes von ihnen berechnet die Turmfunktion mit der oben beschriebenen Technik.  Die Grafik rechts zeigt die gewichtete Ausgabe der zweiten verborgenen Schicht, dh die gewichtete Kombination von Turmfunktionen. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  Insbesondere ist zu sehen, dass Sie durch Ändern des Gewichts in der letzten Schicht die Höhe der Ausgangstürme ändern können. <br><br>  Mit derselben Idee können Sie so viele Türme berechnen, wie Sie möchten.  Wir können sie beliebig dünn und groß machen.  Infolgedessen garantieren wir, dass sich die gewichtete Ausgabe der zweiten verborgenen Schicht jeder gewünschten Funktion zweier Variablen annähert: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  Insbesondere wenn wir die gewichtete Ausgabe der zweiten verborgenen Schicht zwingen, sich gut σ <sup>−1</sup> ⋅f anzunähern, garantieren wir, dass die Ausgabe unseres Netzwerks eine gute Annäherung an die gewünschte Funktion f ist. <br><br>  Was ist mit den Funktionen vieler Variablen? <br><br>  Versuchen wir drei Variablen zu nehmen: x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Kann das folgende Netzwerk verwendet werden, um die Turmfunktion in vier Dimensionen zu berechnen? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Hier bezeichnen x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> den Netzwerkeingang.  s <sub>1</sub> , t <sub>1</sub> usw. - Schrittpunkte für Neuronen - das heißt, alle Gewichte in der ersten Schicht sind groß, und die Offsets werden so zugewiesen, dass die Punkte der Schritte s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... sind. Die Gewichte in der zweiten Schicht wechseln sich ab, + h, −h, wobei h eine sehr große Zahl ist.  Der Ausgangsoffset beträgt −5h / 2. <br><br>  Das Netzwerk berechnet unter drei Bedingungen eine Funktion gleich 1: x <sub>1</sub> liegt zwischen s <sub>1</sub> und t <sub>1</sub> ;  x <sub>2</sub> liegt zwischen s <sub>2</sub> und t <sub>2</sub> ;  x <sub>3</sub> liegt zwischen s <sub>3</sub> und t <sub>3</sub> .  Das Netzwerk ist an allen anderen Orten 0.  Dies ist ein Turm, in dem 1 einen kleinen Teil des Eingangsraums darstellt und 0 alles andere ist. <br><br>  Wenn wir viele solcher Netzwerke zusammenkleben, können wir so viele Türme erhalten, wie wir möchten, und eine beliebige Funktion von drei Variablen approximieren.  Die gleiche Idee funktioniert in m Dimensionen.  Nur der Ausgangsversatz (−m + 1/2) h wird geändert, um die gewünschten Werte richtig zu drücken und das Plateau zu entfernen. <br><br>  Nun wissen wir, wie man NS verwendet, um die reale Funktion vieler Variablen zu approximieren.  Was ist mit den Vektorfunktionen f (x <sub>1</sub> , ..., x <sub>m</sub> ) ∈ R <sup>n</sup> ?  Natürlich kann eine solche Funktion einfach als n separate reelle Funktionen f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ) usw. betrachtet werden.  Und dann kleben wir einfach alle Netzwerke zusammen.  Es ist also einfach, es herauszufinden. <br><br><h3>  Herausforderung </h3><br><ul><li>  Wir haben gesehen, wie man neuronale Netze mit zwei verborgenen Schichten verwendet, um eine beliebige Funktion zu approximieren.  Können Sie beweisen, dass dies mit einer verborgenen Schicht möglich ist?  Tipp - Versuchen Sie, nur mit zwei Ausgabevariablen zu arbeiten, und zeigen Sie Folgendes: (a) Es ist möglich, die Funktionen der Schritte nicht nur entlang der x- oder y-Achse, sondern auch in einer beliebigen Richtung abzurufen.  (b) Addiert man viele Konstruktionen aus Schritt (a), ist es möglich, die Funktion eines runden statt eines rechteckigen Turms zu approximieren;  © Mit runden Türmen kann eine beliebige Funktion angenähert werden.  Schritt © wird mit dem in diesem Kapitel vorgestellten Material etwas einfacher. </li></ul><br><h2>  Über sigmoidale Neuronen hinausgehen </h2><br>  Wir haben bewiesen, dass ein Netzwerk von Sigmoidneuronen jede Funktion berechnen kann.  Denken Sie daran, dass sich in einem Sigmoid-Neuron die Eingänge x <sub>1</sub> , x <sub>2</sub> , ... am Ausgang in σ (∑ <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b) verwandeln, wobei w <sub>j</sub> die Gewichte sind, b die Verschiebung ist, σ das Sigmoid ist. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  Was ist, wenn wir einen anderen Neuronentyp mit einer anderen Aktivierungsfunktion betrachten, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Das heißt, wir nehmen an, dass wenn ein Neuron x <sub>1</sub> , x <sub>2</sub> , ... Gewichte w <sub>1</sub> , w <sub>2</sub> , ... und Vorspannung b hat, s (∑ <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b) ausgegeben wird. <br><br>  Wir können diese Aktivierungsfunktion verwenden, um Schritt zu machen, genau wie im Fall des Sigmoid.  Versuchen Sie (im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel</a> ) im Diagramm, das Gewicht auf beispielsweise w = 100 anzuheben: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Wie im Fall des Sigmoid wird dadurch die Aktivierungsfunktion komprimiert, was zu einer sehr guten Annäherung an die Schrittfunktion führt.  Wenn Sie den Versatz ändern, werden Sie feststellen, dass wir die Position des Schritts in eine beliebige ändern können.  Daher können wir dieselben Tricks wie zuvor verwenden, um jede gewünschte Funktion zu berechnen. <br><br>  Welche Eigenschaften sollte s (z) haben, damit dies funktioniert?  Wir müssen annehmen, dass s (z) gut definiert ist als z → −∞ und z → ∞.  Diese Grenzwerte sind zwei Werte, die von unserer Schrittfunktion akzeptiert werden.  Wir müssen auch davon ausgehen, dass diese Grenzen unterschiedlich sind.  Wenn sie sich nicht unterscheiden würden, würden die Schritte nicht funktionieren, es würde einfach einen flachen Zeitplan geben!  Wenn jedoch die Aktivierungsfunktion s (z) diese Eigenschaften erfüllt, sind die darauf basierenden Neuronen universell für Berechnungen geeignet. <br><br><h3>  Die Aufgaben </h3><br><ul><li>  Zu Beginn des Buches haben wir einen anderen Neuronentyp kennengelernt - ein begradigtes lineares Neuron oder eine gleichgerichtete lineare Einheit, ReLU.  Erklären Sie, warum solche Neuronen die für die Universalität erforderlichen Bedingungen nicht erfüllen.  Finden Sie Beweise für die Vielseitigkeit, die zeigen, dass ReLUs universell für die Datenverarbeitung geeignet sind. </li><li>  Angenommen, wir betrachten lineare Neuronen mit der Aktivierungsfunktion s (z) = z.  Erklären Sie, warum lineare Neuronen die Bedingungen der Universalität nicht erfüllen.  Zeigen Sie, dass solche Neuronen nicht für Universal Computing verwendet werden können. </li></ul><br><h2>  Schrittfunktion korrigieren </h2><br>  Vorläufig gingen wir davon aus, dass unsere Neuronen genaue Schrittfunktionen erzeugen.  Dies ist eine gute Annäherung, aber nur eine Annäherung.  Tatsächlich gibt es eine enge Fehlerlücke, die in der folgenden Grafik dargestellt ist und in der sich die Funktionen überhaupt nicht wie eine Schrittfunktion verhalten: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  In dieser Zeit des Scheiterns funktioniert meine Erklärung der Universalität nicht. <br><br>  Das Scheitern ist nicht so beängstigend.  Durch Einstellen ausreichend großer Eingabegewichte können diese Lücken beliebig klein gemacht werden.  Wir können sie viel kleiner als auf der Karte machen und für das Auge unsichtbar machen.  Vielleicht müssen wir uns also keine Sorgen um dieses Problem machen. <br><br>  Trotzdem hätte ich gerne einen Weg, es zu lösen. <br><br>  Es stellt sich heraus, dass es leicht zu lösen ist.  Schauen wir uns diese Lösung zur Berechnung von NS-Funktionen mit nur einer Eingabe und Ausgabe an.  Dieselben Ideen werden funktionieren, um das Problem mit einer großen Anzahl von Ein- und Ausgängen zu lösen. <br><br>  Angenommen, wir möchten, dass unser Netzwerk eine Funktion f berechnet.  Nach wie vor versuchen wir dies, indem wir das Netzwerk so gestalten, dass die gewichtete Ausgabe der verborgenen Neuronenschicht σ <sup>−1</sup> ⋅f (x) ist: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Wenn wir dies mit der oben beschriebenen Technik tun, werden wir die verborgenen Neuronen zwingen, eine Folge von Vorsprungsfunktionen zu erzeugen: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Natürlich habe ich die Größe der Ausfallintervalle übertrieben, damit es leichter zu sehen war.  Es sollte klar sein, dass wir, wenn wir alle diese Funktionen der Vorsprünge addieren, überall eine ziemlich gute Annäherung von σ <sup>−1</sup> ⋅f (x) erhalten, mit Ausnahme der Ausfallintervalle. <br><br>  Nehmen wir jedoch an, dass wir anstelle der gerade beschriebenen Näherung eine Reihe versteckter Neuronen verwenden, um die Näherung der Hälfte unserer ursprünglichen Zielfunktion zu berechnen, dh σ <sup>−1</sup> ⋅f (x) / 2.  Natürlich sieht es genauso aus wie eine skalierte Version des neuesten Diagramms: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Nehmen wir an, wir lassen einen weiteren Satz versteckter Neuronen die Annäherung an σ <sup>−1</sup> ⋅f (x) / 2 berechnen. An ihrer Basis werden die Vorsprünge jedoch um die Hälfte ihrer Breite verschoben: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Jetzt haben wir zwei verschiedene Näherungen für σ - 1⋅f (x) / 2.  Wenn wir diese beiden Näherungen addieren, erhalten wir eine allgemeine Näherung an σ - 1⋅f (x).  Diese allgemeine Annäherung weist in kleinen Intervallen immer noch Ungenauigkeiten auf.  Das Problem wird jedoch geringer sein als zuvor - da die Punkte, die in die Intervalle des Versagens der ersten Näherung fallen, nicht in die Intervalle des Versagens der zweiten Näherung fallen.  Daher ist die Annäherung in diesen Intervallen ungefähr zweimal besser. <br><br>  Wir können die Situation verbessern, indem wir eine große Anzahl M überlappender Approximationen der Funktion σ - 1⋅f (x) / M hinzufügen.  Wenn alle ihre Ausfallintervalle eng genug sind, wird jeder Strom nur in einem von ihnen sein.  Wenn Sie eine ausreichend große Anzahl überlappender Näherungen von M verwenden, ist das Ergebnis eine ausgezeichnete allgemeine Näherung. <br><br><h2>  Fazit </h2><br>  Die hier diskutierte Erklärung der Universalität kann definitiv nicht als praktische Beschreibung des Zählens von Funktionen unter Verwendung neuronaler Netze bezeichnet werden!  In diesem Sinne ist es eher ein Beweis für die Vielseitigkeit von NAND-Logikgattern und mehr.  Daher habe ich im Grunde versucht, dieses Design klar und einfach zu befolgen, ohne seine Details zu optimieren.  Der Versuch, dieses Design zu optimieren, kann jedoch eine interessante und lehrreiche Übung für Sie sein. <br><br>  Obwohl das erhaltene Ergebnis nicht direkt zum Erstellen von NS verwendet werden kann, ist es wichtig, da es die Frage nach der Berechenbarkeit einer bestimmten Funktion unter Verwendung von NS beseitigt.  Die Antwort auf eine solche Frage wird immer positiv sein.  Daher ist es richtig zu fragen, ob eine Funktion berechenbar ist, aber wie kann sie richtig berechnet werden? <br><br>  Unser universelles Design verwendet nur zwei versteckte Schichten, um eine beliebige Funktion zu berechnen.  Wie bereits erwähnt, ist es möglich, dasselbe Ergebnis mit einer einzelnen verborgenen Ebene zu erzielen.  Vor diesem Hintergrund fragen Sie sich vielleicht, warum wir tiefe Netzwerke benötigen, dh Netzwerke mit einer großen Anzahl versteckter Schichten.  Können wir diese Netzwerke nicht einfach durch flache Netzwerke mit einer verborgenen Schicht ersetzen? <br><br>  Obwohl es im Prinzip möglich ist, gibt es gute praktische Gründe für die Verwendung tiefer neuronaler Netze.  Wie in Kapitel 1 beschrieben, haben tiefe NS eine hierarchische Struktur, die es ihnen ermöglicht, sich gut anzupassen, um hierarchisches Wissen zu studieren, das zur Lösung realer Probleme nützlich ist.  Insbesondere bei der Lösung von Problemen wie der Mustererkennung ist es nützlich, ein System zu verwenden, das nicht nur einzelne Pixel, sondern auch immer komplexere Konzepte versteht: von Rändern über einfache geometrische Formen und darüber hinaus bis hin zu komplexen Szenen mit mehreren Objekten.  In späteren Kapiteln werden wir Beweise dafür sehen, dass tiefe NS besser mit dem Studium solcher Wissenshierarchien umgehen können als flache.  Zusammenfassend: Die Universalität sagt uns, dass NS jede Funktion berechnen kann;  Empirische Erkenntnisse legen nahe, dass tiefe NS besser an die Untersuchung von Funktionen angepasst sind, die zur Lösung vieler realer Probleme nützlich sind. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461659/">https://habr.com/ru/post/de461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461649/index.html">Wie werde ich die Welt retten?</a></li>
<li><a href="../de461651/index.html">Frontend Weekly Digest (22. - 28. Juli 2019)</a></li>
<li><a href="../de461653/index.html">Software Defined Radio - wie funktioniert es? Teil 10</a></li>
<li><a href="../de461655/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 373 (22. - 28. Juli 2019)</a></li>
<li><a href="../de461657/index.html">Kauf eines roten Hutes: Wird es dem blauen Riesen helfen, für eine hybride Cloud-Führung zu kämpfen?</a></li>
<li><a href="../de461661/index.html">Komponentenbasiertes Entwicklungshandbuch</a></li>
<li><a href="../de461663/index.html">Die Geschichte, wie Linux Windows einbrachte</a></li>
<li><a href="../de461665/index.html">Zen2. Die Entwicklung der AM4-Plattform am Beispiel des Ryzen 7 3700x</a></li>
<li><a href="../de461669/index.html">PHP Digest Nr. 161 (15. - 29. Juli 2019)</a></li>
<li><a href="../de461673/index.html">8 Tipps für Programmieranfänger oder eine Retrospektive meiner Karriere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>