<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â˜£ï¸ ğŸš ğŸ¤µğŸ¼ Sharding-Theorie âœğŸ½ ğŸ‘©ğŸ¿â€ğŸ¤â€ğŸ‘¨ğŸ» ğŸ§˜ğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es scheint, dass wir so tief in den Dschungel der Hochlastentwicklung eingetaucht sind, dass wir einfach nicht Ã¼ber die Grundprobleme nachdenken. Nehm...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sharding-Theorie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/433370/">  Es scheint, dass wir so tief in den Dschungel der Hochlastentwicklung eingetaucht sind, dass wir einfach nicht Ã¼ber die Grundprobleme nachdenken.  Nehmen Sie zum Beispiel Scherben.  Was ist zu verstehen, wenn es mÃ¶glich ist, bedingte Shards = n in die Datenbankeinstellungen zu schreiben und alles von selbst erledigt wird?  Das ist richtig, das ist er, aber wenn eher, wenn etwas schief geht, die Ressourcen wirklich knapp werden, wÃ¼rde ich gerne verstehen, was der Grund ist und wie man es behebt. <br><br>  Kurz gesagt, wenn Sie Ihre alternative Hash-Implementierung in Cassandra beigesteuert haben, gibt es kaum EnthÃ¼llungen fÃ¼r Sie.  Wenn Ihre Dienste jedoch bereits ausgelastet sind und die Systemkenntnisse nicht mithalten kÃ¶nnen, sind Sie herzlich willkommen.  Der groÃŸe und schreckliche <strong>Andrei Aksyonov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">Shodan</a> ) wird auf seine Ã¼bliche Weise sagen, dass <strong>Scherben schlecht sind, nicht Scherben auch schlecht</strong> , und wie es im Inneren angeordnet ist.  Und ganz zufÃ¤llig geht es in einem Teil der Geschichte Ã¼ber das Scherben Ã¼berhaupt nicht um Scherben, aber der Teufel weiÃŸ, was - wie man Objekte Scherben zuordnet. <br><img src="https://habrastorage.org/webt/c9/ju/s6/c9jus6tadexnz4aih4q95bl7ega.jpeg"><br>  Das Foto der Robben (obwohl sie sich versehentlich als Welpen herausstellten) scheint bereits die Frage zu beantworten, warum dies alles ist, aber beginnen wir nacheinander. <br><a name="habracut"></a><br><h2>  Was ist Scherben? <br></h2><br>  Wenn Sie beharrlich googeln, stellt sich heraus, dass zwischen der sogenannten Partitionierung und dem sogenannten Sharding eine ziemlich unscharfe Grenze besteht.  Jeder nennt alles, was er will, als er will.  Einige Leute unterscheiden zwischen horizontaler Aufteilung und Scherbenbildung.  Andere sagen, dass Sharding eine bestimmte Art der horizontalen Partitionierung ist. <br><br>  Ich habe keinen einzigen terminologischen Standard gefunden, der von den GrÃ¼ndervÃ¤tern genehmigt und nach ISO zertifiziert wÃ¤re.  Ein persÃ¶nlicher innerer Glaube ist ungefÃ¤hr so: <strong>Partitionierung bedeutet</strong> im Durchschnitt, die Basis auf willkÃ¼rliche Weise in StÃ¼cke zu schneiden. <br><br><ul><li>  <strong>Vertikale</strong> Partitionierung  Zum Beispiel gibt es eine riesige Tabelle mit ein paar Milliarden EintrÃ¤gen in 60 Spalten.  Anstatt eine solche gigantische Tabelle zu fÃ¼hren, fÃ¼hren wir 60 nicht weniger gigantische Tabellen mit jeweils 2 Milliarden DatensÃ¤tzen - und dies ist keine Teilzeitdatenbank, sondern eine vertikale Partitionierung (als Beispiel fÃ¼r Terminologie). <br></li><li>  <strong>Horizontale</strong> Partitionierung - Wir schneiden Zeile fÃ¼r Zeile, mÃ¶glicherweise innerhalb des Servers. <br></li></ul><br>  Der unangenehme Moment hier ist der subtile Unterschied zwischen horizontaler Aufteilung und Scherbenbildung.  Sie kÃ¶nnen mich in StÃ¼cke schneiden, aber ich werde Ihnen nicht mit Sicherheit sagen, woraus es besteht.  Es besteht das GefÃ¼hl, dass Sharding und horizontale Partitionierung ungefÃ¤hr dasselbe sind. <br><br>  Sharding ist im Allgemeinen, wenn eine groÃŸe Tabelle in Bezug auf Datenbanken oder eine Sammlung von Dokumenten, Objekten, wenn Sie keine Datenbank, sondern einen Dokumentenspeicher haben, speziell fÃ¼r Objekte geschnitten wird.  Das heiÃŸt, StÃ¼cke aus 2 Milliarden Objekten werden ausgewÃ¤hlt, egal welcher GrÃ¶ÃŸe.  Objekte fÃ¼r sich in jedem Objekt werden nicht in StÃ¼cke geschnitten, wir zerlegen nicht in separate Spalten, sondern legen BÃ¼ndel an verschiedenen Stellen aus. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/xx_Lv1P_X_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der</a> VollstÃ¤ndigkeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">halber</a> auf die PrÃ¤sentation verlinken.</i> <br><br>  Subtile terminologische Unterschiede sind bereits aufgetreten.  Relativ gesehen kÃ¶nnen Postgres-Entwickler beispielsweise sagen, dass die horizontale Partitionierung erfolgt, wenn alle Tabellen, in die die Haupttabelle unterteilt ist, im selben Schema liegen und wenn sie auf verschiedenen Computern shardet. <br><br>  Im Allgemeinen besteht, ohne an die Terminologie einer bestimmten Datenbank und eines bestimmten Datenverwaltungssystems gebunden zu sein, das GefÃ¼hl, dass das Sharding nur zeilenweise und so weiter erfolgt - und das ist alles: <br><br><blockquote>  Sharding (~ =, \ in ...) Horizontale Partitionierung == ist typisch. <br></blockquote><br>  Ich betone normalerweise.  In dem Sinne, dass wir all dies nicht nur tun, um 2 Milliarden Dokumente in 20 Tabellen zu schneiden, von denen jede besser zu verwalten wÃ¤re, sondern um sie auf viele Kerne, viele Festplatten oder viele verschiedene physische oder virtuelle Server zu verteilen . <br><br>  Es versteht sich, dass wir dies tun, damit jeder Shard - jeder Daten-Shatka - viele Male repliziert wird.  Aber eigentlich nein. <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre> <br>  Wenn Sie eine solche Datenaufteilung vornehmen und aus einer riesigen SQL-Tabelle in MySQL 16 kleine Tabellen auf Ihrem tapferen Laptop generieren, ohne Ã¼ber einen einzigen Laptop, kein einziges Schema, keine einzige Datenbank usw. hinauszugehen.  usw.  - Alles, du hast schon Scherben. <br><br>  Wenn man sich an die Abbildung mit Welpen erinnert, fÃ¼hrt dies zu Folgendem: <br><br><ul><li>  Die Bandbreite nimmt zu. <br></li><li>  Die Latenz Ã¤ndert sich nicht, das heiÃŸt, jeder, sozusagen Arbeiter oder Verbraucher, bekommt in diesem Fall seine eigene.  Es ist nicht bekannt, was Welpen auf dem Bild sehen, aber Anfragen werden ungefÃ¤hr zur gleichen Zeit bearbeitet, als ob der Welpe alleine wÃ¤re. </li><li>  Oder beides und noch eine und immer noch hohe VerfÃ¼gbarkeit (Replikation). <br></li></ul><br>  <strong>Warum Bandbreite?</strong>  Manchmal haben wir solche Datenmengen, die nicht passen - es ist nicht klar, wo, aber sie passen nicht - um 1 {core |  Laufwerk |  Server |  ...}.  Es gibt einfach nicht genug Ressourcen und das wars.  Um mit diesem groÃŸen Datensatz arbeiten zu kÃ¶nnen, mÃ¼ssen Sie ihn ausschneiden. <br><br>  <strong>Warum Latenz?</strong>  Auf einem Kern ist das Scannen einer Tabelle mit 2 Milliarden Zeilen 20-mal langsamer als das Scannen von 20 Tabellen auf 20 Kerneln, und dies parallel.  Daten werden auf einer Ressource zu langsam verarbeitet. <br><br>  <strong>Warum hohe VerfÃ¼gbarkeit?</strong>  Oder wir schneiden die Daten, um das eine und das andere gleichzeitig zu tun, und gleichzeitig bieten mehrere Kopien jeder Shard-Replikation eine hohe VerfÃ¼gbarkeit. <br><br><h2>  Ein einfaches Beispiel fÃ¼r "wie man es mit den HÃ¤nden macht" <br></h2><br>  Das bedingte Sharding kann mithilfe der Testtabelle test.documents fÃ¼r 32 Dokumente und durch Generieren von 16 Testtabellen fÃ¼r jeweils ca. 2 Dokumente test.docs00, 01, 02, ..., 15 aus dieser Tabelle ausgeschnitten werden. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs00 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">0</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> docs15 <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> documents <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>%<span class="hljs-number"><span class="hljs-number">16</span></span>)=<span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre><br>  Warum etwa?  Da wir a priori nicht wissen, wie die ID verteilt wird, wenn von 1 bis einschlieÃŸlich 32, dann gibt es jeweils genau 2 Dokumente, andernfalls nicht. <br><br>  <strong>Wir machen das fÃ¼r was.</strong>  Nachdem wir 16 Tische erstellt haben, kÃ¶nnen wir 16 von dem, was wir brauchen, â€fangenâ€œ.  UnabhÃ¤ngig davon, worauf wir uns ausgeruht haben, kÃ¶nnen wir diese Ressourcen parallelisieren.  Wenn beispielsweise nicht genÃ¼gend Speicherplatz vorhanden ist, ist es sinnvoll, diese Tabellen in separate DatentrÃ¤ger zu zerlegen. <br><br>  All dies ist leider nicht kostenlos.  Ich vermute, dass es im Fall des kanonischen SQL-Standards (ich habe den SQL-Standard lange nicht mehr gelesen, vielleicht wurde er lange nicht mehr aktualisiert) keine offizielle standardisierte Syntax gibt, um einem SQL-Server zu sagen: â€Lieber SQL-Server, machen Sie mich zu 32 Shards und lege sie auf 4 Scheiben. "  In einzelnen Implementierungen gibt es jedoch hÃ¤ufig eine bestimmte Syntax, um dies im Prinzip zu tun.  PostgreSQL hat Mechanismen fÃ¼r die Partitionierung, MySQL MariaDB hat es, Oracle hat dies wahrscheinlich schon vor langer Zeit getan. <br><br>  Wenn wir dies jedoch von Hand tun, ohne DatenbankunterstÃ¼tzung und im Rahmen des Standards, <strong>zahlen</strong> wir <strong>bedingt die KomplexitÃ¤t des Zugriffs auf Daten</strong> .  Wo es ein einfaches SELECT * FROM-Dokument gab WHERE id = 123, jetzt 16 x SELECT * FROM docsXX.  Und nun, wenn wir versuchen wÃ¼rden, die Aufzeichnung per SchlÃ¼ssel zu erhalten.  Deutlich interessanter, wenn wir versuchen, eine frÃ¼he Reihe von Aufzeichnungen zu erhalten.  Nun (wenn ich betone, wie Narren, und innerhalb des Standards bleiben) mÃ¼ssen die Ergebnisse dieser 16 SELECT * FROM in der Anwendung kombiniert werden. <br><br>  <strong>Welche LeistungsÃ¤nderung ist zu erwarten?</strong> <br><br><ul><li>  Intuitiv linear. </li><li>  Theoretisch - sublinear, weil <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amdahl-Gesetz</a> . </li><li>  In der Praxis - vielleicht fast linear, vielleicht nicht. </li></ul><br>  In der Tat ist die richtige Antwort unbekannt.  Durch geschickte Anwendung der Sharding-Technik kÃ¶nnen Sie eine signifikante superlineare Verschlechterung im Betrieb Ihrer Anwendung erzielen, und sogar der DBA wird mit einem glÃ¼henden Poker ausgeliefert. <br><br>  Mal sehen, wie dies erreicht werden kann.  Es ist klar, dass nur die Einstellung auf PostgreSQL-Shards = 16 gesetzt wurde und sich dann von selbst abhebt - das ist nicht interessant.  Lassen Sie uns darÃ¼ber nachdenken, wie wir erreichen kÃ¶nnen, dass <em>wir durch das Splittern auf das 32-fache verlangsamen</em> , was unter dem Gesichtspunkt interessant ist, wie man dies nicht tut. <br><br>  Unsere Versuche, zu beschleunigen oder zu verlangsamen, werden immer gegen die Klassiker gerichtet sein - das gute alte Amdahl-Gesetz, das besagt, dass es keine perfekte Parallelisierung einer Anfrage gibt, es gibt immer einen konsistenten Teil. <br><br><h2>  Amdahl-Gesetz <br></h2><br><blockquote>  <strong><em>Es</em></strong> gibt <strong><em>immer</em></strong> einen serialisierten Teil. <br></blockquote><br>  Es gibt immer einen Teil der AusfÃ¼hrung der Anforderung, der parallel ist, und es gibt immer einen Teil, der nicht parallel ist.  Selbst wenn es Ihnen so scheint, als ob eine perfekt parallele Abfrage, die zumindest eine Zeile des Ergebnisses sammelt, das Sie an den Client senden mÃ¶chten, aus den von jedem Shard empfangenen Zeilen immer und immer konsistent ist. <br><br>  Es gibt immer eine Art sequentiellen Teil.  Es kann winzig sein, vor dem allgemeinen Hintergrund absolut unsichtbar, es kann gigantisch sein und dementsprechend die Parallelisierung stark beeinflussen, aber es ist immer da. <br><br>  DarÃ¼ber hinaus <strong><em>Ã¤ndert sich</em></strong> sein Einfluss und kann erheblich zunehmen. Wenn wir beispielsweise unsere Tabelle von 64 DatensÃ¤tzen auf 16 Tabellen mit 4 DatensÃ¤tzen kÃ¼rzen - erhÃ¶hen wir die Raten -, Ã¤ndert sich dieser Teil.  Angesichts dieser gigantischen Datenmengen arbeiten wir natÃ¼rlich mit einem Mobiltelefon und einem 86-MHz-Prozessor. Wir haben nicht genÃ¼gend Dateien, die gleichzeitig geÃ¶ffnet bleiben kÃ¶nnen.  Anscheinend Ã¶ffnen wir mit solchen Eingaben jeweils eine Datei. <br><br><ul><li>  Es war <strong>Total =</strong> <strong>Serial +</strong> <strong>Parallel</strong> .  Wobei beispielsweise die gesamte Arbeit in der Datenbank parallel ist und seriell das Ergebnis an den Client sendet. <br></li><li>  Es wurde <strong>Total2 = Seriell + Parallel / N + Xserial.</strong>  Zum Beispiel, wenn die allgemeine ORDER BY, Xserial&gt; 0. <br></li></ul><br>  Mit diesem einfachen Beispiel versuche ich zu zeigen, dass Xserial angezeigt wird.  ZusÃ¤tzlich zu der Tatsache, dass es immer ein serialisiertes Teil gibt und dass wir versuchen, parallel mit Daten zu arbeiten, scheint ein zusÃ¤tzliches Teil dieses Daten-Slicing sicherzustellen.  Grob gesagt brauchen wir vielleicht: <br><br><ul><li>  Finden Sie diese 16 Tabellen im internen DatenbankwÃ¶rterbuch. </li><li>  Dateien Ã¶ffnen; </li><li>  Speicher zuweisen; </li><li>  Speicher verschieben; </li><li>  fÃ¤rben Sie die Ergebnisse; </li><li>  zwischen Kernen synchronisieren; </li></ul><br>  Nicht synchronisierte Effekte werden immer angezeigt.  Sie kÃ¶nnen unbedeutend sein und ein Milliardstel der Gesamtzeit einnehmen, aber sie sind immer ungleich Null und existieren immer.  Mit ihrer Hilfe kÃ¶nnen wir nach dem Splittern die ProduktivitÃ¤t drastisch verlieren. <br><br><img src="https://habrastorage.org/webt/fh/mx/yh/fhmxyh9tozfrbd4yszxj2va9a1g.jpeg"><br><br>  Dies ist ein Standardbild Ã¼ber Amdahls Gesetz.  Es ist nicht sehr gut lesbar, aber es ist wichtig, dass die Linien, die idealerweise gerade sind und linear wachsen, an der Asymptote anliegen.  Da die Grafik aus dem Internet jedoch nicht lesbar ist, habe ich meiner Meinung nach mehr visuelle Tabellen mit Zahlen erstellt. <br><br>  Angenommen, wir haben einen serialisierten Teil der Anforderungsverarbeitung, der nur 5% dauert: <strong>serial = 0.05 = 1/20.</strong> <br><br>  Intuitiv scheint es, dass mit dem serialisierten Teil, der nur 1/20 der Anforderungsverarbeitung benÃ¶tigt, wenn wir die Verarbeitung der Anforderung um 20 Kerne parallelisieren, diese im schlimmsten Fall 18-mal schneller wird. <br><br>  In der Tat ist <b>Mathematik eine herzlose Sache</b> : <br><br> <code>wall = 0.05 + 0.95/num_cores, speedup = 1 / (0.05 + 0.95/num_cores)</code> <br> <br>  Es stellt sich heraus, dass bei sorgfÃ¤ltiger Berechnung mit einem serialisierten Teil von 5% die Beschleunigung das 10-fache (10,3) betrÃ¤gt, und dies sind 51% im Vergleich zum theoretischen Ideal. <br><br><table><tbody><tr><td>  8 Kerne </td><td>  = 5,9 </td><td>  <font color="#c45911">= 74%</font> </td></tr><tr><td>  10 Kerne </td><td>  = 6,9 </td><td>  <font color="#c45911">= 69%</font> </td></tr><tr><td>  <strong>20 Kerne</strong> </td><td>  <strong>= 10,3</strong> </td><td>  <strong><font color="#c45911">= 51%</font></strong> </td></tr><tr><td>  40 Kerne </td><td>  = 13,6 </td><td>  <font color="#ff0000">= 34%</font> </td></tr><tr><td>  128 Kerne </td><td>  = 17,4 </td><td>  <font color="#ff0000">= 14%</font> </td></tr></tbody></table><br>  Wenn Sie 20 Kerne (20 Festplatten, wenn Sie mÃ¶chten) fÃ¼r die Aufgabe verwenden, an der Sie zuvor gearbeitet haben, werden wir theoretisch nie mehr als 20 Mal beschleunigt, aber praktisch viel weniger.  DarÃ¼ber hinaus nimmt die Ineffizienz mit zunehmender Anzahl von Parallelen rapide zu. <br><br>  Wenn nur noch 1% der serialisierten Arbeit Ã¼brig bleibt und 99% parallelisiert sind, werden die Beschleunigungswerte etwas verbessert: <br><br><table><tbody><tr><td>  8 Kerne </td><td>  = 7,5 </td><td>  <font color="#538135">= 93%</font> </td></tr><tr><td>  16 Kerne </td><td>  = 13,9 </td><td>  <font color="#538135">= 87%</font> </td></tr><tr><td>  32 Kerne </td><td>  = 24,4 </td><td>  <font color="#c45911">= 76%</font> </td></tr><tr><td>  64 Kerne </td><td>  = 39,3 </td><td>  <font color="#c45911">= 61%</font> </td></tr></tbody></table><br>  Bei einer vollstÃ¤ndig thermonuklearen Abfrage, die natÃ¼rlich stundenlang ausgefÃ¼hrt wird, und bei der Vorbereitung und Zusammenstellung des Ergebnisses ist nur sehr wenig Zeit erforderlich (Seriennummer = 0,001). Wir werden bereits eine gute Effizienz feststellen: <br><br><table><tbody><tr><td>  8 Kerne </td><td>  = 7,94 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  16 Kerne </td><td>  = 15,76 </td><td>  <font color="#538135">= 99%</font> </td></tr><tr><td>  32 Kerne </td><td>  = 31.04 </td><td>  <font color="#538135">= 97%</font> </td></tr><tr><td>  64 Kerne </td><td>  = 60,20 </td><td>  <font color="#538135">= 94%</font> </td></tr></tbody></table><br>  Bitte beachten Sie, dass <strong>wir niemals 100% sehen werden</strong> .  In besonders guten FÃ¤llen sehen Sie beispielsweise 99,999%, aber nicht genau 100%. <br><br><h2>  Wie man N-mal mischt und einbricht? <br></h2><br>  Sie kÃ¶nnen genau N-mal mischen und einbrechen: <br><br><ol><li>  Senden Sie docs00 ... docs15-Anfragen <strong>nacheinander</strong> , nicht parallel. </li><li>  WÃ¤hlen Sie in einfachen Abfragen <strong>nicht</strong> <strong>nach SchlÃ¼ssel aus</strong> , WO etwas = 234. </li></ol><br>  In diesem Fall belegt der serialisierte Teil (seriell) nicht 1% und nicht 5%, sondern etwa 20% in modernen Datenbanken.  Sie kÃ¶nnen 50% des serialisierten Teils erhalten, wenn Sie mit einem Ã¤uÃŸerst effizienten BinÃ¤rprotokoll auf die Datenbank zugreifen oder sie als dynamische Bibliothek mit einem Python-Skript verknÃ¼pfen. <br><br>  Der Rest der Verarbeitungszeit fÃ¼r eine einfache Anforderung wird durch nicht parallelisierte VorgÃ¤nge zum Parsen der Anforderung, Erstellen des Plans usw. belegt.  Das heiÃŸt, es wird langsamer, wenn die Aufzeichnung nicht gelesen wird. <br><br>  Wenn wir die Daten in 16 Tabellen aufteilen und nacheinander ausfÃ¼hren, wie es beispielsweise in der PHP-Programmiersprache Ã¼blich ist (es weiÃŸ nicht, wie asynchrone Prozesse sehr gut ausgefÃ¼hrt werden sollen), wird nur eine 16-fache Verlangsamung angezeigt.  Und vielleicht sogar noch mehr, weil auch Netzwerk-Roundtrips hinzugefÃ¼gt werden. <br><br><blockquote>  Beim Sharding ist plÃ¶tzlich die Wahl einer Programmiersprache wichtig. <br></blockquote><br>  Wir erinnern uns an die Wahl einer Programmiersprache, denn wenn Sie nacheinander Anfragen an die Datenbank (oder den Suchserver) senden, woher kommt dann die Beschleunigung?  Vielmehr wird eine Verlangsamung auftreten. <br><br><h3>  Fahrrad aus dem Leben <br></h3><br>  Wenn Sie sich fÃ¼r C ++ entscheiden, <strong>schreiben Sie in POSIX-Threads</strong> und nicht in Boost I / O.  Ich habe eine ausgezeichnete Bibliothek von erfahrenen Entwicklern von Oracle und MySQL selbst gesehen, die die Kommunikation mit dem MySQL-Server auf Boost geschrieben haben.  Anscheinend waren sie gezwungen, bei der Arbeit in reinem C zu schreiben, aber dann gelang es ihnen, sich umzudrehen, Boost mit asynchroner E / A zu verwenden usw.  Ein Problem - diese asynchrone E / A, die theoretisch 10 Anforderungen parallel hÃ¤tte steuern sollen, hatte aus irgendeinem Grund einen unsichtbaren Synchronisationspunkt im Inneren.  Wenn 10 Anforderungen parallel gestartet wurden, wurden sie genau 20 Mal langsamer als eine ausgefÃ¼hrt, da 10 Mal zu den Anforderungen selbst und einmal zum Synchronisationspunkt. <br><br>  <strong>Fazit:</strong> Schreiben Sie in Sprachen, die paralleles AusfÃ¼hren implementieren und gut auf unterschiedliche Anforderungen warten.  Ich weiÃŸ ehrlich gesagt nicht, was genau neben Go zu raten ist.  Nicht nur, weil ich Go wirklich liebe, sondern weil ich nichts passenderes weiÃŸ. <br><br>  <strong>Schreiben Sie nicht in ungeeigneten Sprachen,</strong> in denen Sie nicht 20 parallele Abfragen an die Datenbank ausfÃ¼hren kÃ¶nnen.  Oder machen Sie bei jeder Gelegenheit nicht alles mit Ihren HÃ¤nden - verstehen Sie, wie es funktioniert, aber machen Sie es nicht manuell. <br><br><h2>  A / B Testrad <br></h2><br>  Manchmal kÃ¶nnen Sie langsamer fahren, weil Sie daran gewÃ¶hnt sind, dass alles funktioniert, und Sie haben nicht bemerkt, dass der serialisierte Teil erstens ein groÃŸer ist. <br><br><ul><li>  Sofort ~ 60 Suchindex-Shards, Kategorien </li><li>  Dies sind korrekte und korrekte Shards unter einem Themenbereich. </li><li>  Es gab bis zu 1000 Dokumente und es gab 50.000 Dokumente. </li></ul><br>  Dies ist ein Serienrad, bei dem die Suchanfragen leicht geÃ¤ndert wurden und viel mehr Dokumente aus 60 Shards des Suchindex ausgewÃ¤hlt wurden.  Alles funktionierte schnell und nach dem Prinzip: â€Es funktioniert - fass es nicht anâ€œ, alle haben es vergessen, das sich tatsÃ¤chlich in 60 Scherben befindet.  Wir haben die Stichprobengrenze fÃ¼r jeden Shard von tausend auf 50.000 Dokumente erhÃ¶ht.  PlÃ¶tzlich wurde es langsamer und die ParallelitÃ¤t hÃ¶rte auf.  Die Anfragen selbst, die nach Scherben ausgefÃ¼hrt wurden, flogen recht gut, und die BÃ¼hne wurde verlangsamt, als 50.000 Dokumente aus 60 Scherben gesammelt wurden.  Diese 3 Millionen endgÃ¼ltigen Dokumente auf einem Kern wurden zusammengefÃ¼hrt, sortiert, die Spitze von 3 Millionen wurde ausgewÃ¤hlt und dem Kunden Ã¼bergeben.  Der gleiche serielle Teil verlangsamte sich, das gleiche rÃ¼cksichtslose Gesetz von Amdal wirkte. <br><br>  <em>Vielleicht solltest du nicht mit deinen HÃ¤nden scherben, sondern nur menschlich</em> <em><br></em>  <em>Sagen Sie der Datenbank: "Mach es!"</em> <em><br></em> <br>  <strong>Haftungsausschluss:</strong> Ich weiÃŸ nicht wirklich, wie ich etwas richtig machen soll.  Ich bin wie aus dem falschen Stock !!! <br><br>  Ich habe wÃ¤hrend meines gesamten bewussten Lebens eine Religion namens â€algorithmischer Fundamentalismusâ€œ gefÃ¶rdert.  Es wird kurz ganz einfach formuliert: <br><br><blockquote>  Sie mÃ¶chten eigentlich nichts mit Ihren HÃ¤nden tun, aber es ist Ã¤uÃŸerst nÃ¼tzlich zu wissen, wie es im Inneren angeordnet ist.  Damit Sie in dem Moment, in dem in der Datenbank etwas schief geht, zumindest verstehen, was dort schief gelaufen ist, wie es im Inneren angeordnet ist und wie es ungefÃ¤hr repariert werden kann. <br></blockquote><br>  Schauen wir uns die Optionen an: <br><br><ol><li>  <strong>"HÃ¤nde</strong> . <strong>"</strong>  Zuvor haben wir die Daten manuell in 16 virtuelle Tabellen fragmentiert und alle Abfragen mit unseren HÃ¤nden neu geschrieben - dies ist Ã¤uÃŸerst unangenehm.  <strong>Wenn die MÃ¶glichkeit besteht, die HÃ¤nde nicht zu mischen, mischen Sie nicht die HÃ¤nde!</strong>  Aber manchmal ist dies nicht mÃ¶glich, zum Beispiel haben Sie MySQL 3.23 und mÃ¼ssen es dann. </li><li>  <strong>"Automatisch".</strong>  Es kommt vor, dass Sie automatisch oder fast automatisch mischen kÃ¶nnen. Wenn die Datenbank die Daten selbst verteilen kann, mÃ¼ssen Sie nur irgendwo eine bestimmte Einstellung grob schreiben.  Es gibt viele Basen und sie haben viele verschiedene Einstellungen.  Ich bin sicher, dass in jeder Datenbank, in der es mÃ¶glich ist, Shards = 16 zu schreiben (unabhÃ¤ngig von der Syntax), viele andere Einstellungen von der Engine auf diesen Fall geklebt werden. </li><li>  <strong>"Halbautomatisch"</strong> - meiner Meinung nach ein vÃ¶llig kosmischer und brutaler Modus.  Das heiÃŸt, die Basis selbst scheint nicht dazu in der Lage zu sein, aber es gibt externe zusÃ¤tzliche Patches. </li></ol><br>  Es ist schwierig, etwas Ã¼ber die Maschine zu erzÃ¤hlen, auÃŸer es an die Dokumentation in der entsprechenden Datenbank zu senden (MongoDB, Elastic, Cassandra, ... im Allgemeinen das sogenannte NoSQL).  Wenn Sie GlÃ¼ck haben, ziehen Sie einfach den Schalter â€Mach mich zu 16 Scherbenâ€œ und alles wird funktionieren.  In diesem Moment, wenn es nicht funktioniert, kann der Rest des Artikels notwendig sein. <br><br><h2>  Ãœber halbautomatisches GerÃ¤t <br></h2><br>  An einigen Stellen inspirieren ausgefeilte Informationstechnologien den chthonischen Horror.  Zum Beispiel hatte MySQL sofort keine Implementierung von Sharding fÃ¼r bestimmte Versionen, dennoch wÃ¤chst die GrÃ¶ÃŸe der im Kampf betriebenen Basen auf unanstÃ¤ndige Werte. <br><br>  Das Leiden der Menschheit angesichts einzelner Datenbankadministratoren wird seit Jahren gequÃ¤lt und schreibt mehrere schlechte Sharding-LÃ¶sungen, die ohne Grund entwickelt wurden.  Danach wird eine mehr oder weniger anstÃ¤ndige Sharding-LÃ¶sung namens ProxySQL geschrieben (MariaDB / Spider, PG / pg_shard / Citus, ...).  Dies ist ein bekanntes Beispiel fÃ¼r denselben Mantel. <br><br>  ProxySQL als Ganzes ist natÃ¼rlich eine KomplettlÃ¶sung der Enterprise-Klasse fÃ¼r Open Source, Routing und mehr.  Eine der zu lÃ¶senden Aufgaben ist jedoch das Sharding fÃ¼r eine Datenbank, die an sich nicht weiÃŸ, wie man menschlich shardet.  Sie sehen, es gibt keinen "Shards = 16" -Schalter. Entweder mÃ¼ssen Sie jede Anforderung in der Anwendung neu schreiben, und es gibt viele davon, oder Sie legen eine Zwischenebene zwischen die Anwendung und die Datenbank, die so aussieht: "Hmm ... SELECT * FROM Documents?"  Ja, es muss in 16 kleine SELECT * FROM server1.document1, SELECT * FROM server2.document2 zerrissen werden - zu diesem Server mit diesem Benutzernamen / Passwort, zu diesem mit einem anderen.  Wenn man nicht antwortete, dann ... "usw. <br><br>  Genau dies kann durch Zwischen-Patches erfolgen.  Sie sind etwas geringer als bei allen Datenbanken.  FÃ¼r PostgreSQL gibt es meines Wissens gleichzeitig einige integrierte LÃ¶sungen (PostgresForeign Data Wrappers sind meiner Meinung nach in PostgreSQL selbst integriert), es gibt externe Patches. <br><br>  Die Konfiguration jedes einzelnen Patches ist ein separates groÃŸes Thema, das nicht in einen Bericht passt. Daher werden nur grundlegende Konzepte erÃ¶rtert. <br><br>  Lassen Sie uns besser ein wenig Ã¼ber die Buzz-Theorie sprechen. <br><br><h2>  Absolut perfekte Automatisierung? <br></h2><br>  Die ganze Theorie des Summens beim Sharding in diesem Buchstaben F (), das Grundprinzip ist <strong>immer</strong> das gleiche RohÃ¶l: <code>shard_id = F(object).</code> <br><br>  Beim Sharding geht es im Allgemeinen um was?  Wir haben 2 Milliarden DatensÃ¤tze (oder 64).  Wir wollen sie in mehrere Teile teilen.  Eine unerwartete Frage stellt sich - wie?  Nach welchem â€‹â€‹Prinzip sollte ich meine 2 Milliarden DatensÃ¤tze (oder 64) auf 16 Server verteilen, die mir zur VerfÃ¼gung stehen? <br><br>  Der latente Mathematiker in uns sollte vorschlagen, dass es am Ende immer eine bestimmte magische Funktion gibt, die fÃ¼r jedes Dokument (Objekt, Linie usw.) bestimmt, in welches StÃ¼ck es eingefÃ¼gt werden soll. <br><br>  Wenn wir tiefer in die Mathematik einsteigen, hÃ¤ngt diese Funktion immer nicht nur vom Objekt selbst (der Linie selbst) ab, sondern auch von externen Einstellungen wie der Gesamtzahl der Shards.  Die Funktion, die fÃ¼r jedes Objekt angeben muss, wo es abgelegt werden soll, kann keinen Wert mehr zurÃ¼ckgeben, als Server auf dem System vorhanden sind.  Und die Funktionen sind etwas anders: <br><br><ul><li>  shard_func = <strong>F1</strong> (Objekt); <br></li><li>  shard_id = <strong>F2</strong> (shard_func, ...); </li><li>  shard_id = <strong>F2</strong> ( <strong>F1</strong> (Objekt), current_num_shards, ...). </li></ul><br>  Aber weiter werden wir uns nicht mit diesen Dschungeln einzelner Funktionen befassen, sondern nur darÃ¼ber sprechen, was magische Funktionen F () sind. <br><br><h2>  Was ist F ()? <br></h2><br>  Sie kÃ¶nnen mit vielen verschiedenen Implementierungsmechanismen aufwarten.  Beispielzusammenfassung: <br><br><ul><li>  F = <strong>rand</strong> ()% nums_shards </li><li>  F = <strong>somehash</strong> (object.id)% num_shards </li><li>  F = object.date% num_shards </li><li>  F = object.user_id% num_shards </li><li>  ... </li><li>  F = shard_table [somehash () | ... object.date | ...] </li></ul><br>  Eine interessante Tatsache - Sie kÃ¶nnen natÃ¼rlich alle Daten zufÃ¤llig verteilen - wir werfen den nÃ¤chsten Datensatz auf einen beliebigen Server, auf einen beliebigen Kernel, in eine beliebige Tabelle.  Es wird nicht viel GlÃ¼ck geben, aber es wird funktionieren. <br><br>  Es gibt etwas intelligentere Methoden zum BetrÃ¼gen nach reproduzierbaren oder sogar konsistenten Hash-Funktionen oder zum BetrÃ¼gen nach bestimmten Attributen.  Lassen Sie uns jede Methode durchgehen. <br><br><h3>  F = rand () <br></h3><br>  Herumstreuen ist keine sehr korrekte Methode.  Ein Problem: Wir haben unsere 2 Milliarden DatensÃ¤tze pro tausend Server zufÃ¤llig verteilt und wissen nicht, wo der Datensatz liegt.  Wir mÃ¼ssen user_1 ziehen, wissen aber nicht, wo es ist.  Wir gehen zu tausend Servern und sortieren alles - irgendwie ist es ineffizient. <br><br><h3>  F = somehash () <br></h3><br>  Lassen Sie uns Benutzer auf erwachsene Weise verteilen: Lesen Sie die reproduzierte Hash-Funktion aus user_id, nehmen Sie den Rest der Division durch die Anzahl der Server und greifen Sie sofort auf den gewÃ¼nschten Server zu. <br><br>  <em>Warum machen wir das?</em>  <em>Und dann, dass wir eine hohe Last haben und nichts auf einen Server bekommen.</em>  <em>Wenn man sich einmischt, wÃ¤re das Leben so einfach.</em> <br><br>  Nun, die Situation hat sich bereits verbessert. Um einen Datensatz zu erhalten, gehen wir zu einem bekannten Server.  Wenn wir jedoch einen SchlÃ¼sselbereich haben, mÃ¼ssen wir in diesem Bereich alle SchlÃ¼sselwerte sortieren und im Limit entweder zu so vielen Shards gehen, wie wir SchlÃ¼ssel im Bereich haben, oder zu jedem Server im Allgemeinen.  Die Situation hat sich natÃ¼rlich verbessert, aber nicht fÃ¼r alle Anfragen.  Einige Anfragen sind betroffen. <br><br><h3>  NatÃ¼rliches Sharding (F = object.date% num_shards) <br></h3><br>  Manchmal, das heiÃŸt oft, sind 95% des Datenverkehrs und 95% der Last Anforderungen, die eine Art natÃ¼rliches Sharding aufweisen. , 95%  -       1 , 3 , 7 ,   5%     .  95% ,  ,    ,        . <br><br>        , ,   ,         -           . <br><br>   â€”        ,      .       ,    , , ,    .        5 %  . <br><br>       ,    : <br><br><ol><li>      ,  95%     . </li><li>  95%    ,       ,     .   ,           .     ,     . </li></ol><br>  ,      â€”    ,         - . <br><br>   ,   ,         ,     ,         .       Â«   -      Â». <br><br> <strong>     Â«Â».</strong> ,            . <br><br><h3> 1.  :   <br></h3><br>    ,      ,  . <br><br><ul><li>    ,   ! </li><li> <strong><em></em></strong>  () . </li></ul><br>   , /  , ,  , PM    (       ,  PM   ),     .     . <br><br>  ,    .      ,       ,    100   .        . <br><br>   ,  ,   ,            ,    - . <br><br><h3> 2. Â«Â» : , join <br></h3><br>   ,             ? <br><br><ul><li>  Â«Â» â€¦ WHERE randcol BETWEEN aaa AND bbb? <br></li><li>  Â«Â» â€¦ users_32shards JOIN posts_1024 shards? </li></ul><br>  : , ! <br><br>           ,    ,       ,           .      .       (, , document store    ),     ,     . <br><br>   â€” <strong>-       </strong> .     .  ,          .     ,       ,    ,   .       - , ,         ,   ,         â€”    . <br><br>       ,             . <br><br><h3> 3. / :  <br></h3><br> :         ,          . <br><br><blockquote>    ,   . <br></blockquote><br>      ,  , ,  .     ,     ,   ,    10 , -        30,       100   .    .          â€”       ,  -   â€”  , -  . <br><br> ,      :  16 -,  32. ,   17,  23 â€”    .      ,  ,    -  ? <br><br>  : ,    ,     . <br><br>  ,    Â«Â»,   Â« Â». <br><br><h4>   #1.   <br></h4><br><ul><li>     NewF(object),    . </li><li>   NewF()=OldF() . </li><li>   <strong> .</strong> </li><li>  Autsch. </li></ul><br>  ,    2       ,  ,  .   :  17 ,  6   ,  2  ,    17   23 .   10  , ,    .      . <br><br><h4>   #2.   <br></h4><br>    â€”       â€”  17    23,     16   32 !         ,        . <br><br><ul><li>     NewF(object),    . </li><li> <strong>  2^N,   2^(N+1) .</strong> </li><li>   NewF()=OldF()  0,5. </li><li>   50% . </li><li> ,   <strong>   .</strong> </li></ul><br>  ,  ,         .   ,   ,  . <br><br>  ,            .   ,  16     16,      â€”    . <br><br> ,        â€”     . <br><br><h4>  #3. Consistent hashing <br></h4><br> ,       consistent hashing <br><img src="https://habrastorage.org/webt/il/ml/rt/ilmlrt9xy-c3wuyfaafntagufay.jpeg"><br><br>   Â«consistent hashingÂ»,    ,    . <br><br> :    ()   ,      .    ,     ,  ,      (  ,     ), . <br><br><ul><li>   :  <strong><em> </em></strong> ,   2 Â«Â»,    1/n. <br></li><li>   :    ,   .  . </li></ul><br>          ,         .  ,      ,      ,     :     ,          . <br><br>        .  ,        .  ,   ..,    .  ,   - , ,        . <br><br>       ,  , ,  Cassandra   .  ,         , ,      , ,  . <br><br>   ,        â€”     /    ,   ,    . <br><br> , :    ?       ? â€” ,  ! <br><br><h4>  #4. Rendezvous/HRW <br></h4><br>    (  ,   ): <strong>shard_id = arg max hash(object_id, shard_id).</strong> <br><br>    Rendezvous hashing,   ,  ,    Highest Random Weight.      : <br><img src="https://habrastorage.org/webt/0t/dt/rm/0tdtrm0iftxxb5ors5a2wxcex8s.jpeg"><br>   , , 16 .    (),   - ,  16 ,      .      -,   . <br><br>    HRW-hashing,   Rendezvous hashing.       , -,        ,   . <br><br>    ,       .  ,        - -        .      . <br><br>   ,       . <br><br><h4>  #5.   <br></h4><br> ,        Google    -   : <br><br><ul><li> Jump Hash â€” Google '2014. </li><li> Multi Probe â€”Google '2015. </li><li> Maglev â€” Google '2016. </li></ul><br>    ,    .      ,   ,    , -,       .      . <br><br><h4>  #6.  <br></h4><br>      â€”  .     ?   ,     2  ,          object_id  2  ,     . <br><br>  ,       ?    ? <br><br>     . ,   -     ,   ,  .  ,      , ,  ,     . <br><br> : <br><br><ul><li>  1  . </li><li>      /  /  /       : min/max_id =&gt; shard_id. </li><li>    8    4    (4      !) â€”  20    . </li><li>      -   ,        20  â€”     . </li><li> 20  â€”                 . </li></ul><br>     2     -    16  â€”   100   -   .       : ,         ,   â€”  1 .     ,  ,   . <br><br> ,    ,     ,    - ,     . <br><br><h1>  Schlussfolgerungen <br></h1><br>            : Â«  ,   !Â».       ,     20 . <br><br>   ,   ,     .   ,  <strong>   </strong> â€”   .     100$        ,     .          -,    .     â€”   . <br><br> <strong>    </strong> , ,  Â«Â» (, DFS, ...)   .   ,   , highload   -   .  ,        ,     - .     â€” <strong> ,    </strong> . <br><br>     <strong> </strong> <strong>F()</strong> ,   , ,  ..  , ,    2   <strong>     </strong> . <br><br><h2>   <br></h2><br> ,      ,        .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> HighLoad++</a> ,  ,     â€”Sphinxâ€”highload  ,   . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qpGljUyIht8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>    <br></h2><br>         Highload User Group.  ,    . <br><br>  , ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HighLoad++</a>     .         , ,  .  ,            , .     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>   highload-,   . <br><br>        ,  ,     ,  . ,           , ,        . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> <strong>24   -</strong>      Â«Â», Â« Â».  ,        .      ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> . <br><br><blockquote>         , ,  <strong>8  9   -  </strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>HighLoad++</strong></a>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> early bird . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de433370/">https://habr.com/ru/post/de433370/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de433360/index.html">Wissenschaftler haben versucht vorherzusagen, wann elektrische Flugzeuge RealitÃ¤t werden</a></li>
<li><a href="../de433362/index.html">9 Prinzipien von SchÃ¶nheit, Einfachheit und Pflege in UX</a></li>
<li><a href="../de433364/index.html">LDraw + Unity. Wie ich Lego generiert habe</a></li>
<li><a href="../de433366/index.html">Arbeiten mit externen Ressourcen in Unity3D</a></li>
<li><a href="../de433368/index.html">Wie man das Denken von Lebensmitteln auf die Welt anwendet: ein Beispiel fÃ¼r ein Sweatshirt</a></li>
<li><a href="../de433372/index.html">Auto Fahrrad</a></li>
<li><a href="../de433374/index.html">Die ganze Wahrheit Ã¼ber RTOS. Artikel Nr. 26. KanÃ¤le: Nebendienstleistungen und Datenstrukturen</a></li>
<li><a href="../de433376/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 21: Datenverfolgung, Teil 1</a></li>
<li><a href="../de433378/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 21: Datenverfolgung, Teil 2</a></li>
<li><a href="../de433380/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 21: Datenverfolgung, Teil 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>