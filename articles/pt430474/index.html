<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêé ‚úîÔ∏è üëºüèø CephFS vs GlusterFS üõí üë®‚Äçüë®‚Äçüë¶ ü§ó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Como engenheiro de infraestrutura da equipe de desenvolvimento da plataforma em nuvem , tive a oportunidade de trabalhar com muitos sistemas de armaze...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CephFS vs GlusterFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/430474/"><p>  Como engenheiro de infraestrutura da equipe de desenvolvimento da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plataforma</a> em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nuvem</a> , tive a oportunidade de trabalhar com muitos sistemas de armazenamento distribu√≠do, incluindo aqueles indicados no cabe√ßalho.  Parece que h√° um entendimento de seus pontos fortes e fracos, e tentarei compartilhar meus pensamentos com voc√™ sobre esse assunto.  Por assim dizer, vamos ver quem tem a fun√ß√£o hash por mais tempo. </p><br><p><img src="https://habrastorage.org/webt/fb/qn/vj/fbqnvjedf1ujf2hknyuxqbv82qy.png"></p><a name="habracut"></a><br><p>  Isen√ß√£o de responsabilidade: No in√≠cio deste blog, voc√™ podia ver artigos sobre o GlusterFS.  N√£o tenho nada a ver com esses artigos.  Este √© o blog do autor da equipe de projeto de nossa nuvem e cada um de seus membros pode contar sua hist√≥ria.  O autor desses artigos √© um engenheiro do nosso grupo de opera√ß√µes e ele tem suas pr√≥prias tarefas e sua experi√™ncia, que ele compartilhou.  Leve isso em considera√ß√£o se de repente voc√™ perceber uma diferen√ßa de opini√£o.  Aproveito esta oportunidade para expressar meus cumprimentos ao autor desses artigos! </p><br><h2 id="o-chem-poydet-rech">  O que ser√° discutido </h2><br><p> Vamos falar sobre sistemas de arquivos que podem ser criados com base no GlusterFS e no CephFS.  Discutiremos a arquitetura desses dois sistemas, olharemos para eles de diferentes √¢ngulos e, no final, correrei o risco de tirar conclus√µes.  Outros recursos do Ceph, como RBD e RGW, n√£o ser√£o afetados. </p><br><h2 id="terminologiya">  Terminologia </h2><br><p>  Para tornar o artigo completo e compreens√≠vel para todos, vejamos a terminologia b√°sica dos dois sistemas: </p><br><p>  Terminologia do Ceph: </p><br><p>  <strong>O RADOS</strong> (Reliable Autonomic Distributed Object Store) √© um armazenamento de objetos independente, que √© a base do projeto Ceph. <br>  <strong>CephFS</strong> , <strong>RBD</strong> (RADOS Block Device), <strong>RGW</strong> (RADOS Gateway) - gadgets de alto n√≠vel para o RADOS, que fornecem aos usu√°rios finais v√°rias interfaces para o RADOS. <br>  Especificamente, o CephFS fornece uma interface de sistema de arquivos compat√≠vel com POSIX.  De fato, os dados do CephFS s√£o armazenados no RADOS. <br>  <strong>OSD</strong> (Object Storage Daemon) √© um processo que serve um armazenamento de disco / objeto separado em um cluster RADOS. <br>  <strong>Pool RADOS</strong> (pool) - v√°rios <strong>OSDs</strong> unidos por um conjunto comum de regras, como, por exemplo, uma pol√≠tica de replica√ß√£o.  Do ponto de vista da hierarquia de dados, um pool √© um diret√≥rio ou um espa√ßo para nome separado (plano, sem subdiret√≥rios) para objetos. <br>  <strong>PG</strong> (Placement Group) - apresentarei o conceito de PG um pouco mais tarde, no contexto, para uma melhor compreens√£o. </p><br><p>  Como o RADOS √© a base sobre a qual o CephFS √© constru√≠do, geralmente falarei sobre isso e isso se aplicar√° automaticamente ao CephFS. </p><br><p>  Terminologia do GlusterFS (a seguir gl): </p><br><p>  <strong>Brick</strong> √© um processo que serve um √∫nico disco, um an√°logo do OSD na terminologia RADOS. <br>  <strong>Volume</strong> - volume no qual os tijolos est√£o unidos.  Tom √© um anal√≥gico de pool no RADOS, tamb√©m possui uma topologia de replica√ß√£o espec√≠fica entre tijolos. </p><br><h2 id="raspredelenie-dannyh">  Distribui√ß√£o de dados </h2><br><p>  Para tornar mais claro, considere um exemplo simples que pode ser implementado pelos dois sistemas. </p><br><p>  A configura√ß√£o a ser usada como exemplo: </p><br><ul><li>  2 servidores (S1, S2) com 3 discos de volume igual (sda, sdb, sdc) em cada um; </li><li>  volume / pool com replica√ß√£o 2. </li></ul><br><p>  Ambos os sistemas precisam de pelo menos 3 servidores para opera√ß√£o normal.  Mas fechamos os olhos para isso, pois esse √© apenas um exemplo para um artigo. </p><br><p>  No caso de gl, ser√° um volume <strong>Replicado Distribu√≠do que</strong> consiste em 3 grupos de replica√ß√£o: </p><br><p><img src="https://habrastorage.org/webt/ai/k_/pg/aik_pgd6mwqy1wlfyhjx-mdsf6u.png"></p><br><p>  Cada grupo de replica√ß√£o possui dois tijolos em servidores diferentes. <br>  De fato, verifica-se o volume que combina os tr√™s RAID-1. <br>  Ao mont√°-lo, obter o sistema de arquivos desejado e come√ßar a gravar arquivos nele, voc√™ descobrir√° que cada arquivo que voc√™ escreve cai em um desses grupos de replica√ß√£o como um todo. <br>  A distribui√ß√£o de arquivos entre esses grupos distribu√≠dos √© feita pelo <strong>DHT</strong> (Distributed Hash Tables), que √© essencialmente uma fun√ß√£o de hash (retornaremos a ele mais tarde). </p><br><p>  No "diagrama", ficar√° assim: </p><br><p><img src="https://habrastorage.org/webt/d1/-8/uk/d1-8ukcptl3owiyuqw11v0plxqw.png"></p><br><p>  Como se os primeiros recursos arquitet√¥nicos j√° estivessem manifestados: </p><br><ul><li>  o local em grupos √© descartado de maneira desigual, depende do tamanho do arquivo; </li><li>  ao escrever um arquivo, o IO vai para apenas um grupo, o restante fica ocioso; </li><li>  Voc√™ n√£o pode obter o pedido de entrada / sa√≠da do volume inteiro ao escrever um √∫nico arquivo; </li><li>  se n√£o houver espa√ßo suficiente no grupo para gravar o arquivo, voc√™ receber√° um erro, o arquivo n√£o ser√° gravado e ser√° redistribu√≠do para outro grupo. </li></ul><br><p>  Se voc√™ usar outros tipos de volumes, por exemplo, Replicado com distribui√ß√£o distribu√≠da ou at√© Disperso (codifica√ß√£o de elimina√ß√£o), somente a mec√¢nica da distribui√ß√£o de dados em um grupo ser√° fundamentalmente alterada.  O DHT tamb√©m decompor√° os arquivos inteiramente nesses grupos e, no final, teremos os mesmos problemas.  Sim, se o volume consistir em apenas um grupo ou se voc√™ tiver todos os arquivos aproximadamente do mesmo tamanho, n√£o haver√° problema.  Mas estamos falando de sistemas normais, com centenas de terabytes de dados, incluindo arquivos de tamanhos diferentes, por isso acreditamos que h√° um problema. </p><br><p>  Agora vamos dar uma olhada no CephFS.  Os RADOS mencionados acima entram em cena.  No RADOS, cada disco √© servido por um processo separado - OSD.  Com base em nossa configura√ß√£o, obtemos apenas 6 deles, 3 em cada servidor.  Em seguida, precisamos criar um pool para os dados e definir o n√∫mero de PGs e o fator de replica√ß√£o de dados nesse pool - no nosso caso 2. <br>  Digamos que criamos um pool com 8 PG.  Esses PGs ser√£o distribu√≠dos de maneira uniforme no OSD: </p><br><p><img src="https://habrastorage.org/webt/cn/ea/hs/cneahsczaws7dzuqtu1syubb4cy.png"></p><br><p>  √â hora de esclarecer que o PG √© um grupo l√≥gico que combina v√°rios objetos.  Como definimos o fato 2 da replica√ß√£o, cada PG possui uma r√©plica em outro OSD em outro servidor (por padr√£o).  Por exemplo, PG1, que est√° no OSD-1 no servidor S1, possui um g√™meo no S2 no OSD-6.  Em cada par de PG (ou triplo, se a replica√ß√£o 3) √© PG PRIM√ÅRIO, que est√° sendo gravado.  Por exemplo, PRIMARY para PG4 est√° em S1, mas PRIMARY para PG3 est√° em S2. </p><br><p>  Agora que voc√™ sabe como o RADOS funciona, podemos passar a gravar arquivos em nosso novo pool.  Embora o RADOS seja um armazenamento completo, n√£o √© poss√≠vel mont√°-lo como um sistema de arquivos ou us√°-lo como um dispositivo de bloco.  Para gravar dados diretamente, voc√™ precisa usar um utilit√°rio ou biblioteca especial. </p><br><p>  Escrevemos os mesmos tr√™s arquivos que no exemplo acima: </p><br><p><img src="https://habrastorage.org/webt/ut/0z/zd/ut0zzd20fmocwke70q9rj-0snog.png"></p><br><p>  No caso do RADOS, tudo se tornou um pouco mais complicado, concorda. </p><br><p>  Em seguida, CRUSH (Replica√ß√£o Controlada sob Hashing Escal√°vel) apareceu na cadeia.  CRUSH √© o algoritmo no qual o RADOS repousa (voltaremos a ele mais tarde).  Nesse caso em particular, usando esse algoritmo, √© determinado onde o arquivo deve ser gravado no qual PG.  Aqui CRUSH executa a mesma fun√ß√£o que DHT na gl.  Como resultado dessa distribui√ß√£o pseudo-aleat√≥ria de arquivos no PG, tivemos os mesmos problemas que o gl, apenas em um esquema mais complexo. </p><br><p>  Mas, deliberadamente, fiquei em sil√™ncio sobre um ponto importante.  Quase ningu√©m usa o RADOS em sua forma pura.  Para um trabalho conveniente com o RADOS, foram desenvolvidas as seguintes camadas: RBD, CephFS, RGW, que eu j√° mencionei. </p><br><p>  Todos esses tradutores (clientes RADOS) fornecem uma interface de cliente diferente, mas s√£o semelhantes em seu trabalho com o RADOS.  A semelhan√ßa mais importante √© que todos os dados que passam por eles s√£o cortados em peda√ßos e colocados no RADOS como objetos RADOS separados.  Por padr√£o, os clientes oficiais cortam o fluxo de entrada em peda√ßos de 4 MB.  Para RBD, o tamanho da faixa pode ser definido ao criar o volume.  No caso do CephFS, esse √© o atributo (xattr) do arquivo e pode ser gerenciado no n√≠vel de arquivos individuais ou para todos os arquivos de cat√°logo.  Bem, o RGW tamb√©m tem um par√¢metro correspondente. </p><br><p>  Agora, suponha que empilhamos o CephFS em cima do pool RADOS que foi apresentado no exemplo anterior.  Agora, os sistemas em quest√£o est√£o em p√© de igualdade e fornecem uma interface de acesso a arquivos id√™ntica. </p><br><p>  Se gravarmos nossos arquivos de teste no novo CephFS, encontraremos uma distribui√ß√£o de dados completamente diferente e quase uniforme no OSD.  Por exemplo, o arquivo2 de tamanho de 2 GB ser√° dividido em 512 partes, que ser√£o distribu√≠das por diferentes PGs e, como resultado, por diferentes OSDs quase uniformemente, e isso praticamente resolve os problemas com a distribui√ß√£o de dados descritos acima. </p><br><p>  No nosso exemplo, apenas 8 PG s√£o usados, embora seja recomend√°vel ter ~ 100 PG em um OSD.  E voc√™ precisa de dois pools para o CephFS funcionar. Voc√™ tamb√©m precisa de alguns daemons de servi√ßo para que o RADOS funcione em princ√≠pio.  N√£o pense que tudo √© t√£o simples, eu especificamente omito muito, para n√£o me afastar da ess√™ncia. </p><br><p>  Ent√£o agora o CephFS parece mais interessante, certo?  Mas n√£o mencionei outro ponto importante, desta vez sobre gl.  O Gl tamb√©m possui um mecanismo para cortar arquivos em peda√ßos e execut√°-los atrav√©s do DHT.  O chamado sharding ( <strong>sharding</strong> ). </p><br><p>  Hist√≥ria de cinco minutos </p><br><blockquote>  Em 21 de abril de 2016, a equipe de desenvolvimento do Ceph lan√ßou "Jewel", o primeiro lan√ßamento do Ceph no qual o CephFS √© considerado est√°vel. </blockquote><p>  Agora est√° tudo √† esquerda e √† direita gritando sobre o CephFS!  E h√° 3-4 anos atr√°s us√°-lo seria pelo menos uma decis√£o duvidosa.  Procuramos outras solu√ß√µes e gl com a arquitetura descrita acima n√£o era boa.  Mas acreditamos nisso mais do que no CephFS e esperamos o sharding, que estava se preparando para o lan√ßamento. </p><br><p>  E aqui est√° o dia X: </p><br><blockquote>  4 de junho de 2015 - A Comunidade Gluster anunciou hoje a disponibilidade geral do software de armazenamento definido por software aberto GlusterFS 3.7. </blockquote><p>  3.7 - a primeira vers√£o do gl, na qual o sharding foi anunciado como uma oportunidade experimental.  Eles tinham quase um ano antes do lan√ßamento est√°vel do CephFS, a fim de ganhar uma posi√ß√£o no p√≥dio ... </p><br><p>  Ent√£o sharding significa.  Como tudo no gl, isso √© implementado em um tradutor separado, que ficava acima do DHT (tamb√©m tradutor) na pilha.  Como √© maior que o DHT, o DHT recebe shards prontos na entrada e os distribui entre os grupos de replica√ß√£o como arquivos regulares.  O sharding √© ativado no n√≠vel do volume individual.  O tamanho do shard pode ser definido, por padr√£o - 4 MB, como lo√ß√µes Ceph. </p><br><p>  Quando realizei os primeiros testes, fiquei encantado!  Eu disse a todos que o gl agora √© a melhor coisa e agora vamos viver!  Com o sharding ativado, a grava√ß√£o de um arquivo √© paralela a diferentes grupos de replica√ß√£o.  A descompress√£o ap√≥s a compress√£o "On-Write" pode ser incremental ao n√≠vel do shard.  Na presen√ßa de grava√ß√£o em cache tamb√©m aqui, tudo se torna bom e os fragmentos individuais s√£o movidos para o cache, e n√£o para os arquivos inteiros.  Em geral, eu me alegrei porque  parecia que ele tinha conseguido um instrumento muito legal em suas m√£os. </p><br><p>  Resta aguardar as primeiras corre√ß√µes e o status de "pronto para produ√ß√£o".  Mas tudo acabou n√£o sendo t√£o otimista ... Para n√£o esticar o artigo com uma lista de bugs cr√≠ticos relacionados ao sharding, que surgem de vez em quando nas pr√≥ximas vers√µes, s√≥ posso dizer que o √∫ltimo "grande problema" com a seguinte descri√ß√£o: </p><br><blockquote>  A expans√£o de um volume de gluster fragmentado pode causar corrup√ß√£o de arquivo.  Os volumes fragmentados s√£o normalmente usados ‚Äã‚Äãpara imagens de VM; se esses volumes forem expandidos ou possivelmente contratados (por exemplo, adicionar / remover tijolos e reequilibrar), h√° relat√≥rios de imagens de VM corrompidas. </blockquote><p>  foi fechado no release 3.13.2, 20 de janeiro de 2018 ... talvez este n√£o seja o √∫ltimo? </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Coment√°rios</a> sobre um de nossos artigos sobre isso, por assim dizer, em primeira m√£o. </p><br><p>  O RedHat em sua documenta√ß√£o para o RedHat Gluster Storage 3.4 atual observa que o √∫nico caso de sharding que eles suportam √© o armazenamento de discos de VM. </p><br><blockquote>  O Sharding possui um caso de uso suportado: no contexto de fornecer o Red Hat Gluster Storage como um dom√≠nio de armazenamento para o Red Hat Enterprise Virtualization, para fornecer armazenamento para imagens ativas de m√°quinas virtuais.  Observe que o sharding tamb√©m √© um requisito para este caso de uso, pois fornece melhorias significativas de desempenho em rela√ß√£o √†s implementa√ß√µes anteriores. </blockquote><p>  N√£o sei por que essa restri√ß√£o, mas voc√™ deve admitir, √© alarmante. </p><br><h2 id="seychas-ya-tebe-tut-vse-zaheshiruyu">  Agora eu tenho tudo aqui para voc√™ </h2><br><p>  Ambos os sistemas usam uma fun√ß√£o hash para distribuir dados pseudo-aleatoriamente entre discos. </p><br><p>  Para o RADOS, √© algo parecido com isto: </p><br><pre><code class="plaintext hljs">PG = pool_id + "." + jenkins_hash(object_name) % pg_coun # eg pool id=5 =&gt; pg = 5.1f OSD = crush_hash_based_on_jenkins(PG) # eg pg=5.1f =&gt; OSD = 12</code> </pre> <br><p>  Gl usa o chamado <strong>hash consistente</strong> .  Cada tijolo recebe um "intervalo dentro de um espa√ßo de hash de 32 bits".  Ou seja, todos os tijolos compartilham todo o espa√ßo de hash do endere√ßo linear sem intervalos ou furos cruzados.  O cliente executa o nome do arquivo atrav√©s da fun√ß√£o hash e determina em qual intervalo de hash o hash recebido se enquadra.  Assim, o tijolo √© selecionado.  Se houver v√°rios tijolos no grupo de replica√ß√£o, todos eles ter√£o o mesmo intervalo de hash.  Algo assim: </p><br><p><img src="https://habrastorage.org/webt/o_/y5/ye/o_y5yeby9vn5enx7r-5xa3zfwuq.png"></p><br><p>  Se levarmos o trabalho de dois sistemas para uma determinada forma l√≥gica, resultar√° algo assim: </p><br><pre> <code class="plaintext hljs">file -&gt; HASH -&gt; placement_unit</code> </pre> <br><p>  onde a position_unit, no caso do RADOS, √© PG, e no caso de gl, √© um grupo de replica√ß√£o de v√°rios tijolos. </p><br><p>  Portanto, uma fun√ß√£o hash, ent√£o esta distribui, distribui arquivos e, de repente, verifica-se que uma unidade de posicionamento √© utilizada mais que a outra.  Essa √© a caracter√≠stica fundamental dos sistemas de distribui√ß√£o de hash.  E enfrentamos uma tarefa muito comum - desequilibrar os dados. </p><br><p>  O Gl √© capaz de reconstruir, mas devido √† arquitetura com intervalos de hash descritos acima, voc√™ pode executar a reconstru√ß√£o o quanto quiser, mas nenhum intervalo de hash (e, como resultado, dados) n√£o ser√° alterado.  O √∫nico crit√©rio para redistribuir intervalos de hash √© uma altera√ß√£o na capacidade do volume.  E voc√™ tem uma op√ß√£o restante - para adicionar tijolos.  E se estamos falando de um volume com replica√ß√£o, devemos adicionar um grupo inteiro de replica√ß√£o, ou seja, dois novos tijolos em nossa configura√ß√£o.  Ap√≥s expandir o volume, voc√™ pode come√ßar a reconstruir - os intervalos de hash ser√£o redistribu√≠dos levando em considera√ß√£o o novo grupo e os dados ser√£o distribu√≠dos.  Quando um grupo de replica√ß√£o √© exclu√≠do, os intervalos de hash s√£o alocados automaticamente. </p><br><p>  RADOS tem um carro inteiro de possibilidades.  Em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo da</a> Ceph, reclamei muito sobre o conceito de PG, mas aqui, comparando com gl, √© claro, RADOS a cavalo.  Cada OSD tem seu pr√≥prio peso, geralmente √© definido com base no tamanho do disco.  Por sua vez, os PGs s√£o distribu√≠dos por OSD, dependendo do peso do √∫ltimo.  Tudo, ent√£o apenas alteramos o peso do OSD para cima ou para baixo e o PG (junto com os dados) come√ßa a se mover para outros OSDs.  Al√©m disso, cada OSD possui um peso de ajuste adicional, que permite equilibrar os dados entre os discos de um servidor.  Tudo isso √© inerente ao CRUSH.  O principal lucro √© que n√£o √© necess√°rio expandir a capacidade do pool para desequilibrar melhor os dados.  E n√£o √© necess√°rio adicionar discos em grupos, voc√™ pode adicionar apenas um OSD e uma parte do PG ser√° transferida para ele. </p><br><p>  Sim, √© poss√≠vel que, ao criar um pool, eles n√£o tenham criado PG suficiente e tenha acontecido que cada um dos PGs seja bastante grande em volume e, onde quer que eles se movam, o desequil√≠brio permanecer√°.  Nesse caso, voc√™ pode aumentar o n√∫mero de PG, e eles s√£o divididos em menores.  Sim, se o cluster estiver cheio de dados, d√≥i, mas o principal em nossa compara√ß√£o √© que existe essa oportunidade.  Agora, apenas um aumento no n√∫mero de PGs √© permitido e, com isso, voc√™ precisa ter mais cuidado, mas na pr√≥xima vers√£o do Ceph - Nautilus, haver√° suporte para reduzir o n√∫mero de PGs (p√°g. Mesclada). </p><br><h2 id="replikaciya-dannyh">  Replica√ß√£o de dados </h2><br><p>  Nossos conjuntos de testes e volumes t√™m um fator de replica√ß√£o 2. Curiosamente, os sistemas em quest√£o usam abordagens diferentes para atingir esse n√∫mero de r√©plicas. </p><br><p>  No caso do RADOS, o esquema de grava√ß√£o √© mais ou menos assim: </p><br><p><img src="https://habrastorage.org/webt/lx/vb/q-/lxvbq-niuingzw76ad7aqanp2pg.png"></p><br><p>  O cliente conhece a topologia de todo o cluster, usa CRUSH (etapa 0) para selecionar um PG espec√≠fico para grava√ß√£o, grava no PRIMARY PG no OSD-0 (etapa 1) e, em seguida, o OSD-0 replica os dados de forma s√≠ncrona no SECONDARY PG (etapa 2) e somente depois etapa 2 com ou sem √™xito, o OSD confirma / n√£o confirma a opera√ß√£o para o cliente (etapa 3).  A replica√ß√£o de dados entre dois OSDs √© transparente para o cliente.  OSDs geralmente podem usar um ‚Äúcluster‚Äù separado, uma rede mais r√°pida para replica√ß√£o de dados. </p><br><p>  Se a replica√ß√£o tripla estiver configurada, ela tamb√©m ser√° executada de forma s√≠ncrona com o OSD PRIM√ÅRIO em dois SECUND√ÅRIOS, transparentes para o cliente ... bem, apenas essa permiss√£o √© maior. </p><br><p>  Gl funciona de maneira diferente: </p><br><p><img src="https://habrastorage.org/webt/ll/zz/q-/llzzq-m2jhfaf83-dtk_fokavw0.png"></p><br><p>  O cliente conhece a topologia do volume, usa DHT (etapa 0) para determinar o bloco desejado e, em seguida, grava nele (etapa 1).  Tudo √© simples e claro.  Mas aqui lembramos que todos os tijolos no grupo de replica√ß√£o t√™m o mesmo intervalo de hash.  E esse recurso menor torna o feriado inteiro.  O cliente grava em paralelo todos os tijolos que possuem um intervalo de hash adequado. </p><br><p>  No nosso caso, com duplica√ß√£o dupla, o cliente executa a grava√ß√£o dupla em paralelo em dois tijolos diferentes.  Durante a replica√ß√£o tripla, a grava√ß√£o tripla ser√° realizada, respectivamente, e 1 MB de dados se transformar√° aproximadamente em 3 MB de tr√°fego de rede do cliente para o lado dos gl-servers.  Concordo, os conceitos de sistemas s√£o perpendiculares. </p><br><p>  Nesse esquema, mais trabalho √© atribu√≠do ao cliente gl e, como resultado, ele precisa de mais CPU, bem, eu j√° disse sobre a rede. </p><br><p>  A replica√ß√£o √© feita pelo tradutor AFP (Replica√ß√£o Autom√°tica de Arquivos) - um xlator do lado do cliente que executa a replica√ß√£o s√≠ncrona.  Duplica as grava√ß√µes em todos os tijolos da r√©plica ‚Üí Usa um modelo de transa√ß√£o. </p><br><p>  Se necess√°rio, sincronize as r√©plicas no grupo (cura), por exemplo, ap√≥s uma indisponibilidade tempor√°ria de um tijolo, os gl daemons fazem isso sozinhos usando o AFP interno, transparente para os clientes e sem a participa√ß√£o deles. </p><br><p>  √â interessante que, se voc√™ n√£o trabalha com o gl client nativo, mas escreve com o servidor NFS embutido no gl, obteremos o mesmo comportamento do RADOS.  Nesse caso, o AFP ser√° usado nos gl daemons para replicar dados sem a interven√ß√£o do cliente.  Mas o NFS interno √© protegido na gl v4 e, se voc√™ deseja esse comportamento, √© recomend√°vel usar o NFS-Ganesha. </p><br><p>  A prop√≥sito, devido a um comportamento t√£o diferente ao usar o NFS e o cliente nativo, voc√™ pode ver indicadores de desempenho completamente diferentes. </p><br><h2 id="a-u-vas-est-takoy-zhe-klaster-tolko-na-kolenke">  Voc√™ tem o mesmo cluster, apenas "no joelho"? </h2><br><p>  Muitas vezes vejo na Internet discuss√µes de todos os tipos de configura√ß√µes de r√≥tulos, onde um cluster de dados √© constru√≠do a partir do que est√° dispon√≠vel.  Nesse caso, uma solu√ß√£o baseada no RADOS pode oferecer mais liberdade ao escolher suas unidades.  No RADOS, voc√™ pode adicionar unidades de praticamente qualquer tamanho.  Cada disco ter√° um peso correspondente ao seu tamanho (geralmente) e os dados ser√£o distribu√≠dos pelos discos quase proporcionalmente ao seu peso.  No caso de gl, n√£o h√° conceito de "discos separados" em volumes com replica√ß√£o.  Os discos s√£o adicionados em pares na replica√ß√£o dupla ou tripla na tripla.  Se houver discos de tamanhos diferentes em um grupo de replica√ß√£o, voc√™ encontrar√° um local no menor disco do grupo e remover√° a capacidade de discos grandes.  Nesse esquema, gl assumir√° que a capacidade de um grupo de replica√ß√£o √© igual √† capacidade do menor disco do grupo, o que √© l√≥gico.  Ao mesmo tempo, √© permitido ter grupos de replica√ß√£o consistindo em discos de tamanhos diferentes - grupos de tamanhos diferentes.  Grupos maiores podem receber um intervalo de hash maior em rela√ß√£o a outros grupos e, como resultado, receber mais dados. </p><br><p>  Vivemos com o Ceph pelo quinto ano.  Come√ßamos com discos do mesmo volume, agora apresentamos outros mais espa√ßosos.  Com o Ceph, voc√™ pode remover o disco e substitu√≠-lo por outro maior ou um pouco menor, sem dificuldades arquitet√¥nicas.  Com gl, tudo fica mais complicado - tirou um disco de 2 TB - coloque o mesmo, por favor.  Bem, ou retire todo o grupo como um todo, o que n√£o √© muito bom, concorda. </p><br><h2 id="obrabotka-otkazov">  Failover </h2><br><p>  J√° nos familiarizamos um pouco com a arquitetura das duas solu√ß√µes e agora podemos falar sobre como viver com ela e quais s√£o os recursos ao fazer a manuten√ß√£o. </p><br><p>  Suponha que recusamos o sda no s1 - uma coisa comum. </p><br><p>  No caso de gl: </p><br><ul><li>  uma c√≥pia dos dados no disco ativo restante no grupo n√£o √© redistribu√≠da automaticamente para outros grupos; </li><li>  at√© que o disco seja substitu√≠do, apenas uma c√≥pia dos dados permanece; </li><li>  ao substituir um disco com falha por um novo, a replica√ß√£o √© executada de um disco de trabalho para um novo (1 em 1). </li></ul><br><p>  √â como servir uma prateleira com v√°rios RAID-1s.  Sim, com replica√ß√£o tripla, se uma unidade falhar, n√£o resta uma c√≥pia, mas duas, mas ainda assim essa abordagem apresenta s√©rias desvantagens, e eu as mostrarei com um bom exemplo com o RADOS. </p><br><p>  Suponha que falhemos sda no S1 (OSD-0) - uma coisa comum: </p><br><ul><li>  Os PGs que estavam no OSD-0 ser√£o remapeados automaticamente para outros OSDs ap√≥s 10 minutos (padr√£o).  No nosso exemplo, no OSD 1 e 2. Se houver mais servidores, em um n√∫mero maior de OSD. </li><li>  Os PGs que armazenam a segunda c√≥pia sobrevivente dos dados os replicam automaticamente para os OSDs para os quais os PGs restaurados s√£o transferidos.  Acontece replica√ß√£o muitos-para-muitos, n√£o replica√ß√£o um-para-um como gl. </li><li>  Quando um novo disco √© introduzido, em vez de um disco quebrado, alguns PGs ser√£o acumulados de acordo com seu peso no novo OSD e os dados de outros OSDs ser√£o redistribu√≠dos. </li></ul><br><p>  Eu acho que n√£o faz sentido explicar as vantagens arquitet√¥nicas do RADOS.  Voc√™ n√£o pode se contorcer ao receber uma carta dizendo que a unidade falhou.  E quando voc√™ chegar ao trabalho de manh√£, descubra que todas as c√≥pias ausentes j√° foram restauradas para dezenas de outros OSDs ou no processo.  Em clusters grandes, onde centenas de PGs est√£o espalhados por v√°rios discos, a recupera√ß√£o de dados de um OSD pode ocorrer em velocidades muito superiores √† velocidade de um disco, devido ao fato de que dezenas de OSDs est√£o envolvidos (leitura e grava√ß√£o).  Bem, voc√™ tamb√©m n√£o deve esquecer o balanceamento de carga. </p><br><h2 id="masshtabirovanie">  Dimensionamento </h2><br><p>  Nesse contexto, provavelmente darei o pedestal gl.  Em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> no Ceph, eu j√° escrevi sobre algumas das complexidades do dimensionamento do RADOS associadas ao conceito de PG.  Se o aumento no PG com o crescimento do cluster ainda puder ser observado, o Ceph MDS n√£o est√° claro.  O CephFS √© executado sobre o RADOS e usa um pool separado para metadados e um processo especial, o servidor de metadados seph (MDS), para atender os metadados do sistema de arquivos e coordenar todas as opera√ß√µes com o FS.  N√£o estou dizendo que o MDS p√µe fim √† escalabilidade do CephFS, n√£o, especialmente porque voc√™ pode executar v√°rios MDS no modo ativo-ativo.  Eu s√≥ quero observar que gl √© arquitetonicamente desprovido de tudo isso.  N√£o possui contrapartida do PG, nada como o MDS.  O Gl realmente escala perfeitamente, simplesmente adicionando grupos de replica√ß√£o, quase linearmente. </p><br><p>  Nos dias anteriores ao CephFS, projetamos a solu√ß√£o para petabytes de dados e analisamos o gl.  Ent√£o tivemos d√∫vidas sobre a escalabilidade do gl e descobrimos atrav√©s da lista de discuss√£o.  Aqui est√° uma das respostas (Q: minha pergunta): </p><br><blockquote>  Estou usando 60 servidores, cada um com discos de 26x8TB, volume total de 1560 discos 16 + 4 EC e 9PB de espa√ßo √∫til. <br><br>  P: Voc√™ usa libgfapi, FUSE ou NFS no lado do cliente? <br><br>  Eu uso o FUSE e tenho quase 1000 clientes. <br><br>  P: Quantos arquivos voc√™ tem no seu volume? <br>  P: Os arquivos s√£o maiores ou menores? <br><br>  Eu tenho mais de 1 milh√£o de arquivos e% 13 do cluster √© usado, o que torna o tamanho m√©dio de 1 GB. <br>  O tamanho m√≠nimo / m√°ximo do arquivo √© 100 MB / 2 GB.  Todos os dias, 10-20 TB de novos dados entram no volume. <br><br>  P: Qual a rapidez com que "ls" funciona)? <br><br>  As opera√ß√µes de metadados s√£o lentas conforme o esperado.  Eu tento n√£o colocar mais de 2-3K arquivos em um diret√≥rio.  Meu caso de uso √© para backup / archive, por isso raramente fa√ßo opera√ß√µes de metadados. </blockquote><br><h2 id="pereimenovanie-faylov">  Renomear arquivos </h2><br><p>  Voltar para as fun√ß√µes de hash novamente.  Descobrimos como arquivos espec√≠ficos s√£o roteados para discos espec√≠ficos e agora a pergunta se torna relevante, mas o que acontecer√° ao renomear arquivos? </p><br><p>  Afinal, se alterarmos o nome do arquivo, o hash em seu nome tamb√©m mudar√°, o que significa o local desse arquivo em outro disco (em um intervalo de hash diferente) ou em outro PG / OSD no caso do RADOS.  Sim, pensamos corretamente, e aqui em dois sistemas tudo √© novamente perpendicular. </p><br><p>  No caso de gl, ao renomear um arquivo, o novo nome √© executado por meio de uma fun√ß√£o hash, um novo bloco √© definido e um link especial √© criado nele para o bloco antigo, onde o arquivo permanece como antes.  Topovka, certo?  Para que os dados realmente sejam movidos para um novo local e o cliente n√£o clicou no link desnecessariamente, √© necess√°rio fazer uma rebeli√£o. </p><br><p>  Mas o RADOS geralmente n√£o possui um m√©todo para renomear objetos apenas devido √† necessidade de seu movimento subsequente.  √â proposto o uso de c√≥pias justas para renomear, o que leva ao movimento s√≠ncrono do objeto.  E o CephFS, que roda sobre o RADOS, tem um trunfo na manga na forma de um pool com metadados e MDS.  Alterar o nome do arquivo n√£o afeta o conte√∫do do arquivo no conjunto de dados. </p><br><h2 id="replikaciya-25">  Replica√ß√£o 2.5 </h2><br><p>  Gl tem um recurso muito interessante que eu gostaria de mencionar separadamente.  Todo mundo entende que a replica√ß√£o 2 n√£o √© uma configura√ß√£o confi√°vel, mas, no entanto, periodicamente √© justificada.  Para se proteger contra o c√©rebro dividido em tais esquemas e garantir a consist√™ncia dos dados, o gl permite criar volumes com a r√©plica 2 e um √°rbitro adicional.  O √°rbitro √© aplic√°vel √† replica√ß√£o de 3 ou mais.  Esse √© o mesmo bloco no grupo que os outros dois, mas na verdade ele cria apenas uma estrutura de arquivos a partir de arquivos e diret√≥rios.  Os arquivos desse bloco s√£o de tamanho zero, mas seus atributos estendidos do sistema de arquivos (atributos estendidos) s√£o mantidos no estado sincronizado com arquivos de tamanho completo na mesma r√©plica.  Eu acho que a ideia √© clara.  Eu acho que essa √© uma oportunidade legal. </p><br><p>  O √∫nico momento ... o tamanho do local no grupo de replica√ß√£o √© determinado pelo tamanho do menor tijolo, e isso significa que o √°rbitro precisa deslizar um disco pelo menos do mesmo tamanho que o resto do grupo.  Para fazer isso, √© recomend√°vel criar tamanhos grandes e fict√≠cios de LV fino (fino), para n√£o usar um disco real. </p><br><h2 id="a-che-po-klientam">  E os clientes? </h2><br><p>  A API nativa dos dois sistemas √© implementada na forma das bibliotecas libgfapi (gl) e libcephfs (CephFS).  Liga√ß√µes para idiomas populares tamb√©m est√£o dispon√≠veis.  Em geral, com bibliotecas, tudo √© igualmente bom.  O onipresente NFS-Ganesha suporta as duas bibliotecas como o FSAL, que tamb√©m √© a norma.  O Qemu tamb√©m suporta a API gl nativa via libgfapi. </p><br><p>  Mas o fio (Flexible I / O Tester) tem suportado o libgfapi por muito tempo e com √™xito, mas n√£o suporta o libcephfs.  Este √© um plus gl, porque  usando fio √© muito bom testar gl diretamente.  Somente trabalhando no espa√ßo do usu√°rio atrav√©s da libgfapi voc√™ obter√° tudo o que pode da gl. </p><br><p>  Mas se estamos falando sobre o sistema de arquivos POSIX e como mont√°-lo, o gl pode oferecer apenas o cliente FUSE e a implementa√ß√£o do CephFS no kernel upstream.  √â claro que no m√≥dulo do kernel voc√™ pode fazer um truque que o FUSE mostrar√° melhor desempenho.  Mas, na pr√°tica, o FUSE √© sempre uma sobrecarga na altern√¢ncia de contexto.  Eu pessoalmente vi mais de uma vez como o FUSE dobrou um servidor de soquete duplo com CSs sozinho. <br>  De alguma forma, Linus disse: </p><br><blockquote>  Sistema de arquivos do espa√ßo do usu√°rio?  O problema est√° a√≠.  Sempre foi.  As pessoas que pensam que os sistemas de arquivos do espa√ßo do usu√°rio s√£o realistas para qualquer coisa, menos os brinquedos, s√£o apenas equivocados. </blockquote><p>  Os desenvolvedores da Gl, pelo contr√°rio, pensam que o FUSE √© legal.  Isso √© dito para dar mais flexibilidade e desanexar das vers√µes do kernel.  Quanto a mim, eles usam o FUSE, porque gl n√£o √© velocidade.  De alguma forma, est√° escrito - bem, √© normal, e se preocupar com a implementa√ß√£o no kernel √© realmente estranho. </p><br><h2 id="proizvoditelnost">  Desempenho </h2><br><p>  N√£o haver√° compara√ß√µes). </p><br><p>  Isso √© muito complicado.  Mesmo em uma configura√ß√£o id√™ntica, √© muito dif√≠cil realizar testes objetivos.  De qualquer forma, haver√° algu√©m nos coment√°rios que fornecer√° 100500 par√¢metros que "aceleram" um dos sistemas e dizem que os testes s√£o besteiras.  Portanto, se estiver interessado, teste-se, por favor. </p><br><h2 id="zaklyuchenie">  Conclus√£o </h2><br><p>  RADOS e CephFS, em particular, s√£o uma solu√ß√£o mais complexa, tanto no entendimento, na configura√ß√£o e na manuten√ß√£o. </p><br><p>  Mas, pessoalmente, eu gosto mais da arquitetura do RADOS e da execu√ß√£o no CephFS do que no GlusterFS.  Mais identificadores (PG, peso OSD, hierarquia CRUSH etc.), os metadados do CephFS aumentam a complexidade, mas oferecem mais flexibilidade e tornam essa solu√ß√£o mais eficaz, na minha opini√£o. </p><br><p>  O Ceph √© muito mais adequado aos crit√©rios atuais de SDS e me parece mais promissor.  Mas esta √© a minha opini√£o, o que voc√™ acha? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt430474/">https://habr.com/ru/post/pt430474/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt430462/index.html">Na R√∫ssia, apareceu uma lei sobre o fornecimento de dados de usu√°rios de redes sociais para um c√≠rculo ilimitado de pessoas. Redes sociais contra</a></li>
<li><a href="../pt430466/index.html">Mini AI Cup # 3: Escrevendo um Top Bot</a></li>
<li><a href="../pt430468/index.html">Sensibilizar os cidad√£os</a></li>
<li><a href="../pt430470/index.html">Por que manter o contexto na conta do cliente - de maneira honesta e lucrativa</a></li>
<li><a href="../pt430472/index.html">Rede DECT sem costura DIY</a></li>
<li><a href="../pt430476/index.html">NCBI Genome Workbench: Pesquisa em Perigo</a></li>
<li><a href="../pt430478/index.html">Bots de negocia√ß√£o para o mercado de criptomoedas. Por onde come√ßar?</a></li>
<li><a href="../pt430480/index.html">Como escrevemos o aplicativo no hackathon da NASA Space Apps Challenge</a></li>
<li><a href="../pt430482/index.html">O tema das placas de armadura na cultura do Oriente e do Ocidente</a></li>
<li><a href="../pt430484/index.html">Cen√°rios t√≠picos de implementa√ß√£o do NGFW</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>