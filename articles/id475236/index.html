<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎮 🔻 🍥 Jangan pernah mengabaikan pelatihan penguatan lagi. 🧓🏽 🤟🏾 📠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya mempersembahkan untuk Anda terjemahan artikel “Jangan Pernah Abaikan Belajar Penguatan Lagi” oleh Michel Kana, Ph.D. 

 Belajar denga...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jangan pernah mengabaikan pelatihan penguatan lagi.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Halo, Habr!  Saya mempersembahkan untuk Anda terjemahan artikel “Jangan Pernah Abaikan Belajar Penguatan Lagi” oleh Michel Kana, Ph.D. <br><br>  Belajar dengan guru dan belajar tanpa guru tidak semuanya.  Semua orang tahu itu.  Mulailah dengan OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="gambar"></div><br>  <i>Apakah Anda akan mengalahkan juara catur dunia, backgammon atau pergi?</i> <br><br>  Ada cara yang akan memungkinkan Anda melakukan ini - pelatihan penguatan. <br><a name="habracut"></a><br><h2>  Apa itu pembelajaran penguatan? </h2><br>  Pembelajaran yang diperkuat adalah pembelajaran untuk membuat keputusan yang konsisten di lingkungan dengan hadiah maksimum yang diterima yang diberikan untuk setiap tindakan. <br><br>  Tidak ada guru di dalam dirinya, hanya sinyal hadiah dari lingkungan.  Waktu penting, dan tindakan memengaruhi data selanjutnya.  Kondisi seperti itu menciptakan kesulitan untuk belajar dengan atau tanpa guru. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="gambar"></div><br>  Pada contoh di bawah ini, mouse mencoba mencari makanan sebanyak mungkin dan menghindari kejutan listrik jika memungkinkan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="gambar"></div><br>  Seekor tikus bisa berani dan bisa keluar untuk sampai ke tempat dengan banyak keju.  Ini akan lebih baik daripada hanya diam dan tidak menerima apa pun. <br><br>  Mouse tidak ingin membuat keputusan terbaik dalam setiap situasi tertentu.  Ini akan membutuhkan pengeluaran mental yang besar darinya, dan itu tidak akan bersifat universal. <br><br>  Pembelajaran bertulang memberikan beberapa set metode ajaib yang memungkinkan mouse kita belajar cara menghindari kejut listrik dan mendapatkan makanan sebanyak mungkin. <br><br>  Mouse adalah agen.  Labirin dengan dinding, keju, dan pistol bius adalah <b>lingkungan</b> .  Mouse dapat bergerak ke kiri, kanan, atas, bawah - ini adalah <b>tindakan</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="gambar"></div><br>  Mouse menginginkan keju, bukan kejutan listrik.  Keju adalah <b>hadiah</b> .  Mouse dapat memeriksa lingkungan - ini adalah <b>pengamatan</b> . <br><br><h2>  Pelatihan penguatan es </h2><br>  Mari kita tinggalkan mouse di labirin dan beralih ke es.  “Musim dingin telah tiba.  Anda dan teman-teman Anda melemparkan frisbee di taman ketika Anda tiba-tiba melemparkan frisbee di tengah danau.  Pada dasarnya, air di danau itu membeku, tetapi ada beberapa lubang di mana es mencair.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sumber</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="gambar"></div><br>  “Jika kamu menginjak salah satu lubang, kamu akan jatuh ke dalam air es.  Selain itu, ada kekurangan besar frisbee di dunia, jadi sangat penting bahwa Anda pergi berkeliling danau dan menemukan drive. "( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sumber</a> ) <br><br><h2>  Bagaimana perasaan Anda dalam situasi yang sama? </h2><br>  Ini adalah tantangan untuk pembelajaran penguatan.  Agen mengontrol pergerakan karakter di dunia kisi.  Beberapa ubin kotak lumayan, sementara yang lain menyebabkan karakter jatuh ke dalam air.  Agen menerima hadiah karena menemukan jalur yang dapat dilalui untuk mencapai tujuan. <br><br>  Kita dapat mensimulasikan lingkungan seperti itu menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenAI Gym</a> - toolkit untuk mengembangkan dan membandingkan algoritma pembelajaran dengan bala bantuan.  Ini memberikan akses ke seperangkat lingkungan standar, seperti dalam contoh kita, yang disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Danau Beku</a> .  Ini adalah media teks yang dapat dibuat dengan beberapa baris kode. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Sekarang kita membutuhkan struktur yang akan memungkinkan kita untuk secara sistematis mendekati masalah pembelajaran dengan penguatan. <br><br><h2>  Proses pengambilan keputusan Markov </h2><br>  Dalam contoh kami, agen mengontrol gerakan karakter di dunia kisi, dan lingkungan ini disebut lingkungan yang dapat diamati sepenuhnya. <br><br>  Karena ubin masa depan tidak bergantung pada ubin masa lalu dengan mempertimbangkan ubin saat ini <br>  (kita berhadapan dengan urutan keadaan acak, yaitu, dengan <b>properti Markov</b> ), oleh karena itu kita berhadapan dengan apa yang disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">proses Markov</a> . <br><br>  Keadaan saat ini merangkum semua yang diperlukan untuk memutuskan apa langkah selanjutnya, tidak ada yang perlu diingat. <br><br>  Pada setiap sel berikutnya (yaitu, situasi), agen memilih dengan beberapa kemungkinan tindakan yang mengarah ke sel berikutnya, yaitu, situasi, dan lingkungan merespons agen dengan pengamatan dan penghargaan. <br><br>  Kami menambahkan fungsi hadiah dan koefisien diskon ke proses Markov, dan kami mendapatkan apa yang disebut <i>proses hadiah Markov</i> .  Menambahkan serangkaian tindakan, kami mendapatkan <i>proses pengambilan keputusan Markov</i> ( <b>MDP</b> ).  Komponen MDP dijelaskan secara lebih rinci di bawah ini. <br><br><h2>  Ketentuan </h2><br>  Keadaan adalah bagian dari lingkungan, representasi numerik dari apa yang diamati agen pada titik waktu tertentu dalam lingkungan, keadaan kisi-kisi danau.  S adalah titik awal, G adalah target, F adalah es padat tempat agen dapat berdiri, dan H adalah lubang di mana agen akan jatuh jika ia menginjaknya.  Kami memiliki 16 negara bagian dalam lingkungan grid 4 x 4, atau 64 negara dalam lingkungan 8 x 8. Di bawah ini kami akan mengambil contoh lingkungan 4 x 4 menggunakan OpenAI Gym. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Tindakan </h2><br>  Agen memiliki 4 tindakan yang mungkin, yang direpresentasikan dalam lingkungan sebagai 0, 1, 2, 3 masing-masing untuk kiri, kanan, bawah, atas. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="gambar"></div><br><h2>  Model transisi negara </h2><br>  Model transisi keadaan menjelaskan bagaimana keadaan lingkungan berubah ketika agen mengambil tindakan berdasarkan kondisi saat ini. <br><br>  Model ini biasanya digambarkan oleh probabilitas transisi, yang dinyatakan sebagai matriks transisi kuadrat dari ukuran N x N, di mana N adalah jumlah keadaan model kami.  Ilustrasi di bawah ini adalah contoh dari matriks untuk kondisi cuaca. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="gambar"></div><br>  Di lingkungan Danau Beku, kita menganggap bahwa danau itu tidak licin.  Jika kita ke kanan, maka kita pasti ke kanan.  Oleh karena itu, semua probabilitas adalah sama. <br><br>  "Kiri" memindahkan agen 1 sel ke kiri atau meninggalkannya di posisi yang sama jika agen berada di perbatasan kiri. <br><br>  "Kanan" memindahkan 1 sel ke kanan atau membiarkannya dalam posisi yang sama jika agen berada di perbatasan kanan. <br><br>  "Atas" menggerakkan agen 1 sel ke atas, atau agen tetap di tempat yang sama jika berada di batas atas. <br><br>  "Turun" memindahkan agen 1 sel ke bawah, atau tetap di tempat yang sama jika berada di batas bawah. <br><br><h2>  Remunerasi </h2><br>  Di setiap negara bagian F, agen menerima 0 hadiah, di negara bagian H, ia menerima -1, karena, setelah melewati kondisi ini, agen tersebut mati.  Dan ketika agen mencapai tujuan, dia menerima hadiah +1. <br><br>  Karena kenyataan bahwa kedua model, model transisi dan model hadiah, adalah fungsi deterministik, ini membuat lingkungan menjadi deterministik.  \ <br><br><h2>  Diskon </h2><br>  Diskon adalah parameter opsional yang mengontrol pentingnya hadiah di masa depan.  Itu diukur dalam kisaran dari 0 hingga 1. Tujuan dari parameter ini adalah untuk mencegah total hadiah dari pergi hingga tak terbatas. <br><br>  Diskon juga memodelkan perilaku agen ketika agen lebih suka hadiah langsung ke hadiah yang mungkin diterima di masa depan. <br><br><h2>  Nilai </h2><br>  Nilai kekayaan adalah pendapatan jangka panjang yang diharapkan dengan diskon untuk kekayaan tersebut. <br><br><h2>  Kebijakan (π) </h2><br>  Strategi yang digunakan agen untuk memilih tindakan selanjutnya disebut kebijakan.  Di antara semua kebijakan yang tersedia, yang optimal adalah yang memaksimalkan jumlah remunerasi yang diterima atau diharapkan selama episode. <br><br><h2>  Episode </h2><br>  Episode dimulai ketika agen muncul di sel awal, dan berakhir ketika agen entah jatuh ke dalam lubang atau mencapai sel target. <br><br><h2>  Mari kita visualisasikan semuanya </h2><br>  Setelah meninjau semua konsep yang terlibat dalam proses pengambilan keputusan Markov, kita sekarang dapat memodelkan beberapa tindakan acak dalam lingkungan 16x16 menggunakan OpenAI Gym.  Setiap kali, agen memilih tindakan acak dan melakukannya.  Sistem menghitung imbalan dan menampilkan keadaan lingkungan yang baru. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="gambar"></div><br><h2>  Kesimpulan </h2><br>  Dalam artikel ini, kami meninjau secara singkat konsep dasar pembelajaran penguatan.  Contoh kami memberikan pengantar pada toolkit OpenAI Gym, yang membuatnya mudah untuk bereksperimen dengan lingkungan yang dibangun sebelumnya. <br><br>  Pada bagian selanjutnya, kami akan mempresentasikan bagaimana merancang dan mengimplementasikan kebijakan yang akan memungkinkan agen untuk mengambil serangkaian tindakan untuk mencapai tujuan dan menerima penghargaan seperti mengalahkan juara dunia. <br><br>  Terima kasih atas perhatian anda </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id475236/">https://habr.com/ru/post/id475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id475208/index.html">Pindah ke Eropa: Petualangan dan Temuan</a></li>
<li><a href="../id475212/index.html">Cari data dan objek di database MS SQL Server menggunakan utilitas Pencarian dbForge gratis</a></li>
<li><a href="../id475214/index.html">Ketika sebuah perusahaan meninggal: bagaimana cara bertahan hidup dari kebangkrutan</a></li>
<li><a href="../id475218/index.html">Protokol kriptografi: definisi, catatan, properti, klasifikasi, serangan</a></li>
<li><a href="../id475228/index.html">Garpu penggajian. Anda adalah seorang programmer untuk ibu</a></li>
<li><a href="../id475238/index.html">Timeline Blade Runner - November 2019. Apakah ramalan itu menjadi kenyataan?</a></li>
<li><a href="../id475240/index.html">Menggunakan modul ketat dalam proyek Python skala besar: pengalaman Instagram. Bagian 1</a></li>
<li><a href="../id475242/index.html">Menggunakan modul ketat dalam proyek Python skala besar: pengalaman Instagram. Bagian 2</a></li>
<li><a href="../id475244/index.html">Fitur JavaScript Baru yang Diharapkan Harus Anda Ketahui</a></li>
<li><a href="../id475246/index.html">Pemrograman Asinkron Python: Tinjauan Singkat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>