<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèª‚Äç‚öñÔ∏è üôéüèø üßóüèª Servicio de cach√© inteligente basado en ZeroMQ y Tarantool üë©üèø‚Äçü§ù‚Äçüë®üèº ü§úüèΩ ‚ÜïÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ruslan Aromatov, desarrollador principal, ICD 



 Hola Habr! Trabajo como desarrollador de backend en Moscow Credit Bank, y durante mi trabajo he adq...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Servicio de cach√© inteligente basado en ZeroMQ y Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mkb/blog/471740/">  <b>Ruslan Aromatov, desarrollador principal, ICD</b> <br><br><img src="https://habrastorage.org/webt/ld/1c/ck/ld1cckil16z47pv5vgyjgi7kwa8.png"><br><br>  Hola Habr!  Trabajo como desarrollador de backend en Moscow Credit Bank, y durante mi trabajo he adquirido cierta experiencia que me gustar√≠a compartir con la comunidad.  Hoy les contar√© c√≥mo escribimos nuestro propio servicio de cach√© para los servidores frontales de nuestros clientes utilizando la aplicaci√≥n m√≥vil MKB Online.  Este art√≠culo puede ser √∫til para aquellos que est√°n involucrados en el dise√±o de servicios y est√°n familiarizados con la arquitectura de microservicios, la base de datos en memoria Tarantool y la biblioteca ZeroMQ.  En el art√≠culo no habr√° pr√°cticamente ejemplos de c√≥digo y explicaci√≥n de los conceptos b√°sicos, sino solo una descripci√≥n de la l√≥gica de los servicios y su interacci√≥n con un ejemplo espec√≠fico que ha estado trabajando en nuestra batalla durante m√°s de dos a√±os. <br><a name="habracut"></a><br><h4>  Como empez√≥ todo </h4><br>  Hace unos 6 a√±os, el esquema era simple.  Como legado de la empresa de outsourcing, obtuvimos dos clientes de banca m√≥vil para iOS y Android, as√≠ como un servidor frontal que los atiende.  El servidor en s√≠ fue escrito en Java, fue a su backend de diferentes maneras (principalmente jab√≥n) y se comunic√≥ con los clientes transmitiendo xml a trav√©s de https. <br><br>  Las aplicaciones de los clientes pudieron autenticarse de alguna manera, mostrar una lista de productos y ... parec√≠an poder realizar algunas transferencias y pagos, pero de hecho no lo hicieron muy bien y no siempre.  Por lo tanto, el servidor frontal no experiment√≥ una gran cantidad de usuarios ni ninguna carga seria (lo que, sin embargo, no evit√≥ que se cayera aproximadamente una vez cada dos d√≠as). <br><br>  Est√° claro que nosotros (y en ese momento nuestro equipo estaba formado por cuatro personas), como los responsables del banco m√≥vil, no nos adaptamos a esta situaci√≥n, y para empezar, pusimos en orden las aplicaciones actuales, pero el servidor frontal result√≥ ser realmente malo, por lo que tuvo que reescriba r√°pidamente todo, reemplazando simult√°neamente xml con json y pasando al servidor de aplicaciones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">WildFly</a> .  Repartido en un par de a√±os, la refactorizaci√≥n no se basa en una publicaci√≥n separada, ya que todo se hizo principalmente para garantizar que el sistema funcionara de manera estable. <br><br>  Poco a poco, las aplicaciones y el servidor desarrollados comenzaron a funcionar de manera m√°s estable y su funcionalidad se expandi√≥ constantemente, lo que vali√≥ la pena: hab√≠a cada vez m√°s usuarios. <br><br>  Al mismo tiempo, comenzaron a surgir problemas como la tolerancia a fallas, la redundancia, la replicaci√≥n y, atemorizante de pensar, la alta carga. <br><br>  Una soluci√≥n r√°pida al problema fue agregar un segundo servidor WildFly, y las aplicaciones aprendieron a cambiar entre ellos.  El problema del trabajo simult√°neo con sesiones de clientes fue resuelto por el m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Infinispan</a> integrado en WildFly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/li/xt/wx/lixtwxkd09us1bfivvdxjv-dxrk.png" alt="Como era antes"></div><br>  Parec√≠a que la vida estaba mejorando ... <br><br><h4>  No puedes vivir as√≠ </h4><br>  Sin embargo, esta opci√≥n de trabajar con sesiones, de hecho, no estuvo exenta de inconvenientes.  Mencionar√© aquellos que no nos conven√≠an. <br><br><ol><li>  P√©rdida de sesiones.  El menos importante.  Por ejemplo, una aplicaci√≥n env√≠a dos solicitudes al servidor-1: la primera solicitud es autenticaci√≥n y la segunda es una solicitud de una lista de cuentas.  La autenticaci√≥n es exitosa, se crea una sesi√≥n en el servidor-1.  En este momento, la segunda solicitud del cliente se interrumpe repentinamente debido a una comunicaci√≥n deficiente y la aplicaci√≥n cambia al servidor-2, reenviando la segunda solicitud.  Pero a una determinada carga de trabajo, Infinispan puede no tener tiempo para sincronizar datos entre nodos.  Como resultado, el servidor 2 no puede verificar la sesi√≥n del cliente, env√≠a una respuesta enojada al cliente, el cliente est√° triste y finaliza su sesi√≥n.  El usuario debe iniciar sesi√≥n nuevamente.  Triste </li><li>  Reiniciar el servidor tambi√©n puede causar la p√©rdida de sesiones.  Por ejemplo, despu√©s de una actualizaci√≥n (y esto sucede con bastante frecuencia).  Cuando se inicia el servidor 2, no puede funcionar hasta que los datos est√©n sincronizados con el servidor 1.  Parece que el servidor se inici√≥, pero de hecho no deber√≠a aceptar solicitudes.  Esto es inconveniente. </li><li>  Este es un m√≥dulo WildFly incorporado que nos impide alejarnos de este servidor de aplicaciones hacia microservicios. </li></ol><br>  A partir de aqu√≠, una lista de lo que nos gustar√≠a se form√≥ de alguna manera por s√≠ misma. <br><br><ol><li>  Queremos almacenar sesiones de clientes para que cualquier servidor (sin importar cu√°ntas haya) inmediatamente despu√©s del lanzamiento tenga acceso a ellas. </li><li>  Queremos almacenar cualquier informaci√≥n del cliente entre solicitudes (por ejemplo, par√°metros de pago y todo eso). </li><li>  Queremos guardar cualquier informaci√≥n arbitraria en una clave arbitraria en general. </li><li>  Y tambi√©n queremos recibir datos del cliente antes de que pase la autenticaci√≥n.  Por ejemplo, el usuario est√° autenticado y todos sus productos est√°n all√≠, frescos y c√°lidos. </li><li>  Y queremos escalar de acuerdo con la carga. </li><li>  Y ejecutar en la ventana acoplable, y escribir registros en una sola pila, y contar las m√©tricas, y as√≠ sucesivamente ... </li><li>  Ah, s√≠, y para que todo funcione r√°pidamente. </li></ol><br><h4>  Harina de elecci√≥n </h4><br>  Anteriormente, no implementamos la arquitectura de microservicios, as√≠ que para empezar nos sentamos a leer, mirar y probar diferentes opciones.  De inmediato qued√≥ claro que necesit√°bamos un repositorio r√°pido y alg√∫n tipo de complemento sobre √©l que se ocupe de la l√≥gica de negocios y sea la interfaz de acceso al repositorio.  Adem√°s, ser√≠a bueno asegurar el transporte r√°pido entre servicios. <br><br>  Eligieron durante mucho tiempo, discutieron mucho y experimentaron.  Ahora no describir√© los pros y los contras de todos los candidatos, esto no se aplica al tema de este art√≠culo, solo digo que el almacenamiento ser√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tarantool</a> , escribiremos nuestro servicio en Java y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ZeroMQ</a> funcionar√° como transporte.  Ni siquiera argumentar√© que la elecci√≥n es muy ambigua, pero fue influenciada en gran medida por el hecho de que no nos gustan los diferentes marcos grandes y pesados ‚Äã‚Äã(por su peso y lentitud), soluciones en caja (por su versatilidad y falta de personalizaci√≥n), pero al mismo tiempo Nos encanta controlar todas las partes de nuestro sistema tanto como sea posible.  Y para controlar el trabajo de los servicios, elegimos el servidor de recopilaci√≥n de m√©tricas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Prometheus</a> con sus agentes convenientes que pueden integrarse en casi cualquier c√≥digo.  Los registros de todo esto ir√°n a la pila de ELK. <br><br>  Bueno, me parece que ya hab√≠a demasiada teor√≠a. <br><br><h4>  Comenzar y terminar </h4><br>  El resultado del dise√±o fue aproximadamente ese esquema. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/he/kw/oh/hekwohpipjcqh4ufeoykog6f8cc.png" alt="Como queremos"></div><br>  <b>Almacenamiento</b> <br><br>  Debe ser lo m√°s est√∫pido posible, solo para almacenar datos y sus estados actuales, pero siempre debe funcionar sin reinicios.  Dise√±ado para servir diferentes versiones de servidores frontales.  Mantenemos todos los datos en la memoria, recuperaci√≥n en caso de reinicio a trav√©s de archivos .snap y .xlog. <br><br>  Tabla (espacio) para sesiones de cliente: <br><br><ul><li>  ID de sesi√≥n </li><li>  Identificaci√≥n del cliente; </li><li>  versi√≥n (servicio) </li><li>  tiempo de actualizaci√≥n (marca de tiempo); </li><li>  tiempo de vida (ttl); </li><li>  datos de sesi√≥n serializados. </li></ul><br>  Aqu√≠ todo es simple: el cliente est√° autenticado, el servidor frontal crea una sesi√≥n y la guarda en el almacenamiento, recordando la hora.  Con cada solicitud de datos, el tiempo se actualiza, por lo que la sesi√≥n se mantiene viva.  Si, a pedido, los datos no est√°n actualizados (o no habr√° ninguno), entonces devolveremos un c√≥digo de retorno especial, despu√©s de lo cual el cliente finalizar√° su sesi√≥n. <br><br>  Tabla de cach√© simple (para cualquier dato de sesi√≥n): <br><br><ul><li>  clave </li><li>  ID de sesi√≥n </li><li>  tipo de datos almacenados (n√∫mero arbitrario); </li><li>  tiempo de actualizaci√≥n (marca de tiempo); </li><li>  tiempo de vida (ttl); </li><li>  Datos serializados. </li></ul><br>  Tabla de datos del cliente que debe calentarse antes de iniciar sesi√≥n: <br><ul><li>  Identificaci√≥n del cliente; </li><li>  ID de sesi√≥n </li><li>  versi√≥n (servicio) </li><li>  tipo de datos almacenados (n√∫mero arbitrario); </li><li>  tiempo de actualizaci√≥n (marca de tiempo); </li><li>  condici√≥n </li><li>  Datos serializados. </li></ul><br>  Un campo importante aqu√≠ es la condici√≥n.  En realidad, solo hay dos: inactivo y actualizando.  Los coloca un servicio superpuesto que va al backend para obtener datos del cliente, de modo que otra instancia de este servicio no haga el mismo trabajo (ya in√∫til) y no cargue el backend. <br><br>  Tabla de dispositivos: <br><br><ul><li>  Identificaci√≥n del cliente; </li><li>  ID del dispositivo </li><li>  tiempo de actualizaci√≥n (marca de tiempo); </li></ul><br>  La tabla de dispositivos es necesaria para que, incluso antes de que el cliente se autentique en el sistema, descubra su ID y comience a recibir sus productos (calentando el cach√©).  La l√≥gica es esta: la primera entrada siempre est√° fr√≠a, porque antes de la autenticaci√≥n no sabemos qu√© tipo de cliente proviene de un dispositivo desconocido (los clientes m√≥viles siempre transmiten ID de dispositivo en cualquier solicitud).  Todas las entradas posteriores de este dispositivo ir√°n acompa√±adas de un cach√© de calentamiento para el cliente asociado con √©l. <br><br>  El trabajo con datos est√° aislado del servicio de Java mediante procedimientos del servidor.  S√≠, tuve que aprender lua, pero no me llev√≥ mucho tiempo.  Adem√°s de la gesti√≥n de datos en s√≠, los procedimientos lua tambi√©n son responsables de devolver los estados actuales, las selecciones de √≠ndice, la limpieza de registros obsoletos en procesos en segundo plano (fibras) y la operaci√≥n del servidor web incorporado a trav√©s del cual se lleva a cabo el servicio directo de acceso a los datos.  Aqu√≠ est√°, el encanto de escribir todo con las manos, la posibilidad de un control ilimitado.  Pero la desventaja es la misma: debe escribir todo usted mismo. <br><br>  Tarantool funciona en un contenedor acoplable, todos los archivos lua necesarios se colocan all√≠ en la etapa de ensamblaje de im√°genes.  Todo el ensamblaje a trav√©s de scripts gradle. <br><br>  Replicaci√≥n maestro-esclavo.  En el otro host, se ejecuta exactamente el mismo contenedor que la r√©plica del almacenamiento principal.  Se necesita en caso de un bloqueo de emergencia del maestro; luego, los servicios de Java cambian a esclavo y se convierte en el maestro.  Hay un tercer esclavo por si acaso.  Sin embargo, incluso una p√©rdida de datos completa en nuestro caso es triste, pero no fatal.  Seg√∫n el peor de los casos, los usuarios tendr√°n que iniciar sesi√≥n y recuperar todos los datos que vuelven a la cach√©. <br><br>  <b>Servicio de Java</b> <br><br>  Dise√±ado como un microservicio sin estado t√≠pico.  No tiene configuraci√≥n, todos los par√°metros necesarios (y hay 6 de ellos) se pasan a trav√©s de las variables de entorno al crear el contenedor acoplable.  Funciona con el servidor frontal a trav√©s del transporte ZeroMQ (org.zeromq.jzmq - la interfaz de Java para el libzmq.so.5.1.1 nativo, que nosotros mismos construimos) usando nuestro propio protocolo.  Funciona con una tar√°ntula a trav√©s de un conector java (org.tarantool.connector). <br><br>  La inicializaci√≥n del servicio es bastante simple: <br><br><ul><li>  Iniciamos un registrador (log4j2); </li><li>  De las variables de entorno (estamos en la ventana acoplable) leemos los par√°metros necesarios para el trabajo; </li><li>  Iniciamos el servidor de m√©tricas (embarcadero); </li><li>  Con√©ctese a la tar√°ntula (asincr√≥nicamente); </li><li>  Comenzamos el n√∫mero requerido de manipuladores de hilos (trabajadores); </li><li>  Comenzamos un corredor (zmq), un ciclo interminable de procesamiento de mensajes. </li></ul><br>  De todo lo anterior, solo el motor de procesamiento de mensajes es interesante.  A continuaci√≥n se muestra un diagrama del microservicio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/fd/ef/myfdef3ggy0oerhyfvec3iozvwc.png" alt="Message Broker Logic"></div><br>  Comencemos con el inicio del corredor.  Nuestro broker es un conjunto de zmq-sockets del tipo ROUTER, que acepta conexiones de varios clientes y es responsable de programar los mensajes que provienen de ellos. <br><br>  En nuestro caso, tenemos un socket de escucha en la interfaz externa que recibe mensajes de clientes que usan el protocolo tcp y el otro recibe mensajes de subprocesos de los trabajadores que usan el protocolo inproc (es mucho m√°s r√°pido que tcp). <br><br><pre><code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** //   (   ,   ) ZContext zctx = new ZContext(); //    ZMQ.Socket clientServicePoint = zctx.createSocket(ZMQ.ROUTER); //    ZMQ.Socket workerServicePoint= zctx.createSocket(ZMQ.ROUTER); //     clientServicePoint.bind("tcp://*:" + Config.ZMQ_LISTEN_PORT); //     workerServicePoint.bind("inproc://worker-proc");</span></span></code> </pre> <br>  Despu√©s de inicializar los sockets, comenzamos un ciclo de eventos sin fin. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *      */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> status;  <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> {   ZMQ.Poller poller = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ZMQ.Poller(<span class="hljs-number"><span class="hljs-number">2</span></span>);    poller.register(workerServicePoint, ZMQ.Poller.POLLIN);    poller.register(clientServicePoint, ZMQ.Poller.POLLIN);    <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> rc;    <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) {      <span class="hljs-comment"><span class="hljs-comment">//        rc = poller.poll(POLL_INTERVAL);      if (rc == -1) {        status = -1;        logger.errorInternal("Broker run error rc = -1");        break; //  -     }    //     ()    if (poller.pollin(0)) {       processBackendMessage(ZMsg.recvMsg(workerServicePoint));    }    //        if (poller.pollin(1)) {       processFrontendMessage(ZMsg.recvMsg(clientServicePoint));    }    processQueueForBackend(); }  } catch (Exception e) {    status = -1;  } finally {    clientServicePoint.close();    workerServicePoint.close();  }  return status; }</span></span></code> </pre><br>  La l√≥gica del trabajo es muy simple: recibimos mensajes de diferentes lugares y hacemos algo con ellos.  Si algo se rompi√≥ cr√≠ticamente con nosotros, salimos del bucle, lo que hace que el proceso se bloquee, lo que ser√° reiniciado autom√°ticamente por el demonio docker. <br><br>  La idea principal es que el intermediario no maneja ninguna l√≥gica comercial, solo analiza el encabezado del mensaje y distribuye las tareas a los hilos de trabajo que se iniciaron antes cuando comenz√≥ el servicio.  En esto, una sola cola de mensajes con priorizaci√≥n de una longitud fija lo ayuda. <br><br>  Analicemos el algoritmo usando el ejemplo del esquema y el c√≥digo anterior. <br><br>  Despu√©s del inicio, los trabajadores de subprocesos que comenzaron m√°s tarde que el intermediario se inicializan y env√≠an un mensaje de preparaci√≥n al intermediario.  El corredor los acepta, los analiza y agrega a cada trabajador a la lista. <br><br>  Ocurre un evento en el socket del cliente: recibimos mensaje1.  El agente llama al manejador de mensajes entrantes, cuya tarea es: <br><br><ul><li>  an√°lisis del encabezado del mensaje; </li><li>  colocar un mensaje en un objeto titular con una prioridad dada (basada en el an√°lisis del encabezado) y de por vida; </li><li>  colocar al titular en la cola de mensajes; </li><li>  si la cola no est√° llena, la tarea del controlador ha terminado; </li><li>  Si la cola est√° llena, llamamos al m√©todo para enviar un mensaje de error al cliente. </li></ul><br>  En la misma iteraci√≥n del bucle, llamamos al manejador de la cola de mensajes: <br><br><ul><li>  solicitamos el mensaje m√°s reciente de la cola (la cola decide esto por s√≠ misma en funci√≥n de la prioridad y el orden de agregar el mensaje); </li><li>  verifique la vida √∫til del mensaje (si ha expirado, llame al m√©todo para enviar un mensaje de error al cliente); </li><li>  si el mensaje para el procesamiento es relevante, intente preparar al primer trabajador libre para trabajar; </li><li>  si no hay ninguno, vuelva a colocar el mensaje en la cola (m√°s precisamente, simplemente no lo elimine de all√≠, permanecer√° all√≠ hasta que expire su vida √∫til); </li><li>  si tenemos un trabajador listo para trabajar, lo marcamos como ocupado y le enviamos un mensaje para que lo procese; </li><li>  Eliminar el mensaje de la cola. </li></ul><br>  Hacemos lo mismo con todos los mensajes posteriores.  El trabajador de subprocesos en s√≠ est√° dise√±ado de la misma manera que un intermediario: tiene el mismo ciclo interminable de procesamiento de mensajes.  Pero en √©l ya no necesitamos procesamiento instant√°neo, est√° dise√±ado para realizar tareas largas. <br><br>  Despu√©s de que el trabajador complet√≥ su tarea (por ejemplo, fue al back-end para los productos del cliente o en la tar√°ntula para la sesi√≥n), env√≠a un mensaje al corredor, que el corredor env√≠a de vuelta al cliente.  La direcci√≥n del cliente a quien se debe enviar la respuesta se recuerda desde el momento en que llega el mensaje del cliente en el objeto titular, que se env√≠a al trabajador como un mensaje en un formato ligeramente diferente, y luego regresa. <br><br>  El formato de los mensajes que menciono constantemente es nuestra propia producci√≥n.  Fuera de la caja, ZeroMQ nos proporciona las clases de ZMsg, el mensaje en s√≠ mismo y el ZFrame, parte de este mensaje, esencialmente solo una matriz de bytes, que soy libre de usar si existe tal deseo.  Nuestro mensaje consta de dos partes (dos ZFrames), la primera de las cuales es un encabezado binario y la segunda son datos (el cuerpo de la solicitud, por ejemplo, en forma de una cadena json representada por una matriz de bytes).  El encabezado del mensaje es universal y viaja tanto de cliente a servidor como de servidor a cliente. <br><br>  De hecho, no tenemos el concepto de "solicitud" o "respuesta", solo mensajes.  El encabezado contiene: versi√≥n del protocolo, tipo de sistema (qu√© sistema se dirige), tipo de mensaje, c√≥digo de error de nivel de transporte (si no es 0, algo sucedi√≥ en el motor de transferencia de mensajes), ID de solicitud (identificador de transferencia que viene del cliente - necesario para el seguimiento), ID de sesi√≥n del cliente (opcional), as√≠ como un signo de un error de nivel de datos (por ejemplo, si no se pudo analizar la respuesta del backend, configuramos este indicador para que el analizador del lado del cliente no deserialice la respuesta, pero reciba datos de error de otra manera). <br><br>  Gracias a un protocolo √∫nico entre todos los microservicios y un encabezado de este tipo, podemos manipular los componentes de nuestros servicios.  Por ejemplo, puede llevar el intermediario a un proceso separado y convertirlo en un √∫nico intermediario de mensajes a nivel de todo el sistema de microservicios.  O, por ejemplo, ejecutar trabajadores no en forma de hilos dentro del proceso, sino como procesos independientes separados.  Y aunque el c√≥digo dentro de ellos no cambia.  En general, hay margen para la creatividad. <br><br><h4>  Un poco sobre rendimiento y recursos </h4><br>  El corredor en s√≠ mismo es r√°pido, y el ancho de banda total del servicio est√° limitado por la velocidad del backend y la cantidad de trabajadores.  Convenientemente, toda la cantidad de memoria necesaria se asigna inmediatamente al inicio del servicio, y todos los subprocesos se inician de inmediato.  El tama√±o de la cola tambi√©n es fijo.  En tiempo de ejecuci√≥n, solo se procesan los mensajes. <br><br>  Como ejemplo: adem√°s del hilo principal, nuestro servicio de combate de cach√© actual lanza otros 100 hilos de trabajo, y el tama√±o de la cola est√° limitado a tres mil mensajes.  En funcionamiento normal, cada instancia procesa hasta 200 mensajes por segundo y consume aproximadamente 250 MB de memoria y aproximadamente el 2-3% de la CPU.  A veces, en las cargas m√°ximas, salta al 7-8%.  Todo funciona en alg√∫n tipo de xeon virtual de doble n√∫cleo. <br><br>  El trabajo regular del servicio implica el empleo simult√°neo de 3-5 trabajadores (de cada 100) con el n√∫mero de mensajes en la cola 0 (es decir, pasan al procesamiento de inmediato).  Si el backend comienza a disminuir, el n√∫mero de trabajadores ocupados aumenta en proporci√≥n al tiempo de su respuesta.  En los casos en que se produce un accidente y el back-end aumenta, todos los trabajadores terminan primero, despu√©s de lo cual la cola de mensajes comienza a obstruirse.  Cuando se obstruye por completo, comenzamos a responder a los clientes con rechazos para procesar.  Al mismo tiempo, no comenzamos a consumir recursos de memoria o CPU, brindando m√©tricas de manera estable y respondiendo claramente a los clientes lo que est√° sucediendo. <br><br>  La primera captura de pantalla muestra el funcionamiento regular del servicio. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ji/ek/ap/jiekapcj1vylguqgijeheydw7nu.png" alt="El trabajo regular del servicio."></div><br>  Y en el segundo, ocurri√≥ un accidente: el backend por alguna raz√≥n no respondi√≥ en 30 segundos.  Se ve que al principio todos los trabajadores se quedaron sin trabajo, despu√©s de lo cual la cola de mensajes comenz√≥ a obstruirse. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4v/he/h8/4vheh8hrqsinriaelyow6bgp9wu.png" alt="Accidente"></div><br><h4>  Pruebas de rendimiento </h4><br>  Las pruebas sint√©ticas en mi m√°quina de trabajo (CentOS 7, Core i5, 16Gb RAM) mostraron lo siguiente. <br><br>  Trabaje con el repositorio (escribiendo en la tar√°ntula e inmediatamente leyendo este registro de 100 bytes de tama√±o, simulando el trabajo con la sesi√≥n) - 12000 rps. <br><br>  Lo mismo, solo se midi√≥ la velocidad no entre los puntos de tar√°ntula del servicio, sino entre el cliente y el servicio.  Por supuesto, tuve que escribirle a un cliente para hacerme una prueba de estr√©s.  Dentro de una m√°quina, fue posible obtener 7000 rps.  En una red local (y tenemos muchas m√°quinas virtuales diferentes que no est√°n claras c√≥mo est√°n f√≠sicamente conectadas), los resultados var√≠an, pero es posible hasta 5000 rps para una instancia.  Dios sabe qu√© tipo de rendimiento, pero cubre m√°s de diez veces nuestras cargas m√°ximas.  Y esto es solo si se est√° ejecutando una instancia del servicio, pero tenemos varios de ellos, y en cualquier momento puede ejecutar tantos como necesite.  Cuando los servicios bloquean la velocidad de almacenamiento, ser√° posible escalar la tar√°ntula horizontalmente (fragmento basado en la identificaci√≥n del cliente, por ejemplo). <br><br><h4>  Servicio de inteligencia </h4><br>  El lector atento probablemente ya hace la pregunta: ¬øcu√°l es la "inteligencia" de este servicio, que se menciona en el t√≠tulo.  Ya mencion√© esto de pasada, pero ahora te contar√© m√°s. <br><br>  Una de las tareas principales del servicio era reducir el tiempo que lleva emitir sus productos a los usuarios (listas de cuentas, tarjetas, dep√≥sitos, pr√©stamos, paquetes de servicios, etc.) mientras se reduce la carga en el back-end (reduciendo el n√∫mero de solicitudes en Oracle grande y pesado) debido al almacenamiento en cach√© en la tar√°ntula. <br><br>  Y lo hizo bastante bien.  La l√≥gica para calentar el cach√© del cliente es la siguiente: <br><br><ul><li>  el usuario inicia la aplicaci√≥n m√≥vil; </li><li>  Se env√≠a una solicitud AppStart que contiene la ID del dispositivo al servidor frontal; </li><li>  el servidor frontal env√≠a un mensaje con esta ID al servicio de cach√©; </li><li>  el servicio busca en la tabla del dispositivo la ID del cliente para este dispositivo; </li><li>  si no est√° all√≠, no pasa nada (la respuesta ni siquiera se env√≠a, el servidor no la espera); </li><li>  Si se encuentra la ID del cliente, el trabajador crea un conjunto de mensajes para recibir listas de productos de usuario que el agente procesa inmediatamente y se distribuyen a los trabajadores en modo normal; </li><li>  cada trabajador env√≠a una solicitud de un cierto tipo de datos al usuario, colocando el estado de "actualizaci√≥n" en la base de datos (este estado protege al backend de repetir las mismas solicitudes si provienen de otras instancias del servicio); </li><li>  despu√©s de recibir los datos, se registran en la tar√°ntula; </li><li>  el usuario inicia sesi√≥n en el sistema, y ‚Äã‚Äãla aplicaci√≥n env√≠a solicitudes para recibir sus productos, y el servidor env√≠a estas solicitudes en forma de mensajes al servicio de cach√©; </li><li>  Si los datos del usuario ya han sido recibidos, simplemente los enviamos desde el cach√©; </li><li>  si los datos est√°n en proceso de ser recibidos (estado de "actualizaci√≥n"), se inicia un ciclo de espera de datos dentro del trabajador (es igual al tiempo de espera de solicitud para el back-end); </li><li>  tan pronto como se reciben los datos (es decir, el estado de este registro (tupla) en la tabla pasa a "inactivo", el servicio se lo dar√° al cliente; </li><li>  Si los datos no se reciben dentro de un cierto intervalo de tiempo, se devolver√° un error al cliente. </li></ul><br>  Por lo tanto, en la pr√°ctica, pudimos reducir el tiempo promedio de recepci√≥n de productos para el servidor frontal de 200 ms a 20 ms, es decir, en aproximadamente 10 veces, y el n√∫mero de solicitudes al backend en aproximadamente 4 veces. <br><br><h4>  Los problemas </h4><br>  El servicio de cach√© ha estado trabajando en la batalla durante aproximadamente dos a√±os y actualmente satisface nuestras necesidades. <br><br>  Por supuesto, todav√≠a hay problemas sin resolver, a veces ocurren problemas.  Los servicios de Java en la batalla a√∫n no han ca√≠do.  La tar√°ntula cay√≥ un par de veces en SIGSEGV, pero era una versi√≥n antigua, y despu√©s de la actualizaci√≥n no volvi√≥ a suceder.  Durante la prueba de esfuerzo, la replicaci√≥n se cae, una tuber√≠a rota ocurri√≥ en el maestro, despu√©s de lo cual el esclavo se cay√≥, aunque el maestro continu√≥ trabajando.  Se decidi√≥ reiniciando el esclavo. <br><br>  Una vez que hubo alg√∫n tipo de accidente en el centro de datos, result√≥ que el sistema operativo (CentOS 7) dej√≥ de ver discos duros.  El sistema de archivos entr√≥ en modo de solo lectura.  Lo m√°s sorprendente fue que los servicios continuaron funcionando, ya que guardamos todos los datos en la memoria.  La tar√°ntula no pudo escribir archivos .xlog, nadie registr√≥ nada, pero de alguna manera todo funcion√≥.  Pero el intento de reiniciar no tuvo √©xito: nadie pudo comenzar. <br><br>  Hay un gran problema sin resolver, y me gustar√≠a escuchar la opini√≥n de la comunidad sobre este tema.  Cuando la tar√°ntula maestra se bloquea, los servicios de Java pueden cambiar a esclavo, que contin√∫a funcionando como maestro.  Sin embargo, esto solo sucede si el maestro falla y no puede funcionar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e6/pu/wh/e6puwh26kmnngnrxkvyelgvvite.png" alt="Problema sin resolver"></div><br>  Supongamos que tenemos 3 instancias de un servicio que funcionan con datos en una tar√°ntula maestra.  Los servicios en s√≠ no se caen, la replicaci√≥n de la base de datos contin√∫a, todo est√° bien.  Pero de repente tenemos una red que se desmorona entre el nodo 1 y el nodo 4, donde funciona el asistente.  El servicio 1 despu√©s de varios intentos fallidos decide cambiar a la base de datos de respaldo y comienza a enviar solicitudes all√≠. <br><br>  Inmediatamente despu√©s de esto, el esclavo de tar√°ntula comienza a aceptar solicitudes de modificaci√≥n de datos, como resultado de lo cual la replicaci√≥n del maestro se desmorona, y obtenemos datos inconsistentes.  Al mismo tiempo, los servicios 2 y 3 funcionan perfectamente con el maestro, y el servicio 1 se comunica bien con el antiguo esclavo.  Est√° claro que en este caso comenzamos a perder sesiones de clientes y cualquier otro dato, aunque todo funciona desde el punto de vista t√©cnico.  Todav√≠a no hemos resuelto un problema tan potencial.  Afortunadamente, esto no ha sucedido en 2 a√±os, pero la situaci√≥n es bastante real.  Ahora cada servicio conoce el n√∫mero de la tienda a la que va, y tenemos una alerta para esta m√©trica, que funcionar√° al cambiar de maestro a esclavo.  Y tienes que reparar todo con tus manos.  ¬øC√≥mo se resuelven estos problemas? <br><br><h4>  Planes </h4><br>  Planeamos trabajar en el problema descrito anteriormente, limitando el n√∫mero de trabajadores simult√°neamente ocupados con un tipo de solicitud, seguro (sin perder las solicitudes actuales) para detener el servicio y pulir a√∫n m√°s. <br><br><h4>  Conclusi√≥n </h4><br>  Eso, quiz√°s, es todo, aunque analic√© el tema de manera bastante superficial, pero la l√≥gica general del trabajo deber√≠a ser clara.  Por lo tanto, si es posible, estoy listo para responder en los comentarios.  Describ√≠ brevemente c√≥mo un peque√±o subsistema auxiliar de los servidores frontales del banco funciona para atender a clientes m√≥viles. <br><br>  Si el tema es de inter√©s para la comunidad, puedo informarle sobre varias de nuestras soluciones que contribuyen a mejorar la calidad del servicio al cliente para el banco. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/471740/">https://habr.com/ru/post/471740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../471724/index.html">Necesidades √°giles y cerebrales: manejo del estr√©s</a></li>
<li><a href="../471726/index.html">M√©todo moderno para medir la respuesta al impulso y la distorsi√≥n no lineal.</a></li>
<li><a href="../471728/index.html">Avalonia mis pros y contras</a></li>
<li><a href="../471736/index.html">Sensor Ethernet sin contacto</a></li>
<li><a href="../471738/index.html">Una breve historia sobre c√≥mo la conveniencia a veces se dispara en la rodilla</a></li>
<li><a href="../471742/index.html">Sberbank AI Journey. C√≥mo ense√±amos a una red neuronal a tomar un examen</a></li>
<li><a href="../471744/index.html">Tarantool Data Grid: arquitectura y caracter√≠sticas</a></li>
<li><a href="../471746/index.html">Gu√≠a completa para configurar encabezados HTTP para seguridad</a></li>
<li><a href="../471748/index.html">Optimizaci√≥n de farmacia: lo que hicimos con las matem√°ticas</a></li>
<li><a href="../471750/index.html">Gesti√≥n de acceso privilegiado como tarea prioritaria en seguridad de la informaci√≥n (por ejemplo, Fudo PAM)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>