<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤸🏿 ⚰️ 🐽 Ein ungewöhnliches Objektiv für eine normale Kamera oder wie man aufhört, über den Fokus nachzudenken 🤶🏾 👨🏿‍💼 👵🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fast zwei Jahrhunderte der Existenz der Kamera hätten den Ingenieuren nicht die Möglichkeit geben dürfen, „etwas anderes“ hinzuzufügen. Moderne Kamera...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ein ungewöhnliches Objektiv für eine normale Kamera oder wie man aufhört, über den Fokus nachzudenken</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/414877/">  Fast zwei Jahrhunderte der Existenz der Kamera hätten den Ingenieuren nicht die Möglichkeit geben dürfen, „etwas anderes“ hinzuzufügen.  Moderne Kameras nehmen qualitativ hochwertige Videos auf, laden Fotos in die Cloud hoch und fangen Geo-Tags ein.  Wir können Panoramen und 360 ° aufnehmen, die Sterne beobachten und die Zeit verlangsamen.  Aber der Fortschritt steht nicht still, sondern eilt in die Zukunft, angeheizt von forschenden Köpfen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cb/co/ue/cbcoueypd1fnm9blmcb3ohpqyeu.png" alt="Bildtest"></div><br>  Die Technologie, die heute diskutiert wird, ist nicht neu.  Aber die Art und Weise, wie es implementiert wird, ist definitiv eine Überlegung wert.  Es wird ein interessantes Lichtfeldobjektiv sein, das mit jeder DSLR-Kamera verwendet werden kann. <br><a name="habracut"></a><br><h2>  Was ist ein Lichtfeld und womit frisst es? </h2><br>  Der Begriff <i>Lichtfeld</i> selbst wurde bereits 1936 vom sowjetischen Physiker Gershun in seiner Arbeit über die radiometrischen Eigenschaften von Licht vorgeschlagen. <br><br>  Ein Lichtfeld ist eine Vektorfunktion, die Licht beschreibt, das in eine beliebige Richtung durch einen Punkt im Raum fällt. <img src="https://habrastorage.org/webt/av/tv/ns/avtvnsfa4n-252jrwzy2cntdf5k.png" alt="Bild" align="right">  Ein Lichtstrahl (oder vielmehr seine Richtung) für einen bestimmten Punkt im Raum kann durch fünf Parameter (die sogenannte 5D-plenoptische Funktion) beschrieben werden: <i>x-</i> , <i>y-</i> , <i>z-</i> Koordinaten und zwei Winkel <i>θ</i> und <i>ϕ</i> .  Durch Integration der aus verschiedenen Blickwinkeln erhaltenen Feldvektoren erhalten wir den Gesamtbeleuchtungswert.  Mit einer vollständigen Beschreibung der Lichtstrahlen im Raum können wir beispielsweise genau bestimmen, wie ein Objekt aus jedem Blickwinkel aussieht. <br><br>  Was ist die praktische Anwendung der Theorie des Lichtfeldes?  Eines der interessantesten Gebiete sind Lichtfeldkameras.  Im Gegensatz zu klassischen Kameras, die die Intensität des Lichts an den Punkten eines Objekts aufzeichnen, berücksichtigt die Kamera des Lichtfelds auch die Richtung der Strahlen, des Ausgangs und dieser Punkte.  Mit anderen Worten, wir erfassen die „individuellen“ Lichtstrahlen, die vom Objekt ausgehen.  Auf diese Weise können Sie die physischen Koordinaten von Objekten im Raum und eine Tiefenkarte abrufen. <br><br><h2>  Wie sind Lichtfeldkameras angeordnet? </h2><br>  Wir wissen bereits, dass eine Kamera dieses Typs nicht nur die Intensität, sondern auch die Richtung der vom Objekt ausgehenden Lichtstrahlen aufzeichnen sollte.  Eine Möglichkeit, dies zu implementieren, besteht darin, eine Anordnung von Linsen vor dem optischen Sensor zu verwenden.  Diese Linsen sammeln Lichtstrahlen von einem Objekt in einem bestimmten Teil der Szene und fokussieren sie auf den Sensor. <br><br>  Es ist wichtig zu verstehen, dass in diesem Fall die Hauptlinse der Linse das Bild nicht mehr auf den Sensor fokussiert.  Stattdessen werden die Strahlen auf die Ebene des Linsenarrays projiziert (bei klassischen Kameras befindet sich der Sensor genau in dieser Ebene), das Linsenarray passiert und fällt erst dann auf den Sensor, wodurch ein Mosaikbild verschiedener Teile der Szene entsteht. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qf/gl/5i/qfgl5i2miblhvrhz_ge49lqfrnq.png" alt="Bild"></div><br>  Die Abbildung zeigt ein vereinfachtes Diagramm des Betriebs einer solchen Linse.  Dank der ausgeklügelten Organisation des optischen Systems erhalten wir am Ende nicht nur ein, sondern viele Bilder des Objekts, und jedes dieser Bilder erzeugt eine einzigartige Darstellung des Objekts aus seinem einzigartigen Blickwinkel. <br><br>  Dieses Schema weist jedoch eine Reihe von Nachteilen auf, wie beispielsweise die hohen Herstellungskosten, die Komplexität der Kalibrierung, die Apertursteuerung und andere Systemparameter.  Eines der bekanntesten Beispiele für solche Kameras ist das Produkt von Lytro - die Lytro Illum-Kamera (das Projekt scheint eingefroren zu sein). <br><br><h2>  Kannst du es einfacher machen? </h2><br>  Du kannst.  Das Objektiv, über das ich in diesem Artikel sprechen möchte, enthält keine Reihe von Mikrolinsen.  Stattdessen wird ein System verwendet, bei dem es sich um einen Spiegelkanal mit rechteckigem Querschnitt (Spiegelbox) handelt, bei dem dank Mehrfachreflexion ein sogenanntes kaleidoskopisches Bild entsteht, das vom Kamerasensor auf übliche Weise aufgezeichnet wird. <br><br><img src="https://habrastorage.org/webt/jp/gt/dh/jpgtdhotrubcdjqctkx1ypxtxv8.png" alt="Bild"><br><br>  Ein kleines deutsches Unternehmen entwickelt sich.  Das Objektiv befindet sich im Stadium eines voll funktionsfähigen Prototyps, und das Prinzip seiner Funktionsweise ist recht einfach. <br><br>  Die vom System erhaltenen Bilder sehen ungefähr so ​​aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bc/0z/u9/bc0zu9ojlfgakmjvfyu2ed6j6og.png" alt="Bild"></div><br>  Elemente hier werden gespiegelt.  Ein solches ungewöhnliches kaleidoskopisches Bild ist eine Folge der Reflexion von Strahlen im „Spiegelkanal“. <br><br>  Und so sieht der absolute Unterschied des Paares wiederhergestellter Elemente aus (helle Pixel bedeuten einen größeren Wertunterschied): <br><br><img src="https://habrastorage.org/webt/e1/t5/po/e1t5poethhxhqe9f2i8xmplliys.png" alt="Bild"><br><br>  Mit anderen Worten, wir haben nichts als ein Stereopaar.  Oder besser gesagt, Stereo 9 (3x3 Elemente).  Wenn wir die geometrischen Parameter des Kanals ändern, können wir 5x5 und sogar große Dimensionen erhalten, was jedoch im wirklichen Leben keinen Sinn macht und sogar schadet. <br><br>  Wir haben also eine Reihe von Bildern, die ein kaleidoskopisches Bild bilden.  Was kommt als nächstes? <br><br>  Hier endet die warme analoge optische Hardware und die kalte digitale Software beginnt. <br><br><h2>  Kalibrierung </h2><br>  Unabhängig von der Anwendung müssen die Bilder wiederhergestellt werden (Sie müssen das gesamte optische System kalibrieren und die erhaltenen Kalibrierungsdaten auf die Bilder anwenden).  Der Prozess ist ziemlich langwierig, aber wichtig, da die verschiedenen Elemente des kaleidoskopischen Bildes unbedingt miteinander „koordiniert“ werden müssen (selbst unbedeutende / mehrere Pixel / Abweichungen der Elemente können das Ergebnis und den Eindruck stark beeinträchtigen).  Es gibt viele Arbeiten zum Thema Kalibrierung, daher macht es keinen Sinn, Details preiszugeben.  Sie müssen sich nur daran erinnern, dass die Kalibrierung für jede Stereoanwendung sehr wichtig ist. <br><br><h2>  Tiefenkarte </h2><br>  Nachdem wir „gerade“ Bilder erhalten haben, können wir eine Tiefenkarte erstellen. <br>  Dies ist vielleicht der wichtigste und schwierigste Teil der Pipeline.  Die Qualität der endgültigen Anwendung hängt von der Qualität der Tiefenkarte ab.  Und die Qualität der Tiefenkarte hängt wiederum von der Qualität der Kalibrierung, dem ausgewählten Algorithmus und der "Komplexität" der Szene ab. <br><br>  Unabhängig vom Algorithmus ist die Aufgabe jedoch immer dieselbe - die entsprechenden Punkte der linken und rechten Bilder (und in unserem Fall + 7 weitere Bilder) zu finden und den Abstand (Disparität) zwischen ihnen zu berechnen.  Der Abstandswert ist der Kehrwert des Tiefenwerts für ein bestimmtes Pixel. <br><br>  Warum 9 Bilder verwenden, wenn Sie mit zwei auskommen können?  Wenn wir mehr Bilder verwenden, haben wir offensichtlich mehr Informationen über die Szene und können einige Probleme bestehender Algorithmen zur Schätzung der Tiefenkarte teilweise lösen. <br><br>  Zu den klassischen Problemen solcher Algorithmen gehören: <br><br><ul><li>  Eintönige, einfarbige Oberflächen ohne Textur - der Algorithmus hat beim Auffinden von Übereinstimmungen einfach nichts zu „fangen“ </li><li>  Überlappende Objekte (sichtbar von einer Ecke und unsichtbar von einer anderen) </li><li>  Schatten und Reflexionen auf verspiegelten oder glänzenden Oberflächen </li><li>  Regelmäßige Strukturen wie Zellen und Streifen stellen Probleme dar, da nicht immer klar ist, welche Zelle von Bild A der Zelle von Bild B entspricht. </li><li>  Rahmen von Bildern - ein Problem ähnlich dem Problem überlappender Objekte.  An den Bildrändern gehen Informationen aus jedem Blickwinkel unweigerlich verloren. </li></ul><br>  Es gibt viele hochwertige und nicht sehr Algorithmen zum Erstellen einer Tiefenkarte.  Die vielversprechendsten Entwicklungen liegen nun im Bereich hybrider Ansätze mit klassischen Methoden und verschiedenen Techniken des maschinellen Lernens (CNN, DNN).  Wie immer ist die Wahl des Algorithmus ein Kompromiss zwischen Geschwindigkeit und Qualität.  Glücklicherweise können wir es uns in der Fotografie leisten, von der Echtzeit zurückzutreten und eine bessere Tiefenkarte zu erhalten. <br><br>  In unserem Beispiel sieht die Tiefenkarte ungefähr so ​​aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/a5/p2/lka5p2k7vkqazvlzads_pqai9y4.png" alt="Bild"></div><br><br><h2>  Post Fokus </h2><br>  Wir haben eine Karte der Tiefen, was machen wir jetzt damit?  Informationen über die Entfernung von Objekten sind oft nützlich.  Eine beliebte Anwendung ist Post Focus. <br><br>  Unscharf zu werden ist eines der Probleme von Fotografen.  Haben Sie bemerkt, dass im Originalbild die gesamte Szene scharfgestellt war?  So sieht die Nachfokussierung basierend auf einer Tiefenkarte aus: <br><br><img src="https://habrastorage.org/webt/5q/qy/dp/5qqydpw85mv-p_b0hzock0cqi20.png" alt="Bild"><br><br>  Es sollte beachtet werden, dass wir mit diesem Ansatz tatsächlich die physikalischen Eigenschaften des optischen Systems beseitigen.  Dies ermöglicht es beispielsweise, ein Bild mit mehreren Tricks algorithmisch zu erstellen.  Oder ändern Sie programmgesteuert die Tiefe eines scharf dargestellten Raums (Schärfentiefe). <br><br><h2>  Andere Anwendungen </h2><br>  Nachfokussierung ist die Hauptanwendung, aber immer noch nicht die einzige.  Im Allgemeinen kann dieses Objektiv als Array virtueller Kameras (9 Stück) betrachtet werden.  Dementsprechend gilt es für alle Anwendungen, die Sie sich für eine Reihe von Kameras vorstellen können, zum Beispiel: <br><br><ul><li>  Polarisationsfilter - Jedes der 9 Bildelemente verfügt über ein eigenes Polarisationsfilter mit einer bestimmten Richtung.  Auf diese Weise können Sie 9 Bilder mit unterschiedlichen Polarisationen auf einmal erhalten und sogar eine Videoserie mit einer sanften Änderung der Polarisationsrichtung erstellen </li><li>  HDR (High-Dynamic-Range) - das gleiche Prinzip: 9 verschiedene Filter + Algorithmus für die optimale "Kombination" der Helligkeit </li><li>  Perspektivenwechsel </li><li>  Tiefenbasierte Bearbeitung - Ermöglicht das Anwenden verschiedener Filter auf verschiedene Tiefen.  Stellen Sie beispielsweise den Hintergrund in Schwarzweiß ein und markieren Sie den Vordergrund. </li><li>  Segmentierung - Auswahl von Objekten, die sich in einer bestimmten Entfernung befinden </li><li>  Entfernungsmessung - ein Lineal für Bilder.  Es funktioniert besonders gut für „flache“ Szenen, für die Disparität leichter zu berechnen ist. </li><li>  Anwendungen für die Industrie - verschiedene Möglichkeiten zur Bewertung der Produktionsqualität und -überwachung </li></ul><br><h2>  Fazit </h2><br>  Die Frage nach den endgültigen Kosten dieses Objektivs ist noch offen, einige physikalische Parameter wurden jedoch bereits ermittelt.  Es ist bekannt, dass die Länge 20 cm und die Masse 800 g nicht überschreiten sollte.  Es wird angegeben, dass dieses Gerät hauptsächlich mit Sony-, Canon- und Nikon-Kameras kompatibel sein wird. <br><br>  Außerhalb des Artikels blieben so wichtige Themen wie die praktische Verwendung von Standardkameras mit Suchern, die Wiederherstellung der Auflösung (Superauflösung), Verarbeitungsalgorithmen und die Integration in Grafikeditoren.  Ich werde das nächste Mal darüber sprechen. <br><br>  Danke für die Aufmerksamkeit! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de414877/">https://habr.com/ru/post/de414877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de414867/index.html">ThinkingHome.Migrator - versionierte Migration des Datenbankschemas auf der .NET Core-Plattform</a></li>
<li><a href="../de414869/index.html">Verschwindende Rahmenbedingungen</a></li>
<li><a href="../de414871/index.html">Der Staubsturm auf dem Mars erreichte das planetare Ausmaß, sogar die Neugier war betroffen</a></li>
<li><a href="../de414873/index.html">IDisposable - dass deine Mutter nicht über die Freigabe von Ressourcen gesprochen hat. Teil 1</a></li>
<li><a href="../de414875/index.html">Kubernetes Containerd-Integration ersetzt Docker für die Produktion</a></li>
<li><a href="../de414879/index.html">Warum 2 Extruder in einem 3D-Drucker?</a></li>
<li><a href="../de414881/index.html">Ein bisschen Backstage VK</a></li>
<li><a href="../de414883/index.html">Erinnerungen klangen neu: BBC hat das Soundarchiv des RemArc-Projekts aktualisiert</a></li>
<li><a href="../de414885/index.html">Wir beschäftigen uns mit Fehlern und „Krücken“ im einheitlichen staatlichen Register der juristischen Personen - dem staatlichen Register der juristischen Personen</a></li>
<li><a href="../de414887/index.html">Erstellen eines Katzenhakens in Unity. Teil 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>