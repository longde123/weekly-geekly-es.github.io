<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤°ğŸ¿ ğŸ‘†ğŸ» ğŸ« Apakah mungkin untuk melatih agen untuk perdagangan di pasar saham dengan bala bantuan? Implementasi bahasa R ğŸ“ ğŸ¤© ğŸ¤›ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mari kita buat prototipe reinforcement learning agent (RL) yang akan menguasai keterampilan perdagangan. 

 Mengingat bahwa implementasi prototipe ber...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Apakah mungkin untuk melatih agen untuk perdagangan di pasar saham dengan bala bantuan? Implementasi bahasa R</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433182/"> Mari kita buat prototipe reinforcement learning agent (RL) yang akan menguasai keterampilan perdagangan. <br><br>  Mengingat bahwa implementasi prototipe berfungsi dalam bahasa R, saya mendorong pengguna R dan programmer untuk mendekati ide-ide yang disajikan dalam artikel ini. <br><br>  Ini adalah terjemahan dari artikel bahasa Inggris saya: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dapatkah Reinforcement Learning Trade Stock?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Implementasi dalam R.</a> <br><br>  <i>Saya ingin memperingatkan para pemburu kode bahwa dalam catatan ini hanya ada kode untuk jaringan saraf yang diadaptasi untuk R.</i> <br><br>  Jika saya tidak membedakan diri saya dalam bahasa Rusia yang baik, tunjukkan kesalahan (teks disiapkan dengan bantuan penerjemah otomatis). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/97b/b88/e48/97bb88e4896efcd9f281cfb1b72830cf.png" alt="gambar"><br><a name="habracut"></a><br><h3>  Pengantar masalah </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/986/550/1f7/9865501f78bdeb575a3017a0c2466d54.png" alt="gambar"><br><br>  Saya menyarankan Anda untuk mulai menyelami topik ini dengan artikel ini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DeepMind</a> <br><br>  Dia akan memperkenalkan Anda dengan gagasan untuk menggunakan Deep Q-Network (DQN) untuk memperkirakan fungsi nilai yang sangat penting dalam proses pengambilan keputusan Markov. <br><br>  Saya juga merekomendasikan untuk masuk lebih jauh ke dalam matematika menggunakan cetakan buku ini oleh Richard S. Sutton dan Andrew J. Barto: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Reinforcement Learning</a> <br><br>  Di bawah ini saya akan menyajikan versi diperpanjang dari DQN asli, yang mencakup lebih banyak ide yang membantu algoritma secara cepat dan efisien menyatu, yaitu: <br><br>  <b>Deep Double Duel Noisy NN</b> dengan pemilihan prioritas dari buffer pemutaran pengalaman. <br><br>  Apa yang membuat pendekatan ini lebih baik daripada DQN klasik? <br><br><ul><li>  Ganda: ada dua jaringan, satu di antaranya dilatih, dan yang lainnya mengevaluasi nilai Q berikut </li><li>  Duel: Ada neuron yang jelas menghargai dan menguntungkan </li><li>  Bising: ada matriks kebisingan yang diterapkan pada bobot lapisan menengah, di mana rata-rata dan standar deviasi adalah bobot yang terlatih </li><li>  Prioritas pengambilan sampel: kumpulan pengamatan dari buffer pemutaran berisi contoh, yang karenanya pelatihan fungsi sebelumnya menghasilkan residu besar yang dapat disimpan dalam larik bantu. </li></ul><br><h4>  Nah, bagaimana dengan perdagangan yang dilakukan oleh agen DQN?  Ini adalah topik yang menarik. </h4><br>  Ada alasan mengapa ini menarik: <br><br><ul><li>  Kebebasan mutlak untuk memilih representasi status, tindakan, penghargaan, dan arsitektur NN.  Anda dapat memperkaya ruang masuk dengan segala sesuatu yang Anda anggap layak untuk dicoba, dari berita hingga saham dan indeks lainnya. </li><li>  Korespondensi dari logika perdagangan dengan logika pembelajaran penguatan adalah bahwa: agen melakukan tindakan diskrit (atau berkelanjutan), jarang dihargai (setelah transaksi ditutup atau periode berakhir), lingkungan dapat diamati sebagian dan mungkin berisi informasi tentang langkah-langkah selanjutnya, perdagangan adalah permainan episodik. </li><li>  Anda dapat membandingkan hasil DQN dengan beberapa tolok ukur, seperti indeks dan sistem perdagangan teknis. </li><li>  Agen dapat terus mempelajari informasi baru dan, dengan demikian, beradaptasi dengan aturan permainan yang berubah. </li></ul><br>  Agar tidak merentangkan materi, lihat kode NN ini, yang ingin saya bagikan, karena ini adalah salah satu bagian misterius dari seluruh proyek. <br><br><h4>  Kode R untuk jaringan neural nilai menggunakan Keras untuk membangun agen RL kami </h4><br><div class="spoiler">  <b class="spoiler_title">Kode</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># configure critic NN ------------ library('keras') library('R6') learning_rate &lt;- 1e-3 state_names_length &lt;- 12 # just for example a_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { x - k_mean(x, axis = 2, keepdims = T) } ) ) a_normalize_layer &lt;- function(object) { create_layer(a_CustomLayer, object, list(name = 'a_normalize_layer')) } v_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { k_concatenate(list(x, x, x), axis = 2) } , compute_output_shape = function(input_shape) { output_shape = input_shape output_shape[[2]] &lt;- input_shape[[2]] * 3L output_shape } ) ) v_normalize_layer &lt;- function(object) { create_layer(v_CustomLayer, object, list(name = 'v_normalize_layer')) } noise_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , lock_objects = FALSE , public = list( initialize = function(output_dim) { self$output_dim &lt;- output_dim } , build = function(input_shape) { self$input_dim &lt;- input_shape[[2]] sqr_inputs &lt;- self$input_dim ** (1/2) self$sigma_initializer &lt;- initializer_constant(.5 / sqr_inputs) self$mu_initializer &lt;- initializer_random_uniform(minval = (-1 / sqr_inputs), maxval = (1 / sqr_inputs)) self$mu_weight &lt;- self$add_weight( name = 'mu_weight', shape = list(self$input_dim, self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_weight &lt;- self$add_weight( name = 'sigma_weight', shape = list(self$input_dim, self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) self$mu_bias &lt;- self$add_weight( name = 'mu_bias', shape = list(self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_bias &lt;- self$add_weight( name = 'sigma_bias', shape = list(self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) } , call = function(x, mask = NULL) { #sample from noise distribution e_i = k_random_normal(shape = list(self$input_dim, self$output_dim)) e_j = k_random_normal(shape = list(self$output_dim)) #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al. eW = k_sign(e_i) * (k_sqrt(k_abs(e_i))) * k_sign(e_j) * (k_sqrt(k_abs(e_j))) eB = k_sign(e_j) * (k_abs(e_j) ** (1/2)) #See section 3 of Fortunato et al. noise_injected_weights = k_dot(x, self$mu_weight + (self$sigma_weight * eW)) noise_injected_bias = self$mu_bias + (self$sigma_bias * eB) output = k_bias_add(noise_injected_weights, noise_injected_bias) output } , compute_output_shape = function(input_shape) { output_shape &lt;- input_shape output_shape[[2]] &lt;- self$output_dim output_shape } ) ) noise_add_layer &lt;- function(object, output_dim) { create_layer( noise_CustomLayer , object , list( name = 'noise_add_layer' , output_dim = as.integer(output_dim) , trainable = T ) ) } critic_input &lt;- layer_input( shape = c(as.integer(state_names_length)) , name = 'critic_input' ) common_layer_dense_1 &lt;- layer_dense( units = 20 , activation = "tanh" ) critic_layer_dense_v_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) critic_layer_dense_v_2 &lt;- layer_dense( units = 5 , activation = "tanh" ) critic_layer_dense_v_3 &lt;- layer_dense( units = 1 , name = 'critic_layer_dense_v_3' ) critic_layer_dense_a_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) # critic_layer_dense_a_2 &lt;- layer_dense( # units = 5 # , activation = "tanh" # ) critic_layer_dense_a_3 &lt;- layer_dense( units = length(acts) , name = 'critic_layer_dense_a_3' ) critic_model_v &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_v_1 %&gt;% critic_layer_dense_v_2 %&gt;% critic_layer_dense_v_3 %&gt;% v_normalize_layer critic_model_a &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_a_1 %&gt;% #critic_layer_dense_a_2 %&gt;% noise_add_layer(output_dim = 5) %&gt;% critic_layer_dense_a_3 %&gt;% a_normalize_layer critic_output &lt;- layer_add( list( critic_model_v , critic_model_a ) , name = 'critic_output' ) critic_model_1 &lt;- keras_model( inputs = critic_input , outputs = critic_output ) critic_optimizer = optimizer_adam(lr = learning_rate) keras::compile( critic_model_1 , optimizer = critic_optimizer , loss = 'mse' , metrics = 'mse' ) train.x &lt;- rnorm(state_names_length * 10) train.x &lt;- array(train.x, dim = c(10, state_names_length)) predict(critic_model_1, train.x) layer_name &lt;- 'noise_add_layer' intermediate_layer_model &lt;- keras_model(inputs = critic_model_1$input, outputs = get_layer(critic_model_1, layer_name)$output) predict(intermediate_layer_model, train.x)[1,] critic_model_2 &lt;- critic_model_1</span></span></code> </pre> <br></div></div><br>  Saya menggunakan sumber ini untuk mengadaptasi kode Python untuk bagian derau jaringan: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">github repo</a> <br><br>  Jaringan saraf ini terlihat seperti ini: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dc/20c/e1e/3dc20ce1eedec47b871ef59ab7911ef3.png" alt="gambar"><br><br>  Ingatlah bahwa dalam arsitektur duel kita menggunakan persamaan (persamaan 1): <br><br>  Q = A '+ V, di mana <br><br>  A '= A - rata-rata (A); <br><br>  Q = nilai tindakan negara; <br><br>  V = nilai negara; <br><br>  A = keuntungan. <br><br>  Variabel lain dalam kode berbicara sendiri.  Selain itu, arsitektur ini hanya baik untuk tugas tertentu, jadi jangan anggap remeh. <br><br>  Sisa kode kemungkinan besar akan cukup umum untuk dipublikasikan, dan akan menarik bagi programmer untuk menulisnya sendiri. <br><br>  Dan sekarang - percobaan.  Pengujian pekerjaan agen dilakukan di kotak pasir, jauh dari kenyataan perdagangan di pasar langsung, dengan broker nyata. <br><br><h3>  Fase I </h3><br>  Kami menjalankan agen kami terhadap dataset sintetis.  Biaya transaksi kami adalah 0,5: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb2/f15/3b6/fb2f153b6c4e14ff1dc19d61a524f5e8.png" alt="gambar"><br><br>  Hasilnya luar biasa.  Rata-rata imbalan episodik maksimum dalam percobaan ini <br>  harus 1,5. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/417/0a0/a84/4170a0a842560a2a2fd3112d601dba54.jpg" alt="gambar"><br><br>  Kita melihat: kehilangan kritik (yang disebut jaringan nilai dalam pendekatan aktor-kritik), hadiah rata-rata untuk sebuah episode, akumulasi hadiah, contoh hadiah baru-baru ini. <br><br><h3>  Fase II </h3><br>  Kami mengajari agen kami simbol saham yang dipilih secara sewenang-wenang yang menunjukkan perilaku menarik: awal yang datar, pertumbuhan yang cepat di tengah dan akhir yang suram.  Dalam kit pelatihan kami sekitar 4300 hari.  Biaya transaksi ditetapkan pada 0,1 dolar AS (sengaja rendah);  Hadiahnya adalah Laba / Rugi USD setelah menutup kesepakatan untuk membeli / menjual 1,0 saham. <br><br>  Sumber: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">finance.yahoo.com/quote/algn?ltr=1</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6d/22c/540/d6d22c540514afc7261f549bea8c74ac.png" alt="gambar"><br><br>  <i>NASDAQ: ALGN</i> <br><br>  Setelah menetapkan beberapa parameter (membiarkan arsitektur NN tetap sama), kami sampai pada hasil berikut: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/612/008/50c/61200850c8a8c0b7963bbee12ca1a2bd.jpg" alt="gambar"><br><br>  Ternyata tidak buruk, karena pada akhirnya agen belajar untuk mendapat untung dengan menekan tiga tombol di konsolnya. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8d/d86/afb/d8dd86afb84b4f4625012a5c3ba9dc4e.png" alt="gambar"><br><br>  <i>spidol merah = jual, spidol hijau = beli, spidol abu-abu = tidak melakukan apa-apa.</i> <br><br>  Harap dicatat bahwa pada puncaknya, hadiah rata-rata per episode melebihi nilai transaksi realistis yang dapat ditemui dalam perdagangan nyata. <br><br>  Sangat disayangkan bahwa saham jatuh seperti orang gila karena berita buruk ... <br><br><h3>  Pengamatan penutup </h3><br>  Berdagang dengan RL tidak hanya sulit, tetapi juga bermanfaat.  Ketika robot Anda melakukannya lebih baik daripada Anda, saatnya untuk menghabiskan waktu pribadi Anda untuk mendapatkan pendidikan dan kesehatan. <br><br>  Saya harap ini adalah perjalanan yang menarik untuk Anda.  Jika Anda menyukai cerita ini, lambaikan tangan.  Jika ada banyak minat, saya dapat melanjutkan dan menunjukkan kepada Anda bagaimana metode gradien kebijakan bekerja menggunakan bahasa R dan API Keras. <br><br>  Saya juga ingin mengucapkan terima kasih kepada teman-teman saya yang tertarik pada jaringan saraf untuk saran mereka. <br><br>  Jika Anda masih memiliki pertanyaan, saya selalu di sini. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id433182/">https://habr.com/ru/post/id433182/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id433172/index.html">Kami kencangkan multipemain ke game seluler "Buat kata-kata dari kata-kata" di iOS dan Android, ditulis dalam C ++</a></li>
<li><a href="../id433174/index.html">Tidak semua tambalan sama-sama bermanfaat.</a></li>
<li><a href="../id433176/index.html">Certificate Remote Authentication Docker Remote dengan verifikasi pencabutan</a></li>
<li><a href="../id433178/index.html">Bagaimana kami memulihkan file .wav yang rusak</a></li>
<li><a href="../id433180/index.html">Memecahkan masalah tipe data di Ruby atau Membuat data kembali dapat diandalkan</a></li>
<li><a href="../id433184/index.html">ASP.NET Core 2.2 dirilis. Apa yang baru? (2 dari 3)</a></li>
<li><a href="../id433186/index.html">Tidak cukup hanya dengan menghitung poligon untuk mengoptimalkan model 3D</a></li>
<li><a href="../id433188/index.html">Negara Duma memperkenalkan RUU tentang pekerjaan otonom Runet</a></li>
<li><a href="../id433192/index.html">Kubernetes: Solusi Proyek Pribadi yang Luar Biasa Terjangkau</a></li>
<li><a href="../id433194/index.html">Lampu malam yang dijadwalkan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>