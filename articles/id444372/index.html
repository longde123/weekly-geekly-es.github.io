<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸšµğŸ¿ ğŸ‘ˆğŸ¼ ğŸ™†ğŸ¼ Orientasi mesin jarak jauh menggunakan pembelajaran yang diperkuat ğŸ‘¨ğŸ¾ ğŸš ğŸ‘¤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Di Amerika Serikat saja, ada 3 juta orang cacat yang tidak bisa meninggalkan rumah mereka. Robot penolong yang dapat menavigasi jarak jauh secara otom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Orientasi mesin jarak jauh menggunakan pembelajaran yang diperkuat</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444372/">  Di Amerika Serikat saja, ada 3 juta orang cacat yang tidak bisa meninggalkan rumah mereka.  Robot penolong yang dapat menavigasi jarak jauh secara otomatis dapat membuat orang-orang ini lebih mandiri dengan membawa makanan, obat-obatan, dan paket.  Studi menunjukkan bahwa pembelajaran mendalam dengan penguatan (OP) sangat cocok untuk membandingkan input data mentah dan tindakan, misalnya, untuk belajar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menangkap objek</a> atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">memindahkan robot</a> , tetapi biasanya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">agen</a> OP kurang memahami ruang fisik besar yang diperlukan untuk orientasi yang aman ke jarak jauh jarak tanpa bantuan manusia dan adaptasi ke lingkungan baru. <br><a name="habracut"></a><br>  Dalam tiga karya terbaru, â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pelatihan Orienteering dari awal dengan AOP</a> ,â€ â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PRM-RL: Menerapkan orientasi robot dari jarak jauh menggunakan kombinasi pembelajaran penguatan dan perencanaan berbasis pola</a> â€ dan â€œ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Orientasi</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">jarak jauh</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dengan PRM-RL,</a> â€ kami Kami mempelajari robot otonom yang mudah beradaptasi dengan lingkungan baru, menggabungkan OP dalam dengan perencanaan jangka panjang.  Kami mengajarkan agen perencana lokal bagaimana melakukan tindakan dasar yang diperlukan untuk orientasi dan cara bergerak jarak pendek tanpa tabrakan dengan benda bergerak.  Perencana lokal melakukan pengamatan lingkungan yang bising menggunakan sensor seperti lidar satu dimensi yang menyediakan jarak ke hambatan dan memberikan kecepatan linier dan sudut untuk mengendalikan robot.  Kami melatih perencana lokal dalam simulasi menggunakan pembelajaran penguatan otomatis (AOP), sebuah metode yang mengotomatiskan pencarian hadiah untuk OP dan arsitektur jaringan saraf.  Meskipun jangkauan terbatas antara 10-15 m, perencana lokal beradaptasi dengan baik untuk digunakan pada robot asli dan untuk lingkungan baru yang sebelumnya tidak dikenal.  Ini memungkinkan Anda untuk menggunakannya sebagai blok bangunan untuk orientasi pada ruang besar.  Kemudian kita membangun peta jalan, grafik di mana simpul-simpul itu merupakan bagian yang terpisah, dan ujung-ujungnya menghubungkan simpul hanya jika perencana lokal, yang dengan baik meniru robot asli menggunakan sensor dan kontrol yang bising, dapat bergerak di antara mereka. <br><br><h2>  Pembelajaran penguatan otomatis (AOP) </h2><br>  Dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pekerjaan pertama kami,</a> kami melatih perencana lokal di lingkungan statis kecil.  Namun, ketika belajar dengan algoritma OP dalam standar, misalnya, gradien deterministik mendalam ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DDPG</a> ), ada beberapa kendala.  Sebagai contoh, tujuan nyata perencana lokal adalah untuk mencapai tujuan yang diberikan, sebagai akibatnya mereka menerima hadiah langka.  Dalam praktiknya, ini menuntut para peneliti untuk menghabiskan banyak waktu pada implementasi algoritma dan penyesuaian penghargaan secara bertahap.  Para peneliti juga harus membuat keputusan tentang arsitektur jaringan saraf tanpa memiliki resep yang jelas dan sukses.  Akhirnya, algoritma seperti DDPG belajar dengan tidak stabil dan sering menunjukkan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pelupa bencana</a> . <br><br>  Untuk mengatasi kendala ini, kami mengotomatisasi pembelajaran mendalam dengan penguatan.  AOP adalah pembungkus otomatis evolusioner di sekitar OP yang dalam, mencari imbalan dan arsitektur jaringan saraf melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">optimisasi hyperparameter skala besar</a> .  Ia bekerja dalam dua tahap, pencarian penghargaan dan pencarian arsitektur.  Selama pencarian hadiah, AOP telah secara bersamaan melatih populasi agen DDPG selama beberapa generasi, masing-masing memiliki fungsi hadiah yang sedikit berubah, dioptimalkan untuk tugas sebenarnya dari perencana lokal: mencapai titik akhir jalan.  Di akhir fase pencarian hadiah, kami memilih satu yang paling sering mengarahkan agen ke tujuan.  Dalam fase pencarian arsitektur jaringan saraf, kami mengulangi proses ini, untuk balapan ini menggunakan penghargaan yang dipilih dan menyesuaikan lapisan jaringan, mengoptimalkan penghargaan kumulatif. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP dengan pencarian penghargaan dan arsitektur jaringan saraf</i> <br><br>  Namun, proses ini selangkah demi selangkah membuat AOP tidak efektif dalam hal jumlah sampel.  Pelatihan AOP dengan 10 generasi 100 agen membutuhkan 5 miliar sampel, setara dengan 32 tahun studi!  Keuntungannya adalah bahwa setelah AOP, proses pembelajaran manual terotomatisasi, dan DDPG tidak memiliki bencana lupa.  Yang paling penting, kualitas kebijakan akhir lebih tinggi - mereka tahan terhadap kebisingan dari sensor, drive, dan pelokalan, dan digeneralisasikan dengan baik ke lingkungan baru.  Kebijakan terbaik kami adalah 26% lebih berhasil daripada metode orientasi lainnya di lokasi pengujian kami. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Merah - AOP berhasil pada jarak pendek (hingga 10 m) di beberapa bangunan yang sebelumnya tidak dikenal.</i>  <i>Perbandingan dengan DDPG yang dilatih secara manual (merah tua), bidang potensial buatan (biru), jendela dinamis (biru) dan kloning perilaku (hijau).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Kebijakan penjadwal AOP lokal berfungsi baik dengan robot di lingkungan nyata yang tidak terstruktur</i> <br><br>  Dan meskipun politisi ini hanya mampu orientasi lokal, mereka tahan terhadap rintangan yang bergerak dan ditoleransi dengan baik oleh robot nyata di lingkungan yang tidak terstruktur.  Dan meskipun mereka dilatih dalam simulasi dengan objek statis, mereka secara efektif mengatasi yang bergerak.  Langkah selanjutnya adalah menggabungkan kebijakan AOP dengan perencanaan berbasis sampel untuk memperluas area kerja mereka dan mengajari mereka cara menavigasi jarak jauh. <br><br><h2>  Orientasi jarak jauh dengan PRM-RL </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Perencana berbasis pola</a> bekerja dengan orientasi jarak jauh, mendekati gerakan robot.  Sebagai contoh, robot membangun <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">roadmap probabilistic</a> (PRMs) dengan menggambar jalur transisi antar bagian.  Dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">karya kedua</a> kami, yang memenangkan penghargaan pada konferensi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ICRA 2018</a> , kami menggabungkan PRM dengan penjadwal OP lokal yang disetel secara manual (tanpa AOP) untuk melatih robot secara lokal dan kemudian menyesuaikannya dengan lingkungan lain. <br><br>  Pertama, untuk setiap robot, kami melatih kebijakan penjadwal lokal dalam simulasi umum.  Kemudian kami membuat PRM dengan mempertimbangkan kebijakan ini, yang disebut PRM-RL, berdasarkan peta lingkungan di mana ia akan digunakan.  Kartu yang sama dapat digunakan untuk robot apa pun yang ingin kita gunakan di gedung. <br><br>  Untuk membuat PRM-RL, kami menggabungkan node dari sampel hanya jika OP-scheduler lokal dapat dipercaya dan berulang kali berpindah di antara mereka.  Ini dilakukan dalam simulasi Monte Carlo.  Peta yang dihasilkan menyesuaikan dengan kemampuan dan geometri robot tertentu.  Kartu untuk robot dengan geometri yang sama, tetapi dengan sensor dan drive yang berbeda, akan memiliki konektivitas yang berbeda.  Karena agen dapat berputar di sudut, node yang tidak berhadapan langsung juga dapat dihidupkan.  Namun, simpul-simpul yang berdekatan dengan dinding dan rintangan akan lebih kecil kemungkinannya untuk dimasukkan dalam peta karena kebisingan sensor.  Pada waktu berjalan, agen OP bergerak melintasi peta dari satu bagian ke bagian lain. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>Peta dibuat dengan tiga simulasi Monte Carlo untuk setiap pasangan node yang dipilih secara acak</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>Peta terbesar berukuran 288x163 m dan berisi hampir 700.000 tepi.</i>  <i>300 pekerja mengumpulkannya selama 4 hari, setelah melakukan 1,1 miliar cek tabrakan.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Karya ketiga</a> menyediakan beberapa perbaikan pada PRM-RL asli.  Pertama, kami mengganti DDPG yang disetel secara manual dengan penjadwal AOP lokal, yang memberikan peningkatan orientasi jarak jauh.  Kedua, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">peta lokalisasi dan penandaan simultan</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SLAM</a> ) ditambahkan, yang digunakan robot saat runtime sebagai sumber untuk membangun peta jalan.  Kartu SLAM tunduk pada kebisingan, dan ini menutup "celah antara simulator dan kenyataan", masalah yang terkenal dalam robotika, karena agen yang dilatih dalam simulasi berperilaku jauh lebih buruk di dunia nyata.  Tingkat kesuksesan kami dalam simulasi bertepatan dengan tingkat keberhasilan robot nyata.  Dan akhirnya, kami menambahkan peta bangunan terdistribusi, sehingga kami dapat membuat peta sangat besar yang berisi hingga 700.000 node. <br><br>  Kami mengevaluasi metode ini dengan bantuan agen AOP kami, yang membuat peta berdasarkan gambar bangunan yang melebihi lingkungan pelatihan sebanyak 200 kali di daerah tersebut, termasuk hanya tulang rusuk, yang berhasil diselesaikan dalam 90% kasus dalam 20 upaya.  Kami membandingkan PRM-RL dengan berbagai metode pada jarak hingga 100 m, yang secara signifikan melebihi kisaran perencana lokal.  PRM-RL mencapai keberhasilan 2-3 kali lebih sering daripada metode konvensional karena koneksi node yang benar, cocok untuk kemampuan robot. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>Tingkat keberhasilan dalam memindahkan 100 m di berbagai bangunan.</i>  <i>Biru - penjadwal AOP lokal, pekerjaan pertama;</i>  <i>PRM merah asli;</i>  <i>kuning - bidang potensial buatan;</i>  <i>hijau adalah pekerjaan kedua;</i>  <i>merah - pekerjaan ketiga, PRM dengan AOP.</i> <br><br>  Kami menguji PRM-RL pada banyak robot nyata di banyak bangunan.  Di bawah ini adalah salah satu suite tes;  robot andal bergerak hampir ke mana-mana, kecuali untuk tempat dan area paling berantakan yang melampaui kartu SLAM. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Kesimpulan </h2><br>  Orientasi alat berat dapat secara serius meningkatkan kemandirian orang dengan gangguan mobilitas.  Ini dapat dicapai dengan mengembangkan robot otonom yang dapat dengan mudah beradaptasi dengan lingkungan, dan metode yang tersedia untuk implementasi di lingkungan baru berdasarkan informasi yang ada.  Ini dapat dilakukan dengan mengotomatisasi pelatihan orientasi dasar untuk jarak pendek dengan AOP, dan kemudian menggunakan keterampilan yang diperoleh bersama dengan kartu SLAM untuk membuat peta jalan.  Peta jalan terdiri dari simpul-simpul yang dihubungkan oleh tulang rusuk, tempat robot dapat bergerak dengan andal.  Akibatnya, kebijakan perilaku robot dikembangkan yang, setelah satu pelatihan, dapat digunakan di lingkungan yang berbeda dan mengeluarkan peta jalan yang secara khusus disesuaikan untuk robot tertentu. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id444372/">https://habr.com/ru/post/id444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id444362/index.html">Unity dan Havok sedang mengerjakan mesin fisika baru</a></li>
<li><a href="../id444364/index.html">24 jam permainan Rust: pengalaman pengembangan pribadi</a></li>
<li><a href="../id444366/index.html">Seminar "Persyaratan Keamanan Informasi: Bagaimana Bisnis Bisa Hidup Bersama Mereka"</a></li>
<li><a href="../id444368/index.html">Kami baru saja mencetak mikrofon pada printer 3D di laboratorium - dan kemudian akan ada fiksi ilmiah lengkap</a></li>
<li><a href="../id444370/index.html">Apa yang mampu dilakukan oleh format Mini PCI-e?</a></li>
<li><a href="../id444374/index.html">Efek hipster: mengapa orang yang tidak patuh sering terlihat sama</a></li>
<li><a href="../id444376/index.html">Ekonomi perhatian hampir mati</a></li>
<li><a href="../id444378/index.html">USPACE - Ruang Tunggal untuk Pesawat Berawak dan Tidak Berawak</a></li>
<li><a href="../id444382/index.html">Bagaimana cara mengunjungi Universitas Korea dengan Sistem File Jaringan</a></li>
<li><a href="../id444384/index.html">Buku "Analisis Data Teks Terapan dengan Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>