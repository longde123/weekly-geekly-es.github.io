<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õπüèª üç® üëè Clusters Kubernetes dans le service VPC üö£üèª üëéüèª üìö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous avons ajout√© la possibilit√© de lancer facilement Kubernetes dans le service Virtual Private Cloud en mode de test b√™ta pr√©coce. 


 Cette fonctio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Clusters Kubernetes dans le service VPC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/416951/"><img src="https://habrastorage.org/webt/bc/b6/cy/bcb6cykv49fwewsnhfr-7ny-0uc.png"><br><p><br>  Nous avons ajout√© la possibilit√© de lancer facilement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">Kubernetes</a> dans le service <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">Virtual Private Cloud</a> en mode de test b√™ta pr√©coce. </p><br><p>  Cette fonctionnalit√© sera utile aux utilisateurs qui ont besoin d'une gestion pratique d'un grand nombre d'applications ex√©cut√©es en tant que conteneurs.  Kubernetes propose des outils de mise √† l'√©chelle, d'autor√©paration et d'√©quilibrage de charge pour les conteneurs ex√©cut√©s √† l'int√©rieur d'un cluster. </p><br><p>  √âtant donn√© que le service <strong>Virtual Private Cloud</strong> est bas√© sur OpenStack, nous utilisons l'un de ses composants - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">OpenStack Magnum</a> .  Il vous permet de cr√©er rapidement des clusters Kubernetes priv√©s avec le nombre de n≈ìuds souhait√©. </p><br><p>  Actuellement, tout utilisateur de notre service peut cr√©er plusieurs clusters ind√©pendants dans son projet.  En tant que n≈ìuds de cluster, des machines virtuelles seront utilis√©es, dont la configuration peut √™tre s√©lectionn√©e et modifi√©e. </p><br><p>  Dans cet article, nous parlerons des principaux objets du cluster Kubernetes et utiliserons les exemples pour examiner le processus de cr√©ation d'un cluster √† l'aide d'OpenStack Magnum. </p><a name="habracut"></a><br><h2>  Cr√©er et g√©rer un cluster Kubernetes </h2><br><p>  Actuellement, la cr√©ation d'un cluster Kubernetes n'est possible que via les utilitaires de console ou l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">API OpenStack</a> dans les zones de disponibilit√© <strong>ru-1a</strong> et <strong>ru-1b</strong> (Saint-P√©tersbourg). </p><br><p>  Pour commencer, vous aurez besoin de: </p><br><ul><li>  Cr√©er un nouveau ou utiliser un projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">VPC</a> existant; </li><li>  Cr√©ez un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">utilisateur avec une cl√© SSH</a> ; </li><li>  Ajoutez un utilisateur au projet cr√©√© sur la page de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">gestion de</a> projet; </li><li>  Acc√©dez au projet et r√©cup√©rez le fichier d'acc√®s dans l'onglet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">Acc√®s</a> ; </li><li>  Installez le client de console <strong>openstack</strong> avec la <strong>biblioth√®que python-magnumclient</strong> ; </li><li>  Installez le client de console <strong>kubectl</strong> . </li></ul><br><p>  Pour installer le client de console <strong>openstack</strong> , <strong>vous</strong> pouvez utiliser les instructions <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">sur le lien</a> , cependant, il convient de garder √† l'esprit que pour ce client, vous devrez √©galement installer la biblioth√®que <strong>python-magnumclient</strong> pour prendre en charge la cr√©ation de clusters Kubernetes. </p><br><p>  L'ensemble complet de commandes pour installer un client openstack avec le plug-in requis pour les syst√®mes d'exploitation de la famille Ubuntu / Debian: </p><br><pre><code class="bash hljs">$ sudo apt update $ sudo apt -y install curl python-pip python-dev python3-dev git libxml2-dev libxslt1-dev python-openssl python3-openssl python-pyasn1 libffi-dev libssl-dev build-essential $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  L'ensemble complet de commandes pour installer un client openstack avec le plug-in requis pour les syst√®mes d'exploitation de la famille Fedora / CentOS: </p><br><pre> <code class="bash hljs">$ sudo yum -y install python-pip gcc libffi-devel python-devel libxslt-devel openssl-devel git libffi-devel $ sudo pip install -UI pbr setuptools pytz $ sudo pip install -UI git+https://github.com/openstack/python-openstackclient $ sudo pip install -UI git+https://github.com/openstack/python-magnumclient</code> </pre> <br><p>  Pour g√©rer les objets Kubernetes, vous avez besoin du client de console <strong>kubectl</strong> .  Les m√©thodes d'installation de divers syst√®mes d'exploitation sont d√©crites dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">la documentation officielle</a> . </p><br><p>  Pour cr√©er un cluster, vous devrez cr√©er ou utiliser ceux existants: </p><br><ul><li>  Mod√®le de cluster; </li><li>  Un ensemble de param√®tres pour le CPU et la RAM des machines virtuelles (version). </li></ul><br><p>  Vous pouvez cr√©er un mod√®le de cluster et un ar√¥me vous-m√™me, ou utiliser des mod√®les publics pr√©-cr√©√©s. </p><br><p>  Vous devrez √©galement d√©terminer la zone de disponibilit√©, le type de disques pour votre cluster et le nombre de n≈ìuds.  Il convient de noter que nous ne prenons pas encore en charge la possibilit√© de cr√©er un cluster dans plusieurs zones.  Vous pouvez choisir n'importe quel type de lecteur r√©seau (rapide, universel ou basique). <br>  Vous pouvez en savoir plus sur les types de lecteurs dans notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener noreferrer">base de connaissances</a> . </p><br><p>  Le nombre de n≈ìuds peut √™tre diff√©rent pour les r√¥les <strong>ma√Ætre</strong> et pour les <strong>sbires</strong> .  Sur les n≈ìuds jouant le r√¥le de ma√Ætre, les √©l√©ments de contr√¥le du cluster seront lanc√©s - <strong>contr√¥leur-gestionnaire</strong> , <strong>ordonnanceur</strong> , <strong>api</strong> .  Sur les autres n≈ìuds, les services <strong>kubelet</strong> , <strong>kube-proxy</strong> et tous les conteneurs d'application seront lanc√©s.  Vous pouvez en savoir plus sur les composants qui s'ex√©cutent sur les n≈ìuds de cluster √† partir de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><p>  Pour acc√©der aux n≈ìuds via SSH, vous devrez utiliser la cl√© SSH cr√©√©e pr√©c√©demment.  Les exemples de commandes utiliseront une cl√© appel√©e <strong>ssh-test</strong> . </p><br><p>  Nous utiliserons le mod√®le et la saveur de cluster public, le type de disque rapide et la zone de disponibilit√© <strong>ru-1b</strong> . <br>  Dans notre cluster, 2 n≈ìuds ma√Ætres et 3 n≈ìuds sbires seront initialement lanc√©s. </p><br><p>  Pour v√©rifier ces param√®tres, nous utilisons les commandes openstackclient et le fichier d'acc√®s t√©l√©charg√© ( <strong>rc.sh</strong> ): </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#          . $ source rc.sh #  ,         $ openstack flavor show BL1.2-4096 -c ram -c vcpus +-------+-------+ | Field | Value | +-------+-------+ | ram | 4096 | | vcpus | 2 | +-------+-------+ #       ru-1b $ openstack volume type show fast.ru-1b -c name +-------+------------+ | Field | Value | +-------+------------+ | name | fast.ru-1b | +-------+------------+ #    Kubernetes $ openstack coe cluster template list -c name +---------------------------------------+ | name | +---------------------------------------+ | kubernetes-nofloatingips-ru-1b-v1.9.3 | | kubernetes-nofloatingips-ru-1b-v1.9.6 | | kubernetes-nofloatingips-ru-1b-v1.9.9 | | kubernetes-floatingips-ru-1b-v1.9.3 | | kubernetes-floatingips-ru-1b-v1.9.6 | | kubernetes-floatingips-ru-1b-v1.9.9 | | kubernetes-nofloatingips-ru-1a-v1.9.3 | | kubernetes-nofloatingips-ru-1a-v1.9.6 | | kubernetes-nofloatingips-ru-1a-v1.9.9 | | kubernetes-floatingips-ru-1a-v1.9.3 | | kubernetes-floatingips-ru-1a-v1.9.6 | | kubernetes-floatingips-ru-1a-v1.9.9 | +---------------------------------------+</span></span></code> </pre> <br><p>  Par exemple, nous choisirons le deuxi√®me mod√®le de cluster; les adresses flottantes accessibles au public pour chacun des n≈ìuds ne seront pas cr√©√©es √† partir de celui-ci.  Nous n'en aurons pas besoin. </p><br><pre> <code class="plaintext hljs">#   Kubernetes   test-cluster #   keypair   ,   $ openstack coe cluster create \ --cluster-template kubernetes-nofloatingips-ru-1b-v1.9.9 \ --master-count 2 \ --node-count 3 \ --keypair ssh-test \ --master-flavor BL1.2-4096 \ --flavor BL1.2-4096 \ test-cluster</code> </pre> <br><p>  <em>Veuillez noter que nous avons choisi la m√™me configuration pour diff√©rents n≈ìuds (ma√Ætre-saveur et param√®tres de saveur), vous pouvez choisir diff√©rents jeux de configuration en fonction des exigences du cluster.</em>  <em>Leur changement est possible apr√®s sa cr√©ation.</em> </p><br><p>  Il convient √©galement de noter que lors de la cr√©ation d'un cluster avec plusieurs n≈ìuds ma√Ætres, un √©quilibreur de charge sera automatiquement cr√©√© pour acc√©der √† l'API Kubernetes. </p><br><p>  Apr√®s quelques minutes, un cluster Kubernetes appara√Ætra dans votre projet.  Dans le panneau de configuration du projet, vous verrez de nouvelles machines virtuelles, disques et objets r√©seau. </p><br><p>  Vous pouvez v√©rifier l'√©tat de votre cluster via openstackclient: </p><br><pre> <code class="bash hljs">openstack coe cluster list -c name -c status +--------------+--------------------+ | name | status | +--------------+--------------------+ | <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster | CREATE_IN_PROGRESS | +--------------+--------------------+</code> </pre> <br><p>  Une fois que le cluster est entr√© dans l'√©tat CREATE_COMPLETE, vous pouvez g√©rer ses objets via l'utilitaire kubectl en t√©l√©chargeant le fichier de configuration √† l'aide des commandes suivantes: </p><br><pre> <code class="bash hljs">$ mkdir -p ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster $ openstack coe cluster config <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster --dir ~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster</code> </pre> <br><p>  Apr√®s cela, vous pouvez travailler avec le cluster √† l'aide de l'utilitaire kubectl: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=~/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster/config $ kubectl get pods --all-namespaces -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase NAME STATUS coredns-785dcf9c58-6gnfp Running heapster-6846cdc674-rm4k6 Running kube-dns-autoscaler-6b94f7bbf8-x5clt Running kubernetes-dashboard-747575c864-wlg6p Running monitoring-grafana-84b4596dd7-zf5rx Running monitoring-influxdb-c8486fc95-bqqb6 Running node-exporter-test-cluster-robvp4cvwpt7-minion-0 Running</code> </pre> <br><p>  Si n√©cessaire, vous pouvez augmenter ou diminuer le nombre de n≈ìuds Minion dans le cluster via openstackclient en transmettant la nouvelle valeur node_count: </p><br><pre> <code class="bash hljs">$ openstack coe cluster update <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster replace node_count=4</code> </pre> <br><h2>  Objets cl√©s du cluster Kubernetes </h2><br><h3>  Pods </h3><br><p>  Bien que Kubernetes g√®re un ensemble de conteneurs, l'entit√© sous-jacente que Kubernetes g√®re n'est pas un conteneur, mais un <strong>pod</strong> . </p><br><p>  Pod est un ensemble d'espaces de noms de noyau Linux et de param√®tres de pile r√©seau qui vous permettent d'assembler un ensemble de conteneurs en une seule entit√©. <br>  Le plus souvent, un conteneur avec l'application est lanc√© dans un pod distinct. <br>  Si n√©cessaire, vous pouvez ex√©cuter plusieurs conteneurs √† l'int√©rieur d'un pod, cela peut √™tre utile lorsque vous devez fournir un acc√®s d'un conteneur √† un autre via l'interface r√©seau localhost, ou pour une autre raison, ex√©cuter plusieurs conteneurs sur le m√™me h√¥te. <br>  Tous les conteneurs ex√©cut√©s dans le m√™me pod auront un nom d'h√¥te, une adresse IP, une table de routage et des disques. </p><br><p>  Il convient de noter que lors de la mise √† l'√©chelle du nombre d'instances de votre application dans Kubernetes, il est n√©cessaire d'augmenter le nombre de pods, et non le nombre de conteneurs dans un pod sp√©cifique. <br>  Plus de d√©tails dans la documentation officielle des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">Pods</a> . </p><br><p>  Par exemple, cr√©ez le pod le plus simple avec Nginx en utilisant la description au format yaml: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-basic.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Pour cr√©er un pod, nous pouvons utiliser l'utilitaire <strong>kubectl</strong> . <br>  Nous avons ajout√© tous les exemples pr√©sent√©s dans l'article √† notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">groupe Github</a> , vous ne pouvez donc pas cr√©er de fichiers sur votre ordinateur, mais utilisez l'url du fichier √† partir du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">r√©f√©rentiel</a> public: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-basic.yaml</code> </pre> <br><p>  Apr√®s la cr√©ation, nous pouvons demander des informations compl√®tes sur l'√©tat du pod √† l'aide de la commande kubectl describe: </p><br><pre> <code class="bash hljs">$ kubectl describe pod nginx Name: nginx Namespace: default Node: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0/10.0.0.5 Start Time: Sun, 17 Jun 2018 12:29:03 +0000 Labels: &lt;none&gt; Annotations: &lt;none&gt; Status: Running IP: 10.100.88.9 Containers: nginx: Container ID: docker://6ca6383b66686c05c61c1f690737110e0f8994eda393f44a7ebfbbf2b2026267 Image: library/nginx:1.14-alpine Image ID: docker-pullable://docker.io/nginx@sha256:944b79ca7dbe456ce72e73e70816c1990e39967c8f010349a388c00b77ec519c Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 17 Jun 2018 12:29:16 +0000 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rp5ls (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-rp5ls: Type: Secret (a volume populated by a Secret) SecretName: default-token-rp5ls Optional: <span class="hljs-literal"><span class="hljs-literal">false</span></span> QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: &lt;none&gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 52s default-scheduler Successfully assigned nginx to <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Normal SuccessfulMountVolume 51s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 MountVolume.SetUp succeeded <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> volume <span class="hljs-string"><span class="hljs-string">"default-token-rp5ls"</span></span> Normal Pulling 50s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 pulling image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Pulled 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Successfully pulled image <span class="hljs-string"><span class="hljs-string">"library/nginx:1.14-alpine"</span></span> Normal Created 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Created container Normal Started 39s kubelet, <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-cluster-nd5c5y6lsfxb-minion-0 Started container</code> </pre> <br><p>  Comme vous pouvez le voir, Pod a d√©marr√© sur un n≈ìud nomm√© test-cluster-nd5c5y6lsfxb-minion-0 et a re√ßu une adresse IP interne de 10.100.88.9. </p><br><p>  Dans la section √âv√©nements, vous pouvez voir les principaux √©v√©nements de lancement - s√©lectionner un n≈ìud pour lancer et t√©l√©charger l'image. </p><br><p>  Nous pouvons entrer dans le Pod et v√©rifier l'√©tat des processus √† l'int√©rieur du conteneur: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh ps aux PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 7 nginx 0:00 nginx: worker process 20 root 0:00 sh 24 root 0:00 ps aux <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Il convient de garder √† l'esprit que l'adresse IP 10.100.88.9 ne sera pas disponible pour d'autres applications √† l'int√©rieur et √† l'ext√©rieur du cluster Kubernetes, l'acc√®s au Nginx en cours d'ex√©cution ne sera possible qu'√† partir du pod lui-m√™me: </p><br><pre> <code class="bash hljs">$ ping -c 1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes --- 10.100.88.9 ping statistics --- 1 packets transmitted, 0 packets received, 100% packet loss $ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> nginx -- ping -c1 10.100.88.9 PING 10.100.88.9 (10.100.88.9): 56 data bytes 64 bytes from 10.100.88.9: seq=0 ttl=64 time=0.075 ms --- 10.100.88.9 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.075/0.075 ms</code> </pre> <br><p>  Outre le fait que l'adresse IP sp√©cifi√©e n'est accessible que depuis le conteneur, elle n'est pas non plus permanente.  Cela signifie que si ce pod est recr√©√©, il peut obtenir une adresse IP diff√©rente. </p><br><p>  Pour r√©soudre ces probl√®mes, vous pouvez utiliser un objet appel√© Service. </p><br><h3>  Les services </h3><br><p>  Le service vous permet d'attribuer des adresses IP permanentes aux pods, de leur fournir l'acc√®s √† partir de r√©seaux externes et d'√©quilibrer les demandes entre les pods. <br>  Vous pouvez en savoir plus sur Service dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><p>  Par exemple, nous devons supprimer le pod en cours d'ex√©cution: </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Ajoutez √† la description du pod une √©tiquette (√©tiquette), qui est requise pour le service: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-labeled.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Nous aurons √©galement besoin d'une description du service: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-nodeport.yaml apiVersion: v1 kind: Service metadata: name: nginx-nodeport labels: app: webservice spec: type: NodePort ports: - port: 80 nodePort: 30001 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Cr√©er un pod et un service: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-labeled.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-nodeport.yaml</code> </pre> <br><p>  √âtant donn√© que le service cr√©√© est de type NodePort, le port 30001 indiqu√© par nous sur toutes les interfaces r√©seau sera ouvert sur tous les n≈ìuds du cluster. <br>  Cela signifie que si nous ajoutons une adresse IP externe √† n'importe quel n≈ìud, nous pouvons acc√©der au pod en cours d'ex√©cution avec Nginx √† partir d'un r√©seau externe. </p><br><p>  Afin de ne pas utiliser les adresses externes des n≈ìuds de cluster pour acc√©der au service, nous pouvons utiliser le type LoadBalancer au lieu de NodePort. <br>  Nous aurons besoin d'une nouvelle description du service: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-loadbalancer.yaml apiVersion: v1 kind: Service metadata: name: nginx-loadbalancer labels: app: webservice spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: app: webservice</span></span></code> </pre> <br><p>  Supprimez le service actuel et appliquez la nouvelle description: </p><br><pre> <code class="bash hljs">$ kubectl delete service nginx-service $ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/services/nginx-loadbalancer.yaml</code> </pre> <br><p>  Apr√®s le d√©marrage du service, Nginx sera disponible sur le port TCP 80 √† partir d'un r√©seau externe, et il ne sera pas n√©cessaire d'attribuer et d'utiliser des adresses externes pour les n≈ìuds de cluster.  Le service de type LoadBalancer allouera automatiquement une nouvelle adresse externe √† votre projet VPC et commencera √† l'utiliser. </p><br><p>  Vous pouvez obtenir des informations sur l'adresse externe en surbrillance √† l'aide de kubectl: </p><br><pre> <code class="bash hljs">$ kubectl get service nginx-service -o=custom-columns=IP:status.loadBalancer.ingress[0].ip IP xxx.xxx.xxx.xxx</code> </pre> <br><p>  Dans nos exemples, un seul Pod a √©t√© lanc√© avec Nginx.  Pour faire √©voluer l'application sur plusieurs pods, nous pouvons utiliser le d√©ploiement. </p><br><h3>  D√©ploiements </h3><br><p>  Le d√©ploiement est l'essence m√™me du cluster Kubernetes, qui vous permet de faire √©voluer les pods et de mettre √† jour ou de restaurer facilement les versions d'un grand nombre de pods. <br>  Au lieu du d√©ploiement, vous pouvez √©galement utiliser l'objet ReplicaSet, mais nous ne le toucherons pas dans nos exemples. <br>  Vous pouvez en savoir plus sur le d√©ploiement dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><p>  Nous devrons √† nouveau retirer le Pod (nous n'avons pas besoin de supprimer le Service): </p><br><pre> <code class="bash hljs">$ kubectl delete pod nginx</code> </pre> <br><p>  Ajoutez la description de d√©ploiement suivante: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.14.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80</span></span></code> </pre> <br><p>  Cr√©ez le d√©ploiement sp√©cifi√©: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.14.yaml</code> </pre> <br><p>  Nous avons choisi 10 pour le param√®tre des r√©pliques, donc 10 Pods avec l'application Nginx seront cr√©√©s dans notre cluster: </p><br><pre> <code class="bash hljs">$ kubectl get pods --selector app=webservice NAME READY STATUS RESTARTS AGE nginx-deployment-54bfdc4489-42rrb 1/1 Running 0 4m nginx-deployment-54bfdc4489-5lvtc 1/1 Running 0 4m nginx-deployment-54bfdc4489-g7rk2 1/1 Running 0 4m nginx-deployment-54bfdc4489-h5rxp 1/1 Running 0 4m nginx-deployment-54bfdc4489-l9l2d 1/1 Running 0 4m nginx-deployment-54bfdc4489-pjpvg 1/1 Running 0 4m nginx-deployment-54bfdc4489-q8dnp 1/1 Running 0 4m nginx-deployment-54bfdc4489-s4wzf 1/1 Running 0 4m nginx-deployment-54bfdc4489-tfxf9 1/1 Running 0 4m nginx-deployment-54bfdc4489-xjzb5 1/1 Running 0 4m</code> </pre> <br><p>  Vous pouvez acc√©der √† l'application en cours d'ex√©cution √† partir du r√©seau externe √† l'aide du service cr√©√© dans la section pr√©c√©dente.  Le service √©quilibrera automatiquement les demandes provenant du r√©seau externe entre 10 instances Nginx. </p><br><p>  Si n√©cessaire, nous pouvons mettre √† jour la version de Nginx.  Mettez √† jour la description du d√©ploiement en changeant la version de l'image de 1.14-alpine √† 1.15-alpine: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-1.15.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 10 selector: matchLabels: app: webservice minReadySeconds: 10 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: labels: app: webservice spec: containers: - name: nginx image: library/nginx:1.15-alpine # &lt;-- changed ports: - containerPort: 80</span></span></code> </pre> <br><p>  Pour d√©marrer le processus de mise √† jour des pods, nous utilisons la commande kubectl.  Faites attention √† l'argument --record, il nous est utile pour la restauration pratique ult√©rieure de la version Nginx: </p><br><pre> <code class="bash hljs">$ kubectl apply -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml \ --record</code> </pre> <br><p>  Vous pouvez surveiller la progression de la mise √† jour √† l'aide de la commande suivante: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 4 out of 10 new replicas have been updated...</code> </pre> <br><p>  Kubernetes attendra 10 secondes apr√®s la mise √† jour r√©ussie d'un pod, car nous avons sp√©cifi√© une valeur de 10 pour le param√®tre minReadySeconds dans la description du d√©ploiement. </p><br><p>  Une fois la mise √† jour termin√©e, tous les pods pour le d√©ploiement passeront √† l'√©tat actif: </p><br><pre> <code class="bash hljs">$ kubectl get deployment --selector app=webservice NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 23m</code> </pre> <br><p>  Nous pouvons restaurer la version de l'application en cas de probl√®me.  Pour ce faire, nous devons s√©lectionner la r√©vision de d√©ploiement souhait√©e: </p><br><pre> <code class="bash hljs">$ kubectl rollout <span class="hljs-built_in"><span class="hljs-built_in">history</span></span> deployment nginx-deployment deployments <span class="hljs-string"><span class="hljs-string">"nginx-deployment"</span></span> REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 kubectl apply --filename=https://raw.githubusercontent.com/selectel/kubernetes-examples/master/deployments/nginx-1.15.yaml --record=<span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Il y a 2 r√©visions dans la sortie de la commande - la premi√®re est la cr√©ation initiale de D√©ploiement, la seconde est une mise √† jour.  Puisque nous avons utilis√© l'argument --record lors de la mise √† jour, nous voyons la commande qui a cr√©√© la deuxi√®me r√©vision de Deployment. </p><br><p>  Pour restaurer la version, utilisez la commande suivante: </p><br><pre> <code class="bash hljs">$ kubectl rollout undo deployment nginx-deployment --to-revision=1</code> </pre> <br><p>  De m√™me avec la mise √† jour, nous pouvons surveiller la restauration de la version en utilisant la commande: </p><br><pre> <code class="bash hljs">$ kubectl rollout status deployment nginx-deployment Waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rollout to finish: 6 out of 10 new replicas have been updated‚Ä¶</code> </pre> <br><p>  Dans tous nos exemples, nous avons utilis√© des conteneurs sans magasin de donn√©es persistant.  Dans la section suivante, nous allons le corriger. </p><br><h2>  Stockage de donn√©es </h2><br><p>  Par d√©faut, toutes les donn√©es des conteneurs ex√©cut√©s √† l'int√©rieur des pods sont √©ph√©m√®res et seront perdues lorsque les pods plantent. </p><br><p>  Vous pouvez utiliser l'objet PersistentVolumeClaim pour ex√©cuter des pods avec un entrep√¥t de donn√©es persistant. </p><br><p>  La cr√©ation d'un tel objet dans un cluster est tr√®s simple - ajoutez simplement sa description, semblable √† la fa√ßon dont nous avons cr√©√© Pod, Service ou Deployment dans les sections pr√©c√©dentes. </p><br><p>  Plus d'informations peuvent √™tre trouv√©es dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><p>  Exemple de description de PersistentVolumeClaim cr√©ant un disque de 10 Go: </p><br><pre> <code class="bash hljs">apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi</code> </pre> <br><p>  Nous pouvons le connecter en tant que disque √† notre Pod en mettant √† jour la description de Pod avec Nginx cr√©√©e pr√©c√©demment: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nginx-with-volume.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: library/nginx:1.14-alpine ports: - containerPort: 80 volumeMounts: - mountPath: "/var/www/html" name: data volumes: - name: data persistentVolumeClaim: claimName: my-pv-claim</span></span></code> </pre> <br><p>  Cependant, pour que le disque soit cr√©√©, vous devrez sp√©cifier les propri√©t√©s du disque cr√©√© sous la forme de StorageClass.  Dans le service "Virtual Private Cloud", vous pouvez utiliser des lecteurs r√©seau de types rapides, universels et basiques comme stockage permanent des donn√©es Kubernetes Pod. </p><br><p>  Par exemple, pour cr√©er une StorageClass qui vous permet d'utiliser des disques rapides dans la zone de disponibilit√© ru-1b, vous avez besoin de la description suivante: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># fast.ru-1b.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast.ru-1b annotations: storageclass.beta.kubernetes.io/is-default-class: "true" provisioner: kubernetes.io/cinder parameters: type: fast.ru-1b availability: ru-1b</span></span></code> </pre> <br><p>  Avant de cr√©er les objets sp√©cifi√©s, supprimez le d√©ploiement cr√©√© pr√©c√©demment: </p><br><pre> <code class="bash hljs">$ kubectl delete deployment nginx-deployment</code> </pre> <br><p>  Tout d'abord, cr√©ons une StorageClass, donc elle deviendra la classe par d√©faut, et PersistentVolumeClaim cr√©√© plus tard l'utilisera: </p><br><pre> <code class="bash hljs">$ kubectl create -f \ https://raw.githubusercontent.com/selectel/kubernetes-examples/master/storageclasses/fast.ru-1b.yaml</code> </pre> <br><p>  Cr√©ez PersistentVolumeClaim et Pod: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/persistentvolumeclaims/my-pv-claim.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/pods/nginx-with-volume.yaml</code> </pre> <br><p>  Apr√®s cela, un disque sera automatiquement cr√©√© dans votre projet qui sera connect√© √† l'un des n≈ìuds sbires du cluster.  Lorsqu'il tombe, le disque bascule automatiquement vers un autre n≈ìud. </p><br><p>  Nous pouvons voir le disque √† l'int√©rieur du conteneur avec Nginx: </p><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it nginx sh mount | grep <span class="hljs-string"><span class="hljs-string">"/var/www/html"</span></span> /dev/sdc on /var/www/html <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> ext4 (rw,seclabel,relatime,data=ordered) <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><p>  Vous pouvez connecter le lecteur au d√©ploiement.  Un exemple correspondant peut √™tre trouv√© dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><h2>  Panneau de configuration Kubernetes </h2><br><p>  Vous pouvez utiliser le tableau de bord int√©gr√© de Kubernetes lui-m√™me pour afficher l'√©tat des objets de cluster et leur gestion. </p><br><p>  Pour acc√©der √† toutes les fonctionnalit√©s du panneau, vous devrez cr√©er un compte avec le r√¥le d'administrateur dans votre cluster. </p><br><p>  Pour ce faire, nous avons besoin d'une description de compte: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># admin-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  Et description du r√¥le: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cluster-admin.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system</span></span></code> </pre> <br><p>  Cr√©ez les objets sp√©cifi√©s: </p><br><pre> <code class="bash hljs">$ kubectl create \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/accounts/admin-user.yaml \ -f https://raw.githubusercontent.com/selectel/kubernetes-examples/master/clusterrolebindings/cluster-admin.yaml</code> </pre> <br><p>  Ensuite, vous devrez d√©couvrir la valeur du jeton g√©n√©r√© pour ce compte. <br>  Pour ce faire, recherchez l'objet Secret correspondant dans le cluster: </p><br><pre> <code class="bash hljs">$ kubectl get secret --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"admin-user-token"</span></span> admin-user-token-bkfhb kubernetes.io/service-account-token 3 22m</code> </pre> <br><p>  Et regardez la valeur du jeton du secret trouv√© avec le nom admin-user-token-bkfhb: </p><br><pre> <code class="bash hljs">$ kubectl describe secret admin-user-token-bkfhb --namespace=kube-system | grep <span class="hljs-string"><span class="hljs-string">"token:"</span></span> token: XXXXXX...</code> </pre> <br><p>  En r√©ponse, vous recevrez la valeur du jeton, enregistrez-la, elle nous sera utile √† l'avenir. <br>  Pour plus de d√©tails sur le contr√¥le d'acc√®s pour les objets Kubernetes, consultez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><p>  Dans le cas o√π vous avez cr√©√© un cluster √† partir d'un mod√®le public, le pod et le service existent d√©j√† pour garantir le fonctionnement du panneau: </p><br><pre> <code class="bash hljs">$ kubectl get svc kubernetes-dashboard --namespace=kube-system 206ms Tue Jun 19 14:35:19 2018 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard ClusterIP 10.254.122.245 &lt;none&gt; 443/TCP 2d $ kubectl get pod --namespace=kube-system --selector k8s-app=kubernetes-dashboard 119ms Tue Jun 19 14:36:48 2018 NAME READY STATUS RESTARTS AGE kubernetes-dashboard-747575c864-jpxvt 1/1 Running 0 2d</code> </pre> <br><p>  √âtant donn√© que Service est de type ClusterIP, il ne sera disponible qu'√† partir du cluster lui-m√™me. <br>  Vous pouvez acc√©der au panneau depuis votre ordinateur de travail avec le fichier de configuration du cluster √† l'aide de la commande kubectl: </p><br><pre> <code class="bash hljs">$ kubectl proxy Starting to serve on 127.0.0.1:8001</code> </pre> <br><p>  Testez le proxy en ouvrant l'adresse sp√©cifi√©e dans le navigateur: </p><br><img src="https://habrastorage.org/webt/lc/zm/k1/lczmk1ud4tjatfu3lthvkrcu66e.png"><br><p>  Si vous voyez une r√©ponse similaire √† la capture d'√©cran, vous pouvez acc√©der √† l'√©cran du panneau de configuration en utilisant l'adresse suivante: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code> </pre> <br><p>  En le parcourant, vous devriez voir l'√©cran de connexion dans le panneau: </p><br><img src="https://habrastorage.org/webt/60/fj/p5/60fjp5-2xjyn_p5mz-jh_ozwzxe.png"><br><p>  Vous devrez sp√©cifier le jeton re√ßu plus t√¥t.  Une fois connect√©, vous pouvez utiliser le panneau de configuration: </p><br><img src="https://habrastorage.org/webt/38/ry/dr/38rydrraxndpo0hyqhnm-k1nb_k.png"><br><p>  Vous pouvez d√©couvrir toutes les fonctionnalit√©s du panneau de contr√¥le dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="nofollow noopener noreferrer">documentation officielle</a> . </p><br><h2>  Surveillance des objets Kubernetes </h2><br><p>  Si vous utilisez le mod√®le de cluster public, vous ex√©cuterez automatiquement les composants de collecte et d'affichage des m√©triques - Prometheus et Grafana. </p><br><p>  De m√™me que le panneau de configuration, ClusterIP est install√© en tant que type de service; l'acc√®s √† celui-ci n'est possible que depuis le cluster ou via le proxy kubectl.  Vous pouvez acc√©der √† Grafana depuis votre ordinateur professionnel √† l'adresse suivante: </p><br><pre> <code class="bash hljs">http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana:80</code> </pre> <br><img src="https://habrastorage.org/webt/p3/7e/go/p37egoksdoz8bvp8tsq2fgpsuru.png"><br><h2>  Conclusion </h2><br><p>  Dans cet article, nous avons examin√© les objets Kubernetes les plus couramment utilis√©s et examin√© des exemples de d√©marrage et de gestion d'un cluster √† l'aide d'OpenStack Magnum. </p><br><p>  Dans un avenir proche, il sera possible d'utiliser les derni√®res versions de Kubernetes et la gestion des clusters sera disponible via <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le panneau de configuration</a> . </p><br><p>  Nous serons heureux si vous utilisez notre service en mode test et fournissez des commentaires. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416951/">https://habr.com/ru/post/fr416951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416941/index.html">Th√©orie du bonheur. Introduction √† la merphologie</a></li>
<li><a href="../fr416943/index.html">Mat√©riaux utiles pour la conception d'interfaces vocales</a></li>
<li><a href="../fr416945/index.html">Comme nous l'avons fait pour BelAZ. Partie 1 - Fer</a></li>
<li><a href="../fr416947/index.html">Jouez le jeu avant les Jeux olympiques: l'eSport devient officiel</a></li>
<li><a href="../fr416949/index.html">La mise √† niveau √† grande √©chelle de M. Steven pour installer un r√©seau de chasse quadrupl√© est termin√©e</a></li>
<li><a href="../fr416953/index.html">Cr√©ez un shader d'eau de dessin anim√© pour le Web. Partie 1</a></li>
<li><a href="../fr416955/index.html">Petits trucs avec Elasticsearch</a></li>
<li><a href="../fr416957/index.html">Quelle machine laser acheter? Examen fiable de la machine laser Raylogic 11G</a></li>
<li><a href="../fr416959/index.html">Apple pr√©sente une nouvelle fonctionnalit√© antivol iOS</a></li>
<li><a href="../fr416961/index.html">R√©solution automatique des conflits √† l'aide de transformations op√©rationnelles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>