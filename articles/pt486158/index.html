<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë®‚Äçüëß‚Äçüëß üë©üèæ üì± Visualiza√ß√£o da tradu√ß√£o autom√°tica neural (modelos seq2seq com mecanismo de aten√ß√£o) üí© üîÅ üï•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Apresento a voc√™ a tradu√ß√£o do artigo "Visualizando um modelo de tradu√ß√£o autom√°tica neural (mec√¢nica dos modelos Seq2seq com aten√ß√£o)", de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visualiza√ß√£o da tradu√ß√£o autom√°tica neural (modelos seq2seq com mecanismo de aten√ß√£o)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/486158/"><p>  Ol√° Habr!  Apresento a voc√™ a tradu√ß√£o do artigo <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">"Visualizando um modelo de tradu√ß√£o autom√°tica neural (mec√¢nica dos modelos Seq2seq com aten√ß√£o)",</a> de Jay Alammar. </p><br><p>  Os modelos sequ√™ncia a sequ√™ncia (seq2seq) s√£o modelos de aprendizado profundo que obtiveram grande sucesso em tarefas como tradu√ß√£o autom√°tica, resumo de texto, anota√ß√£o de imagem etc. Por exemplo, no final de 2016, um modelo semelhante <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" rel="nofollow">foi incorporado</a> ao Google Translate.  As bases dos modelos seq2seq foram estabelecidas em 2014 com o lan√ßamento de dois artigos - <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">Sutskever et al., 2014</a> , <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" rel="nofollow">Cho et al., 2014</a> . </p><br><p> Para entender e usar adequadamente esses modelos, alguns conceitos devem ser esclarecidos.  As visualiza√ß√µes propostas neste artigo ser√£o um bom complemento para os artigos mencionados acima. </p><br><p>  O modelo de sequ√™ncia a sequ√™ncia √© um modelo que aceita uma sequ√™ncia de entrada de elementos (palavras, letras, atributos de imagem etc.) e retorna outra sequ√™ncia de elementos.  O modelo treinado funciona da seguinte maneira: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_1.mp4" type="video/mp4"></video></div></div></div><a name="habracut"></a><br><p>  Na tradu√ß√£o autom√°tica neural, uma sequ√™ncia de elementos √© uma cole√ß√£o de palavras que s√£o processadas por sua vez.  A conclus√£o tamb√©m √© um conjunto de palavras: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_2.mp4" type="video/mp4"></video></div></div></div><br><h1 id="zaglyanem-pod-kapot">  D√™ uma olhada sob o cap√¥ </h1><br><p>  Sob o cap√¥, o modelo possui um codificador e decodificador. </p><br><p>  O codificador processa cada elemento da sequ√™ncia de entrada, converte as informa√ß√µes recebidas em um vetor chamado contexto.  Ap√≥s processar toda a sequ√™ncia de entrada, o codificador envia o contexto para o decodificador, que ent√£o come√ßa a gerar o elemento de sequ√™ncia de sa√≠da por elemento. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_3.mp4" type="video/mp4"></video></div></div></div><br><p>  O mesmo acontece com a tradu√ß√£o autom√°tica. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_4.mp4" type="video/mp4"></video></div></div></div><br><p>  Para tradu√ß√£o autom√°tica, o contexto √© um vetor (uma matriz de n√∫meros), e o codificador e decodificador, por sua vez, s√£o frequentemente redes neurais recorrentes (consulte a introdu√ß√£o ao RNN - <a href="https://www.youtube.com/watch%3Fv%3DUNmqTiOnRfg" rel="nofollow">uma introdu√ß√£o amig√°vel √†s redes neurais recorrentes</a> ). </p><br><p><img src="https://habrastorage.org/webt/yr/ir/92/yrir92d8paf_xdm29bovaw109gu.png" alt="contexto"></p><br><p>  <em>Contexto √© um vetor de n√∫meros de ponto flutuante.</em>  <em>Al√©m disso, no artigo, os vetores ser√£o visualizados em cores para que a cor mais clara corresponda √†s c√©lulas com valores grandes.</em> </p><br><p>  Ao treinar o modelo, voc√™ pode definir o tamanho do vetor de contexto - o n√∫mero de neur√¥nios ocultos (unidades ocultas) no codificador RNN.  Os dados de visualiza√ß√£o mostram um vetor quadridimensional, mas em aplicativos reais o vetor de contexto ter√° uma dimens√£o da ordem de 256, 512 ou 1024. </p><br><p>  Por padr√£o, a cada intervalo de tempo, a RNN recebe dois elementos para entrada: o pr√≥prio elemento de entrada (no caso de um codificador, uma palavra da frase original) e o estado oculto.  A palavra, no entanto, deve ser representada por um vetor.  Para converter uma palavra em vetor, eles recorrem a uma s√©rie de algoritmos chamados incorpora√ß√£o de palavras.  Casamentos traduzem palavras em espa√ßos vetoriais contendo informa√ß√µes sem√¢nticas e sem√¢nticas sobre eles (por exemplo, <a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" rel="nofollow">"rei" - "homem" + "mulher" = "rainha"</a> ). </p><br><p><img src="https://habrastorage.org/webt/87/pp/s9/87pps99ndmgvp6gqxskkodf4zl4.png" alt="incorpora√ß√£o"></p><br><p>  <em>Antes de processar palavras, voc√™ deve convert√™-las em vetores.</em>  <em>Essa transforma√ß√£o √© realizada usando o algoritmo de incorpora√ß√£o de palavras.</em>  <em>Voc√™ pode usar as combina√ß√µes pr√©-treinadas e as treinadas no seu conjunto de dados.</em>  <em>200-300 - dimens√£o t√≠pica do vetor de incorpora√ß√£o;</em>  <em>este artigo usa a dimens√£o 4 para simplificar.</em> </p><br><p>  Agora que nos familiarizamos com nossos principais vetores / tensores, vamos relembrar o mecanismo da RNN e criar visualiza√ß√µes para descrev√™-lo: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/RNN_1.mp4" type="video/mp4"></video></div></div></div><br><p>  Na pr√≥xima etapa, a RNN pega o segundo vetor de entrada e o estado latente n¬∫ 1 para formar a sa√≠da nesse intervalo de tempo.  Mais adiante neste artigo, anima√ß√£o semelhante √© usada para descrever vetores dentro de um modelo de tradu√ß√£o autom√°tica neural. </p><br><p>  Na visualiza√ß√£o a seguir, cada quadro descreve o processamento de entradas por um codificador e a gera√ß√£o de sa√≠das por um decodificador em um intervalo de tempo.  Como o codificador e o decodificador s√£o RNN, a cada intervalo de tempo, a rede neural est√° ocupada processando e atualizando seus estados ocultos com base nas entradas atuais e em todas as anteriores.  Nesse caso, o √∫ltimo dos estados ocultos do codificador √© o pr√≥prio contexto que √© transmitido ao decodificador. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_5.mp4" type="video/mp4"></video></div></div></div><br><p>  O decodificador tamb√©m cont√©m estados ocultos que s√£o transferidos de um intervalo de tempo para outro.  (Isso n√£o est√° na visualiza√ß√£o, representando apenas as partes principais do modelo.) </p><br><p>  Passamos agora a outro tipo de visualiza√ß√£o de modelos de sequ√™ncia a sequ√™ncia.  Esta anima√ß√£o ajudar√° a entender os gr√°ficos est√°ticos que descrevem esses modelos - os chamados  uma vis√£o desenrolada, onde, em vez de mostrar um decodificador, mostramos uma c√≥pia para cada intervalo de tempo.  Assim, podemos observar os elementos de entrada e sa√≠da a cada intervalo de tempo. </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_6.mp4" type="video/mp4"></video></div></div></div><br><h1 id="obratite-vnimanie">  Preste aten√ß√£o! </h1><br><p>  O vetor de contexto √© um gargalo para esse tipo de modelo, dificultando o tratamento de frases longas.  A solu√ß√£o foi proposta nos artigos de <a href="" rel="nofollow">Bahdanau et al., 2014</a> e <a href="https://arxiv.org/abs/1508.04025" rel="nofollow">Luong et al., 2015</a> , que apresentaram uma t√©cnica chamada mecanismo de aten√ß√£o.  Esse mecanismo melhora significativamente a qualidade dos sistemas de tradu√ß√£o autom√°tica, permitindo que os modelos se concentrem nas partes relevantes das seq√º√™ncias de entrada. </p><br><p><img src="https://habrastorage.org/webt/b1/ru/kj/b1rukj6w-dgtyfncd3a62635zb0.png" alt="aten√ß√£o"></p><br><p>  <em>No 7¬∫ per√≠odo de tempo, o mecanismo de aten√ß√£o permite que o decodificador se concentre na palavra √©tudiant (estudante de franc√™s) antes de gerar uma tradu√ß√£o para o ingl√™s.</em>  <em>Essa capacidade de amplificar o sinal da parte relevante da sequ√™ncia de entrada permite que os modelos baseados no mecanismo de aten√ß√£o obtenham um melhor resultado em compara√ß√£o com outros modelos.</em> </p><br><p>  Ao considerar um modelo com um mecanismo de aten√ß√£o em um alto n√≠vel de abstra√ß√£o, duas diferen√ßas principais em rela√ß√£o ao modelo cl√°ssico de sequ√™ncia a sequ√™ncia podem ser distinguidas. </p><br><p>  Primeiramente, o codificador transfere significativamente mais dados para o decodificador: em vez de transmitir apenas o √∫ltimo estado oculto ap√≥s o est√°gio de codifica√ß√£o, o codificador envia todos os seus estados ocultos para ele: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_7.mp4" type="video/mp4"></video></div></div></div><br><p>  Em segundo lugar, o decodificador passa por uma etapa adicional antes de gerar a sa√≠da.  Para focar nas partes da sequ√™ncia de entrada relevantes para o intervalo de tempo correspondente, o decodificador faz o seguinte: </p><br><ol><li>  Examina um conjunto de estados latentes recebidos de um codificador - cada um dos estados latentes se correlaciona melhor com uma das palavras na sequ√™ncia de entrada; </li><li>  Atribui uma determinada avalia√ß√£o a cada estado latente (vamos omitir por enquanto como o procedimento de estimativa acontece); </li><li>  Multiplica cada estado oculto por uma fun√ß√£o de avalia√ß√£o convertida em softmax, destacando estados ocultos com uma classifica√ß√£o grande e relegando estados ocultos com um pequeno para o fundo. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/attention_process.mp4" type="video/mp4"></video></div></div></div><br><p>  Este "exerc√≠cio de classifica√ß√£o" √© realizado no decodificador a cada intervalo de tempo. </p><br><p>  Portanto, resumindo tudo isso, consideramos o processo do modelo com o mecanismo de aten√ß√£o: </p><br><ol><li>  No decodificador, o RNN recebe a incorpora√ß√£o &lt;END&gt; do token e o estado oculto inicial. </li><li>  A RNN processa o elemento de entrada, gera a sa√≠da e um novo vetor de estado oculto (h4).  A sa√≠da √© descartada. </li><li>  O mecanismo de aten√ß√£o usa os estados ocultos do codificador e o vetor h4 para calcular o vetor de contexto (C4) em um determinado intervalo de tempo. </li><li>  Os vetores h4 e C4 s√£o concatenados em um √∫nico vetor. </li><li>  Esse vetor √© passado atrav√©s de uma rede neural feedforward (FFN), treinada em conjunto com o modelo. </li><li>  A sa√≠da da rede FFN indica a palavra de sa√≠da em um determinado intervalo de tempo. </li><li>  O algoritmo √© repetido para o pr√≥ximo intervalo de tempo. </li></ol><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/attention_tensor_dance.mp4" type="video/mp4"></video></div></div></div><br><p>  Outra maneira de analisar qual parte da frase original o modelo foca em cada est√°gio do decodificador: </p><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="https://jalammar.github.io/images/seq2seq_9.mp4" type="video/mp4"></video></div></div></div><br><p>  Observe que o modelo n√£o conecta apenas a primeira palavra na entrada com a primeira palavra na sa√≠da.  Ela realmente entendeu durante o processo de treinamento como combinar as palavras nesse par de idiomas considerado (no nosso caso, franc√™s e ingl√™s).  Um exemplo de como esse mecanismo pode funcionar com precis√£o pode ser encontrado nos artigos sobre o mecanismo de aten√ß√£o mencionado acima. </p><br><p><img src="https://habrastorage.org/webt/cl/dv/9f/cldv9f7zdegsobuszn10hyy1nde.png" alt="aten√ß√£o_senten√ßa"></p><br><p>  Se voc√™ sentir que est√° pronto para aprender como aplicar esse modelo, consulte o manual de <a href="https://github.com/tensorflow/nmt" rel="nofollow">Tradu√ß√£o Autom√°tica Neural (seq2seq)</a> no TensorFlow. </p><br><h1 id="avtory">  Os autores </h1><br><ul><li>  <strong>Original</strong> de <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">Jay Alammar</a> </li><li>  <strong>Tradu√ß√£o</strong> - <a href="https://habr.com/ru/users/smekur/">Ekaterina Smirnova</a> </li><li>  <strong>Edi√ß√£o e layout</strong> - <a href="https://habr.com/ru/users/kouki_rus/">Shkarin Sergey</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt486158/">https://habr.com/ru/post/pt486158/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt486124/index.html">Impala vs Hive vs Spark SQL: Escolhendo o mecanismo SQL certo para funcionar corretamente no Cloudera Data Warehouse</a></li>
<li><a href="../pt486128/index.html">Arquiteto de solu√ß√µes de teste: quem √© e quando √© necess√°rio</a></li>
<li><a href="../pt486144/index.html">Por que altcoins morrem e o que pode acontecer com a criptomoeda em um futuro pr√≥ximo?</a></li>
<li><a href="../pt486150/index.html">Desenvolvimento da esfera de TI na Eslov√°quia. Benef√≠cios de trabalho para jovens profissionais</a></li>
<li><a href="../pt486156/index.html">Como eu ensinei, e depois escrevi um manual de treinamento em Python</a></li>
<li><a href="../pt486164/index.html">Coronav√≠rus 2019-nCoV. Perguntas frequentes sobre prote√ß√£o respirat√≥ria e desinfec√ß√£o</a></li>
<li><a href="../pt486174/index.html">Tenho rotatividade zero</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>