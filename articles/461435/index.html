<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ∑Ô∏è üöñ üåó Reconocimiento de emociones usando una red neuronal convolucional üìµ üë®üèø‚Äçüç≥ üôåüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reconocer las emociones siempre ha sido un desaf√≠o emocionante para los cient√≠ficos. Recientemente, estoy trabajando en un proyecto experimental SER (...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconocimiento de emociones usando una red neuronal convolucional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  Reconocer las emociones siempre ha sido un desaf√≠o emocionante para los cient√≠ficos.  Recientemente, estoy trabajando en un proyecto experimental SER (Reconocimiento de Emociones del Habla) para comprender el potencial de esta tecnolog√≠a. Para esto seleccion√© los repositorios m√°s populares en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Github</a> y los convert√≠ en la base de mi proyecto. <br><br>  Antes de comenzar a comprender el proyecto, ser√° bueno recordar qu√© tipo de cuellos de botella tiene SER. <br><a name="habracut"></a><br><h2>  Principales obst√°culos </h2><br><ul><li>  las emociones son subjetivas, incluso las personas las interpretan de manera diferente.  Es dif√≠cil definir el concepto mismo de "emoci√≥n"; </li><li>  comentar sobre el audio es dif√≠cil.  ¬øDeber√≠amos marcar de alguna manera cada palabra individual, oraci√≥n o toda la comunicaci√≥n como un todo?  ¬øUn conjunto de qu√© tipo de emociones usar en reconocimiento? </li><li>  Recopilar datos tampoco es f√°cil.  Se pueden recopilar muchos datos de audio de pel√≠culas y noticias.  Sin embargo, ambas fuentes est√°n "sesgadas" porque las noticias deben ser neutrales y se juegan las emociones de los actores.  Es dif√≠cil encontrar una fuente "objetiva" de datos de audio. </li><li>  los datos de marcado requieren grandes recursos humanos y de tiempo.  A diferencia de dibujar cuadros en im√°genes, se requiere personal especialmente capacitado para escuchar grabaciones de audio completas, analizarlas y proporcionar comentarios.  Y luego <b>muchas</b> personas deben apreciar estos comentarios, porque las calificaciones son subjetivas. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Descripci√≥n del proyecto </h2><br>  Usar una red neuronal convolucional para reconocer emociones en grabaciones de audio.  Y s√≠, el propietario del repositorio no se refiri√≥ a ninguna fuente. <br><br><h2>  Descripci√≥n de datos </h2><br>  Hay dos conjuntos de datos que se utilizaron en los repositorios RAVDESS y SAVEE, acabo de adaptar RAVDESS en mi modelo.  Hay dos tipos de datos en el contexto RAVDESS: voz y canci√≥n. <br><br>  Conjunto de datos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RAVDESS (la base de datos audiovisuales de Ryerson de discurso y canci√≥n emocional)</a> : <br><br><ul><li>  12 actores y 12 actrices grabaron sus discursos y canciones en su actuaci√≥n; </li><li>  el actor # 18 no tiene canciones grabadas; </li><li>  Las emociones disgusto (asco), neutral (neutral) y sorpresas (sorprendido) est√°n ausentes en los datos de la "canci√≥n". </li></ul><br>  Desglose de emociones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Cuadro de distribuci√≥n de emociones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Extracci√≥n de caracter√≠sticas </h3><br>  Cuando trabajamos con tareas de reconocimiento de voz, los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">coeficientes cepstrales (MFCC)</a> son una tecnolog√≠a avanzada, a pesar de que apareci√≥ en los a√±os 80. <br><br>  Cita del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tutorial de MFCC</a> : <br><blockquote>  Esta forma determina cu√°l es el sonido de salida.  Si podemos identificar el formulario, nos dar√° una representaci√≥n precisa del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fonema</a> sonado.  La forma del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tracto vocal se</a> manifiesta en una envoltura de corto espectro, y el trabajo del MFCC es mostrar con precisi√≥n esta envoltura. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Forma de onda</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Espectrograma</font> <br><br>  Usamos MFCC como una caracter√≠stica de entrada.  Si est√° interesado en aprender m√°s sobre lo que es MFCC, entonces este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tutorial</a> es para usted.  La descarga de datos y su conversi√≥n al formato MFCC se puede hacer f√°cilmente usando el paquete de librosa Python. <br><br><h3>  Arquitectura de modelo predeterminada </h3><br>  El autor desarroll√≥ un modelo CNN utilizando el paquete Keras, creando 7 capas: seis capas Con1D y una capa de densidad (Densa). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  El autor coment√≥ las capas 4 y 5 en la √∫ltima versi√≥n (18 de septiembre de 2018) y el tama√±o del archivo final de este modelo no se ajusta a la red proporcionada, por lo que no puedo lograr el mismo resultado en precisi√≥n: 72%. </blockquote><br>  El modelo simplemente se entrena con los par√°metros <code>batch_size=16</code> y <code>batch_size=16</code> <code>epochs=700</code> , sin ning√∫n programa de entrenamiento, etc. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Aqu√≠ <code>categorical_crossentropy</code> es una funci√≥n de las p√©rdidas, y la medida de la evaluaci√≥n es la precisi√≥n. <br><br><h2>  Mi experimento </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">An√°lisis exploratorio de datos</a> </h3><br>  En el conjunto de datos RAVDESS, cada actor muestra 8 emociones, pronunciando y cantando 2 oraciones, 2 veces cada una.  Como resultado, se obtienen 4 ejemplos de cada emoci√≥n de cada actor, con la excepci√≥n de las emociones neutrales, el asco y la sorpresa mencionados anteriormente.  Cada audio tiene una duraci√≥n aproximada de 4 segundos, en el primer y √∫ltimo segundo suele ser silencio. <br><br>  <b>Ofertas tipicas</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Observaci√≥n </h3><br>  Despu√©s de seleccionar un conjunto de datos de 1 actor y 1 actriz, y luego escuchar todos sus registros, me di cuenta de que los hombres y las mujeres expresan sus emociones de manera diferente.  Por ejemplo: <br><br><ul><li>  la ira masculina (enojada) es m√°s fuerte; </li><li>  alegr√≠a de los hombres (feliz) y frustraci√≥n (triste): una caracter√≠stica en los tonos de risa y llanto durante el "silencio"; </li><li>  alegr√≠a femenina (feliz), enojo (enojado) y frustraci√≥n (triste) son m√°s fuertes; </li><li>  El asco femenino (asco) contiene el sonido del v√≥mito. </li></ul><br><h3>  Experimento de repetici√≥n </h3><br>  El autor elimin√≥ las clases neutrales, disgustadas y sorprendidas para hacer el reconocimiento RAVDESS de 10 clases del conjunto de datos.  Al intentar repetir la experiencia del autor, obtuve este resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  Sin embargo, descubr√≠ que hay una fuga de datos cuando el conjunto de datos para la validaci√≥n es id√©ntico al conjunto de datos de prueba.  Por lo tanto, repet√≠ la separaci√≥n de los datos, aislando los conjuntos de datos de dos actores y dos actrices para que no fueran visibles durante la prueba: <br><br><ul><li>  los actores 1 a 20 se usan para los conjuntos Train / Valid en una proporci√≥n de 8: 2; </li><li>  los actores 21 a 24 est√°n aislados de las pruebas; </li><li>  Par√°metros del conjunto de trenes: (1248, 216, 1); </li><li>  Par√°metros de configuraci√≥n v√°lidos: (312, 216, 1); </li><li>  Par√°metros del conjunto de pruebas: (320, 216, 1) - (aislado). </li></ul><br>  Volv√≠ a entrenar el modelo y aqu√≠ est√° el resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Prueba de rendimiento </h3><br>  En el gr√°fico Train Valid Gross queda claro que no hay convergencia para las 10 clases seleccionadas.  Por lo tanto, decid√≠ reducir la complejidad del modelo y dejar solo las emociones masculinas.  Aisl√© a dos actores en el conjunto de prueba y puse el resto en el tren / conjunto v√°lido, relaci√≥n 8: 2.  Esto asegura que no haya desequilibrio en el conjunto de datos.  Luego entren√© los datos masculinos y femeninos por separado para realizar la prueba. <br><br>  <b>Conjunto de datos masculino</b> <br><br><ul><li>  Conjunto de trenes: 640 muestras de los actores 1-10; </li><li>  Conjunto v√°lido: 160 muestras de los actores 1-10; </li><li>  Conjunto de prueba: 160 muestras de actores 11-12. </li></ul><br>  <b>L√≠nea de referencia: hombres</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Conjunto de datos femenino</b> <br><br><ul><li>  Conjunto de trenes: 608 muestras de actrices 1-10; </li><li>  Conjunto v√°lido: 152 muestras de las actrices 1-10; </li><li>  Conjunto de prueba: 160 muestras de actrices 11-12. </li></ul><br>  <b>L√≠nea de referencia: mujeres</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  Como puede ver, las matrices de error son diferentes. <br><br>  Hombres: Angry y Happy son las principales clases predichas en el modelo, pero no son iguales. <br><br>  Mujeres: desorden (triste) y alegr√≠a (feliz): clases b√°sicamente predichas en el modelo;  la ira y la alegr√≠a se confunden f√°cilmente. <br><br>  Recordando las observaciones del <b>An√°lisis de Datos de Inteligencia</b> , sospecho que las mujeres Angry y Happy son similares al punto de confusi√≥n, porque su forma de expresi√≥n es simplemente levantar la voz. <br><br>  Adem√°s de eso, tengo curiosidad de que si simplifico a√∫n m√°s el modelo, dejar√© solo las clases Positivo, Neutral y Negativo.  O solo Positivo y Negativo.  En resumen, agrup√© las emociones en 2 y 3 clases, respectivamente. <br><br>  <b>2 clases:</b> <br><br><ul><li>  Positivo: alegr√≠a (feliz), calma (calma); </li><li>  Negativo: ira, miedo (miedo), frustraci√≥n (triste). </li></ul><br>  <b>3 clases:</b> <br><br><ul><li>  Positivo: alegr√≠a (feliz); </li><li>  Neutral: calma (calma), neutral (neutral); </li><li>  Negativo: ira, miedo (miedo), frustraci√≥n (triste). </li></ul><br>  Antes de comenzar el experimento, configur√© la arquitectura del modelo utilizando datos masculinos, haciendo un reconocimiento de 5 clases. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  Agregu√© 2 capas de Conv1D, una capa de MaxPooling1D y 2 capas de BarchNormalization;  Tambi√©n cambi√© el valor de deserci√≥n a 0.25.  Finalmente, cambi√© el optimizador a SGD con una velocidad de aprendizaje de 0.0001. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  Para entrenar el modelo, apliqu√© una reducci√≥n en la "meseta de entrenamiento" y <code>val_loss</code> solo el mejor modelo con un valor m√≠nimo de <code>val_loss</code> .  Y aqu√≠ est√°n los resultados para las diferentes clases objetivo. <br><br><h2>  Nuevo modelo de rendimiento </h2><br>  <b>Hombres, 5 clases</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Mujeres, grado 5</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>Hombres, grado 2</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>Hombres, 3 clases</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Incremento (aumento) </h2><br>  Cuando fortalec√≠ la arquitectura del modelo, el optimizador y la velocidad del entrenamiento, result√≥ que el modelo todav√≠a no converge en el modo de entrenamiento.  Suger√≠ que este es un problema de cantidad de datos, ya que solo tenemos 800 muestras.  Esto me llev√≥ a m√©todos para aumentar el audio, al final dupliqu√© los conjuntos de datos.  Echemos un vistazo a estos m√©todos. <br><br><h3>  Hombres, 5 clases </h3><br>  <b>Incremento Din√°mico</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>Ajuste de tono</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>Offset</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Agregar ruido blanco</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  Es notable que el aumento aumenta en gran medida la precisi√≥n, hasta un 70 +% en el caso general.  Especialmente en el caso de la adici√≥n de blanco, que aumenta la precisi√≥n al 87.19%; sin embargo, la precisi√≥n de la prueba y la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">medida F1</a> caen en m√°s del 5%.  Y luego tuve la idea de combinar varios m√©todos de aumento para un mejor resultado. <br><br><h3>  Combinando varios m√©todos </h3><br>  <b>Ruido blanco + sesgo</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Prueba de aumento en hombres </h2><br><h3>  Hombres, grado 2 </h3><br>  <b>Ruido blanco + sesgo</b> <br><br>  Para todas las muestras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>Ruido blanco + sesgo</b> <br><br>  Solo para muestras positivas, ya que el conjunto de 2 clases est√° desequilibrado (hacia muestras negativas). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Echada + Ruido Blanco</b> <br>  Para todas las muestras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Echada + Ruido Blanco</b> <br><br>  Solo para muestras positivas <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Conclusi√≥n </h2><br>  Al final, pude experimentar solo con un conjunto de datos masculino.  Re-divid√≠ los datos para evitar el desequilibrio y, como consecuencia, la fuga de datos.  Configur√© el modelo para experimentar con voces masculinas, ya que quer√≠a simplificar el modelo tanto como sea posible para comenzar.  Tambi√©n realic√© pruebas usando diferentes m√©todos de aumento;  La adici√≥n de ruido blanco y sesgo ha funcionado bien en datos no balanceados. <br><br><h2>  Conclusiones </h2><br><ul><li>  las emociones son subjetivas y dif√≠ciles de arreglar; </li><li>  es necesario determinar de antemano qu√© emociones son adecuadas para el proyecto; </li><li>  No conf√≠es siempre en el contenido con Github, incluso si tiene muchas estrellas; </li><li>  intercambio de datos: tenlo en cuenta; </li><li>  el an√°lisis exploratorio de datos siempre da una buena idea, pero debe ser paciente cuando se trata de trabajar con datos de audio; </li><li>  determine qu√© le dar√° a la entrada de su modelo: ¬øuna oraci√≥n, un registro completo o una exclamaci√≥n? </li><li>  la falta de datos es un factor de √©xito importante en SER, sin embargo, crear un buen conjunto de datos con emociones es una tarea compleja y costosa; </li><li>  simplifique su modelo en caso de falta de datos. </li></ul><br><h2>  Mejora adicional </h2><br><ul><li>  Us√© solo los primeros 3 segundos como entrada para reducir el tama√±o general de los datos; el proyecto original us√≥ 2.5 segundos.  Me gustar√≠a experimentar con grabaciones de tama√±o completo; </li><li>  puede preprocesar los datos: recortar el silencio, normalizar la longitud rellenando con ceros, etc. </li><li>  pruebe redes neuronales recurrentes para esta tarea. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/461435/">https://habr.com/ru/post/461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461421/index.html">C√≥mo asegurarse contra posibles p√©rdidas al invertir en el intercambio: productos estructurales</a></li>
<li><a href="../461423/index.html">11 consejos: c√≥mo presentar el trabajo de UI / UX a "no dise√±adores"</a></li>
<li><a href="../461425/index.html">C√≥mo convertirse en un gerente de producto y crecer m√°s</a></li>
<li><a href="../461431/index.html">"Ama y no le gusta": DNS sobre HTTPS</a></li>
<li><a href="../461433/index.html">Uso de Identity Server 4 en Net Core 3.0</a></li>
<li><a href="../461437/index.html">370 bombillas</a></li>
<li><a href="../461439/index.html">Inicio de la biblioteca de componentes React y TypeScript</a></li>
<li><a href="../461441/index.html">Informes sobre el estado de almacenamiento utilizando R. Computaci√≥n paralela, gr√°ficos, xlsx, correo electr√≥nico y todo esto</a></li>
<li><a href="../461443/index.html">An√°lisis posterior: lo que se sabe sobre el √∫ltimo ataque a la red del servidor de claves criptogr√°ficas SKS Keyserver</a></li>
<li><a href="../461447/index.html">La √©pica sobre los administradores del sistema como una especie en peligro de extinci√≥n.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>