<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöü üå™Ô∏è ü•´ Cassandra Sink untuk Streaming Terstruktur Spark üê• ‚ñ∂Ô∏è üë≤üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Beberapa bulan yang lalu saya mulai mempelajari Spark, dan pada titik tertentu saya dihadapkan dengan masalah penghematan perhitungan Streaming Terstr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink untuk Streaming Terstruktur Spark</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Beberapa bulan yang lalu saya mulai mempelajari Spark, dan pada titik tertentu saya dihadapkan dengan masalah penghematan perhitungan Streaming Terstruktur dalam database Cassandra. <br><br>  Dalam posting ini, saya memberikan contoh sederhana tentang membuat dan menggunakan Cassandra Sink untuk Spark Structured Streaming.  Saya berharap posting ini akan bermanfaat bagi mereka yang baru saja mulai bekerja dengan Spark Structured Streaming dan bertanya-tanya bagaimana cara mengunggah hasil perhitungan ke database. <br><br>  Gagasan aplikasi ini sangat sederhana - untuk menerima dan mem-parsing pesan dari Kafka, melakukan transformasi sederhana berpasangan dan menyimpan hasilnya dalam cassandra. <br><a name="habracut"></a><br><h3>  Kelebihan Streaming Terstruktur </h3><br>  Anda dapat membaca lebih lanjut tentang Streaming Terstruktur dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> .  Singkatnya, Structured Streaming adalah mesin pengolah informasi streaming berskala baik yang didasarkan pada mesin Spark SQL.  Hal ini memungkinkan Anda untuk menggunakan Dataset / DataFrame untuk mengumpulkan data, menghitung fungsi jendela, koneksi, dll. Artinya, Streaming Terstruktur memungkinkan Anda untuk menggunakan SQL lama yang baik untuk bekerja dengan aliran data. <br><br><h3>  Apa masalahnya? </h3><br>  Rilis stabil Spark Structured Streaming dirilis pada 2017.  Artinya, ini adalah API yang cukup baru yang mengimplementasikan fungsionalitas dasar, tetapi beberapa hal harus dilakukan sendiri.  Misalnya, Streaming Terstruktur memiliki fungsi standar untuk menulis output ke file, ubin, konsol atau memori, tetapi untuk menyimpan data ke database, Anda harus menggunakan penerima <i>foreach yang</i> tersedia di Structured Streaming dan mengimplementasikan antarmuka <i>ForeachWriter</i> .  <b>Dimulai dengan Spark 2.3.1, fungsi ini hanya dapat diimplementasikan di Scala dan Java</b> . <br><br>  Saya berasumsi bahwa pembaca sudah tahu bagaimana Structured Streaming bekerja secara umum, tahu bagaimana menerapkan transformasi yang diperlukan dan sekarang siap untuk mengunggah hasilnya ke database.  Jika beberapa langkah di atas tidak jelas, dokumentasi resmi dapat berfungsi sebagai titik awal yang baik dalam mempelajari Streaming Terstruktur.  Pada artikel ini, saya ingin fokus pada langkah terakhir ketika Anda perlu menyimpan hasilnya dalam database. <br><br>  Di bawah ini, saya akan menjelaskan contoh penerapan wastafel Cassandra untuk Structured Streaming dan menjelaskan cara menjalankannya dalam sebuah cluster.  Kode lengkap tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Ketika saya pertama kali menemukan masalah di atas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">proyek ini</a> ternyata sangat berguna.  Namun, mungkin tampak sedikit rumit jika pembaca baru saja mulai bekerja dengan Structured Streaming dan sedang mencari contoh sederhana tentang cara mengunggah data ke cassandra.  Selain itu, proyek ditulis untuk bekerja dalam mode lokal dan memerlukan beberapa perubahan untuk dijalankan di cluster. <br><br>  Saya juga ingin memberikan contoh cara menyimpan data ke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">MongoDB</a> dan basis data lainnya menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">JDBC</a> . <br><br><h3>  Solusi sederhana </h3><br>  Untuk mengunggah data ke sistem eksternal, Anda harus menggunakan penerima di <i>muka</i> .  Baca lebih lanjut tentang ini di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Singkatnya, antarmuka <i>ForeachWriter</i> harus diimplementasikan.  Artinya, perlu untuk menentukan cara membuka koneksi, cara memproses setiap bagian data dan cara menutup koneksi pada akhir pemrosesan.  Kode sumber adalah sebagai berikut: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  Definisi <i>CassandraDriver</i> dan struktur tabel output akan saya jelaskan nanti, tetapi untuk sekarang, mari kita lihat lebih dekat bagaimana cara kerja kode di atas.  Untuk terhubung ke Kasandra dari Spark, saya membuat objek <i>CassandraDriver</i> yang menyediakan akses ke <i>CassandraConnector</i> , konektor yang dikembangkan oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DataStax</a> .  CassandraConnector bertanggung jawab untuk membuka dan menutup koneksi ke database, jadi saya hanya menampilkan pesan debug pada metode <i>buka</i> dan <i>tutup</i> dari kelas <i>CassandraSinkForeach</i> . <br><br>  Kode di atas dipanggil dari aplikasi utama sebagai berikut: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> dibuat untuk setiap baris data, sehingga setiap node yang bekerja memasukkan bagian dari baris ke dalam database.  Artinya, setiap simpul yang bekerja mengeksekusi <i>val cassandraDriver = new CassandraDriver ();</i>  Seperti inilah bentuk CassandraDriver: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Mari kita melihat lebih dekat pada objek <i>percikan</i> .  Kode untuk <i>SparkSessionBuilder adalah</i> sebagai berikut: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Pada setiap node yang berfungsi, <i>SparkSessionBuilder</i> menyediakan akses ke <i>SparkSession</i> yang dibuat pada driver.  Untuk memungkinkan akses seperti itu, perlu untuk membuat serial <i>SparkSessionBuilder</i> dan menggunakan <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">transient</a> lazy val</i> , yang memungkinkan sistem serialisasi untuk mengabaikan <i>conf</i> dan <i>spark</i> objek ketika program diinisialisasi dan sampai objek diakses.  Jadi, ketika program <i>buildSparkSession dimulai, ia</i> diserialisasi dan dikirim ke setiap simpul yang berfungsi, tetapi objek <i>conf</i> dan <i>spark</i> hanya diizinkan ketika simpul yang bekerja mengaksesnya. <br><br>  Sekarang mari kita lihat kode aplikasi utama: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Ketika aplikasi dikirim untuk dieksekusi, <i>buildSparkSession adalah</i> serial dan dikirim ke node yang bekerja, namun, <i>conf</i> dan <i>spark</i> objek tetap tidak terselesaikan.  Kemudian pengemudi membuat objek percikan di dalam <i>KafkaToCassandra</i> dan mendistribusikan pekerjaan antara node yang bekerja.  Setiap node yang bekerja membaca data dari Kafka, membuat transformasi sederhana pada bagian yang diterima dari catatan, dan ketika node yang bekerja siap untuk menulis hasilnya ke database, itu memungkinkan <i>conf</i> dan <i>spark</i> objek, sehingga mendapatkan akses ke <i>SparkSession yang</i> dibuat pada driver. <br><br><h3>  Bagaimana cara membangun dan menjalankan aplikasi? </h3><br>  Ketika saya pindah dari PySpark ke Scala, butuh beberapa saat untuk mencari cara membangun aplikasi.  Karena itu, saya memasukkan Maven <i>pom.xml</i> dalam proyek saya.  Pembaca dapat membangun aplikasi menggunakan Maven dengan menjalankan perintah <i>paket mvn</i> .  Setelah aplikasi dapat dikirim untuk dieksekusi menggunakan <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Untuk membangun dan menjalankan aplikasi, perlu mengganti nama mesin AWS saya dengan milik Anda (mis. Ganti semua yang tampak seperti ec2-xx-xxx-xx-xx.xx.compute-1.amazonaws.com). <br><br>  Spark dan Structured Streaming pada khususnya adalah topik baru bagi saya, jadi saya akan sangat berterima kasih kepada pembaca atas komentar, diskusi dan koreksi. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id425503/">https://habr.com/ru/post/id425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id425489/index.html">Otomasi: Ancaman Robot Berlebihan</a></li>
<li><a href="../id425493/index.html">Mengkonfigurasi MikroTik hAP mini untuk IPTV Beeline</a></li>
<li><a href="../id425497/index.html">Tutu PHP Meetup # 2: Siaran Acara Langsung</a></li>
<li><a href="../id425499/index.html">HyperX Impact DDR4 - SO-DIMM yang bisa! Atau mengapa di laptop memori 64 GB dengan frekuensi 3200 MHz?</a></li>
<li><a href="../id425501/index.html">Tes A / B di Android dari A hingga Z</a></li>
<li><a href="../id425505/index.html">Analisis proses boot kernel Linux</a></li>
<li><a href="../id425507/index.html">Parsim Wikipedia untuk tugas NLP dalam 4 tim</a></li>
<li><a href="../id425511/index.html">Fitur-fitur aplikasi Rotativa yang tidak jelas untuk menghasilkan PDF dalam aplikasi ASP.NET MVC</a></li>
<li><a href="../id425515/index.html">Apple memblokir perbaikan independen model MacBook baru</a></li>
<li><a href="../id425517/index.html">Bagaimana Yandex membuat perkiraan curah hujan global menggunakan radar dan satelit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>