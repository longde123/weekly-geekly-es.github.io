<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÖ ü§æüèª üò¶ C√≥mo implementamos el cach√© en la base de datos Tarantool üòô üà≤ üë©üèø‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Buen dia 

 Quiero compartir con ustedes una historia sobre la implementaci√≥n de cach√© en la base de datos Tarantool y mis caracter√≠sticas de trabajo....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo implementamos el cach√© en la base de datos Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Buen dia <br><br>  Quiero compartir con ustedes una historia sobre la implementaci√≥n de cach√© en la base de datos Tarantool y mis caracter√≠sticas de trabajo. <br>  Trabajo como desarrollador de Java en una empresa de telecomunicaciones.  La tarea principal: la implementaci√≥n de la l√≥gica de negocios para la plataforma que la empresa compr√≥ al proveedor.  De las primeras caracter√≠sticas, esto es trabajo de jab√≥n y la ausencia casi completa de almacenamiento en cach√©, excepto en la memoria JVM.  Por supuesto, todo esto es bueno hasta que el n√∫mero de instancias de la aplicaci√≥n exceda de dos docenas ... <br><br>  En el curso del trabajo y la aparici√≥n de una comprensi√≥n de las caracter√≠sticas de la plataforma, se hizo un intento de almacenar en cach√©.  En ese momento, MongoDB ya se lanz√≥, y como resultado, no obtuvimos ning√∫n resultado positivo como en la prueba. <br><br>  Al buscar alternativas y consejos de mi buen amigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">mr_elzor</a> , se decidi√≥ probar la base de datos Tarantool. <br><a name="habracut"></a><br>  En un estudio superficial, solo aparecieron dudas en lua, ya que no hab√≠a escrito sobre √©l desde la palabra "completamente".  Pero dejando de lado todas las dudas, se puso a instalar.  Acerca de las redes cerradas y los cortafuegos, creo que pocas personas est√°n interesadas, pero le aconsejo que trate de sortearlas y poner todo de fuentes p√∫blicas. <br><br>  Servidores de prueba con configuraci√≥n: 8 CPU, 16 GB de RAM, 100 Gb HDD, Debian 9.4. <br><br>  La instalaci√≥n se realiz√≥ de acuerdo con las instrucciones del sitio.  Y entonces obtuve una opci√≥n de ejemplo.  La idea apareci√≥ inmediatamente de una interfaz visual con la que el soporte funcionar√≠a convenientemente.  Durante una b√∫squeda r√°pida, encontr√© y configur√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tarantool-admin</a> .  Trabaja en Docker y cubre las tareas de soporte al 100%, al menos por ahora. <br><br>  Pero hablemos de m√°s interesante. <br><br>  El siguiente pensamiento fue configurar mi versi√≥n en la configuraci√≥n maestro - esclavo dentro del mismo servidor, ya que la documentaci√≥n contiene solo ejemplos con dos servidores diferentes. <br><br>  Despu√©s de pasar un tiempo entendiendo lua y describiendo la configuraci√≥n, lanzo el asistente. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Inmediatamente caigo en un estupor y no entiendo por qu√© es el error, pero veo que est√° en el estado de "carga". <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ‚óè tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service ‚îî‚îÄ20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Yo corro esclavo: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Y veo el mismo error.  Aqu√≠, generalmente empiezo a esforzarme y no entiendo lo que est√° sucediendo, ya que no hay nada en la documentaci√≥n al respecto ... Pero cuando verifico el estado, veo que no comenz√≥ en absoluto, aunque dice que el estado se est√° "ejecutando": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ‚óè tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Pero al mismo tiempo, el maestro comenz√≥ a trabajar: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Reiniciar el esclavo no ayuda.  Me pregunto por qu√© <br><br>  Yo detengo al maestro.  Y realizar las acciones en orden inverso. <br><br>  Veo que el esclavo est√° tratando de comenzar. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Comienzo el asistente y veo que no ha subido y, en general, ha cambiado al estado hu√©rfano, mientras que el esclavo generalmente ha ca√≠do. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Se vuelve a√∫n m√°s interesante. <br><br>  Veo en los registros del esclavo que incluso vio al maestro e intent√≥ sincronizar. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  Y el intento fue exitoso: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  Incluso comenz√≥: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Pero por razones desconocidas, perdi√≥ la conexi√≥n y cay√≥: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Intentar iniciar esclavo de nuevo no ayuda. <br><br>  Ahora elimine los archivos creados por las instancias.  En mi caso, elimino todo del directorio / var / lib / tarantool. <br><br>  Comienzo esclavo primero, y solo luego amo.  Y he aqu√≠ que ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  No encontr√© ninguna explicaci√≥n para este comportamiento, excepto como una "caracter√≠stica de este software". <br>  Esta situaci√≥n aparecer√° cada vez que su servidor se haya reiniciado por completo. <br><br>  Tras un an√°lisis m√°s detallado de la arquitectura de este software, resulta que est√° planeado usar solo una vCPU para una instancia y muchos m√°s recursos permanecen libres. <br><br>  En la ideolog√≠a de n vCPU, podemos criar al maestro y n-2 esclavos para leer. <br><br>  Dado que en el servidor de prueba 8 vCPU podemos elevar el maestro y 6 instancias para leer. <br>  Copio el archivo para esclavo, corrijo los puertos y ejecuto, es decir  Se agregan algunos esclavos m√°s. <br><br>  Importante!  Al agregar otra instancia, debe registrarla en el asistente. <br>  Pero primero debe iniciar un nuevo esclavo, y solo luego reiniciar el maestro. <br><br><h4>  Ejemplo </h4><br>  Ya ten√≠a una configuraci√≥n ejecut√°ndose con un asistente y dos esclavos. <br><br>  Decid√≠ agregar un tercer esclavo. <br><br>  Lo registr√© en el maestro y reinici√© el maestro primero, y esto es lo que vi: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Es decir  nuestro maestro se volvi√≥ solitario y la replicaci√≥n se vino abajo. <br><br>  Iniciar un nuevo esclavo ya no ayudar√° y dar√° como resultado un error: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Y en los registros vi una peque√±a entrada informativa: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Paramos al mago y comenzamos un nuevo esclavo.  Tambi√©n habr√° un error, como en el primer inicio, pero veremos que se est√° cargando. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Pero cuando inicia el maestro, el nuevo esclavo se bloquea y el maestro no va con el estado de ejecuci√≥n. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  En esta situaci√≥n, solo hay una salida.  Como escrib√≠ anteriormente, elimino archivos creados por instancias y primero ejecuto esclavos, y luego master. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Todo comenz√≥ con √©xito. <br><br>  As√≠ es como, mediante prueba y error, descubr√≠ c√≥mo configurar e iniciar la replicaci√≥n correctamente. <br><br>  Como resultado, se ensambl√≥ la siguiente configuraci√≥n: <br><br>  <i>2 servidores</i> <i><br></i>  <i>2 master.</i>  <i>Reserva caliente</i> <i><br></i>  <i>12 esclavos.</i>  <i>Todos estan activos.</i> <br><br>  En la l√≥gica de tarantool, se utiliz√≥ http.server <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://github.com/tarantool/">para</a> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://github.com/tarantool/">bloquear el</a> adaptador adicional (recuerde el proveedor, la plataforma y el jab√≥n) o sujetar la biblioteca a cada proceso comercial. <br><br>  Para evitar una discrepancia entre los maestros, en el equilibrador (NetScaler, HAProxy o cualquier otro favorito), establecemos la regla de reserva, es decir.  Las operaciones de inserci√≥n, actualizaci√≥n y eliminaci√≥n van solo al primer maestro activo. <br><br>  En este momento, el segundo simplemente replica los registros del primero.  Los esclavos mismos est√°n conectados al primer maestro especificado desde la configuraci√≥n, que es lo que necesitamos en esta situaci√≥n. <br><br>  En lua, se implementaron operaciones CRUD para clave-valor.  Por el momento, esto es suficiente para resolver el problema. <br><br>  En vista de las caracter√≠sticas de trabajar con soap, se implement√≥ un proceso de negocio proxy, en el que se estableci√≥ la l√≥gica de trabajar con una tar√°ntula a trav√©s de http. <br><br>  Si los datos clave est√°n presentes, se devuelven inmediatamente.  De lo contrario, se env√≠a una solicitud al sistema maestro y se almacena en la base de datos Tarantool. <br><br>  Como resultado, un proceso de negocio en pruebas procesa hasta 4k solicitudes.  En este caso, el tiempo de respuesta de la tar√°ntula es de ~ 1 ms.  El tiempo de respuesta promedio es de hasta 3 ms. <br><br>  Aqu√≠ hay alguna informaci√≥n de las pruebas: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Hubo 50 procesos de negocios que van a 4 sistemas maestros y almacenan datos en la memoria cach√©.  Duplicaci√≥n de informaci√≥n en pleno crecimiento en cada instancia.  Dado que Java ya ama la memoria ... la perspectiva no es la mejor. <br><br><h4>  Ahora </h4><br>  50 procesos de negocio solicitan informaci√≥n a trav√©s del cach√©.  Ahora la informaci√≥n de 4 instancias del asistente se almacena en un lugar y no se almacena en la memoria cach√© en cada instancia.  Fue posible reducir significativamente la carga en el sistema maestro, no hay duplicados de informaci√≥n y el consumo de memoria en instancias con l√≥gica empresarial ha disminuido. <br><br>  Un ejemplo del tama√±o del almacenamiento de informaci√≥n en la memoria de la tar√°ntula: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  Al final del d√≠a, estos n√∫meros pueden duplicarse, pero no hay una "reducci√≥n" en el rendimiento. <br><br>  En batalla, la versi√≥n actual crea 2k - 2.5k solicitudes por segundo de carga real.  El tiempo de respuesta promedio es similar a las pruebas de hasta 3 ms. <br><br>  Si observa htop en uno de los servidores con tarantool, veremos que se est√°n "enfriando": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  Resumen </h4><br>  A pesar de todas las sutilezas y matices de la base de datos Tarantool, puede lograr un gran rendimiento. <br><br>  Espero que este proyecto se desarrolle y estos momentos inc√≥modos se resuelvan. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/441686/">https://habr.com/ru/post/441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441676/index.html">C√≥mo hacer amigos PLUTO y HDSDR</a></li>
<li><a href="../441678/index.html">Juego de f√≠sica de tornados: c√≥mo se implementa la aerodin√°mica en Just Cause 4 (tr√°fico)</a></li>
<li><a href="../441680/index.html">Programa de conferencia Lua In Moscow 2019</a></li>
<li><a href="../441682/index.html">HyperX Fury 3D: SSD con un pedigr√≠ claro</a></li>
<li><a href="../441684/index.html">Predicciones: las nubes cambiar√°n 2019</a></li>
<li><a href="../441688/index.html">Los juegos cambian el mundo: c√≥mo Hellblade llama la atenci√≥n sobre los problemas de las personas con enfermedades mentales</a></li>
<li><a href="../441690/index.html">No necesita Blockchain: ocho casos de uso bien conocidos y por qu√© no funcionan</a></li>
<li><a href="../441692/index.html">¬øC√≥mo cubrir las pistas en la cadena de bloques? Nuestro concepto de mezclador de transacciones</a></li>
<li><a href="../441694/index.html">Por qu√© los "gr√°ficos de tr√°fico" mienten</a></li>
<li><a href="../441696/index.html">La historia de Cyrillic LiveJournal: c√≥mo la administraci√≥n rusa aplast√≥ el auge de los blogs en ruso</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>