<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåù üìê üòç AI, cours pratique. Vue d'ensemble des r√©seaux de neurones pour la classification d'images üéÖ ü•ß üîó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cet article fournit un aper√ßu th√©orique accessible des r√©seaux de neurones convolutifs (CNN) et explique leur application au probl√®me de classificatio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI, cours pratique. Vue d'ensemble des r√©seaux de neurones pour la classification d'images</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/415811/">  Cet article fournit un aper√ßu th√©orique accessible des r√©seaux de neurones convolutifs (CNN) et explique leur application au probl√®me de classification d'images. <br><br><img src="https://habrastorage.org/webt/_d/ve/hi/_dvehi4kbgauxndfn56s7-tmtku.jpeg"><br><a name="habracut"></a><br><h2>  <font color="#0071c5">Approche commune: pas d'apprentissage en profondeur</font> </h2><br>  Le terme "traitement d'image" se r√©f√®re √† une large classe de t√¢ches pour lesquelles les donn√©es d'entr√©e sont des images, et la sortie peut √™tre soit des images soit des ensembles de caract√©ristiques caract√©ristiques associ√©es.  Il existe de nombreuses options: classification, segmentation, annotation, d√©tection d'objets, etc. Dans cet article, nous examinons la classification des images, non seulement parce que c'est la t√¢che la plus simple, mais aussi parce qu'elle sous-tend de nombreuses autres t√¢ches. <br><br>  L'approche g√©n√©rale de la classification des images comprend les deux √©tapes suivantes: <br><br><ol><li>  G√©n√©ration de caract√©ristiques importantes de l'image. </li><li>  Classification d'une image en fonction de ses attributs. </li></ol><br>  La s√©quence d'op√©rations courante utilise des mod√®les simples tels que le Perceptron multicouche (MLP), la Machine √† vecteurs de support (SVM), la m√©thode k des voisins les plus proches et la r√©gression logistique en plus des fonctionnalit√©s cr√©√©es manuellement.  Les attributs sont g√©n√©r√©s √† l'aide de diverses transformations (par exemple, la d√©tection des niveaux de gris et des seuils) et des descripteurs, par exemple, l'histogramme des gradients orient√©s ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HOG</a> ) ou les transformations <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> (invariable √† l'√©chelle), et etc. <br><br>  La principale limitation des m√©thodes g√©n√©ralement accept√©es est la participation d'un expert qui choisit un ensemble et une s√©quence d'√©tapes pour g√©n√©rer des fonctionnalit√©s. <br><br>  Au fil du temps, il a √©t√© constat√© que la plupart des techniques de g√©n√©ration d'entit√©s peuvent √™tre g√©n√©ralis√©es √† l'aide de noyaux (filtres) - de petites matrices (g√©n√©ralement de taille 5 √ó 5), qui sont des convolutions des images originales.  La convolution peut √™tre consid√©r√©e comme un processus s√©quentiel en deux √©tapes: <br><br><ol><li> Passez le m√™me noyau fixe dans toute l'image source. </li><li>  √Ä chaque √©tape, calculez le produit scalaire du noyau et l'image d'origine √† l'emplacement actuel du noyau. </li></ol><br>  Le r√©sultat de la convolution de l'image et du noyau est appel√© une carte de caract√©ristiques. <br>  Une explication math√©matiquement plus rigoureuse est donn√©e dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chapitre pertinent du</a> livre r√©cemment publi√©, Deep Learning, par I. Goodfellow, I. Benjio et A. Courville. <br><br><img src="https://habrastorage.org/webt/kw/lm/rd/kwlmrdg1y8wsniz94riol_8fiie.png"><br>  <i>Processus de convolution du noyau (vert fonc√©) avec l'image d'origine (vert), √† la suite de quoi une carte des caract√©ristiques est obtenue (jaune).</i> <br><br>  Un exemple simple d'une transformation qui peut √™tre effectu√©e avec des filtres est le flou d'une image.  Prenez un filtre compos√© de toutes les unit√©s.  Il calcule la moyenne du voisinage d√©termin√©e par le filtre.  Dans ce cas, le quartier est une section carr√©e, mais il peut √™tre cruciforme ou autre.  La moyenne conduit √† la perte d'informations sur la position exacte des objets, brouillant ainsi l'image enti√®re.  Une explication intuitive similaire peut √™tre donn√©e pour tout filtre cr√©√© manuellement. <br><br><img src="https://habrastorage.org/webt/5t/ud/g7/5tudg7ebng4ocb6jdc1-1alsqpo.png"><br>  <i>Le r√©sultat de la convolution de l'image du b√¢timent de l'Universit√© Harvard avec trois noyaux diff√©rents.</i> <br><br><h2>  <font color="#0071c5">R√©seaux de neurones convolutifs</font> </h2><br>  L'approche convolutionnelle de la classification des images pr√©sente un certain nombre d'inconv√©nients importants: <br><br><ul><li>  Un processus en plusieurs √©tapes au lieu d'une s√©quence de bout en bout. </li><li>  Les filtres sont un excellent outil de g√©n√©ralisation, mais ce sont des matrices fixes.  Comment choisir les poids dans les filtres? </li></ul><br>  Heureusement, des filtres apprenants ont √©t√© invent√©s, qui sont le principe de base qui sous-tend CNN.  Le principe est simple: nous formerons les filtres appliqu√©s √† la description des images afin de remplir au mieux leur t√¢che. <br><br>  CNN n'a pas d'inventeur, mais l'un des premiers cas de leur application est LeNet-5 * dans l'ouvrage <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"</a> Gradient-based Learning Applied to Document Recognition" de I. LeCun et d'autres auteurs. <br><br>  CNN tue deux oiseaux avec une pierre: aucune d√©finition pr√©liminaire des filtres n'est n√©cessaire et le processus d'apprentissage devient de bout en bout.  Une architecture CNN typique se compose des parties suivantes: <br><br><ul><li>  Couches convolutives </li><li>  Sous-√©chantillonnage des couches </li><li>  Couches denses (enti√®rement connect√©es) </li></ul><br>  Examinons chaque partie plus en d√©tail. <br><br><h3>  <font color="#0071c5">Couches convolutives</font> </h3><br>  La couche convolutionnelle est le principal √©l√©ment structurel de CNN.  La couche convolutionnelle pr√©sente un ensemble de caract√©ristiques: <br>  <i>Connectivit√© locale (clairsem√©e)</i> .  Dans les couches denses, chaque neurone est connect√© √† chaque neurone de la couche pr√©c√©dente (ils √©taient donc appel√©s denses).  Dans la couche convolutionnelle, chaque neurone n'est connect√© qu'√† une petite partie des neurones de la couche pr√©c√©dente. <br><br><img src="https://habrastorage.org/webt/nl/1a/6t/nl1a6t8bvbzna0rv_y3qjv_gsua.png"><br>  <i>Un exemple de r√©seau neuronal unidimensionnel.</i>  <i>(√† gauche) Connexion de neurones dans un r√©seau dense typique, (√† droite) Caract√©risation de la connectivit√© locale inh√©rente √† la couche convolutionnelle.</i>  <i>Images prises de I. Goodfellow et d'autres par Deep Learning</i> <br><br>  <i>La taille de la zone √† laquelle le neurone est connect√©</i> est appel√©e taille du filtre (la longueur du filtre dans le cas de donn√©es unidimensionnelles, par exemple, des s√©ries chronologiques, ou la largeur / hauteur dans le cas de donn√©es bidimensionnelles, par exemple, des images).  Dans la figure de droite, la taille du filtre est 3. Les <i>poids avec lesquels la connexion</i> est √©tablie sont appel√©s un filtre (un vecteur dans le cas de donn√©es unidimensionnelles et une matrice pour des donn√©es bidimensionnelles).  <i>Le pas est la distance parcourue par le filtre sur les donn√©es</i> (dans la figure de droite, le pas est 1).  L'id√©e de connectivit√© locale n'est rien de plus qu'un noyau qui d√©place une √©tape.  Chaque neurone de niveau convolutionnel repr√©sente et met en ≈ìuvre une position sp√©cifique du noyau glissant le long de l'image d'origine. <br><br><img src="https://habrastorage.org/webt/nu/we/gf/nuwegftvmoigtcdz2mlt0xwmm5g.png"><br>  <i>Deux couches convolutives unidimensionnelles adjacentes</i> <br><br>  Une autre propri√©t√© importante est la <i>zone</i> dite de <i>susceptibilit√©</i> .  Il refl√®te le nombre de positions du signal d'origine que le neurone actuel peut ¬´voir¬ª.  Par exemple, la zone de susceptibilit√© de la premi√®re couche r√©seau, repr√©sent√©e sur la figure, est √©gale √† la taille du filtre 3, car chaque neurone est connect√© √† seulement trois neurones du signal d'origine.  Cependant, sur la deuxi√®me couche, la zone de susceptibilit√© est d√©j√† de 5, puisque le neurone de la deuxi√®me couche regroupe trois neurones de la premi√®re couche, dont chacun a une zone de susceptibilit√© 3. Avec une profondeur croissante, la zone de susceptibilit√© cro√Æt lin√©airement. <br><br>  <i>Param√®tres partag√©s</i> .  Rappelons que dans le traitement d'image classique, le m√™me noyau glissait sur l'image enti√®re.  La m√™me id√©e s'applique ici.  Nous fixons uniquement la taille du filtre de poids pour une couche et nous appliquerons ces poids √† tous les neurones de la couche.  Cela revient √† faire glisser le m√™me noyau sur toute l'image.  Mais la question peut se poser: comment apprendre quelque chose avec un si petit nombre de param√®tres? <br><br><img src="https://habrastorage.org/webt/ue/qo/gi/ueqogixaqstaotlmnlfnjfn3dtc.png"><br>  <i>Les fl√®ches fonc√©es repr√©sentent les m√™mes poids.</i>  <i>(√† gauche) MLP r√©gulier, o√π chaque facteur de pond√©ration est un param√®tre distinct, (√† droite) Un exemple de s√©paration des param√®tres, o√π plusieurs facteurs de pond√©ration indiquent le m√™me param√®tre de formation</i> <br><br>  <i>Structure spatiale</i> .  La r√©ponse √† cette question est simple: nous formerons plusieurs filtres en une seule couche!  Ils sont plac√©s parall√®lement les uns aux autres, formant ainsi une nouvelle dimension. <br><br>  Nous faisons une courte pause et expliquons l'id√©e pr√©sent√©e par l'exemple d'une image RVB bidimensionnelle de 227 √ó 227. Notez qu'il s'agit ici d'une image d'entr√©e √† trois canaux, ce qui signifie, en substance, que nous avons trois images d'entr√©e ou des donn√©es d'entr√©e tridimensionnelles. <br><br><img src="https://habrastorage.org/webt/ol/9r/ty/ol9rtyiz5s9btzfnldhf6bo_0wi.png"><br>  <i>La structure spatiale de l'image d'entr√©e</i> <br><br>  Nous consid√©rerons les dimensions des canaux comme la profondeur de l'image (notez que ce n'est pas la m√™me que la profondeur des r√©seaux de neurones, qui est √©gale au nombre de couches de r√©seau).  La question est de savoir comment d√©terminer le noyau pour ce cas. <br><br><img src="https://habrastorage.org/webt/l6/pt/be/l6ptberff4a-rbz81bq-uxa-10w.png"><br>  <i>Un exemple d'un noyau bidimensionnel, qui est essentiellement une matrice tridimensionnelle avec une mesure de profondeur suppl√©mentaire.</i>  <i>Ce filtre donne une convolution avec l'image;</i>  <i>c'est-√†-dire glisse sur l'image dans l'espace, calculant des produits scalaires</i> <br><br>  La r√©ponse est simple, mais pas encore √©vidente: nous allons √©galement rendre le noyau tridimensionnel.  Les deux premi√®res dimensions resteront les m√™mes (largeur et hauteur du noyau) et la troisi√®me dimension est toujours √©gale √† la profondeur des donn√©es d'entr√©e. <br><br><img src="https://habrastorage.org/webt/yu/e3/xz/yue3xziqupgpwtmqvtit8ik5hos.png"><br>  <i>Un exemple d'une √©tape de convolution spatiale.</i>  <i>Le r√©sultat du produit scalaire du filtre et d'une petite partie de l'image 5 √ó 5 √ó 3 (c'est-√†-dire 5 √ó 5 √ó 5 + 1 = 76, la dimension du produit scalaire + d√©calage) est un nombre</i> <br><br>  Dans ce cas, toute la section 5 √ó 5 √ó 3 de l'image d'origine est transform√©e en un seul num√©ro, et l'image tridimensionnelle elle-m√™me sera transform√©e en <i>une carte d'</i> <i>entit√©s</i> ( <i>carte d'activation</i> ).  Une carte des caract√©ristiques est un ensemble de neurones, chacun calculant sa propre fonction, en tenant compte de deux principes de base discut√©s ci-dessus: <i>la connectivit√© locale</i> (chaque neurone n'est associ√© qu'√† une petite partie des donn√©es d'entr√©e) et la <i>s√©paration des param√®tres</i> (tous les neurones utilisent le m√™me filtre).  Id√©alement, cette carte de caract√©ristiques sera la m√™me que celle d√©j√† rencontr√©e dans l'exemple d'un r√©seau g√©n√©ralement accept√© - elle stocke les r√©sultats de la convolution de l'image d'entr√©e et du filtre. <br><br><img src="https://habrastorage.org/webt/iw/4w/qr/iw4wqr77vslpfjqblrgtjlxyxkw.png"><br>  <i>Carte des entit√©s r√©sultant de la convolution du noyau avec toutes les positions spatiales</i> <br><br>  Notez que la profondeur de la carte d'entit√©s est de 1, car nous n'avons utilis√© qu'un seul filtre.  Mais rien ne nous emp√™che d'utiliser plus de filtres;  par exemple, 6. Tous interagiront avec les m√™mes donn√©es d'entr√©e et fonctionneront ind√©pendamment les uns des autres.  Allons un peu plus loin et combinons ces cartes de fonctionnalit√©s.  Leurs dimensions spatiales sont les m√™mes puisque les dimensions des filtres sont les m√™mes.  Ainsi, les cartes d'entit√©s collect√©es ensemble peuvent √™tre consid√©r√©es comme une nouvelle matrice tridimensionnelle, dont la dimension en profondeur est repr√©sent√©e par des cartes d'entit√©s provenant de diff√©rents noyaux.  En ce sens, les canaux RVB de l'image d'entr√©e ne sont autres que les trois cartes d'entit√©s originales. <br><br><img src="https://habrastorage.org/webt/c8/f1/23/c8f123tp1nnbklt5oimm4gkmwj4.png"><br>  <i>L'application parall√®le de plusieurs filtres √† l'image d'entr√©e et au jeu de cartes d'activation r√©sultant</i> <br><br>  Une telle compr√©hension des cartes d'entit√©s et de leur combinaison est tr√®s importante, car, apr√®s avoir r√©alis√© cela, nous pouvons √©tendre l'architecture du r√©seau et installer des couches convolutives les unes sur les autres, augmentant ainsi la zone de susceptibilit√© et enrichissant notre classificateur. <br><br><img src="https://habrastorage.org/webt/7_/3g/hp/7_3ghputtiaz-2l-hbbs7dagmvi.png"><br>  <i>Couches convolutives install√©es les unes sur les autres.</i>  <i>Dans chaque couche, la taille des filtres et leur nombre peuvent varier</i> <br><br>  Nous comprenons maintenant ce qu'est un r√©seau convolutionnel.  L'objectif principal de ces couches est le m√™me qu'avec l'approche g√©n√©ralement accept√©e - d√©tecter les signes significatifs de l'image.  Et, si dans la premi√®re couche ces signes peuvent √™tre tr√®s simples (pr√©sence de lignes verticales / horizontales), la profondeur du r√©seau augmente le degr√© de leur abstraction (pr√©sence d'un chien / chat / personne). <br><br><h3>  <font color="#0071c5">Sous-√©chantillonnage des couches</font> </h3><br>  Les couches convolutives sont le principal √©l√©ment constitutif de CNN.  Mais il y a une autre partie importante et souvent utilis√©e - ce sont les couches de sous-√©chantillons.  Dans le traitement d'image conventionnel, il n'y a pas d'analogue direct, mais un sous-√©chantillon peut √™tre consid√©r√© comme un autre type de noyau.  Qu'est-ce que c'est? <br><br><img src="https://habrastorage.org/webt/n4/fm/df/n4fmdf4o-i1pa7qs4w4oj7wmzgy.png"><br>  <i>Exemples de sous-√©chantillonnage.</i>  <i>(√† gauche) Comment un sous-√©chantillon modifie la taille spatiale (mais pas le canal!) des tableaux de donn√©es, (√† droite) Un sch√©ma de base du fonctionnement d'un sous-√©chantillon</i> <br><br>  Un sous-√©chantillon filtre une partie du voisinage de chaque pixel des donn√©es d'entr√©e avec une fonction d'agr√©gation sp√©cifique, par exemple, maximum, moyenne, etc. Le sous-√©chantillon est essentiellement le m√™me que la convolution, mais la fonction de combinaison de pixels n'est pas limit√©e au produit scalaire.  Une autre diff√©rence importante est que le sous-√©chantillonnage ne fonctionne que dans la dimension spatiale.  Une caract√©ristique de la couche de sous-√©chantillonnage est que le <i>pas est g√©n√©ralement √©gal √† la taille du filtre</i> (la valeur typique est 2). <br><br>  Un sous-√©chantillon a trois objectifs principaux: <br><br><ul><li>  Diminution de la dimension spatiale ou sous-√©chantillonnage.  Ceci est fait pour r√©duire le nombre de param√®tres. </li><li>  La croissance de la zone de sensibilit√©.  En raison du sous-√©chantillon de neurones dans les couches suivantes, davantage d'√©tapes du signal d'entr√©e sont accumul√©es </li><li>  Invariance translationnelle √† de petites h√©t√©rog√©n√©it√©s dans la position des motifs dans le signal d'entr√©e.  En calculant les statistiques d'agr√©gation de petits voisinages du signal d'entr√©e, un sous-√©chantillon peut ignorer les petits d√©placements spatiaux qu'il contient. </li></ul><br><h3>  <font color="#0071c5">Couches √©paisses</font> </h3><br>  Les couches convolutives et les couches de sous-√©chantillons ont le m√™me objectif - g√©n√©rer des attributs d'image.  La derni√®re √©tape consiste √† classer l'image d'entr√©e en fonction des caract√©ristiques d√©tect√©es.  Chez CNN, des couches denses au sommet du r√©seau le font.  Cette partie du r√©seau est appel√©e <i>classification</i> .  Il peut contenir plusieurs couches les unes sur les autres avec une connectivit√© compl√®te, mais se termine g√©n√©ralement par une couche de classe <i>softmax</i> activ√©e par une fonction d'activation logistique multi-variable, dans laquelle le nombre de blocs est √©gal au nombre de classes.  √Ä la sortie de cette couche se trouve la distribution de probabilit√© par classe pour l'objet d'entr√©e.  Maintenant, l'image peut √™tre class√©e en choisissant la classe la plus probable. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr415811/">https://habr.com/ru/post/fr415811/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr415795/index.html">√âcrire une interface utilisateur Snapchat sur Swift</a></li>
<li><a href="../fr415797/index.html">Expressions r√©guli√®res + programmation logique. Quel est le r√©sultat?</a></li>
<li><a href="../fr415801/index.html">Google: notre IA "t√©l√©phone" n'est pas assez bonne pour √™tre dangereuse</a></li>
<li><a href="../fr415805/index.html">Modification du module barri√®re GSM Doorhan pour le contr√¥le Internet</a></li>
<li><a href="../fr415809/index.html">Comment utiliser le soja, requirejs, backbone js dans les plugins pour Atlassian Jira</a></li>
<li><a href="../fr415813/index.html">Quelques notes sur l'√©tat actuel du Cloud Gaming</a></li>
<li><a href="../fr415815/index.html">√Ä la pointe de la science: une analyse des articles arxiv.org</a></li>
<li><a href="../fr415817/index.html">Nous overclockons la sauvegarde. Conf√©rence Yandex</a></li>
<li><a href="../fr415819/index.html">Rapport 2018 du Club de Rome, chapitre 3.16: Gouvernement mondial</a></li>
<li><a href="../fr415821/index.html">La fa√ßon d'organiser une maison "intelligente" avec la commande √©lectrique la plus large possible</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>