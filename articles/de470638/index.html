<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏻‍🏫 👸🏾 👍🏽 Quasi-Newtonsche Methoden oder wenn es zu viele zweite Ableitungen für Athos gibt 🕔 👩🏿‍🏫 👨🏼‍🎓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der ersten Bekanntschaft mit quasi-Newtonschen Methoden kann man zweimal überrascht sein. Erstens ergeben sich nach einem kurzen Blick auf die For...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Quasi-Newtonsche Methoden oder wenn es zu viele zweite Ableitungen für Athos gibt</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470638/">  Bei der ersten Bekanntschaft mit quasi-Newtonschen Methoden kann man zweimal überrascht sein.  Erstens ergeben sich nach einem kurzen Blick auf die Formeln Zweifel, dass dies überhaupt funktionieren kann.  Sie funktionieren jedoch.  Weiterhin scheint es zweifelhaft, dass sie gut funktionieren werden.  Und es ist umso überraschender zu sehen, wie viel schneller sie sind als die verschiedenen Variationen des Gefälles, nicht bei speziell konstruierten Aufgaben, sondern bei realen Aufgaben aus der Praxis.  Und wenn danach immer noch Zweifel mit Interesse vermischt sind, müssen Sie verstehen, warum dieses Etwas überhaupt funktioniert. <br><a name="habracut"></a><br>  Der Ursprung und die Grundideen, die Gradientenmethoden antreiben, einschließlich der Newton-Methode, wurden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bereits berücksichtigt</a> .  Wir haben uns nämlich auf die Informationen über das Verhalten der Funktion in der Nähe der aktuellen Position gestützt, was uns eine einfache mathematische Analyse ermöglicht.  Es wurde zumindest davon ausgegangen, dass uns Informationen zu den ersten Derivaten zur Verfügung standen.  Was ist, wenn dies alles ist, was uns zur Verfügung steht?  Ist Gradientenabstieg unser Satz?  Natürlich, ja, es sei denn, Sie erinnern sich plötzlich daran, dass es sich um einen <i>Prozess handelt,</i> bei dem die Zielfunktion ordnungsgemäß verarbeitet wird.  Und wenn ja, warum verwenden wir die gesammelten Informationen über das Verhalten der Funktion nicht, um unseren Gang auf ihrer Oberfläche etwas weniger blind zu machen? <br><br>  Die Idee, Informationen über den zurückgelegten Weg zu verwenden, steht im Mittelpunkt der meisten Möglichkeiten, die Abstiegsmethoden zu beschleunigen.  Dieser Artikel beschreibt eine der effektivsten, wenn auch nicht die billigste Art, diese Art von Informationen zu berücksichtigen, was zur Idee quasi-Newtonscher Methoden führt. <br><br>  Um zu verstehen, wo die Beine der quasi-Newtonschen Methoden wachsen und woher der Name kommt, müssen wir wieder zur Minimierungsmethode zurückkehren, die auf der direkten Lösung der stationären Punktgleichung basiert <img src="https://habrastorage.org/getpro/habr/post_images/c43/946/5f9/c439465f905d9f366a2f4b3296306290.gif" title="&quot;\ bigtriangledown f = 0&quot;">  .  So wie die Betrachtung der Newton-Methode, die auf die Lösung dieser Gleichung angewendet wurde, uns zu der gleichnamigen Optimierungsmethode führte (die im Gegensatz zu ihrem Vorläufer eine globale Konvergenzregion aufweist), können wir erwarten, dass die Berücksichtigung anderer Methoden zur Lösung von Systemen nichtlinearer Gleichungen fruchtbar sein wird Planen Sie Ideen für den Aufbau anderer Optimierungsmethoden. <br><br><h2>  Sekantenmethoden </h2><br>  Ich möchte Sie daran erinnern, dass die Newton-Methode zur Lösung des Gleichungssystems <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  , basiert auf dem Austausch in der Nähe eines Punktes in der Nähe der Lösung <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  die Funktionen <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  seine lineare Annäherung <img src="https://habrastorage.org/getpro/habr/post_images/d15/479/f23/d15479f235f0d60ce8837c9043a0d2cc.gif" title="L (p) = F (x) + J (x) p">  wo <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  Ist ein linearer Operator, der, wenn <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  ist ein Vektor und <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  hat partielle Ableitungen in Bezug auf jede Variable, fällt mit der Jacobi-Matrix zusammen <img src="https://habrastorage.org/getpro/habr/post_images/4d2/826/ff6/4d2826ff6ba22f9f67cab70bfbe17a16.gif" title="&quot;J_ {ij} = \ dfrac {\ partielle F_ {i}} {\ partielle x_ {j}}&quot;">  .  Als nächstes wird die Gleichung gelöst <img src="https://habrastorage.org/getpro/habr/post_images/c9d/8be/3f2/c9d8be3f2d70054db890ea34e3409544.gif" title="L (p) = 0">  und Punkt <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&quot;x '= x + p&quot;">  als neue Annäherung an die gewünschte Lösung genommen.  Es ist einfach und es funktioniert. <br><br>  Aber was ist, wenn wir aus irgendeinem Grund die Jacobi-Matrix nicht berechnen können?  Das erste, was uns in diesem Fall einfällt, ist, dass wir, wenn wir die partiellen Ableitungen nicht analytisch berechnen können, eine numerische Näherung für sie erhalten können.  Die einfachste (wenn auch keineswegs einzige) Option für eine solche Annäherung kann die Formel der richtigen endlichen Differenzen sein: <img src="https://habrastorage.org/getpro/habr/post_images/149/708/f5b/149708f5b8bab4374023295557622e82.gif" title="&quot;\ dfrac {\ partielle F_ {i}} {\ partielle x_ {j}} \ ca. \ dfrac {F_ {i} (x + h_ {j} e_ {j}) - F_ {i} (x)} { h_ {j}} &quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/459/e61/aa0/459e61aa08f7fe807167a596e7ebd8a9.gif" title="&quot;e_ {j}&quot;">  Ist der j-te Basisvektor.  Die aus solchen Näherungen zusammengesetzte Matrix wird mit bezeichnet <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  .  Eine Analyse, wie viel Ersatz <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  auf <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  In Newtons Methode wirkt sich die Konvergenz auf eine ziemlich große Anzahl von Werken aus, aber in diesem Fall interessieren wir uns für einen anderen Aspekt.  Eine solche Annäherung erfordert nämlich die Berechnung der Funktion an N zusätzlichen Punkten und zusätzlich der Funktion <img src="https://habrastorage.org/getpro/habr/post_images/6c4/afd/100/6c4afd1002ddcfa43d07afbc9f103a9d.gif" title="&quot;\ bar {L} (p) = F (x) + \ bar {J} p&quot;">  an diesen Punkten <i>interpoliert die</i> Funktion <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  d.h. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c56/5c9/4b4/c565c94b4a37b9cd5f42fc1be92b2e15.gif" title="&quot;\ bar {L (} h_ {j} e_ {j}) = F (x) + h_ {j} \ dfrac {F (x + h_ {j} e_ {j}) - F (x)} {h_ {j}} = F (x) + F (x + h_ {j} e_ {j}) - F (x) = F (x + h_ {j} e_ {j}). &quot;"><br><br>  Nicht jede Approximation der Jacobi-Matrix hat diese Eigenschaft, aber jede Matrix einer affinen Funktion, die diese Eigenschaft hat, ist eine Approximation der Jacobi-Matrix.  In der Tat, wenn <img src="https://habrastorage.org/getpro/habr/post_images/88f/5c8/dd7/88f5c8dd7e9876a2d0e0980882f261da.gif" title="&quot;F (x + p_ {j}) = F (x) + J (x) p_ {j} + o \ left (\ left \ Vert p_ {j} \ right \ Vert ^ {2} \ right)&quot;">  und <img src="https://habrastorage.org/getpro/habr/post_images/5ad/e1e/ad2/5ade1ead2804a3bfaa8ffdf9122a179a.gif" title="&quot;\ bar {J} p_ {j} = F (x + p_ {j}) - F (x)&quot;">  dann bei <img src="https://habrastorage.org/getpro/habr/post_images/803/ca4/351/803ca4351b87edf1a13a2a2947772fa7.gif" title="&quot;\ left \ Vert p_ {j} \ right \ Vert \ rightarrow0 \ quad \ bar {J} (x) p_ {j} \ rightarrow J (x) p_ {j}&quot;">  .  Diese Eigenschaft, nämlich die Interpolationseigenschaft, gibt uns eine konstruktive Möglichkeit, die Newton-Methode zu verallgemeinern. <br><br>  Lass <img src="https://habrastorage.org/getpro/habr/post_images/194/ad1/d42/194ad1d42aa4320679b9498748ceb78d.gif" title="&quot;\ bar {L} (p) = a + Ap&quot;">  - Funktion, die die Anforderung erfüllt <img src="https://habrastorage.org/getpro/habr/post_images/06e/34e/3a7/06e34e3a7a0d0058ef351da74258a637.gif" title="&quot;\ bar {L} (p_ {i}) = F (x + p_ {i})&quot;">  für ein System linear unabhängiger Vektoren <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  .  Dann wird eine solche Funktion als <i>Sekantenfunktion</i> bezeichnet <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  und die Gleichung, die es definiert, ist <i>die Sekantengleichung</i> .  Wenn das System der Vektoren <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  ist vollständig (das heißt, es gibt genau N von ihnen und sie sind immer noch linear unabhängig), und zusätzlich das Vektorsystem <img src="https://habrastorage.org/getpro/habr/post_images/94f/cf5/579/94fcf55798902795ffb670e35359d2af.gif" title="&quot;\ left \ {F (x + p_ {i}), i = 1 \ dots N \ right \}&quot;">  dann linear unabhängig <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  eindeutig definiert. <br><br>  Jede Methode, die auf einer lokalen Änderung der Gleichung basiert <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  Gleichung der Form <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  erfüllt <i>die Sekantengleichung</i> , die als <i>Sekantenmethode bezeichnet wird</i> . <br><br>  Es stellt sich die Frage, wie der Sekant für eine Funktion auf rationellste Weise konstruiert werden kann. <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Die folgende Argumentation scheint offensichtlich: Lassen Sie am Punkt x ein affines Modell konstruieren, das die gegebene Funktion an den Punkten interpoliert <img src="https://habrastorage.org/getpro/habr/post_images/462/bcf/f32/462bcff32469c0ec5f8ccfc80534c05c.gif" title="&quot;x-x_ {1}, x-x_ {2}, \ dots, x-x_ {N}&quot;">  .  Gleichungslösung <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  gibt uns einen neuen Punkt <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&quot;x '= x + p&quot;">  .  Dann, um an einem Punkt ein affines Modell zu erstellen <img src="https://habrastorage.org/getpro/habr/post_images/787/cf7/c3a/787cf7c3a3d374114b3a07305b7fa446.gif" title="&amp; quot; x '&amp; quot;">  Es ist am sinnvollsten, Interpolationspunkte so zu wählen, dass der Wert <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  bereits bekannt - das heißt, nehmen Sie sie aus dem Set <img src="https://habrastorage.org/getpro/habr/post_images/333/297/225/33329722533f0b608b0994d2a5ba83fa.gif" title="&quot;\ left \ {x'-x, x'-x_ {1}, x'-x_ {2}, \ dots, x'-x_ {N} \ right \}&quot;">  .  Es gibt verschiedene Optionen, für die Sie aus den vielen zuvor verwendeten Punkten auswählen können.  Sie können beispielsweise diejenigen als Interpolationspunkte verwenden, in denen <img src="https://habrastorage.org/getpro/habr/post_images/bb3/e3a/cd5/bb3e3acd5043b859fe89006d4cabe5a0.gif" title="&quot;\ left \ Vert F \ right \ Vert&quot;">  zählt am wenigsten oder nur am ersten <img src="https://habrastorage.org/getpro/habr/post_images/055/8e9/3d9/0558e93d918ff32e873b6a71703e9969.gif" title="&quot;N&quot;">  Punkte.  Auf jeden Fall scheint es offensichtlich, dass <img src="https://habrastorage.org/getpro/habr/post_images/95f/756/92b/95f75692ba0aeefcef24ae42714dbc1b.gif" title="&quot;p = x'-x&quot;">  sollte in vielen Interpolationspunkten für das neue affine Modell enthalten sein.  Also darüber hinaus <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  Schritte des iterativen Prozesses in unserem Set können bis zu sein <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  Verschiebungen, die auf zuvor übergebenen Punkten aufgebaut sind.  Wenn der Prozess so aufgebaut ist, dass das neue affine Modell nicht mehr verwendet <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  Von den vorherigen Werten wird ein solcher Prozess als p-Punkt-Sekantenmethode bezeichnet. <br><br>  Auf den ersten Blick scheint die N-Punkt-Sekantenmethode der beste Kandidat für die Rolle des Ersetzens der Newton-Methode zu sein, da sie die Informationen, die wir beim Lösen erhalten, maximal nutzt und gleichzeitig die Anzahl zusätzlicher Berechnungen minimiert - wir verwenden den Wert der Funktion in letzterer N Punkte bestanden.  Dies ist leider nicht so.  Die Sache ist, dass das Vektorsystem <img src="https://habrastorage.org/getpro/habr/post_images/ed0/117/8ca/ed01178ca46506fa4588780d16d705a1.gif" title="F (x_ {0}), F (x_ {1}), \ Punkte F (x_ {N})">  weigert sich hartnäckig, linear unabhängig mit einem ausreichend großen N zu sein. Selbst wenn sich herausstellt, dass diese Bedingung erfüllt ist und das entsprechende affine Modell noch existiert, besteht die Möglichkeit, dass die Richtungen <img src="https://habrastorage.org/getpro/habr/post_images/602/ff2/50c/602ff250c473d5b28e08a1453d4175b3.gif" title="&quot;p_ {j} = x_ {j} -x_ {0}&quot;">  erweisen sich auch als linear unabhängig, es stellt sich noch weniger heraus.  Und dies beinhaltet die Tatsache, dass das affine Modell, obwohl es existiert, entartet und praktisch ungeeignet ist. <br><br>  Im Allgemeinen ist die 2-Punkt-Sekantenmethode am stabilsten.  Das heißt, eine Methode, bei der bei jeder Iteration zusätzliche N-1-Werte der Funktion berechnet werden müssen.  Dies ist eindeutig nicht für unsere praktischen Zwecke geeignet. <br><br>  Dann ist die Frage - was war das alles? <br><br><h2>  Quasi-Newtonsche Methoden zur Lösung von Gleichungen </h2><br><br>  Der Ausweg ist einfach, wenn auch nicht offensichtlich.  Wenn wir nicht die technische Fähigkeit haben, basierend auf den bereits berechneten Werten das affine Modell, das die Sekantengleichung erfüllt, eindeutig zu bestimmen, ist dies nicht erforderlich.  Wir nehmen die Sekantengleichung als Grundlage, aber wir werden verlangen, dass sie nur für ein unvollständiges Vektorsystem erfüllt ist <img src="https://habrastorage.org/getpro/habr/post_images/e5e/f2b/432/e5ef2b43292735aa2a68afffb80bf520.gif" title="&quot;\ left \ {p_ {1}, p_ {2}, \ dots, p_ {m} \ right \}, m &amp; lt; N&quot;">  .  Mit anderen Worten, wir werden verlangen, dass die Interpolationsbedingung nur für eine ausreichend kleine Anzahl bekannter Werte erfüllt ist.  In diesem Fall können wir natürlich nicht mehr garantieren, dass die in einem solchen Modell verwendete Matrix zur Jacobi-Matrix tendiert, aber wir werden dies nicht benötigen.  Hinzu kommt, dass das affine Modell die Funktion am aktuellen Punkt interpolieren muss, d. H. <img src="https://habrastorage.org/getpro/habr/post_images/3b9/9d1/7fa/3b99d17fa378aeaf36097faef3830bd5.gif" title="&quot;\ bar {L} (0) = F (x)&quot;">  erhalten wir die folgende Formulierung der Sekantenmethode: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/78c/f30/421/78cf304219a4bc90d4f900062bf2d027.gif" title="&quot;\\ \ bar {L} (p_ {i}) = F (x) + Ap_ {i} = F (x + p_ {i}), \ quad i = 1 \ Punkte m \\ \ bar {L} (p) = 0 \ quad \ Rightarrow p = A ^ {- 1} F (x)"><br><br>  Bruiden war der erste, der Methoden dieser Art für m = 1 in Betracht zog und sie quasi-Newtonsch nannte.  Es ist klar, dass die Sekantenbedingung in diesem Fall es uns ermöglicht, die Matrix eindeutig zu identifizieren <img src="https://habrastorage.org/getpro/habr/post_images/c9d/999/d9a/c9d999d9a4e8bd3d6f8e50519d1dfaa8.gif" title="&quot;A&quot;">  nur wenn ihm zusätzliche Bedingungen auferlegt werden und jede dieser zusätzlichen Bedingungen zu einer eigenen Methode führt.  Bruyden selbst argumentierte wie folgt: <br><br>  <i>als die Bewegung in die Richtung</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>von Punkt</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;"></i>  <i>auf den Punkt</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/1d0/56f/301/1d056f3016bc715aacc23418d8629173.gif" title="&quot;x_ {1}&quot;"></i>  <i>gibt uns keine zusätzlichen Informationen darüber, wie sich die Funktion in anderen als ändert</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>Richtungen, dann die Wirkung der neuen affinen Funktion auf den Vektor</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>sollte sich von der Wirkung der alten Funktion auf denselben Vektor unterscheiden, je weniger desto unterschiedlicher</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>von</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>.</i>  <i>Als letztes Mittel, wenn</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>orthogonal</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>sollte sich das Verhalten der neuen Funktion nicht vom Verhalten der alten unterscheiden.</i> <i><br></i> <br>  Breidens Idee ist in ihrer Einfachheit brillant.  Wenn wir keine neuen Informationen über das Verhalten der Funktion haben, können wir am besten versuchen, die alte nicht zu beschmutzen.  Dann die zusätzliche Bedingung <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b3/d43/d20/0b3d43d207b144b926274d0c81abccbf.gif" title="&quot;\ bar {L} _ {1} q = \ bar {L} _ {0} q&quot;">  für alle <img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q">  so dass <img src="https://habrastorage.org/getpro/habr/post_images/c16/9f6/315/c169f6315171249a34b50b26a2975c6e.gif" title="q ^ {T} p = 0"><br><br>  Mit dieser Option können Sie die Matrix der neuen Transformation eindeutig bestimmen. Sie wird durch Hinzufügen einer Rang 1-Korrektur zur alten Matrix erhalten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/522/f36/1f9/522f361f94a7b5a2e9da68094983b21d.gif" title="&quot;\\ A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T}} {p ^ {T} p} \\ y = F (x_ {0}) -F (x_ {1}) &quot;"><br><br>  Trotz der Einfachheit und Konsistenz der Schlussfolgerungen von Bruiden bieten sie jedoch nicht den Dreh- und Angelpunkt, der als Grundlage für die Konstruktion anderer ähnlicher Methoden dienen könnte.  Glücklicherweise gibt es einen formelleren Ausdruck seiner Idee.  Die Matrix ist nämlich auf diese Weise aufgebaut <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  Es stellt sich als Lösung für das folgende Problem heraus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/028/96f/3fa/02896f3facb898d70f26abad02fe90a9.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ F (x_ {1}) - Ap = F (x_ {0})&quot;"><br><br>  Die Aufgabenbeschränkung ist nichts anderes als die Sekantengleichung, und die Minimierungsbedingung spiegelt unseren Wunsch wider, so viele Informationen wie möglich in der Matrix zu speichern <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  .  Das Maß für die Diskrepanz zwischen den Matrizen ist in diesem Fall die Frobenius-Norm, in der das gestellte Problem eine eindeutige Lösung hat.  Diese Formulierung kann durchaus als Ausgangspunkt für die Konstruktion anderer Methoden dienen.  Wir können nämlich sowohl das <i>Maß</i> ändern <i>,</i> mit dem wir die eingeführten Änderungen bewerten, als auch die der Matrix auferlegten <i>Bedingungen</i> verschärfen.  Im Allgemeinen kann man bereits mit einer solchen Formulierung der Methode arbeiten. <br><br><h2>  Quasi-Newton-Optimierungsmethoden </h2><br><br>  Nachdem wir die Hauptidee verstanden haben, können wir endlich zu Optimierungsproblemen zurückkehren und feststellen, dass die Anwendung der Bruyden-Formel zur Neuberechnung des affinen Modells nicht sehr gut zu unserer Aufgabe passt.  Tatsächlich ist die erste Ableitung der Gradientenfunktion <img src="https://habrastorage.org/getpro/habr/post_images/6b8/82e/be7/6b882ebe727121dcb5fc21b091044b5a.gif" title="&quot;\ bigtriangledown f&quot;">  es gibt nichts anderes als die hessische Matrix, die konstruktionsbedingt symmetrisch ist.  Gleichzeitig führt die Aktualisierung nach der Bruyden-Regel zu einer asymmetrischen Matrix <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  auch wenn <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  war symmetrisch.  Dies bedeutet nicht, dass die Bruden-Methode nicht zur Lösung der stationären Punktgleichung angewendet werden kann, aber basierend auf einer solchen Aktualisierungsregel ist es unwahrscheinlich, dass wir gute Optimierungsmethoden konstruieren können.  Im Allgemeinen ist es ziemlich offensichtlich, dass die Quasi-Newton-Methode umso besser funktionieren sollte, je genauer das System der Bedingungen des Problems die Besonderheiten einer bestimmten Jacobi-Matrix beschreibt. <br><br>  Um diesen Nachteil zu beheben, fügen wir dem Broyden-Minimierungsproblem eine zusätzliche Einschränkung hinzu, die ausdrücklich erfordert, dass die neue Matrix zusammen mit der alten symmetrisch ist: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/03e/167/aa2/03e167aa25e0f4aa6b8df8546552e79a.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0} ) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Die Lösung für dieses Problem ist <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df8/356/74b/df835674b94bab190bca3c18efed98ce.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T} + p (y-A_ {0} p) ^ {T}} {p ^ {T} p} - \ dfrac {(y-A_ {0} p) ^ {T} p} {\ left (p ^ {T} p \ right) ^ {2}} pp ^ {T} &quot;"><br><br>  Hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/89c/f84/f29/89cf84f292ed72d1b20755677688a054.gif" title="y = \ bigtriangledown f (x_ {1}) - \ bigtriangledown f (x_ {0})"></a>  und die Matrix-Neuberechnungsformel ist nach ihren Erstellern benannt - Powell, Shanno und Bruyden (PSB).  Die resultierende Matrix ist symmetrisch, aber eindeutig nicht eindeutig positiv, wenn auch nur plötzlich <img src="https://habrastorage.org/getpro/habr/post_images/6c7/040/47d/6c704047d3148fd7a8b563aaf79dd7f4.gif" title="&quot;y&quot;">  wird nicht kollinear sein <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  .  Und wir haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gesehen,</a> dass positive Sicherheit bei Optimierungsmethoden sehr wünschenswert ist. <br><br>  Wieder werden wir den Zustand des Problems korrigieren, indem wir diesmal die skalierte Frobenius-Norm als Maß für die Matrixdivergenz verwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f37/eb1/0f4/f37eb10f4eb10d0c54acc9adab962f10.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} -A_ {0} \ right) T ^ {- 1} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Der Ursprung einer solchen Aussage der Frage ist ein separates großes Thema, aber es ist interessant, dass, wenn die Matrix T so ist, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/673/872/131/673872131fa6cb0f44e6839be0e448e7.gif" title="T ^ {T} T = G, Gp = y"></a>  (das heißt, G ist auch eine affine Transformationsmatrix, die die Sekantengleichung für die Richtung p erfüllt), dann stellt sich heraus, dass die Lösung dieses Problems unabhängig von der Wahl von T ist und zur Aktualisierungsformel führt <br><br><img src="https://habrastorage.org/getpro/habr/post_images/135/ea6/c14/135ea6c14ea8f63f961e83576f1be5d5.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) y ^ {T} + y (y-A_ {0} p) ^ {T}} {y ^ {T} p} - \ dfrac {\ left (y-A_ {0} p \ right) ^ {T} p} {\ left (y ^ {T} p \ right) ^ {2}} yy ^ {T} &quot;"><br><br>  bekannt als die Davidon-Fletcher-Powell-Formel.  Diese Aktualisierungsmethode hat sich in der Praxis bewährt, da sie die folgende Eigenschaft aufweist: <br><br>  <i>wenn</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/e3e/c44/1c1/e3ec441c17e1b43df108a7d8e15d3dd6.gif" title="y ^ {T} p &amp; gt; 0"></i>  <i>und</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;"></i>  <i>positiv definitiv dann</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;"></i>  <i>auch positiv identifiziert.</i> <br><br>  Ich stelle danach fest, dass, wenn die erste Bedingung nicht erfüllt ist, keine affine Funktion mit einer positiven bestimmten Matrix existiert, die die Sekantengleichung erfüllt. <br><br>  Wenn wir in dem Problem, das zur DFP-Methode führt, als Maß für die Diskrepanz affiner Modelle den Abstand nicht zwischen den Matrizen selbst, sondern zwischen den zu ihnen inversen Matrizen nehmen, erhalten wir ein Problem <br><br><img src="https://habrastorage.org/getpro/habr/post_images/337/0b0/af2/3370b0af216ab9695789eeb586cf3604.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} ^ {- 1} -A_ {0} ^ {- 1} \ right) T ^ {- 1} \ right \ Vert _ { F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Seine Lösung ist eine bekannte Formel, die fast gleichzeitig von Breiden, Fletcher, Goldfarb und Shanno (BFGS) entdeckt wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/840/c7b/3fa840c7b6ec3de81eb02bb0e9240722.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {yy ^ {T}} {y ^ {T} p} - \ dfrac {A_ {0} pp ^ {T} A_ {0}} {p ^ { T} A_ {0} p} &quot;"><br><br>  Bisher wird angenommen, dass eine Neuberechnung nach dieser Formel aus rechnerischer Sicht am effizientesten ist und gleichzeitig weniger anfällig für eine Degeneration der Matrix mit einer großen Anzahl von Iterationen ist.  Unter den gleichen Bedingungen wie DFP bewahrt diese Formel die Eigenschaft der positiven Bestimmtheit. <br><br>  Alle beschriebenen Methoden zum Aktualisieren der Matrix erfordern eine Korrektur von Rang 2. Dies macht es einfach und leicht, die Matrix zu invertieren <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  unter Verwendung der Sherman-Morrison-Formel und des Wertes <img src="https://habrastorage.org/getpro/habr/post_images/5f6/3ac/2d9/5f63ac2d91f47a730fee01b5db38f3bd.gif" title="&quot;A_ {0} ^ {- 1}&quot;">  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed0/9f8/002/ed09f80027e56f58a3502cc943758509.gif" title="&quot;B_ {1} = B_ {0} + uv ^ {T} \ Rightarrow B_ {1} ^ {- 1} = B_ {0} ^ {- 1} + \ dfrac {B_ {0} ^ {- 1} uv ^ {T} B_ {0} ^ {- 1}} {1 + v ^ {T} B_ {0} ^ {- 1} u} &quot;"><br><br>  vorausgesetzt, der Nenner der Formel ist ungleich Null.  Ich werde keine spezifischen Formeln zum Aktualisieren der inversen Matrizen der aufgelisteten Methoden angeben, da sie leicht zu finden oder unabhängig voneinander abzuleiten sind.  Das einzige, was in diesem Fall beachtet werden sollte, ist, dass Varianten von Methoden mit Aktualisierung der inversen Matrix normalerweise viel weniger stabil sind (das heißt, sie leiden mehr unter Rundungsfehlern) als diejenigen, die eine Aktualisierung der ursprünglichen Matrix vorschlagen.  Es ist am effektivsten, nicht die Matrix selbst, sondern ihre Cholesky-Zerlegung zu aktualisieren (es sei denn natürlich, eine solche Zerlegung findet statt), da eine solche Implementierungsoption numerisch stabiler ist und außerdem die Kosten für die Lösung einer Gleichung minimiert, die die Bewegungsrichtung bestimmt. <br><br>  Es bleibt die Frage zu prüfen, wie die allererste Matrix im quasi-Newtonschen Prozess aussehen soll.  Hier ist alles offensichtlich - je näher es der hessischen Matrix oder ihrer korrigierten Version ist, wenn sich das hessische plötzlich nicht als positiv definitiv herausstellt, desto besser wird es unter dem Gesichtspunkt der Konvergenz sein.  Grundsätzlich kann jedoch jede positive definitive Matrix für uns geeignet sein.  Die einfachste Version einer solchen Matrix ist eine einzelne, und dann fällt die erste Iteration mit der Iteration des Gradientenabfalls zusammen.  Fletcher und Powell zeigten (natürlich für die DFP-Methode), dass wenn die quadratische Funktion minimiert wird, unabhängig davon, welche (positiv definierte) Matrix als anfängliche DFP-Iteration verwendet wird, sie zu einer Lösung in genau N Iterationen führen, wobei N ist Dimension des Problems, und die quasi-Newtonsche Matrix fällt mit der hessischen Matrix am minimalen Punkt zusammen.  Im allgemeinen nichtlinearen Fall eines solchen Glücks werden wir natürlich nicht warten, aber dies gibt zumindest Anlass, sich nicht zu viele Sorgen über die schlechte Wahl der Ausgangsmatrix zu machen. <br><br><h2>  Fazit </h2><br><br>  Der beschriebene Ansatz zur Konstruktion quasi-Newtonscher Methoden ist nicht der einzig mögliche.  Zumindest kamen die Entdecker der beschriebenen quasi-Newtonschen Methoden und viele nachfolgende Forscher aufgrund völlig unterschiedlicher Überlegungen zu denselben Formeln.  Es ist jedoch interessant, dass, sobald eine bestimmte quasi-Newtonsche Methode auftauchte, nach relativ kurzer Zeit klar wurde, dass es sich um eine Lösung für ein sehr leicht zu interpretierendes Optimierungsproblem handelt.  Meiner Meinung nach ist es bemerkenswert, dass es möglich ist, einen gemeinsamen Nenner für so unterschiedliche Methoden zu liefern, da dies die Grundlage für die Konstruktion anderer Methoden bildet, die die Besonderheiten einer bestimmten Aufgabe besser berücksichtigen.  Insbesondere gibt es quasi-Newtonsche Methoden zur Aktualisierung spärlicher Matrizen, Methoden, bei denen so wenig Elemente wie möglich geändert werden, und viele andere wären eine Fantasie. <br><br>  Es sollte auch beachtet werden, dass die Methoden variabler Metriken trotz ihres Namens nicht immer zur Konstruktion von Matrizen führen, die tatsächlich Metriken sind, obwohl sie dies jedes Mal tun, wenn es überhaupt möglich ist.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist normalerweise kein großes Problem, aber diejenigen, die sich vor einer möglichen Verlegenheit schützen möchten, greifen möglicherweise auf dieselben Tricks zurück, die zur Überwindung eines ähnlichen Problems mit der Newton-Methode angewendet wurden - beispielsweise durch Richtungsänderung oder Anwendung </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des Levenberg-Marquardt-Schemas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . In diesem Fall werden zwar Fragen der Wahl der Form einer vertrauensvollen Region wieder relevant, aber hier ist es notwendig, das kleinere Übel zu wählen. Eine andere Lösung des Problems besteht darin, lineare Suchmethoden zu verwenden, um sicherzustellen, dass die notwendigen Bedingungen zur Aufrechterhaltung einer positiven Sicherheit erfüllt sind. Die Wolfe-Regel garantiert die Erfüllung dieser Bedingung, während die Armijo- und Goldstein-Regeln dies nicht tun.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Theoretisch ist es fast unmöglich zu bestimmen, welche der vielen möglichen quasi-Newtonschen Methoden in Bezug auf eine bestimmte Klasse von Problemen am effektivsten ist. Normalerweise beschränken sie sich bei der Formulierung einer Methode darauf, ihre Wirksamkeit bei der Minimierung einer quadratischen Funktion zu zeigen (eine Methode wird übrigens als effektiv angesehen, wenn sie zu einer exakten Lösung in N Iterationen führt, dh nicht langsamer als direkte Methoden zur Lösung von SLAEs). In selteneren Fällen kann man Studien zur Konvergenzreihenfolge der Methode (die normalerweise superlinear ist, dh deutlich besser als die Gradientenabnahme), zur Stabilität und zu anderen interessanten Merkmalen finden. Im Allgemeinen ist das einzig vernünftige Kriterium für die Beurteilung der Wirksamkeit einer bestimmten Methode für eine bestimmte Aufgabenklasse die Praxis.Also Schaufeln in der Hand - und Erfolg bei der Anwendung.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470638/">https://habr.com/ru/post/de470638/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470620/index.html">Infrastruktur als Code: So überwinden Sie Probleme mit XP</a></li>
<li><a href="../de470622/index.html">Übersicht über Methoden zur Merkmalsauswahl</a></li>
<li><a href="../de470628/index.html">Raumschiff Simulator Schiffbau</a></li>
<li><a href="../de470632/index.html">Arend - HoTT-basierte abhängige Typensprache (Teil 2)</a></li>
<li><a href="../de470634/index.html">Identifizieren Sie Community-übergreifend auf Instagram, um Benutzerinteressen zu identifizieren</a></li>
<li><a href="../de470640/index.html">Dimensionierung Elasticsearch</a></li>
<li><a href="../de470642/index.html">Treffen Sie Yandex.Station Mini. Große Geschichte eines kleinen Geräts</a></li>
<li><a href="../de470646/index.html">Mathematik für Data Science. Neuer Kurs von OTUS</a></li>
<li><a href="../de470648/index.html">IBM LTO-8 - Einfache Möglichkeit zum Speichern kalter Daten</a></li>
<li><a href="../de470650/index.html">Datenaufbereitung in einem Data Science-Projekt: Rezepte für junge Hausfrauen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>