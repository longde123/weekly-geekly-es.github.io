<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüè´ üë∏üèæ üëçüèΩ Quasi-Newtonsche Methoden oder wenn es zu viele zweite Ableitungen f√ºr Athos gibt üïî üë©üèø‚Äçüè´ üë®üèº‚Äçüéì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei der ersten Bekanntschaft mit quasi-Newtonschen Methoden kann man zweimal √ºberrascht sein. Erstens ergeben sich nach einem kurzen Blick auf die For...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Quasi-Newtonsche Methoden oder wenn es zu viele zweite Ableitungen f√ºr Athos gibt</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470638/">  Bei der ersten Bekanntschaft mit quasi-Newtonschen Methoden kann man zweimal √ºberrascht sein.  Erstens ergeben sich nach einem kurzen Blick auf die Formeln Zweifel, dass dies √ºberhaupt funktionieren kann.  Sie funktionieren jedoch.  Weiterhin scheint es zweifelhaft, dass sie gut funktionieren werden.  Und es ist umso √ºberraschender zu sehen, wie viel schneller sie sind als die verschiedenen Variationen des Gef√§lles, nicht bei speziell konstruierten Aufgaben, sondern bei realen Aufgaben aus der Praxis.  Und wenn danach immer noch Zweifel mit Interesse vermischt sind, m√ºssen Sie verstehen, warum dieses Etwas √ºberhaupt funktioniert. <br><a name="habracut"></a><br>  Der Ursprung und die Grundideen, die Gradientenmethoden antreiben, einschlie√ülich der Newton-Methode, wurden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bereits ber√ºcksichtigt</a> .  Wir haben uns n√§mlich auf die Informationen √ºber das Verhalten der Funktion in der N√§he der aktuellen Position gest√ºtzt, was uns eine einfache mathematische Analyse erm√∂glicht.  Es wurde zumindest davon ausgegangen, dass uns Informationen zu den ersten Derivaten zur Verf√ºgung standen.  Was ist, wenn dies alles ist, was uns zur Verf√ºgung steht?  Ist Gradientenabstieg unser Satz?  Nat√ºrlich, ja, es sei denn, Sie erinnern sich pl√∂tzlich daran, dass es sich um einen <i>Prozess handelt,</i> bei dem die Zielfunktion ordnungsgem√§√ü verarbeitet wird.  Und wenn ja, warum verwenden wir die gesammelten Informationen √ºber das Verhalten der Funktion nicht, um unseren Gang auf ihrer Oberfl√§che etwas weniger blind zu machen? <br><br>  Die Idee, Informationen √ºber den zur√ºckgelegten Weg zu verwenden, steht im Mittelpunkt der meisten M√∂glichkeiten, die Abstiegsmethoden zu beschleunigen.  Dieser Artikel beschreibt eine der effektivsten, wenn auch nicht die billigste Art, diese Art von Informationen zu ber√ºcksichtigen, was zur Idee quasi-Newtonscher Methoden f√ºhrt. <br><br>  Um zu verstehen, wo die Beine der quasi-Newtonschen Methoden wachsen und woher der Name kommt, m√ºssen wir wieder zur Minimierungsmethode zur√ºckkehren, die auf der direkten L√∂sung der station√§ren Punktgleichung basiert <img src="https://habrastorage.org/getpro/habr/post_images/c43/946/5f9/c439465f905d9f366a2f4b3296306290.gif" title="&quot;\ bigtriangledown f = 0&quot;">  .  So wie die Betrachtung der Newton-Methode, die auf die L√∂sung dieser Gleichung angewendet wurde, uns zu der gleichnamigen Optimierungsmethode f√ºhrte (die im Gegensatz zu ihrem Vorl√§ufer eine globale Konvergenzregion aufweist), k√∂nnen wir erwarten, dass die Ber√ºcksichtigung anderer Methoden zur L√∂sung von Systemen nichtlinearer Gleichungen fruchtbar sein wird Planen Sie Ideen f√ºr den Aufbau anderer Optimierungsmethoden. <br><br><h2>  Sekantenmethoden </h2><br>  Ich m√∂chte Sie daran erinnern, dass die Newton-Methode zur L√∂sung des Gleichungssystems <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  , basiert auf dem Austausch in der N√§he eines Punktes in der N√§he der L√∂sung <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  die Funktionen <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  seine lineare Ann√§herung <img src="https://habrastorage.org/getpro/habr/post_images/d15/479/f23/d15479f235f0d60ce8837c9043a0d2cc.gif" title="L (p) = F (x) + J (x) p">  wo <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  Ist ein linearer Operator, der, wenn <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  ist ein Vektor und <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  hat partielle Ableitungen in Bezug auf jede Variable, f√§llt mit der Jacobi-Matrix zusammen <img src="https://habrastorage.org/getpro/habr/post_images/4d2/826/ff6/4d2826ff6ba22f9f67cab70bfbe17a16.gif" title="&quot;J_ {ij} = \ dfrac {\ partielle F_ {i}} {\ partielle x_ {j}}&quot;">  .  Als n√§chstes wird die Gleichung gel√∂st <img src="https://habrastorage.org/getpro/habr/post_images/c9d/8be/3f2/c9d8be3f2d70054db890ea34e3409544.gif" title="L (p) = 0">  und Punkt <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&quot;x '= x + p&quot;">  als neue Ann√§herung an die gew√ºnschte L√∂sung genommen.  Es ist einfach und es funktioniert. <br><br>  Aber was ist, wenn wir aus irgendeinem Grund die Jacobi-Matrix nicht berechnen k√∂nnen?  Das erste, was uns in diesem Fall einf√§llt, ist, dass wir, wenn wir die partiellen Ableitungen nicht analytisch berechnen k√∂nnen, eine numerische N√§herung f√ºr sie erhalten k√∂nnen.  Die einfachste (wenn auch keineswegs einzige) Option f√ºr eine solche Ann√§herung kann die Formel der richtigen endlichen Differenzen sein: <img src="https://habrastorage.org/getpro/habr/post_images/149/708/f5b/149708f5b8bab4374023295557622e82.gif" title="&quot;\ dfrac {\ partielle F_ {i}} {\ partielle x_ {j}} \ ca. \ dfrac {F_ {i} (x + h_ {j} e_ {j}) - F_ {i} (x)} { h_ {j}} &quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/459/e61/aa0/459e61aa08f7fe807167a596e7ebd8a9.gif" title="&quot;e_ {j}&quot;">  Ist der j-te Basisvektor.  Die aus solchen N√§herungen zusammengesetzte Matrix wird mit bezeichnet <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  .  Eine Analyse, wie viel Ersatz <img src="https://habrastorage.org/getpro/habr/post_images/206/f34/999/206f349991c0724c2fdce788124abe1c.gif" title="&quot;J&quot;">  auf <img src="https://habrastorage.org/getpro/habr/post_images/a51/533/990/a5153399048e881eb8661304792b8c81.gif" title="&quot;\ bar {J}&quot;">  In Newtons Methode wirkt sich die Konvergenz auf eine ziemlich gro√üe Anzahl von Werken aus, aber in diesem Fall interessieren wir uns f√ºr einen anderen Aspekt.  Eine solche Ann√§herung erfordert n√§mlich die Berechnung der Funktion an N zus√§tzlichen Punkten und zus√§tzlich der Funktion <img src="https://habrastorage.org/getpro/habr/post_images/6c4/afd/100/6c4afd1002ddcfa43d07afbc9f103a9d.gif" title="&quot;\ bar {L} (p) = F (x) + \ bar {J} p&quot;">  an diesen Punkten <i>interpoliert die</i> Funktion <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  d.h. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c56/5c9/4b4/c565c94b4a37b9cd5f42fc1be92b2e15.gif" title="&quot;\ bar {L (} h_ {j} e_ {j}) = F (x) + h_ {j} \ dfrac {F (x + h_ {j} e_ {j}) - F (x)} {h_ {j}} = F (x) + F (x + h_ {j} e_ {j}) - F (x) = F (x + h_ {j} e_ {j}). &quot;"><br><br>  Nicht jede Approximation der Jacobi-Matrix hat diese Eigenschaft, aber jede Matrix einer affinen Funktion, die diese Eigenschaft hat, ist eine Approximation der Jacobi-Matrix.  In der Tat, wenn <img src="https://habrastorage.org/getpro/habr/post_images/88f/5c8/dd7/88f5c8dd7e9876a2d0e0980882f261da.gif" title="&quot;F (x + p_ {j}) = F (x) + J (x) p_ {j} + o \ left (\ left \ Vert p_ {j} \ right \ Vert ^ {2} \ right)&quot;">  und <img src="https://habrastorage.org/getpro/habr/post_images/5ad/e1e/ad2/5ade1ead2804a3bfaa8ffdf9122a179a.gif" title="&quot;\ bar {J} p_ {j} = F (x + p_ {j}) - F (x)&quot;">  dann bei <img src="https://habrastorage.org/getpro/habr/post_images/803/ca4/351/803ca4351b87edf1a13a2a2947772fa7.gif" title="&quot;\ left \ Vert p_ {j} \ right \ Vert \ rightarrow0 \ quad \ bar {J} (x) p_ {j} \ rightarrow J (x) p_ {j}&quot;">  .  Diese Eigenschaft, n√§mlich die Interpolationseigenschaft, gibt uns eine konstruktive M√∂glichkeit, die Newton-Methode zu verallgemeinern. <br><br>  Lass <img src="https://habrastorage.org/getpro/habr/post_images/194/ad1/d42/194ad1d42aa4320679b9498748ceb78d.gif" title="&quot;\ bar {L} (p) = a + Ap&quot;">  - Funktion, die die Anforderung erf√ºllt <img src="https://habrastorage.org/getpro/habr/post_images/06e/34e/3a7/06e34e3a7a0d0058ef351da74258a637.gif" title="&quot;\ bar {L} (p_ {i}) = F (x + p_ {i})&quot;">  f√ºr ein System linear unabh√§ngiger Vektoren <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  .  Dann wird eine solche Funktion als <i>Sekantenfunktion</i> bezeichnet <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  und die Gleichung, die es definiert, ist <i>die Sekantengleichung</i> .  Wenn das System der Vektoren <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  ist vollst√§ndig (das hei√üt, es gibt genau N von ihnen und sie sind immer noch linear unabh√§ngig), und zus√§tzlich das Vektorsystem <img src="https://habrastorage.org/getpro/habr/post_images/94f/cf5/579/94fcf55798902795ffb670e35359d2af.gif" title="&quot;\ left \ {F (x + p_ {i}), i = 1 \ dots N \ right \}&quot;">  dann linear unabh√§ngig <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  eindeutig definiert. <br><br>  Jede Methode, die auf einer lokalen √Ñnderung der Gleichung basiert <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="F (x) = 0">  Gleichung der Form <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  wo <img src="https://habrastorage.org/getpro/habr/post_images/77f/eec/dd2/77feecdd2ae9a4795d2f81f3eec18b1b.gif" title="&quot;\ bar {L}&quot;">  erf√ºllt <i>die Sekantengleichung</i> , die als <i>Sekantenmethode bezeichnet wird</i> . <br><br>  Es stellt sich die Frage, wie der Sekant f√ºr eine Funktion auf rationellste Weise konstruiert werden kann. <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Die folgende Argumentation scheint offensichtlich: Lassen Sie am Punkt x ein affines Modell konstruieren, das die gegebene Funktion an den Punkten interpoliert <img src="https://habrastorage.org/getpro/habr/post_images/462/bcf/f32/462bcff32469c0ec5f8ccfc80534c05c.gif" title="&quot;x-x_ {1}, x-x_ {2}, \ dots, x-x_ {N}&quot;">  .  Gleichungsl√∂sung <img src="https://habrastorage.org/getpro/habr/post_images/ccb/557/80f/ccb55780f0c8e65187b0f4c9126be81c.gif" title="&quot;\ bar {L} (p) = 0&quot;">  gibt uns einen neuen Punkt <img src="https://habrastorage.org/getpro/habr/post_images/2c4/a7b/e55/2c4a7be5582848bfbcdd9ee141e7d764.gif" title="&quot;x '= x + p&quot;">  .  Dann, um an einem Punkt ein affines Modell zu erstellen <img src="https://habrastorage.org/getpro/habr/post_images/787/cf7/c3a/787cf7c3a3d374114b3a07305b7fa446.gif" title="&amp; quot; x '&amp; quot;">  Es ist am sinnvollsten, Interpolationspunkte so zu w√§hlen, dass der Wert <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  bereits bekannt - das hei√üt, nehmen Sie sie aus dem Set <img src="https://habrastorage.org/getpro/habr/post_images/333/297/225/33329722533f0b608b0994d2a5ba83fa.gif" title="&quot;\ left \ {x'-x, x'-x_ {1}, x'-x_ {2}, \ dots, x'-x_ {N} \ right \}&quot;">  .  Es gibt verschiedene Optionen, f√ºr die Sie aus den vielen zuvor verwendeten Punkten ausw√§hlen k√∂nnen.  Sie k√∂nnen beispielsweise diejenigen als Interpolationspunkte verwenden, in denen <img src="https://habrastorage.org/getpro/habr/post_images/bb3/e3a/cd5/bb3e3acd5043b859fe89006d4cabe5a0.gif" title="&quot;\ left \ Vert F \ right \ Vert&quot;">  z√§hlt am wenigsten oder nur am ersten <img src="https://habrastorage.org/getpro/habr/post_images/055/8e9/3d9/0558e93d918ff32e873b6a71703e9969.gif" title="&quot;N&quot;">  Punkte.  Auf jeden Fall scheint es offensichtlich, dass <img src="https://habrastorage.org/getpro/habr/post_images/95f/756/92b/95f75692ba0aeefcef24ae42714dbc1b.gif" title="&quot;p = x'-x&quot;">  sollte in vielen Interpolationspunkten f√ºr das neue affine Modell enthalten sein.  Also dar√ºber hinaus <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  Schritte des iterativen Prozesses in unserem Set k√∂nnen bis zu sein <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" title="&quot;n&quot;">  Verschiebungen, die auf zuvor √ºbergebenen Punkten aufgebaut sind.  Wenn der Prozess so aufgebaut ist, dass das neue affine Modell nicht mehr verwendet <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  Von den vorherigen Werten wird ein solcher Prozess als p-Punkt-Sekantenmethode bezeichnet. <br><br>  Auf den ersten Blick scheint die N-Punkt-Sekantenmethode der beste Kandidat f√ºr die Rolle des Ersetzens der Newton-Methode zu sein, da sie die Informationen, die wir beim L√∂sen erhalten, maximal nutzt und gleichzeitig die Anzahl zus√§tzlicher Berechnungen minimiert - wir verwenden den Wert der Funktion in letzterer N Punkte bestanden.  Dies ist leider nicht so.  Die Sache ist, dass das Vektorsystem <img src="https://habrastorage.org/getpro/habr/post_images/ed0/117/8ca/ed01178ca46506fa4588780d16d705a1.gif" title="F (x_ {0}), F (x_ {1}), \ Punkte F (x_ {N})">  weigert sich hartn√§ckig, linear unabh√§ngig mit einem ausreichend gro√üen N zu sein. Selbst wenn sich herausstellt, dass diese Bedingung erf√ºllt ist und das entsprechende affine Modell noch existiert, besteht die M√∂glichkeit, dass die Richtungen <img src="https://habrastorage.org/getpro/habr/post_images/602/ff2/50c/602ff250c473d5b28e08a1453d4175b3.gif" title="&quot;p_ {j} = x_ {j} -x_ {0}&quot;">  erweisen sich auch als linear unabh√§ngig, es stellt sich noch weniger heraus.  Und dies beinhaltet die Tatsache, dass das affine Modell, obwohl es existiert, entartet und praktisch ungeeignet ist. <br><br>  Im Allgemeinen ist die 2-Punkt-Sekantenmethode am stabilsten.  Das hei√üt, eine Methode, bei der bei jeder Iteration zus√§tzliche N-1-Werte der Funktion berechnet werden m√ºssen.  Dies ist eindeutig nicht f√ºr unsere praktischen Zwecke geeignet. <br><br>  Dann ist die Frage - was war das alles? <br><br><h2>  Quasi-Newtonsche Methoden zur L√∂sung von Gleichungen </h2><br><br>  Der Ausweg ist einfach, wenn auch nicht offensichtlich.  Wenn wir nicht die technische F√§higkeit haben, basierend auf den bereits berechneten Werten das affine Modell, das die Sekantengleichung erf√ºllt, eindeutig zu bestimmen, ist dies nicht erforderlich.  Wir nehmen die Sekantengleichung als Grundlage, aber wir werden verlangen, dass sie nur f√ºr ein unvollst√§ndiges Vektorsystem erf√ºllt ist <img src="https://habrastorage.org/getpro/habr/post_images/e5e/f2b/432/e5ef2b43292735aa2a68afffb80bf520.gif" title="&quot;\ left \ {p_ {1}, p_ {2}, \ dots, p_ {m} \ right \}, m &amp; lt; N&quot;">  .  Mit anderen Worten, wir werden verlangen, dass die Interpolationsbedingung nur f√ºr eine ausreichend kleine Anzahl bekannter Werte erf√ºllt ist.  In diesem Fall k√∂nnen wir nat√ºrlich nicht mehr garantieren, dass die in einem solchen Modell verwendete Matrix zur Jacobi-Matrix tendiert, aber wir werden dies nicht ben√∂tigen.  Hinzu kommt, dass das affine Modell die Funktion am aktuellen Punkt interpolieren muss, d. H. <img src="https://habrastorage.org/getpro/habr/post_images/3b9/9d1/7fa/3b99d17fa378aeaf36097faef3830bd5.gif" title="&quot;\ bar {L} (0) = F (x)&quot;">  erhalten wir die folgende Formulierung der Sekantenmethode: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/78c/f30/421/78cf304219a4bc90d4f900062bf2d027.gif" title="&quot;\\ \ bar {L} (p_ {i}) = F (x) + Ap_ {i} = F (x + p_ {i}), \ quad i = 1 \ Punkte m \\ \ bar {L} (p) = 0 \ quad \ Rightarrow p = A ^ {- 1} F (x)"><br><br>  Bruiden war der erste, der Methoden dieser Art f√ºr m = 1 in Betracht zog und sie quasi-Newtonsch nannte.  Es ist klar, dass die Sekantenbedingung in diesem Fall es uns erm√∂glicht, die Matrix eindeutig zu identifizieren <img src="https://habrastorage.org/getpro/habr/post_images/c9d/999/d9a/c9d999d9a4e8bd3d6f8e50519d1dfaa8.gif" title="&quot;A&quot;">  nur wenn ihm zus√§tzliche Bedingungen auferlegt werden und jede dieser zus√§tzlichen Bedingungen zu einer eigenen Methode f√ºhrt.  Bruyden selbst argumentierte wie folgt: <br><br>  <i>als die Bewegung in die Richtung</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>von Punkt</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;"></i>  <i>auf den Punkt</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/1d0/56f/301/1d056f3016bc715aacc23418d8629173.gif" title="&quot;x_ {1}&quot;"></i>  <i>gibt uns keine zus√§tzlichen Informationen dar√ºber, wie sich die Funktion in anderen als √§ndert</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>Richtungen, dann die Wirkung der neuen affinen Funktion auf den Vektor</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>sollte sich von der Wirkung der alten Funktion auf denselben Vektor unterscheiden, je weniger desto unterschiedlicher</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>von</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>.</i>  <i>Als letztes Mittel, wenn</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q"></i>  <i>orthogonal</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i>  <i>sollte sich das Verhalten der neuen Funktion nicht vom Verhalten der alten unterscheiden.</i> <i><br></i> <br>  Breidens Idee ist in ihrer Einfachheit brillant.  Wenn wir keine neuen Informationen √ºber das Verhalten der Funktion haben, k√∂nnen wir am besten versuchen, die alte nicht zu beschmutzen.  Dann die zus√§tzliche Bedingung <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b3/d43/d20/0b3d43d207b144b926274d0c81abccbf.gif" title="&quot;\ bar {L} _ {1} q = \ bar {L} _ {0} q&quot;">  f√ºr alle <img src="https://habrastorage.org/getpro/habr/post_images/9fc/c76/a21/9fcc76a21130891ea5d5b10efa979bff.gif" title="q">  so dass <img src="https://habrastorage.org/getpro/habr/post_images/c16/9f6/315/c169f6315171249a34b50b26a2975c6e.gif" title="q ^ {T} p = 0"><br><br>  Mit dieser Option k√∂nnen Sie die Matrix der neuen Transformation eindeutig bestimmen. Sie wird durch Hinzuf√ºgen einer Rang 1-Korrektur zur alten Matrix erhalten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/522/f36/1f9/522f361f94a7b5a2e9da68094983b21d.gif" title="&quot;\\ A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T}} {p ^ {T} p} \\ y = F (x_ {0}) -F (x_ {1}) &quot;"><br><br>  Trotz der Einfachheit und Konsistenz der Schlussfolgerungen von Bruiden bieten sie jedoch nicht den Dreh- und Angelpunkt, der als Grundlage f√ºr die Konstruktion anderer √§hnlicher Methoden dienen k√∂nnte.  Gl√ºcklicherweise gibt es einen formelleren Ausdruck seiner Idee.  Die Matrix ist n√§mlich auf diese Weise aufgebaut <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  Es stellt sich als L√∂sung f√ºr das folgende Problem heraus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/028/96f/3fa/02896f3facb898d70f26abad02fe90a9.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ F (x_ {1}) - Ap = F (x_ {0})&quot;"><br><br>  Die Aufgabenbeschr√§nkung ist nichts anderes als die Sekantengleichung, und die Minimierungsbedingung spiegelt unseren Wunsch wider, so viele Informationen wie m√∂glich in der Matrix zu speichern <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  .  Das Ma√ü f√ºr die Diskrepanz zwischen den Matrizen ist in diesem Fall die Frobenius-Norm, in der das gestellte Problem eine eindeutige L√∂sung hat.  Diese Formulierung kann durchaus als Ausgangspunkt f√ºr die Konstruktion anderer Methoden dienen.  Wir k√∂nnen n√§mlich sowohl das <i>Ma√ü</i> √§ndern <i>,</i> mit dem wir die eingef√ºhrten √Ñnderungen bewerten, als auch die der Matrix auferlegten <i>Bedingungen</i> versch√§rfen.  Im Allgemeinen kann man bereits mit einer solchen Formulierung der Methode arbeiten. <br><br><h2>  Quasi-Newton-Optimierungsmethoden </h2><br><br>  Nachdem wir die Hauptidee verstanden haben, k√∂nnen wir endlich zu Optimierungsproblemen zur√ºckkehren und feststellen, dass die Anwendung der Bruyden-Formel zur Neuberechnung des affinen Modells nicht sehr gut zu unserer Aufgabe passt.  Tats√§chlich ist die erste Ableitung der Gradientenfunktion <img src="https://habrastorage.org/getpro/habr/post_images/6b8/82e/be7/6b882ebe727121dcb5fc21b091044b5a.gif" title="&quot;\ bigtriangledown f&quot;">  es gibt nichts anderes als die hessische Matrix, die konstruktionsbedingt symmetrisch ist.  Gleichzeitig f√ºhrt die Aktualisierung nach der Bruyden-Regel zu einer asymmetrischen Matrix <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  auch wenn <img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;">  war symmetrisch.  Dies bedeutet nicht, dass die Bruden-Methode nicht zur L√∂sung der station√§ren Punktgleichung angewendet werden kann, aber basierend auf einer solchen Aktualisierungsregel ist es unwahrscheinlich, dass wir gute Optimierungsmethoden konstruieren k√∂nnen.  Im Allgemeinen ist es ziemlich offensichtlich, dass die Quasi-Newton-Methode umso besser funktionieren sollte, je genauer das System der Bedingungen des Problems die Besonderheiten einer bestimmten Jacobi-Matrix beschreibt. <br><br>  Um diesen Nachteil zu beheben, f√ºgen wir dem Broyden-Minimierungsproblem eine zus√§tzliche Einschr√§nkung hinzu, die ausdr√ºcklich erfordert, dass die neue Matrix zusammen mit der alten symmetrisch ist: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/03e/167/aa2/03e167aa25e0f4aa6b8df8546552e79a.gif" title="&quot;\\ \ left \ Vert A_ {1} -A_ {0} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0} ) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Die L√∂sung f√ºr dieses Problem ist <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df8/356/74b/df835674b94bab190bca3c18efed98ce.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) p ^ {T} + p (y-A_ {0} p) ^ {T}} {p ^ {T} p} - \ dfrac {(y-A_ {0} p) ^ {T} p} {\ left (p ^ {T} p \ right) ^ {2}} pp ^ {T} &quot;"><br><br>  Hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/89c/f84/f29/89cf84f292ed72d1b20755677688a054.gif" title="y = \ bigtriangledown f (x_ {1}) - \ bigtriangledown f (x_ {0})"></a>  und die Matrix-Neuberechnungsformel ist nach ihren Erstellern benannt - Powell, Shanno und Bruyden (PSB).  Die resultierende Matrix ist symmetrisch, aber eindeutig nicht eindeutig positiv, wenn auch nur pl√∂tzlich <img src="https://habrastorage.org/getpro/habr/post_images/6c7/040/47d/6c704047d3148fd7a8b563aaf79dd7f4.gif" title="&quot;y&quot;">  wird nicht kollinear sein <img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;">  .  Und wir haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gesehen,</a> dass positive Sicherheit bei Optimierungsmethoden sehr w√ºnschenswert ist. <br><br>  Wieder werden wir den Zustand des Problems korrigieren, indem wir diesmal die skalierte Frobenius-Norm als Ma√ü f√ºr die Matrixdivergenz verwenden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f37/eb1/0f4/f37eb10f4eb10d0c54acc9adab962f10.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} -A_ {0} \ right) T ^ {- 1} \ right \ Vert _ {F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Der Ursprung einer solchen Aussage der Frage ist ein separates gro√ües Thema, aber es ist interessant, dass, wenn die Matrix T so ist, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/673/872/131/673872131fa6cb0f44e6839be0e448e7.gif" title="T ^ {T} T = G, Gp = y"></a>  (das hei√üt, G ist auch eine affine Transformationsmatrix, die die Sekantengleichung f√ºr die Richtung p erf√ºllt), dann stellt sich heraus, dass die L√∂sung dieses Problems unabh√§ngig von der Wahl von T ist und zur Aktualisierungsformel f√ºhrt <br><br><img src="https://habrastorage.org/getpro/habr/post_images/135/ea6/c14/135ea6c14ea8f63f961e83576f1be5d5.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {(y-A_ {0} p) y ^ {T} + y (y-A_ {0} p) ^ {T}} {y ^ {T} p} - \ dfrac {\ left (y-A_ {0} p \ right) ^ {T} p} {\ left (y ^ {T} p \ right) ^ {2}} yy ^ {T} &quot;"><br><br>  bekannt als die Davidon-Fletcher-Powell-Formel.  Diese Aktualisierungsmethode hat sich in der Praxis bew√§hrt, da sie die folgende Eigenschaft aufweist: <br><br>  <i>wenn</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/e3e/c44/1c1/e3ec441c17e1b43df108a7d8e15d3dd6.gif" title="y ^ {T} p &amp; gt; 0"></i>  <i>und</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/107/a45/803/107a45803b226180325815eaa7be8706.gif" title="&quot;A_ {0}&quot;"></i>  <i>positiv definitiv dann</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;"></i>  <i>auch positiv identifiziert.</i> <br><br>  Ich stelle danach fest, dass, wenn die erste Bedingung nicht erf√ºllt ist, keine affine Funktion mit einer positiven bestimmten Matrix existiert, die die Sekantengleichung erf√ºllt. <br><br>  Wenn wir in dem Problem, das zur DFP-Methode f√ºhrt, als Ma√ü f√ºr die Diskrepanz affiner Modelle den Abstand nicht zwischen den Matrizen selbst, sondern zwischen den zu ihnen inversen Matrizen nehmen, erhalten wir ein Problem <br><br><img src="https://habrastorage.org/getpro/habr/post_images/337/0b0/af2/3370b0af216ab9695789eeb586cf3604.gif" title="&quot;\\ \ left \ Vert T ^ {- T} \ left (A_ {1} ^ {- 1} -A_ {0} ^ {- 1} \ right) T ^ {- 1} \ right \ Vert _ { F} \ rightarrow \ min \\ \ bigtriangledown f (x_ {1}) - Ap = \ bigtriangledown f (x_ {0}) \\ A_ {1} ^ {T} = A_ {1} &quot;"><br><br>  Seine L√∂sung ist eine bekannte Formel, die fast gleichzeitig von Breiden, Fletcher, Goldfarb und Shanno (BFGS) entdeckt wurde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/840/c7b/3fa840c7b6ec3de81eb02bb0e9240722.gif" title="&quot;A_ {1} = A_ {0} + \ dfrac {yy ^ {T}} {y ^ {T} p} - \ dfrac {A_ {0} pp ^ {T} A_ {0}} {p ^ { T} A_ {0} p} &quot;"><br><br>  Bisher wird angenommen, dass eine Neuberechnung nach dieser Formel aus rechnerischer Sicht am effizientesten ist und gleichzeitig weniger anf√§llig f√ºr eine Degeneration der Matrix mit einer gro√üen Anzahl von Iterationen ist.  Unter den gleichen Bedingungen wie DFP bewahrt diese Formel die Eigenschaft der positiven Bestimmtheit. <br><br>  Alle beschriebenen Methoden zum Aktualisieren der Matrix erfordern eine Korrektur von Rang 2. Dies macht es einfach und leicht, die Matrix zu invertieren <img src="https://habrastorage.org/getpro/habr/post_images/147/7e7/ca0/1477e7ca06155c3e43fd4a640e0f7f98.gif" title="&quot;A_ {1}&quot;">  unter Verwendung der Sherman-Morrison-Formel und des Wertes <img src="https://habrastorage.org/getpro/habr/post_images/5f6/3ac/2d9/5f63ac2d91f47a730fee01b5db38f3bd.gif" title="&quot;A_ {0} ^ {- 1}&quot;">  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed0/9f8/002/ed09f80027e56f58a3502cc943758509.gif" title="&quot;B_ {1} = B_ {0} + uv ^ {T} \ Rightarrow B_ {1} ^ {- 1} = B_ {0} ^ {- 1} + \ dfrac {B_ {0} ^ {- 1} uv ^ {T} B_ {0} ^ {- 1}} {1 + v ^ {T} B_ {0} ^ {- 1} u} &quot;"><br><br>  vorausgesetzt, der Nenner der Formel ist ungleich Null.  Ich werde keine spezifischen Formeln zum Aktualisieren der inversen Matrizen der aufgelisteten Methoden angeben, da sie leicht zu finden oder unabh√§ngig voneinander abzuleiten sind.  Das einzige, was in diesem Fall beachtet werden sollte, ist, dass Varianten von Methoden mit Aktualisierung der inversen Matrix normalerweise viel weniger stabil sind (das hei√üt, sie leiden mehr unter Rundungsfehlern) als diejenigen, die eine Aktualisierung der urspr√ºnglichen Matrix vorschlagen.  Es ist am effektivsten, nicht die Matrix selbst, sondern ihre Cholesky-Zerlegung zu aktualisieren (es sei denn nat√ºrlich, eine solche Zerlegung findet statt), da eine solche Implementierungsoption numerisch stabiler ist und au√üerdem die Kosten f√ºr die L√∂sung einer Gleichung minimiert, die die Bewegungsrichtung bestimmt. <br><br>  Es bleibt die Frage zu pr√ºfen, wie die allererste Matrix im quasi-Newtonschen Prozess aussehen soll.  Hier ist alles offensichtlich - je n√§her es der hessischen Matrix oder ihrer korrigierten Version ist, wenn sich das hessische pl√∂tzlich nicht als positiv definitiv herausstellt, desto besser wird es unter dem Gesichtspunkt der Konvergenz sein.  Grunds√§tzlich kann jedoch jede positive definitive Matrix f√ºr uns geeignet sein.  Die einfachste Version einer solchen Matrix ist eine einzelne, und dann f√§llt die erste Iteration mit der Iteration des Gradientenabfalls zusammen.  Fletcher und Powell zeigten (nat√ºrlich f√ºr die DFP-Methode), dass wenn die quadratische Funktion minimiert wird, unabh√§ngig davon, welche (positiv definierte) Matrix als anf√§ngliche DFP-Iteration verwendet wird, sie zu einer L√∂sung in genau N Iterationen f√ºhren, wobei N ist Dimension des Problems, und die quasi-Newtonsche Matrix f√§llt mit der hessischen Matrix am minimalen Punkt zusammen.  Im allgemeinen nichtlinearen Fall eines solchen Gl√ºcks werden wir nat√ºrlich nicht warten, aber dies gibt zumindest Anlass, sich nicht zu viele Sorgen √ºber die schlechte Wahl der Ausgangsmatrix zu machen. <br><br><h2>  Fazit </h2><br><br>  Der beschriebene Ansatz zur Konstruktion quasi-Newtonscher Methoden ist nicht der einzig m√∂gliche.  Zumindest kamen die Entdecker der beschriebenen quasi-Newtonschen Methoden und viele nachfolgende Forscher aufgrund v√∂llig unterschiedlicher √úberlegungen zu denselben Formeln.  Es ist jedoch interessant, dass, sobald eine bestimmte quasi-Newtonsche Methode auftauchte, nach relativ kurzer Zeit klar wurde, dass es sich um eine L√∂sung f√ºr ein sehr leicht zu interpretierendes Optimierungsproblem handelt.  Meiner Meinung nach ist es bemerkenswert, dass es m√∂glich ist, einen gemeinsamen Nenner f√ºr so unterschiedliche Methoden zu liefern, da dies die Grundlage f√ºr die Konstruktion anderer Methoden bildet, die die Besonderheiten einer bestimmten Aufgabe besser ber√ºcksichtigen.  Insbesondere gibt es quasi-Newtonsche Methoden zur Aktualisierung sp√§rlicher Matrizen, Methoden, bei denen so wenig Elemente wie m√∂glich ge√§ndert werden, und viele andere w√§ren eine Fantasie. <br><br>  Es sollte auch beachtet werden, dass die Methoden variabler Metriken trotz ihres Namens nicht immer zur Konstruktion von Matrizen f√ºhren, die tats√§chlich Metriken sind, obwohl sie dies jedes Mal tun, wenn es √ºberhaupt m√∂glich ist.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist normalerweise kein gro√ües Problem, aber diejenigen, die sich vor einer m√∂glichen Verlegenheit sch√ºtzen m√∂chten, greifen m√∂glicherweise auf dieselben Tricks zur√ºck, die zur √úberwindung eines √§hnlichen Problems mit der Newton-Methode angewendet wurden - beispielsweise durch Richtungs√§nderung oder Anwendung </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">des Levenberg-Marquardt-Schemas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . In diesem Fall werden zwar Fragen der Wahl der Form einer vertrauensvollen Region wieder relevant, aber hier ist es notwendig, das kleinere √úbel zu w√§hlen. Eine andere L√∂sung des Problems besteht darin, lineare Suchmethoden zu verwenden, um sicherzustellen, dass die notwendigen Bedingungen zur Aufrechterhaltung einer positiven Sicherheit erf√ºllt sind. Die Wolfe-Regel garantiert die Erf√ºllung dieser Bedingung, w√§hrend die Armijo- und Goldstein-Regeln dies nicht tun.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Theoretisch ist es fast unm√∂glich zu bestimmen, welche der vielen m√∂glichen quasi-Newtonschen Methoden in Bezug auf eine bestimmte Klasse von Problemen am effektivsten ist. Normalerweise beschr√§nken sie sich bei der Formulierung einer Methode darauf, ihre Wirksamkeit bei der Minimierung einer quadratischen Funktion zu zeigen (eine Methode wird √ºbrigens als effektiv angesehen, wenn sie zu einer exakten L√∂sung in N Iterationen f√ºhrt, dh nicht langsamer als direkte Methoden zur L√∂sung von SLAEs). In selteneren F√§llen kann man Studien zur Konvergenzreihenfolge der Methode (die normalerweise superlinear ist, dh deutlich besser als die Gradientenabnahme), zur Stabilit√§t und zu anderen interessanten Merkmalen finden. Im Allgemeinen ist das einzig vern√ºnftige Kriterium f√ºr die Beurteilung der Wirksamkeit einer bestimmten Methode f√ºr eine bestimmte Aufgabenklasse die Praxis.Also Schaufeln in der Hand - und Erfolg bei der Anwendung.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470638/">https://habr.com/ru/post/de470638/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470620/index.html">Infrastruktur als Code: So √ºberwinden Sie Probleme mit XP</a></li>
<li><a href="../de470622/index.html">√úbersicht √ºber Methoden zur Merkmalsauswahl</a></li>
<li><a href="../de470628/index.html">Raumschiff Simulator Schiffbau</a></li>
<li><a href="../de470632/index.html">Arend - HoTT-basierte abh√§ngige Typensprache (Teil 2)</a></li>
<li><a href="../de470634/index.html">Identifizieren Sie Community-√ºbergreifend auf Instagram, um Benutzerinteressen zu identifizieren</a></li>
<li><a href="../de470640/index.html">Dimensionierung Elasticsearch</a></li>
<li><a href="../de470642/index.html">Treffen Sie Yandex.Station Mini. Gro√üe Geschichte eines kleinen Ger√§ts</a></li>
<li><a href="../de470646/index.html">Mathematik f√ºr Data Science. Neuer Kurs von OTUS</a></li>
<li><a href="../de470648/index.html">IBM LTO-8 - Einfache M√∂glichkeit zum Speichern kalter Daten</a></li>
<li><a href="../de470650/index.html">Datenaufbereitung in einem Data Science-Projekt: Rezepte f√ºr junge Hausfrauen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>