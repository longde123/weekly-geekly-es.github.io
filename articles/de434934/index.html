<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèæ üöè ü§Æ Wie KI-Lernskalen üîµ üêü üôáüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir bei OpenAI haben festgestellt, dass die Gradientenrauschskala, eine einfache statistische Methode, die Parallelisierbarkeit des Lernens eines neut...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie KI-Lernskalen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434934/"> Wir bei OpenAI haben festgestellt, dass die Gradientenrauschskala, eine einfache statistische Methode, die Parallelisierbarkeit des Lernens eines neutralen Netzwerks f√ºr eine Vielzahl von Aufgaben vorhersagt.  Da der Gradient bei komplexeren Aufgaben normalerweise lauter wird, wird sich eine Vergr√∂√üerung der f√ºr die gleichzeitige Verarbeitung verf√ºgbaren Pakete in Zukunft als n√ºtzlich erweisen und eine der potenziellen Einschr√§nkungen von KI-Systemen beseitigen.  Im allgemeinen Fall zeigen diese Ergebnisse, dass das Training neuronaler Netze nicht als mysteri√∂se Kunst betrachtet werden sollte und dass es genau und systematisiert werden kann. <br><br>  In den letzten Jahren war es KI-Forschern zunehmend gelungen, das Lernen neuronaler Netze durch Parallelisierung von Daten zu beschleunigen und gro√üe Datenpakete in mehrere Computer aufzuteilen.  Forscher haben erfolgreich Zehntausende von Einheiten f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Bildklassifizierung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachmodellierung</a> und sogar f√ºr Millionen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von verst√§rkenden Lernagenten verwendet</a> , die Dota 2 gespielt haben. Solche gro√üen Pakete k√∂nnen die Rechenleistung erh√∂hen, die effektiv f√ºr das Unterrichten eines Modells erforderlich ist, und sind eins der Kr√§fte, die das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wachstum im</a> KI- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Training</a> vorantreiben.  Bei zu gro√üen Datenpaketen nehmen die algorithmischen R√ºckgaben jedoch rapide ab, und es ist nicht klar, warum sich diese Einschr√§nkungen f√ºr einige Aufgaben als gr√∂√üer und f√ºr andere als kleiner herausstellen. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/021/924/2f0/0219242f04c9a77d8c783c1ac9f143aa.svg"><br>  <i>Die √ºber Trainingsans√§tze gemittelte Gradientenrauschskalierung erkl√§rt die Mehrheit (r <sup>2</sup> = 80%) der kritischen Variationen der Datenpaketgr√∂√üe f√ºr verschiedene Probleme, die sich um sechs Gr√∂√üenordnungen unterscheiden.</i>  <i>Die Paketgr√∂√üen werden anhand der Anzahl der Bilder, Token (f√ºr Sprachmodelle) oder Beobachtungen (f√ºr Spiele) gemessen.</i> <br><br>  Wir haben festgestellt, dass durch Messen der Gradientenrauschskala, einfache Statistiken, die das Signal-Rausch-Verh√§ltnis in den Gradienten des Netzwerks numerisch bestimmen, wir die maximale Paketgr√∂√üe ungef√§hr vorhersagen k√∂nnen.  Heuristisch misst die Rauschskala die Variation von Daten aus Sicht des Modells (in einem bestimmten Trainingsstadium).  Wenn die Rauschskala klein ist, wird paralleles Lernen mit einer gro√üen Datenmenge schnell √ºberfl√ºssig, und wenn sie gro√ü ist, k√∂nnen wir mit gro√üen Datenmengen viel lernen. <br><br>  Statistiken dieser Art werden h√§ufig verwendet, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">um die</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gr√∂√üe der</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stichprobe</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bestimmen</a> , und es wurde <a href="">vorgeschlagen,</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sie f√ºr</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tiefes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lernen zu verwenden</a> , sie wurden jedoch nicht systematisch f√ºr das moderne Training neuronaler Netze verwendet.  Wir haben diese Vorhersage f√ºr eine Vielzahl von maschinellen Lernaufgaben best√§tigt, die in der obigen Grafik dargestellt sind, einschlie√ülich Mustererkennung, Sprachmodellierung, Atari- und Dota-Spielen.  Insbesondere haben wir neuronale Netze trainiert, um jedes dieser Probleme an Datenpaketen unterschiedlicher Gr√∂√üe zu l√∂sen (wobei die Lerngeschwindigkeit f√ºr jedes von ihnen separat angepasst wurde), und die Lernbeschleunigung mit der durch die Rauschskala vorhergesagten verglichen.  Da gro√üe Datenpakete h√§ufig eine sorgf√§ltige und kostspielige Anpassung oder einen speziellen Zeitplan f√ºr die Trainingsgeschwindigkeit erfordern, damit das Training effektiv ist und Sie die Obergrenze im Voraus kennen, k√∂nnen Sie beim Training neuer Modelle einen erheblichen Vorteil erzielen. <br><br>  Wir fanden es n√ºtzlich, die Ergebnisse dieser Experimente als Kompromiss zwischen der tats√§chlichen Trainingszeit und dem f√ºr das Training insgesamt erforderlichen Rechenaufwand (proportional zu den Geldkosten) zu visualisieren.  Bei sehr kleinen Datenpaketen kann durch Verdoppeln der Paketgr√∂√üe das Training ohne zus√§tzlichen Rechenaufwand doppelt so schnell durchgef√ºhrt werden (wir f√ºhren doppelt so viele einzelne Threads aus, die doppelt so schnell arbeiten).  Bei sehr gro√üen Datenmodellen beschleunigt die Parallelisierung das Lernen nicht.  Die Kurve in der mittleren Biegung und die Gradientenrauschskala sagen voraus, wo genau die Biegung auftritt. <br><br><img src="https://habrastorage.org/webt/0z/bo/jj/0zbojjfcs2eewyd_ve-wiijmoak.png">  <i>Durch Erh√∂hen der Anzahl paralleler Prozesse k√∂nnen Sie komplexere Modelle in angemessener Zeit trainieren.</i>  <i>Das Pareto-Grenzdiagramm ist die intuitivste M√∂glichkeit, Vergleiche von Algorithmen und Skalen zu visualisieren.</i> <br><br>  Wir erhalten diese Kurven, indem wir der Aufgabe ein Ziel zuweisen (z. B. 1000 Punkte im Atari Beam Rider-Spiel) und beobachten, wie lange es dauert, das neuronale Netzwerk zu trainieren, um dieses Ziel bei verschiedenen Paketgr√∂√üen zu erreichen.  Die Ergebnisse stimmen ziemlich genau mit den Vorhersagen unseres Modells √ºberein, wobei die verschiedenen Werte der von uns gesetzten Ziele ber√ºcksichtigt werden. <br><br>  [ <i>Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Seite mit dem Originalartikel</a> enth√§lt interaktive Grafiken eines Kompromisses zwischen Erfahrung und Trainingszeit, die zur Erreichung eines bestimmten Ziels erforderlich sind.</i> ] <br><br><h2>  Muster der Gradientenrauschskala </h2><br>  Wir sind auf verschiedene Muster in der Skala des Gradientenrauschens gesto√üen, auf deren Grundlage wir Annahmen √ºber die Zukunft des KI-Trainings treffen k√∂nnen. <br><br>  Erstens nimmt in unseren Experimenten im Lernprozess die Rauschskala normalerweise um eine Gr√∂√üenordnung oder mehr zu.  Anscheinend bedeutet dies, dass das Netzwerk zu Beginn des Trainings mehr ‚Äûoffensichtliche‚Äú Merkmale des Problems lernt und dann die kleineren Details untersucht.  Beispielsweise kann ein neuronales Netzwerk bei der Klassifizierung von Bildern zun√§chst lernen, Merkmale in kleinem Ma√üstab wie die auf den meisten Bildern gezeigten Kanten oder Texturen zu identifizieren und diese kleinen Dinge erst sp√§ter miteinander zu vergleichen, um allgemeinere Konzepte wie Katzen oder Hunde zu erstellen.  Um sich ein Bild von der ganzen Vielfalt der Gesichter und Texturen zu machen, m√ºssen neuronale Netze eine kleine Anzahl von Bildern sehen, damit die Rauschskala kleiner ist.  Sobald das Netzwerk mehr √ºber gr√∂√üere Objekte wei√ü, kann es viel mehr Bilder gleichzeitig verarbeiten, ohne doppelte Daten zu ber√ºcksichtigen. <br><br>  Wir haben einige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorl√§ufige Hinweise gesehen,</a> dass ein √§hnlicher Effekt auch bei anderen Modellen mit demselben Datensatz funktioniert - bei leistungsst√§rkeren Modellen ist die Gradientenrauschskala h√∂her, jedoch nur, weil sie weniger Verluste aufweisen.  Daher gibt es Hinweise darauf, dass die Erh√∂hung des Rauschma√üstabs w√§hrend des Trainings nicht nur ein Konvergenzartefakt ist, sondern auch auf eine Verbesserung des Modells zur√ºckzuf√ºhren ist.  Wenn ja, dann k√∂nnen wir erwarten, dass zuk√ºnftige, verbesserte Modelle ein gro√ües Ma√ü an Rauschen aufweisen und besser f√ºr die Parallelisierung geeignet sind. <br><br>  Zweitens lassen sich objektiv komplexere Aufgaben besser parallelisieren.  Im Zusammenhang mit dem Unterrichten mit einem Lehrer sind beim √úbergang von MNIST zu SVHN und ImageNet offensichtliche Fortschritte zu verzeichnen.  Im Rahmen des Verst√§rkungstrainings sind beim √úbergang von Atari Pong zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dota 1v1</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dota 5v5</a> deutliche Fortschritte zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verzeichnen</a> , und die Gr√∂√üe des optimalen Datenpakets variiert 10.000-mal.  Da die KI immer komplexere Aufgaben bew√§ltigt, wird erwartet, dass Modelle immer gr√∂√üere Datenmengen bew√§ltigen. <br><br><h2>  Die Folgen </h2><br>  Der Grad der Datenparallelisierung beeinflusst die Geschwindigkeit der Entwicklung von KI-F√§higkeiten erheblich.  Durch die Beschleunigung des Lernens k√∂nnen leistungsf√§higere Modelle erstellt und die Forschung beschleunigt werden, sodass Sie die Zeit f√ºr jede Iteration verk√ºrzen k√∂nnen. <br><br>  In einer fr√ºheren Studie, ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">KI und Berechnungen</a> ‚Äú, haben wir gesehen, dass sich die Berechnungen f√ºr das Training der gr√∂√üten Modelle alle 3,5 Monate verdoppeln, und festgestellt, dass dieser Trend auf einer Kombination aus Wirtschaftlichkeit (dem Wunsch, Geld f√ºr Berechnungen auszugeben) und algorithmischen F√§higkeiten zur Parallelisierung des Lernens beruht .  Der letzte Faktor (algorithmische Parallelisierbarkeit) ist schwieriger vorherzusagen, und seine Grenzen wurden noch nicht vollst√§ndig untersucht. Unsere aktuellen Ergebnisse sind jedoch ein Fortschritt in der Systematisierung und im numerischen Ausdruck.  Insbesondere haben wir Hinweise darauf, dass komplexere Aufgaben oder leistungsf√§higere Modelle, die auf eine bekannte Aufgabe abzielen, eine parallelere Arbeit mit Daten erm√∂glichen.  Dies wird ein Schl√ºsselfaktor sein, der das exponentielle Wachstum des lernbezogenen Rechnens unterst√ºtzt.  Und wir ber√ºcksichtigen nicht einmal die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">j√ºngsten Entwicklungen</a> auf dem Gebiet der parallelen Modelle, die es uns erm√∂glichen, die Parallelisierung weiter zu verbessern, indem wir sie der vorhandenen parallelen Datenverarbeitung hinzuf√ºgen. <br><br>  Das anhaltende Wachstum des Bereichs des Trainingscomputers und seine vorhersehbare algorithmische Basis sprechen f√ºr die M√∂glichkeit einer explosiven Steigerung der F√§higkeiten der KI in den n√§chsten Jahren und unterstreichen die Notwendigkeit einer fr√ºhzeitigen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Untersuchung des</a> sicheren und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verantwortungsvollen</a> Einsatzes solcher Systeme.  Die Hauptschwierigkeit bei der Erstellung einer KI-Politik wird darin bestehen, zu entscheiden, wie solche Ma√ünahmen zur Vorhersage der Merkmale k√ºnftiger KI-Systeme verwendet werden k√∂nnen, und dieses Wissen zu nutzen, um Regeln zu erstellen, die es der Gesellschaft erm√∂glichen, ihren Nutzen zu maximieren und den Schaden dieser Technologien zu minimieren. <br><br>  OpenAI plant eine strenge Analyse, um die Zukunft der KI vorherzusagen und die durch diese Analyse aufgeworfenen Herausforderungen proaktiv anzugehen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434934/">https://habr.com/ru/post/de434934/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434912/index.html">Der j√§hrliche Bestand von Porsche Taycan ist bereits reserviert, haupts√§chlich von Tesla-Besitzern</a></li>
<li><a href="../de434924/index.html">Was Sie √ºber die Organisation von Arbeitspl√§tzen, Coworking und die Gestaltung von R√§umen f√ºr Fernarbeit lesen sollten</a></li>
<li><a href="../de434928/index.html">Methoden der Anwendung und Verzerrung der Genauigkeit in Spielen. Visuelle Diagramme zum Vergleich</a></li>
<li><a href="../de434930/index.html">Videoanzeige heute: muss ein erfolgreiches Gesch√§ft haben</a></li>
<li><a href="../de434932/index.html">R√∂hrenverst√§rker ohne Kanon</a></li>
<li><a href="../de434938/index.html">Was Sie 2019 von Tesla erwarten k√∂nnen: Modell Y, Modell S / X-Update und mehr</a></li>
<li><a href="../de434940/index.html">Gesch√§ft Ich liebe dich</a></li>
<li><a href="../de434942/index.html">Die Kunst des Schamanismus oder der benutzerdefinierten Firmware f√ºr Olinuxino. UBOOT Teil 2</a></li>
<li><a href="../de434944/index.html">Notizen eines Phytochemikers. Bananenschale schl√§gt zur√ºck</a></li>
<li><a href="../de434946/index.html">Mein Umzug nach Norwegen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>