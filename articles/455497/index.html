<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòΩ üßîüèΩ üëµüèº Filtrado lineal √≥ptimo: desde descenso de gradiente hasta filtros adaptativos üññüèº üèÜ ‚Ü©Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Desarrollando el tema de los res√∫menes en la especialidad de maestr√≠a "Procesamiento de comunicaci√≥n y se√±al" (TU Ilmenau), me gustar√≠a continuar uno ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Filtrado lineal √≥ptimo: desde descenso de gradiente hasta filtros adaptativos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455497/"><p>  Desarrollando el tema de los res√∫menes en la especialidad de maestr√≠a "Procesamiento de comunicaci√≥n y se√±al" (TU Ilmenau), me gustar√≠a continuar uno de los temas principales del curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Procesamiento de se√±al adaptativo y de matriz"</a> .  A saber, los conceptos b√°sicos del filtrado adaptativo. </p><br><p>  <u>Para qui√©n se escribi√≥ este art√≠culo por primera vez:</u> <u><br></u> <br>  1) para una hermandad estudiantil de una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">especialidad nativa</a> ; <br>  2) para los docentes que preparan seminarios pr√°cticos, pero que a√∫n no han decidido las herramientas: a continuaci√≥n se muestran ejemplos en <strong>python</strong> y <strong>Matlab / Octave</strong> ; <br>  3) para cualquier persona interesada en el tema del filtrado. </p><br><p>  <u>Lo que se puede encontrar debajo del corte:</u> <u><br></u> <br>  1) informaci√≥n de la teor√≠a, que trat√© de organizar de la manera m√°s concisa posible, pero me parece informativa; <br>  2) ejemplos del uso de filtros: en particular, como parte del ecualizador para el conjunto de antenas; <br>  3) enlaces a literatura b√°sica y bibliotecas abiertas (en python), que pueden ser √∫tiles para la investigaci√≥n. </p><br><p>  En general, bienvenido y clasifiquemos todo por puntos. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f1/98b/d67/2f198bd673789161db58ad770c629faf.jpg"></p><a name="habracut"></a><br><p>  <em>La persona pensativa en la foto es familiar para muchos, creo, Norbert Wiener.</em>  <em>En su mayor parte, estudiaremos el filtro de su nombre.</em>  <em>Sin embargo, uno no puede dejar de mencionar a nuestro compatriota: Andrei Nikolaevich Kolmogorov, cuyo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo de 1941</a> tambi√©n hizo una contribuci√≥n significativa al desarrollo de la teor√≠a de filtrado √≥ptima, que incluso en fuentes en ingl√©s se llama la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">teor√≠a de filtrado de Kolmogorov-Wiener</a> .</em> </p><br><h2 id="chto-rassmatrivaem">  ¬øQu√© estamos considerando? </h2><br><p>  Hoy estamos considerando un filtro cl√°sico con una respuesta de impulso finita (FIR, respuesta de impulso finita), que puede describirse mediante el siguiente circuito simple (Fig. 1). </p><br><p><img src="https://habrastorage.org/webt/to/or/u7/tooru7vj_f6aj0oe9wvpcde28kw.png"></p><br><p>  <em>Fig.1.</em>  <em>El esquema de filtro FIR para estudiar el filtro Wiener. [1.</em>  <em>p.117]</em> </p><br><p>  Escribiremos en forma matricial lo que ser√° exactamente a la salida de este stand: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/43f/615/39b/43f61539ba1f68998b24c39a8c539706.svg" alt="e (n) = d (n) - \ hat {d} (n | \ mathcal {U} _n) = d (n) - \ mathbf {w} ^ H \ mathbf {u} \ qquad (1)"></div><br><p>  Descifrar la notaci√≥n: </p><br><ul><li><img src="https://habrastorage.org/getpro/habr/post_images/6c5/882/04e/6c588204ed25c8bbb270106d7f08a4dd.svg" alt="e (n)">  Es la diferencia (error) entre las se√±ales dadas y recibidas </li><li><img src="https://habrastorage.org/getpro/habr/post_images/771/807/bef/771807bef08a5612654d97e67695cf07.svg" alt="d (n)">  Es alguna se√±al predefinida </li><li><img src="https://habrastorage.org/getpro/habr/post_images/960/b4e/a48/960b4ea48f3968f42c64eed1af640e1d.svg" alt="\ mathbf {u}">  Es un vector de muestras o, en otras palabras, una se√±al en la entrada del filtro </li><li><img src="https://habrastorage.org/getpro/habr/post_images/75f/f22/a5a/75ff22a5a4f95cbe489056bf704597f0.svg" alt="\ hat {d} (n | \ mathcal {U} _n)">  ¬øEs la se√±al en la salida del filtro? </li><li><img src="https://habrastorage.org/getpro/habr/post_images/6f6/81f/9be/6f681f9be2ae30d1666fec498b59b3a3.svg" alt="\ mathbf {w} ^ H">  - esta es una conjugaci√≥n hermitiana del vector de coeficiente de filtro - <u>es en su selecci√≥n √≥ptima donde radica la adaptabilidad del filtro</u> </li></ul><br><p>  Probablemente ya haya adivinado que buscaremos la menor diferencia entre la se√±al dada y la filtrada, es decir, el error m√°s peque√±o.  Esto significa que nos enfrentamos a una tarea de optimizaci√≥n. </p><br><h2 id="chto-budem-optimizirovat">  ¬øQu√© vamos a optimizar? </h2><br><p>  Para optimizar, o m√°s bien minimizar, no solo nos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">referiremos</a> <strong>al</strong> error, el <strong>error cuadr√°tico medio</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MSE - Error cuadr√°tico medio</a> ): </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b83/b62/193/b83b62193ca3490681c2cd8910e4d99a.svg" alt="MSE: J (\ mathbf {w}) = E \ {e (n) ^ 2 \} \ qquad (2)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/c48/76a/b10/c4876ab1024579fa30ea997a45efd50a.svg" alt="J (\ mathbf {w})">  denota la funci√≥n de costo del vector de coeficientes de filtro, y <img src="https://habrastorage.org/getpro/habr/post_images/f39/8e4/ded/f398e4ded6db9e55108d575a9b7d2f1f.svg" alt="E \ {* \}">  denota mat.  esperando </p><br><p>  El cuadrado en este caso es muy agradable, ya que significa que nos enfrentamos con el problema de la <em>programaci√≥n convexa</em> (busqu√© en Google solo un an√°logo de la <em>optimizaci√≥n convexa en</em> ingl√©s), que, a su vez, implica un <u>extremo √∫nico</u> (en nuestro caso, un m√≠nimo). </p><br><p><img src="https://habrastorage.org/webt/hr/xj/mb/hrxjmbmimv7c2uvvicnuklqn9y0.png"></p><br><p>  <em>Fig.2.</em>  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La superficie del error cuadr√°tico medio</a> .</em> </p><br><p>  Nuestra funci√≥n de error tiene una forma can√≥nica [1, p. 121]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5da/121/788/5da121788f801656b55ee459c2f4d56d.svg" alt="J (\ mathbf {w}) = \ sigma ^ 2_d - \ mathbf {w} ^ H \ mathbf {p} - \ mathbf {p} ^ H \ mathbf {w} + \ mathbf {w} ^ H \ mathbf { R} \ mathbf {w} \ qquad (3)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/24f/50c/410/24f50c410ab0c2ca3dd302c630c734e8.svg" alt="\ sigma ^ 2_d">  Es la varianza de la se√±al esperada, <img src="https://habrastorage.org/getpro/habr/post_images/82c/861/559/82c86155992ccb2e83d6f9f3f9e92737.svg" alt="\ mathbf {p} = E \ {\ mathbf {u} (n) d ^ * (n) \}">  Es el vector de correlaci√≥n cruzada entre el vector de entrada y la se√±al esperada, y <img src="https://habrastorage.org/getpro/habr/post_images/664/01c/a88/66401ca883516093b2e73b7d519588ac.svg" alt="\ mathbf {R} = E \ {\ mathbf {u} (n) \ mathbf {u} ^ H (n) \}">  Es la matriz de autocorrelaci√≥n de la se√±al de entrada. </p><br><div class="spoiler">  <b class="spoiler_title">La conclusi√≥n de esta f√≥rmula est√° aqu√≠ (lo prob√© m√°s claramente).</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/q6/sw/p5/q6swp5meopsxauj7yvygkwuc7-g.png" width="650"></div></div><br><p>  Como se√±alamos anteriormente, si estamos hablando de programaci√≥n convexa, tendremos un extremo (m√≠nimo).  Entonces, para encontrar el valor m√≠nimo de la funci√≥n de costo (el m√≠nimo error cuadr√°tico medio), es suficiente encontrar la tangente de la pendiente de la tangente o, en otras palabras, la <u>derivada parcial</u> con <u>respecto</u> a nuestra variable estudiada: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1cd/d0b/114/1cdd0b114b5457761dd27338b4ee57f4.svg" alt="\ frac {\ delta J (\ mathbf {w})} {\ delta w ^ *} = - \ mathbf {p} + \ mathbf {R} \ mathbf {w} \ qquad (4)"></div><br><p>  En el mejor de los casos ( <img src="https://habrastorage.org/getpro/habr/post_images/ac6/219/d1c/ac6219d1cc1885e6f5936e40b5c7a980.svg" alt="\ mathbf {w} = \ mathbf {w} _ {opt}">  ), el error deber√≠a ser, por supuesto, m√≠nimo, lo que significa que equiparamos la derivada a cero: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/777/46e/9a7/77746e9a7ab3ff9df3cf340da4b7238d.svg" alt="\ mathbf {R} \ mathbf {w} _ {opt} = \ mathbf {p} \ qquad (5)"></div><br><p>  En realidad, aqu√≠ est√°, nuestra estufa, desde la cual bailaremos m√°s: delante de nosotros hay un <u>sistema de ecuaciones lineales</u> . </p><br><h2 id="kak-budem-reshat">  ¬øC√≥mo vamos a decidir? </h2><br><p>  Cabe se√±alar de inmediato que ambas soluciones, que consideraremos a continuaci√≥n, en este caso son te√≥ricas y educativas, ya que <img src="https://habrastorage.org/getpro/habr/post_images/1cf/d71/499/1cfd714992b16fcc961ad10bcc855134.svg" alt="\ mathbf {R}">  y <img src="https://habrastorage.org/getpro/habr/post_images/1fa/0e8/e9e/1fa0e8e9e33d7dbd533901bbf025bd9f.svg" alt="\ mathbf {p}">  conocido de antemano (es decir, ten√≠amos la supuesta capacidad de recopilar estad√≠sticas suficientes para calcularlos).  Sin embargo, el an√°lisis de tales ejemplos simplificados aqu√≠ es lo mejor que puede pensar para comprender los enfoques b√°sicos. </p><br><h3 id="analiticheskoe-reshenie">  Soluci√≥n anal√≠tica </h3><br><p>  Este problema puede resolverse, por as√≠ decirlo, en la frente, utilizando matrices inversas: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/759/c3e/052/759c3e052c502aa04fe6da682a41ea2a.svg" alt="\ mathbf {w} _ {opt} = \ mathbf {R} ^ {- 1} \ mathbf {p} \ qquad (6)"></div><br><p>  Tal expresi√≥n se llama <strong>la</strong> ecuaci√≥n de Wiener-Hopf; todav√≠a es √∫til para nosotros como referencia. </p><br><blockquote>  Por supuesto, para ser completamente meticuloso, probablemente ser√≠a m√°s correcto escribir este caso de manera general, es decir  no con <img src="https://habrastorage.org/getpro/habr/post_images/bd8/f0f/04a/bd8f0f04a92fe1055c350d4e32a8a256.svg" alt="^ {-}">  y con <img src="https://habrastorage.org/getpro/habr/post_images/017/3d5/ed9/0173d5ed99b25d00ec4245287142f165.svg" alt="^ +">  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pseudoinvertir</a> ): <br><img src="https://habrastorage.org/getpro/habr/post_images/003/47f/bca/00347fbca15f40943c7fb7b20c38a3f9.svg" alt="\ mathbf {R} ^ + = \ mathbf {R} ^ H (\ mathbf {R} \ mathbf {R} ^ H) ^ {- 1}"><br><br>  Sin embargo, la matriz de autocorrelaci√≥n no puede ser no cuadrada o singular, por lo tanto, con bastante raz√≥n, creemos que no hay contradicci√≥n. </blockquote><p>  A partir de esta ecuaci√≥n, es anal√≠ticamente posible deducir cu√°l ser√° el valor m√≠nimo de la funci√≥n de costo (es decir, en nuestro caso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MMSE</a> - error cuadrado m√≠nimo m√≠nimo): </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c00/d2d/1f9/c00d2d1f9427762a17deae92ae3ea77c.svg" alt="J_ {min} = J (\ mathbf {w} _ {opt}) = \ sigma ^ 2_d - \ mathbf {p} ^ H \ mathbf {R} ^ {- 1} \ mathbf {p} \ qquad (7)"></div><br><div class="spoiler">  <b class="spoiler_title">La derivaci√≥n de la f√≥rmula est√° aqu√≠ (tambi√©n trat√© de hacerla m√°s colorida).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/ds/vh/nx/dsvhnxoudmo_qd4hesg_qkysdg8.jpeg"></p></div></div><br><p>  Bueno, hay una soluci√≥n. </p><br><h3 id="reshenie-iterativnym-metodom">  Soluci√≥n iterativa </h3><br><p>  Sin embargo, s√≠, es posible resolver un sistema de ecuaciones lineales sin invertir la matriz de autocorrelaci√≥n de forma iterativa ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">para guardar los c√°lculos</a> ).  Para este prop√≥sito, considere el <strong>m√©todo</strong> nativo y comprensible <strong>de descenso de gradiente</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√©todo de descenso m√°s pronunciado / gradiente</a> ). </p><br><p>  La esencia del algoritmo se puede reducir a lo siguiente: </p><br><ol><li>  Establecemos la variable deseada en alg√∫n valor predeterminado (por ejemplo, <img src="https://habrastorage.org/getpro/habr/post_images/231/554/6f0/2315546f0e8aa127a8da693d41c53ff6.svg" alt="\ mathbf {w} (0) = \ mathbf {0}">  ) </li><li>  Elige un paso <img src="https://habrastorage.org/getpro/habr/post_images/849/a42/16c/849a4216c1bc55877bc86f4a97513f7a.svg" alt="\ mu">  (c√≥mo exactamente elegimos, hablaremos a continuaci√≥n). </li><li>  Y luego, por as√≠ decirlo, bajamos a lo largo de nuestra superficie original (en nuestro caso, esta es la superficie MSE) con un paso dado <img src="https://habrastorage.org/getpro/habr/post_images/849/a42/16c/849a4216c1bc55877bc86f4a97513f7a.svg" alt="\ mu">  y una cierta velocidad determinada por la magnitud del gradiente. </li></ol><br><p>  De ah√≠ el nombre: <em>gradiente</em> - gradiente o <em>m√°s empinado</em> - <em>descenso</em> gradual - descenso. </p><br><p>  El gradiente en nuestro caso ya es conocido: de hecho, lo encontramos cuando diferenciamos la funci√≥n de costo (la superficie es c√≥ncava, comp√°rela con [1, p. 220]).  Escribimos c√≥mo se ver√° la f√≥rmula para la actualizaci√≥n iterativa de la variable deseada (coeficientes de filtro) [1, p.  220]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d6/4ff/7eb/6d64ff7eb98c4274e05a33a3eb127933.svg" alt="\ mathbf {w} (n + 1) = \ mathbf {w} (n) - \ mu [- \ mathbf {p} + \ mathbf {R} \ mathbf {w} (n)] \ qquad (8)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/fd6/0b2/b5b/fd60b2b5be4b7e93a0d905dd970c314f.svg" alt="n">  Es el n√∫mero de iteraci√≥n. </p><br><p>  Ahora hablemos de elegir un tama√±o de paso. </p><br><p>  Enumeramos las premisas obvias: </p><br><ul><li>  el paso no puede ser negativo o cero </li><li>  el paso no debe ser demasiado grande, de lo contrario el algoritmo no converger√° (saltar√° de borde a borde, sin caer en el extremo) </li><li>  el paso, por supuesto, puede ser muy peque√±o, pero esto tampoco es del todo deseable: el algoritmo pasar√° m√°s tiempo </li></ul><br><p>  Con respecto al filtro Wiener, las restricciones, por supuesto, ya se han encontrado hace mucho tiempo [1, p. 222-226]: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/67d/64a/b6c/67d64ab6cf46d3438791ccea853421fb.svg" alt="0 <\ mu <\ frac {2} {\ lambda_ {max}} \ qquad (9)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/e5d/fa8/35d/e5dfa835ded907ecd7cf2b56d7061307.svg" alt="\ lambda_ {max}">  Es el valor propio m√°s grande de la matriz de autocorrelaci√≥n. <img src="https://habrastorage.org/getpro/habr/post_images/1cf/d71/499/1cfd714992b16fcc961ad10bcc855134.svg" alt="\ mathbf {R}">  . </p><br><blockquote>  Por cierto, los valores propios y los vectores son un tema interesante aparte en el contexto del filtrado lineal.  Incluso hay un <em>filtro Eigen</em> completo <em>para</em> este caso (ver Ap√©ndice 1). </blockquote><p>  Pero esto, afortunadamente, no es todo.  Tambi√©n hay una soluci√≥n √≥ptima y adaptativa: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/3c2/fdf/f4c3c2fdf8ae926094bb67b391cd896b.svg" alt="\ mu (n) = \ frac {\ mathbf {\ gamma} (n) ^ H \ mathbf {\ gamma} (n)} {\ mathbf {\ gamma} (n) ^ H \ mathbf {R} \ mathbf { \ gamma} (n)} \ qquad (10)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/1b0/4a6/a31/1b04a6a318f99ec501290f59c0f924ac.svg" alt="\ mathbf {\ gamma} (n) = \ mathbf {p} - \ mathbf {R} \ mathbf {w} (n)">  Es un gradiente negativo.  Como se puede ver en la f√≥rmula, el paso se relata en cada iteraci√≥n, es decir, se adapta. </p><br><div class="spoiler">  <b class="spoiler_title">La conclusi√≥n de la f√≥rmula est√° aqu√≠ (muchas matem√°ticas, mira solo a los mismos nerds notorios como yo).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/au/hq/0s/auhq0sxrspaduxdkclqctns1xtw.jpeg"></p></div></div><br><p>  De acuerdo, para la segunda decisi√≥n, tambi√©n preparamos el escenario. </p><br><h2 id="a-nelzya-li-na-primerah">  ¬øPero es posible con ejemplos? </h2><br><p>  En aras de la claridad, realizaremos una peque√±a simulaci√≥n.  Utilizaremos <strong>Python 3.6.4</strong> . </p><br><blockquote>  Dir√© de inmediato que estos ejemplos son parte de una de las tareas asignadas, cada una de las cuales se ofrece a los estudiantes para su soluci√≥n dentro de dos semanas.  Reescrib√≠ la parte bajo Python (para popularizar el lenguaje entre los ingenieros de radio).  Quiz√°s encuentre otras opciones en la Web de otros antiguos alumnos. </blockquote><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.linalg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> toeplitz <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convmtx</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(h,n)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> toeplitz(np.hstack([h, np.zeros(n<span class="hljs-number"><span class="hljs-number">-1</span></span>)]),\ np.hstack([h[<span class="hljs-number"><span class="hljs-number">0</span></span>], np.zeros(n<span class="hljs-number"><span class="hljs-number">-1</span></span>)])) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">MSE_calc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sigmaS, R, p, w)</span></span></span><span class="hljs-function">:</span></span> w = w.reshape(w.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) wH = np.conj(w).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, w.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) p = p.reshape(p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) pH = np.conj(p).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) MSE = sigmaS - np.dot(wH, p) - np.dot(pH, w) + np.dot(np.dot(wH, R), w) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> MSE[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mu_opt_calc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(gamma, R)</span></span></span><span class="hljs-function">:</span></span> gamma = gamma.reshape(gamma.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) gammaH = np.conj(gamma).reshape(<span class="hljs-number"><span class="hljs-number">1</span></span>, gamma.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) mu_opt = np.dot(gammaH, gamma) / np.dot(np.dot(gammaH, R), gamma) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> mu_opt[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  Utilizaremos nuestro filtro lineal para el problema de <u>ecualizaci√≥n</u> del <u>canal</u> , cuyo objetivo principal es nivelar los diversos efectos de este canal en la se√±al √∫til. </p><br><blockquote>  El c√≥digo fuente se puede descargar en un archivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> o <a href="">aqu√≠</a> (s√≠, ten√≠a un pasatiempo, editar Wikipedia). </blockquote><br><h3 id="model-sistemy">  Modelo del sistema </h3><br><p>  Supongamos que hay un conjunto de antenas (ya lo examinamos en un art√≠culo sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">M√öSICA</a> ). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/61a/9c2/da7/61a9c2da745081459f9001d0252936f1.png"></p><br><p>  <em>Fig.</em>  <em>3. Conjunto de antenas lineales no direccionales (ULAA - conjunto de antenas lineales uniforme) [2, p.</em>  <em>32]</em> </p><br><p>  Defina los par√°metros iniciales de la red: </p><br><pre> <code class="python hljs">M = <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-comment"><span class="hljs-comment">#    (number of sensors)</span></span></code> </pre> <br><p>  En este art√≠culo, consideraremos algo as√≠ como un <u>canal de banda ancha con desvanecimiento</u> , una caracter√≠stica de la cual es <u>la propagaci√≥n por trayectos m√∫ltiples</u> .  Para tales casos, generalmente se aplica un enfoque en el que cada haz se modela utilizando un retraso de cierta magnitud (Fig. 4). </p><br><p><img src="https://habrastorage.org/webt/3t/tc/va/3ttcvau0o4njat-1beejefcudpy.png"></p><br><p>  <em>Fig.</em>  <em>4. El modelo del canal de banda ancha con n retrasos fijos. [3, p.</em>  <em>29]</em>  <em>Como comprender√°, las designaciones espec√≠ficas no juegan un papel, en lo que sigue utilizaremos algunas ligeramente diferentes.</em> </p><br><p>  El modelo de la se√±al recibida para un sensor se expresa de la siguiente manera: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a3f/c3f/2f0/a3fc3f2f0cd278622946ce2da28005fd.svg" alt="x (n) = \ sum_ {l = 0} ^ Lh (l) s (n-l) + \ nu (n)"></div><br><p>  En este caso <img src="https://habrastorage.org/getpro/habr/post_images/fd6/0b2/b5b/fd60b2b5be4b7e93a0d905dd970c314f.svg" alt="n">  indica el n√∫mero de referencia, <img src="https://habrastorage.org/getpro/habr/post_images/e9f/c39/8e5/e9fc398e58e24442ddc2cf11684debbc.svg" alt="h (l)">  Es la respuesta del canal a lo largo del <em>l-</em> √©simo haz, <em>L</em> es el n√∫mero de registros de retardo, <em>s</em> es la se√±al transmitida (√∫til), <img src="https://habrastorage.org/getpro/habr/post_images/270/81a/400/27081a40025995898a2b982ff59a7e39.svg" alt="\ nu (n)">  - ruido aditivo. </p><br><p>  Para varios sensores, la f√≥rmula tomar√° la forma: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c0e/835/aa7/c0e835aa74d5dccf4ccdf625ebcc88b4.svg" alt="\ mathbf {x} (n) = \ mathbf {H} \ mathbf {s} (n) + \ mathbf {\ nu} (n)"></div><br><p>  donde <img src="https://habrastorage.org/getpro/habr/post_images/b0e/184/0ed/b0e1840edef9169f3e1f52974bea066d.svg" alt="\ mathbf {x} (n)">  y <img src="https://habrastorage.org/getpro/habr/post_images/270/81a/400/27081a40025995898a2b982ff59a7e39.svg" alt="\ mathbf {\ nu} (n)">  - tener dimensi√≥n <img src="https://habrastorage.org/getpro/habr/post_images/132/724/0f2/1327240f26480a83dffca393bb730c44.svg" alt="M \ veces 1">  dimensi√≥n <img src="https://habrastorage.org/getpro/habr/post_images/ed0/8c1/e77/ed08c1e77cfaf357d5e90e9e2ae918aa.svg" alt="\ mathbf {H}">  es igual a <img src="https://habrastorage.org/getpro/habr/post_images/25e/e63/fbb/25ee63fbb4a32f1a00d5c02aaea9b80c.svg" alt="M \ veces (M-L)">  y la dimensi√≥n <img src="https://habrastorage.org/getpro/habr/post_images/5e3/d36/045/5e3d360455a5a33db6c17f93c119a694.svg" alt="\ mathbf {s} (n)">  es igual <img src="https://habrastorage.org/getpro/habr/post_images/6e2/fc1/636/6e2fc16365ac2698555dd979ed6b5eeb.svg" alt="(M-L) \ veces 1">  . </p><br><p>  Suponga que cada sensor tambi√©n recibe una se√±al con cierto retraso, debido a la incidencia de la onda en √°ngulo.  Matriz <img src="https://habrastorage.org/getpro/habr/post_images/ed0/8c1/e77/ed08c1e77cfaf357d5e90e9e2ae918aa.svg" alt="\ mathbf {H}">  en nuestro caso, ser√° una matriz convolucional para el vector de respuesta para cada rayo.  Creo que el c√≥digo ser√° m√°s claro: </p><br><pre> <code class="python hljs">h = np.array([<span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">0.779</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">0.722</span></span>, <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1j</span></span>*<span class="hljs-number"><span class="hljs-number">1.862</span></span>]) L = len(h)<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-comment"><span class="hljs-comment"># number of signal sources H = convmtx(h,ML) print(H.shape) print(H)</span></span></code> </pre> <br><p>  La conclusi√≥n ser√°: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) &gt;&gt;&gt; array([[ <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> ], [<span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>, <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> ], [<span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>, <span class="hljs-number"><span class="hljs-number">0.722</span></span><span class="hljs-number"><span class="hljs-number">-0.779j</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>, <span class="hljs-number"><span class="hljs-number">-0.257</span></span><span class="hljs-number"><span class="hljs-number">-0.722j</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">0.</span></span> +<span class="hljs-number"><span class="hljs-number">0.j</span></span> , <span class="hljs-number"><span class="hljs-number">-0.789</span></span><span class="hljs-number"><span class="hljs-number">-1.862j</span></span>]])</code> </pre> <br><p>  A continuaci√≥n, establecemos los datos iniciales para la se√±al y el ruido √∫tiles: </p><br><pre> <code class="python hljs">sigmaS = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-comment"><span class="hljs-comment">#    (the signal's s(n) power) sigmaN = 0.01 #   (the noise's n(n) power)</span></span></code> </pre> <br><p>  Ahora pasamos a las correlaciones. </p><br><pre> <code class="python hljs">Rxx = (sigmaS)*(np.dot(H,np.matrix(H).H))+(sigmaN)*np.identity(M) p = (sigmaS)*H[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] p = p.reshape((len(p), <span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">La derivaci√≥n de las f√≥rmulas aqu√≠ (tambi√©n una hoja para los m√°s desesperados).</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/hi/lh/ks/hilhksxoc_rkum_5ibn3m42ukxc.jpeg"></p></div></div><br><p>  Encontramos una soluci√≥n para Wiener: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Solution of the Wiener-Hopf equation: wopt = np.dot(np.linalg.inv(Rxx), p) MSEopt = MSE_calc(sigmaS, Rxx, p, wopt)</span></span></code> </pre> <br><p>  Ahora pasemos al m√©todo de descenso de gradiente. </p><br><p>  Encuentre el valor propio m√°s grande para que el l√≠mite superior del paso pueda derivarse de √©l (vea la f√≥rmula (9)): </p><br><pre> <code class="python hljs">lamda_max = max(np.linalg.eigvals(Rxx))</code> </pre> <br><p>  Ahora establezcamos una serie de pasos que constituir√°n una cierta fracci√≥n del m√°ximo: </p><br><pre> <code class="python hljs">coeff = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>]) mus = <span class="hljs-number"><span class="hljs-number">2</span></span>/lamda_max*coeff <span class="hljs-comment"><span class="hljs-comment"># different step sizes</span></span></code> </pre> <br><p>  Defina el n√∫mero m√°ximo de iteraciones: </p><br><pre> <code class="python hljs">N_steps = <span class="hljs-number"><span class="hljs-number">100</span></span></code> </pre> <br><p>  Ejecute el algoritmo: </p><br><pre> <code class="python hljs">MSE = np.empty((len(mus), N_steps), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> mu_idx, mu <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(mus): w = np.zeros((M,<span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> N_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(N_steps): w = w - mu*(np.dot(Rxx, w) - p) MSE[mu_idx, N_i] = MSE_calc(sigmaS, Rxx, p, w)</code> </pre> <br><p>  Ahora haremos lo mismo, pero para el paso adaptativo (f√≥rmula (10)): </p><br><pre> <code class="python hljs">MSEoptmu = np.empty((<span class="hljs-number"><span class="hljs-number">1</span></span>, N_steps), dtype=complex) w = np.zeros((M,<span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> N_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(N_steps): gamma = p - np.dot(Rxx,w) mu_opt = mu_opt_calc(gamma, Rxx) w = w - mu_opt*(np.dot(Rxx,w) - p) MSEoptmu[:, N_i] = MSE_calc(sigmaS, Rxx, p, w)</code> </pre> <br><p>  Deber√≠as obtener algo como esto: </p><br><div class="spoiler">  <b class="spoiler_title">Dibujo</b> <div class="spoiler_text"><pre> <code class="python hljs">x = [i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, N_steps+<span class="hljs-number"><span class="hljs-number">1</span></span>)] plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>), dpi=<span class="hljs-number"><span class="hljs-number">300</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx, item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(coeff): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> item == <span class="hljs-number"><span class="hljs-number">1</span></span>: item = <span class="hljs-string"><span class="hljs-string">''</span></span> plt.loglog(x, np.abs(MSE[idx, :]),\ label=<span class="hljs-string"><span class="hljs-string">'$\mu = '</span></span>+str(item)+<span class="hljs-string"><span class="hljs-string">'\mu_{max}$'</span></span>) plt.loglog(x, np.abs(MSEoptmu[<span class="hljs-number"><span class="hljs-number">0</span></span>, :]),\ label=<span class="hljs-string"><span class="hljs-string">'$\mu = \mu_{opt}$'</span></span>) plt.loglog(x, np.abs(MSEopt*np.ones((len(x), <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=complex)),\ label = <span class="hljs-string"><span class="hljs-string">'Wiener solution'</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'Number of steps'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'Mean-Square Error'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Steepest descent'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'best'</span></span>) plt.minorticks_on() plt.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>) plt.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>) plt.show()</code> </pre> </div></div><br><p><img src="https://habrastorage.org/webt/il/fa/8d/ilfa8dmoxgt4sjitiyvwbdjga6m.png"></p><br><p>  <em>Fig.</em>  <em>5. Curvas de aprendizaje para pasos de diferentes tama√±os.</em> </p><br><p>  Fijaciones por el simple hecho de hablar los puntos principales sobre el descenso de gradiente: </p><br><ul><li>  como se esperaba, el paso √≥ptimo da la convergencia m√°s r√°pida; </li><li>  ya no significa mejor: habiendo excedido el l√≠mite superior, no hemos alcanzado la convergencia en absoluto. </li></ul><br><p>  As√≠ que encontramos el vector √≥ptimo de coeficientes de filtro que nivelar√° mejor los efectos del canal: hemos <u>entrenado el ecualizador</u> . </p><br><h2 id="a-est-chto-to-bolee-blizkoe-k-realnosti">  ¬øHay algo m√°s cercano a la realidad? </h2><br><p>  Por supuesto!  Ya hemos dicho varias veces que recopilar estad√≠sticas (es decir, calcular matrices de correlaci√≥n y vectores) en sistemas en tiempo real est√° lejos de ser siempre un lujo asequible.  Sin embargo, la humanidad se ha adaptado a estas dificultades: en lugar de un enfoque <em>determinista</em> en la pr√°ctica, se utilizan enfoques <u>adaptativos</u> .  Se pueden dividir en dos grandes grupos [1, p.  246]: </p><br><ul><li>  <em>probabil√≠stico (estoc√°stico)</em> (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SG</a> - Gradiente estoc√°stico) </li><li>  y basado en el m√©todo de <em>m√≠nimos cuadrados</em> (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LMS</a> : m√≠nimos cuadrados m√≠nimos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RLS</a> : m√≠nimos cuadrados recursivos) </li></ul><br><p>  El tema de los filtros adaptativos est√° bien representado en la comunidad de c√≥digo abierto (ejemplos para python): </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">piromacoustica</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">padasip</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">adaptfilt</a> </li></ul><br><blockquote>  En el segundo ejemplo, me gusta especialmente la documentaci√≥n.  Sin embargo, ten cuidado!  Cuando prob√© el paquete <strong>padasip</strong> , me encontr√© con dificultades para manejar n√∫meros complejos (por defecto, float64 est√° impl√≠cito all√≠).  Quiz√°s surjan los mismos problemas al trabajar con otras implementaciones. </blockquote><p>  Los algoritmos, por supuesto, tienen sus propias ventajas y desventajas, cuya suma determina el alcance del algoritmo. </p><br><p>  Echemos un vistazo r√°pido a los ejemplos: consideraremos los tres algoritmos <em>SG</em> , <em>LMS</em> y <em>RLS</em> que ya hemos mencionado (modelaremos en el lenguaje MATLAB, lo confieso, ya hab√≠a espacios en blanco y reescribiremos todo para uniformizar Python por el bien de ... bueno ...). </p><br><p>  Se puede encontrar una descripci√≥n de los algoritmos <em>LMS</em> y <em>RLS</em> , por ejemplo, en el muelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">padasip</a> . </p><br><div class="spoiler">  <b class="spoiler_title">La descripci√≥n de SG se puede encontrar aqu√≠.</b> <div class="spoiler_text"><p>  La principal diferencia del descenso de gradiente es un gradiente variable: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/76f/9e3/fbb/76f9e3fbbb4c7643f5af595103791091.svg" alt="\ mathbf {w} [n] = \ mathbf {w} [n-1] + \ mu \ left (\ mathbf {\ hat {p}} [n] - \ mathbf {\ hat {R}} _ {xx } [n] \ mathbf {w} [n-1] \ right)"></div><br><p>  a las </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/633/bac/0aa/633bac0aae95be15cd31a312b4e0d2c5.svg" alt="\ mathbf {\ hat {R}} _ {xx} [n] = \ frac {1} {n} \ left ((n-1) \ mathbf {\ hat {R}} _ {xx} [n-1 ] + \ mathbf {x} [n] \ mathbf {x} [n] ^ H \ right)"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f7/bfd/f81/4f7bfdf81571e19eac474c7a8c380093.svg" alt="\ mathbf {\ hat {p}} [n] = \ frac {1} {n} \ left ((n-1) \ mathbf {\ hat {p}} [n-1] + \ mathbf {x} [ n] d [n] ^ * \ derecha)"></div></div></div><br><p>  1) Un caso similar al considerado anteriormente. </p><br><div class="spoiler">  <b class="spoiler_title">Fuentes (MatLab / Octave).</b> <div class="spoiler_text"><p>  Las fuentes se pueden descargar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </p></div></div><br><p><img src="https://habrastorage.org/webt/ff/zm/hq/ffzmhqsrnwvvc0hdrcyzhapropw.png"></p><br><p>  <em>Fig.</em>  <em>6. Curvas de aprendizaje para LMS, RLS y SG.</em> </p><br><p>  Se puede notar de inmediato que, con su relativa simplicidad, el algoritmo LMS puede, en principio, no llegar a una soluci√≥n √≥ptima con un paso relativamente grande.  RLS da el resultado m√°s r√°pido, pero tambi√©n puede fallar con un <em>factor de olvido</em> relativamente peque√±o.  Hasta ahora, SG est√° bien, pero veamos otro ejemplo. </p><br><p>  2) El caso cuando el canal cambia en el tiempo. </p><br><div class="spoiler">  <b class="spoiler_title">Fuentes (MatLab / Octave).</b> <div class="spoiler_text"><p>  Las fuentes se pueden descargar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </p></div></div><br><p><img src="https://habrastorage.org/webt/v-/9d/sx/v-9dsxxwzr9jnbnswf0dmnvqrcu.png"></p><br><p>  <em>Fig.</em>  <em>7. Curvas de aprendizaje para LMS, RLS y SG (cambios de canal con el tiempo).</em> </p><br><p>  Y aqu√≠ la imagen ya es mucho m√°s interesante: con un cambio brusco en la respuesta del canal, LMS ya parece ser la soluci√≥n m√°s confiable.  ¬øQui√©n lo hubiera pensado?  Aunque RLS con el factor de olvido correcto tambi√©n proporciona un resultado aceptable. </p><br><div class="spoiler">  <b class="spoiler_title">Algunas palabras sobre el rendimiento.</b> <div class="spoiler_text"><p>  S√≠, por supuesto, cada algoritmo tiene su propia complejidad computacional espec√≠fica, pero seg√∫n mis mediciones, mi vieja m√°quina puede hacer frente a un conjunto de aproximadamente 120 Œºs por iteraci√≥n en el caso de LMS y SG y aproximadamente 250 Œºs por iteraci√≥n en el caso de RLS.  Es decir, la diferencia es, en general, comparable. </p></div></div><br><p>  Y eso es todo por hoy.  Gracias a todos los que miraron! </p><br><h2 id="literatura">  Literatura </h2><br><ol><li>  Teor√≠a de filtro adaptativo Haykin SS.  - Pearson Education India, 2005. </li><li>  Haykin, Simon y KJ Ray Liu.  Manual sobre procesamiento de matriz y redes de sensores.  Vol.  63. John Wiley &amp; Sons, 2010. pp.  102-107 </li><li>  Arndt, D. (2015).  Sobre el modelado de canales para la recepci√≥n satelital m√≥vil terrestre (disertaci√≥n doctoral). </li></ol><br><h2 id="prilozhenie-1">  Ap√©ndice 1 </h2><br><div class="spoiler">  <b class="spoiler_title">Filtro Eigen</b> <div class="spoiler_text"><p>  El objetivo principal de dicho filtro es maximizar la relaci√≥n se√±al / ruido (SNR). </p><br><p><img src="https://habrastorage.org/webt/kk/v_/uu/kkv_uu-08dppu5i4yhkucc_b0ww.jpeg"></p><br><p>  Pero a juzgar por la presencia de correlaciones en los c√°lculos, esto tambi√©n es m√°s una construcci√≥n te√≥rica que una soluci√≥n pr√°ctica. </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455497/">https://habr.com/ru/post/455497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455483/index.html">Artista Ai-Da: robot humanoide se prepara para su primera exposici√≥n individual</a></li>
<li><a href="../455485/index.html">Check Point Scripts: ejecute scripts directamente desde Smart Console</a></li>
<li><a href="../455487/index.html">Entrenamiento Cisco 200-125 CCNA v3.0. D√≠a 10. Cambiar modos de funcionamiento del puerto</a></li>
<li><a href="../455489/index.html">Conexi√≥n de soluciones de audio y video de terceros a los equipos de Microsoft</a></li>
<li><a href="../455493/index.html">Lo nuevo en la versi√≥n Angular 8</a></li>
<li><a href="../455501/index.html">C√≥mo Hollywood usa secretamente la inteligencia artificial para tomar decisiones clave de filmaci√≥n</a></li>
<li><a href="../455503/index.html">19 conceptos que debes aprender para convertirte en un desarrollador eficaz de Angular</a></li>
<li><a href="../455505/index.html">Reaccionar la aceleraci√≥n de la aplicaci√≥n cuatro veces</a></li>
<li><a href="../455507/index.html">Descripci√≥n general del paquete Datatable Python</a></li>
<li><a href="../455509/index.html">La historia de por qu√© sigo usando jQuery</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>