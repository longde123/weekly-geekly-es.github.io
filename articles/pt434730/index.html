<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÖ üñçÔ∏è üéâ Bancos de dados na mem√≥ria: aplica√ß√£o, dimensionamento e adi√ß√µes importantes üåÆ üö∏ üöÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuamos a experimentar os formatos das mitaps. Recentemente, em um ringue de boxe, colidimos com um barramento de dados centralizado e Service Mes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bancos de dados na mem√≥ria: aplica√ß√£o, dimensionamento e adi√ß√µes importantes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/psb/blog/434730/">  Continuamos a experimentar os formatos das mitaps.  Recentemente, em um ringue de boxe, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">colidimos com um</a> barramento de dados centralizado e Service Mesh.  Desta vez, decidimos tentar algo mais pac√≠fico - StandUp, ou seja, um microfone aberto.  O t√≥pico foi escolhido no banco de dados em mem√≥ria. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ffd/790/486/ffd790486ff5792ffd1bbebb3cc69be0.png"><br><br>  Em que casos devo mudar para a mem√≥ria?  Como e por que escalar?  E no que vale a pena prestar aten√ß√£o?  As respostas est√£o nos discursos dos palestrantes, que abordaremos neste post. <br><a name="habracut"></a><br>  Mas primeiro, imagine os alto-falantes: <br><br><ul><li>  Andrey Trushkin, Chefe do Centro de Inova√ß√£o e Tecnologias Avan√ßadas do Promsvyazbank <br></li><li>  Vladislav Shpileva, desenvolvedor Tarantool <br></li><li>  Artyom Shitov, arquiteto de solu√ß√µes GridGain <br></li></ul><br><h2>  Mudar para a mem√≥ria </h2><br>  As tend√™ncias atuais do mercado financeiro imp√µem requisitos muito mais rigorosos ao tempo de resposta e opera√ß√£o da automa√ß√£o de processos em geral.  Al√©m disso, quase todas as maiores institui√ß√µes financeiras hoje procuram construir seus pr√≥prios ecossistemas. <br><br>  Nesse sentido, vemos por n√≥s mesmos duas principais aplica√ß√µes de solu√ß√µes na mem√≥ria.  O primeiro √© o armazenamento em cache dos dados de integra√ß√£o.  De acordo com o cen√°rio cl√°ssico, em grandes empresas, existem v√°rios sistemas automatizados que fornecem dados a pedido do usu√°rio.  Ou um sistema externo - mas, neste caso, o iniciador na maioria dos casos √© o usu√°rio.  Tradicionalmente, esses sistemas armazenavam dados estruturados de uma certa maneira no banco de dados, acessando-os sob demanda. <br><br>  Hoje, esses sistemas n√£o atendem mais aos requisitos em termos de carga.  Aqui n√£o devemos esquecer as chamadas remotas desses sistemas pelos sistemas do consumidor.  Isso implica a necessidade de revisar abordagens para armazenamento e apresenta√ß√£o de dados - para usu√°rios, sistemas automatizados ou servi√ßos individuais.  Sa√≠da l√≥gica - armazenamento de dados relevantes usados ‚Äã‚Äãpelos servi√ßos no n√≠vel da camada na mem√≥ria;  existem muitos casos de sucesso semelhantes no mercado. <br><br>  Este foi o primeiro caso.  O segundo √© eficaz, do ponto de vista t√©cnico, para o gerenciamento de processos de neg√≥cios.  Os sistemas BPM tradicionais automatizam a execu√ß√£o de determinadas opera√ß√µes de acordo com um algoritmo predefinido.  E, em muitos casos, surgem quest√µes: por que esses sistemas n√£o s√£o suficientemente eficientes e r√°pidos o suficiente? <br><br>  Normalmente, esses sistemas gravam cada etapa (ou um pequeno conjunto de etapas, projetado como uma transa√ß√£o comercial) no banco de dados.  Portanto, eles est√£o ligados ao tempo de resposta e √† intera√ß√£o com esses sistemas.  Agora, o n√∫mero de inst√¢ncias de processos de neg√≥cios em execu√ß√£o simultaneamente em tempo real √© de magnitude superior a 10 anos atr√°s.  Portanto, os sistemas modernos de gerenciamento de processos de neg√≥cios devem ter desempenho significativamente mais alto e garantir a execu√ß√£o de aplicativos descentralizados.  Al√©m disso, hoje todas as empresas est√£o caminhando para a forma√ß√£o de um grande ambiente de microsservi√ßo.  O desafio √© que diferentes inst√¢ncias dos processos de neg√≥cios possam compartilhar e usar com efici√™ncia dados operacionais.  Dentro da estrutura da orquestra√ß√£o, faz sentido armazen√°-los em uma solu√ß√£o na mem√≥ria. <br><br><h2>  Problema de reconcilia√ß√£o </h2><br>  Suponha que tenhamos um grande n√∫mero de n√≥s e servi√ßos, que v√°rios processos de neg√≥cios sejam realizados, cujas a√ß√µes s√£o implementadas na forma de microsservi√ßos.  Para melhorar o desempenho, cada um deles come√ßa a gravar seu estado em uma inst√¢ncia de mem√≥ria local.  Temos um grande n√∫mero de inst√¢ncias locais.  Como garantir relev√¢ncia e consist√™ncia para todos? <br><br>  Usamos √°reas de zoneamento na mem√≥ria.  Por exemplo, dependendo do dom√≠nio comercial.  Quando cortamos um dom√≠nio comercial, determinamos que determinados microsservi√ßos / processos comerciais funcionam apenas dentro da estrutura da zona respons√°vel pelo dom√≠nio correspondente.  Dessa forma, podemos acelerar a atualiza√ß√£o do cache e toda a solu√ß√£o na mem√≥ria. <br><br>  Ao mesmo tempo, o cache respons√°vel pelo dom√≠nio opera no modo de replica√ß√£o completa - o n√∫mero limitado de n√≥s devido √† distribui√ß√£o entre dom√≠nios garante a velocidade e a corre√ß√£o da solu√ß√£o nesse modo.  O zoneamento e a fragmenta√ß√£o m√°xima ajudam a resolver os problemas de sincroniza√ß√£o, opera√ß√£o de cluster, etc.  em um grande n√∫mero total de n√≥s. <br><br>  Naturalmente, muitas vezes surgem d√∫vidas sobre a confiabilidade das solu√ß√µes na mem√≥ria.  Sim, nem tudo pode ser colocado l√°.  Para garantir a confiabilidade, sempre temos bancos de dados pr√≥ximos √† mem√≥ria.  Por exemplo, para problemas importantes nos relat√≥rios que precisam ser reunidos, o que pode ser dif√≠cil em um grande n√∫mero de n√≥s.  Ent√£o, qual √© a nossa vis√£o hoje: a <i>sinergia das duas abordagens</i> . <br><br>  Tamb√©m √© importante notar que essas duas abordagens tamb√©m n√£o s√£o totalmente corretas apenas para contrastar.  E, ao mesmo tempo, concentre-se neles.  Fabricantes e colaboradores de sistemas avan√ßados de virtualiza√ß√£o em cont√™iner, como o Kubernetes, j√° nos oferecem op√ß√µes para armazenamento confi√°vel a longo prazo.  J√° surgiram bons casos industriais para implementar solu√ß√µes, nos quais o armazenamento √© realizado em um formato virtualizado. <br><br>  Um dos maiores jornais dos EUA oferece aos seus leitores a oportunidade de receber qualquer edi√ß√£o on-line publicada desde o in√≠cio da publica√ß√£o deste jornal no s√©culo XIX.  Podemos imaginar o n√≠vel de carga.  O armazenamento √© implementado por eles atrav√©s da plataforma Apache Kafka, implantada no Kubernetes.  Aqui est√° outra op√ß√£o para armazenar informa√ß√µes e fornecer acesso a elas sob uma grande carga para um grande n√∫mero de clientes.  Ao projetar novas solu√ß√µes, essa op√ß√£o tamb√©m merece aten√ß√£o. <br><br><h2>  Escalando bancos de dados na mem√≥ria com Tarantool </h2><br>  Suponha que tenhamos um servidor.  Aceita pedidos, armazena dados.  De repente, h√° mais solicita√ß√µes e dados, o servidor para de lidar com a carga.  Voc√™ pode carregar mais hardware no servidor e ele aceitar√° mais solicita√ß√µes.  Mas esse √© um beco sem sa√≠da por tr√™s raz√µes ao mesmo tempo: alto custo, recursos t√©cnicos limitados e problemas com toler√¢ncia a falhas.  Em vez disso, existe uma escala horizontal: "amigos" v√™m ao servidor para ajud√°-lo a concluir tarefas.  Os dois principais tipos de escala horizontal s√£o replica√ß√£o e sharding. <br><br>  A replica√ß√£o ocorre quando h√° muitos servidores, todos eles armazenam os mesmos dados e as solicita√ß√µes do cliente est√£o espalhadas por todos esses servidores.  √â assim que a computa√ß√£o, n√£o os dados, √© dimensionada.  Isso funciona quando os dados s√£o colocados em um n√≥, mas h√° tantas solicita√ß√µes de cliente que um servidor n√£o pode lidar com isso.  Al√©m disso, a toler√¢ncia a falhas √© bastante aprimorada aqui. <br><br>  O sharding √© usado para dimensionar dados: muitos servidores s√£o criados e eles armazenam dados diferentes.  Ent√£o voc√™ dimensiona os c√°lculos e os dados.  Mas a toler√¢ncia a falhas neste caso √© baixa.  Se um servidor falhar, parte dos dados ser√° perdida. <br><br>  Existe uma terceira abordagem - combin√°-los.  Dividimos o cluster em sub-clusters, chamados de conjuntos de r√©plicas.  Cada um deles armazena os mesmos dados e os dados n√£o se cruzam entre os conjuntos de r√©plicas.  O resultado √© o dimensionamento de dados, computa√ß√£o e toler√¢ncia a falhas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/afe/63c/657/afe63c657f8ffb83527f4a5b298425a1.png"><br><br><h2>  Replica√ß√£o </h2><br>  A replica√ß√£o pode ser de dois tipos: ass√≠ncrona e s√≠ncrona.  Ass√≠ncrono √© quando as solicita√ß√µes do cliente n√£o esperam at√© que os dados se espalhem pelas r√©plicas: basta gravar em uma r√©plica.  Assim que os dados chegam ao disco, para o log, a transa√ß√£o √© bem-sucedida e, algum dia, em segundo plano, esses dados s√£o replicados.  S√≠ncrona - quando uma transa√ß√£o √© dividida em 2 fases: preparar e confirmar.  A confirma√ß√£o n√£o retornar√° √™xito at√© que os dados sejam replicados para algum quorum de r√©plicas. <br><br>  A replica√ß√£o ass√≠ncrona √© obviamente mais r√°pida, porque n√£o h√° nada na rede.  Os dados ser√£o enviados para a rede em segundo plano e a transa√ß√£o em si, conforme registrada no log, foi conclu√≠da.  Mas h√° um problema: r√©plicas podem ficar atr√°s uma da outra, fora de sincronia. <br>  A replica√ß√£o s√≠ncrona √© mais confi√°vel, mas muito mais lenta e mais dif√≠cil de implementar.  Existem protocolos complexos.  No Tarantool, voc√™ pode escolher qualquer um desses tipos de replica√ß√£o, dependendo da tarefa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/ca3/bd2/32fca3bd2ad0289626e48a5efc6a8e8d.png"><br><br>  O atraso das r√©plicas d√° origem n√£o apenas √† dessincroniza√ß√£o, mas tamb√©m ao problema de ignor√¢ncia do mestre: ele n√£o sabe como passar suas altera√ß√µes para a r√©plica.  As altera√ß√µes geralmente s√£o fornecidas de forma incremental - elas s√£o aplicadas e, da mesma forma, elas voam para a r√©plica.  Mas o que fazer com eles se a r√©plica n√£o estiver dispon√≠vel?  Por exemplo, tudo pode ser configurado no Tarantool, e o assistente se torna muito flex√≠vel. <br><br>  Outro desafio: como tornar a topologia complexa?  O Mail.ru, por exemplo, possui uma topologia com centenas de Tarantool.  Ele possui um kernel tarantool ao qual as tar√¢ntulas de r√©plica para backups est√£o ligadas em um c√≠rculo.  No Tarantool, voc√™ pode criar topologias completamente arbitr√°rias, a replica√ß√£o com isso permanece perfeitamente. <br><br><h2>  Sharding </h2><br>  Agora vamos para o dimensionamento de dados: sharding.  Pode ser de dois tipos: intervalos e hashes.  O sharding de intervalo √© quando todos os dados s√£o classificados por alguma chave de sharding, e essa sequ√™ncia grande √© dividida em intervalos para que cada intervalo tenha aproximadamente a mesma quantidade de dados.  E cada intervalo √© inteiramente armazenado em qualquer n√≥ f√≠sico.  Mas geralmente esse sharding n√£o √© necess√°rio.  Al√©m disso, √© sempre muito complicado. <br><br>  H√° tamb√©m sharding com hashes.  √â apenas apresentado em Tarantool.  √â muito mais f√°cil implementar, usar e quase sempre adequado, em vez de intervalos de sharding.  Funciona assim: consideramos a fun√ß√£o hash do registro e retorna o n√∫mero do n√≥ f√≠sico no qual armazenar.  Existem problemas: em primeiro lugar, √© dif√≠cil concluir rapidamente uma consulta complexa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dcc/eb9/f36/dcceb9f36f82f045eb0c461f0f934459.png"><br><br>  Em segundo lugar, h√° o problema de compartilhar novamente.  H√° algum tipo de fun√ß√£o de fragmento que retorna o n√∫mero do fragmento f√≠sico no qual a chave deve ser salva.  E quando o n√∫mero de n√≥s muda, a fun√ß√£o shard tamb√©m muda.  Isso significa que, para todos os dados que est√£o no cluster, eles dever√£o ser recalculados e verificados novamente.  Al√©m disso, no sharding cl√°ssico, alguns dados n√£o ser√£o transferidos para um novo n√≥, mas simplesmente embaralhados entre os n√≥s antigos.  Transfer√™ncias in√∫teis n√£o podem ser reduzidas a zero no sharding cl√°ssico. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/479/b1f/4e4/479b1f4e48b552723c900d49a4104412.png"><br><br>  O Tarantool usa sharding virtual: os dados s√£o distribu√≠dos n√£o em n√≥s f√≠sicos, mas em n√≥s virtuais.  Balde virtual em um cluster virtual.  E hist√≥rias virtuais s√£o apresentadas em f√≠sicas.  E j√° est√° garantido que cada andar virtual esteja inteiramente em um andar f√≠sico. <br><br>  Como isso resolve o problema da revenda?  O fato √© que o n√∫mero de buckets √© fixo e excede seriamente o n√∫mero de n√≥s f√≠sicos.  Portanto, n√£o importa o tamanho f√≠sico do seu cluster, o bucket sempre ser√° suficiente para armazenar dados e distribu√≠-los uniformemente.  E devido ao fato de a fun√ß√£o shard ser inalterada, voc√™ n√£o precisar√° recalcular quando a composi√ß√£o do cluster for alterada. <br><br>  Como resultado, obtemos <i>tr√™s tipos de sharding: intervalos, hashes e buckets virtuais</i> .  No caso de intervalos e intervalos, h√° um problema de pesquisa f√≠sica. <br><br>  Como resolver isso?  A primeira maneira: apenas pro√≠ba o compartilhamento de compartilhamento.  Para compartilhar novamente, voc√™ precisar√° criar um novo cluster e transferir tudo para l√°.  A segunda maneira: sempre v√° para todos os n√≥s.  Mas isso n√£o faz sentido, porque voc√™ precisa escalar, e os c√°lculos n√£o escalam dessa maneira.  Terceira op√ß√£o: um m√≥dulo proxy, que serve como um tipo de roteador para baldes.  Voc√™ o inicia, envia uma solicita√ß√£o para l√°, indicando o n√∫mero do bucket, e ele envia sua solicita√ß√£o como proxy para o n√≥ f√≠sico desejado. <br><br><h2>  Exemplo avan√ßado de mem√≥ria com o exemplo da plataforma GridGain </h2><br>  A empresa possui requisitos adicionais de banco de dados.  Ele quer que tudo isso seja tolerante a falhas e catastr√≥fico.  Ele quer alta disponibilidade: para que nada se perca, para que voc√™ possa se recuperar rapidamente.  Tamb√©m √© necess√°ria escalabilidade f√°cil e barata, suporte descomplicado, confian√ßa na plataforma e mecanismos de acesso eficientes. <br><br>  Todas essas id√©ias n√£o s√£o novas.  Muitas dessas coisas s√£o, em um grau ou outro, implementadas em DBMSs cl√°ssicos, em particular, replica√ß√£o entre data centers. <br><br>  O In-Memory n√£o √© mais uma tecnologia de inicializa√ß√£o, s√£o produtos maduros usados ‚Äã‚Äãnas maiores empresas do mundo (Barclays, Citi Group, Microsoft etc.).  Sup√µe-se que todos esses requisitos sejam atendidos. <br><br>  Portanto, se uma cat√°strofe acontecer repentinamente, deve haver uma oportunidade de se recuperar do backup.  E se estamos falando de uma organiza√ß√£o financeira, √© importante que esse backup seja consistente, e n√£o apenas uma c√≥pia de todas as unidades.  Para que n√£o exista uma situa√ß√£o em que em algumas partes dos n√≥s os dados foram restaurados no momento X e, no outro, no momento Y. √â muito importante ter o Point-in-time Recovery, para que, mesmo em uma situa√ß√£o de corrup√ß√£o de dados ou de um acidente particularmente grave, minimize a quantidade de perdas. <br><br>  √â importante poder enviar dados para o disco.  Para que o cluster n√£o fique sobrecarregado e continue a trabalhar ainda mais devagar.  E para sair rapidamente do disco e depois bombear os dados para a mem√≥ria. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/042/edc/328/042edc32872f7b8687bf5677818c712e.png"><br>  <i>Resposta na mem√≥ria a falhas com e sem componentes de toler√¢ncia a falhas GridGain</i> <br><br>  Um cluster de failover deve ser dimensionado facilmente na horizontal e na vertical.  N√£o tenho vontade de pagar pelo meu servidor e observar como metade dos recursos est√° ociosa.  N√£o quero ter centenas de processos que precisam ser gerenciados.  Eu quero um sistema simples do ponto de vista do suporte, com entrada / sa√≠da f√°cil de n√≥s do cluster e um sistema de monitoramento desenvolvido e maduro. <br><br>  Considere o MongoDB nesta perspectiva.  Todo mundo que trabalhou com o MongoDB est√° ciente de um grande n√∫mero de processos.  Se tivermos um MongoDB sombreado de 5 shards, cada shard ter√° um conjunto de r√©plicas de tr√™s processos (com uma taxa de redund√¢ncia de 3).  E s√£o 15 processos apenas nos pr√≥prios dados.  O armazenamento de configura√ß√£o de cluster √© outro mais tr√™s processos, no total, obt√©m 18, e isso n√£o inclui roteadores.  Se voc√™ quiser 20 shards, seja bem-vindo de mais de 63 processos (por exemplo, outros 8, total de 71). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/015/389/b64/015389b641f5babfe44815fd85f258cc.png"><br><br>  Compare com Cassandra.  Tomamos todos os mesmos 5 shards - s√£o 5 processos e 5 n√≥s com a mesma taxa de redund√¢ncia de 3, o que √© muito mais simples em termos de controle.  Quero 20 shards - s√£o 20 processos.  Posso dimensionar meu cluster para qualquer n√∫mero de n√≥s, n√£o necessariamente um m√∫ltiplo de 3 (ou para outro valor do coeficiente de redund√¢ncia).  Muito mais f√°cil e barato de implementar e manter do que conjuntos de r√©plicas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bcf/70d/4a4/bcf70d4a4883f9fdc951c871b99c57be.png"><br><br>  Al√©m disso, voc√™ precisa confiar no sistema para entender o que as pessoas est√£o por tr√°s de cada produto.  Idealmente, a licen√ßa deve ser de c√≥digo aberto ou n√∫cleo aberto.  Para que, em caso de morte do fornecedor, algo possa ser feito.  Tamb√©m √© bom se o c√≥digo-fonte for gerenciado por uma comunidade independente - todos nos lembramos de como o MongoDB e o Redis alteraram as licen√ßas a pedido da empresa de gerenciamento.  Como a Aerospike introduziu restri√ß√µes na edi√ß√£o da comunidade de "c√≥digo aberto" no in√≠cio do ano. <br><br>  Precisa de acesso efetivo aos dados.  Quase todos t√™m uma linguagem de consulta estruturada de uma forma ou de outra.  Na maioria das vezes eles usam SQL, √© necess√°rio que a adapta√ß√£o com essa linguagem seja o mais f√°cil poss√≠vel.  Isso ajudar√° a execu√ß√£o da consulta distribu√≠da, quando voc√™ n√£o precisar enviar uma solicita√ß√£o separadamente para cada n√≥, mas poder√° se comunicar com o cluster como em uma "janela √∫nica".  Sem pensar do ponto de vista da API, este √© um conjunto de n√≥s (lembre-se de como √© dif√≠cil trabalhar com o Memcache em grandes volumes, mesmo no n√≠vel mais simples de put / get, sem consultas SQL potencialmente complexas), garantias DDL e ACID distribu√≠das. <br><br>  E, finalmente, suporte.  Se algo de repente n√£o funcionar, a empresa simplesmente perde dinheiro.  Para algumas √°reas, isso n√£o √© cr√≠tico, mas geralmente √© importante que algu√©m seja respons√°vel pelo produto e seu trabalho.  Que era poss√≠vel a qualquer momento fazer uma reclama√ß√£o, e ela foi rapidamente resolvida. <br><br>  Com este post, estamos completando o ano do Promsvyazbank em Habr√©.  Reunimos os desejos de Ano Novo para os moradores de Khabrovsk em um pequeno v√≠deo: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yqp6V3Wqniw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt434730/">https://habr.com/ru/post/pt434730/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt434720/index.html">Verso do conhecimento zero: um backdoor no zk-SNARK que n√£o pode ser detectado</a></li>
<li><a href="../pt434722/index.html">Dor, p√≠lulas e duas ambul√¢ncias, ou Como todos subimos para o quinto lugar do IronStar 226 em Sochi</a></li>
<li><a href="../pt434724/index.html">Agricultores chineses fazem transmiss√£o ao vivo</a></li>
<li><a href="../pt434726/index.html">Armadilhas da identifica√ß√£o de um dispositivo Android</a></li>
<li><a href="../pt434728/index.html">Pessoas e processos: por que a udalenka n√£o √© adequada para todas as empresas?</a></li>
<li><a href="../pt434732/index.html">Vida em 6200 DPI. Revis√£o do HyperX Pulsefire Core</a></li>
<li><a href="../pt434734/index.html">Transformada de Fourier. Os velozes e furiosos</a></li>
<li><a href="../pt434736/index.html">Usando o banco de dados de log Mikrotik para suprimir a for√ßa bruta</a></li>
<li><a href="../pt434738/index.html">Aprendizado por Refor√ßo em Python</a></li>
<li><a href="../pt434740/index.html">Rede neural ensinada a detectar pain√©is solares em imagens de sat√©lite e prever o n√≠vel de sua distribui√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>