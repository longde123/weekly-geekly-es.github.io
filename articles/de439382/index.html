<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö£ üëçüèª üï£ Mit Ansible, Terraform, Docker, Consul, Nomad in den Wolken (Alexey Vakhov, Uchi.ru) üëâüèº üíáüèΩ üåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel ist eine Abschrift des Videoberichts von Alexei Vakhov von Uchi.ru ‚ÄûWolken in den Wolken‚Äú. 


 Uchi.ru ist eine Online-Plattform f√ºr di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mit Ansible, Terraform, Docker, Consul, Nomad in den Wolken (Alexey Vakhov, Uchi.ru)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439382/"><p>  Dieser Artikel ist eine Abschrift des Videoberichts von Alexei Vakhov von Uchi.ru ‚ÄûWolken in den Wolken‚Äú. </p><br><p>  Uchi.ru ist eine Online-Plattform f√ºr die Schulbildung, mehr als 2 Millionen Sch√ºler, interaktive Klassen entscheiden regelm√§√üig mit uns.  Alle unsere Projekte werden vollst√§ndig in √∂ffentlichen Clouds gehostet. 100% der Anwendungen arbeiten in Containern, angefangen bei den kleinsten f√ºr den internen Gebrauch bis hin zu gro√üen Produktionen mit mehr als 1.000 Anfragen pro Sekunde.  So kam es, dass wir 15 isolierte Docker-Cluster (nicht Kubernetes, sic!) In f√ºnf Cloud-Anbietern haben.  Eintausendf√ºnfhundert Benutzeranwendungen, deren Anzahl st√§ndig w√§chst. </p><br><p>  Ich werde √ºber ganz bestimmte Dinge sprechen: wie wir auf Container umgestiegen sind, wie wir die Infrastruktur verwalten, auf welche Probleme wir gesto√üen sind, was funktioniert hat und was nicht. </p><br><p>  W√§hrend des Berichts werden wir diskutieren: </p><br><ul><li>  Motivation f√ºr Technologieauswahl und Gesch√§ftsmerkmale </li><li>  Tools: Ansible, Terraform, Docker, Github Flow, Konsul, Nomad, Prometheus, Schamane - eine Weboberfl√§che f√ºr Nomad. </li><li>  Verwenden der Cluster-F√∂deration zum Verwalten der verteilten Infrastruktur </li><li>  NoOps-Rollouts, Testumgebungen, Anwendungsschaltungen (Entwickler nehmen ihre eigenen √Ñnderungen praktisch selbst vor) </li><li>  Unterhaltsame Geschichten aus der Praxis </li></ul><br><iframe width="560" height="315" src="https://www.youtube.com/embed/C7utdhh6UCk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Wen k√ºmmert es bitte unter der Katze. </p><a name="habracut"></a><br><p>  Ich hei√üe Alexey Vakhov.  Ich arbeite als technischer Direktor bei Uchi.ru.  Wir hosten in √∂ffentlichen Wolken.  Wir nutzen Terraform, Ansible aktiv.  Seitdem sind wir komplett auf Docker umgestiegen.  Sehr zufrieden.  Wie gl√ºcklich, wie gl√ºcklich wir sind - ich werde es sagen. </p><br><p><img src="https://habrastorage.org/webt/l3/zh/6x/l3zh6xrikma1bku1ks6vo7hixva.png"></p><br><p>  Die Firma Uchi.ru produziert Produkte f√ºr die Schulbildung.  Wir haben eine Hauptplattform, auf der Kinder interaktive Probleme in verschiedenen F√§chern in Russland, Brasilien und den USA l√∂sen.  Wir f√ºhren Online-Olympiaden, Wettbewerbe, Vereine und Camps durch.  Jedes Jahr w√§chst diese Aktivit√§t. </p><br><p><img src="https://habrastorage.org/webt/ms/m8/8h/msm88h-eoc67vkjnokat3ssq_wo.png"></p><br><p>  Aus technischer Sicht der klassische Webstack (Ruby, Python, NodeJS, Nginx, Redis, ELK, PostgreSQL).  Das Hauptmerkmal ist, dass viele Anwendungen.  Anwendungen werden weltweit gehostet.  Jeden Tag gibt es Rollouts in der Produktion. </p><br><p>  Das zweite Merkmal ist, dass sich unsere Systeme sehr oft √§ndern.  Sie bitten darum, eine neue Anwendung zu erstellen, die alte zu stoppen und Cron f√ºr Hintergrundjobs hinzuzuf√ºgen.  Alle 2 Wochen gibt es eine neue Olympiade - dies ist eine neue Anwendung.  Es ist alles notwendig, um zu begleiten, zu √ºberwachen, zu sichern.  Daher ist die Umgebung superdynamisch.  Dynamik ist unsere Hauptschwierigkeit. </p><br><p><img src="https://habrastorage.org/webt/gm/dn/vu/gmdnvuag9g19b-bkzqbamcrwdw8.png"></p><br><p>  Unsere Arbeitseinheit ist der Standort.  In Bezug auf Cloud-Anbieter ist dies Project.  Unsere Site ist eine vollst√§ndig isolierte Einheit mit einer API und einem privaten Subnetz.  Bei der Einreise suchen wir nach lokalen Cloud-Anbietern.  Nicht √ºberall gibt es Google und Amazon.  Manchmal gibt es keine API f√ºr den Cloud-Anbieter.  √Ñu√üerlich ver√∂ffentlichen wir VPN und HTTP, HTTPS f√ºr Balancer.  Alle anderen Dienste kommunizieren innerhalb der Cloud. </p><br><p><img src="https://habrastorage.org/webt/vz/b6/w2/vzb6w2yehx_arktffwsjxildnhe.png"></p><br><p>  F√ºr jede Site haben wir ein eigenes Ansible-Repository erstellt.  Das Repository enth√§lt hosts.yml, ein Playbook, Rollen und 3 geheime Ordner, √ºber die ich sp√§ter sprechen werde.  Dies ist Terraform, Bereitstellung, Routing.  Wir sind Fans der Standardisierung.  Unser Repository sollte immer als "Ansible-Name der Site" bezeichnet werden.  Wir standardisieren jeden Dateinamen und jede interne Struktur.  Dies ist sehr wichtig f√ºr die weitere Automatisierung. </p><br><p><img src="https://habrastorage.org/webt/v3/a0/kk/v3a0kke8aauqojk4hj5blhgwmt4.png"></p><br><p>  Wir haben Terraform vor anderthalb Jahren gegr√ºndet und verwenden es daher.  Terraform ohne Module, ohne Dateistruktur (flache Struktur wird verwendet).  Terraform-Dateistruktur: 1 Server - 1 Datei, Netzwerkeinstellungen und andere Einstellungen.  Mit terraform beschreiben wir Server, Laufwerke, Dom√§nen, s3-Buckets, Netzwerke usw.  Terraform vor Ort bereitet das Eisen vollst√§ndig vor. </p><br><p><img src="https://habrastorage.org/webt/wq/qz/ly/wqqzly1s4_6brivoh4hkeyvexmu.png"></p><br><p>  Terraform erstellt den Server, dann rollt das Ensemble diese Server.  Aufgrund der Tatsache, dass wir √ºberall dieselbe Version des Betriebssystems verwenden, haben wir alle Rollen von Grund auf neu geschrieben.  Ansible Rollen werden normalerweise im Internet f√ºr alle Betriebssysteme ver√∂ffentlicht, die nirgendwo funktionieren.  Wir haben alle Ansible-Rollen √ºbernommen und nur das gelassen, was wir brauchten.  Standardisierte Ansible-Rollen.  Wir haben 6 grundlegende Spielb√ºcher.  Beim Start installiert Ansible eine Standardliste von Software: OpenVPN, PostgreSQL, Nginx, Docker.  Kubernetes verwenden wir nicht. </p><br><p><img src="https://habrastorage.org/webt/fh/x4/i4/fhx4i4ztn5fvmjj6uivdbkdrub8.png"></p><br><p>  Wir verwenden Consul + Nomad.  Dies sind sehr einfache Programme.  F√ºhren Sie auf jedem Server 2 in Golang geschriebene Programme aus.  Consul ist f√ºr die Serviceerkennung, die Integrit√§tspr√ºfung und den Schl√ºsselwert zum Speichern der Konfiguration verantwortlich.  Nomad ist f√ºr die Planung und den Rollout verantwortlich.  Nomad startet Container, bietet Rollouts, einschlie√ülich Rolling-Updates zur Integrit√§tspr√ºfung, und erm√∂glicht das Ausf√ºhren von Beiwagencontainern.  Der Cluster l√§sst sich leicht erweitern oder umgekehrt reduzieren.  Nomad unterst√ºtzt verteilte Cron. </p><br><p><img src="https://habrastorage.org/webt/dc/ic/sm/dcicsmnhzdduqbnxsdpe6ewkvws.png"></p><br><p>  Nachdem wir die Site betreten haben, f√ºhrt Ansible das Playbook aus, das sich im Bereitstellungsverzeichnis befindet.  Das Playbook in diesem Verzeichnis ist f√ºr die Installation der Software im Docker-Cluster verantwortlich, die Administratoren verwenden.  Installieren Sie Prometheus, Grafana und geheime Schamanen-Software. </p><br><p>  Shaman ist ein Web-Dashboard f√ºr Nomaden.  Nomad ist auf niedrigem Niveau und ich m√∂chte Entwickler nicht wirklich darin lassen.  In Schamanen sehen wir eine Liste von Anwendungen, wir geben Entwicklern eine Bereitstellungsschaltfl√§che f√ºr Anwendungen.  Entwickler k√∂nnen Konfigurationen √§ndern: Container hinzuf√ºgen, Umgebungsvariablen, Dienste starten. </p><br><p><img src="https://habrastorage.org/webt/cm/jw/ry/cmjwryicd-9dvhlpg--bhfrfe9w.png"></p><br><p>  Und schlie√ülich ist die letzte Komponente der Site das Routing.  Das Routing wird im K / V-Speicher des Konsuls gespeichert, dh es besteht eine Verbindung zwischen Upstream, Service, URL usw.  Auf jedem Balancer gibt es eine Consul-Vorlage, die eine Nginx-Konfiguration generiert und neu l√§dt.  Eine sehr zuverl√§ssige Sache, wir hatten nie ein Problem damit.  Das Merkmal dieses Schemas ist, dass der Datenverkehr Standard-Nginx akzeptiert und Sie immer sehen k√∂nnen, welche Konfiguration generiert wurde und wie mit Standard-Nginx funktioniert. </p><br><p><img src="https://habrastorage.org/webt/rn/ew/7g/rnew7gm_fe4gcc57clp3f41ob68.png"></p><br><p>  Somit besteht jede Stelle aus 5 Schichten.  Mit Terraform passen wir die Hardware an.  Ansible f√ºhren wir die Grundkonfiguration der Server durch, setzen den Docker-Cluster.  Provision rollt Systemsoftware zusammen.  Das Routing leitet den Verkehr innerhalb der Site.  Anwendungen enth√§lt Benutzeranwendungen und Administratoranwendungen. </p><br><p>  Wir haben diese Ebenen lange getestet, damit sie so identisch wie m√∂glich sind.  Bereitstellung, Routing stimmen zu 100% zwischen Standorten √ºberein.  Daher ist f√ºr Entwickler jede Site absolut gleich. </p><br><p>  Wenn IT-Experten von Projekt zu Projekt wechseln, fallen sie in eine v√∂llig typische Umgebung.  In ansible konnten wir die Firewall- und VPN-Einstellungen f√ºr verschiedene Cloud-Anbieter nicht identisch machen.  Mit einem Netzwerk arbeiten alle Cloud-Anbieter unterschiedlich.  Terraform ist √ºberall seine eigene, da es spezifische Designs f√ºr jeden Cloud-Anbieter enth√§lt. </p><br><p><img src="https://habrastorage.org/webt/36/kg/ti/36kgtipfv8rl9lhjevdjhhiwp5m.png"></p><br><p>  Wir haben 14 Produktionsst√§tten.  Es stellt sich die Frage: Wie geht man damit um?  Wir haben die 15. Master-Site erstellt, in der nur Administratoren zugelassen sind.  Sie arbeitet an einem F√∂derationsschema. </p><br><p>  Die Idee wurde von Prometheus √ºbernommen.  Es gibt einen Modus in Prometheus, wenn wir Prometheus auf jeder Site installieren.  Wir ver√∂ffentlichen Prometheus √ºber die HTTPS-Basisauthentifizierungsautorisierung.  Prometheus-Meister √ºbernimmt nur die erforderlichen Metriken von Remote-Prometheus.  Auf diese Weise k√∂nnen Sie Anwendungsmetriken in verschiedenen Clouds vergleichen und die am h√§ufigsten heruntergeladenen oder entladenen Anwendungen finden.  Die zentralisierte Benachrichtigung (Alarmierung) durchl√§uft den Prometheus-Master f√ºr Administratoren.  Entwickler erhalten Benachrichtigungen von lokalen Prometheus. </p><br><p><img src="https://habrastorage.org/webt/cw/c9/yd/cwc9yd3hpmbhbxvlz0ygacktloa.png"></p><br><p>  Der Schamane ist auf die gleiche Weise konfiguriert.  √úber den Hauptstandort k√∂nnen Administratoren √ºber eine einzige Schnittstelle auf jedem Standort bereitstellen und konfigurieren.  Wir l√∂sen eine ausreichend gro√üe Klasse von Problemen, ohne diese Master-Site zu verlassen. </p><br><p><img src="https://habrastorage.org/webt/kz/ul/hi/kzulhi1jrz8c3bssubwqzremd6g.png"></p><br><p>  Ich werde Ihnen sagen, wie wir zu Docker gewechselt sind.  Dieser Prozess ist sehr langsam.  Wir haben ungef√§hr 10 Monate gekreuzt.  Im Sommer 2017 hatten wir 0 Produktionsbeh√§lter.  Im April 2018 haben wir unsere neueste Anwendung angedockt und in die Produktion eingef√ºhrt. </p><br><p><img src="https://habrastorage.org/webt/h5/9t/ib/h59tibngi0uru-f4vehjhnq3e1c.png"></p><br><p>  Wir sind aus der Welt des Rubins auf Schienen.  Fr√ºher gab es 99% der Ruby on Rails-Anwendungen.  Schienen rollen durch Capistrano.  Technisch gesehen funktioniert Capistrano wie folgt: Der Entwickler startet Cap Deploy, Capistrano geht √ºber ssh zu allen Anwendungsservern, nimmt die neueste Version des Codes, sammelt Assets und Datenbankmigrationen.  Capistrano stellt einen Symlink zur neuen Version des Codes her und sendet ein USR2-Signal an die Webanwendung.  Bei diesem Signal nimmt der Webserver neuen Code auf. </p><br><p><img src="https://habrastorage.org/webt/il/us/4e/ilus4ejwfkd6cvjnmjrcln2ugyy.png"></p><br><p>  Der letzte Schritt im Docker wird nicht so gemacht.  Im Docker m√ºssen Sie den alten Container stoppen und den neuen Container anheben.  Dies wirft die Frage auf: Wie wird der Verkehr gewechselt?  In der Cloud-Welt ist die Serviceerkennung daf√ºr verantwortlich. </p><br><p><img src="https://habrastorage.org/webt/k3/il/ig/k3iligqv5uphvlp1l3cvkmbggem.png"></p><br><p>  Aus diesem Grund haben wir jedem Standort einen Konsul hinzugef√ºgt.  Konsul wurde hinzugef√ºgt, weil sie Terraform verwendeten.  Wir haben alle Nginx-Konfigurationen in eine Konsul-Vorlage eingewickelt.  Formal das Gleiche, aber wir waren bereits bereit, den Verkehr innerhalb der Websites dynamisch zu verwalten. </p><br><p><img src="https://habrastorage.org/webt/u9/qg/ag/u9qgagdnxiwo4lowkqi7micapqu.png"></p><br><p>  Als n√§chstes schrieben wir ein Ruby-Skript, das ein Bild auf einem der Server sammelte, es in die Registrierung schob, dann per SSH zu jedem Server ging, neue aufnahm und die alten Container stoppte und sie beim Konsul registrierte.  Die Entwickler f√ºhrten auch weiterhin Cap Deploy aus, aber die Dienste wurden bereits im Docker ausgef√ºhrt. </p><br><p>  Ich erinnere mich, dass es zwei Versionen des Skripts gab, die zweite erwies sich als ziemlich fortgeschritten, es gab ein fortlaufendes Update, als eine kleine Anzahl von Containern anhielt, neue auftauchten, der Konsul Helfcheki wartete und weiterging. </p><br><p><img src="https://habrastorage.org/webt/07/fx/gk/07fxgkf2fcvnhs8gpfjfrygalxs.png"></p><br><p>  Dann erkannten sie, dass dies eine Sackgasse ist.  Das Skript wurde auf 600 Zeilen erh√∂ht.  Im n√§chsten Schritt der manuellen Planung haben wir Nomad ersetzt.  Versteckt die Details der Arbeit vor dem Entwickler.  Das hei√üt, sie nannten Cap Deploy immer noch, aber im Inneren befand sich bereits eine v√∂llig andere Technologie. </p><br><p><img src="https://habrastorage.org/webt/4j/o-/ms/4jo-ms4v5mx1q4cn6sqt6n0jr4a.png"></p><br><p>  Am Ende haben wir die Bereitstellung auf die Benutzeroberfl√§che verschoben und den Zugriff auf den Server entfernt, wobei die gr√ºne Schaltfl√§che f√ºr die Bereitstellung und die Steuerungsoberfl√§che erhalten blieben. </p><br><p>  Im Prinzip stellte sich ein solcher √úbergang nat√ºrlich als lang heraus, aber wir haben das Problem vermieden, das ich einige Male getroffen habe. </p><br><p>  Es gibt eine Art Legacy-Stack, System oder √§hnliches.  Khachennaya schon gerade in Klappen.  Die Entwicklung einer neuen Version beginnt.  Nach ein paar Monaten oder ein paar Jahren, abh√§ngig von der Gr√∂√üe des Unternehmens, ist in der neuen Version weniger als die H√§lfte der erforderlichen Funktionen implementiert, und die alte Version ist immer noch entkommen.  Und dieses neue wurde auch sehr Verm√§chtnis.  Und es ist Zeit, eine neue, dritte Version von Grund auf neu zu starten.  Im Allgemeinen ist dies ein endloser Prozess. </p><br><p>  Daher verschieben wir immer den gesamten Stapel als Ganzes.  In kleinen Schritten schief, mit Kr√ºcken, aber ganz.  Wir k√∂nnen beispielsweise keine Docker-Engine an einem Standort aktualisieren.  Es ist notwendig, √ºberall zu aktualisieren, wenn es einen Wunsch gibt. </p><br><p><img src="https://habrastorage.org/webt/qy/31/jr/qy31jrpbehnyyrcozqaxnj41jk8.png"></p><br><p>  Roll-outs.  Alle Docker-Anweisungen rollen 10 Nginx-Container oder 10 Redis-Container in Docker aus.  Dies ist ein schlechtes Beispiel, da die Bilder bereits zusammengestellt sind und die Bilder hell sind.  Wir haben unsere Schienenanwendungen in Docker verpackt.  Die Gr√∂√üe der Docker-Images betrug 2-3 Gigabyte.  Sie werden nicht so schnell herausspringen. </p><br><p><img src="https://habrastorage.org/webt/2q/lx/bf/2qlxbfo5lnhpjfxcjtlljrntgxy.png"></p><br><p>  Das zweite Problem kam aus dem Hipster-Web.  Ein Hipster-Web ist immer Github Flow.  Im Jahr 2011 gab es einen epochalen Beitrag, den Github Flow steuert, sodass die gesamte Bahn rollt.  Wie sieht es aus?  Master Branch ist immer Produktion.  Beim Hinzuf√ºgen neuer Funktionen erstellen wir eine Verzweigung.  Bei der Fusion f√ºhren wir eine Code√ºberpr√ºfung durch, f√ºhren Tests durch und erh√∂hen die Staging-Umgebung.  Business suchende Inszenierungsumgebung.  Wenn zum Zeitpunkt X alles erfolgreich ist, f√ºhren wir die Niederlassung in Master zusammen und rollen sie in die Produktion aus. </p><br><p>  Auf Capistrano hat das gut funktioniert, weil es daf√ºr geschaffen wurde.  Docker verkauft uns immer eine Pipeline.  Den Beh√§lter zusammengebaut.  Der Beh√§lter kann an den Entwickler, Tester, √ºbergeben und an die Produktion √ºbergeben werden.  Zum Zeitpunkt der Zusammenf√ºhrung im Master ist der Code jedoch bereits anders.  Alle Docker-Bilder, die aus dem Feature-Zweig erfasst wurden, wurden nicht vom Master erfasst. </p><br><p><img src="https://habrastorage.org/webt/vl/th/8g/vlth8gjbcyby-rpamtltaijqxcy.png"></p><br><p>  Wie haben wir das gemacht?  Wir sammeln das Bild und legen es in der lokalen Docker-Registrierung ab.  Danach erledigen wir den Rest der Vorg√§nge: Migration, Bereitstellung in der Produktion. </p><br><p><img src="https://habrastorage.org/webt/1i/kb/yu/1ikbyuh88pytsfprkkwdi6_6ppq.png"></p><br><p>  Um dieses Bild schnell zusammenzusetzen, verwenden wir Docker-in-Docker.  Im Internet schreibt jeder, dass dies ein Anti-Muster ist, es st√ºrzt ab.  Wir hatten nichts dergleichen.  Wie viele schon mit ihm arbeiten, hatte nie ein Problem.  Wir leiten das Verzeichnis / var / lib / docker unter Verwendung des persistenten Volumes an den Hauptserver weiter.  Alle Zwischenabbilder befinden sich auf dem Prim√§rserver.  Das Zusammenstellen eines neuen Bildes dauert nur wenige Minuten. </p><br><p><img src="https://habrastorage.org/webt/6q/_g/67/6q_g67tuzx_n078pff6-aisdvdc.png"></p><br><p>  F√ºr jede Anwendung erstellen wir eine lokale interne Docker-Registrierung und unser Build-Volume.  Weil Docker alle Ebenen auf der Festplatte speichert und schwer zu reinigen ist.  Jetzt kennen wir die Festplattenauslastung jeder lokalen Docker-Registrierung.  Wir wissen, wie viel Festplatte ben√∂tigt wird.  Sie k√∂nnen Benachrichtigungen √ºber zentralisierte Grafana erhalten und bereinigen.  W√§hrend wir ihre H√§nde reinigen.  Aber wir werden es automatisieren. </p><br><p><img src="https://habrastorage.org/webt/mf/py/wj/mfpywjgwpsj0tbqb6vvvyzz8_1u.png"></p><br><p>  Ein weiterer Punkt.  Docker-Bild gesammelt.  Jetzt muss dieses Image in Server zerlegt werden.  Beim Kopieren eines gro√üen Docker-Images kommt das Netzwerk nicht zurecht.  In der Cloud haben wir 1 Gbit / s.  In der Cloud wird global heruntergefahren.  Jetzt stellen wir ein Docker-Image auf 4 schweren Produktionsservern bereit.  In der Grafik sehen Sie, wie die Festplatte auf einem Serverpaket funktioniert.  Dann wird das zweite Serverpaket bereitgestellt.  Unten sehen Sie die Auslastung des Kanals.  √úber 1 Gbit / s ziehen wir fast.  Dort gibt es nicht mehr viel Beschleunigung. </p><br><p><img src="https://habrastorage.org/webt/lk/eo/hl/lkeohl4fjm-y97kmp3hyogr3p1a.png"></p><br><p>  Meine Lieblingsproduktion ist S√ºdafrika.  Es gibt sehr teures und langsames Eisen.  Viermal teurer als in Russland.  Es gibt sehr schlechtes Internet.  Internet auf Modem-Ebene, aber nicht fehlerhaft.  Dort rollen wir Anwendungen in 40 Minuten aus, wobei die Optimierung der Caches und die Timeout-Parameter ber√ºcksichtigt werden. </p><br><p><img src="https://habrastorage.org/webt/zl/8x/ig/zl8xig0k9liveeiuzoykh-ctgwm.png"></p><br><p>  Das letzte Problem, das mich beunruhigte, bevor Docker Kontakt aufnahm, war die Ladung.  In der Tat ist die Ladung die gleiche wie ohne Docker mit identischem Eisen.  Die einzige Nuance, auf die wir nur einen Punkt stie√üen.  Wenn Sie Protokolle von der Docker-Engine √ºber den integrierten Fluentd-Treiber sammeln, wird bei einer Last von etwa 1000 U / s der interne Fluentd-Puffer verschmutzt und die Anforderungen werden langsamer.  Wir haben uns in Beiwagencontainern angemeldet.  In Nomaden wird dies als Log-Shipper bezeichnet.  Ein kleiner Container h√§ngt neben einem gro√üen Anwendungscontainer.  Die einzige Aufgabe besteht darin, es aufzunehmen und an ein zentrales Repository zu senden. </p><br><p><img src="https://habrastorage.org/webt/8r/fv/7x/8rfv7xzfamwjrwemisa1laingzy.png"></p><br><p>  Was waren die Probleme / L√∂sungen / Herausforderungen.  Ich habe versucht zu analysieren, was die Aufgabe war.  Merkmale unserer Probleme sind: </p><br><ul><li>  viele unabh√§ngige Anwendungen </li><li>  kontinuierliche Ver√§nderungen in der Infrastruktur </li><li>  Github Flow und gro√üe Docker-Bilder </li></ul><br><p><img src="https://habrastorage.org/webt/da/xm/jm/daxmjmflat8cz4a5b-2nmx4pip4.png"></p><br><p>  Unsere L√∂sungen </p><br><ul><li>  F√∂deration von Docker-Clustern.  Aus Sicht der Handhabung ist es schwierig.  Docker ist jedoch gut darin, Gesch√§ftsfunktionen in der Produktion einzuf√ºhren.  Wir arbeiten mit personenbezogenen Daten und sind in jedem Land zertifiziert.  An einem isolierten Standort ist eine solche Zertifizierung leicht zu bestehen.  W√§hrend der Zertifizierung stellen sich alle Fragen: Wo hosten Sie, wie haben Sie einen Cloud-Anbieter, wo speichern Sie personenbezogene Daten, wo sichern Sie, wer hat Zugriff auf die Daten.  Wenn alles isoliert ist, ist es viel einfacher, den Kreis der Verd√§chtigen zu beschreiben und all dies zu √ºberwachen. </li><li>  Orchestrierung.  Es ist klar, dass Kubernetes.  Er ist √ºberall.  Aber ich m√∂chte sagen, dass Consul + Nomad eine vollst√§ndig produktive L√∂sung ist. </li><li>  Zusammenstellung von Bildern.  Sie k√∂nnen Bilder schnell in Docker-in-Docker erstellen. </li><li>  Bei Verwendung von Docker ist es auch m√∂glich, eine Last von 1000 U / s zu halten. </li></ul><br><p>  Entwicklungsrichtungsvektor </p><br><p>  Eines der gro√üen Probleme ist nun die Unsynchronisation der Softwareversionen auf den Websites.  Zuvor haben wir den Server von Hand eingerichtet.  Dann wurden wir Entwickleringenieure.  Konfigurieren Sie nun den Server mit ansible.  Jetzt haben wir v√∂llige Vereinheitlichung, Standardisierung.  Wir f√ºhren gew√∂hnliches Denken in den Kopf ein.  Wir k√∂nnen PostgreSQL nicht mit unseren H√§nden auf dem Server reparieren.  Wenn Sie eine Feinabstimmung auf nur einem Server ben√∂tigen, √ºberlegen wir, wie Sie diese Einstellung √ºberall verbreiten k√∂nnen.  Wenn Sie nicht standardisieren, gibt es einen Zoo mit Einstellungen. </p><br><p>  Ich freue mich und bin sehr froh, dass wir kostenlos eine wirklich, wirklich gut funktionierende Infrastruktur haben. </p><br><p><img src="https://habrastorage.org/webt/w6/nz/pw/w6nzpwpzt1tsxifw0gksrvoplpg.png"></p><br><p>  Sie k√∂nnen mich auf Facebook hinzuf√ºgen.  Wenn wir etwas Gutes tun, werde ich dar√ºber schreiben. </p><br><p>  Fragen: </p><br><p>  Was ist der Vorteil der Consul-Vorlage gegen√ºber der Ansible-Vorlage, um beispielsweise Firewall-Regeln und mehr zu konfigurieren? </p><br><p>  Antwort: Jetzt haben wir Datenverkehr von externen Balancern, die direkt zu Containern geleitet werden.  Dazwischen ist niemand.  Dort wird eine Konfiguration gebildet, die die IP-Adressen und Ports des Clusters weiterleitet.  Au√üerdem haben wir alle Balance-Einstellungen in K / V in Consul.  Wir haben die Idee, Entwicklern Routing-Einstellungen √ºber eine sichere Schnittstelle zu geben, damit sie nichts kaputt machen. </p><br><p>  Frage: In Bezug auf die Homogenit√§t aller Standorte.  Gibt es wirklich keine Anfragen von Unternehmen oder Entwicklern, dass Sie auf dieser Website etwas Ungew√∂hnliches einf√ºhren m√ºssen?  Zum Beispiel Tarantool mit Cassandra. </p><br><p>  Antwort: Es passiert, aber es ist sehr selten.  Hiermit erstellen wir ein internes separates Artefakt.  Es gibt ein solches Problem, aber es ist selten. </p><br><p>  Frage: Die L√∂sung f√ºr das √úbermittlungsproblem besteht darin, an jedem Standort eine private Docker-Registrierung zu verwenden. Von dort aus k√∂nnen Docker-Images bereits schnell abgerufen werden. </p><br><p>  Antwort: Auf jeden Fall wird die Bereitstellung im Netzwerk ausgef√ºhrt, da wir das Docker-Image dort gleichzeitig auf 15 Servern bereitstellen.  Wir ruhen uns gegen das Netzwerk aus.  Innerhalb des Netzwerks 1 Gbit / s. </p><br><p>  Frage: So viele Docker-Container basieren auf ungef√§hr demselben Technologie-Stack? </p><br><p>  Antwort: Ruby, Python, NodeJS. </p><br><p>  Frage: Wie oft testen oder √ºberpr√ºfen Sie Ihre Docker-Images auf Updates?  Wie l√∂sen Sie Update-Probleme, zum Beispiel wenn glibc, openssl in allen Docker behoben werden muss? </p><br><p>  Antwort: Wenn Sie einen solchen Fehler oder eine solche Sicherheitsl√ºcke finden, setzen wir uns f√ºr eine Woche hin und reparieren ihn.  Wenn Sie ein Rollout durchf√ºhren m√ºssen, k√∂nnen wir die gesamte Cloud (alle Anwendungen) von Grund auf √ºber den Verbund ausrollen.  Wir k√∂nnen auf alle gr√ºnen Schaltfl√§chen klicken, um Anwendungen bereitzustellen, und Tee trinken lassen. </p><br><p>  Frage: Wirst du deinen Schamanen in Open Source freigeben? </p><br><p>  Antwort: Hier verspricht uns Andrei (zeigt auf die Person aus dem Publikum), im Herbst einen Schamanen auszulegen.  Dort m√ºssen Sie jedoch Unterst√ºtzung f√ºr Kubernetes hinzuf√ºgen.  OpenSource sollte immer besser sein. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439382/">https://habr.com/ru/post/de439382/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439372/index.html">Willst du einen Detektiv spielen? Finden Sie den Fehler in einer Funktion von Midnight Commander</a></li>
<li><a href="../de439374/index.html">F√ºr diejenigen, die Detektiv spielen m√∂chten: Finden Sie den Fehler in der Funktion von Midnight Commander</a></li>
<li><a href="../de439376/index.html">Interessenclub</a></li>
<li><a href="../de439378/index.html">Buch (des Seins?). Reflexionen √ºber die Natur des Geistes. Teil I.</a></li>
<li><a href="../de439380/index.html">Wie ich eine Erweiterung f√ºr Atom und VS Code erstellt habe: pers√∂nliche Erfahrungen und Quellen</a></li>
<li><a href="../de439384/index.html">Metropolis-Modellierung</a></li>
<li><a href="../de439388/index.html">Roboter im Journalismus oder Verwendung k√ºnstlicher Intelligenz zum Erstellen von Inhalten</a></li>
<li><a href="../de439390/index.html">Die besten Innovationen sozialer Netzwerke im Jahr 2018</a></li>
<li><a href="../de439392/index.html">Die Meisterschaftssaison 2019 ist er√∂ffnet! Das SNA Hackathon Ala ML Boot Camp 8 startet</a></li>
<li><a href="../de439394/index.html">Als Programmierer haben Rechenzentrumskerne geschrieben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>