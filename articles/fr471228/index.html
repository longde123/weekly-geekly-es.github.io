<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üó®Ô∏è üòø üéã Grokay PyTorch üëÉüèº üßïüèº üßê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 Nous avons en pr√©-commande un livre tant attendu sur la biblioth√®que PyTorch . 



 Puisque vous apprendrez tout le mat√©riel de base...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Bonjour, Habr! <br><br>  Nous avons en pr√©-commande un livre tant attendu sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la biblioth√®que PyTorch</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Puisque vous apprendrez tout le mat√©riel de base n√©cessaire sur PyTorch dans ce livre, nous vous rappelons les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">avantages d'un</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processus</a> appel√© ¬´grokking¬ª ou ¬´compr√©hension approfondie¬ª du sujet que vous souhaitez apprendre.  Dans le post d'aujourd'hui, nous vous expliquerons comment Kai Arulkumaran a critiqu√© PyTorch (pas d'image).  Bienvenue au chat. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PyTorch</a> est un cadre d'apprentissage en profondeur flexible qui distingue automatiquement les objets utilisant des r√©seaux de neurones dynamiques (c'est-√†-dire les r√©seaux utilisant le contr√¥le de flux dynamique, comme les <code>if</code> et les boucles <code>while</code> ).  PyTorch prend en charge l'acc√©l√©ration GPU, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation distribu√©e</a> , divers types d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">optimisation</a> et de nombreuses autres fonctionnalit√©s int√©ressantes.  Ici, je pr√©sente quelques r√©flexions sur la fa√ßon, √† mon avis, d'utiliser PyTorch;  tous les aspects de la biblioth√®que et les pratiques recommand√©es ne sont pas trait√©s ici, mais j'esp√®re que ce texte vous sera utile. <br><br>  Les r√©seaux de neurones sont une sous-classe de graphiques de calcul.  Les graphiques de calcul re√ßoivent des donn√©es en entr√©e, puis ces donn√©es sont achemin√©es (et peuvent √™tre converties) aux n≈ìuds o√π elles sont trait√©es.  En apprentissage profond, les neurones (n≈ìuds) transforment g√©n√©ralement les donn√©es en leur appliquant des param√®tres et des fonctions diff√©renciables, de sorte que les param√®tres peuvent √™tre optimis√©s pour minimiser les pertes par la m√©thode de descente de gradient.  Dans un sens plus large, je note que les fonctions peuvent √™tre stochastiques et un graphique dynamique.  Ainsi, alors que les r√©seaux de neurones correspondent bien au paradigme de programmation de flux de donn√©es, l'API PyTorch se concentre sur le paradigme de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">programmation imp√©ratif</a> , et cette fa√ßon d'interpr√©ter les programmes cr√©√©s est beaucoup plus famili√®re.  C'est pourquoi le code PyTorch est plus facile √† lire, il est plus facile de juger de la conception de programmes complexes, qui, cependant, ne n√©cessitent pas de compromis s√©rieux sur les performances: en fait, PyTorch est assez rapide et fournit de nombreuses optimisations dont vous, en tant qu'utilisateur final, ne pouvez pas vous inqui√©ter du tout (cependant, si vous √™tes vraiment int√©ress√© par eux, vous pouvez creuser un peu plus et apprendre √† les conna√Ætre). <br><br>  Le reste de cet article est une analyse de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exemple officiel du jeu de donn√©es MNIST</a> .  Ici, nous <i>jouons</i> PyTorch, donc je recommande de comprendre l'article uniquement apr√®s avoir pris connaissance <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des manuels officiels du d√©butant</a> .  Pour plus de commodit√©, le code est pr√©sent√© sous la forme de petits fragments √©quip√©s de commentaires, c'est-√†-dire qu'il n'est pas distribu√© dans des fonctions / fichiers s√©par√©s que vous avez l'habitude de voir dans du code modulaire pur. <br><br><h4>  Importations </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  Tous ces √©l√©ments sont des importations tout √† fait standard, √† l'exception des modules <code>torchvision</code> , qui sont particuli√®rement utilis√©s pour r√©soudre des t√¢ches li√©es √† la vision par ordinateur. <br><br><h4>  Personnalisation </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> est la mani√®re standard de g√©rer les arguments de ligne de commande en Python. <br><br>  Si vous devez √©crire du code con√ßu pour fonctionner sur diff√©rents appareils (en utilisant l'acc√©l√©ration GPU, quand il est disponible, mais s'il n'est pas restaur√© aux calculs sur le CPU), s√©lectionnez et enregistrez le <code>torch.device</code> appropri√©, avec lequel vous pouvez d√©terminer o√π vous devez les tenseurs sont stock√©s.  Voir la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle pour</a> plus de d√©tails sur la cr√©ation d'un tel code.  L'approche de PyTorch est d'amener la s√©lection des appareils au contr√¥le de l'utilisateur, ce qui peut sembler ind√©sirable dans des exemples simples.  Cependant, cette approche simplifie consid√©rablement le travail lorsque vous devez g√©rer des tenseurs, ce qui a) est pratique pour le d√©bogage b) vous permet d'utiliser efficacement les p√©riph√©riques manuellement. <br><br>  Pour la reproductibilit√© des exp√©riences, vous devez d√©finir des valeurs initiales al√©atoires pour tous les composants qui utilisent la g√©n√©ration de nombres al√©atoires (y compris <code>random</code> ou <code>numpy</code> , si vous les utilisez <code>numpy</code> ).  Remarque: cuDNN utilise des algorithmes non d√©terministes et est facultativement d√©sactiv√© √† l'aide de <code>torch.backends.cudnn.enabled = False</code> . <br><br><h4>  Les donn√©es </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  √âtant <code>torchvision</code> mod√®les <code>torchvision</code> stock√©s sous <code>~/.torch/models/</code> , je pr√©f√®re stocker les jeux de <code>torchvision</code> torchvision sous <code>~/.torch/datasets</code> .  Ceci est mon accord de copyright, mais il est tr√®s pratique √† utiliser dans des projets d√©velopp√©s sur la base de MNIST, CIFAR-10, etc.  En g√©n√©ral, les ensembles de donn√©es doivent √™tre stock√©s s√©par√©ment du code si vous avez l'intention de r√©utiliser plusieurs ensembles de donn√©es. <br><br>  <code>torchvision.transforms</code> contient de nombreuses options de conversion pratiques pour des images individuelles, telles que le recadrage et la normalisation. <br><br>  Il existe de nombreuses options dans le <code>batch_size</code> , mais en plus de <code>batch_size</code> et <code>shuffle</code> , vous devez √©galement garder √† l'esprit <code>num_workers</code> et <code>pin_memory</code> , ils aident √† augmenter l'efficacit√©.  <code>num_workers &gt; 0</code> utilise des sous-processus pour le chargement asynchrone des donn√©es et ne bloque pas le processus principal pour cela.  Un cas d'utilisation typique consiste √† charger des donn√©es (par exemple, des images) √† partir d'un disque et, √©ventuellement, √† les convertir;  tout cela peut √™tre fait en parall√®le, avec le traitement des donn√©es r√©seau.  Le degr√© de traitement peut devoir √™tre ajust√© afin de a) minimiser le nombre de travailleurs et, par cons√©quent, la quantit√© de CPU et de RAM utilis√©e (chaque travailleur charge un lot s√©par√©, plut√¥t que des √©chantillons individuels inclus dans le lot) b) minimiser la dur√©e pendant laquelle les donn√©es attendent sur le r√©seau.  <code>pin_memory</code> utilise la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√©moire √©pingl√©e</a> (par opposition √† la pagination) pour acc√©l√©rer toutes les op√©rations de transfert de donn√©es de la RAM vers le GPU (et ne fait rien avec le code sp√©cifique au CPU). <br><br><h4>  Mod√®le </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  L'initialisation du r√©seau s'√©tend g√©n√©ralement aux variables membres, aux couches qui contiennent des param√®tres d'apprentissage et, √©ventuellement, aux param√®tres d'apprentissage individuels et aux tampons non form√©s.  Ensuite, avec un passage direct, ils sont utilis√©s en combinaison avec des fonctions de <code>F</code> qui sont purement fonctionnelles et ne contiennent pas de param√®tres.  Certaines personnes aiment travailler avec des r√©seaux purement fonctionnels (par exemple, conserver les param√®tres et utiliser <code>F.conv2d</code> au lieu de <code>nn.Conv2d</code> ) ou des r√©seaux enti√®rement constitu√©s de couches (par exemple, <code>nn.ReLU</code> au lieu de <code>F.relu</code> ). <br><br>  <code>.to(device)</code> est un moyen pratique d'envoyer des param√®tres de p√©riph√©rique (et des tampons) au GPU si le <code>device</code> d√©fini sur GPU, car sinon (si le p√©riph√©rique est d√©fini sur CPU), rien ne sera fait.  Il est important de transf√©rer les param√®tres du p√©riph√©rique vers le p√©riph√©rique appropri√© avant de les transmettre √† l'optimiseur;  sinon, l'optimiseur ne pourra pas suivre correctement les param√®tres! <br><br>  Les r√©seaux de neurones ( <code>nn.Module</code> ) et les optimiseurs ( <code>optim.Optimizer</code> ) peuvent enregistrer et charger leur √©tat interne, et il est recommand√© de le faire avec <code>.load_state_dict(state_dict)</code> - il est n√©cessaire de recharger l'√©tat des deux afin de reprendre la formation sur la base des dictionnaires pr√©c√©demment enregistr√©s √©tats.  L'enregistrement de l'objet entier peut √™tre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lourd d'erreurs</a> .  Si vous avez enregistr√© les tenseurs sur le GPU et que vous souhaitez les charger sur le CPU ou un autre GPU, alors le moyen le plus simple est de les charger directement sur le CPU en utilisant l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">option</a> <code>map_location</code> , par exemple <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Voici quelques autres points qui ne sont pas pr√©sent√©s ici, mais qui m√©ritent d'√™tre mentionn√©s, que vous pouvez utiliser le flux de contr√¥le avec un passage direct (par exemple, l'ex√©cution de l' <code>if</code> peut d√©pendre de la variable membre ou des donn√©es elles-m√™mes. En outre, elle est parfaitement valide au milieu du processus √† afficher. ( <code>print</code> ) tensors, ce qui simplifie grandement le d√©bogage. Enfin, avec un passage direct, beaucoup d'arguments peuvent √™tre utilis√©s. J'illustrerai ce point avec une courte liste qui n'est li√©e √† aucune id√©e particuli√®re: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  La formation </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Les modules r√©seau sont mis en mode de formation par d√©faut - ce qui affecte dans une certaine mesure le fonctionnement des modules, surtout - l'amincissement et la normalisation des lots.  D'une mani√®re ou d'une autre, il est pr√©f√©rable de d√©finir ces √©l√©ments manuellement √† l'aide de <code>.train()</code> , qui filtre l'indicateur "training" sur tous les modules enfants. <br><br>  Ici, la m√©thode <code>.to()</code> accepte non seulement le p√©riph√©rique, mais d√©finit √©galement <code>non_blocking=True</code> , garantissant ainsi la copie asynchrone des donn√©es vers le GPU √† partir de la m√©moire <code>non_blocking=True</code> , permettant au CPU de rester op√©rationnel pendant le transfert de donn√©es;  sinon, <code>non_blocking=True</code> tout simplement pas une option. <br><br>  Avant de cr√©er un nouvel ensemble de d√©grad√©s √† l'aide de <code>loss.backward()</code> et de <code>optimiser.step()</code> en <code>optimiser.step()</code> aide d' <code>optimiser.step()</code> , vous devez r√©initialiser manuellement les d√©grad√©s des param√®tres √† optimiser √† l'aide d' <code>optimiser.zero_grad()</code> .  Par d√©faut, PyTorch accumule les d√©grad√©s, ce qui est tr√®s pratique si vous ne disposez pas de suffisamment de ressources pour calculer tous les d√©grad√©s dont vous avez besoin en une seule passe. <br><br>  PyTorch utilise un syst√®me de ¬´bandes¬ª de d√©grad√©s automatiques - il collecte des informations sur les op√©rations et l'ordre dans lesquels les tenseurs ont √©t√© effectu√©s, puis les lit dans le sens oppos√© pour effectuer la diff√©renciation dans l'ordre inverse (diff√©renciation en mode inverse).  C'est pourquoi il est si super flexible et permet des graphes de calcul arbitraires.  Si aucun de ces tenseurs ne n√©cessite de d√©grad√©s (vous devez d√©finir <code>requires_grad=True</code> , en cr√©ant un tenseur √† cet effet), alors aucun graphique n'est enregistr√©!  Cependant, les r√©seaux ont g√©n√©ralement des param√®tres qui n√©cessitent des gradients, donc tous les calculs effectu√©s sur la base de la sortie du r√©seau seront stock√©s dans le graphique.  Donc, si vous souhaitez enregistrer les donn√©es r√©sultant de cette √©tape, vous devrez d√©sactiver manuellement les d√©grad√©s ou (une approche plus courante), enregistrer ces informations sous la forme d'un num√©ro Python (en utilisant <code>.item()</code> dans le scalaire PyTorch) ou un tableau <code>numpy</code> .  En savoir plus sur <code>autograd</code> dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> . <br><br>  Une fa√ßon de raccourcir le graphe de calcul consiste √† utiliser <code>.detach()</code> lorsque l'√©tat masqu√© est pass√© lors de l'apprentissage de RNN avec une version tronqu√©e de r√©tropropagation dans le temps.  Il est √©galement pratique pour la diff√©renciation des pertes, lorsque l'un des composants est la sortie d'un autre r√©seau, mais cet autre r√©seau ne doit pas √™tre optimis√© en ce qui concerne les pertes.  √Ä titre d'exemple, j'enseignerai la partie discriminatoire sur le mat√©riel de sortie g√©n√©r√© lors de la collaboration avec le GAN, ou la formation politique √† l'algorithme acteur-critique en utilisant la fonction objectif comme fonction de base (par exemple, A2C).  Une autre technique qui emp√™che le calcul des gradients est efficace dans la formation du GAN (formation de la partie g√©n√©ratrice sur un mat√©riau discriminant) et typique dans le r√©glage fin est l'√©num√©ration cyclique des param√®tres de r√©seau pour lesquels <code>param.requires_grad = False</code> . <br><br>  Il est important non seulement d'enregistrer les r√©sultats dans le fichier console / journal, mais √©galement de d√©finir des points de contr√¥le dans les param√®tres du mod√®le (et l'√©tat de l'optimiseur) au cas o√π.  Vous pouvez √©galement utiliser <code>torch.save()</code> pour enregistrer des objets Python normaux, ou utiliser une autre solution standard - le <code>pickle</code> int√©gr√©. <br><br><h4>  Test </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  En r√©ponse √† <code>.train()</code> r√©seaux doivent √™tre explicitement mis en mode d'√©valuation √† l'aide de <code>.eval()</code> . <br><br>  Comme mentionn√© ci-dessus, lors de l'utilisation d'un r√©seau, un graphe de calcul est g√©n√©ralement compil√©.  Pour √©viter cela, utilisez le <code>no_grad</code> contexte <code>no_grad</code> avec <code>with torch.no_grad()</code> . <br><br><h4>  Un peu plus </h4><br>  Ceci est une section suppl√©mentaire dans laquelle j'ai fait quelques digressions plus utiles. <br>  Voici la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> expliquant l'utilisation de la m√©moire. <br><br>  Erreurs CUDA?  Il est difficile de les corriger, et ils sont g√©n√©ralement li√©s √† des incoh√©rences logiques, selon lesquelles des messages d'erreur plus sensibles sont affich√©s sur le processeur que sur le GPU.  Mieux encore, si vous pr√©voyez de travailler avec le GPU, vous pouvez rapidement basculer entre le CPU et le GPU.  Une astuce de d√©veloppement plus g√©n√©rale consiste √† organiser le code afin qu'il puisse √™tre rapidement v√©rifi√© avant de commencer une t√¢che √† part enti√®re.  Par exemple, pr√©parez un petit ensemble de donn√©es ou synth√©tique, ex√©cutez un train de l'√©poque + test, etc.  Si le probl√®me est une erreur CUDA ou si vous ne pouvez pas du tout basculer vers le CPU, d√©finissez CUDA_LAUNCH_BLOCKING = 1.  Cela rendra le lancement du noyau CUDA synchrone et vous recevrez des messages d'erreur plus pr√©cis. <br><br>  Une note sur <code>torch.multiprocessing</code> ou simplement ex√©cuter plusieurs scripts PyTorch en m√™me temps.  √âtant donn√© que PyTorch utilise des biblioth√®ques BLAS multithread pour acc√©l√©rer les calculs d'alg√®bre lin√©aire sur le processeur, plusieurs c≈ìurs sont g√©n√©ralement impliqu√©s.  Si vous souhaitez faire plusieurs choses en m√™me temps, en utilisant un traitement multithread ou plusieurs scripts, il peut √™tre conseill√© de r√©duire manuellement leur nombre en d√©finissant la variable d'environnement <code>OMP_NUM_THREADS</code> sur 1 ou une autre valeur faible.  Ainsi, la probabilit√© de glissement du processeur est r√©duite.  La documentation officielle contient d'autres commentaires concernant le traitement multithread. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr471228/">https://habr.com/ru/post/fr471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr471212/index.html">10 trucs et astuces pour vous aider √† devenir le meilleur d√©veloppeur sur VueJS</a></li>
<li><a href="../fr471216/index.html">La longue histoire du guide - comment j'ai √©crit un service pour des sentiers de randonn√©e intelligents pendant 5 ans</a></li>
<li><a href="../fr471220/index.html">Cockpit - simplifiez les t√¢ches administratives typiques sous Linux gr√¢ce √† une interface Web pratique</a></li>
<li><a href="../fr471222/index.html">La compr√©hension des politiques de confidentialit√© des applications et des services aidera les r√©seaux de neurones</a></li>
<li><a href="../fr471226/index.html">Linux a plusieurs visages: comment travailler sur n'importe quelle distribution</a></li>
<li><a href="../fr471232/index.html">Mon exp√©rience de connexion du LPS331AP √† Omega Onion2</a></li>
<li><a href="../fr471236/index.html">Dosim√®tre pour Seryozha. Partie III. Radiom√®tre national</a></li>
<li><a href="../fr471240/index.html">¬´Bitchy Betty¬ª et interfaces audio modernes: pourquoi parlent-elles d'une voix f√©minine?</a></li>
<li><a href="../fr471242/index.html">Introduction √† Bash Shell</a></li>
<li><a href="../fr471244/index.html">Code Rosetta: mesurez la longueur du code dans un grand nombre de langages de programmation, √©tudiez la proximit√© des langages entre eux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>