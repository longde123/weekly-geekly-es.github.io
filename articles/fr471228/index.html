<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🗨️ 😿 🎋 Grokay PyTorch 👃🏼 🧕🏼 🧐</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! 

 Nous avons en pré-commande un livre tant attendu sur la bibliothèque PyTorch . 



 Puisque vous apprendrez tout le matériel de base...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Bonjour, Habr! <br><br>  Nous avons en pré-commande un livre tant attendu sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la bibliothèque PyTorch</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Puisque vous apprendrez tout le matériel de base nécessaire sur PyTorch dans ce livre, nous vous rappelons les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">avantages d'un</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processus</a> appelé «grokking» ou «compréhension approfondie» du sujet que vous souhaitez apprendre.  Dans le post d'aujourd'hui, nous vous expliquerons comment Kai Arulkumaran a critiqué PyTorch (pas d'image).  Bienvenue au chat. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PyTorch</a> est un cadre d'apprentissage en profondeur flexible qui distingue automatiquement les objets utilisant des réseaux de neurones dynamiques (c'est-à-dire les réseaux utilisant le contrôle de flux dynamique, comme les <code>if</code> et les boucles <code>while</code> ).  PyTorch prend en charge l'accélération GPU, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation distribuée</a> , divers types d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">optimisation</a> et de nombreuses autres fonctionnalités intéressantes.  Ici, je présente quelques réflexions sur la façon, à mon avis, d'utiliser PyTorch;  tous les aspects de la bibliothèque et les pratiques recommandées ne sont pas traités ici, mais j'espère que ce texte vous sera utile. <br><br>  Les réseaux de neurones sont une sous-classe de graphiques de calcul.  Les graphiques de calcul reçoivent des données en entrée, puis ces données sont acheminées (et peuvent être converties) aux nœuds où elles sont traitées.  En apprentissage profond, les neurones (nœuds) transforment généralement les données en leur appliquant des paramètres et des fonctions différenciables, de sorte que les paramètres peuvent être optimisés pour minimiser les pertes par la méthode de descente de gradient.  Dans un sens plus large, je note que les fonctions peuvent être stochastiques et un graphique dynamique.  Ainsi, alors que les réseaux de neurones correspondent bien au paradigme de programmation de flux de données, l'API PyTorch se concentre sur le paradigme de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">programmation impératif</a> , et cette façon d'interpréter les programmes créés est beaucoup plus familière.  C'est pourquoi le code PyTorch est plus facile à lire, il est plus facile de juger de la conception de programmes complexes, qui, cependant, ne nécessitent pas de compromis sérieux sur les performances: en fait, PyTorch est assez rapide et fournit de nombreuses optimisations dont vous, en tant qu'utilisateur final, ne pouvez pas vous inquiéter du tout (cependant, si vous êtes vraiment intéressé par eux, vous pouvez creuser un peu plus et apprendre à les connaître). <br><br>  Le reste de cet article est une analyse de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exemple officiel du jeu de données MNIST</a> .  Ici, nous <i>jouons</i> PyTorch, donc je recommande de comprendre l'article uniquement après avoir pris connaissance <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des manuels officiels du débutant</a> .  Pour plus de commodité, le code est présenté sous la forme de petits fragments équipés de commentaires, c'est-à-dire qu'il n'est pas distribué dans des fonctions / fichiers séparés que vous avez l'habitude de voir dans du code modulaire pur. <br><br><h4>  Importations </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  Tous ces éléments sont des importations tout à fait standard, à l'exception des modules <code>torchvision</code> , qui sont particulièrement utilisés pour résoudre des tâches liées à la vision par ordinateur. <br><br><h4>  Personnalisation </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> est la manière standard de gérer les arguments de ligne de commande en Python. <br><br>  Si vous devez écrire du code conçu pour fonctionner sur différents appareils (en utilisant l'accélération GPU, quand il est disponible, mais s'il n'est pas restauré aux calculs sur le CPU), sélectionnez et enregistrez le <code>torch.device</code> approprié, avec lequel vous pouvez déterminer où vous devez les tenseurs sont stockés.  Voir la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle pour</a> plus de détails sur la création d'un tel code.  L'approche de PyTorch est d'amener la sélection des appareils au contrôle de l'utilisateur, ce qui peut sembler indésirable dans des exemples simples.  Cependant, cette approche simplifie considérablement le travail lorsque vous devez gérer des tenseurs, ce qui a) est pratique pour le débogage b) vous permet d'utiliser efficacement les périphériques manuellement. <br><br>  Pour la reproductibilité des expériences, vous devez définir des valeurs initiales aléatoires pour tous les composants qui utilisent la génération de nombres aléatoires (y compris <code>random</code> ou <code>numpy</code> , si vous les utilisez <code>numpy</code> ).  Remarque: cuDNN utilise des algorithmes non déterministes et est facultativement désactivé à l'aide de <code>torch.backends.cudnn.enabled = False</code> . <br><br><h4>  Les données </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Étant <code>torchvision</code> modèles <code>torchvision</code> stockés sous <code>~/.torch/models/</code> , je préfère stocker les jeux de <code>torchvision</code> torchvision sous <code>~/.torch/datasets</code> .  Ceci est mon accord de copyright, mais il est très pratique à utiliser dans des projets développés sur la base de MNIST, CIFAR-10, etc.  En général, les ensembles de données doivent être stockés séparément du code si vous avez l'intention de réutiliser plusieurs ensembles de données. <br><br>  <code>torchvision.transforms</code> contient de nombreuses options de conversion pratiques pour des images individuelles, telles que le recadrage et la normalisation. <br><br>  Il existe de nombreuses options dans le <code>batch_size</code> , mais en plus de <code>batch_size</code> et <code>shuffle</code> , vous devez également garder à l'esprit <code>num_workers</code> et <code>pin_memory</code> , ils aident à augmenter l'efficacité.  <code>num_workers &gt; 0</code> utilise des sous-processus pour le chargement asynchrone des données et ne bloque pas le processus principal pour cela.  Un cas d'utilisation typique consiste à charger des données (par exemple, des images) à partir d'un disque et, éventuellement, à les convertir;  tout cela peut être fait en parallèle, avec le traitement des données réseau.  Le degré de traitement peut devoir être ajusté afin de a) minimiser le nombre de travailleurs et, par conséquent, la quantité de CPU et de RAM utilisée (chaque travailleur charge un lot séparé, plutôt que des échantillons individuels inclus dans le lot) b) minimiser la durée pendant laquelle les données attendent sur le réseau.  <code>pin_memory</code> utilise la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mémoire épinglée</a> (par opposition à la pagination) pour accélérer toutes les opérations de transfert de données de la RAM vers le GPU (et ne fait rien avec le code spécifique au CPU). <br><br><h4>  Modèle </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  L'initialisation du réseau s'étend généralement aux variables membres, aux couches qui contiennent des paramètres d'apprentissage et, éventuellement, aux paramètres d'apprentissage individuels et aux tampons non formés.  Ensuite, avec un passage direct, ils sont utilisés en combinaison avec des fonctions de <code>F</code> qui sont purement fonctionnelles et ne contiennent pas de paramètres.  Certaines personnes aiment travailler avec des réseaux purement fonctionnels (par exemple, conserver les paramètres et utiliser <code>F.conv2d</code> au lieu de <code>nn.Conv2d</code> ) ou des réseaux entièrement constitués de couches (par exemple, <code>nn.ReLU</code> au lieu de <code>F.relu</code> ). <br><br>  <code>.to(device)</code> est un moyen pratique d'envoyer des paramètres de périphérique (et des tampons) au GPU si le <code>device</code> défini sur GPU, car sinon (si le périphérique est défini sur CPU), rien ne sera fait.  Il est important de transférer les paramètres du périphérique vers le périphérique approprié avant de les transmettre à l'optimiseur;  sinon, l'optimiseur ne pourra pas suivre correctement les paramètres! <br><br>  Les réseaux de neurones ( <code>nn.Module</code> ) et les optimiseurs ( <code>optim.Optimizer</code> ) peuvent enregistrer et charger leur état interne, et il est recommandé de le faire avec <code>.load_state_dict(state_dict)</code> - il est nécessaire de recharger l'état des deux afin de reprendre la formation sur la base des dictionnaires précédemment enregistrés états.  L'enregistrement de l'objet entier peut être <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lourd d'erreurs</a> .  Si vous avez enregistré les tenseurs sur le GPU et que vous souhaitez les charger sur le CPU ou un autre GPU, alors le moyen le plus simple est de les charger directement sur le CPU en utilisant l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">option</a> <code>map_location</code> , par exemple <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Voici quelques autres points qui ne sont pas présentés ici, mais qui méritent d'être mentionnés, que vous pouvez utiliser le flux de contrôle avec un passage direct (par exemple, l'exécution de l' <code>if</code> peut dépendre de la variable membre ou des données elles-mêmes. En outre, elle est parfaitement valide au milieu du processus à afficher. ( <code>print</code> ) tensors, ce qui simplifie grandement le débogage. Enfin, avec un passage direct, beaucoup d'arguments peuvent être utilisés. J'illustrerai ce point avec une courte liste qui n'est liée à aucune idée particulière: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  La formation </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Les modules réseau sont mis en mode de formation par défaut - ce qui affecte dans une certaine mesure le fonctionnement des modules, surtout - l'amincissement et la normalisation des lots.  D'une manière ou d'une autre, il est préférable de définir ces éléments manuellement à l'aide de <code>.train()</code> , qui filtre l'indicateur "training" sur tous les modules enfants. <br><br>  Ici, la méthode <code>.to()</code> accepte non seulement le périphérique, mais définit également <code>non_blocking=True</code> , garantissant ainsi la copie asynchrone des données vers le GPU à partir de la mémoire <code>non_blocking=True</code> , permettant au CPU de rester opérationnel pendant le transfert de données;  sinon, <code>non_blocking=True</code> tout simplement pas une option. <br><br>  Avant de créer un nouvel ensemble de dégradés à l'aide de <code>loss.backward()</code> et de <code>optimiser.step()</code> en <code>optimiser.step()</code> aide d' <code>optimiser.step()</code> , vous devez réinitialiser manuellement les dégradés des paramètres à optimiser à l'aide d' <code>optimiser.zero_grad()</code> .  Par défaut, PyTorch accumule les dégradés, ce qui est très pratique si vous ne disposez pas de suffisamment de ressources pour calculer tous les dégradés dont vous avez besoin en une seule passe. <br><br>  PyTorch utilise un système de «bandes» de dégradés automatiques - il collecte des informations sur les opérations et l'ordre dans lesquels les tenseurs ont été effectués, puis les lit dans le sens opposé pour effectuer la différenciation dans l'ordre inverse (différenciation en mode inverse).  C'est pourquoi il est si super flexible et permet des graphes de calcul arbitraires.  Si aucun de ces tenseurs ne nécessite de dégradés (vous devez définir <code>requires_grad=True</code> , en créant un tenseur à cet effet), alors aucun graphique n'est enregistré!  Cependant, les réseaux ont généralement des paramètres qui nécessitent des gradients, donc tous les calculs effectués sur la base de la sortie du réseau seront stockés dans le graphique.  Donc, si vous souhaitez enregistrer les données résultant de cette étape, vous devrez désactiver manuellement les dégradés ou (une approche plus courante), enregistrer ces informations sous la forme d'un numéro Python (en utilisant <code>.item()</code> dans le scalaire PyTorch) ou un tableau <code>numpy</code> .  En savoir plus sur <code>autograd</code> dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> . <br><br>  Une façon de raccourcir le graphe de calcul consiste à utiliser <code>.detach()</code> lorsque l'état masqué est passé lors de l'apprentissage de RNN avec une version tronquée de rétropropagation dans le temps.  Il est également pratique pour la différenciation des pertes, lorsque l'un des composants est la sortie d'un autre réseau, mais cet autre réseau ne doit pas être optimisé en ce qui concerne les pertes.  À titre d'exemple, j'enseignerai la partie discriminatoire sur le matériel de sortie généré lors de la collaboration avec le GAN, ou la formation politique à l'algorithme acteur-critique en utilisant la fonction objectif comme fonction de base (par exemple, A2C).  Une autre technique qui empêche le calcul des gradients est efficace dans la formation du GAN (formation de la partie génératrice sur un matériau discriminant) et typique dans le réglage fin est l'énumération cyclique des paramètres de réseau pour lesquels <code>param.requires_grad = False</code> . <br><br>  Il est important non seulement d'enregistrer les résultats dans le fichier console / journal, mais également de définir des points de contrôle dans les paramètres du modèle (et l'état de l'optimiseur) au cas où.  Vous pouvez également utiliser <code>torch.save()</code> pour enregistrer des objets Python normaux, ou utiliser une autre solution standard - le <code>pickle</code> intégré. <br><br><h4>  Test </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  En réponse à <code>.train()</code> réseaux doivent être explicitement mis en mode d'évaluation à l'aide de <code>.eval()</code> . <br><br>  Comme mentionné ci-dessus, lors de l'utilisation d'un réseau, un graphe de calcul est généralement compilé.  Pour éviter cela, utilisez le <code>no_grad</code> contexte <code>no_grad</code> avec <code>with torch.no_grad()</code> . <br><br><h4>  Un peu plus </h4><br>  Ceci est une section supplémentaire dans laquelle j'ai fait quelques digressions plus utiles. <br>  Voici la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> expliquant l'utilisation de la mémoire. <br><br>  Erreurs CUDA?  Il est difficile de les corriger, et ils sont généralement liés à des incohérences logiques, selon lesquelles des messages d'erreur plus sensibles sont affichés sur le processeur que sur le GPU.  Mieux encore, si vous prévoyez de travailler avec le GPU, vous pouvez rapidement basculer entre le CPU et le GPU.  Une astuce de développement plus générale consiste à organiser le code afin qu'il puisse être rapidement vérifié avant de commencer une tâche à part entière.  Par exemple, préparez un petit ensemble de données ou synthétique, exécutez un train de l'époque + test, etc.  Si le problème est une erreur CUDA ou si vous ne pouvez pas du tout basculer vers le CPU, définissez CUDA_LAUNCH_BLOCKING = 1.  Cela rendra le lancement du noyau CUDA synchrone et vous recevrez des messages d'erreur plus précis. <br><br>  Une note sur <code>torch.multiprocessing</code> ou simplement exécuter plusieurs scripts PyTorch en même temps.  Étant donné que PyTorch utilise des bibliothèques BLAS multithread pour accélérer les calculs d'algèbre linéaire sur le processeur, plusieurs cœurs sont généralement impliqués.  Si vous souhaitez faire plusieurs choses en même temps, en utilisant un traitement multithread ou plusieurs scripts, il peut être conseillé de réduire manuellement leur nombre en définissant la variable d'environnement <code>OMP_NUM_THREADS</code> sur 1 ou une autre valeur faible.  Ainsi, la probabilité de glissement du processeur est réduite.  La documentation officielle contient d'autres commentaires concernant le traitement multithread. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr471228/">https://habr.com/ru/post/fr471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr471212/index.html">10 trucs et astuces pour vous aider à devenir le meilleur développeur sur VueJS</a></li>
<li><a href="../fr471216/index.html">La longue histoire du guide - comment j'ai écrit un service pour des sentiers de randonnée intelligents pendant 5 ans</a></li>
<li><a href="../fr471220/index.html">Cockpit - simplifiez les tâches administratives typiques sous Linux grâce à une interface Web pratique</a></li>
<li><a href="../fr471222/index.html">La compréhension des politiques de confidentialité des applications et des services aidera les réseaux de neurones</a></li>
<li><a href="../fr471226/index.html">Linux a plusieurs visages: comment travailler sur n'importe quelle distribution</a></li>
<li><a href="../fr471232/index.html">Mon expérience de connexion du LPS331AP à Omega Onion2</a></li>
<li><a href="../fr471236/index.html">Dosimètre pour Seryozha. Partie III. Radiomètre national</a></li>
<li><a href="../fr471240/index.html">«Bitchy Betty» et interfaces audio modernes: pourquoi parlent-elles d'une voix féminine?</a></li>
<li><a href="../fr471242/index.html">Introduction à Bash Shell</a></li>
<li><a href="../fr471244/index.html">Code Rosetta: mesurez la longueur du code dans un grand nombre de langages de programmation, étudiez la proximité des langages entre eux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>