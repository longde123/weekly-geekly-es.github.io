<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüîß üñïüèª üíπ Journalisation et suivi distribu√©s pour les microservices üë©üèΩ‚Äçüé§ üôÜüèø üë©‚Äç‚öïÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La journalisation est une partie importante de toute application. Tout syst√®me d'enregistrement passe par trois √©tapes √©volutives principales. Le prem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Journalisation et suivi distribu√©s pour les microservices</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/473946/">  La journalisation est une partie importante de toute application.  Tout syst√®me d'enregistrement passe par trois √©tapes √©volutives principales.  Le premier est sorti sur la console, le second est la journalisation dans un fichier et l'apparition d'un cadre pour la journalisation structur√©e, et le troisi√®me est la journalisation distribu√©e ou la collecte des journaux de divers services dans un seul centre. <br><br>  Si la journalisation est bien organis√©e, elle vous permet de comprendre ce qui, quand et comment √ßa va mal, et de transmettre les informations n√©cessaires aux personnes qui doivent corriger ces erreurs.  Pour un syst√®me dans lequel 100 000 messages sont envoy√©s chaque seconde dans 10 centres de donn√©es dans 190 pays et 350 ing√©nieurs d√©ploient quelque chose chaque jour, le syst√®me de journalisation est particuli√®rement important. <br><br><img src="https://habrastorage.org/webt/sy/7i/u_/sy7iu_dnjrrvar7krt8llrje1ga.jpeg"><br><br>  <b>Ivan Letenko</b> est chef d'√©quipe et d√©veloppeur chez Infobip.  Pour r√©soudre le probl√®me du traitement centralis√© et du tra√ßage des journaux dans l'architecture de microservice sous de telles charges, la soci√©t√© a essay√© diff√©rentes combinaisons de la pile ELK, Graylog, Neo4j et MongoDB.  En cons√©quence, apr√®s beaucoup de ratissage, ils ont √©crit leur service de journalisation sur Elasticsearch, et PostgreSQL a √©t√© pris comme base de donn√©es pour des informations suppl√©mentaires. <br><br>  Sous le chat en d√©tail, avec des exemples et des graphiques: l'architecture et l'√©volution du syst√®me, les r√¢teaux, la journalisation et le tra√ßage, les m√©triques et la surveillance, la pratique de travailler avec les clusters Elasticsearch et de les administrer avec des ressources limit√©es. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Sr71xsI6X5I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Pour vous pr√©senter le contexte, je vais vous parler un peu de l'entreprise.  Nous aidons les clients-organisations √† transmettre des messages √† leurs clients: messages d'un service de taxi, SMS d'une banque au sujet de l'annulation, ou un mot de passe √† usage unique lors de la saisie de VC.  <b>350 millions de messages</b> nous traversent chaque jour pour des clients dans 190 pays.  Nous acceptons, traitons, facturons, acheminons, adaptons, envoyons aux op√©rateurs et traitons les rapports de livraison dans la direction oppos√©e et formons des analyses. <br><br>  Pour que tout cela fonctionne dans de tels volumes, nous avons: <br><br><ul><li>  36 centres de donn√©es dans le monde; <br></li><li>  Plus de 5000 machines virtuelles <br></li><li>  350+ ing√©nieurs; <br></li><li>  730+ microservices diff√©rents. <br></li></ul><br>  C'est un syst√®me complexe, et aucun gourou ne peut √† lui seul comprendre la pleine √©chelle.  L'un des principaux objectifs de notre soci√©t√© est la rapidit√© de livraison de nouvelles fonctionnalit√©s et de nouvelles versions pour les entreprises.  Dans ce cas, tout devrait fonctionner et ne pas tomber.  Nous y travaillons: 40000 d√©ploiements en 2017, 80000 en 2018, 300 d√©ploiements par jour. <br><br>  Nous avons 350 ing√©nieurs - il s'av√®re que <b>chaque ing√©nieur d√©ploie quelque chose quotidiennement</b> .  Il y a quelques ann√©es √† peine, une seule personne dans une entreprise avait une telle productivit√© - Kreshimir, notre ing√©nieur principal.  Mais nous nous sommes assur√©s que chaque ing√©nieur se sente aussi confiant que Kresimir lorsqu'il appuie sur le bouton D√©ployer ou ex√©cute un script. <br><br>  Que faut-il pour cela?  Tout d'abord, la <b>confiance que nous comprenons ce qui se passe dans le syst√®me</b> et dans quel √©tat il se trouve.  La confiance est donn√©e par la possibilit√© de poser une question au syst√®me et de d√©couvrir la cause du probl√®me lors de l'incident et lors de l'√©laboration du code. <br><br>  Pour atteindre cette confiance, nous investissons dans l' <b>observabilit√©</b> .  Traditionnellement, ce terme combine trois composantes: <br><br><ul><li>  enregistrement; <br></li><li>  m√©triques <br></li><li>  trace. <br></li></ul><br>  Nous en parlerons.  Tout d'abord, examinons notre solution de journalisation, mais nous aborderons √©galement les m√©triques et les traces. <br><br><h2>  √âvolution </h2><br>  Presque toute application ou syst√®me de journalisation, y compris le n√¥tre, passe par plusieurs √©tapes d'√©volution. <br><br>  La premi√®re √©tape consiste √† <b>sortir sur la console</b> . <br><br>  Deuxi√®mement - nous commen√ßons <b>√† √©crire des journaux dans un fichier</b> , un <b>cadre</b> appara√Æt pour une sortie structur√©e dans un fichier.  Nous utilisons g√©n√©ralement Logback car nous vivons dans la JVM.  √Ä ce stade, une journalisation structur√©e dans un fichier appara√Æt, comprenant que les diff√©rents journaux doivent avoir diff√©rents niveaux, avertissements et erreurs. <br><br>  D√®s <b>qu'il existe plusieurs</b> <b>instances de notre service</b> ou diff√©rents services, la t√¢che d' <b>acc√®s centralis√© aux</b> logs pour les d√©veloppeurs et le support appara√Æt.  Nous passons √† la journalisation distribu√©e - nous combinons diff√©rents services en un seul service de journalisation. <br><br><h2>  Journalisation distribu√©e </h2><br>  L'option la plus connue est la pile ELK: Elasticsearch, Logstash et Kibana, mais nous avons choisi <b>Graylog</b> .  Il a une interface cool qui est orient√©e vers la journalisation.  Les alarmes sortent de la bo√Æte d√©j√† dans la version gratuite, qui n'est pas dans Kibana, par exemple.  Pour nous, c'est un excellent choix en termes de b√ªches, et sous le capot, c'est le m√™me Elasticsearch. <br><br><img src="https://habrastorage.org/webt/x1/en/7f/x1en7fmypsgbipabhmfhy6y7rts.jpeg"><br>  <i>Dans Graylog, vous pouvez cr√©er des alertes, des graphiques comme Kibana et m√™me des m√©triques de journal.</i> <br><br><h3>  Les probl√®mes </h3><br>  Notre entreprise √©tait en pleine croissance et √† un moment donn√©, il est devenu clair que quelque chose n'allait pas avec Graylog. <br><br>  <b>Charge excessive</b> .  Il y avait des probl√®mes de performances.  De nombreux d√©veloppeurs ont commenc√© √† utiliser les fonctionnalit√©s int√©ressantes de Graylog: ils ont construit des m√©triques et des tableaux de bord qui effectuent l'agr√©gation de donn√©es.  Ce n'est pas le meilleur choix pour cr√©er des analyses complexes sur le cluster Elasticsearch, qui est soumis √† une lourde charge d'enregistrement. <br><br>  <b>Collisions</b>  Il y a beaucoup d'√©quipes, il n'y a pas de sch√©ma unique.  Traditionnellement, lorsqu'un identifiant touchait Graylog pour la premi√®re fois, un mappage se produisait automatiquement.  Si une autre √©quipe d√©cide qu'il doit √™tre √©crit l'UUID sous forme de cha√Æne - cela cassera le syst√®me. <br><br><h2>  Premi√®re d√©cision </h2><br>  <b>Journaux d'application et journaux de communication s√©par√©s</b> .  Diff√©rents journaux ont diff√©rents sc√©narios et m√©thodes d'application.  Il existe, par exemple, des journaux d'application pour lesquels diff√©rentes √©quipes ont des exigences diff√©rentes pour diff√©rents param√®tres: par le temps de stockage dans le syst√®me, par la vitesse de recherche. <br><br>  Par cons√©quent, la premi√®re chose que nous avons faite a √©t√© de s√©parer les journaux d'application et les journaux de communication.  Le deuxi√®me type est constitu√© de journaux importants qui stockent des informations sur l'interaction de notre plateforme avec le monde ext√©rieur et sur l'interaction au sein de la plateforme.  Nous en parlerons davantage. <br><br>  <b>Remplac√© une partie substantielle des journaux par des m√©triques</b> .  Dans notre entreprise, le choix standard est Prom√©th√©e et Grafana.  Certaines √©quipes utilisent d'autres solutions.  Mais il est important que nous nous soyons d√©barrass√©s d'un grand nombre de tableaux de bord avec des agr√©gations √† l'int√©rieur de Graylog, que nous ayons tout transf√©r√© √† Prometheus et Grafana.  Cela a consid√©rablement all√©g√© la charge sur les serveurs. <br><br>  Examinons les sc√©narios d'application des journaux, des m√©triques et des traces. <br><br><h3>  Journaux </h3><br>  <b>Haute dimensionnalit√©, d√©bogage et recherche</b> .  Quels sont les bons journaux? <br><blockquote>  Les journaux sont les √©v√©nements que nous enregistrons. </blockquote>  Ils peuvent avoir une grande dimension: vous pouvez enregistrer l'ID de demande, l'ID utilisateur, les attributs de demande et d'autres donn√©es, dont la dimension n'est pas limit√©e.  Ils sont √©galement utiles pour le d√©bogage et la recherche, pour poser des questions au syst√®me sur ce qui s'est pass√© et rechercher les causes et les effets. <br><br><h3>  Mesures </h3><br>  <b>Faible dimensionnalit√©, agr√©gation, surveillance et alertes</b> .  Sous le capot de tous les syst√®mes de collecte m√©trique se trouvent les bases de donn√©es de s√©ries chronologiques.  Ces bases de donn√©es font un excellent travail d'agr√©gation, de sorte que les mesures conviennent pour l'agr√©gation, la surveillance et la cr√©ation d'alertes. <br><blockquote>  Les m√©triques sont tr√®s sensibles √† la dimension des donn√©es. </blockquote>  Pour les m√©triques, la dimension des donn√©es ne doit pas d√©passer mille.  Si nous ajoutons des ID de demande, dans lesquels la taille des valeurs n'est pas limit√©e, nous rencontrerons rapidement de graves probl√®mes.  Nous avons d√©j√† march√© sur ce r√¢teau. <br><br><h3>  Corr√©lation et trace </h3><blockquote>  Les journaux doivent √™tre corr√©l√©s. </blockquote>  Les journaux structur√©s ne nous suffisent pas pour effectuer facilement des recherches par donn√©es.  Il doit y avoir des champs avec certaines valeurs: ID de demande, ID utilisateur, autres donn√©es des services dont proviennent les journaux. <br><br>  La solution traditionnelle consiste √† attribuer un ID unique √† la transaction (journal) √† l'entr√©e du syst√®me.  Cet ID (contexte) est ensuite transmis √† travers l'ensemble du syst√®me via une cha√Æne d'appels au sein d'un service ou entre services. <br><br><img src="https://habrastorage.org/webt/sk/ba/ht/skbahtrbo1zjpc7x8u8hy_odxg4.png"><br>  <i>Corr√©lation et tra√ßage.</i> <br><br>  Il y a des termes bien √©tablis.  La trace est divis√©e en √©tendues et illustre la pile d'appels d'un service par rapport √† un autre, une m√©thode par rapport √† une autre par rapport √† la chronologie.  Vous pouvez clairement tracer le chemin du message, tous les timings. <br><br>  Nous avons d'abord utilis√© Zipkin.  D√©j√† en 2015, nous avions une Proof of Concept (projet pilote) de ces solutions. <br><br><img src="https://habrastorage.org/webt/iw/jg/f1/iwjgf1st5rm6ymwppx3g9scb_uw.jpeg"><br>  <i>Trace distribu√©e</i> <br><br>  Pour obtenir une telle image, le <b>code doit √™tre instrument√©</b> .  Si vous travaillez d√©j√† avec une base de code qui existe, vous devez la parcourir - elle n√©cessite des modifications. <br><br>  Pour obtenir une image compl√®te et b√©n√©ficier des traces, vous devez <b>instrumenter tous les services de la cha√Æne</b> , et pas seulement un service sur lequel vous travaillez actuellement. <br><br>  Il s'agit d'un outil puissant, mais qui n√©cessite des co√ªts d'administration et de mat√©riel importants, nous avons donc bascul√© de Zipkin vers une autre solution, qui est fournie par ¬´as a service¬ª. <br><br><h2>  Rapports de livraison </h2><br>  Les journaux doivent √™tre corr√©l√©s.  Les traces doivent √©galement √™tre corr√©l√©es.  Nous avons besoin d'un identifiant unique - un contexte commun qui peut √™tre transmis tout au long de la cha√Æne d'appel.  Mais souvent, cela n'est pas possible - une <b>corr√©lation se produit au sein du syst√®me en raison de son fonctionnement</b> .  Lorsque nous d√©marrons une ou plusieurs transactions, nous ne savons toujours pas qu'elles font partie d'un seul grand ensemble. <br><br>  Prenons le premier exemple. <br><br><img src="https://habrastorage.org/webt/05/lf/ds/05lfdssmlzvh6h41nzgl3w8cjwu.jpeg"><br>  <i>Rapports de livraison.</i> <br><br><ul><li>  Le client a envoy√© une demande de message et notre plateforme interne l'a trait√©e. <br></li><li>  Le service, qui est en interaction avec l'op√©rateur, a envoy√© ce message √† l'op√©rateur - une entr√©e est apparue dans le syst√®me de journalisation. <br></li><li>  Plus tard, l'op√©rateur nous envoie un rapport de livraison. <br></li><li>  Le service de traitement ne sait pas √† quel message se rapporte ce rapport de remise.  Cette relation est cr√©√©e plus tard dans notre plateforme. <br></li></ul><br>  Deux transactions li√©es font partie d'une seule transaction enti√®re.  Ces informations sont tr√®s importantes pour les ing√©nieurs de support et les d√©veloppeurs d'int√©gration.  Mais cela est compl√®tement impossible √† voir sur la base d'une seule trace ou d'un seul ID. <br><br>  Le deuxi√®me cas est similaire - le client nous envoie un message dans un grand paquet, puis nous les d√©montons, ils reviennent √©galement par lots.  Le nombre de packs peut m√™me varier, mais tous sont combin√©s. <br><br><img src="https://habrastorage.org/webt/67/jg/w3/67jgw3eyg33a7bwjsx8savqmgxo.jpeg"><br><br>  Du point de vue du client, il a envoy√© un message et re√ßu une r√©ponse.  Mais nous avons obtenu plusieurs transactions ind√©pendantes qui doivent √™tre combin√©es.  Il se r√©v√®le une relation un-√†-plusieurs, et avec un rapport de livraison - un √† un.  Il s'agit essentiellement d'un graphique. <br><br><img src="https://habrastorage.org/webt/h6/bm/ak/h6bmak77dcsvoqebnfriqxuiqcq.jpeg"><br>  <i>Nous construisons un graphique.</i> <br><br>  Une fois que nous voyons un graphique, alors un choix ad√©quat est les bases de donn√©es graphiques, par exemple, Neo4j.  Le choix √©tait √©vident car Neo4j offre des T-shirts sympas et des livres gratuits lors de conf√©rences. <br><br><h3>  Neo4j </h3><br>  Nous avons impl√©ment√© Proof of Concept: un h√¥te 16 c≈ìurs pouvant traiter un graphique de 100 millions de n≈ìuds et 150 millions de liens.  Le graphique n'occupait que 15 Go de disque - alors il nous convenait. <br><br><img src="https://habrastorage.org/webt/aa/5q/bk/aa5qbkmdwshpycitkyz3-lztkck.jpeg"><br>  <i>Notre d√©cision.</i>  <i>Architecture des journaux.</i> <br><br>  En plus de Neo4j, nous avons maintenant une interface simple pour afficher les journaux associ√©s.  Avec lui, les ing√©nieurs voient la situation dans son ensemble. <br><br>  Mais assez rapidement, nous avons √©t√© d√©√ßus par cette base de donn√©es. <br><br><h3>  Probl√®mes avec Neo4j </h3><br>  <b>Rotation des donn√©es</b> .  Nous avons des volumes puissants et les donn√©es doivent √™tre tourn√©es.  Mais lorsqu'un n≈ìud est supprim√© de Neo4j, les donn√©es sur le disque ne sont pas effac√©es.  J'ai d√ª construire une solution complexe et reconstruire compl√®tement les graphiques. <br><br>  <b>Performance</b> .  Toutes les bases de donn√©es graphiques sont en lecture seule.  √Ä l'enregistrement, les performances sont sensiblement inf√©rieures.  Notre cas est tout √† fait le contraire: nous √©crivons beaucoup et lisons relativement rarement - ce sont des unit√©s de requ√™tes par seconde voire par minute. <br><br>  <b>Haute disponibilit√© et analyse de cluster moyennant des frais</b> .  √Ä notre √©chelle, cela se traduit par des co√ªts d√©cents. <br><br>  Par cons√©quent, nous sommes all√©s dans l'autre sens. <br><br><h3>  Solution avec PostgreSQL </h3><br>  Nous avons d√©cid√© que, comme nous lisons rarement, le graphique peut √™tre construit √† la vol√©e lors de la lecture.  Ainsi, dans la base de donn√©es relationnelle PostgreSQL, nous stockons la liste d'adjacence de nos ID sous la forme d'une plaque simple avec deux colonnes et un index sur les deux.  Lorsque la demande arrive, nous contournons le graphique de connectivit√© en utilisant l'algorithme DFS familier (travers√©e en profondeur) et obtenons tous les ID associ√©s.  Mais c'est n√©cessaire. <br><br>  La rotation des donn√©es est √©galement facile √† r√©soudre.  Pour chaque jour, nous commen√ßons une nouvelle plaque et apr√®s quelques jours, le moment venu, nous la supprimons et lib√©rons les donn√©es.  Une solution simple. <br><br>  Nous avons maintenant 850 millions de connexions dans PostgreSQL, elles occupent 100 Go de disque.  Nous y √©crivons √† une vitesse de 30 000 par seconde, et pour cela dans la base de donn√©es, il n'y a que deux VM avec 2 CPU et 6 Go de RAM.  Au besoin, PostgreSQL peut √©crire des longs longs. <br><br>  Il existe encore de petites machines pour le service lui-m√™me, qui tournent et contr√¥lent. <br><br><img src="https://habrastorage.org/webt/yk/re/vq/ykrevqnk3xx8rpa3lkc9lkgiram.jpeg"><br>  <i>Comment notre architecture a chang√©.</i> <br><br><h2>  D√©fis avec Graylog </h2><br>  La soci√©t√© a grandi, de nouveaux centres de donn√©es sont apparus, la charge a consid√©rablement augment√©, m√™me avec une solution avec des journaux de communication.  Nous pensions que Graylog n'est plus parfait. <br><br>  <b>Sch√©ma unifi√© et centralisation</b> .  J'aimerais avoir un outil de gestion de cluster unique dans 10 centres de donn√©es.  En outre, la question s'est pos√©e d'un sch√©ma de cartographie des donn√©es unifi√© afin qu'il n'y ait pas de collisions. <br><br>  <b>API</b>  Nous utilisons notre propre interface pour afficher les connexions entre les journaux et l'API Graylog standard n'√©tait pas toujours pratique √† utiliser, par exemple, lorsque vous devez afficher des donn√©es de diff√©rents centres de donn√©es, les trier et les marquer correctement.  Par cons√©quent, nous voulions pouvoir changer l'API √† notre guise. <br><br>  <b>Les performances, il est difficile d'√©valuer la perte</b> .  Notre trafic est de 3 To de journaux par jour, ce qui est d√©cent.  Par cons√©quent, Graylog n'a pas toujours fonctionn√© de mani√®re stable, il a fallu entrer dans ses entrailles afin de comprendre les causes des pannes.  Il s'est av√©r√© que nous ne l'utilisions plus comme un outil - nous devions y faire quelque chose. <br><br>  <b>Retards de traitement (files d'attente)</b> .  Nous n'avons pas aim√© l'impl√©mentation standard de la file d'attente dans Graylog. <br><br>  <b>La n√©cessit√© de supporter MongoDB</b> .  Graylog tra√Æne MongoDB, il fallait aussi administrer ce syst√®me. <br><br>  Nous avons r√©alis√© qu'√† ce stade, nous voulons notre propre solution.  Il y a peut-√™tre moins de fonctionnalit√©s int√©ressantes pour les alertes qui n'ont pas √©t√© utilis√©es, pour les tableaux de bord, mais les leurs sont meilleures. <br><br><h3>  Notre d√©cision </h3><br>  Nous avons d√©velopp√© notre propre service Logs. <br><br><img src="https://habrastorage.org/webt/2e/cl/zb/2eclzbtgkkzyjdy1u9amvwujcw8.jpeg"><br>  <i>Service de journalisation.</i> <br><br>  √Ä ce moment-l√†, nous avions d√©j√† une expertise dans le service et la maintenance de grands clusters Elasticsearch, nous avons donc pris Elasticsearch comme base.  La pile standard de l'entreprise est JVM, mais pour le backend, nous utilisons √©galement Kotlin de mani√®re c√©l√®bre, nous avons donc pris ce langage pour le service. <br><br>  La premi√®re question est de savoir comment faire pivoter les donn√©es et que faire de la cartographie.  Nous utilisons une cartographie fixe.  Dans Elasticsearch, il est pr√©f√©rable d'avoir des index de m√™me taille.  Mais avec de tels indices, nous devons en quelque sorte cartographier les donn√©es, en particulier pour plusieurs centres de donn√©es, un syst√®me distribu√© et un √©tat distribu√©.  Il y avait des id√©es pour fixer ZooKeeper, mais c'est encore une complication de maintenance et de code. <br><blockquote>  Par cons√©quent, nous avons d√©cid√© simplement - √©crire √† temps. </blockquote>  Un index pendant une heure, dans d'autres centres de donn√©es 2 index pendant une heure, dans le troisi√®me index pendant 3 heures, mais tout le temps.  Les indices sont obtenus en diff√©rentes tailles, car la nuit, le trafic est inf√©rieur √† celui de la journ√©e, mais en g√©n√©ral cela fonctionne.  L'exp√©rience a montr√© qu'aucune complication n'est n√©cessaire. <br><br>  Pour faciliter la migration et compte tenu de la grande quantit√© de donn√©es, nous avons choisi le protocole GELF, un protocole Graylog simple bas√© sur TCP.  Nous avons donc obtenu un serveur GELF pour Netty et un d√©codeur GELF. <br><br>  Ensuite, JSON est cod√© pour √©crire dans Elasticsearch.  Nous utilisons l'API Java officielle d'Elasticsearch et √©crivons en vrac. <br><blockquote>  Pour une vitesse d'enregistrement √©lev√©e, vous devez √©crire Bulk'ami. </blockquote>  Il s'agit d'une optimisation importante.  L'API fournit un processeur en bloc qui accumule automatiquement les demandes, puis les envoie pour enregistrement dans un ensemble ou au fil du temps. <br><br><h3>  Probl√®me avec le processeur en vrac </h3><br>  Tout semble aller bien.  Mais nous avons commenc√© et r√©alis√© que nous nous reposions sur le processeur Bulk - c'√©tait inattendu.  Nous ne pouvons pas atteindre les valeurs sur lesquelles nous comptions - le probl√®me est venu de nulle part. <br><br><img src="https://habrastorage.org/webt/f8/eh/aq/f8ehaq7rjnpje-lcnrr3gtbk6ho.jpeg"><br><br>  Dans l'impl√©mentation standard, le processeur Bulk est un thread unique, synchrone, malgr√© le fait qu'il existe un param√®tre de parall√©lisme.  C'√©tait √ßa le probl√®me. <br><br>  Nous avons fouill√© et il s'est av√©r√© qu'il s'agit d'un bogue connu mais non r√©solu.  Nous avons un peu chang√© le processeur Bulk - nous avons fait un verrou explicite via ReentrantLock.  Ce n'est qu'en mai que des modifications similaires ont √©t√© apport√©es au r√©f√©rentiel officiel d'Elasticsearch et ne seront disponibles qu'√† partir de la version 7.3.  La version actuelle est 7.1, et nous utilisons la version 6.3. <br><br>  Si vous travaillez √©galement avec un processeur en bloc et que vous souhaitez overclocker une entr√©e dans Elasticsearch - regardez ces <a href="">changements sur GitHub</a> et <a href="">portez</a> √† votre version.  Les modifications affectent uniquement le processeur Bulk.  Il n'y aura aucune difficult√© si vous devez porter sur la version ci-dessous. <br><br>  Tout va bien, le processeur Bulk est parti, la vitesse s'est acc√©l√©r√©e. <br><br><img src="https://habrastorage.org/webt/al/kk/nz/alkknzzqgvbtqc-lx0alpunp27y.jpeg"><br><br>  Les performances d'√©criture d'Elasticsearch sont instables dans le temps, car diverses op√©rations s'y d√©roulent: fusion d'index, vidage.  En outre, les performances ralentissent pendant un certain temps lors de la maintenance, lorsqu'une partie des n≈ìuds est supprim√©e du cluster, par exemple. <br><br>  √Ä cet √©gard, nous avons r√©alis√© que nous devons impl√©menter non seulement le tampon en m√©moire, mais √©galement la file d'attente.  Nous avons d√©cid√© que nous n'enverrions que des messages rejet√©s √† la file d'attente - uniquement ceux que le processeur de masse ne pouvait pas √©crire dans Elasticsearch. <br><br><h3>  R√©essayer le repli </h3><br>  Il s'agit d'une impl√©mentation simple. <br><br><ul><li> Nous enregistrons les messages rejet√©s dans le fichier - <code>RejectedExecutionHandler</code> . <br></li><li>  Soumettez √† nouveau √† l'intervalle sp√©cifi√© dans un ex√©cuteur distinct. <br></li><li>  Cependant, nous ne retardons pas le nouveau trafic. <br></li></ul><br>  Pour les ing√©nieurs de support et les d√©veloppeurs, le nouveau trafic dans le syst√®me est nettement plus important que celui qui, pour une raison quelconque, a √©t√© retard√© lors de la mont√©e en puissance ou du ralentissement d'Elasticsearch.  Il s'attarda, mais il reviendrait plus tard - ce n'√©tait pas grave.  Le nouveau trafic est prioris√©. <br><br><img src="https://habrastorage.org/webt/_f/jf/xz/_fjfxzutbmmt88ibablan9les0i.jpeg"><br>  <i>Notre sch√©ma a commenc√© √† ressembler √† ceci.</i> <br><br>  Parlons maintenant de la fa√ßon dont nous pr√©parons Elasticsearch, des param√®tres que nous avons utilis√©s et de la configuration. <br><br><h2>  Configuration d'Elasticsearch </h2><br>  Le probl√®me auquel nous sommes confront√©s est la n√©cessit√© d'overclocker Elasticsearch et de l'optimiser pour l'√©criture, car le nombre de lectures est sensiblement plus petit. <br><br>  Nous avons utilis√© plusieurs param√®tres. <br><br>  <code>"ignore_malformed": true</code> - <b>supprime les champs avec le mauvais type, et non le document entier</b> .  Nous voulons toujours stocker les donn√©es, m√™me si, pour une raison quelconque, des champs avec un mappage incorrect y ont √©t√© divulgu√©s.  Cette option n'est pas enti√®rement li√©e aux performances. <br><br>  Pour le fer, Elasticsearch a une nuance.  Lorsque nous avons commenc√© √† demander de gros clusters, on nous a dit que les matrices RAID √† partir de disques SSD pour vos volumes co√ªtent terriblement cher.  Mais les tableaux ne sont pas n√©cessaires car la tol√©rance aux pannes et le partitionnement sont d√©j√† int√©gr√©s dans Elasticsearch.  M√™me sur le site officiel, il est recommand√© de prendre plus de fer bon march√© que moins cher et bon.  Cela s'applique √† la fois aux disques et au nombre de c≈ìurs de processeur, car l'ensemble Elasticsearch est tr√®s bien parall√®le. <br><br>  <code>"index.merge.scheduler.max_thread_count": 1</code> - <b>recommand√© pour le disque dur</b> . <br>  Si vous n'avez pas obtenu de SSD, mais des disques durs ordinaires, d√©finissez ce param√®tre sur un.  Les index sont √©crits en morceaux, puis ces morceaux sont fig√©s.  Cela √©conomise un peu de disque, mais surtout acc√©l√®re la recherche.  En outre, lorsque vous arr√™tez d'√©crire dans l'index, vous pouvez <code>force merge</code> .  Lorsque la charge sur le cluster est moindre, il se bloque automatiquement. <br><br>  <code>"index.unassigned.node_left.delayed_timeout": "5m"</code> - <b>retard de d√©ploiement lorsqu'un n≈ìud dispara√Æt</b> .  C'est le temps apr√®s lequel Elasticsearch commencera √† impl√©menter les index et les donn√©es si un n≈ìud est red√©marr√©, d√©ploy√© ou retir√© pour maintenance.  Mais si vous avez une lourde charge sur le disque et le r√©seau, le d√©ploiement est une op√©ration difficile.  Afin de ne pas les surcharger, cette temporisation est pr√©f√©rable pour contr√¥ler et comprendre les retards n√©cessaires. <br><br>  <code>"index.refresh_interval": -1</code> - <b>ne met pas √† jour les index s'il n'y a pas de requ√™tes de recherche</b> .  Ensuite, l'index sera mis √† jour lorsqu'une requ√™te de recherche appara√Æt.  Cet index peut √™tre d√©fini en secondes et minutes. <br><br>  <code>"index.translogDurability": "async"</code> - √† quelle fr√©quence ex√©cuter fsync: √† chaque demande ou par heure.  Donne des gains de performances pour les disques lents. <br><br>  Nous avons √©galement une mani√®re int√©ressante de l'utiliser.  Le support et les d√©veloppeurs souhaitent pouvoir effectuer des recherches en texte int√©gral et utiliser regexp'ov dans tout le corps du message.  Mais dans Elasticsearch, cela n'est pas possible - il ne peut rechercher que par jetons qui existent d√©j√† dans son syst√®me.  RegExp et le caract√®re g√©n√©rique peuvent √™tre utilis√©s, mais le jeton ne peut pas d√©marrer avec certains RegExp.  Par cons√©quent, nous avons ajout√© <code>word_delimiter</code> au filtre: <br><br><pre> <code class="plaintext hljs">"tokenizer": "standard" "filter" : [ "word_delimiter" ]</code> </pre> <br>  Il divise automatiquement les mots en jetons: <br><br><ul><li>  ¬´Wi-Fi¬ª ‚Üí ¬´Wi¬ª, ¬´Fi¬ª; <br></li><li>  "PowerShot" ‚Üí "Power", "Shot"; <br></li><li>  "SD500" ‚Üí "SD", "500". <br></li></ul><br>  De mani√®re similaire, le nom de la classe est √©crit, diverses informations de d√©bogage.  Avec cela, nous avons r√©solu certains des probl√®mes li√©s √† la recherche en texte int√©gral.  Je vous conseille d'ajouter de tels param√®tres lorsque vous travaillez avec la connexion. <br><br><h3>  √Ä propos du cluster </h3><br>  <b>Le nombre de fragments doit √™tre √©gal au nombre de n≈ìuds de donn√©es pour l'√©quilibrage de charge</b> .  Le nombre minimum de r√©pliques est 1, puis chaque n≈ìud aura un fragment principal et une r√©plique.  Mais si vous avez des donn√©es pr√©cieuses, par exemple, des transactions financi√®res, il vaut mieux en faire 2 ou plus. <br><br>  <b>La taille du fragment est de quelques Go √† plusieurs dizaines de Go</b> .  Le nombre de fragments sur un n≈ìud n'est pas sup√©rieur √† 20 pour 1 Go de hanche Elasticsearch, bien s√ªr.  Elasticsearch ralentit davantage - nous l'avons √©galement attaqu√©.  Dans les centres de donn√©es o√π le trafic est faible, les donn√©es n'ont pas tourn√© en volume, des milliers d'index sont apparus et le syst√®me s'est √©cras√©. <br><br>  <b>Utilisez la</b> <code>allocation awareness</code> , par exemple, par le nom d'un hyperviseur en cas de service.  Aide √† disperser les index et les fragments sur diff√©rents hyperviseurs afin qu'ils ne se chevauchent pas lorsqu'un hyperviseur tombe. <br><br>  <b>Cr√©ez des index au pr√©alable</b> .  Bonne pratique, surtout lorsque vous √©crivez √† temps.  L'index est imm√©diatement chaud, pr√™t et il n'y a aucun retard. <br><br>  <b>Limitez le nombre de fragments d'un index par n≈ìud</b> .  <code>"index.routing.allocation.total_shards_per_node": 4</code> est le nombre maximal de fragments d'un index par n≈ìud.  Dans le cas id√©al, il y en a 2, nous en mettons 4 juste au cas o√π, si nous avons encore moins de voitures. <br><br>  Quel est le probl√®me ici?  Nous utilisons la <code>allocation awareness</code> - Elasticsearch sait comment r√©partir correctement les index entre les hyperviseurs.  Mais nous avons d√©couvert qu'apr√®s avoir √©teint le n≈ìud pendant une longue p√©riode, puis revenir au cluster, Elasticsearch constate qu'il y a formellement moins d'index dessus et qu'ils sont restaur√©s.  Tant que les donn√©es ne sont pas synchronis√©es, il existe officiellement peu d'index sur le n≈ìud.  Si n√©cessaire, allouez un nouvel index, Elasticsearch essaie de marteler cette machine aussi dens√©ment que possible avec de nouveaux index.  Ainsi, un n≈ìud re√ßoit une charge non seulement du fait que les donn√©es y sont r√©pliqu√©es, mais √©galement avec du trafic frais, des index et de nouvelles donn√©es qui tombent sur ce n≈ìud.  Contr√¥lez et limitez-le. <br><br><h3>  Recommandations de maintenance Elasticsearch </h3><br>  Ceux qui travaillent avec Elasticsearch connaissent ces recommandations. <br><blockquote>  Pendant la maintenance planifi√©e, appliquez les recommandations de mise √† niveau progressive: d√©sactivez l'allocation des partitions, vidage synchronis√©. </blockquote>  <b>D√©sactivez l'allocation des fragments</b> .  D√©sactivez l'allocation des fragments de r√©pliques, laissez la capacit√© d'allouer uniquement le primaire.  Cela aide sensiblement Elasticsearch - il ne r√©affectera pas les donn√©es dont vous n'avez pas besoin.  Par exemple, vous savez que dans une demi-heure, le n≈ìud augmentera - pourquoi transf√©rer tous les fragments d'un n≈ìud √† un autre?  Rien de terrible ne se produira si vous vivez avec le cluster jaune pendant une demi-heure, lorsque seuls les fragments primaires sont disponibles. <br><br>  <b>Rin√ßage synchronis√©</b> .  Dans ce cas, le n≈ìud se synchronise beaucoup plus rapidement lorsqu'il revient dans le cluster. <br><blockquote>  Avec une lourde charge d'√©criture dans l'index ou de r√©cup√©ration, vous pouvez r√©duire le nombre de r√©pliques. </blockquote>  Si vous t√©l√©chargez une grande quantit√© de donn√©es, par exemple la charge de pointe, vous pouvez d√©sactiver les fragments et donner ensuite une commande √† Elasticsearch pour les cr√©er lorsque la charge est d√©j√† moindre. <br><br>  Voici quelques commandes que j'aime utiliser: <br><br><ul><li>  <code>GET _cat/thread_pool?v</code> - vous permet de voir <code>thread_pool</code> sur chaque n≈ìud: ce qui est chaud maintenant, quelles sont les files d'attente d'√©criture et de lecture. <br></li><li>  <code>GET _cat/recovery/?active_only=true</code> - quels index sont d√©ploy√©s o√π, o√π la r√©cup√©ration a lieu. <br></li><li>  <code>GET _cluster/allocation/explain</code> - sous une forme humaine pratique pourquoi et quels index ou r√©pliques n'ont pas √©t√© allou√©s. <br></li></ul><br>  Pour la surveillance, nous utilisons Grafana. <br><br><img src="https://habrastorage.org/webt/zg/wz/pm/zgwzpmdlzs580aaf88kv-l_rnh4.jpeg"><br><br>  <b>Vincent van Hollebeke</b> offre un excellent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">exportateur</a> et un esprit d'√©quipe Grafana, qui vous permet de voir visuellement l'√©tat du cluster et tous ses principaux param√®tres.  Nous l'avons ajout√© √† notre image Docker et √† toutes les m√©triques lors du d√©ploiement √† partir de notre box. <br><br><h2>  Conclusions de journalisation </h2><br>  Les journaux doivent √™tre: <br><br><ul><li>  <b>centralis√©</b> - un point d'entr√©e unique pour les d√©veloppeurs; <br></li><li>  <b>disponible</b> - la possibilit√© de rechercher rapidement; <br></li><li>  <b>structur√©</b> - pour l'extraction rapide et pratique d'informations pr√©cieuses; <br></li><li>  <b>en corr√©lation</b> - non seulement entre eux, mais aussi avec d'autres mesures et syst√®mes que vous utilisez. <br></li></ul><br>  Le concours su√©dois <b>Melodifestivalen</b> a r√©cemment eu lieu.  Ceci est une s√©lection de repr√©sentants de la Su√®de pour l'Eurovision.  Avant la comp√©tition, notre service d'assistance nous a contact√©s: ¬´Maintenant, en Su√®de, il y aura une grosse charge.  Le trafic est assez sensible et nous voulons corr√©ler certaines donn√©es.  Vous avez des donn√©es dans les journaux qui manquent sur le tableau de bord Grafana.  Nous avons des mesures qui peuvent √™tre extraites de Prometheus, mais nous avons besoin de donn√©es sur des demandes d'identification sp√©cifiques. ¬ª <br><br>  Ils ont ajout√© Elasticsearch comme source de Grafana et ont pu corr√©ler ces donn√©es, r√©soudre le probl√®me et obtenir de bons r√©sultats assez rapidement. <br><blockquote>  Il est beaucoup plus facile d'exploiter vos propres solutions. </blockquote>  Maintenant, au lieu des 10 clusters Graylog qui ont fonctionn√© pour cette solution, nous avons plusieurs services.  Ce sont 10 centres de donn√©es, mais nous n'avons m√™me pas une √©quipe d√©di√©e et des personnes qui les servent.  Plusieurs personnes y ont travaill√© et ont chang√© quelque chose au besoin.  Cette petite √©quipe est parfaitement int√©gr√©e √† notre infrastructure - le d√©ploiement et l'entretien sont plus faciles et moins chers. <br><blockquote>  S√©parez les cas et utilisez les outils appropri√©s. </blockquote>  Ce sont des outils distincts pour la journalisation, le suivi et la surveillance.  Il n'y a pas d '¬´instrument d'or¬ª qui couvrira tous vos besoins. <br><br>  Pour comprendre quel outil est n√©cessaire, ce qu'il faut surveiller, quels journaux utiliser, quelles exigences de journal, vous devez absolument vous r√©f√©rer √† <b>SLI / SLO</b> - Indicateur de niveau de service / objectif de niveau de service.  Vous devez savoir ce qui est important pour vos clients et votre entreprise, quels indicateurs ils examinent. <br><br><blockquote>  Une semaine plus tard, SKOLKOVO h√©bergera <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HighLoad ++ 2019</a> .  Dans la soir√©e du 7 novembre, Ivan Letenko <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vous racontera</a> comment il vit avec Redis sur la prod, et au total il <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">y a</a> 150 reportages au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">programme</a> sur une vari√©t√© de sujets. <br><br>  Si vous rencontrez des probl√®mes pour visiter HighLoad ++ 2019 en direct, nous avons de bonnes nouvelles.  Cette ann√©e, la conf√©rence se tiendra dans trois villes √† la fois - √† Moscou, Novossibirsk et Saint-P√©tersbourg.  En m√™me temps.  Comment cela se fera et comment s'y rendre - d√©couvrez-le sur une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">page promotionnelle</a> distincte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de l'</a> √©v√©nement. </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr473946/">https://habr.com/ru/post/fr473946/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr473932/index.html">Comment fonctionne une IA de jeu hybride et quels sont ses avantages</a></li>
<li><a href="../fr473936/index.html">Performances audio interactives - Une nouvelle √®re de jeux d'assistant vocal</a></li>
<li><a href="../fr473938/index.html">Stockez universellement les param√®tres d'application via IConfiguration</a></li>
<li><a href="../fr473940/index.html">Test de r√©sistance: nanom√©canique de la nacre noble nacre</a></li>
<li><a href="../fr473944/index.html">Conseils du cr√©ateur de RimWorld: distorsions cognitives pour pr√©dire un fan du jeu</a></li>
<li><a href="../fr473948/index.html">Operon: acc√©l√®re les performances d'Ansible</a></li>
<li><a href="../fr473950/index.html">Mettre en ≈ìuvre, √©voluer: l'exp√©rience de l'utilisation des autotests dans VTB</a></li>
<li><a href="../fr473952/index.html">Comme j'ai √©crit l'IA pour la strat√©gie au tour par tour</a></li>
<li><a href="../fr473956/index.html">Informations secr√®tes d'une compagnie de t√©l√©phone de trafiquants de drogue</a></li>
<li><a href="../fr473958/index.html">Le japonais de NICT a pr√©sent√© un cluster de fibres de travail avec une bande passante de 1 Pbit / s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>