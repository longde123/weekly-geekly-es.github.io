<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📦 🐝 👩🏿‍✈️ Kubernetes HA集群与容器化 还是没有docker的生活？ ✍🏻 ⚠️ 🕑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="使用容器部署kubernetes HA 



 尊敬的哈伯读者，下午好！ 2018年5月24日，在Kubernetes官方博客上发表了一篇名为Kubernetes Containered Integration Goes GA的文章，其中指出Containerd与Kubernetes的集成已准备就...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes HA集群与容器化 还是没有docker的生活？</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd"> 使用容器部署kubernetes HA </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br> 尊敬的哈伯读者，下午好！  2018年5月24日，在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes</a>官方博客上发表了一篇名为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes Containered Integration Goes GA的</a>文章，其中指出Containerd与Kubernetes的集成已准备就绪。 另外，Flant公司的家伙<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在</a>博客<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">上</a>发布了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文章翻译成俄语的内容</a> ，并增加了一些澄清。 阅读<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">github上</a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">项目文档</a>后，我决定尝试将容器<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">包装</a>在“我自己的皮肤”上。 </p><br><p> 我公司有几个项目处于“还远远没有生产”阶段。 因此它们将成为我们的实验； 对于他们来说，我们决定尝试使用容器化部署Kubernetes的故障转移群集，并查看没有docker的生命。 </p><br><p> 如果您有兴趣了解我们的工作方式和发展成果，欢迎与我们联系。 </p><a name="habracut"></a><br><p>  <b>示意图和部署说明</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p> 像往常一样，在部署集群时，（我在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">上一篇文章中</a>谈到了这一点。 </p><div class="spoiler">  <b class="spoiler_title">keepalived-适用于Linux的VRRP（虚拟路由器冗余协议）的实现</b> <div class="spoiler_text">  Keepalived创建一个虚拟IP（VIRTIP），该虚拟IP“指向”（创建子接口）三个主机之一的IP。  keepalived守护程序监视计算机的运行状况，并在发生故障时，根据在每台服务器上配置keepalived时指定的“权重”，通过将VIRTIP切换到另一台服务器的IP，将故障服务器从活动服务器列表中排除。 </div></div><br><p>  Keepalived守护程序通过VRRP进行通信，互相发送消息到地址224.0.0.18。 </p><br><p> 如果邻居还没有发送消息，则在这段时间之后，他被认为已经死亡。 崩溃的服务器开始将其消息发送到网络后，一切都恢复原状 </p><br><p> 我们在kubernetes节点上使用API​​服务器配置工作，如下所示。 </p><br><p> 安装集群后，配置kube-proxy，将端口从6443更改为16443（以下详细信息）。 在每个主节点上，都部署了nginx，它充当负载平衡器，侦听端口16443，并在端口6443上对所有三个主节点进行上游（详细信息如下）。 </p><br><p> 通过使用keepalived以及使用nginx，此方案已提高了容错能力，从而实现了向导上API服务器之间的平衡。 <br></p><br><p> 在上一篇文章中，我描述了在docker中部署nginx和etcd的方法。 但是在这种情况下，我们没有docker，因此nginx和etcd将在masternodes上本地工作。 </p><br><p> 从理论上讲，可以使用容器化部署nginx和etcd，但是如果出现任何问题，这种方法会使诊断变得复杂，因此我们决定不进行本地试验和运行。 </p><br><p>  <b>部署服务器的说明：</b> </p><br><table><thead><tr><th> 名称 </th><th> 知识产权 </th><th> 服务项目 </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ </td></tr><tr><td>  kube-master01 </td><td>  172.26.133.161 </td><td>  kubeadm，kubelet，kubectl，etcd，容器化，nginx，keepalived </td></tr><tr><td>  kube-master02 </td><td>  172.26.133.162 </td><td>  kubeadm，kubelet，kubectl，etcd，容器化，nginx，keepalived </td></tr><tr><td>  kube-master03 </td><td>  172.26.133.163 </td><td>  kubeadm，kubelet，kubectl，etcd，容器化，nginx，keepalived </td></tr><tr><td>  kube-node01 </td><td>  172.26.133.164 </td><td>  kubeadm，kubelet，kubectl，集装箱 </td></tr><tr><td>  kube-node02 </td><td>  172.26.133.165 </td><td>  kubeadm，kubelet，kubectl，集装箱 </td></tr><tr><td>  kube-node03 </td><td>  172.26.133.166 </td><td>  kubeadm，kubelet，kubectl，集装箱 </td></tr></tbody></table><br><p>  <b>安装kubeadm，kubelet，kubectl和相关软件包</b> </p><br><p> 所有命令均从root运行 </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>安装conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>配置容器配置</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p> 添加到文件： </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p> 我们开始conteinerd，我们检查一切正常 </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service └─10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>安装并运行etcd</b> </p><br><p> 重要说明，我安装了kubernetes集群版本1.10。 几天之后，在撰写本文时，发布了1.11版，如果安装了1.11版，则设置变量ETCD_VERSION =“ v3.2.17”，如果设置1.10，则设置ETCD_VERSION =“ v3.1.12”。 </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p> 从gitahab复制配置。 </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p> 在配置文件中配置变量。 </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">create-config.sh文件变量的描述</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p> 每个节点的本地计算机上的设置（每个节点都有自己的） <br>  <b>K8SHA_IPLOCAL-</b>在其上配置脚本的节点的IP地址 <br>  <b>K8SHA_ETCDNAME</b> -ETCD集群中的本地计算机名称 <br>  <b>K8SHA_KA_STATE-保持</b>角色 一个MASTER节点，所有其他BACKUP。 <br>  <b>K8SHA_KA_PRIO-保持</b>优先级，主节点的剩余101和100为102。当编号为102的节点掉下时，编号为101的节点代替。 <br>  <b>K8SHA_KA_INTF-保持</b>网络接口。  keepalived将监听的接口的名称。 </p><br><p> 所有主节点的常规设置都相同： </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160-群集的虚拟IP <br>  <b>K8SHA_IP1 ... K8SHA_IP3-</b>主站的<b>IP</b>地址 <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3-主节点的</b>主机名。 重要的一点是，通过这些名称kubeadm将生成证书。 <br>  <b>K8SHA_KA_AUTH-保持</b>密码。 您可以指定任何 <br>  <b>K8SHA_TOKEN-</b>集群令牌。 可以使用<b>kubeadm令牌generate</b>命令<b>生成</b> <br>  <b>K8SHA_CIDR-</b>炉膛的子网地址。 我使用法兰绒，所以CIDR为0.244.0.0/16。 确保屏幕-在配置中应为K8SHA_CIDR = 10.244.0.0 \ / 16 </p></div></div><br><p> 运行将配置Nginx，Keepalived，etcd和kubeadmin的脚本 </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p> 我们开始etcd。 </p><br>  etcd我没有tls就解除了。 如果需要tls，则在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubernetes官方文档</a>中会详细说明如何为etcd生成证书。 <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p> 状态检查 </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p> 如果一切正常，请继续执行下一步。 </p><br><p>  <b>配置kubeadmin</b> <br> 如果您使用的是kubeadm 1.11版，则可以跳过此步骤 <br> 为了使kybernetes不能与docker开始工作，而是与容器化工作，请配置kubeadmin配置 </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  [服务]之后，在块中添加一行 </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">整个配置应如下所示：</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p> 如果您安装版本1.11并想尝试CoreDNS而不是kube-dns并测试动态配置，请在kubeadm-init.yaml配置文件中取消注释以下块： </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p> 重新启动kubelet </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>初始化第一个向导</b> </p><br><p> 在启动kubeadm之前，您需要重新启动keepalived并检查其状态。 </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ├─4590 /usr/sbin/keepalived ├─4591 /usr/sbin/keepalived ├─4593 /usr/sbin/keepalived ├─5222 /usr/sbin/keepalived ├─5223 sh -c /etc/keepalived/check_apiserver.sh ├─5224 /bin/bash /etc/keepalived/check_apiserver.sh └─5231 sleep 5</code> </pre> <br><p> 检查是否VIRTIP ping </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p> 之后，运行kubeadmin。 确保包括--skip-preflight-checks行。 默认情况下，Kubeadmin搜索docker且不跳过检查将失败并显示错误。 </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  kubeadm工作后，保存生成的行。 需要将工作节点输入集群。 </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p> 接下来，指示admin.conf文件的存储位置 <br> 如果我们以root身份工作，则： </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p> 对于简单的用户，请按照屏幕上的说明进行操作。 </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p> 将另外两个向导添加到群集。 为此，请将证书从kube-master01复制到kube-master02，将kube-master03复制到/ etc / kubernetes /目录。 为此，我为root配置了ssh访问，并在复制文件后将设置返回了。 </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p> 复制到kube-master02和kube-master03后，运行。 </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>安装CIDR法兰绒</b> </p><br><p> 在kube-master01上执行 </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p> 当前版本的flanel可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubernetes文档中</a>找到。 </p><br><p> 我们等到所有容器创建完毕。 </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>我们将kube-dn复制到所有三个主机</b> </p><br><p> 在kube-master01上执行 </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>安装和配置Nginx</b> </p><br><p> 在每个主节点上，安装nginx作为Kubernetes API的平衡器 <br> 我在debian上拥有所有集群机器。 在nginx软件包中，流模块不支持，因此添加nginx存储库并从nginx`a存储库安装。 如果您使用的是其他操作系统，请参阅<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">nginx文档</a> 。 </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p> 创建nginx配置（如果尚未创建） </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p> 用户nginx; <br>  worker_processes自动； </p><br><p>  error_log /var/log/nginx/error.log警告； <br>  pid /var/run/nginx.pid; </p><br><p> 事件{ <br>  worker_connections 1024; <br>  } </p><br><p>  http { <br> 包括/etc/nginx/mime.types; <br>  default_type应用程序/八位字节流； </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  } </p><br><p> 流{ <br> 上游apiserver { <br> 服务器172.26.133.161:6443权重= 5 max_fails = 3 fail_timeout = 30s; <br> 服务器172.26.133.162:6443权重= 5 max_fails = 3 fail_timeout = 30s; <br> 服务器172.26.133.163:6443权重= 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  } </p></div></div><br><p> 我们检查一切正常，然后应用配置 </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ● nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ├─22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf └─22134 nginx: worker process</code> </pre> <br><p> 测试平衡器 </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>配置kube-proxy以与平衡器一起使用</b> </p><br><p> 配置平衡器后，在kubernetes设置中编辑端口。 </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p> 将服务器设置更改为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://172.26.133.160:16443</a> <br> 接下来，您需要配置kube-proxy以使用新端口 </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p> 我们删除所有Pod，将其删除后会自动使用新设置重新创建 </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>将工作节点添加到集群</b> </p><br><p> 在每个根注释上，执行kubeadm生成的命令 </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p> 如果该行“丢失”，则需要生成一个新的 </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p> 在/etc/kubernetes/bootstrap-kubelet.conf和/etc/kubernetes/kubelet.conf文件中的工作节点上 <br> 服务器变量值到我们的virtip </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p> 并重启容器化和kubernetes </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>仪表板安装</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p> 创建具有管理员权限的用户： </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p> 我们获得了进入令牌： </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p> 通过VIRTIP上的NodePort配置仪表板访问 </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p> 我们将类型：ClusterIP的值替换为类型：NodePort，并在端口部分：添加nodePort的值：30000（或您要访问面板的端口在30000至32000的范围内）： </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p> 该面板现在位于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https：// VIRTIP：30000</a> </p><br><p>  <b>堆子</b> </p><br><p> 接下来，安装Heapster，这是一种用于获取群集组件指标的工具。 </p><br><p> 安装方式： </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>结论</b> </p><br><p> 使用容器处理时，我没有发现任何特殊问题。 移除部署后，一旦出现炉膛故障，情况将难以理解。  Kubernetes认为under被删除了，但是under变成了一个奇特的“僵尸”。它仍然存在于节点上，但处于扩展状态。 </p><br><p> 我相信Containerd更适合作为kubernetes的容器运行时。 将来，很可能作为在Kubernetes中启动微服务的环境，有可能并且有必要使用面向不同任务，项目等的不同环境。 </p><br><p> 该项目发展很快。 阿里云已经开始积极使用conatinerd，并强调这是运行容器的理想环境。 </p><br><p> 根据开发人员的说法，谷歌云平台Kubernetes中容器的集成现在等同于Docker集成。 </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">crictl控制台实用程序的一个很好的例子</a> 。 我还将从创建的集群中给出一些示例： </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  Docker CLI缺少Kubernetes的基本概念，例如pod和命名空间，而crictl支持这些概念 </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p> 如有必要，我们可以使用常用格式查看容器，例如docker </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p> 我们可以看到节点上的图像 </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p> 事实证明，没有码头工人的生活是:) </p><br><p> 现在谈论错误和故障还为时过早，该集群已经与我们合作了大约一个星期。 在不久的将来，测试将被转移到其中，如果成功，则很可能是其中一个项目的开发人员。 为此，有一个想法是撰写涵盖DevOps流程的一系列文章，例如：创建集群，设置入口控制器并将其移动到单独的集群节点，自动化映像组装，检查映像的漏洞，部署等。 同时，我们将研究工作的稳定性，寻找错误并开发新产品。 </p><br><p> 另外，本手册适用于通过<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">docker</a>部署故障转移群集，您只需要根据<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes官方文档中</a>的说明安装<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">docker，</a>并跳过安装容器和配置kubeadm配置的步骤。 </p><br><p> 或者，您可以将容器化的容器和docker同时放置在同一主机上，并且，正如开发人员保证的那样，它们将完美地协同工作。 容器化是konbernetes容器启动器的启动环境，而docker就像docker））） </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br> 容器式存储库有一个<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">有趣的手册，</a>用于设置单向导集群。 但是，用我的双手“举起”系统以更详细地了解每个组件的配置并了解其工作原理对我来说更有趣。 <br><p> 也许有一天，我的手会伸手去拿，而且我会写我的剧本来部署HA集群，因为在过去的六个月中，我已经部署了十几个集群，可能是时候使该过程自动化了。 </p><br><p> 另外，在撰写本文时，发布了kubernetes 1.11版。 您可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Flant博客</a>或<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方kubernetes博客中</a>了解主要更改。 我们将测试群集更新到版本1.11，并用CoreDNS替换了kube-dns。 此外，我们还包括DynamicKubeletConfig函数，用于测试配置的动态更新功能。 </p><br><p> 所用材料： </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes容器集成进入GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes容器集成取代Docker并投入生产</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">容器化的github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes文档</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">NGINX文档</a> </li></ul><br><p> 感谢您阅读到最后。 </p><br><p> 由于在RuNet中关于kubernetes的信息（尤其是在实际条件下运行的集群上）非常稀缺，因此欢迎提供不准确的迹象，以及对一般集群部署方案的评论。 我将尝试考虑它们并进行适当的更正。 我随时准备在评论，有关githab以及我个人资料中指示的任何社交网络中回答问题。 </p><br><p> 真诚的，尤金。 </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN415601/">https://habr.com/ru/post/zh-CN415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN415591/index.html">苹果工程师陷阱MacBook Pro键盘陷阱</a></li>
<li><a href="../zh-CN415593/index.html">6年后，传奇的崩溃发行版Hiren's BootCD的新版本</a></li>
<li><a href="../zh-CN415595/index.html">以SLOT游戏为例，提高玩家保留率的方法：第1部分</a></li>
<li><a href="../zh-CN415597/index.html">Postfix-在没有本地主机或邮件服务器的情况下以新方式进行的amavisd-new</a></li>
<li><a href="../zh-CN415599/index.html">印度也想得到氦3</a></li>
<li><a href="../zh-CN415605/index.html">我们如何使用Exadata保存卡处理</a></li>
<li><a href="../zh-CN415611/index.html">PKI：GCrypt和KSBA库是对OpenSSL的替代，具有对俄罗斯密码的支持。 延续性</a></li>
<li><a href="../zh-CN415613/index.html">为什么不应该购买LED吊灯</a></li>
<li><a href="../zh-CN415615/index.html">通过Swift 3上的iOS中的API与服务器进行交互。第2部分</a></li>
<li><a href="../zh-CN415617/index.html">在Delphi中使用InternetPCools FPC库</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>