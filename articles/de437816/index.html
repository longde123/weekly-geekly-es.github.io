<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔌 ➖ 👨🏼‍🎤 MPLS ist überall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur? 🙏🏿 🙄 ⭕️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Beitrag erstellt von: Alexander Virilin xscrew - Autor, Leiter des Netzwerkinfrastrukturdienstes, Leonid Klyuyev - Herausgeber 

 Wir machen Sie weite...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>MPLS ist überall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437816/"> <sup><i>Beitrag erstellt von: Alexander Virilin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">xscrew</a> - Autor, Leiter des Netzwerkinfrastrukturdienstes, Leonid Klyuyev - Herausgeber</i></sup> <br><br><img src="https://habrastorage.org/webt/td/uq/e-/tduqe-fvjvebbot1h11mdm0ri9g.png" align="right" width="400">  Wir machen Sie weiterhin mit der internen Struktur von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex.Cloud vertraut</a> .  Heute werden wir über Netzwerke sprechen - wir werden Ihnen erklären, wie die Netzwerkinfrastruktur funktioniert, warum sie das für Rechenzentren unpopuläre MPLS-Paradigma verwendet, welche anderen komplexen Entscheidungen wir beim Aufbau eines Cloud-Netzwerks treffen mussten, wie wir es verwalten und welche Art von Überwachung wir verwenden. <br><br>  Das Netzwerk in der Cloud besteht aus drei Schichten.  Die unterste Schicht ist die bereits erwähnte Infrastruktur.  Dies ist ein physisches „eisernes“ Netzwerk innerhalb von Rechenzentren, zwischen Rechenzentren und an Orten, an denen Verbindungen zu externen Netzwerken bestehen.  Ein virtuelles Netzwerk wird auf der Netzwerkinfrastruktur aufgebaut, und Netzwerkdienste werden auf dem virtuellen Netzwerk aufgebaut.  Diese Struktur ist nicht monolithisch: Die Schichten überschneiden sich, das virtuelle Netzwerk und die Netzwerkdienste interagieren direkt mit der Netzwerkinfrastruktur.  Da das virtuelle Netzwerk häufig als Overlay bezeichnet wird, wird es normalerweise als Unterlage der Netzwerkinfrastruktur bezeichnet. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/lr/c_/kr/lrc_krqlbldrs_spninjomwdekm.png"><br><br>  Jetzt befindet sich die Cloud-Infrastruktur in der Zentralregion Russlands und umfasst drei Zugangszonen, dh drei geografisch verteilte unabhängige Rechenzentren.  Unabhängig - unabhängig voneinander im Zusammenhang mit Netzen, technischen und elektrischen Systemen usw. <br><br>  Über die Eigenschaften.  Die geografische Lage der Rechenzentren ist so, dass die Umlaufzeit (RTT) der Umlaufzeit zwischen ihnen immer 6–7 ms beträgt.  Die Gesamtkapazität der Kanäle hat bereits 10 Terabit überschritten und wächst ständig, da Yandex über ein eigenes Glasfasernetz zwischen den Zonen verfügt.  Da wir keine Kommunikationskanäle leasen, können wir die Kapazität des Streifens zwischen den DCs schnell erhöhen: Jeder von ihnen verwendet Spektralmultiplexgeräte. <br><br>  Hier ist die schematischste Darstellung der Zonen: <br><br><img src="https://habrastorage.org/webt/iw/tz/11/iwtz11yihyj7beyxar1pefzif-4.png"><br><br>  Die Realität sieht wiederum etwas anders aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/bf/fp/aibffptzbz0jtemoiczeiledcdm.png" width="500"></div><br>  Hier ist das aktuelle Yandex-Backbone-Netzwerk in der Region.  Alle Yandex-Dienste arbeiten darüber hinaus, ein Teil des Netzwerks wird von der Cloud verwendet.  (Dies ist ein Bild für den internen Gebrauch, daher werden Dienstinformationen absichtlich ausgeblendet. Dennoch ist es möglich, die Anzahl der Knoten und Verbindungen zu schätzen.) Die Entscheidung für die Verwendung des Backbone-Netzwerks war logisch: Wir konnten nichts erfinden, sondern die aktuelle Infrastruktur wiederverwenden - die über die Jahre der Entwicklung „gelitten“ hat. <br><br>  Was ist der Unterschied zwischen dem ersten und dem zweiten Bild?  Erstens sind Zugangszonen nicht direkt miteinander verbunden: Technische Standorte befinden sich zwischen ihnen.  Die Sites enthalten keine Serverausrüstung - nur Netzwerkgeräte, um die Konnektivität sicherzustellen, werden auf ihnen platziert.  Präsenzpunkte, an denen Yandex und Cloud mit der Außenwelt verbunden sind, sind mit technischen Standorten verbunden.  Alle Präsenzpunkte arbeiten für die gesamte Region.  Übrigens ist zu beachten, dass unter dem Gesichtspunkt des externen Zugriffs aus dem Internet alle Cloud-Zugriffszonen gleichwertig sind.  Mit anderen Worten, sie bieten die gleiche Konnektivität - das heißt die gleiche Geschwindigkeit und den gleichen Durchsatz sowie gleich niedrige Latenzen. <br><br>  Darüber hinaus gibt es Geräte an den Präsenzpunkten, zu denen Kunden - wenn lokale Ressourcen vorhanden sind und die lokale Infrastruktur mit Cloud-Einrichtungen erweitert werden soll - über einen garantierten Kanal eine Verbindung herstellen können.  Dies kann mit Hilfe von Partnern oder auf eigene Faust erfolgen. <br><br>  Das Kernnetz wird von der Cloud als MPLS-Transport verwendet. <br><br><h2>  MPLS </h2><br><img src="https://habrastorage.org/webt/ul/kj/rv/ulkjrvkt7kk2igkl_sbjzivjfxk.png"><br><br>  Multiprotokoll-Label-Switching ist eine in unserer Branche weit verbreitete Technologie.  Wenn beispielsweise ein Paket zwischen Zugriffszonen oder zwischen einer Zugriffszone und dem Internet übertragen wird, achten Transitgeräte nur auf das oberste Etikett und „denken“ nicht darüber nach, was sich darunter befindet.  Auf diese Weise können Sie mit MPLS die Cloud-Komplexität vor der Transportschicht verbergen.  Im Allgemeinen lieben wir in der Cloud MPLS sehr.  Wir haben es sogar zu einem Teil der unteren Ebene gemacht und es direkt in der Vermittlungsfabrik im Rechenzentrum verwendet: <br><br><img src="https://habrastorage.org/webt/k-/iy/hg/k-iyhg3ru8bkto7rqrawug0ivra.png"><br><br>  (Tatsächlich gibt es viele parallele Verbindungen zwischen Blattschaltern und Stacheln.) <br><br><h4>  Warum MPLS? </h4><br>  Es stimmt, MPLS ist in Rechenzentrumsnetzwerken keineswegs häufig anzutreffen.  Oft werden völlig unterschiedliche Technologien eingesetzt. <br><br>  Wir verwenden MPLS aus mehreren Gründen.  Zunächst fanden wir es zweckmäßig, die Technologien der Steuerebene und der Datenebene zu vereinheitlichen.  Das heißt, anstelle einiger Protokolle im Rechenzentrumsnetzwerk andere Protokolle im Kernnetzwerk und die Verbindung dieser Protokolle - eine einzelne MPLS.  So haben wir den technologischen Stack vereinheitlicht und die Komplexität des Netzwerks reduziert. <br><br>  Zweitens verwenden wir in der Cloud verschiedene Netzwerkgeräte wie Cloud Gateway und Network Load Balancer.  Sie müssen miteinander kommunizieren, Datenverkehr ins Internet senden und umgekehrt.  Diese Netzwerk-Appliances können mit zunehmender Last horizontal skaliert werden. Da die Cloud nach dem Hyperkonvergenz-Modell erstellt wurde, können sie aus Sicht des Netzwerks im Rechenzentrum, dh in einem gemeinsamen Ressourcenpool, an absolut jedem Ort gestartet werden. <br><br>  Somit können diese Appliances hinter jedem Port des Rack-Switches starten, an dem sich der Server befindet, und über MPLS mit dem Rest der Infrastruktur kommunizieren.  Das einzige Problem beim Aufbau einer solchen Architektur war der Alarm. <br><br><h2>  Alarm </h2><br>  Der klassische MPLS-Protokollstapel ist recht komplex.  Dies ist übrigens einer der Gründe für die Nichtverbreitung von MPLS in Rechenzentrumsnetzwerken. <br><br>  Wir haben wiederum weder IGP (Interior Gateway Protocol) noch LDP (Label Distribution Protocol) oder andere Label Distribution-Protokolle verwendet.  Es wird nur BGP-Label-Unicast (Border Gateway Protocol) verwendet.  Jede Appliance, die beispielsweise als virtuelle Maschine ausgeführt wird, erstellt eine BGP-Sitzung vor dem Leaf-Switch für die Rackmontage. <br><br><img src="https://habrastorage.org/webt/z7/-o/03/z7-o03kr2x9-v-yuawoom5xegq4.png"><br><br>  Eine BGP-Sitzung wird an einer bekannten Adresse erstellt.  Es ist nicht erforderlich, den Switch automatisch so zu konfigurieren, dass jede Appliance ausgeführt wird.  Alle Switches sind vorkonfiguriert und konsistent. <br><br>  Innerhalb einer BGP-Sitzung sendet jede Appliance ihren eigenen Loopback und empfängt Loopbacks der übrigen Geräte, mit denen sie Datenverkehr austauschen muss.  Beispiele für solche Geräte sind verschiedene Arten von Routenreflektoren, Grenzroutern und anderen Geräten.  Infolgedessen werden auf den Geräten Informationen darüber angezeigt, wie Sie sich gegenseitig erreichen können.  Vom Cloud-Gateway über den Leaf-Switch, den Spine-Switch und das Netzwerk bis zum Border-Router wird ein Label-Switch-Pfad erstellt.  Switches sind L3-Switches, die sich wie ein Label Switch Router verhalten und nicht wissen, wie komplex sie sind. <br><br>  MPLS auf allen Ebenen unseres Netzwerks hat es uns unter anderem ermöglicht, das Konzept von Eat your own dogfood anzuwenden. <br><br><h2>  Iss dein eigenes Hundefutter </h2><br>  Aus Netzwerksicht impliziert dieses Konzept, dass wir in derselben Infrastruktur leben, die wir dem Benutzer zur Verfügung stellen.  Hier sind Diagramme von Racks in barrierefreien Bereichen: <br><br><img src="https://habrastorage.org/webt/tz/zq/o0/tzzqo08b1pv-whl9mqu9e_xnups.png"><br><br>  Cloud-Host nimmt die Last vom Benutzer, enthält seine virtuellen Maschinen.  Im wahrsten Sinne des Wortes kann ein benachbarter Host in einem Rack die Infrastrukturlast aus Sicht des Netzwerks tragen, einschließlich Routenreflektoren, Verwaltung, Überwachung von Servern usw. <br><br>  Warum wurde das gemacht?  Es bestand die Versuchung, Routenreflektoren und alle Infrastrukturelemente in einem separaten fehlertoleranten Segment zu betreiben.  Wenn das Benutzersegment irgendwo im Rechenzentrum ausgefallen wäre, würden die Infrastrukturserver weiterhin die gesamte Netzwerkinfrastruktur verwalten.  Dieser Ansatz erschien uns jedoch bösartig. Wenn wir unserer eigenen Infrastruktur nicht vertrauen, wie können wir sie dann unseren Kunden zur Verfügung stellen?  Schließlich arbeiten absolut die gesamte Cloud, alle virtuellen Netzwerke, Benutzer- und Cloud-Dienste darüber. <br><br>  Aus diesem Grund haben wir ein separates Segment aufgegeben.  Unsere Infrastrukturelemente werden in derselben Netzwerktopologie und Netzwerkkonnektivität ausgeführt.  Natürlich laufen sie in einer dreifachen Instanz - genau wie unsere Kunden ihre Dienste in der Cloud starten. <br><br><h2>  IP / MPLS-Fabrik </h2><br>  Hier ist ein Beispieldiagramm einer der Verfügbarkeitszonen: <br><br><img src="https://habrastorage.org/webt/jn/xi/rw/jnxirwsmzj62bwzvldfnrq3n2_4.png"><br><br>  In jeder Verfügbarkeitszone gibt es ungefähr fünf Module und in jedem Modul ungefähr hundert Racks.  Leaf-Rack-Switches, die innerhalb ihres Moduls über die Spine-Ebene verbunden sind, und die Konnektivität zwischen Modulen werden über die Netzwerkverbindung bereitgestellt.  Dies ist die nächste Ebene, die die sogenannten Super-Spines- und Edge-Switches umfasst, die bereits die Zugriffszonen verbinden.  Wir haben L2 absichtlich aufgegeben, wir sprechen nur über L3 IP / MPLS-Konnektivität.  BGP wird zum Verteilen von Routing-Informationen verwendet. <br><br>  Tatsächlich gibt es viel mehr parallele Verbindungen als auf dem Bild.  Eine solch große Anzahl von ECMP-Verbindungen (Equal-Cost Multi-Path) stellt besondere Überwachungsanforderungen.  Darüber hinaus gibt es auf den ersten Blick unerwartete Grenzen in der Ausrüstung - zum Beispiel die Anzahl der ECMP-Gruppen. <br><br><h2>  Serververbindung </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xf/ay/jo/xfayjofymnoqjc-1u1otd0axhgm.png"></div><br>  Aufgrund der hohen Investitionen baut Yandex Services so auf, dass ein Ausfall eines Servers, Server-Racks, Moduls oder sogar eines gesamten Rechenzentrums niemals zu einem vollständigen Stopp des Service führt.  Wenn wir irgendwelche Netzwerkprobleme haben - nehmen wir an, ein Rack-Mount-Switch ist defekt - sehen externe Benutzer dies nie. <br><br>  Yandex.Cloud ist ein Sonderfall.  Wir können dem Kunden nicht vorschreiben, wie er seine eigenen Services aufbauen soll, und wir haben uns entschlossen, diesen möglichen Single Point of Failure auszugleichen.  Daher sind alle Server in der Cloud mit zwei Rack-Mount-Switches verbunden. <br><br>  Wir verwenden auch keine Redundanzprotokolle auf L2-Ebene, sondern verwenden sofort nur L3 mit BGP - wiederum aus Gründen der Protokollvereinheitlichung.  Diese Verbindung bietet jedem Dienst IPv4- und IPv6-Konnektivität: Einige Dienste funktionieren über IPv4, andere über IPv6. <br><br>  Physisch ist jeder Server über zwei 25-Gigabit-Schnittstellen verbunden.  Hier ist ein Foto aus dem Rechenzentrum: <br><br><img src="https://habrastorage.org/webt/jz/sr/xj/jzsrxj39equkvhixj3bjoj5kn6a.png"><br><br>  Hier sehen Sie zwei Rack-Switches mit 100-Gigabit-Ports.  Es sind unterschiedliche Breakout-Kabel sichtbar, die den 100-Gigabit-Port des Switch in 4 Ports mit 25 Gigabit pro Server unterteilen.  Wir nennen diese Kabel "Hydra". <br><br><h2>  Infrastrukturmanagement </h2><br>  Die Cloud-Netzwerkinfrastruktur enthält keine proprietären Verwaltungslösungen: Alle Systeme sind entweder Open Source mit Anpassung für die Cloud oder vollständig selbst geschrieben. <br><br><img src="https://habrastorage.org/webt/-g/if/qu/-gifqu8wgzw3xwehpuy5_ejfapc.png"><br><br>  Wie wird diese Infrastruktur verwaltet?  Es ist in der Cloud nicht so verboten, aber es wird dringend davon abgeraten, zu einem Netzwerkgerät zu gehen und Anpassungen vorzunehmen.  Es gibt den aktuellen Status des Systems, und wir müssen die Änderungen anwenden: zu einem neuen Zielstatus kommen.  Führen Sie ein Skript durch alle Drüsen, ändern Sie etwas in der Konfiguration - Sie sollten dies nicht tun.  Stattdessen nehmen wir Änderungen an den Vorlagen vor, an einer einzigen Quelle des Wahrheitssystems, und übertragen unsere Änderungen auf das Versionskontrollsystem.  Dies ist sehr praktisch, da Sie jederzeit einen Rollback durchführen, den Verlauf anzeigen, herausfinden können, wer für das Commit verantwortlich ist usw. <br><br>  Wenn wir die Änderungen vorgenommen haben, werden Konfigurationen generiert und in der Labortesttopologie bereitgestellt.  Aus Netzwerksicht ist dies eine kleine Cloud, die die gesamte vorhandene Produktion vollständig wiederholt.  Wir werden sofort sehen, ob die gewünschten Änderungen etwas bewirken: erstens durch Überwachung und zweitens durch Feedback unserer internen Benutzer. <br><br>  Wenn die Überwachung besagt, dass alles ruhig ist, fahren wir mit der Einführung fort - wenden die Änderung jedoch nur auf einen Teil der Topologie an (zwei oder mehr Zugänglichkeiten haben aus demselben Grund nicht das Recht, zu brechen).  Darüber hinaus überwachen wir die Überwachung weiterhin genau.  Dies ist ein ziemlich komplizierter Prozess, über den wir weiter unten sprechen werden. <br><br>  Nachdem wir sichergestellt haben, dass alles in Ordnung ist, wenden wir die Änderung auf die gesamte Produktion an.  Sie können jederzeit einen Rollback durchführen und zum vorherigen Status des Netzwerks zurückkehren, um das Problem schnell zu verfolgen und zu beheben. <br><br><h4>  Überwachung </h4><br>  Wir brauchen eine andere Überwachung.  Eine der gefragtesten ist die Überwachung der End-to-End-Konnektivität.  Jeder Server sollte jederzeit in der Lage sein, mit jedem anderen Server zu kommunizieren.  Tatsache ist, dass wir, wenn irgendwo ein Problem auftritt, so früh wie möglich genau herausfinden möchten, wo (dh welche Server Probleme haben, aufeinander zuzugreifen).  Die Gewährleistung einer durchgängigen Konnektivität ist unser Hauptanliegen. <br><br>  Jeder Server listet eine Reihe aller Server auf, mit denen er zu einem bestimmten Zeitpunkt kommunizieren kann.  Der Server nimmt eine zufällige Teilmenge dieser Menge und sendet ICMP-, TCP- und UDP-Pakete an alle ausgewählten Computer.  Dadurch wird geprüft, ob im Netzwerk Verluste auftreten, ob sich die Verzögerung erhöht hat usw. Das gesamte Netzwerk wird innerhalb einer der Zugriffszonen und zwischen diesen "aufgerufen".  Die Ergebnisse werden an ein zentrales System gesendet, das sie für uns visualisiert. <br><br>  So sehen die Ergebnisse aus, wenn nicht alles sehr gut ist: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yd/-1/bs/yd-1bs7mguqfcy-xhdko674rvu0.png"></div><br>  Hier können Sie sehen, zwischen welchen Netzwerksegmenten ein Problem besteht (in diesem Fall A und B) und wo alles in Ordnung ist (A und D).  Hier können bestimmte Server, Rack-Switches, Module und gesamte Verfügbarkeitszonen angezeigt werden.  Wenn eines der oben genannten Probleme das Problem verursacht, wird es in Echtzeit angezeigt. <br><br>  Zusätzlich gibt es eine Ereignisüberwachung.  Wir überwachen alle Verbindungen, Signalpegel auf Transceivern, BGP-Sitzungen usw. genau. Angenommen, drei BGP-Sitzungen werden aus einem Netzwerksegment erstellt, von denen eine nachts unterbrochen wurde.  Wenn wir die Überwachung so einrichten, dass der Ausfall einer BGP-Sitzung für uns nicht kritisch ist und bis zum Morgen warten kann, werden die Netzwerktechniker durch die Überwachung nicht geweckt.  Wenn jedoch die zweite der drei Sitzungen ausfällt, ruft ein Techniker automatisch an. <br><br>  Zusätzlich zur End-to-End- und Ereignisüberwachung verwenden wir eine zentralisierte Sammlung von Protokollen, deren Echtzeitanalyse und nachfolgender Analyse.  Sie können die Korrelationen sehen, Probleme identifizieren und herausfinden, was auf den Netzwerkgeräten passiert ist. <br><br>  Das Überwachungsthema ist groß genug, es gibt einen großen Spielraum für Verbesserungen.  Ich möchte das System zu mehr Automatisierung und wahrer Selbstheilung bringen. <br><br><h2>  Was weiter? </h2><br>  Wir haben viele Pläne.  Es ist notwendig, Steuerungssysteme, Überwachung, IP / MPLS-Fabriken und vieles mehr zu verbessern. <br><br>  Wir suchen auch aktiv nach White-Box-Schaltern.  Dies ist ein fertiges "Eisen" -Gerät, ein Schalter, auf dem Sie Ihre Software rollen können.  Erstens, wenn alles richtig gemacht wurde, ist es möglich, die Switches auf die gleiche Weise wie die Server zu „behandeln“, einen wirklich praktischen CI / CD-Prozess zu erstellen, Konfigurationen schrittweise einzuführen usw. <br><br>  Zweitens ist es bei Problemen besser, eine Gruppe von Ingenieuren und Entwicklern zu behalten, die diese Probleme beheben, als lange auf eine Lösung durch den Anbieter zu warten. <br><br>  Damit alles klappt, wird in zwei Richtungen gearbeitet: <br><br><ul><li>  Wir haben die Komplexität der IP / MPLS-Fabrik erheblich reduziert.  Einerseits ist die Ebene des virtuellen Netzwerks und der daraus resultierenden Automatisierungstools etwas komplizierter geworden.  Andererseits ist das Unterlagennetzwerk selbst einfacher geworden.  Mit anderen Worten, es gibt eine gewisse „Komplexität“, die nicht gespeichert werden kann.  Es kann von einer Ebene zur anderen "geworfen" werden - beispielsweise zwischen Netzwerkebenen oder von der Netzwerkebene zur Anwendungsebene.  Und Sie können diese Komplexität, die wir versuchen, richtig verteilen. </li><li>  Und natürlich stellen wir unsere Tools für die Verwaltung der gesamten Infrastruktur fertig. </li></ul><br>  Dies ist alles, was wir über unsere Netzwerkinfrastruktur sprechen wollten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist ein Link</a> zum Cloud Telegram-Kanal mit Neuigkeiten und Tipps. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437816/">https://habr.com/ru/post/de437816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437806/index.html">EcmaScript 10 - Das diesjährige JavaScript (ES2019)</a></li>
<li><a href="../de437808/index.html">Perf und Flammengraphen</a></li>
<li><a href="../de437810/index.html">Unternehmensrealität</a></li>
<li><a href="../de437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Betas</a></li>
<li><a href="../de437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Beta-Versionen</a></li>
<li><a href="../de437818/index.html">Wir bringen dem Computer bei, Geräusche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen</a></li>
<li><a href="../de437820/index.html">50 Farbtöne Drupal-Sicherheit</a></li>
<li><a href="../de437824/index.html">Universelle 1C-Erweiterung für Google Sheets and Docs - nehmen und verwenden</a></li>
<li><a href="../de437826/index.html">Wie wir die Datenbank von Redis und Riak KV auf PostgreSQL migriert haben. Teil 1: Der Prozess</a></li>
<li><a href="../de437828/index.html">Öffnen Sie das Webinar "SELECT-Abfrageausführungsreihenfolge und Abfrageplan in MS SQL Server".</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>