<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîå ‚ûñ üë®üèº‚Äçüé§ MPLS ist √ºberall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur? üôèüèø üôÑ ‚≠ïÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Beitrag erstellt von: Alexander Virilin xscrew - Autor, Leiter des Netzwerkinfrastrukturdienstes, Leonid Klyuyev - Herausgeber 

 Wir machen Sie weite...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>MPLS ist √ºberall. Wie ist die Yandex.Cloud-Netzwerkinfrastruktur?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437816/"> <sup><i>Beitrag erstellt von: Alexander Virilin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">xscrew</a> - Autor, Leiter des Netzwerkinfrastrukturdienstes, Leonid Klyuyev - Herausgeber</i></sup> <br><br><img src="https://habrastorage.org/webt/td/uq/e-/tduqe-fvjvebbot1h11mdm0ri9g.png" align="right" width="400">  Wir machen Sie weiterhin mit der internen Struktur von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex.Cloud vertraut</a> .  Heute werden wir √ºber Netzwerke sprechen - wir werden Ihnen erkl√§ren, wie die Netzwerkinfrastruktur funktioniert, warum sie das f√ºr Rechenzentren unpopul√§re MPLS-Paradigma verwendet, welche anderen komplexen Entscheidungen wir beim Aufbau eines Cloud-Netzwerks treffen mussten, wie wir es verwalten und welche Art von √úberwachung wir verwenden. <br><br>  Das Netzwerk in der Cloud besteht aus drei Schichten.  Die unterste Schicht ist die bereits erw√§hnte Infrastruktur.  Dies ist ein physisches ‚Äûeisernes‚Äú Netzwerk innerhalb von Rechenzentren, zwischen Rechenzentren und an Orten, an denen Verbindungen zu externen Netzwerken bestehen.  Ein virtuelles Netzwerk wird auf der Netzwerkinfrastruktur aufgebaut, und Netzwerkdienste werden auf dem virtuellen Netzwerk aufgebaut.  Diese Struktur ist nicht monolithisch: Die Schichten √ºberschneiden sich, das virtuelle Netzwerk und die Netzwerkdienste interagieren direkt mit der Netzwerkinfrastruktur.  Da das virtuelle Netzwerk h√§ufig als Overlay bezeichnet wird, wird es normalerweise als Unterlage der Netzwerkinfrastruktur bezeichnet. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/lr/c_/kr/lrc_krqlbldrs_spninjomwdekm.png"><br><br>  Jetzt befindet sich die Cloud-Infrastruktur in der Zentralregion Russlands und umfasst drei Zugangszonen, dh drei geografisch verteilte unabh√§ngige Rechenzentren.  Unabh√§ngig - unabh√§ngig voneinander im Zusammenhang mit Netzen, technischen und elektrischen Systemen usw. <br><br>  √úber die Eigenschaften.  Die geografische Lage der Rechenzentren ist so, dass die Umlaufzeit (RTT) der Umlaufzeit zwischen ihnen immer 6‚Äì7 ms betr√§gt.  Die Gesamtkapazit√§t der Kan√§le hat bereits 10 Terabit √ºberschritten und w√§chst st√§ndig, da Yandex √ºber ein eigenes Glasfasernetz zwischen den Zonen verf√ºgt.  Da wir keine Kommunikationskan√§le leasen, k√∂nnen wir die Kapazit√§t des Streifens zwischen den DCs schnell erh√∂hen: Jeder von ihnen verwendet Spektralmultiplexger√§te. <br><br>  Hier ist die schematischste Darstellung der Zonen: <br><br><img src="https://habrastorage.org/webt/iw/tz/11/iwtz11yihyj7beyxar1pefzif-4.png"><br><br>  Die Realit√§t sieht wiederum etwas anders aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/bf/fp/aibffptzbz0jtemoiczeiledcdm.png" width="500"></div><br>  Hier ist das aktuelle Yandex-Backbone-Netzwerk in der Region.  Alle Yandex-Dienste arbeiten dar√ºber hinaus, ein Teil des Netzwerks wird von der Cloud verwendet.  (Dies ist ein Bild f√ºr den internen Gebrauch, daher werden Dienstinformationen absichtlich ausgeblendet. Dennoch ist es m√∂glich, die Anzahl der Knoten und Verbindungen zu sch√§tzen.) Die Entscheidung f√ºr die Verwendung des Backbone-Netzwerks war logisch: Wir konnten nichts erfinden, sondern die aktuelle Infrastruktur wiederverwenden - die √ºber die Jahre der Entwicklung ‚Äûgelitten‚Äú hat. <br><br>  Was ist der Unterschied zwischen dem ersten und dem zweiten Bild?  Erstens sind Zugangszonen nicht direkt miteinander verbunden: Technische Standorte befinden sich zwischen ihnen.  Die Sites enthalten keine Serverausr√ºstung - nur Netzwerkger√§te, um die Konnektivit√§t sicherzustellen, werden auf ihnen platziert.  Pr√§senzpunkte, an denen Yandex und Cloud mit der Au√üenwelt verbunden sind, sind mit technischen Standorten verbunden.  Alle Pr√§senzpunkte arbeiten f√ºr die gesamte Region.  √úbrigens ist zu beachten, dass unter dem Gesichtspunkt des externen Zugriffs aus dem Internet alle Cloud-Zugriffszonen gleichwertig sind.  Mit anderen Worten, sie bieten die gleiche Konnektivit√§t - das hei√üt die gleiche Geschwindigkeit und den gleichen Durchsatz sowie gleich niedrige Latenzen. <br><br>  Dar√ºber hinaus gibt es Ger√§te an den Pr√§senzpunkten, zu denen Kunden - wenn lokale Ressourcen vorhanden sind und die lokale Infrastruktur mit Cloud-Einrichtungen erweitert werden soll - √ºber einen garantierten Kanal eine Verbindung herstellen k√∂nnen.  Dies kann mit Hilfe von Partnern oder auf eigene Faust erfolgen. <br><br>  Das Kernnetz wird von der Cloud als MPLS-Transport verwendet. <br><br><h2>  MPLS </h2><br><img src="https://habrastorage.org/webt/ul/kj/rv/ulkjrvkt7kk2igkl_sbjzivjfxk.png"><br><br>  Multiprotokoll-Label-Switching ist eine in unserer Branche weit verbreitete Technologie.  Wenn beispielsweise ein Paket zwischen Zugriffszonen oder zwischen einer Zugriffszone und dem Internet √ºbertragen wird, achten Transitger√§te nur auf das oberste Etikett und ‚Äûdenken‚Äú nicht dar√ºber nach, was sich darunter befindet.  Auf diese Weise k√∂nnen Sie mit MPLS die Cloud-Komplexit√§t vor der Transportschicht verbergen.  Im Allgemeinen lieben wir in der Cloud MPLS sehr.  Wir haben es sogar zu einem Teil der unteren Ebene gemacht und es direkt in der Vermittlungsfabrik im Rechenzentrum verwendet: <br><br><img src="https://habrastorage.org/webt/k-/iy/hg/k-iyhg3ru8bkto7rqrawug0ivra.png"><br><br>  (Tats√§chlich gibt es viele parallele Verbindungen zwischen Blattschaltern und Stacheln.) <br><br><h4>  Warum MPLS? </h4><br>  Es stimmt, MPLS ist in Rechenzentrumsnetzwerken keineswegs h√§ufig anzutreffen.  Oft werden v√∂llig unterschiedliche Technologien eingesetzt. <br><br>  Wir verwenden MPLS aus mehreren Gr√ºnden.  Zun√§chst fanden wir es zweckm√§√üig, die Technologien der Steuerebene und der Datenebene zu vereinheitlichen.  Das hei√üt, anstelle einiger Protokolle im Rechenzentrumsnetzwerk andere Protokolle im Kernnetzwerk und die Verbindung dieser Protokolle - eine einzelne MPLS.  So haben wir den technologischen Stack vereinheitlicht und die Komplexit√§t des Netzwerks reduziert. <br><br>  Zweitens verwenden wir in der Cloud verschiedene Netzwerkger√§te wie Cloud Gateway und Network Load Balancer.  Sie m√ºssen miteinander kommunizieren, Datenverkehr ins Internet senden und umgekehrt.  Diese Netzwerk-Appliances k√∂nnen mit zunehmender Last horizontal skaliert werden. Da die Cloud nach dem Hyperkonvergenz-Modell erstellt wurde, k√∂nnen sie aus Sicht des Netzwerks im Rechenzentrum, dh in einem gemeinsamen Ressourcenpool, an absolut jedem Ort gestartet werden. <br><br>  Somit k√∂nnen diese Appliances hinter jedem Port des Rack-Switches starten, an dem sich der Server befindet, und √ºber MPLS mit dem Rest der Infrastruktur kommunizieren.  Das einzige Problem beim Aufbau einer solchen Architektur war der Alarm. <br><br><h2>  Alarm </h2><br>  Der klassische MPLS-Protokollstapel ist recht komplex.  Dies ist √ºbrigens einer der Gr√ºnde f√ºr die Nichtverbreitung von MPLS in Rechenzentrumsnetzwerken. <br><br>  Wir haben wiederum weder IGP (Interior Gateway Protocol) noch LDP (Label Distribution Protocol) oder andere Label Distribution-Protokolle verwendet.  Es wird nur BGP-Label-Unicast (Border Gateway Protocol) verwendet.  Jede Appliance, die beispielsweise als virtuelle Maschine ausgef√ºhrt wird, erstellt eine BGP-Sitzung vor dem Leaf-Switch f√ºr die Rackmontage. <br><br><img src="https://habrastorage.org/webt/z7/-o/03/z7-o03kr2x9-v-yuawoom5xegq4.png"><br><br>  Eine BGP-Sitzung wird an einer bekannten Adresse erstellt.  Es ist nicht erforderlich, den Switch automatisch so zu konfigurieren, dass jede Appliance ausgef√ºhrt wird.  Alle Switches sind vorkonfiguriert und konsistent. <br><br>  Innerhalb einer BGP-Sitzung sendet jede Appliance ihren eigenen Loopback und empf√§ngt Loopbacks der √ºbrigen Ger√§te, mit denen sie Datenverkehr austauschen muss.  Beispiele f√ºr solche Ger√§te sind verschiedene Arten von Routenreflektoren, Grenzroutern und anderen Ger√§ten.  Infolgedessen werden auf den Ger√§ten Informationen dar√ºber angezeigt, wie Sie sich gegenseitig erreichen k√∂nnen.  Vom Cloud-Gateway √ºber den Leaf-Switch, den Spine-Switch und das Netzwerk bis zum Border-Router wird ein Label-Switch-Pfad erstellt.  Switches sind L3-Switches, die sich wie ein Label Switch Router verhalten und nicht wissen, wie komplex sie sind. <br><br>  MPLS auf allen Ebenen unseres Netzwerks hat es uns unter anderem erm√∂glicht, das Konzept von Eat your own dogfood anzuwenden. <br><br><h2>  Iss dein eigenes Hundefutter </h2><br>  Aus Netzwerksicht impliziert dieses Konzept, dass wir in derselben Infrastruktur leben, die wir dem Benutzer zur Verf√ºgung stellen.  Hier sind Diagramme von Racks in barrierefreien Bereichen: <br><br><img src="https://habrastorage.org/webt/tz/zq/o0/tzzqo08b1pv-whl9mqu9e_xnups.png"><br><br>  Cloud-Host nimmt die Last vom Benutzer, enth√§lt seine virtuellen Maschinen.  Im wahrsten Sinne des Wortes kann ein benachbarter Host in einem Rack die Infrastrukturlast aus Sicht des Netzwerks tragen, einschlie√ülich Routenreflektoren, Verwaltung, √úberwachung von Servern usw. <br><br>  Warum wurde das gemacht?  Es bestand die Versuchung, Routenreflektoren und alle Infrastrukturelemente in einem separaten fehlertoleranten Segment zu betreiben.  Wenn das Benutzersegment irgendwo im Rechenzentrum ausgefallen w√§re, w√ºrden die Infrastrukturserver weiterhin die gesamte Netzwerkinfrastruktur verwalten.  Dieser Ansatz erschien uns jedoch b√∂sartig. Wenn wir unserer eigenen Infrastruktur nicht vertrauen, wie k√∂nnen wir sie dann unseren Kunden zur Verf√ºgung stellen?  Schlie√ülich arbeiten absolut die gesamte Cloud, alle virtuellen Netzwerke, Benutzer- und Cloud-Dienste dar√ºber. <br><br>  Aus diesem Grund haben wir ein separates Segment aufgegeben.  Unsere Infrastrukturelemente werden in derselben Netzwerktopologie und Netzwerkkonnektivit√§t ausgef√ºhrt.  Nat√ºrlich laufen sie in einer dreifachen Instanz - genau wie unsere Kunden ihre Dienste in der Cloud starten. <br><br><h2>  IP / MPLS-Fabrik </h2><br>  Hier ist ein Beispieldiagramm einer der Verf√ºgbarkeitszonen: <br><br><img src="https://habrastorage.org/webt/jn/xi/rw/jnxirwsmzj62bwzvldfnrq3n2_4.png"><br><br>  In jeder Verf√ºgbarkeitszone gibt es ungef√§hr f√ºnf Module und in jedem Modul ungef√§hr hundert Racks.  Leaf-Rack-Switches, die innerhalb ihres Moduls √ºber die Spine-Ebene verbunden sind, und die Konnektivit√§t zwischen Modulen werden √ºber die Netzwerkverbindung bereitgestellt.  Dies ist die n√§chste Ebene, die die sogenannten Super-Spines- und Edge-Switches umfasst, die bereits die Zugriffszonen verbinden.  Wir haben L2 absichtlich aufgegeben, wir sprechen nur √ºber L3 IP / MPLS-Konnektivit√§t.  BGP wird zum Verteilen von Routing-Informationen verwendet. <br><br>  Tats√§chlich gibt es viel mehr parallele Verbindungen als auf dem Bild.  Eine solch gro√üe Anzahl von ECMP-Verbindungen (Equal-Cost Multi-Path) stellt besondere √úberwachungsanforderungen.  Dar√ºber hinaus gibt es auf den ersten Blick unerwartete Grenzen in der Ausr√ºstung - zum Beispiel die Anzahl der ECMP-Gruppen. <br><br><h2>  Serververbindung </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xf/ay/jo/xfayjofymnoqjc-1u1otd0axhgm.png"></div><br>  Aufgrund der hohen Investitionen baut Yandex Services so auf, dass ein Ausfall eines Servers, Server-Racks, Moduls oder sogar eines gesamten Rechenzentrums niemals zu einem vollst√§ndigen Stopp des Service f√ºhrt.  Wenn wir irgendwelche Netzwerkprobleme haben - nehmen wir an, ein Rack-Mount-Switch ist defekt - sehen externe Benutzer dies nie. <br><br>  Yandex.Cloud ist ein Sonderfall.  Wir k√∂nnen dem Kunden nicht vorschreiben, wie er seine eigenen Services aufbauen soll, und wir haben uns entschlossen, diesen m√∂glichen Single Point of Failure auszugleichen.  Daher sind alle Server in der Cloud mit zwei Rack-Mount-Switches verbunden. <br><br>  Wir verwenden auch keine Redundanzprotokolle auf L2-Ebene, sondern verwenden sofort nur L3 mit BGP - wiederum aus Gr√ºnden der Protokollvereinheitlichung.  Diese Verbindung bietet jedem Dienst IPv4- und IPv6-Konnektivit√§t: Einige Dienste funktionieren √ºber IPv4, andere √ºber IPv6. <br><br>  Physisch ist jeder Server √ºber zwei 25-Gigabit-Schnittstellen verbunden.  Hier ist ein Foto aus dem Rechenzentrum: <br><br><img src="https://habrastorage.org/webt/jz/sr/xj/jzsrxj39equkvhixj3bjoj5kn6a.png"><br><br>  Hier sehen Sie zwei Rack-Switches mit 100-Gigabit-Ports.  Es sind unterschiedliche Breakout-Kabel sichtbar, die den 100-Gigabit-Port des Switch in 4 Ports mit 25 Gigabit pro Server unterteilen.  Wir nennen diese Kabel "Hydra". <br><br><h2>  Infrastrukturmanagement </h2><br>  Die Cloud-Netzwerkinfrastruktur enth√§lt keine propriet√§ren Verwaltungsl√∂sungen: Alle Systeme sind entweder Open Source mit Anpassung f√ºr die Cloud oder vollst√§ndig selbst geschrieben. <br><br><img src="https://habrastorage.org/webt/-g/if/qu/-gifqu8wgzw3xwehpuy5_ejfapc.png"><br><br>  Wie wird diese Infrastruktur verwaltet?  Es ist in der Cloud nicht so verboten, aber es wird dringend davon abgeraten, zu einem Netzwerkger√§t zu gehen und Anpassungen vorzunehmen.  Es gibt den aktuellen Status des Systems, und wir m√ºssen die √Ñnderungen anwenden: zu einem neuen Zielstatus kommen.  F√ºhren Sie ein Skript durch alle Dr√ºsen, √§ndern Sie etwas in der Konfiguration - Sie sollten dies nicht tun.  Stattdessen nehmen wir √Ñnderungen an den Vorlagen vor, an einer einzigen Quelle des Wahrheitssystems, und √ºbertragen unsere √Ñnderungen auf das Versionskontrollsystem.  Dies ist sehr praktisch, da Sie jederzeit einen Rollback durchf√ºhren, den Verlauf anzeigen, herausfinden k√∂nnen, wer f√ºr das Commit verantwortlich ist usw. <br><br>  Wenn wir die √Ñnderungen vorgenommen haben, werden Konfigurationen generiert und in der Labortesttopologie bereitgestellt.  Aus Netzwerksicht ist dies eine kleine Cloud, die die gesamte vorhandene Produktion vollst√§ndig wiederholt.  Wir werden sofort sehen, ob die gew√ºnschten √Ñnderungen etwas bewirken: erstens durch √úberwachung und zweitens durch Feedback unserer internen Benutzer. <br><br>  Wenn die √úberwachung besagt, dass alles ruhig ist, fahren wir mit der Einf√ºhrung fort - wenden die √Ñnderung jedoch nur auf einen Teil der Topologie an (zwei oder mehr Zug√§nglichkeiten haben aus demselben Grund nicht das Recht, zu brechen).  Dar√ºber hinaus √ºberwachen wir die √úberwachung weiterhin genau.  Dies ist ein ziemlich komplizierter Prozess, √ºber den wir weiter unten sprechen werden. <br><br>  Nachdem wir sichergestellt haben, dass alles in Ordnung ist, wenden wir die √Ñnderung auf die gesamte Produktion an.  Sie k√∂nnen jederzeit einen Rollback durchf√ºhren und zum vorherigen Status des Netzwerks zur√ºckkehren, um das Problem schnell zu verfolgen und zu beheben. <br><br><h4>  √úberwachung </h4><br>  Wir brauchen eine andere √úberwachung.  Eine der gefragtesten ist die √úberwachung der End-to-End-Konnektivit√§t.  Jeder Server sollte jederzeit in der Lage sein, mit jedem anderen Server zu kommunizieren.  Tatsache ist, dass wir, wenn irgendwo ein Problem auftritt, so fr√ºh wie m√∂glich genau herausfinden m√∂chten, wo (dh welche Server Probleme haben, aufeinander zuzugreifen).  Die Gew√§hrleistung einer durchg√§ngigen Konnektivit√§t ist unser Hauptanliegen. <br><br>  Jeder Server listet eine Reihe aller Server auf, mit denen er zu einem bestimmten Zeitpunkt kommunizieren kann.  Der Server nimmt eine zuf√§llige Teilmenge dieser Menge und sendet ICMP-, TCP- und UDP-Pakete an alle ausgew√§hlten Computer.  Dadurch wird gepr√ºft, ob im Netzwerk Verluste auftreten, ob sich die Verz√∂gerung erh√∂ht hat usw. Das gesamte Netzwerk wird innerhalb einer der Zugriffszonen und zwischen diesen "aufgerufen".  Die Ergebnisse werden an ein zentrales System gesendet, das sie f√ºr uns visualisiert. <br><br>  So sehen die Ergebnisse aus, wenn nicht alles sehr gut ist: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yd/-1/bs/yd-1bs7mguqfcy-xhdko674rvu0.png"></div><br>  Hier k√∂nnen Sie sehen, zwischen welchen Netzwerksegmenten ein Problem besteht (in diesem Fall A und B) und wo alles in Ordnung ist (A und D).  Hier k√∂nnen bestimmte Server, Rack-Switches, Module und gesamte Verf√ºgbarkeitszonen angezeigt werden.  Wenn eines der oben genannten Probleme das Problem verursacht, wird es in Echtzeit angezeigt. <br><br>  Zus√§tzlich gibt es eine Ereignis√ºberwachung.  Wir √ºberwachen alle Verbindungen, Signalpegel auf Transceivern, BGP-Sitzungen usw. genau. Angenommen, drei BGP-Sitzungen werden aus einem Netzwerksegment erstellt, von denen eine nachts unterbrochen wurde.  Wenn wir die √úberwachung so einrichten, dass der Ausfall einer BGP-Sitzung f√ºr uns nicht kritisch ist und bis zum Morgen warten kann, werden die Netzwerktechniker durch die √úberwachung nicht geweckt.  Wenn jedoch die zweite der drei Sitzungen ausf√§llt, ruft ein Techniker automatisch an. <br><br>  Zus√§tzlich zur End-to-End- und Ereignis√ºberwachung verwenden wir eine zentralisierte Sammlung von Protokollen, deren Echtzeitanalyse und nachfolgender Analyse.  Sie k√∂nnen die Korrelationen sehen, Probleme identifizieren und herausfinden, was auf den Netzwerkger√§ten passiert ist. <br><br>  Das √úberwachungsthema ist gro√ü genug, es gibt einen gro√üen Spielraum f√ºr Verbesserungen.  Ich m√∂chte das System zu mehr Automatisierung und wahrer Selbstheilung bringen. <br><br><h2>  Was weiter? </h2><br>  Wir haben viele Pl√§ne.  Es ist notwendig, Steuerungssysteme, √úberwachung, IP / MPLS-Fabriken und vieles mehr zu verbessern. <br><br>  Wir suchen auch aktiv nach White-Box-Schaltern.  Dies ist ein fertiges "Eisen" -Ger√§t, ein Schalter, auf dem Sie Ihre Software rollen k√∂nnen.  Erstens, wenn alles richtig gemacht wurde, ist es m√∂glich, die Switches auf die gleiche Weise wie die Server zu ‚Äûbehandeln‚Äú, einen wirklich praktischen CI / CD-Prozess zu erstellen, Konfigurationen schrittweise einzuf√ºhren usw. <br><br>  Zweitens ist es bei Problemen besser, eine Gruppe von Ingenieuren und Entwicklern zu behalten, die diese Probleme beheben, als lange auf eine L√∂sung durch den Anbieter zu warten. <br><br>  Damit alles klappt, wird in zwei Richtungen gearbeitet: <br><br><ul><li>  Wir haben die Komplexit√§t der IP / MPLS-Fabrik erheblich reduziert.  Einerseits ist die Ebene des virtuellen Netzwerks und der daraus resultierenden Automatisierungstools etwas komplizierter geworden.  Andererseits ist das Unterlagennetzwerk selbst einfacher geworden.  Mit anderen Worten, es gibt eine gewisse ‚ÄûKomplexit√§t‚Äú, die nicht gespeichert werden kann.  Es kann von einer Ebene zur anderen "geworfen" werden - beispielsweise zwischen Netzwerkebenen oder von der Netzwerkebene zur Anwendungsebene.  Und Sie k√∂nnen diese Komplexit√§t, die wir versuchen, richtig verteilen. </li><li>  Und nat√ºrlich stellen wir unsere Tools f√ºr die Verwaltung der gesamten Infrastruktur fertig. </li></ul><br>  Dies ist alles, was wir √ºber unsere Netzwerkinfrastruktur sprechen wollten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist ein Link</a> zum Cloud Telegram-Kanal mit Neuigkeiten und Tipps. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437816/">https://habr.com/ru/post/de437816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437806/index.html">EcmaScript 10 - Das diesj√§hrige JavaScript (ES2019)</a></li>
<li><a href="../de437808/index.html">Perf und Flammengraphen</a></li>
<li><a href="../de437810/index.html">Unternehmensrealit√§t</a></li>
<li><a href="../de437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Betas</a></li>
<li><a href="../de437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 und andere Beta-Versionen</a></li>
<li><a href="../de437818/index.html">Wir bringen dem Computer bei, Ger√§usche zu unterscheiden: Lernen Sie den DCASE-Wettbewerb kennen und bauen Sie Ihren Audio-Klassifikator in 30 Minuten zusammen</a></li>
<li><a href="../de437820/index.html">50 Farbt√∂ne Drupal-Sicherheit</a></li>
<li><a href="../de437824/index.html">Universelle 1C-Erweiterung f√ºr Google Sheets and Docs - nehmen und verwenden</a></li>
<li><a href="../de437826/index.html">Wie wir die Datenbank von Redis und Riak KV auf PostgreSQL migriert haben. Teil 1: Der Prozess</a></li>
<li><a href="../de437828/index.html">√ñffnen Sie das Webinar "SELECT-Abfrageausf√ºhrungsreihenfolge und Abfrageplan in MS SQL Server".</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>