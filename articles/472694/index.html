<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚öîÔ∏è ü§∏üèª üë®‚Äçüíª M√°s que Ceph: MCS Block Cloud Storage üßëüèΩ‚Äçü§ù‚Äçüßëüèº üì± üéΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Carro volador, Afu Chan 

 Trabajo en Mail.ru Cloud Solutons como arquitecto y desarrollador, incluida mi nube. Se sabe que una infraestructura de nub...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√°s que Ceph: MCS Block Cloud Storage</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472694/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/wx/av/di/wxavdimhgftb4rj-bjzsjjbdwts.jpeg"></div> <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Carro volador, Afu Chan</a></i> <br><br>  Trabajo en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mail.ru Cloud Solutons como</a> arquitecto y desarrollador, incluida mi nube.  Se sabe que una infraestructura de nube distribuida necesita un almacenamiento de bloques productivo, del que depende la operaci√≥n de los servicios y soluciones de PaaS creados con ellos. <br><br>  Inicialmente, cuando implementamos dicha infraestructura, usamos solo Ceph, pero gradualmente el almacenamiento en bloque evolucion√≥.  Quer√≠amos que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestras bases de datos</a> , almacenamiento de archivos y varios servicios funcionaran con el m√°ximo rendimiento, por lo que agregamos almacenes localizados y configuramos la supervisi√≥n avanzada de Ceph. <br><br>  Te dir√© c√≥mo fue, tal vez esta historia, los problemas que encontramos y nuestras soluciones ser√°n √∫tiles para aquellos que tambi√©n usan Ceph.  Por cierto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠ hay una</a> versi√≥n en video de este informe. <br><a name="habracut"></a><br><h2>  De los procesos de DevOps a su propia nube </h2><br>  Las pr√°cticas de DevOps tienen como objetivo implementar el producto lo m√°s r√°pido posible: <br><br><ul><li>  Automatizaci√≥n de procesos: todo el ciclo de vida: montaje, prueba, entrega a prueba y productividad.  Automatice los procesos gradualmente, comenzando con peque√±os pasos. <br></li><li>  La infraestructura como c√≥digo es un modelo cuando el proceso de configuraci√≥n de la infraestructura es similar al proceso de programaci√≥n del software.  Primero prueban el producto, el producto tiene ciertos requisitos para la infraestructura y la infraestructura necesita ser probada.  En esta etapa, los deseos de que ella aparezca, quiero "ajustar" la infraestructura, primero en el entorno de prueba, luego en el supermercado.  En la primera etapa, esto puede hacerse manualmente, pero luego pasan a la automatizaci√≥n, al modelo de "infraestructura como c√≥digo". <br></li><li>  Virtualizaci√≥n y contenedores: aparecen en la empresa cuando est√° claro que necesita poner procesos en una v√≠a industrial, implementar nuevas funciones m√°s r√°pido con una m√≠nima intervenci√≥n manual. <br></li></ul><br><img src="https://habrastorage.org/webt/wz/im/rw/wzimrwndijvqdzrmn6pjlunr_ci.jpeg">  <i>La arquitectura de todos los entornos virtuales es similar: m√°quinas invitadas con contenedores, aplicaciones, redes p√∫blicas y privadas, almacenamiento.</i> <br><br>  Gradualmente, se implementan m√°s y m√°s servicios en la infraestructura virtual integrada en los procesos de DevOps, y el entorno virtual se est√° convirtiendo no solo en una prueba (utilizada para el desarrollo y las pruebas), sino tambi√©n productiva. <br><br>  Como regla general, en las etapas iniciales son ignorados por las herramientas de automatizaci√≥n b√°sicas m√°s simples.  Pero a medida que se atraen nuevas herramientas, tarde o temprano es necesario implementar una plataforma en la nube completa para utilizar las herramientas m√°s avanzadas como Terraform. <br><br>  En esta etapa, la infraestructura virtual de "hipervisores, redes y almacenamiento" se convierte en una infraestructura de nube completa con herramientas y componentes desarrollados para orquestar procesos.  Luego aparece su propia nube, en la que tienen lugar los procesos de prueba y entrega autom√°tica de actualizaciones a los servicios existentes y la implementaci√≥n de nuevos servicios. <br><br>  El segundo camino hacia su propia nube es la necesidad de no depender de recursos externos y proveedores de servicios externos, es decir, proporcionar cierta independencia t√©cnica para sus propios servicios. <br><br><img src="https://habrastorage.org/webt/5y/wb/pm/5ywbpmuwrruww-bfep4f34p4eja.jpeg">  <i>La primera nube parece casi una infraestructura virtual: un hipervisor (uno o varios), m√°quinas virtuales con contenedores, almacenamiento compartido: si construye la nube no en soluciones propietarias, generalmente es Ceph o DRBD.</i> <br><br><h2>  Resiliencia y rendimiento de la nube privada </h2><br>  La nube est√° creciendo, el negocio depende cada vez m√°s de ella, la empresa comienza a exigir mayor confiabilidad. <br><br>  Aqu√≠, la distribuci√≥n se agrega a la nube privada, aparece la infraestructura de nube distribuida: puntos adicionales donde se encuentra el equipo.  La nube gestiona dos, tres o m√°s instalaciones creadas para proporcionar una soluci√≥n tolerante a fallas. <br><br>  Al mismo tiempo, se necesitan datos de todos los sitios, y existe un problema: dentro de un sitio no hay grandes retrasos en la transferencia de datos, pero entre los sitios los datos se transmiten m√°s lentamente. <br><br><img src="https://habrastorage.org/webt/q6/__/3m/q6__3mcgjw-wsknmxy-etas8oky.jpeg">  <i>Sitios de instalaci√≥n y almacenamiento com√∫n.</i>  <i>Los rect√°ngulos rojos son cuellos de botella en el nivel de la red.</i> <br><br>  La parte externa de la infraestructura desde el punto de vista de la red de administraci√≥n o la red p√∫blica no est√° tan ocupada, pero en la red interna los vol√∫menes de datos transferidos son mucho mayores.  Y en los sistemas distribuidos, comienzan los problemas, expresados ‚Äã‚Äãen un largo tiempo de servicio.  Si el cliente llega a un grupo de nodos de almacenamiento, los datos deben replicarse instant√°neamente en el segundo grupo para que los cambios no se pierdan. <br><br>  Para algunos procesos, la latencia de replicaci√≥n de datos es aceptable, pero en casos como el procesamiento de transacciones, las transacciones no se pueden perder.  Si se utiliza la replicaci√≥n asincr√≥nica, se produce un retraso de tiempo que puede conducir a la p√©rdida de una parte de los datos si falla una de las "colas" del sistema de almacenamiento (sistema de almacenamiento de datos).  Si se usa la replicaci√≥n sincr√≥nica, el tiempo de servicio aumenta. <br><br>  Tambi√©n es bastante natural que cuando aumenta el tiempo de procesamiento (latencia) del almacenamiento, las bases de datos comienzan a disminuir y hay efectos negativos que deben combatirse. <br><br>  En nuestra nube, buscamos soluciones equilibradas para mantener la fiabilidad y el rendimiento.  La t√©cnica m√°s simple es localizar los datos, y luego agregamos grupos de Ceph localizados adicionales. <br><br><img src="https://habrastorage.org/webt/qf/_g/k3/qf_gk33i3esefh0xgnjuhegczia.jpeg">  <i>El color verde indica grupos de Ceph localizados adicionales.</i> <br><br>  La ventaja de una arquitectura tan compleja es que aquellos que necesitan una entrada / salida r√°pida de datos pueden usar almacenamientos localizados.  Los datos para los cuales la disponibilidad total es cr√≠tica dentro de dos sitios se encuentran en un cl√∫ster distribuido.  Funciona m√°s lento, pero los datos que contiene se replican en ambos sitios.  Si su rendimiento no es suficiente, puede usar cl√∫steres Ceph localizados. <br><br>  La mayor√≠a de las nubes p√∫blicas y privadas llegan a tener aproximadamente el mismo patr√≥n de trabajo, cuando, seg√∫n los requisitos, la carga se despliega en diferentes tipos de almacenamientos (diferentes tipos de discos). <br><br><h2>  Diagn√≥stico ceph: c√≥mo construir monitoreo </h2><br>  Cuando implementamos y lanzamos la infraestructura, era hora de asegurar su funcionamiento, para minimizar el tiempo y la cantidad de fallas.  Por lo tanto, el siguiente paso en el desarrollo de la infraestructura fue la construcci√≥n de diagn√≥sticos y monitoreo. <br><br>  Considere la tarea de monitoreo en todo momento: tenemos una pila de aplicaciones en un entorno de nube virtual: una aplicaci√≥n, un sistema operativo invitado, un dispositivo de bloque, los controladores de este dispositivo de bloque en un hipervisor, una red de almacenamiento y el sistema de almacenamiento real (sistema de almacenamiento).  Y todo esto a√∫n no ha sido cubierto por el monitoreo. <br><br><img src="https://habrastorage.org/webt/z1/nk/nk/z1nknkrnannwnfn2fa7jwf6w0jq.jpeg">  <i>Elementos no cubiertos por el monitoreo.</i> <br><br>  El monitoreo se implementa en varias etapas, comenzamos con discos.  Obtenemos el n√∫mero de operaciones de lectura / escritura, con cierta precisi√≥n, el tiempo de servicio (megabytes por segundo), la profundidad de la cola, otras caracter√≠sticas, y tambi√©n recopilamos SMART sobre el estado de los discos. <br><br><img src="https://habrastorage.org/webt/2m/b_/7s/2mb_7s2vda6qq6dtem9iwsnlup8.jpeg">  <i>La primera etapa: cubrimos los discos de monitoreo.</i> <br><br>  El monitoreo de disco no es suficiente para obtener una imagen completa de lo que est√° sucediendo en el sistema.  Por lo tanto, pasamos a monitorear un elemento cr√≠tico de la infraestructura: la red del sistema de almacenamiento.  En realidad, hay dos: el cl√∫ster interno y el cliente, que conecta cl√∫steres de almacenamiento con hipervisores.  Aqu√≠ obtenemos las velocidades de transferencia de paquetes de datos (megabytes por segundo, paquetes por segundo), el tama√±o de las colas de la red, las memorias intermedias y posiblemente las rutas de datos. <br><br><img src="https://habrastorage.org/webt/pn/dd/wu/pnddwuxah0r9sngg8awzcxhzlom.jpeg">  <i>Segunda etapa: monitoreo de la red.</i> <br><br>  A menudo se detienen en esto, pero esto no se puede hacer, porque la mayor parte de la infraestructura a√∫n no se ha cerrado por monitoreo. <br><br>  Todo el almacenamiento distribuido utilizado en nubes p√∫blicas y privadas es SDS, almacenamiento definido por software.  Se pueden implementar en las soluciones de un proveedor en particular, soluciones de c√≥digo abierto, puede hacer algo usted mismo utilizando una pila de tecnolog√≠as familiares.  Pero siempre es SDS, y el trabajo de estas partes de software debe ser monitoreado. <br><br><img src="https://habrastorage.org/webt/mt/o5/cb/mto5cbnz456tvs6mg_6i-b5lm8o.jpeg">  <i>Tercer paso: monitorear el demonio de almacenamiento.</i> <br><br>  La mayor√≠a de los operadores de Ceph utilizan datos recopilados de los demonios de control y monitoreo de Ceph (monitor y gerente, tambi√©n conocido como mgr).  Inicialmente, seguimos el mismo camino, pero r√°pidamente nos dimos cuenta de que esta informaci√≥n no era suficiente: las advertencias sobre solicitudes pendientes aparecen tarde: la solicitud se suspendi√≥ durante 30 segundos, solo entonces la vimos.  Mientras se trate de monitoreo, mientras el monitoreo activa la alarma, pasar√°n al menos tres minutos.  En el mejor de los casos, esto significa que parte del almacenamiento y las aplicaciones estar√°n inactivas durante tres minutos. <br><br>  Naturalmente, decidimos expandir el monitoreo y pasamos al elemento principal de Ceph: el demonio OSD.  Al monitorear el daemon de Object Storage, obtenemos el tiempo aproximado de operaci√≥n como lo ve el OSD, as√≠ como estad√≠sticas sobre solicitudes bloqueadas: qui√©n, cu√°ndo, en qu√© PG, durante cu√°nto tiempo. <br><br><h2>  ¬øPor qu√© solo Ceph no es suficiente y qu√© hacer al respecto? </h2><br>  Ceph por s√≠ solo no es suficiente por varias razones.  Por ejemplo, tenemos un cliente con un perfil de base de datos.  Implement√≥ todas las bases de datos en el cl√∫ster todo flash, la latencia de las operaciones que se emitieron all√≠ le conven√≠a, sin embargo, hubo quejas de tiempo de inactividad. <br><br>  El sistema de monitoreo no le permite ver lo que sucede dentro de los clientes del entorno virtual.  Como resultado, para identificar el problema, utilizamos el an√°lisis avanzado, que se solicit√≥ utilizando la utilidad blktrace de su m√°quina virtual. <br><br><img src="https://habrastorage.org/webt/uj/ch/vk/ujchvkl3ozoarbjooedieuqzf80.jpeg">  <i>El resultado de un an√°lisis extendido.</i> <br><br>  Los resultados del an√°lisis contienen operaciones marcadas con las banderas W y WS.  El indicador W es un registro, el indicador WS es un registro s√≠ncrono, esperando que el dispositivo complete la operaci√≥n.  Cuando trabajamos con bases de datos, casi todas las bases de datos SQL tienen un cuello de botella: WAL (registro de escritura anticipada). <br><br>  La base de datos siempre escribe primero los datos en el registro, recibe la confirmaci√≥n del disco con buffers de descarga y luego escribe los datos en la base de datos.  Si no ha recibido la confirmaci√≥n de un reinicio del b√∫fer, cree que un reinicio de energ√≠a puede borrar una transacci√≥n confirmada por el cliente.  Esto es inaceptable para la base de datos, por lo que muestra "escribir SYNC / FLUSH", luego escribe los datos.  Cuando los registros est√°n llenos, se produce su cambio y todo lo que ingres√≥ en la cach√© de la p√°gina tambi√©n se muestra a la fuerza.  <i>Agregado: no hay reinicio en la imagen en s√≠, es decir, operaciones con el indicador de pre-vaciado.</i>  <i>Se ven como FWS - pre-flush + write + sync o FWSF - pre-flush + write + sync + FUA</i> <br><br>  Cuando un cliente tiene muchas transacciones peque√±as, pr√°cticamente todas sus E / S se convierten en una cadena secuencial: escritura - descarga - escritura - descarga.  Como no puede hacer algo con la base de datos, comenzamos a trabajar con el sistema de almacenamiento.  En este momento, entendemos que las capacidades de Ceph no son suficientes. <br><br>  Para nosotros, en esta etapa, la mejor soluci√≥n era agregar repositorios locales peque√±os y r√°pidos que no se implementaron utilizando herramientas Ceph (b√°sicamente agotamos sus capacidades).  Y convertimos el almacenamiento en la nube en algo m√°s que Ceph.  En nuestro caso, hemos agregado muchas historias locales (locales en t√©rminos del centro de datos, no del hipervisor). <br><br><img src="https://habrastorage.org/webt/4o/hh/rr/4ohhrre0loyyk4gmxijlpwy6pry.jpeg">  <i>Repositorios localizados adicionales Objetivo A y B.</i> <br><br>  El tiempo de servicio de dicho almacenamiento local es de aproximadamente 0,3 ms por flujo.  Si se encuentra en otro centro de datos, funciona m√°s lentamente, con un rendimiento de aproximadamente 0,7 ms.  Este es un aumento significativo en comparaci√≥n con Ceph, que produce 1,2 ms, y se distribuye en centros de datos: 2 ms.  El rendimiento de estas peque√±as f√°bricas, de las cuales tenemos m√°s de una docena, es de aproximadamente 100 mil por m√≥dulo, 100 mil IOPS por registro. <br><br>  Despu√©s de tal cambio en la infraestructura, nuestra nube exprime menos de un mill√≥n de IOPS para escribir, o alrededor de dos a tres millones de IOPS para leer en total para todos los clientes: <br><br><img src="https://habrastorage.org/webt/hw/zq/aj/hwzqajyzsmewq-s2h6iw8mnf4bk.jpeg"><br><br>  Es importante tener en cuenta que este tipo de almacenamiento no es el m√©todo principal de expansi√≥n, colocamos la apuesta principal en Ceph, y la presencia de almacenamiento r√°pido es importante solo para los servicios que requieren tiempo de respuesta del disco. <br><br><h2>  Nuevas iteraciones: mejoras de c√≥digo e infraestructura </h2><br>  Todas nuestras historias son recursos compartidos.  Dicha infraestructura requiere que <b>implementemos una pol√≠tica de nivel de servicio</b> : debemos proporcionar un cierto nivel de servicio y no permitir que un cliente interfiera con otro por accidente o a prop√≥sito, deshabilitando el almacenamiento. <br><br>  Para hacer esto, tuvimos que hacer la finalizaci√≥n y el despliegue no trivial: entrega iterativa a los productivos. <br><br>  Este despliegue fue diferente de las pr√°cticas habituales de DevOps, cuando todos los procesos: ensamblaje, prueba, despliegue de c√≥digo, reinicio del servicio, si es necesario, comienzan con un clic de un bot√≥n, y luego todo funciona.  Si despliega las pr√°cticas de DevOps en la infraestructura, permanecer√° vigente hasta el primer error. <br><br>  Es por eso que la "automatizaci√≥n completa" no se arraig√≥ particularmente en el equipo de infraestructura.  Por supuesto, existe un cierto enfoque para la automatizaci√≥n de pruebas y entregas, pero siempre est√° controlado y la entrega es iniciada por los ingenieros de SRE del equipo de la nube. <br><br>  Implementamos cambios en varios servicios: en el backend de Cinder, en la interfaz de Cinder (cliente de Cinder) y en el servicio Nova.  Los cambios se aplicaron en varias iteraciones, una iteraci√≥n a la vez.  Despu√©s de la tercera iteraci√≥n, los cambios correspondientes se aplicaron a las m√°quinas invitadas de los clientes: alguien migr√≥, alguien reinici√≥ la VM (reinicio completo) o plane√≥ la migraci√≥n para servir a los hipervisores. <br><br>  El siguiente problema que surgi√≥ son los <b>saltos en la velocidad de escritura</b> .  Cuando trabajamos con almacenamiento conectado a la red, el hipervisor predeterminado considera que la red es lenta y, por lo tanto, almacena en cach√© todos los datos.  Escribe r√°pidamente, hasta varias decenas de megabytes, y luego comienza a vaciar el cach√©.  Hubo muchos momentos desagradables debido a tales saltos. <br><br>  Descubrimos que si enciende el cach√©, el rendimiento de la SSD disminuye en un 15%, y si apaga el cach√©, el rendimiento del HDD disminuye en un 35%.  Se requiri√≥ otro desarrollo, implementado la administraci√≥n de cach√© administrada, cuando el almacenamiento en cach√© se asigna expl√≠citamente para cada tipo de disco.  Esto nos permiti√≥ conducir SSD sin cach√© y HDD: con un cach√©, como resultado, dejamos de perder rendimiento. <br><br>  La pr√°ctica de entregar desarrollo a un productivo es similar: iteraciones.  Implementamos el c√≥digo, reiniciamos el demonio y luego, seg√∫n sea necesario, reiniciamos o migramos m√°quinas virtuales invitadas, que deber√≠an estar sujetas a cambios.  La m√°quina virtual del cliente migr√≥ del HDD, su cach√© se activ√≥: todo funciona o, por el contrario, el cliente migr√≥ con SSD, su cach√© se activ√≥: todo funciona. <br><br>  El tercer problema es el <b>funcionamiento incorrecto de las m√°quinas virtuales implementadas desde im√°genes GOLD en el HDD</b> . <br><br>  Hay muchos clientes de este tipo, y la peculiaridad de la situaci√≥n es que el trabajo de la VM se ajust√≥ por s√≠ mismo: se garantiz√≥ que el problema ocurriera durante la implementaci√≥n, pero se resolvi√≥ mientras el cliente contactaba con el soporte t√©cnico.  Al principio, les pedimos a los clientes que esperaran media hora hasta que la VM se estabilizara, pero luego comenzamos a trabajar en la calidad del servicio. <br><br>  En el proceso de investigaci√≥n, nos dimos cuenta de que las capacidades de nuestra infraestructura de monitoreo a√∫n no son suficientes. <br><br><img src="https://habrastorage.org/webt/bl/tj/ft/bltjftnwasbvxd_iep-wwfsepkw.jpeg">  <i>El monitoreo cerr√≥ la parte azul, y el problema estaba en la parte superior de la infraestructura, no cubierto por el monitoreo.</i> <br><br>  Comenzamos a lidiar con lo que est√° sucediendo en la parte de la infraestructura que no estaba cubierta por el monitoreo.  Para hacer esto, utilizamos el diagn√≥stico avanzado de Ceph (o m√°s bien, una de las variedades del cliente Ceph - librbd).  Utilizando herramientas de automatizaci√≥n, realizamos cambios en la configuraci√≥n del cliente Ceph para acceder a las estructuras de datos internas a trav√©s del socket del dominio Unix, y comenzamos a tomar estad√≠sticas de los clientes Ceph en el hipervisor. <br><br>  Que vimos  No vimos estad√≠sticas en el cl√∫ster / OSD / cl√∫ster Ceph, sino estad√≠sticas en cada disco de la m√°quina virtual del cliente cuyos discos estaban en Ceph, es decir, estad√≠sticas asociadas con el dispositivo. <br><br><img src="https://habrastorage.org/webt/us/hr/5j/ushr5jfx40pvedrcqwpvj9ai-qy.jpeg">  <i>Resultados avanzados de estad√≠sticas de monitoreo.</i> <br><br>  Fueron las estad√≠sticas ampliadas las que dejaron en claro que el problema solo ocurre en discos clonados de otros discos. <br><br>  Luego, observamos las estad√≠sticas sobre las operaciones, en particular las operaciones de lectura y escritura.  Result√≥ que la carga en las im√°genes de nivel superior es relativamente peque√±a, y en las iniciales, de las cuales proviene el clon, es grande pero sin equilibrio: una gran cantidad de lectura sin ninguna grabaci√≥n. <br><br>  El problema est√° localizado, ahora se necesita una soluci√≥n: ¬øc√≥digo o infraestructura? <br><br>  No se puede hacer nada con el c√≥digo Ceph, es "dif√≠cil".  Adem√°s, la seguridad de los datos del cliente depende de ello.  Pero hay un problema, debe resolverse y cambiamos la arquitectura del repositorio.  El cl√∫ster HDD se convirti√≥ en un cl√∫ster h√≠brido: se agreg√≥ una cierta cantidad de SSD al HDD, luego se cambiaron las prioridades de los demonios OSD para que el SSD siempre tuviera prioridad y se convirtiera en el OSD primario dentro del grupo de colocaci√≥n (PG). <br><br>  Ahora, cuando el cliente despliega la m√°quina virtual desde el disco clonado, sus operaciones de lectura van al SSD.  Como resultado, la recuperaci√≥n del disco se hizo r√°pida, y solo los datos del cliente que no sean la imagen original se escriben en el HDD.  Recibimos un triple aumento en la productividad casi gratis (en relaci√≥n con el costo inicial de la infraestructura). <br><br><h1>  Por qu√© es importante el monitoreo de la infraestructura </h1><br><ol><li>  La infraestructura de monitoreo debe incluirse al m√°ximo en toda la pila, comenzando con la m√°quina virtual y terminando con el disco.  Despu√©s de todo, mientras un cliente que utiliza una nube p√∫blica o privada llega a su infraestructura y proporciona la informaci√≥n necesaria, el problema cambiar√° o se trasladar√° a otro lugar. <br></li><li>  Monitorear todo el hipervisor, la m√°quina virtual o el contenedor "en su totalidad" no produce casi nada.  Intentamos entender del tr√°fico de red lo que est√° sucediendo con Ceph: es in√∫til, los datos vuelan a alta velocidad (de 500 megabytes por segundo), es extremadamente dif√≠cil seleccionar los necesarios.  Se necesitar√° un volumen monstruoso de discos para almacenar tales estad√≠sticas y mucho tiempo para analizarlas. <br></li><li>       ,     - .   :     ,           ,   ‚Äî     ,        . <br></li><li>   ‚Äî     .   ,   .   ‚Äî     ,      .        ,   .             ‚Äî    ,   ,      . <br></li><li>  MCS Cloud Solutions ‚Äî  ,            .             . <br></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472694/">https://habr.com/ru/post/472694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472682/index.html">Retrasando el envejecimiento con sinergias de drogas en C. elegans</a></li>
<li><a href="../472684/index.html">Sorpresa fsync () PostgreSQL</a></li>
<li><a href="../472686/index.html">Video Studio basado en i486</a></li>
<li><a href="../472688/index.html">C√≥mo funciona la renderizaci√≥n de juegos en 3D: procesamiento de v√©rtices</a></li>
<li><a href="../472690/index.html">Novedades de Zabbix 4.4</a></li>
<li><a href="../472702/index.html">JH Rainwater "C√≥mo pastar gatos": razas de programadores y caracter√≠sticas de su cr√≠a</a></li>
<li><a href="../472708/index.html">Imperva revel√≥ detalles t√©cnicos del hack de Cloud WAF</a></li>
<li><a href="../472714/index.html">D√≥nde buscar trabajadores y no enamorarse del trabajador front-end: Telegram, Slack y no solo</a></li>
<li><a href="../472716/index.html">Obtener Spring Bean del contexto de aplicaci√≥n de terceros correctamente</a></li>
<li><a href="../472720/index.html">ERP no funciona ... ¬øCu√°l es la alternativa? o justo a tiempo. Para Rusia?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>