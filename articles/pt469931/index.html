<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🔬 🚬 🕠 O que é um fator de velocidade de aprendizado e como ele melhora as características de aprendizado profundo? 💺 🚶🏾 🛶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este artigo é minha tentativa de expressar minha opinião sobre os seguintes aspectos: 



1. O que é um fator de velocidade de aprendizado e qual é o ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O que é um fator de velocidade de aprendizado e como ele melhora as características de aprendizado profundo?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469931/">  Este artigo é minha tentativa de expressar minha opinião sobre os seguintes aspectos: <br><br><ol><li>  O que é um fator de velocidade de aprendizado e qual é o seu valor? </li><li>  Como escolher esse coeficiente ao treinar modelos? </li><li>  Por que é necessário alterar o coeficiente da velocidade de aprendizado durante o treinamento dos modelos? </li><li>  O que fazer com um fator de velocidade de aprendizado ao usar um modelo pré-treinado? </li></ol><br>  A maior parte deste post é baseada em materiais preparados por <i>fast.ai</i> : [1], [2], [5] e [3] - representando uma versão concisa de seu trabalho, destinada ao entendimento mais rápido da essência da questão.  Para se familiarizar com os detalhes, é recomendável clicar nos links abaixo. <br><a name="habracut"></a><br><h3>  <b>O que é um fator de velocidade de aprendizado?</b> </h3><br>  O coeficiente de velocidade de aprendizado é um hiperparâmetro que determina a ordem em que ajustaremos nossas escalas, levando em consideração a função de perda na descida do gradiente.  Quanto menor o valor, mais lento nos movemos ao longo da inclinação.  Embora ao usar um baixo coeficiente de velocidade de aprendizado, possamos obter um efeito positivo no sentido de que não perdemos um único mínimo local, isso também pode significar que teremos que gastar muito tempo em convergência, especialmente se estivermos na região do platô. <br><br>  O relacionamento é ilustrado pela seguinte fórmula <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo>&amp;#x2022;</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><msub><mi>t</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mo>&amp;#x2212;</mo><mi>l</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>n</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi><mi>r</mi></msub><mi>a</mi><mi>t</mi><mi>e</mi><mo>&amp;#x2217;</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="65.706ex" height="2.419ex" viewBox="0 -780.1 28290 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMAIN-2219" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="1101" y="0"></use><g transform="translate(1567,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-77" x="1013" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="2890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="3357" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-67" x="3702" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-68" x="4183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="4759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMAIN-3D" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-77" x="6455" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="7171" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="7638" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-67" x="7983" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-68" x="8464" y="0"></use><g transform="translate(9040,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-77" x="511" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="10008" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="10475" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-67" x="10820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-68" x="11301" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="11877" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="12239" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-78" x="12705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="13278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-73" x="13623" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="14093" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="14454" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6E" x="14921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="15521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="15883" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMAIN-2212" x="16572" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6C" x="17572" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="17871" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-61" x="18337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-72" x="18867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6E" x="19318" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="19919" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6E" x="20264" y="0"></use><g transform="translate(20865,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-72" x="675" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-61" x="21762" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="22291" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="22653" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMAIN-2217" x="23341" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-67" x="24064" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-72" x="24544" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-61" x="24996" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-64" x="25525" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-69" x="26049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="26394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-6E" x="26861" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-74" x="27461" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhj9KKdnd69eXRBlsOqNOgSOrtNpsg#MJMATHI-65" x="27823" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo>•</mo><mi>n</mi><mi>e</mi><msub><mi>w</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><msub><mi>t</mi><mi>w</mi></msub><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mo>−</mo><mi>l</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>n</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi><mi>r</mi></msub><mi>a</mi><mi>t</mi><mi>e</mi><mo>∗</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> • new_weight = weight_weight existente - learning_rate * gradiente </script></p><br><img src="https://habrastorage.org/webt/dn/j1/nj/dnj1njm2womrahwlbv_dzs25xqs.jpeg"><br>  <b>Descida de gradiente com fatores de velocidade de aprendizado pequenos (em cima) e grandes (em baixo).</b>  <b>Fonte: Curso de aprendizado de máquina de Andrew Ng no Coursera</b> <b><br></b> <br>  Na maioria das vezes, o fator de velocidade de aprendizado é definido arbitrariamente pelo usuário.  Na melhor das hipóteses, para uma compreensão intuitiva de qual valor é mais adequado para determinar o coeficiente de velocidade de aprendizado, ele pode confiar em experimentos anteriores (ou em outro tipo de material de treinamento). <br><br>  Basicamente, já é difícil escolher o valor certo.  O diagrama abaixo ilustra vários cenários que podem surgir quando o usuário ajusta independentemente a taxa de velocidade de aprendizado. <br><br><img src="https://habrastorage.org/webt/qm/uh/qd/qmuhqdtbcagnnzltgnfy53wuxxi.jpeg"><br>  <b>A influência de vários fatores da taxa de aprendizado na convergência.</b>  <b>(Img de crédito: cs231n)</b> <b><br></b> <br>  Além disso, o fator de velocidade de aprendizado afeta a rapidez com que nosso modelo atinge um mínimo local (ou seja, alcançará a melhor precisão).  Assim, a escolha certa desde o início garante menos perda de tempo para o treinamento do modelo.  Quanto menos tempo de treinamento, menos dinheiro é gasto no poder de computação da GPU na nuvem. <br><br><h4>  Existe uma maneira mais conveniente de determinar a taxa de coeficiente de aprendizagem? <br></h4><br>  No ponto 3.3.  “ <i>Coeficientes de taxa de aprendizado cíclico para redes neurais</i> ” Leslie Smith defendeu o seguinte ponto: a eficácia da velocidade de aprendizado pode ser estimada treinando o modelo com uma velocidade de aprendizado baixa inicialmente definida, que depois aumenta (linear ou exponencialmente) a cada iteração. <br><br><img src="https://habrastorage.org/webt/j9/zu/yi/j9zuyi5do_thph6ylxtaxetvm7q.jpeg"><br>  <b>O fator de velocidade de aprendizado aumenta após cada mini-pacote.</b> <b><br></b> <br>  Fixando os valores dos indicadores em cada iteração, veremos que à medida que a velocidade de aprendizado aumenta, será atingido um ponto no qual o valor da função de perda deixa de diminuir e começa a aumentar.  Na prática, nossa velocidade de aprendizado deve estar idealmente em algum lugar à esquerda do ponto inferior do gráfico (como mostrado no gráfico abaixo).  Nesse caso (o valor será) de 0,001 a 0,01. <br><br><img src="https://habrastorage.org/webt/tq/sw/m7/tqswm7bda8qr9zed3h9s3fafbj4.jpeg"><br><br><h4>  O texto acima parece útil.  Como começar a usá-lo? </h4><br>  No momento, há uma função pronta no pacote <i>fast.ia</i> desenvolvido por Jeremy Howard, esse é um tipo de abstração / complemento na parte superior da biblioteca pytorch (semelhante à maneira como é feita no caso de Keras e Tensorflow). <br><br>  Só é necessário digitar o comando a seguir para iniciar a busca pelo coeficiente ideal de velocidade de aprendizado antes de (iniciar) o treinamento da rede neural. <br><br><pre><code class="python hljs">learn.lr_find() learn.sched.plot_lr()</code> </pre> <br><br><h3>  <b>Melhorando o modelo</b> </h3><br>  Então, conversamos sobre qual é o coeficiente de velocidade de aprendizado, qual é o seu valor e como atingir seu valor ideal antes de começar a treinar o próprio modelo. <br>  Agora, vamos nos concentrar em como o fator de velocidade de aprendizado pode ser usado para ajustar modelos. <br><br><h4>  Sabedoria convencional </h4><br>  Geralmente, quando o usuário define seu coeficiente de velocidade de aprendizado e começa a treinar o modelo, ele precisa esperar até que o coeficiente de velocidade de aprendizado comece a cair e o modelo atinja o valor ideal. <br><br>  No entanto, a partir do momento em que o gradiente atinge um platô, fica mais difícil melhorar os valores da função de perda ao treinar o modelo.  Em [3], Dauphin expressa a opinião de que a dificuldade em minimizar a função de perda decorre do ponto de sela e não do mínimo local. <br><br><img src="https://habrastorage.org/webt/-t/jm/uw/-tjmuwg7a8etbhsc36cw97flhh8.png"><br>  <b>Um ponto de sela na superfície dos erros.</b>  <b>Um ponto de sela é um ponto do domínio de definição de uma função que é estacionária para uma determinada função, mas não é seu extremo local</b> .  (ImgCredit: safaribooksonline) <br><br><h4>  Então, como isso pode ser evitado? </h4><br>  Proponho considerar várias opções.  Um deles, geral, usando a citação de [1], <br><blockquote>  ... em vez de usar um valor fixo para o coeficiente de velocidade de aprendizado e diminuí-lo ao longo do tempo, se o treinamento não suavizar mais nossa perda, mudaremos o coeficiente de velocidade de aprendizado em cada iteração, de acordo com alguma função cíclica f.  Cada loop possui - em termos de número de iterações - um comprimento fixo.  Este método permite que o coeficiente de velocidade de aprendizado varie entre valores razoáveis ​​de limite.  Isso realmente ajuda, porque, ao ficar preso nos pontos de sela, aumentando o coeficiente de velocidade da aprendizagem, obtemos uma interseção mais rápida do platô de pontos de sela </blockquote><br>  Em [2], Leslie propõe o "método do triângulo", no qual o coeficiente de velocidade de aprendizado é revisado após cada uma das várias iterações. <br><br><img src="https://habrastorage.org/webt/j4/_w/ga/j4_wga1vdfg3qmiovtzdbtrmb4e.jpeg"><br><br><img src="https://habrastorage.org/webt/pn/tu/f5/pntuf5w2svpsbk9nsiyme8iqf98.jpeg"><br>  <b>"O método dos triângulos" e o "método dos triângulos-2" são métodos para teste cíclico dos coeficientes da taxa de aprendizado, propostos por Leslie N. Smith.</b>  <b>No gráfico superior, o Ir mínimo e o máximo são mantidos iguais.</b> <br><br>  Outro método, que não é menos popular e denominado “descida gradiente estocástica com um reset a quente”, foi proposto por Lonchilov &amp; Hutter [6].  Este método, baseado no uso da função cosseno como uma função cíclica, reinicia o coeficiente da velocidade de aprendizado no ponto máximo de cada ciclo.  A aparência do bit "Quente" se deve ao fato de que, quando o coeficiente da taxa de aprendizado é reiniciado, ele começa não a partir do nível zero, mas a partir dos parâmetros aos quais o modelo atingiu a etapa anterior. <br><br>  Como esse método possui variações, o gráfico abaixo mostra um dos métodos de sua aplicação, em que cada ciclo está vinculado ao mesmo intervalo de tempo. <br><br><img src="https://habrastorage.org/webt/kb/8o/pe/kb8opexd3ppx8ynj2bj7dtaphzg.jpeg"><br>  <b>SGDR - gráfico, coeficiente de taxa de aprendizado vs.</b>  <b>iterações</b> <b><br></b> <br>  Assim, temos uma maneira de reduzir a duração do treinamento simplesmente pulando os "picos" de tempos em tempos (como mostrado abaixo). <br><br><img src="https://habrastorage.org/webt/85/cn/-b/85cn-blk82myspio7d5pxfwfeqk.png"><br>  <b>Comparação de coeficientes de taxa de aprendizagem fixa e cíclica</b> (img credit: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arxiv.org/abs/1704.00109</a> <br>  Além de economizar tempo, esse método, de acordo com estudos, melhora a precisão da classificação sem ajuste e por menos iterações. <br><br><h3>  Taxa de transferência de aprendizado em Transferência de aprendizado </h3><br>  No curso de <i>fast.ai, a</i> ênfase está no gerenciamento de um modelo pré-treinado na solução de problemas de inteligência artificial.  Por exemplo, ao resolver problemas de classificação de imagem, os alunos são treinados no uso de modelos pré-treinados, como VGG e Resnet50, e vinculá-los à amostra de dados de imagem que precisa ser prevista. <br>  Para resumir como o modelo é construído no programa <i>fast.ai</i> (para não confundir com <i>o pacote fast. Ai</i> ), o pacote do programa), abaixo estão as etapas que <i>executaremos</i> em uma situação comum: <br><br><ol><li>  Ativar aumento de dados e pré-calcular = True </li><li>  Use Ir_find () para encontrar o maior coeficiente de taxa de aprendizado, onde a perda ainda está claramente melhorando. </li><li>  Treine a última camada de ativações pré-calculadas para a era 1-2. </li><li>  Treine a última camada com ganho de dados (ou seja, calcule = false) por 1-2 épocas com o ciclo _len 1. </li><li>  Descongele todas as camadas. </li><li>  Coloque as camadas anteriores em um fator de velocidade de aprendizado 3x-10x abaixo da próxima camada alta </li><li>  Reutilizar Ir_find () </li><li>  Treine uma rede completa com o ciclo _mult = 2 = 2 até começar a reciclagem. </li></ol><br>  Você pode perceber que as etapas dois, cinco e sete (acima) estão relacionadas à taxa do fator de aprendizado.  Em uma parte anterior de nosso post, destacamos o ponto das segundas etapas mencionadas - onde abordamos como obter o melhor coeficiente de velocidade de aprendizado antes de começar a treinar o modelo. <br><br>  No próximo parágrafo, falamos sobre como você pode reduzir o tempo de treinamento usando o SGDR e, reiniciando periodicamente o fator de velocidade de aprendizado, aprimora a precisão para evitar áreas onde o gradiente está próximo de zero. <br>  Na última seção, abordaremos o conceito de aprendizado diferenciado e explicaremos como ele é usado para determinar o coeficiente de velocidade de aprendizado quando um modelo treinado é associado a um pré-treinado ... <br><br><h3>  O que é aprendizagem diferencial </h3><br>  Este é um método no qual vários fatores de velocidade de treinamento são definidos na rede durante o treinamento.  Ele fornece uma alternativa à maneira pela qual os usuários geralmente ajustam os fatores de velocidade de aprendizado - ou seja, usando o mesmo fator de velocidade de aprendizado pela rede durante o treinamento. <br><br><img src="https://habrastorage.org/webt/xb/aw/-e/xbaw-e9-pehhvaeylpidgeykwwo.png"><br>  <b>A razão pela qual eu amo o Twitter é uma resposta direta da própria pessoa.</b> <b><br></b>  (No momento da redação deste post, Jeremy publicou um artigo com Sebastian Ruder, que se aprofundou ainda mais neste tópico. Então, acredito que o coeficiente diferencial de velocidade de aprendizado agora tem outro nome - ajuste exato discriminatório :) <br><br>  Para demonstrar o conceito mais claramente, podemos nos referir ao diagrama abaixo, no qual o modelo previamente treinado é “dividido” em 3 grupos, onde cada um é ajustado com um valor crescente do coeficiente de velocidade de aprendizado. <br><br><img src="https://habrastorage.org/webt/cv/3l/ax/cv3laxkfy-60oz9ftqnhotviqss.jpeg"><br>  <b>Exemplo da CNN com coeficiente de taxa de aprendizado diferenciado</b> .  Crédito de imagem de [3] <br><br>  Esse método de configuração é baseado no seguinte entendimento: as primeiras camadas geralmente contêm detalhes muito pequenos dos dados, como linhas e ângulos - dos quais não tentaremos mudar muito e salvar as informações nelas contidas.  Em geral, não há necessidade séria de alterar seu peso para qualquer número grande. <br><br>  Pelo contrário, para as camadas subseqüentes - como as da imagem pintadas de verde, onde obtemos sinais detalhados dos dados, como brancos dos olhos, boca ou nariz - a necessidade de salvá-las desaparece. <br><br><h4>  Como isso se compara com outros métodos de ajuste fino? </h4><br>  Em [9], foi provado que o ajuste fino de todo o modelo seria muito caro, uma vez que os usuários podem obter mais de 100 camadas.  Na maioria das vezes, as pessoas recorrem à otimização do modelo, uma camada de cada vez. <br><br>  No entanto, esse é o motivo de vários requisitos, os chamados  simultaneidade interferente e requer várias entradas por meio de um conjunto de dados, o que leva ao treinamento excessivo de pequenos conjuntos <br><br>  Também mostramos que os métodos apresentados em [9] são capazes de melhorar a precisão e reduzir o número de erros em várias tarefas relacionadas à classificação NRL. <br><br><img src="https://habrastorage.org/webt/by/no/yr/bynoyrrrl8edulvd8udbo8hv-uk.png"><br>  <b>Resultados obtidos da fonte [9]</b> <br><br>  Referências: <br>  [1] Melhorando a maneira como trabalhamos com a taxa de aprendizado. <br>  [2] A técnica da Taxa de Aprendizagem Cíclica. <br>  [3] Transferir aprendizado usando taxas diferenciais de aprendizado. <br>  [4] Leslie N. Smith.  Taxas de aprendizado cíclico para treinamento de redes neurais. <br>  [5] Estimando uma taxa ideal de aprendizado para uma rede neural profunda <br>  [6] Descida estocástica de gradiente com reinicializações a quente <br>  [7] Otimização para os destaques da aprendizagem profunda em 2017 <br>  [8] Caderno da lição 1, fast.ai Parte 1 V2 <br>  [9] Modelos de linguagem refinados para classificação de texto. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt469931/">https://habr.com/ru/post/pt469931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt469917/index.html">Texto rápido em PHP \ Python. Primeiros passos</a></li>
<li><a href="../pt469919/index.html">Verificação do código do Telegram Open Network pelo analisador PVS-Studio</a></li>
<li><a href="../pt469923/index.html">Vulnerabilidade inesperada em produtos Apple. Totalmente inesperado</a></li>
<li><a href="../pt469925/index.html">“F # não é mais difícil de dominar do que o Entity Framework ou o WPF”: Entrevista com Scott Vlashin</a></li>
<li><a href="../pt469927/index.html">10 mandamentos do desenvolvedor</a></li>
<li><a href="../pt469933/index.html">Como medir a eficácia e resolver os problemas dos desenvolvedores, se você tiver cem</a></li>
<li><a href="../pt469935/index.html">Curso "Fundamentos do trabalho eficaz com a Wolfram Technologies": mais de 13 horas de vídeo aulas, teoria e problemas</a></li>
<li><a href="../pt469939/index.html">Roteador CNC doméstico como alternativa a uma impressora 3D, parte quatro. Conceitos gerais de processamento</a></li>
<li><a href="../pt469941/index.html">Nematóides extremos do lago Mono: nadar em arsênico e sobreviver</a></li>
<li><a href="../pt469945/index.html">É importante que computadores e pessoas vejam o mundo de maneira diferente?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>