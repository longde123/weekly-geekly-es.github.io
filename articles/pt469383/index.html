<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ã ‚ôÇÔ∏è ‚åõÔ∏è Solu√ß√£o hiperconvergente AERODISK vAIR. Base - Sistema de Arquivos ARDFS ü§ûüèæ üíò ü¶Å</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° leitores da Habr. Com este artigo, estamos abrindo um ciclo que abordar√° o sistema hiperconvergente AERODISK vAIR que desenvolvemos. Inicialmente,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Solu√ß√£o hiperconvergente AERODISK vAIR. Base - Sistema de Arquivos ARDFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/469383/"><p><img src="https://habrastorage.org/webt/sk/n3/zh/skn3zhr0fgbcuqfar5ozlg0v2tk.jpeg"></p><br><p>  Ol√° leitores da Habr.  Com este artigo, estamos abrindo um ciclo que abordar√° o sistema hiperconvergente AERODISK vAIR que desenvolvemos.  Inicialmente, quer√≠amos que o primeiro artigo contasse tudo sobre tudo, mas o sistema √© bastante complexo, portanto, comeremos um elefante em partes. </p><br><p>  Vamos come√ßar a hist√≥ria com a hist√≥ria do sistema, aprofundar o sistema de arquivos ARDFS, que √© a base do vAIR, e tamb√©m falar um pouco sobre o posicionamento dessa solu√ß√£o no mercado russo. </p><br><p>  Em artigos futuros, falaremos mais sobre diferentes componentes arquiteturais (cluster, hypervisor, balanceador de carga, sistema de monitoramento etc.), o processo de configura√ß√£o, levantaremos problemas de licenciamento, mostraremos separadamente os testes de falha e, √© claro, escreveremos sobre testes de carga e dimensionamento.  Tamb√©m dedicaremos um artigo √† vers√£o comunit√°ria do vAIR. </p><a name="habracut"></a><br><h2 id="aerodisk---eto-vrode-istoriya-pro-shd-ili-zachem-my-voobsche-nachali-zanimatsya-giperkonvergentom">  Um airdisc √© uma hist√≥ria sobre armazenamento?  Ou por que come√ßamos a hiperconverg√™ncia? </h2><br><p>  Inicialmente, a id√©ia de criar nosso pr√≥prio hiperconvergente surgiu em algum momento do ano de 2010.  Ent√£o n√£o havia Aerodisk e solu√ß√µes similares (sistemas hiperconvergentes em caixas comerciais) no mercado.  Nossa tarefa foi a seguinte: de um conjunto de servidores com discos locais conectados por uma interconex√£o Ethernet, tivemos que fazer armazenamento estendido e executar m√°quinas virtuais e uma rede de software no mesmo local.  Tudo isso era necess√°rio para ser implementado sem sistemas de armazenamento (porque simplesmente n√£o havia dinheiro para armazenamento e sua liga√ß√£o, mas ainda n√£o t√≠nhamos inventado nosso pr√≥prio sistema de armazenamento). </p><br><p>  Tentamos muitas solu√ß√µes de c√≥digo aberto e ainda resolvemos esse problema, mas a solu√ß√£o era muito complicada e dif√≠cil de repetir.  Al√©m disso, essa decis√£o foi da categoria ‚ÄúTrabalhos?  N√£o toque! "  Portanto, tendo resolvido esse problema, n√£o desenvolvemos mais a id√©ia de transformar o resultado do nosso trabalho em um produto completo. </p><br><p> Ap√≥s esse incidente, nos afastamos dessa id√©ia, mas ainda t√≠nhamos a sensa√ß√£o de que esse problema era completamente solucion√°vel, e os benef√≠cios de tal solu√ß√£o eram mais do que √≥bvios.  Posteriormente, os produtos HCI de empresas estrangeiras lan√ßadas apenas confirmaram esse sentimento. </p><br><p>  Portanto, em meados de 2016, retornamos a essa tarefa como parte da cria√ß√£o de um produto completo.  Como ainda n√£o t√≠nhamos rela√ß√µes com investidores, tivemos que comprar um estande de desenvolvimento por nosso dinheiro n√£o muito grande.  Depois de digitar os servidores e comutadores Avito BU-shyh, come√ßamos a trabalhar. </p><br><p><img src="https://habrastorage.org/webt/wy/ir/jr/wyirjr50guvzpnolcvdvniyn2mo.jpeg"></p><br><p>  A principal tarefa inicial foi criar seu pr√≥prio sistema de arquivos, embora simples, mas com capacidade de distribuir dados de forma autom√°tica e uniforme na forma de blocos virtuais no en√©simo n√∫mero de n√≥s do cluster interconectados via Ethernet.  Nesse caso, o FS deve ser bem e facilmente dimensionado e independente dos sistemas adjacentes, ou seja,  ser alienado do vAIR na forma de "apenas armazenamento". </p><br><p><img src="https://habrastorage.org/webt/il/vk/zs/ilvkzsyjkyr6pkcgoqs_ibumyus.jpeg"></p><br><p>  Primeiro conceito VAIR </p><br><p><img src="https://habrastorage.org/webt/h1/e0/pd/h1e0pda3j1ebbxls_cn_gmzm5ug.jpeg"></p><br><p>  Recusamos intencionalmente o uso de solu√ß√µes de c√≥digo aberto prontas para organizar o armazenamento estendido (ceph, gluster, brilho e similares) em favor do nosso desenvolvimento, pois j√° t√≠nhamos muita experi√™ncia com o projeto.  Obviamente, essas solu√ß√µes s√£o maravilhosas e, antes de trabalharmos no Aerodisk, implementamos mais de um projeto de integra√ß√£o com eles.  Mas uma coisa √© realizar a tarefa espec√≠fica de um cliente, treinar funcion√°rios e, possivelmente, adquirir suporte para um grande fornecedor, e outra coisa √© criar um produto facilmente replic√°vel que ser√° usado para v√°rias tarefas, as quais n√≥s, como fornecedor, podemos conhecer a n√≥s mesmos n√≥s n√£o vamos.  Para o segundo objetivo, os produtos de c√≥digo aberto existentes n√£o eram adequados para n√≥s, por isso decidimos ver o sistema de arquivos distribu√≠dos. <br>  Dois anos depois, v√°rios desenvolvedores (que combinaram o trabalho no vAIR com o trabalho no cl√°ssico Storage Engine) alcan√ßaram um certo resultado. </p><br><p>  At√© o ano de 2018, escrevemos o sistema de arquivos mais simples e o complementamos com a liga√ß√£o necess√°ria.  O sistema integrou discos f√≠sicos (locais) de diferentes servidores em um pool simples por meio de uma interconex√£o interna e os "corta" em blocos virtuais. Em seguida, os dispositivos de bloco com graus variados de toler√¢ncia a falhas foram criados a partir de blocos virtuais, nos quais os hipervisores KVM virtuais foram criados e executados carros. </p><br><p>  N√≥s n√£o nos incomodamos com o nome do sistema de arquivos e o chamamos sucintamente de ARDFS (adivinhe como ele descriptografa) </p><br><p>  Esse prot√≥tipo parecia bom (n√£o visualmente, √© claro, n√£o havia design visual na √©poca) e mostrava bons resultados em desempenho e dimensionamento.  Ap√≥s o primeiro resultado real, definimos o rumo do projeto, tendo organizado um ambiente de desenvolvimento completo e uma equipe separada, envolvida apenas no vAIR. </p><br><p>  Naquele momento, a arquitetura geral da solu√ß√£o havia amadurecido, que at√© agora n√£o havia sofrido grandes mudan√ßas. </p><br><h2 id="pogruzhaemsya-v-faylovuyu-sistemu-ardfs">  Mergulhando no sistema de arquivos ARDFS </h2><br><p>  O ARDFS √© a base do vAIR, que fornece armazenamento de failover distribu√≠do de todo o cluster.  Um (mas n√£o o √∫nico) recurso distintivo do ARDFS √© que ele n√£o usa nenhum servidor dedicado adicional para meta e gerenciamento.  Originalmente, isso pretendia simplificar a configura√ß√£o da solu√ß√£o e sua confiabilidade. </p><br><h3 id="struktura-hraneniya">  Estrutura de armazenamento </h3><br><p>  Em todos os n√≥s do cluster, o ARDFS organiza um pool l√≥gico de todo o espa√ßo em disco dispon√≠vel.  √â importante entender que um pool ainda n√£o √© de dados e nem de espa√ßo formatado, mas simplesmente de marca√ß√£o, ou seja,  todos os n√≥s com vAIR instalados quando adicionados ao cluster s√£o automaticamente adicionados ao pool ARDFS compartilhado e os recursos de disco s√£o automaticamente compartilhados em todo o cluster (e dispon√≠veis para armazenamento futuro de dados).  Essa abordagem permite adicionar e remover n√≥s em tempo real sem nenhum impacto s√©rio em um sistema j√° em execu√ß√£o.  I.e.  o sistema √© muito f√°cil de escalar com "tijolos", adicionando ou removendo n√≥s no cluster, se necess√°rio. </p><br><p>  Os discos virtuais (objetos de armazenamento para m√°quinas virtuais) s√£o adicionados √† parte superior do pool ARDFS, constru√≠do a partir de blocos virtuais de 4 megabytes de tamanho.  Discos virtuais armazenam dados diretamente.  No n√≠vel do disco virtual, tamb√©m √© definido um esquema de toler√¢ncia a falhas. </p><br><p>  Como voc√™ deve ter adivinhado, para a toler√¢ncia a falhas do subsistema de disco, n√£o usamos o conceito de RAID (matriz redundante de discos independentes), mas usamos RAIN (matriz redundante de n√≥s independentes).  I.e.  a toler√¢ncia a falhas √© medida, automatizada e gerenciada com base em n√≥s, n√£o em discos.  Os discos, √© claro, tamb√©m s√£o um objeto de armazenamento; eles, como todo o resto, s√£o monitorados, √© poss√≠vel executar todas as opera√ß√µes padr√£o com eles, incluindo a cria√ß√£o de RAID de hardware local, mas o cluster opera com n√≥s. </p><br><p>  Em uma situa√ß√£o em que voc√™ realmente deseja RAID (por exemplo, um cen√°rio que oferece suporte a v√°rias falhas em pequenos clusters), nada impede que voc√™ use controladores RAID locais e fa√ßa armazenamento estendido e uma arquitetura RAIN no topo.  Esse cen√°rio √© bastante din√¢mico e √© suportado por n√≥s; portanto, falaremos sobre isso em um artigo sobre cen√°rios t√≠picos para o uso do vAIR. </p><br><h3 id="shemy-otkazoustoychivosti-hranilischa">  Esquemas de failover de armazenamento </h3><br><p>  Pode haver dois esquemas de resili√™ncia de disco virtual vAIR: </p><br><p>  1) Fator de replica√ß√£o ou apenas replica√ß√£o - esse m√©todo de toler√¢ncia a falhas √© simples "como um pau e uma corda".  A replica√ß√£o s√≠ncrona entre n√≥s com um fator de 2 (2 c√≥pias por cluster) ou 3 (3 c√≥pias, respectivamente) √© executada.  O RF-2 permite que um disco virtual resista a uma falha de um n√≥ em um cluster, mas "consome" metade do volume utiliz√°vel, e o RF-3 suporta uma falha de 2 n√≥s em um cluster, mas reservar√° 2/3 do volume utiliz√°vel para suas necessidades.  Esse esquema √© muito semelhante ao RAID-1, ou seja, um disco virtual configurado no RF-2 √© resistente a falhas de qualquer n√≥ do cluster.  Nesse caso, os dados estar√£o corretos e at√© a E / S n√£o ser√° interrompida.  Quando um n√≥ em queda retorna √† opera√ß√£o, a recupera√ß√£o / sincroniza√ß√£o autom√°tica de dados ser√° iniciada. </p><br><p>  A seguir, s√£o apresentados exemplos da distribui√ß√£o dos dados de RF-2 e RF-3 no modo normal e em uma situa√ß√£o de falha. </p><br><p>  Temos uma m√°quina virtual com capacidade de 8 MB de dados exclusivos (√∫teis) que s√£o executados em 4 n√≥s vAIR.  √â claro que, na realidade, √© improv√°vel que exista uma quantidade t√£o pequena, mas para um esquema que reflete a l√≥gica do ARDFS, este exemplo √© mais compreens√≠vel.  AB s√£o blocos virtuais de 4 MB que cont√™m dados exclusivos da m√°quina virtual.  Com o RF-2, duas c√≥pias desses blocos A1 + A2 e B1 + B2 s√£o criadas, respectivamente.  Esses blocos s√£o "dispostos" pelos n√≥s, evitando a interse√ß√£o dos mesmos dados no mesmo n√≥, ou seja, a c√≥pia A1 n√£o estar√° na mesma nota que a c√≥pia A2.  Com B1 e B2 √© semelhante. </p><br><p><img src="https://habrastorage.org/webt/ho/xm/oh/hoxmohhemj_whmr38pvgfyfousm.png"></p><br><p>  No caso de uma falha de um dos n√≥s (por exemplo, n√≥ 3, que cont√©m uma c√≥pia de B1), essa c√≥pia √© ativada automaticamente no n√≥ em que n√£o h√° c√≥pia de sua c√≥pia (ou seja, c√≥pia B2). </p><br><p><img src="https://habrastorage.org/webt/ex/xl/o4/exxlo4frqr3crhwbh_orvlcwl9g.png"></p><br><p>  Assim, o disco virtual (e VMs, respectivamente) sobreviver√° facilmente √† falha de um n√≥ no esquema RF-2. </p><br><p>  Um circuito com replica√ß√£o, com sua simplicidade e confiabilidade, sofre da mesma dor do RAID1 - h√° pouco espa√ßo utiliz√°vel. </p><br><p>  2) A codifica√ß√£o de apagamento ou exclus√£o (tamb√©m conhecida como ‚Äúcodifica√ß√£o redundante‚Äù, ‚Äúcodifica√ß√£o apagada‚Äù ou ‚Äúc√≥digo de redund√¢ncia‚Äù) existe apenas para resolver o problema acima.  O EC √© um esquema de redund√¢ncia que fornece alta disponibilidade de dados com menos sobrecarga de disco em compara√ß√£o com a replica√ß√£o.  O princ√≠pio de opera√ß√£o desse mecanismo √© semelhante ao RAID 5, 6, 6P. </p><br><p>  Ao codificar, o processo EC divide o bloco virtual (4 MB por padr√£o) em v√°rias "partes de dados" menores, dependendo do esquema EC (por exemplo, um esquema 2 + 1 divide cada bloco de 4 MB em 2 partes de 2 MB).  Al√©m disso, esse processo gera "blocos de paridade" para "peda√ßos de dados" de n√£o mais do que uma das partes separadas anteriormente.  Ao decodificar, o EC gera as pe√ßas ausentes, lendo os dados "sobreviventes" em todo o cluster. </p><br><p>  Por exemplo, um disco virtual com um esquema EC 2 + 1, implementado em 4 n√≥s de um cluster, pode suportar facilmente uma falha de n√≥ √∫nico em um cluster da mesma maneira que o RF-2.  Ao mesmo tempo, os custos indiretos ser√£o mais baixos, em particular, o fator de capacidade com RF-2 √© 2 e, com EC 2 + 1, ser√° 1,5. </p><br><p>  Se for mais f√°cil descrever, o resultado final √© que o bloco virtual √© dividido em 2 a 8 (por que de 2 a 8 veja abaixo) "peda√ßos" e, para esses peda√ßos, os "peda√ßos" de paridade do mesmo volume s√£o calculados. </p><br><p>  Como resultado, os dados e a paridade s√£o distribu√≠dos igualmente em todos os n√≥s do cluster.  Ao mesmo tempo, como na replica√ß√£o, o ARDFS distribui automaticamente os dados entre os n√≥s de forma a impedir o armazenamento dos mesmos dados (c√≥pias dos dados e sua paridade) em um n√≥, a fim de eliminar a chance de perda de dados devido ao fato de que os dados e seus dados de repente, a paridade acabar√° no mesmo n√≥ de armazenamento, o que falhar√°. </p><br><p>  Abaixo est√° um exemplo, com a mesma m√°quina virtual em 8 MB e 4 n√≥s, mas j√° com o esquema EC 2 + 1. </p><br><p>  Os blocos A e B s√£o divididos em duas partes de 2 MB cada (duas porque 2 + 1), ou seja, A1 + A2 e B1 + B2.  Diferentemente da r√©plica, A1 n√£o √© uma c√≥pia de A2, √© um bloco virtual A, dividido em duas partes, tamb√©m com o bloco B. No total, obtemos dois conjuntos de 4 MB, cada um contendo duas pe√ßas de dois megabytes.  Al√©m disso, para cada um desses conjuntos, a paridade √© calculada com um volume n√£o superior a uma parte (ou seja, 2 MB), obtemos mais + 2 partes de paridade (AP e BP).  Total, temos dados 4x2 + paridade 2x2. </p><br><p>  Em seguida, as partes s√£o "dispostas" pelos n√≥s para que os dados n√£o se sobreponham √† sua paridade.  I.e.  A1 e A2 n√£o estar√£o no mesmo n√≥ do AP. </p><br><p><img src="https://habrastorage.org/webt/s9/xi/om/s9xiombqm4ep-bos8lntf8kjq4s.png"></p><br><p>  No caso de uma falha de um n√≥ (por exemplo, tamb√©m o terceiro), o bloco ca√≠do B1 ser√° automaticamente restaurado da paridade BP, que √© armazenada no n√≥ n¬∫ 2, e ser√° ativado no n√≥ onde n√£o h√° paridade B, ou seja,  peda√ßos de BP.  Neste exemplo, este √© o n√≥ # 1 </p><br><p><img src="https://habrastorage.org/webt/wu/gk/dr/wugkdrhgfund89fswagb7wc4iba.png"></p><br><p>  Estou certo de que o leitor tem uma pergunta: </p><br><blockquote>  "Tudo o que voc√™ descreveu foi implementado h√° muito tempo pelos concorrentes e pelas solu√ß√µes de c√≥digo aberto. Qual √© a diferen√ßa entre sua implementa√ß√£o do EC no ARDFS?" </blockquote><p>  E ent√£o haver√° caracter√≠sticas interessantes do trabalho do ARDFS. </p><br><h3 id="erasure-coding-s-uporom-na-gibkost">  Codifica√ß√£o de apagamento com √™nfase na flexibilidade </h3><br><p>  Inicialmente, fornecemos um esquema EC X + Y bastante flex√≠vel, em que X √© igual a um n√∫mero de 2 a 8 e Y √© igual a um n√∫mero de 1 a 8, mas sempre menor ou igual a X. Esse esquema √© fornecido para flexibilidade.  Aumentar o n√∫mero de dados (X) nos quais a unidade virtual √© dividida permite reduzir a sobrecarga, ou seja, aumentar o espa√ßo utiliz√°vel. <br>  Um aumento no n√∫mero de blocos de paridade (Y) aumenta a confiabilidade do disco virtual.  Quanto maior o valor Y, mais n√≥s no cluster podem falhar.  Obviamente, um aumento na paridade reduz a quantidade de capacidade utiliz√°vel, mas isso √© uma cobran√ßa pela confiabilidade. </p><br><p>  A depend√™ncia do desempenho em circuitos CE √© quase direta: quanto mais "pe√ßas", menor o desempenho, aqui, √© claro, voc√™ precisa de uma apar√™ncia equilibrada. </p><br><p>  Essa abordagem permite aos administradores a maneira mais flex√≠vel de configurar o armazenamento estendido.  Dentro do pool ARDFS, voc√™ pode usar qualquer esquema de toler√¢ncia a falhas e suas combina√ß√µes, o que tamb√©m √©, em nossa opini√£o, muito √∫til. </p><br><p>  A tabela abaixo compara v√°rios circuitos de RF e EC (nem todos poss√≠veis). </p><br><p><img src="https://habrastorage.org/webt/g0/vj/oj/g0vjojekzkzjv2xbxbdhyw1fjy0.png"></p><br><p>  A tabela mostra que mesmo a combina√ß√£o mais "terry" do EC 8 + 7, que permite a perda de at√© 7 n√≥s por vez em um cluster, "consome" menos espa√ßo √∫til (1,875 versus 2) do que a replica√ß√£o padr√£o e protege 7 vezes melhor, o que torna esse mecanismo de prote√ß√£o, embora mais complexo, mas muito mais atraente em situa√ß√µes em que voc√™ precisa garantir a m√°xima confiabilidade nas condi√ß√µes de falta de espa√ßo em disco.  Ao mesmo tempo, voc√™ precisa entender que cada ‚Äúmais‚Äù a X ou Y ser√° uma sobrecarga adicional para a produtividade; portanto, voc√™ deve escolher com muito cuidado no tri√¢ngulo entre confiabilidade, economia e desempenho.  Por esse motivo, dedicaremos um artigo separado √† codifica√ß√£o de exclus√£o de tamanho. </p><br><p><img src="https://habrastorage.org/webt/5v/2o/-j/5v2o-jpchgy8rqib7bugk35vmj4.png"></p><br><h3 id="nadezhnost-i-avtonomnost-faylovoy-sistemy">  Confiabilidade e autonomia do sistema de arquivos </h3><br><p>  O ARDFS √© executado localmente em todos os n√≥s do cluster e os sincroniza por seus pr√≥prios meios atrav√©s de interfaces Ethernet dedicadas.  Um ponto importante √© que o ARDFS sincroniza independentemente n√£o apenas dados, mas tamb√©m metadados relacionados ao armazenamento.  Enquanto trabalhamos no ARDFS, estudamos simultaneamente v√°rias solu√ß√µes existentes e descobrimos que muitas realizam a meta-sincroniza√ß√£o do sistema de arquivos usando um DBMS externo distribu√≠do, que tamb√©m usamos para sincronizar, mas apenas configura√ß√µes, n√£o os metadados do FS (sobre este e outros subsistemas relacionados no pr√≥ximo artigo). </p><br><p>  A sincroniza√ß√£o dos metadados do FS usando um DBMS externo √©, obviamente, uma solu√ß√£o funcional, mas a consist√™ncia dos dados armazenados no ARDFS dependeria do DBMS externo e de seu comportamento (e ela, francamente, √© uma dama caprichosa), o que √© ruim em nossa opini√£o.  Porque  Se os metadados do FS estiverem danificados, os dados do FS tamb√©m poder√£o ser considerados "adeus", por isso decidimos seguir um caminho mais complicado, mas confi√°vel. </p><br><p>  Criamos o subsistema de sincroniza√ß√£o de metadados para o ARDFS de forma independente e ele vive completamente independente dos subsistemas adjacentes.  I.e.  nenhum outro subsistema pode corromper os dados do ARDFS.  Em nossa opini√£o, esta √© a maneira mais confi√°vel e correta, e √© realmente assim - o tempo dir√°.  Al√©m disso, com essa abordagem, uma vantagem adicional aparece.  O ARDFS pode ser usado independentemente do vAIR, assim como o armazenamento estendido, que certamente usaremos em produtos futuros. </p><br><p>  Como resultado, tendo desenvolvido o ARDFS, obtivemos um sistema de arquivos flex√≠vel e confi√°vel que permite a voc√™ escolher onde voc√™ pode economizar capacidade ou doar tudo no desempenho ou tornar o armazenamento altamente confi√°vel por uma taxa moderada, mas reduzindo os requisitos de desempenho. </p><br><p>  Juntamente com uma pol√≠tica de licenciamento simples e um modelo de entrega flex√≠vel (olhando para o futuro, √© licenciado pelo vAIR por n√≥s e √© entregue por software ou como um PAC), isso permite que voc√™ adapte com precis√£o a solu√ß√£o aos mais diferentes requisitos dos clientes e, no futuro, √© f√°cil manter esse equil√≠brio. </p><br><h2 id="komu-eto-chudo-nuzhno">  Quem precisa desse milagre? </h2><br><p>  Por um lado, podemos dizer que j√° existem players no mercado que tomam decis√µes s√©rias no campo da hiperconverg√™ncia e para onde estamos indo.  Esta afirma√ß√£o parece verdadeira, MAS ... </p><br><p>  Por outro lado, saindo para o campo e se comunicando com os clientes, n√≥s e nossos parceiros vemos que esse n√£o √© o caso.  Existem muitos problemas para os hiperconvergentes, em algum lugar as pessoas simplesmente n√£o sabiam que existiam tais solu√ß√µes, em algum lugar parecia caro, em algum lugar havia testes sem √™xito de solu√ß√µes alternativas, mas em algum lugar eles geralmente pro√≠bem a compra por causa de san√ß√µes.  Em geral, o campo n√£o era arado, ent√£o fomos aumentar as terras virgens))). </p><br><h3 id="kogda-shd-luchshe-chem-gks">  Quando o armazenamento √© melhor que o GCS? </h3><br><p>  Durante o trabalho com o mercado, muitas vezes nos perguntam quando √© melhor usar o esquema cl√°ssico com sistemas de armazenamento e quando √© hiperconvergente?  Muitas empresas - fabricantes de GCS (especialmente aquelas que n√£o t√™m armazenamento em seu portf√≥lio) afirmam: "O armazenamento √© mais duradouro, apenas hiperconvergente!"  Esta √© uma afirma√ß√£o ousada, mas n√£o reflete bem a realidade. </p><br><p>  Na verdade, o mercado de armazenamento, de fato, nada em busca de solu√ß√µes hiperconvergentes e similares, mas sempre existe um "mas". </p><br><p>  Em primeiro lugar, os datacenters e as infraestruturas de TI constru√≠das de acordo com o esquema cl√°ssico com sistemas de armazenamento n√£o podem ser facilmente reconstru√≠dos dessa maneira; portanto, a moderniza√ß√£o e a conclus√£o de tais infraestruturas ainda √© um legado de 5 a 7 anos. </p><br><p>  Em segundo lugar, essas infraestruturas que est√£o sendo constru√≠das agora em grande parte (ou seja, a Federa√ß√£o Russa) est√£o sendo constru√≠das de acordo com o esquema cl√°ssico usando sistemas de armazenamento e n√£o porque as pessoas n√£o conhecem hiperconvergente, mas porque o mercado hiperconvergente √© novo, solu√ß√µes e padr√µes ainda n√£o foram estabelecidos , Os funcion√°rios de TI ainda n√£o foram treinados, h√° pouca experi√™ncia e precisamos construir data centers aqui e agora.  E essa tend√™ncia √© de mais 3-5 anos (e depois outro legado, veja o par√°grafo 1). </p><br><p>  Em terceiro lugar, uma limita√ß√£o puramente t√©cnica em pequenos atrasos adicionais de 2 milissegundos por grava√ß√£o (excluindo o cache local, √© claro), que s√£o taxas pelo armazenamento distribu√≠do. </p><br><p>  Bem, n√£o vamos esquecer o uso de grandes servidores f√≠sicos que adoram o dimensionamento vertical do subsistema de disco. </p><br><p>  Existem muitas tarefas necess√°rias e populares em que o sistema de armazenamento se comporta melhor que o GCS.  Aqui, √© claro, os fabricantes que n√£o possuem sistemas de armazenamento em seu portf√≥lio de produtos discordar√£o de n√≥s, mas estamos prontos para argumentar razoavelmente.  Obviamente, n√≥s, como desenvolvedores de ambos os produtos em uma das futuras publica√ß√µes, faremos definitivamente uma compara√ß√£o entre sistemas de armazenamento e GCS, onde demonstraremos claramente o que √© melhor sob quais condi√ß√µes. </p><br><h3 id="a-gde-giperkonvergentnye-resheniya-budut-rabotat-luchshe-shd">  E onde as solu√ß√µes hiperconvergentes funcionar√£o melhor que os sistemas de armazenamento? </h3><br><p>  Com base nas teses acima, existem tr√™s conclus√µes √≥bvias: </p><br><ol><li>  Onde 2 milissegundos adicionais de atrasos na grava√ß√£o que ocorrem de maneira est√°vel em qualquer produto (agora n√£o estamos falando de sint√©ticos, voc√™ pode mostrar nanossegundos em sint√©ticos) s√£o n√£o cr√≠ticos, o que ser√° um hiperconvergente. </li><li>  Onde a carga de grandes servidores f√≠sicos pode ser transformada em muitos pequenos servidores virtuais e distribu√≠da por n√≥s, o hiperconvergente tamb√©m funcionar√° bem l√°. </li><li>  Onde a escala horizontal √© mais importante que a vertical, o GCS tamb√©m funcionar√° bem l√°. </li></ol><br><h3 id="kakie-eto-resheniya">  Quais s√£o essas solu√ß√µes? </h3><br><ol><li>  Todos os servi√ßos de infraestrutura padr√£o (servi√ßo de diret√≥rio, correio, EDS, servidores de arquivos, pequenos ou m√©dios sistemas ERP e BI, etc.).  Chamamos isso de "computa√ß√£o geral". </li><li>  A infraestrutura dos provedores de nuvem, onde √© necess√°rio expandir r√°pida e padronizar horizontalmente e "dividir" facilmente um grande n√∫mero de m√°quinas virtuais para os clientes. </li><li>  Infraestrutura de √°rea de trabalho virtual (VDI), onde muitas pequenas m√°quinas virtuais de usu√°rio s√£o iniciadas e "flutuam" silenciosamente dentro de um cluster uniforme. </li><li>  ,       , ,       15-20  . </li><li>    (big data-, ). ,     ¬´¬ª,  ¬´¬ª. </li><li>  ,     ,    ,   . </li></ol><br><p>          AERODISK vAIR       ( ). ,   , ..     . </p><br><h3 id="itak"> ‚Ä¶ </h3><br><p>        ,           . </p><br><p>   ,    . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt469383/">https://habr.com/ru/post/pt469383/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt469371/index.html">Descri√ß√£o da abordagem para organizar e testar c√≥digo usando o Redux Thunk</a></li>
<li><a href="../pt469373/index.html">Os resultados do projeto para criar uma interface neural para pacientes completamente paralisados ‚Äã‚Äãquestionaram</a></li>
<li><a href="../pt469375/index.html">Por que Mozilla, Coil e Creative Commons destinam US $ 100 milh√µes para projetos de c√≥digo aberto?</a></li>
<li><a href="../pt469379/index.html">Aplica√ß√£o de m√©todos formais de valida√ß√£o de modelo para UI</a></li>
<li><a href="../pt469381/index.html">Agones, crie um servidor de jogos multiusu√°rio. Arquitetura e Instala√ß√£o</a></li>
<li><a href="../pt469387/index.html">A hist√≥ria de um "desenvolvedor" ou como um iniciante deve escrever um aplicativo para iOS</a></li>
<li><a href="../pt469389/index.html">Parametriza√ß√£o por uma rede neural de um modelo f√≠sico para resolver um problema de otimiza√ß√£o topol√≥gica</a></li>
<li><a href="../pt469391/index.html">Interfaces de √°udio: o som como fonte de informa√ß√£o na estrada, no escrit√≥rio e no c√©u</a></li>
<li><a href="../pt469393/index.html">Reda√ß√£o do Flare-On 2019</a></li>
<li><a href="../pt469395/index.html">Onde e como usar v√°rias colunas (colunas CSS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>