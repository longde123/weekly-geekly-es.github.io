<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è∏Ô∏è üßòüèø üöî Un nuevo enfoque para comprender el pensamiento autom√°tico ‚õπüèø üóæ üìê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las redes neuronales son conocidas por su incomprensibilidad: la computadora puede dar una buena respuesta, pero no puede explicar qu√© la llev√≥ a esta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Un nuevo enfoque para comprender el pensamiento autom√°tico</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440426/"><h3>  Las redes neuronales son conocidas por su incomprensibilidad: la computadora puede dar una buena respuesta, pero no puede explicar qu√© la llev√≥ a esta conclusi√≥n.  Bin Kim est√° desarrollando un "traductor humano" para que si la inteligencia artificial se descompone, podamos entenderlo. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/069/0a0/2c1/0690a02c197e3af97ec6e5cec9754fd6.jpg"><br>  <i>Bean Kim, investigadora de Google Brain, est√° desarrollando una forma de cuestionar un sistema de aprendizaje autom√°tico sobre sus decisiones.</i> <br><br>  Si el m√©dico le dice que necesita cirug√≠a, querr√° averiguar por qu√©, y esperar√° que su explicaci√≥n le parezca significativa, incluso si no ha recibido capacitaci√≥n como m√©dico.  Been Kim, investigador de Google Brain, cree que deber√≠amos poder esperar lo mismo de la inteligencia artificial (IA).  Ella es especialista en aprendizaje autom√°tico "interpretado" (MO), y quiere crear una IA que pueda explicar sus acciones a cualquiera. <br><a name="habracut"></a><br>  Desde hace diez a√±os, la tecnolog√≠a de las redes neuronales detr√°s de la IA comenz√≥ a extenderse cada vez m√°s, fue capaz de transformar todos los procesos, desde ordenar el correo electr√≥nico hasta encontrar nuevos medicamentos, gracias a su capacidad de aprender de los datos y buscar patrones en ellos.  Pero esta habilidad tiene una trampa inexplicable: la propia complejidad que permite a las redes neuronales modernas con capacitaci√≥n en profundidad aprender con √©xito c√≥mo conducir un autom√≥vil y reconocer el fraude con un seguro hace que sea casi imposible para los expertos comprender los principios de su trabajo.  Si una red neuronal est√° capacitada para buscar pacientes con riesgo de c√°ncer de h√≠gado o esquizofrenia, y un sistema llamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Patient se</a> lanz√≥ en el Hospital Mount Sinai en Nueva York en 2015, entonces no hay forma de averiguar cu√°les los datos presentan la red neuronal "presta atenci√≥n".  Este "conocimiento" se extiende sobre muchas capas de neuronas artificiales, cada una de las cuales tiene conexiones con cientos o miles de otras neuronas. <br><br>  A medida que m√°s y m√°s industrias intentan automatizar o mejorar sus procesos de toma de decisiones utilizando IA, este problema de "caja negra" parece ser menos una falla tecnol√≥gica y m√°s una falla fundamental.  Un proyecto de DARPA llamado XAI (abreviatura de "IA explicable", IA explicable) est√° explorando activamente este problema, y ‚Äã‚Äãla interpretabilidad se est√° moviendo desde las primeras l√≠neas de investigaci√≥n en el campo de MO m√°s cerca de su centro.  "La IA est√° en un momento cr√≠tico cuando nosotros, la humanidad, estamos tratando de averiguar si esta tecnolog√≠a es adecuada para nosotros", dice Kim.  "Si no resolvemos el problema de la interpretabilidad, creo que no podremos seguir adelante con esta tecnolog√≠a, y tal vez simplemente lo rechacemos". <br><br>  Kim y sus colegas de Google Brain desarrollaron recientemente el sistema Pruebas con vectores de activaci√≥n de conceptos (TCAV), que ella describe como un traductor humano que permite al usuario hacer una pregunta al recuadro negro de IA sobre cu√°nto estuvo involucrado cierto concepto de alto nivel en la toma de decisiones.  Por ejemplo, si el sistema MO est√° capacitado para encontrar im√°genes de cebra, una persona podr√≠a pedirle a TCAV que describa cu√°nto aporta el concepto de "franjas" al proceso de toma de decisiones. <br><br>  El TCAV se prob√≥ inicialmente en modelos entrenados para reconocer im√°genes, pero tambi√©n funciona con modelos dise√±ados para el procesamiento de texto o ciertas tareas de visualizaci√≥n de datos, por ejemplo, gr√°ficos EEG.  "Es generalizado y simple: se puede conectar a muchos modelos diferentes", dice Kim. <br><br>  Quanta habl√≥ con Kim sobre lo que significa la interpretabilidad, qui√©n la necesita y por qu√© es importante. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4c8/f00/6c4/4c8f006c4ccacab3d668b366f5cbecee.jpg"><br><br>  <b>Usted en su carrera se ha centrado en la "interpretabilidad" para el MO.</b>  <b>Pero, ¬øqu√© significa exactamente este t√©rmino?</b> <br><br>  La interpretabilidad tiene dos ramas.  Una es la interpretabilidad para la ciencia: si considera la red neuronal como un objeto de estudio, puede realizar experimentos cient√≠ficos sobre ella para comprender realmente todos los entresijos del modelo, las razones de su reacci√≥n, etc. <br><br>  La segunda rama, en la que concentro principalmente mis esfuerzos, es la interpretabilidad para crear una IA capaz de responder preguntas.  No es necesario que comprenda cada peque√±o detalle del modelo.  Pero nuestro objetivo es comprender lo suficiente para que esta herramienta pueda usarse de manera segura. <br><br>  <b>Pero, ¬øc√≥mo se puede creer en un sistema, si no se comprende completamente c√≥mo funciona?</b> <br><br>  Te dar√© una analog√≠a.  Supongamos que en mi patio hay un √°rbol que quiero cortar.  Tengo una motosierra para esto.  No entiendo exactamente c√≥mo funciona una motosierra.  Pero las instrucciones dicen: "Algo que debe manejarse con cuidado para no cortarse".  Al recibir instrucciones, es mejor que use una motosierra en lugar de una sierra manual; esta √∫ltima es m√°s f√°cil de entender, pero tendr√≠a que verla durante cinco horas. <br><br>  <b>Entiende lo que significa "cortar", incluso si no sabe todo sobre el mecanismo que lo hace posible.</b> <br><br>  Si  El prop√≥sito de la segunda rama de la interpretabilidad es el siguiente: ¬øpodemos entender la herramienta lo suficiente como para ser seguro de usar?  Y podemos crear esta comprensi√≥n confirmando que el conocimiento humano √∫til se refleja en el instrumento. <br><br>  <b>¬øPero c√≥mo la "reflexi√≥n del conocimiento humano" hace que la caja negra de IA sea m√°s comprensible?</b> <br><br>  Aqu√≠ hay otro ejemplo.  Si el m√©dico utiliza el modelo MO para hacer un diagn√≥stico de c√°ncer, deber√° saber que el modelo no selecciona simplemente una correlaci√≥n aleatoria en los datos que no necesitamos.  Una forma de verificar esto es confirmar que el modelo MO hace aproximadamente lo mismo que har√≠a el m√©dico.  Es decir, para mostrar que el conocimiento diagn√≥stico del m√©dico se refleja en el modelo. <br><br>  Por ejemplo, si un m√©dico busca una instancia celular adecuada para el diagn√≥stico de c√°ncer, buscar√° algo llamado "gl√°ndula fusionada".  Tambi√©n tendr√° en cuenta indicadores como la edad del paciente y si ha recibido quimioterapia en el pasado.  Estos factores o conceptos ser√°n tomados en cuenta por un m√©dico que intente diagnosticar el c√°ncer.  Si podemos demostrar que el modelo MO tambi√©n llama la atenci√≥n sobre ellos, entonces el modelo ser√° m√°s comprensible, ya que reflejar√° el conocimiento humano de los m√©dicos. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/8Bi-EhFPSLk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  <b>De esto trata TCAV: ¬øqu√© conceptos de alto nivel utiliza el modelo MO para la toma de decisiones?</b> <br><br>  Si  Antes de esto, los m√©todos de interpretaci√≥n explicaban solo lo que hace la red neuronal en t√©rminos de "caracter√≠sticas de entrada".  ¬øQu√© significa esto?  Si tiene una imagen, cada uno de sus p√≠xeles ser√° una funci√≥n de entrada.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Yang Lekun</a> (pionero del aprendizaje profundo, director de investigaci√≥n de IA en Facebook), dijo que considera que estos modelos son s√∫per interpretables, ya que puede observar cada nodo de la red neuronal y ver los valores num√©ricos para cada una de las caracter√≠sticas de entrada.  Para las computadoras, esto puede ser adecuado, pero la gente piensa de manera diferente.  No te estoy diciendo "Mira los p√≠xeles de 100 a 200, sus valores RGB son 0.2 y 0.3".  Yo digo: "Esta es una imagen de un perro muy peludo".  Las personas se comunican de esta manera, a trav√©s de conceptos. <br><br>  <b>¬øC√≥mo se traduce TCAV entre las caracter√≠sticas y los conceptos de entrada?</b> <br><br>  Volvamos al ejemplo de un m√©dico que usa el modelo MO, que ya ha sido entrenado para clasificar im√°genes de muestras de c√©lulas seg√∫n el c√°ncer.  Usted, como m√©dico, necesita descubrir qu√© tan importante fue el concepto de "gl√°ndulas fusionadas" para que el modelo haga predicciones positivas para el c√°ncer.  Primero, recopila, digamos, 20 im√°genes que muestran ejemplos de gl√°ndulas fusionadas.  Luego conecta estos ejemplos etiquetados al modelo. <br><br>  Entonces, TCAV dentro de s√≠ mismo lleva a cabo el llamado  "Control de sensibilidad".  Cuando agregamos estas im√°genes etiquetadas de las gl√°ndulas fusionadas, ¬øcu√°nto aumenta la probabilidad de una predicci√≥n positiva de c√°ncer?  La respuesta se puede estimar por un n√∫mero de 0 a 1. Y estos ser√°n sus puntos en TCAV.  Si la probabilidad aumentaba, este concepto era importante para el modelo.  Si no, este concepto no es importante. <br><br>  <b>"Concepto" es un t√©rmino vago.</b>  <b>¬øHay alg√∫n concepto que no funcione con TCAV?</b> <br><br>  Si no puede describir un concepto usando un subconjunto de su conjunto de datos, entonces no funcionar√°.  Si su modelo MO est√° entrenado en im√°genes, entonces el concepto debe expresarse visualmente.  Si, por ejemplo, quiero expresar visualmente el concepto de amor, ser√° bastante dif√≠cil de hacer. <br><br>  Tambi√©n verificamos cuidadosamente el concepto.  Tenemos un procedimiento de verificaci√≥n estad√≠stica que rechaza el vector de concepto si tiene un efecto equivalente al azar en el modelo.  Si su concepto no pasa esta prueba, entonces TCAV le dir√°: "No s√©, este concepto no parece algo importante para el modelo". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e7d/2e4/bb5/e7d2e4bb5cb0093890c6938abdb5acdc.jpg"><br><br>  <b>¬øEl proyecto TCAV est√° m√°s centrado en generar confianza en IA que en generalizar su comprensi√≥n?</b> <br><br>  No, y explicar√© por qu√©, ya que esta diferencia es muy sutil. <br><br>  De muchos estudios en el campo de la ciencia cognitiva y la psicolog√≠a, sabemos que las personas son muy confiables.  Esto significa que es muy f√°cil enga√±ar a una persona oblig√°ndola a creer en algo.  El objetivo de la interpretabilidad de MO es lo contrario.  Consiste en informar a una persona que no es seguro usar un sistema en particular.  El objetivo es descubrir la verdad.  Por lo tanto, "confianza" no es la palabra correcta. <br><br>  <b>Entonces, ¬øel objetivo de la interpretabilidad es descubrir posibles fallas en el razonamiento de IA?</b> <br><br>  Si exactamente. <br><br>  <b>¬øC√≥mo puede ella revelar los defectos?</b> <br><br>  El TCAV se puede usar para hacerle una pregunta al modelo sobre conceptos que no est√°n relacionados con el campo de investigaci√≥n.  Volviendo al ejemplo de los m√©dicos que usan IA para predecir la probabilidad de c√°ncer.  Los m√©dicos pueden pensar de repente: ‚ÄúAparentemente, la m√°quina ofrece predicciones positivas de la presencia de c√°ncer para muchas im√°genes en las que el color se desplaza ligeramente al azul.  Creemos que este factor no debe tenerse en cuenta ".  Y si obtienen una alta puntuaci√≥n TCAV para azul, significa que encontraron un problema en su modelo MO. <br><br>  <b>TCAV est√° dise√±ado para colgarse en sistemas de IA existentes que no se pueden interpretar.</b>  <b>¬øPor qu√© no hacer inmediatamente sistemas interpretados en lugar de cajas negras?</b> <br><br>  Existe una rama del estudio de la interpretabilidad, que se centra en la creaci√≥n de modelos inicialmente interpretados que reflejan el razonamiento de una persona.  Pero creo que s√≠: ahora ya estamos llenos de modelos de IA listos para usar que ya se utilizan para resolver problemas importantes, y al crearlos, inicialmente no pensamos en la interpretabilidad.  Muy f√°cil de comer.  ¬°Muchos de ellos trabajan en Google!  Puede decir: "La interpretabilidad es tan √∫til que nos permite crear otro modelo para que reemplace el que tiene".  Pues buena suerte. <br><br>  ¬øY luego qu√© hacer?  Todav√≠a tenemos que pasar por este momento crucial para decidir si esta tecnolog√≠a nos es √∫til o no.  Por lo tanto, estoy trabajando en m√©todos de interpretabilidad posteriores al entrenamiento.  Si alguien le dio un modelo y usted no puede cambiarlo, ¬øc√≥mo aborda la tarea de generar explicaciones de su comportamiento para que pueda usarlo de manera segura?  Esto es exactamente lo que hace TCAV. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/05d/9ba/522/05d9ba52265cd534871c8f62f4ce05f7.jpg"><br><br>  <b>TCAV permite a las personas preguntar a AI sobre la importancia de ciertos conceptos.</b>  <b>Pero, ¬øqu√© sucede si no sabemos qu√© preguntar? ¬øQu√© sucede si queremos que la IA simplemente explique?</b> <br><br>  En este momento estamos trabajando en un proyecto que puede encontrar autom√°ticamente conceptos para usted.  Lo llamamos DTCAV, el TCAV de apertura.  Pero creo que el principal problema de interpretabilidad es que las personas participan en este proceso y que permitimos que las personas y las m√°quinas se comuniquen. <br><br>  En muchos casos, cuando se trabaja con aplicaciones de las que depende mucho, los expertos en un campo en particular ya tienen una lista de conceptos que son importantes para ellos.  En Google Brain nos enfrentamos constantemente a esto en las aplicaciones m√©dicas de la IA.  No necesitan un conjunto de conceptos; quieren proporcionar modelos conceptuales que les resulten interesantes.  Estamos trabajando con un m√©dico que trata la retinopat√≠a diab√©tica, la enfermedad ocular, y cuando le contamos sobre el TCAV, estaba muy feliz porque ya ten√≠a muchas hip√≥tesis sobre lo que el modelo puede hacer, y ahora puede verificar todas las preguntas que surgieron.  Esta es una gran ventaja, y una forma muy centrada en el usuario para implementar el aprendizaje autom√°tico colaborativo. <br><br>  <b>Crees que sin interpretabilidad, la humanidad simplemente puede abandonar la tecnolog√≠a de IA.</b>  <b>Teniendo en cuenta las oportunidades que tiene, ¬ørealmente eval√∫a esa opci√≥n como real?</b> <br><br>  Si  Esto es exactamente lo que sucedi√≥ con los sistemas expertos.  En la d√©cada de 1980, determinamos que son m√°s baratos que las personas para resolver algunos problemas.  ¬øY qui√©n usa sistemas expertos hoy?  Nadie  Y despu√©s de eso lleg√≥ el invierno con IA. <br><br>  Hasta ahora, esto no parece probable, se est√° invirtiendo tanta publicidad y dinero en IA.  Pero a la larga, creo que la humanidad puede decidir, tal vez por miedo, tal vez por falta de evidencia, que esta tecnolog√≠a no nos conviene.  Es posible </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/440426/">https://habr.com/ru/post/440426/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../440414/index.html">Sobre linter, calidad de c√≥digo, calidad en general y gesti√≥n de calidad</a></li>
<li><a href="../440416/index.html">Colonia Cap√≠tulo 25: Salida nocturna</a></li>
<li><a href="../440420/index.html">Bienvenido a Devleads Meetup 21 de febrero</a></li>
<li><a href="../440422/index.html">Cuando eres responsable de la calidad del regalo. La historia de un experimento blockchain</a></li>
<li><a href="../440424/index.html">Algoritmo de Pensamiento y Conciencia</a></li>
<li><a href="../440428/index.html">SMAA: suavizado morfol√≥gico de subp√≠xeles mejorado</a></li>
<li><a href="../440430/index.html">¬øDe d√≥nde viene el eslogan "Don't Be Evil"?</a></li>
<li><a href="../440432/index.html">Friday SciFi sobre las profesiones del futuro: "Real Girls"</a></li>
<li><a href="../440434/index.html">Industria automotriz rusa: el camino hacia las tecnolog√≠as aditivas</a></li>
<li><a href="../440436/index.html">Tareas pr√°cticas de Java: para cursos y otras actividades</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>