<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìï üíÆ üöµüèª O problema dos bandidos com v√°rias armas - compare a estrat√©gia Epsilon-Greedy e a amostragem Thompson üè° ü§ûüèæ üìñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Apresento a voc√™s a primeira p√°gina do Solving bandits multiarmed: Uma compara√ß√£o entre o artigo epsilon-greedy e Thompson . 

 O problema d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O problema dos bandidos com v√°rias armas - compare a estrat√©gia Epsilon-Greedy e a amostragem Thompson</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425619/">  <i>Ol√° Habr!</i>  <i>Apresento a voc√™s a primeira p√°gina do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Solving bandits multiarmed: Uma compara√ß√£o entre o artigo epsilon-greedy e Thompson</a> .</i> <br><br><h1>  O problema dos bandidos armados </h1><br><p>  O problema dos bandidos com v√°rias armas √© uma das tarefas mais b√°sicas na ci√™ncia das solu√ß√µes.  Ou seja, esse √© o problema da aloca√ß√£o √≥tima de recursos em condi√ß√µes de incerteza.  O nome "bandido multi-armado" veio das antigas m√°quinas ca√ßa-n√≠queis controladas por al√ßas.  Esses rifles de assalto foram apelidados de "bandidos", porque depois de conversar com eles, as pessoas geralmente se sentiam assaltadas.  Agora imagine que existem v√°rias dessas m√°quinas e a chance de ganhar contra carros diferentes √© diferente.  Desde que come√ßamos a brincar com essas m√°quinas, queremos determinar qual chance √© maior e usar essa m√°quina com mais frequ√™ncia do que outras. </p><br><p>  O problema √© o seguinte: como podemos entender com mais efici√™ncia qual m√°quina √© mais adequada e, ao mesmo tempo, experimentar muitos recursos em tempo real?  Este n√£o √© um tipo de problema te√≥rico, √© um problema que uma empresa enfrenta o tempo todo.  Por exemplo, uma empresa possui v√°rias op√ß√µes de mensagens que precisam ser exibidas aos usu√°rios (por exemplo, as mensagens incluem an√∫ncios, sites, imagens) para que as mensagens selecionadas maximizem uma determinada tarefa comercial (convers√£o, clicabilidade etc.) </p><br><a name="habracut"></a><p>  Uma maneira t√≠pica de resolver esse problema √© executar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">testes A / B</a> v√°rias vezes.  Ou seja, por v√°rias semanas para mostrar cada uma das op√ß√µes com a mesma frequ√™ncia e, com base em testes estat√≠sticos, decida qual op√ß√£o √© melhor.  Esse m√©todo √© adequado quando h√° poucas op√ß√µes, por exemplo, 2 ou 4. Mas quando h√° muitas op√ß√µes, essa abordagem se torna ineficaz - tanto no tempo perdido quanto no lucro perdido. </p><br><p>  A origem do tempo perdido deve ser f√°cil de entender.  Mais op√ß√µes - s√£o necess√°rios mais testes A / B - √© necess√°rio mais tempo para tomar uma decis√£o.  A ocorr√™ncia de lucros perdidos n√£o √© t√£o √≥bvia.  Perda de oportunidade (custo de oportunidade) - os custos associados ao fato de que, em vez de uma a√ß√£o, realizamos outra, ou seja, em termos simples, √© isso que perdemos investindo em A em vez de B. Investir em B √© o lucro perdido de investir em A. O mesmo com a op√ß√£o de verifica√ß√£o.  Os testes A / B n√£o devem ser interrompidos at√© que sejam conclu√≠dos.  Isso significa que o pesquisador n√£o sabe qual op√ß√£o √© melhor at√© o teste terminar.  No entanto, acredita-se que uma op√ß√£o seja melhor que a outra.  Isso significa que, prolongando os testes A / B, n√£o mostramos as melhores op√ß√µes para um n√∫mero suficientemente grande de visitantes (embora n√£o saibamos quais s√£o as melhores), perdendo assim nosso lucro.  Esse √© o benef√≠cio perdido dos testes A / B.  Se houver apenas um teste A / B, talvez o lucro perdido n√£o seja nada bom.  Um grande n√∫mero de testes A / B significa que, por um longo tempo, temos que mostrar aos clientes muitas das melhores op√ß√µes.  Seria melhor se voc√™ pudesse rapidamente jogar fora as op√ß√µes ruins em tempo real e, somente ent√£o, quando houver poucas op√ß√µes, use os testes A / B para elas. </p><br><p>  Samplers ou agentes s√£o maneiras de testar e otimizar rapidamente a distribui√ß√£o de op√ß√µes.  Neste artigo, apresentarei a <i>amostragem Thompson</i> e suas propriedades.  Tamb√©m compararei a amostragem de Thompson com o algoritmo √©psilon-ganancioso, outra op√ß√£o popular para o problema de bandidos com v√°rias armas.  Tudo ser√° implementado no Python do zero - todo o c√≥digo pode ser encontrado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . </p><br><h2>  Breve Dicion√°rio de Conceitos </h2><br><p></p><ul><li>  Agente, amostrador, bandido ( <i>agente, amostrador, bandido</i> ) - um algoritmo que toma decis√µes sobre qual op√ß√£o exibir. </li><li>  Variante - uma variante diferente da mensagem que o visitante v√™. </li><li>  A√ß√£o - a a√ß√£o que o algoritmo escolheu (qual op√ß√£o mostrar). </li><li>  Usar ( <i>explorar</i> ) - fa√ßa uma escolha para maximizar a recompensa total com base nos dados dispon√≠veis. </li><li>  Explorar, <i>explorar</i> - fa√ßa escolhas para entender melhor o retorno de cada op√ß√£o. </li><li>  Pr√™mio, pontos ( <i>pontua√ß√£o, recompensa</i> ) - uma tarefa comercial, por exemplo, convers√£o ou clicabilidade.  Por uma quest√£o de simplicidade, acreditamos que ele √© distribu√≠do binomialmente e √© igual a 1 ou 0 - clicou ou n√£o. </li><li>  Ambiente - o contexto em que o agente opera - op√ß√µes e seu "retorno" oculto para o usu√°rio. </li><li>  Payback, probabilidade de sucesso ( <i>taxa de pagamento</i> ) - uma vari√°vel oculta igual √† probabilidade de obter pontua√ß√£o = 1, para cada op√ß√£o √© diferente.  Mas o usu√°rio n√£o a v√™. </li><li>  Try ( <i>avalia√ß√£o</i> ) - o usu√°rio visita a p√°gina. </li><li>  Lamento √© a diferen√ßa entre qual seria o melhor resultado de todas as op√ß√µes dispon√≠veis e qual foi o resultado da op√ß√£o dispon√≠vel na tentativa atual.  Quanto menos se arrepender das a√ß√µes j√° tomadas, melhor. </li><li>  Mensagem ( <i>mensagem</i> ) - um banner, op√ß√£o de p√°gina e mais vers√µes diferentes das quais queremos experimentar. </li><li>  Amostragem - a gera√ß√£o de uma amostra a partir de uma determinada distribui√ß√£o. </li></ul><br><h2>  Explorar e explorar </h2><br><p>  Os agentes s√£o algoritmos que procuram uma abordagem para a tomada de decis√µes em tempo real, a fim de alcan√ßar um equil√≠brio entre explorar o espa√ßo de op√ß√µes e usar a melhor op√ß√£o.  Esse equil√≠brio √© muito importante.  O espa√ßo das op√ß√µes deve ser investigado para se ter uma id√©ia de qual op√ß√£o √© a melhor.  Se descobrimos pela primeira vez essa op√ß√£o ideal e a usamos o tempo todo, maximizaremos a recompensa total que est√° dispon√≠vel para n√≥s do ambiente.  Por outro lado, tamb√©m queremos explorar outras op√ß√µes poss√≠veis - e se elas se tornarem melhores no futuro, mas ainda n√£o sabemos?  Em outras palavras, queremos garantir poss√≠veis perdas, tentando experimentar um pouco de op√ß√µes abaixo do ideal para esclarecer por si mesmas o seu retorno.  Se o retorno deles for realmente maior, eles poder√£o ser mostrados com mais frequ√™ncia.  Outra vantagem das op√ß√µes de explora√ß√£o √© que podemos entender melhor n√£o apenas o retorno m√©dio, mas tamb√©m o quanto o retorno √© distribu√≠do, ou seja, podemos estimar melhor a incerteza. <br>  O principal problema, portanto, √© resolver - qual √© a melhor sa√≠da do dilema entre explora√ß√£o e explora√ß√£o (tradeoff explora√ß√£o-explora√ß√£o). </p><br><h2>  Algoritmo Epsilon-ganancioso </h2><br><p>  Uma sa√≠da t√≠pica desse dilema √© o algoritmo epsilon-ganancioso.  "Ganancioso" significa exatamente o que voc√™ pensou.  Ap√≥s um per√≠odo inicial, quando acidentalmente fazemos tentativas - digamos, 1000 vezes, o algoritmo escolhe ansiosamente a melhor op√ß√£o k em <i>e</i> por cento das tentativas.  Por exemplo, se <i>e</i> = 0,05, o algoritmo 95% do tempo seleciona a melhor op√ß√£o e, nos 5% restantes, seleciona tentativas aleat√≥rias.  De fato, este √© um algoritmo bastante eficaz, no entanto, pode n√£o ser suficiente explorar o espa√ßo de op√ß√µes e, portanto, n√£o ser√° bom o suficiente avaliar qual op√ß√£o √© a melhor para ficar preso a uma op√ß√£o abaixo do ideal.  Vamos mostrar no c√≥digo como esse algoritmo funciona. </p><br><p>  Mas primeiro, algumas depend√™ncias.  N√≥s devemos definir o ambiente.  Este √© o contexto no qual os algoritmos ser√£o executados.  Nesse caso, o contexto √© muito simples.  Ele chama o agente para que ele decida qual a√ß√£o escolher; o contexto inicia essa a√ß√£o e retorna os pontos recebidos para ela de volta ao agente (que de alguma forma atualiza seu estado). </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Environment</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, variants, payouts, n_trials, variance=False)</span></span></span><span class="hljs-function">:</span></span> self.variants = variants <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> variance: self.payouts = np.clip(payouts + np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.04</span></span>, size=len(variants)), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">.2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: self.payouts = payouts <span class="hljs-comment"><span class="hljs-comment">#self.payouts[5] = self.payouts[5] if i &lt; n_trials/2 else 0.1 self.n_trials = n_trials self.total_reward = 0 self.n_k = len(variants) self.shape = (self.n_k, n_trials) def run(self, agent): """Run the simulation with the agent. agent must be a class with choose_k and update methods.""" for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_k() # Environment returns reward reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward</span></span></code> </pre> <br>  Os pontos s√£o distribu√≠dos binomialmente com probabilidade p, dependendo do n√∫mero da a√ß√£o (assim como eles poderiam ser distribu√≠dos continuamente, a ess√™ncia n√£o teria mudado).  Tamb√©m definirei a classe BaseSampler - √© necess√°ria apenas para armazenar logs e v√°rios atributos. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">BaseSampler</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_samples=None, n_learning=None, e=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.05</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.env = env self.shape = (env.n_k, n_samples) self.variants = env.variants self.n_trials = env.n_trials self.payouts = env.payouts self.ad_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.regret_i = np.zeros(env.n_trials) self.thetaregret = np.zeros(self.n_trials) self.a = np.ones(env.n_k) self.b = np.ones(env.n_k) self.theta = np.zeros(env.n_k) self.data = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> self.k = <span class="hljs-number"><span class="hljs-number">0</span></span> self.i = <span class="hljs-number"><span class="hljs-number">0</span></span> self.n_samples = n_samples self.n_learning = n_learning self.e = e self.ep = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, size=env.n_trials) self.exploit = (<span class="hljs-number"><span class="hljs-number">1</span></span> - e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">collect_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.data = pd.DataFrame(dict(ad=self.ad_i, reward=self.r_i, regret=self.regret_i))</code> </pre> <br>  Abaixo, definimos 10 op√ß√µes e retorno para cada uma.  A melhor op√ß√£o √© a op√ß√£o 9, com um retorno de 0,11%. <br><br><pre> <code class="python hljs">variants = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>] payouts = [<span class="hljs-number"><span class="hljs-number">0.023</span></span>, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">0.029</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.06</span></span>, <span class="hljs-number"><span class="hljs-number">0.0234</span></span>, <span class="hljs-number"><span class="hljs-number">0.035</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.11</span></span>]</code> </pre> <br>  Para ter algo para construir, tamb√©m definimos a classe RandomSampler.  Essa classe √© necess√°ria como modelo de linha de base.  Ele simplesmente escolhe aleatoriamente uma op√ß√£o em cada tentativa e n√£o atualiza seus par√¢metros. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RandomSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.k = np.random.choice(self.variants) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.k <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># nothing to update #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.thetaregret) - self.theta[self.k] #self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.theta) - self.theta[self.k] self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><p>  Outros modelos t√™m a seguinte estrutura.  Todos t√™m m√©todos choose_k e de atualiza√ß√£o.  choose_k implementa o m√©todo pelo qual o agente seleciona uma op√ß√£o.  update atualiza os par√¢metros do agente - esse m√©todo caracteriza como a capacidade do agente de escolher a op√ß√£o muda (com o RandomSampler, essa capacidade n√£o muda de forma alguma).  Executamos o agente no ambiente usando o seguinte padr√£o. </p><br><pre> <code class="python hljs">en0 = Environment(machines, payouts, n_trials=<span class="hljs-number"><span class="hljs-number">10000</span></span>) rs = RandomSampler(env=en0) en0.run(agent=rs)</code> </pre> <br><p>  A ess√™ncia do algoritmo √©psilon-ganancioso √© a seguinte. <br><br></p><ol><li>  Selecione k aleatoriamente para n tentativas. </li><li>  Em cada tentativa, para cada op√ß√£o, avalie o ganho. </li><li>  Depois de todas as n tentativas: </li><li>  Com probabilidade 1 - <i>e</i> escolha k com o maior ganho; </li><li>  Com probabilidade <i>e</i> escolha K aleatoriamente. </li></ol><br>  C√≥digo Epsilon-ganancioso: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">eGreedy</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env, n_learning, e)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env, n_learning, e) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># e% of the time take a random draw from machines # random k for n learning trials, then the machine with highest theta self.k = np.random.choice(self.variants) if self.i &lt; self.n_learning else np.argmax(self.theta) # with 1 - e probability take a random sample (explore) otherwise exploit self.k = np.random.choice(self.variants) if self.ep[self.i] &gt; self.exploit else self.k return self.k # every 100 trials update the successes # update the count of successes for the chosen machine def update(self): # update the probability of payout for each machine self.a[self.k] += self.reward self.b[self.k] += 1 self.theta = self.a/self.b #self.total_reward += self.reward #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] #self.thetaregret[self.i] = self.thetaregret[self.i] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br><br><p>  Abaixo no gr√°fico, voc√™ pode ver os resultados de uma amostra puramente aleat√≥ria, ou seja, n√£o h√° modelo aqui.  O gr√°fico mostra que escolha o algoritmo fez em cada tentativa, se houvesse 10 mil tentativas.  O algoritmo apenas tenta, mas n√£o aprende.  No total, ele marcou 418 pontos. <br> <a href=""><img src="https://habrastorage.org/webt/sn/ql/2r/snql2roqbdiruuskxsathithz8i.jpeg"></a> </p><br><p>  Vamos ver como o algoritmo epsilon-ganancioso se comporta no mesmo ambiente.  Execute o algoritmo para 10 mil tentativas com <i>e</i> = 0,1 e n_learning = 500 (o agente simplesmente tenta as primeiras 500 tentativas e, em seguida, tenta com probabilidade <i>e</i> = 0,1).  Vamos avaliar o algoritmo de acordo com o n√∫mero total de pontos que obt√©m no ambiente. </p><br><pre> <code class="python hljs">en1 = Environment(machines, payouts, n_trials) eg = eGreedy(env=en1, n_learning=<span class="hljs-number"><span class="hljs-number">500</span></span>, e=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) en1.run(agent=eg)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/-f/zf/14/-fzf14djbqtdl5-0vyrapuhcp5c.jpeg"></a> <br><p>  O algoritmo ganancioso Epsilon obteve 788 pontos, quase 2 vezes melhor que o algoritmo aleat√≥rio - super!  O segundo gr√°fico explica esse algoritmo muito bem.  Vemos que, para as primeiras 500 etapas, as a√ß√µes s√£o distribu√≠das aproximadamente uniformemente e K √© escolhido aleatoriamente.  No entanto, come√ßa a explorar fortemente a op√ß√£o 5 - essa √© uma op√ß√£o bastante forte, mas n√£o √© a melhor.  Tamb√©m vemos que o agente ainda seleciona aleatoriamente 10% do tempo. </p><br><p>  Isso √© muito legal - escrevemos apenas algumas linhas de c√≥digo e agora j√° temos um algoritmo bastante poderoso que pode explorar o espa√ßo de op√ß√µes e tomar decis√µes pr√≥ximas do ideal.  Por outro lado, o algoritmo n√£o encontrou a melhor op√ß√£o.  Sim, podemos aumentar o n√∫mero de etapas para o aprendizado, mas dessa maneira gastaremos ainda mais tempo em uma pesquisa aleat√≥ria, piorando ainda mais o resultado final.  Al√©m disso, a aleatoriedade √© costurada nesse processo por padr√£o - o melhor algoritmo pode n√£o ser encontrado. </p><br><p>  Mais tarde, rodarei cada um dos algoritmos v√°rias vezes para que possamos compar√°-los um com o outro.  Mas, por enquanto, vamos dar uma olhada na amostragem Thompson e test√°-la no mesmo ambiente. </p><br><h2>  Amostragem Thompson </h2><br><p>  A amostragem de Thompson √© fundamentalmente diferente do algoritmo epsilon-ganancioso em tr√™s pontos principais: <br><br></p><ol><li>  N√£o √© ganancioso. </li><li>  Faz tentativas de uma maneira mais sofisticada. </li><li>  √â bayesiano. </li></ol><br>  O ponto principal √© o par√°grafo 3, os par√°grafos 1 e 2 seguem a partir dele. <br><p>  A ess√™ncia do algoritmo √© esta: <br><br></p><ol><li>  Defina a distribui√ß√£o beta inicial entre 0 e 1 para o retorno de cada op√ß√£o. </li><li>  Prove as op√ß√µes desta distribui√ß√£o, selecione o par√¢metro Theta m√°ximo. </li><li>  Escolha a op√ß√£o k que est√° associada ao maior teta. </li><li>  Veja quantos pontos foram marcados, atualize os par√¢metros de distribui√ß√£o. </li></ol><br>  Leia mais sobre a distribui√ß√£o beta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br>  E sobre o seu uso em Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><p>  C√≥digo do algoritmo: <br><br></p><pre> <code class="python hljs"> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ThompsonSampler</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(BaseSampler)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, env)</span></span></span><span class="hljs-function">:</span></span> super().__init__(env) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">choose_k</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.a, self.b) # select machine with highest posterior p of payout self.k = self.variants[np.argmax(self.theta)] #self.k = np.argmax(self.a/(self.a + self.b)) return self.k def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) self.a[self.k] += self.reward self.b[self.k] += 1 - self.reward # ie only increment b when it's a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 #self.thetaregret[self.i] = self.thetaregret[self.i] #self.regret_i[self.i] = np.max(self.theta) - self.theta[self.k] self.thetas[self.i] = self.theta[self.k] self.thetaregret[self.i] = np.max(self.thetas) - self.theta[self.k] self.ad_i[self.i] = self.k self.r_i[self.i] = self.reward self.i += 1</span></span></code> </pre> <br>  A nota√ß√£o formal do algoritmo se parece com isso. <br> <a href=""><img src="https://habrastorage.org/webt/5f/n6/xe/5fn6xew2i7v1jjh_10h9jqkjdzu.png"></a> <br><p>  Vamos programar esse algoritmo.  Como outros agentes, o ThompsonSampler herda do BaseSampler e define seus pr√≥prios m√©todos choose_k e update.  Agora lance nosso novo agente. </p><br><pre> <code class="python hljs"> en2 = Environment(machines, payouts, n_trials) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa)</code> </pre> <br> <a href=""><img src="https://habrastorage.org/webt/ml/kj/1p/mlkj1pvs8xnpxtkqmehmm_3xhgo.jpeg"></a> <br><p>  Como voc√™ pode ver, ele marcou mais do que o algoritmo epsilon-ganancioso.  √ìtimo!  Vejamos o gr√°fico da sele√ß√£o de tentativas.  Duas coisas interessantes s√£o vis√≠veis nele.  Primeiro, o agente descobriu corretamente a melhor op√ß√£o (op√ß√£o 9) e a usou ao m√°ximo.  Em segundo lugar, o agente usou outras op√ß√µes, mas de uma maneira mais complicada - ap√≥s cerca de 1000 tentativas, o agente, al√©m da op√ß√£o principal, usou principalmente as op√ß√µes mais poderosas entre as outras.  Em outras palavras, ele n√£o escolheu aleatoriamente, mas com mais compet√™ncia. </p><br><p>  Por que isso funciona?  √â simples - a incerteza na distribui√ß√£o posterior dos benef√≠cios esperados para cada op√ß√£o significa que cada op√ß√£o √© selecionada com uma probabilidade aproximadamente proporcional √† sua forma, determinada pelos par√¢metros alfa e beta.  Em outras palavras, em cada tentativa, a amostragem Thompson aciona a op√ß√£o de acordo com a probabilidade posterior de que ele tenha o benef√≠cio m√°ximo.  Grosso modo, tendo pela distribui√ß√£o informa√ß√µes sobre incertezas, o agente decide quando examinar o ambiente e quando usar as informa√ß√µes.  Por exemplo, uma op√ß√£o fraca com alta incerteza posterior pode pagar mais por essa tentativa.  Mas para a maioria das tentativas, quanto mais forte sua distribui√ß√£o posterior, maior sua m√©dia e menor seu desvio padr√£o e, portanto, maior a chance de escolh√™-la. </p><br><p>  Outra propriedade not√°vel do algoritmo de Thompson: como √© bayesiano, podemos estimar a incerteza na estimativa de retorno de cada op√ß√£o usando seus par√¢metros.  O gr√°fico abaixo mostra as distribui√ß√µes posteriores em 6 pontos diferentes e em 20.000 tentativas.  Voc√™ v√™ como as distribui√ß√µes gradualmente come√ßam a convergir para a op√ß√£o com o melhor retorno. </p><br> <a href=""><img src="https://habrastorage.org/webt/bb/ka/fb/bbkafb4nv1pajwkygxy2brtmowy.jpeg"></a> <br><p>  Agora compare todos os 3 agentes em 100 simula√ß√µes.  1 simula√ß√£o √© um lan√ßamento do agente em 10.000 tentativas. </p><br> <a href=""><img src="https://habrastorage.org/webt/j6/v1/sm/j6v1smcrwkwyhlo27ffhly13pwk.jpeg"></a> <br><p>  Como voc√™ pode ver no gr√°fico, a estrat√©gia epsilon-greedy e a amostragem Thompson funcionam muito melhor do que a amostragem aleat√≥ria.  Voc√™ pode se surpreender com o fato de a estrat√©gia epsilon-gananciosa e a amostragem Thompson serem realmente compar√°veis ‚Äã‚Äãem termos de desempenho.  A estrat√©gia gananciosa de Epsilon pode ser muito eficaz, mas √© mais arriscada, porque pode ficar presa em uma op√ß√£o abaixo do ideal - isso pode ser visto nas falhas no gr√°fico.  Mas a amostragem Thompson n√£o pode, porque faz a escolha no espa√ßo de op√ß√µes de uma maneira mais complexa. </p><br><h2>  Arrependimento </h2><br><p>  Outra maneira de avaliar qu√£o bem o algoritmo funciona √© avaliar o arrependimento.  Grosso modo, quanto menor, em rela√ß√£o √†s a√ß√µes j√° tomadas, melhor.  Abaixo est√° um gr√°fico do total arrependimento e arrependimento pelo erro.  Mais uma vez - quanto menos arrependimento, melhor. </p><br> <a href=""><img src="https://habrastorage.org/webt/8p/kd/o3/8pkdo3bilrde28bwsimdnbesqwg.jpeg"></a> <br><p>  No gr√°fico superior, vemos o arrependimento total e, no arrependimento inferior, a tentativa.  Como pode ser visto nos gr√°ficos, a amostragem Thompson converge para um arrependimento m√≠nimo muito mais r√°pido do que a estrat√©gia epsilon-gananciosa.  E converge para um n√≠vel mais baixo.  Com a amostragem Thompson, o agente se arrepende menos porque pode detectar melhor a melhor op√ß√£o e tentar as op√ß√µes mais promissoras melhor - portanto, a amostragem Thompson √© particularmente adequada para casos de uso mais avan√ßados, como modelos estat√≠sticos ou redes neurais para escolher k. </p><br><h2>  Conclus√µes </h2><br><p>  Este √© um post t√©cnico bastante longo.  Para resumir, podemos usar m√©todos de amostragem bastante sofisticados se tivermos muitas op√ß√µes que queremos testar em tempo real.  Uma das caracter√≠sticas muito boas da amostragem Thompson √© que ela equilibra o uso e a explora√ß√£o de uma maneira bastante complicada.  Ou seja, podemos deix√°-lo otimizar a distribui√ß√£o das op√ß√µes da solu√ß√£o em tempo real.  Esses s√£o algoritmos interessantes e devem ser mais √∫teis para os neg√≥cios do que os testes A / B. </p><br><p>  <b>Importante!</b>  <b>A amostragem Thompson n√£o significa que voc√™ n√£o precisa fazer testes A / B.</b>  <b>Normalmente, eles primeiro encontram as melhores op√ß√µes com a ajuda dele e depois fazem testes A / B j√°.</b> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt425619/">https://habr.com/ru/post/pt425619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt425605/index.html">Aceita√ß√£o de pagamentos de um cart√£o sem jur. rostos em Yandex.Money</a></li>
<li><a href="../pt425607/index.html">Identifique fraudes usando o conjunto de dados da Enron. Parte 2, encontrando o melhor modelo</a></li>
<li><a href="../pt425609/index.html">Teoria dos Jogos: Tomada de Decis√£o com Exemplos no Kotlin</a></li>
<li><a href="../pt425611/index.html">Arquitetura de front-end de n√≠vel superior. Palestra Yandex</a></li>
<li><a href="../pt425613/index.html">Como eu combinei os dados do plug-in Tempo para o Jira Server e o Jira Cloud e os migrei de volta para o Jira Cloud</a></li>
<li><a href="../pt425621/index.html">Uma empresa que utiliza di√≥xido de carbono atmosf√©rico inicia a produ√ß√£o de metano</a></li>
<li><a href="../pt425623/index.html">Tour fotogr√°fico de Coworking ‚ÄúKey‚Äù</a></li>
<li><a href="../pt425625/index.html">Gasto ou Por que os localizadores traduzem mal os jogos</a></li>
<li><a href="../pt425627/index.html">IaaS para o desenvolvimento de servi√ßos: quem e por que mudou para a infraestrutura virtual</a></li>
<li><a href="../pt425629/index.html">Como fizemos um jogo de tabuleiro com controle remoto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>