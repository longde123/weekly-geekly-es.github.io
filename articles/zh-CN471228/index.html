<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚂 🗒️ ✊🏿 Grokay PyTorch 👩🏽‍🚒 🚜 🤴🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="哈Ha！ 

 我们已经预购了一本期待已久的有关PyTorch库的书。 



 由于您将从本书中学习有关PyTorch的所有必要基本材料，因此，我们提醒您想学习的主题称为“摸索”或“深入理解”的好处 。 在今天的帖子中，我们将告诉您Kai Arulkumaran如何抨击PyTorch（无图片）。 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/">哈Ha！ <br><br> 我们已经预购了一本期待已久的有关<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">PyTorch库的</a>书。 <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br> 由于您将从本书中学习有关PyTorch的所有必要基本材料，因此，我们提醒您想学习的主题称为“摸索”或“深入理解”的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">好处</a> 。 在今天的帖子中，我们将告诉您Kai Arulkumaran如何抨击PyTorch（无图片）。 欢迎来到猫。 <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">PyTorch</a>是一个灵活的深度学习框架，它使用动态神经网络（即使用动态流控制的网络，例如<code>if</code>和<code>while</code>循环）自动区分对象。  PyTorch支持GPU加速， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">分布式培训</a> ，各种类型的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">优化</a>以及许多其他不错的功能。 在这里，我对如何使用PyTorch提出了一些想法。 库的所有方面和建议的实践都不在此处，但我希望本文对您有所帮助。 <br><br> 神经网络是计算图的子类。 计算图接收数据作为输入，然后在处理它们的节点上将这些数据路由（并可以转换）。 在深度学习中，神经元（节点）通常通过向其应用参数和可微函数来转换数据，以便可以通过梯度下降法对参数进行优化以使损失最小化。 从广义上讲，我注意到函数可以是随机的，而图可以是动态的。 因此，虽然神经网络非常适合数据流编程范例，但PyTorch API专注于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">命令式编程</a>范例，这种解释创建的程序的方式更加熟悉。 这就是为什么PyTorch代码更易于阅读，更容易判断复杂程序的设计的原因，然而，这并不需要严重影响性能：实际上，PyTorch足够快，并且提供了许多优化，作为最终用户，您完全不必担心（但是，如果您真的对它们感兴趣，则可以更深入地了解它们）。 <br><br> 本文的其余部分是<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">对MNIST数据集上官方示例</a>的分析。 在这里我们<i>玩</i> PyTorch，因此，我建议您仅在熟悉<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">正式的初学者手册</a>后才了解本文。 为了方便起见，该代码以带有注释的小片段的形式呈现，也就是说，它没有分发到您习惯于在纯模块化代码中看到的单独的函数/文件中。 <br><br><h4> 进口货 </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br> 除<code>torchvision</code>模块外，所有这些都是非常标准的导入，特别是在解决与计算机视觉有关的任务时， <code>torchvision</code>模块除外。 <br><br><h4> 客制化 </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code>是在Python中处理命令行参数的标准方法。 <br><br> 如果您需要编写旨在在不同设备上工作的代码（使用GPU加速，如果可用，但如果没有回滚到CPU上的计算），则选择并保存适当的<code>torch.device</code> ，您可以使用该文件确定应该在哪里张量被存储。 有关创建此类代码的更多信息，请参见<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方文档</a> 。  PyTorch的方法是将设备的选择带给用户控制，这在简单的示例中似乎是不可取的。 但是，这种方法极大地简化了必须处理张量的工作，其中a）调试方便b）允许您有效地手动使用设备。 <br><br> 为了提高实验的可重复性，您需要为所有使用随机数生成的组件（包括<code>random</code>或<code>numpy</code> ，如果也使用）设置随机初始值。 请注意：cuDNN使用非确定性算法，并且可以选择使用<code>torch.backends.cudnn.enabled = False</code>禁用。 <br><br><h4> 资料 </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br> 由于<code>torchvision</code>模型存储在<code>~/.torch/models/</code> ，因此我更喜欢将torchvision <code>torchvision</code>存储在<code>~/.torch/datasets</code> 。 这是我的版权协议，但是在基于MNIST，CIFAR-10等开发的项目中使用非常方便。 通常，如果要重复使用多个数据集，则应将数据集与代码分开存储。 <br><br>  <code>torchvision.transforms</code>包含许多方便的用于单个图像的转换选项，例如裁剪和归一化。 <br><br>  <code>batch_size</code>有很多选项，但是除了<code>batch_size</code>和<code>shuffle</code>之外，您还应该记住<code>num_workers</code>和<code>pin_memory</code> ，它们有助于提高效率。  <code>num_workers &gt; 0</code>使用子进程进行异步数据加载，并且不会为此阻塞主进程。 一个典型的用例是从磁盘加载数据（例如图像），并可能对其进行转换。 所有这些以及网络数据处理可以并行完成。 可能需要调整处理程度，以便a）最小化工作人员的数量，并因此减少所使用的CPU和RAM的数量（每个工作人员加载一个单独的批次，而不是批次中包含的单个样本）b）最小化数据在网络上等待的时间长度。  <code>pin_memory</code>使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">固定的内存</a> （而不是分页的）来加速从RAM到GPU的任何数据传输操作（对CPU特定的代码不执行任何操作）。 <br><br><h4> 型号 </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br> 网络初始化通常扩展到成员变量，包含学习参数以及可能包含单个学习参数和未经训练的缓冲区的层。 然后，通过直接传递，将它们与<code>F</code>中的纯函数且不包含参数的函数结合使用。 有些人喜欢使用纯功能网络（例如保留参数并使用<code>F.conv2d</code>而不是<code>nn.Conv2d</code> ）或完全由层组成的网络（例如<code>nn.ReLU</code>而不是<code>F.relu</code> ）。 <br><br> 如果将<code>device</code>设置为GPU， <code>.to(device)</code>是一种将设备参数（和缓冲区）发送到GPU的便捷方法，因为否则（如果将设备设置为CPU）则不会执行任何操作。 在将设备参数传递给优化器之前，将其传递到适当的设备很重要； 否则，优化器将无法正确跟踪参数！ <br><br> 神经网络（ <code>nn.Module</code> ）和优化器（ <code>optim.Optimizer</code> ）均可保存和加载其内部状态，建议使用<code>.load_state_dict(state_dict)</code>进行此<code>.load_state_dict(state_dict)</code> -必须重新加载两者的状态，以便基于先前保存的字典来恢复训练状态。 保存整个对象可能会<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">充满错误</a> 。 如果将张量保存在GPU上并想将它们加载到CPU或另一个GPU上，那么最简单的方法是使用<code>map_location</code> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">选项</a>将它们直接加载到CPU上，例如<code>torch.load('model.pth'</code> ， <code>map_location='cpu'</code> ）。 <br><br> 这里有一些其他地方未在此处显示，但值得一提，您可以直接使用控制流（例如， <code>if</code>的执行可能取决于成员变量或数据本身。此外，在过程中间进行输出是完全可以接受的（ <code>print</code> ）张量，极大地简化了调试；最后，通过直接传递，可以使用很多参数。我将通过简短的清单来说明这一点，该清单与任何特定思想无关： <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4> 培训课程 </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br> 默认情况下，网络模块进入训练模式-在一定程度上影响模块的运行-最重要的是-精简和批量标准化。 一种或另一种方式，最好使用<code>.train()</code>手动设置此类设置，该设置会过滤所有子模块的“ training”标志。 <br><br> 在这里， <code>.to()</code>方法不仅接受设备，还设置<code>non_blocking=True</code> ，从而确保将数据从已提交的内存异步复制到GPU，从而使CPU在数据传输期间保持可操作状态。 否则， <code>non_blocking=True</code>根本不是一种选择。 <br><br> 在使用<code>loss.backward()</code>和<code>loss.backward()</code>建立新的梯度集之前，必须先使用<code>loss.backward()</code>手动重置要优化的参数的梯度。 默认情况下，PyTorch会累积梯度，如果您没有足够的资源来一次计算所需的所有梯度，这将非常方便。 <br><br>  PyTorch使用自动渐变的“磁带”系统-它收集有关在张量上执行的操作和顺序的信息，然后以相反的方向播放它们以相反的顺序执行微分（反向模式微分）。 这就是为什么它是如此的灵活，并允许任意的计算图。 如果这些张量都不要求渐变（您必须将<code>requires_grad=True</code>设置为为此目的创建张量），则不会保存任何图形！ 但是，网络通常具有需要梯度的参数，因此，基于网络输出执行的任何计算都将存储在图中。 因此，如果要保存此步骤生成的数据，则需要手动禁用渐变或（更常见的方法）将此信息另存为Python数字（在PyTorch标量中使用<code>.item()</code> ）或<code>numpy</code>数组。 在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方文档中</a>阅读有关<code>autograd</code>更多信息。 <br><br> 缩短计算图的一种方法是在学习带有截断时间反向传播版本的RNN时，通过隐藏状态时使用<code>.detach()</code> 。 当区分损失时，当组件之一是另一个网络的输出时，这也很方便，但是不应针对损失优化该另一个网络。 例如，我将讲授与GAN合作时产生的输出材料的区别部分，或使用目标函数作为基本函数（例如A2C）的行为者评论算法中的策略训练。 防止梯度计算的另一种技术有效地训练GAN（对区分材料进行生成部分的训练），并且在微调中通常采用的典型方法<code>param.requires_grad = False</code>参数<code>param.requires_grad = False</code>的网络参数的循环枚举。 <br><br> 重要的是，不仅要在控制台/日志文件中记录结果，而且还要以防万一，在模型参数（和优化器状态）中设置控制点。 您还可以使用<code>torch.save()</code>保存常规的Python对象，或使用另一个标准解决方案-内置<code>pickle</code> 。 <br><br><h4> 测试中 </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br> 响应<code>.train()</code>需要使用<code>.eval()</code>将网络明确地置于评估模式。 <br><br> 如上所述，当使用网络时，通常会编译计算图。 为了避免这种情况，请在<code>no_grad</code>上下文<code>no_grad</code>中<code>with torch.no_grad()</code> 。 <br><br><h4> 还有一些 </h4><br> 这是附加的部分，在其中我做了一些更有用的论述。 <br> 这是解释使用内存的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方文档</a> 。 <br><br>  CUDA错误？ 很难修复它们，通常它们之间存在逻辑上的不一致，据此，在CPU上比在GPU上显示的错误信息更多。 最重要的是，如果您打算使用GPU，则可以在CPU和GPU之间快速切换。 一个更通用的开发技巧是组织代码，以便在开始一项完整的任务之前可以对其进行快速检查。 例如，准备一个小型或综合数据集，运行一个时代训练+测试等。 如果问题是CUDA错误，或者您根本无法切换到CPU，请设置CUDA_LAUNCH_BLOCKING = 1。 这将使CUDA内核启动同步，并且您将收到更准确的错误消息。 <br><br> 关于<code>torch.multiprocessing</code>的说明，或仅同时运行多个PyTorch脚本。 由于PyTorch使用多线程BLAS库来加速CPU上的线性代数计算，因此通常涉及多个内核。 如果要使用多线程处理或多个脚本同时执行多个操作，建议通过将环境变量<code>OMP_NUM_THREADS</code>设置为1或另一个较低的值来手动减少它们的数量。 因此，减少了滑倒处理器的可能性。 官方文档中还有关于多线程处理的其他注释。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN471228/">https://habr.com/ru/post/zh-CN471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN471212/index.html">10条提示和技巧，以帮助您成为VueJS上最好的开发人员</a></li>
<li><a href="../zh-CN471216/index.html">该指南的悠久历史-我如何编写5年的智能远足径服务</a></li>
<li><a href="../zh-CN471220/index.html">座舱-通过便捷的Web界面简化Linux中的典型管理任务</a></li>
<li><a href="../zh-CN471222/index.html">了解应用程序和服务隐私策略将有助于神经网络</a></li>
<li><a href="../zh-CN471226/index.html">Linux有很多面孔：如何在任何发行版上工作</a></li>
<li><a href="../zh-CN471232/index.html">我将LPS331AP连接到Omega Onion2的经验</a></li>
<li><a href="../zh-CN471236/index.html">Seryozha的剂量计。 第三部分 国家辐射计</a></li>
<li><a href="../zh-CN471240/index.html">“ Bitchy Betty”和现代音频接口：为什么它们以女性声音说话？</a></li>
<li><a href="../zh-CN471242/index.html">Bash Shell简介</a></li>
<li><a href="../zh-CN471244/index.html">Rosetta代码：测量大量编程语言中代码的长度，研究语言之间的接近度</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>