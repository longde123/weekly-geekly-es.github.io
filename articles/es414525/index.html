<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèº üëãüèΩ üåö El libro "Eficaz chispa. Escalado y optimizaci√≥n " üî∂ üîß üìè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En esta publicaci√≥n, analizaremos el acceso a la API de Spark desde varios lenguajes de programaci√≥n en la JVM, as√≠ como algunos problemas de rendimie...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El libro "Eficaz chispa. Escalado y optimizaci√≥n "</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/414525/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/g1/uu/lu/g1uulu2edgzcixecswin9lfylnc.jpeg" align="left" alt="imagen"></a>  En esta publicaci√≥n, analizaremos el acceso a la API de Spark desde varios lenguajes de programaci√≥n en la JVM, as√≠ como algunos problemas de rendimiento al ir m√°s all√° del lenguaje Scala.  Incluso si trabaja fuera de la JVM, esta secci√≥n puede ser √∫til, ya que los lenguajes que no son JVM a menudo dependen de la API de Java y no de la API de Scala. <br><br>  Trabajar en otros lenguajes de programaci√≥n no siempre significa que deba ir m√°s all√° de la JVM, y trabajar en la JVM tiene muchas ventajas en t√©rminos de rendimiento, principalmente debido al hecho de que no necesita copiar datos.  Aunque no es necesario utilizar bibliotecas o adaptadores de enlace especiales para acceder a Spark desde fuera del lenguaje Scala, invocar el c√≥digo Scala desde otros lenguajes de programaci√≥n puede ser dif√≠cil.  El marco de Spark admite el uso de Java 8 en expresiones lambda, y aquellos que usan versiones anteriores de JDK tienen la oportunidad de implementar la interfaz adecuada desde el paquete org.apache.spark.api.java.function.  Incluso en los casos en que no necesite copiar datos, trabajar en otro lenguaje de programaci√≥n puede tener peque√±os pero importantes matices relacionados con el rendimiento. <br><a name="habracut"></a><br>  Particularmente llamativas son las dificultades para acceder a varias API de Scala al invocar funciones con etiquetas de clase o al usar propiedades proporcionadas mediante conversiones de tipo impl√≠citas (por ejemplo, toda la funcionalidad de conjuntos RDD relacionados con las clases Double y Tuple).  Para los mecanismos que dependen de conversiones de tipo impl√≠cito, a menudo se proporcionan clases concretas equivalentes junto con conversiones expl√≠citas a ellas.  Las etiquetas de clase ficticias (digamos, AnyRef) se pueden pasar a funciones que dependen de etiquetas de clase (a menudo los adaptadores lo hacen autom√°ticamente.  El uso de clases espec√≠ficas en lugar de conversiones de tipo impl√≠citas generalmente no genera gastos generales adicionales, pero las etiquetas de clase ficticias pueden imponer restricciones en algunas optimizaciones del compilador. <br><br>  La API de Java no es muy diferente de la API de Scala en t√©rminos de propiedades, solo ocasionalmente faltan algunas funcionalidades o API de desarrollador.  Otros lenguajes de programaci√≥n JVM, como el lenguaje Clojure con DSL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Flambo</a> y la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">brillante</a> , son <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">compatibles</a> con varias API de Java en lugar de llamar directamente a la API de Scala.  Dado que la mayor√≠a de los enlaces de idiomas, incluso los lenguajes que no son JVM como Python y R, pasan por la API de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Java</a> , ser√° √∫til tratar con ellos. <br><br>  Las API de Java son muy similares a las API de Scala, aunque son independientes de las etiquetas de clase y las conversiones impl√≠citas.  La ausencia de este √∫ltimo significa que, en lugar de convertir autom√°ticamente los conjuntos RDD de Tuple u objetos dobles en clases especiales con funciones adicionales, debe usar funciones de conversi√≥n de tipo expl√≠cito (por ejemplo, mapToDouble o mapToPair).  Las funciones especificadas se definen solo para conjuntos Java RDD;  Afortunadamente para la compatibilidad, estos tipos especiales son solo adaptadores para conjuntos Scala RDD.  Adem√°s, estas funciones especiales devuelven varios tipos de datos, como JavaDoubleRDD y JavaPairRDD, con caracter√≠sticas proporcionadas por transformaciones de lenguaje Scala impl√≠citas. <br><br>  Pasemos nuevamente al ejemplo can√≥nico del conteo de palabras usando la API de Java (Ejemplo 7.1).  Dado que llamar a la API de Scala desde Java a veces puede ser una tarea desalentadora, casi todas las API de framework de Spark Java se implementan en el lenguaje Scala con etiquetas de clase ocultas y conversiones impl√≠citas.  Debido a esto, los adaptadores Java son una capa muy delgada, que en promedio consta de solo unas pocas l√≠neas de c√≥digo, y reescribirlos es pr√°cticamente sin esfuerzo. <br><br>  Ejemplo 7.1  Recuento de palabras (Java) <br><br><pre><code class="hljs actionscript"><span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> scala.Tuple2;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaRDD;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaPairRDD </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaSparkContext;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.regex.Pattern;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.Arrays;</span></span>  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">WordCount</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> Pattern pattern = Pattern.compile(<span class="hljs-string"><span class="hljs-string">" "</span></span>);  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">void</span></span> main(String[] args) throws Exception { JavaSparkContext jsc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JavaSparkContext(); JavaRDD&lt;String&gt; lines = jsc.textFile(args[<span class="hljs-number"><span class="hljs-number">0</span></span>]); JavaRDD&lt;String&gt; words = lines.flatMap(e -&gt; Arrays.asList(                                           pattern.split(e)).iterator()); JavaPairRDD&lt;String, Integer&gt; wordsIntial = words.mapToPair(  e -&gt; <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Tuple2&lt;String, Integer&gt;(e, <span class="hljs-number"><span class="hljs-number">1</span></span>));   } }</code> </pre> <br>  A veces puede necesitar convertir RDD de Java a RDD de Scala o viceversa.  Esto se necesita con mayor frecuencia para las bibliotecas que requieren entrada o devuelven conjuntos Scala RDD, pero a veces las propiedades b√°sicas de Spark a√∫n no est√°n disponibles en la API Java.  La conversi√≥n de RDD de Java a RDD de Scala es la forma m√°s f√°cil de usar estas nuevas funciones. <br><br>  Si necesita transferir el conjunto RDD de Java a la biblioteca Scala, que espera un RDD Spark regular en la entrada, puede acceder al RDD Scala subyacente utilizando el m√©todo rdd ().  Muy a menudo, esto es suficiente para transferir el RDD final a cualquier biblioteca Scala deseada;  Entre las excepciones notables se encuentran las bibliotecas Scala, que se basan en conversiones impl√≠citas de tipos de tipos de contenido o informaci√≥n de etiquetas de clase en su trabajo.  En este caso, la forma m√°s f√°cil de acceder a las conversiones impl√≠citas es escribir un peque√±o adaptador en Scala.  Si no se pueden usar los shells de Scala, puede llamar a la funci√≥n correspondiente de la clase <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">JavaConverters</a> y formar una etiqueta de clase ficticia. <br><br>  Para crear una etiqueta de clase ficticia, puede usar el m√©todo scala.reflect.ClassTag $ .MODULE $ .AnyRef () u obtener el real usando scala.reflect.ClassTag $ .MODULE $ .apply (CLASS), como se muestra en los ejemplos 7.2 y 7.3. <br><br>  Para convertir de Scala RDD a RDD Java, la informaci√≥n de etiqueta de clase suele ser m√°s importante que la mayor√≠a de las bibliotecas de Spark.  La raz√≥n es que, aunque varias clases JavaRDD proporcionan constructores de acceso p√∫blico que toman Scala RDD como argumentos, est√°n destinados a ser llamados desde el c√≥digo Scala y, por lo tanto, requieren informaci√≥n sobre la etiqueta de clase. <br><br>  Las etiquetas de clase ficticias se usan con mayor frecuencia en c√≥digo gen√©rico o de plantilla, donde los tipos exactos son desconocidos en el momento de la compilaci√≥n.  Tales etiquetas son a menudo suficientes, aunque existe la posibilidad de perder algunos matices en el lado del c√≥digo Scala;  en casos muy raros, el c√≥digo Scala requiere informaci√≥n precisa de la etiqueta de clase.  En este caso, tendr√° que usar una etiqueta real.  En la mayor√≠a de los casos, esto no requiere mucho esfuerzo y mejora el rendimiento, por lo tanto, intente utilizar dichas etiquetas siempre que sea posible. <br><br>  Ejemplo 7.2.  Hacer que Java / Scala RDD sea compatible con una etiqueta de clase ficticia <br><br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static JavaPairRDD wrapPairRDDFakeCt( RDD&lt;Tuple2&lt;String, <span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt;&gt; RDD) { //       AnyRef ‚Äî   //        , //        , //        //    ClassTag&lt;<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt; fake = ClassTag$.MODULE$.AnyRef(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> JavaPairRDD(rdd, fake, fake); }</code> </pre> <br>  Ejemplo 7.3.  Garantizar la compatibilidad de Java / Scala RDD <br><br><pre> <code class="hljs ruby">public static JavaPairRDD wrapPairRDD( RDD&lt;Tuple2&lt;String, Object<span class="hljs-meta"><span class="hljs-meta">&gt;&gt; </span></span>RDD) { <span class="hljs-regexp"><span class="hljs-regexp">//</span></span>    ClassTag&lt;String&gt; strCt = ClassTag$.MODULE$.apply(String.class); ClassTag&lt;Long&gt; longCt = ClassTag$.MODULE$.apply(scala.Long.class); return new JavaPairRDD(rdd, strCt, longCt); }</code> </pre> <br>  Tanto las API de canalizaci√≥n de Spark SQL como las de ML se hicieron en su mayor parte consistentes en Java y Scala.  Sin embargo, existen funciones auxiliares espec√≠ficas de Java, y las funciones de Scala equivalentes a ellas no son f√°ciles de llamar.  Estos son sus ejemplos: varias funciones num√©ricas, como m√°s, menos, etc., para la clase Columna.  Es dif√≠cil llamar a sus equivalentes sobrecargados del lenguaje Scala (+, -).  En lugar de utilizar JavaDataFrame y JavaSQLContext, los m√©todos requeridos por Java est√°n disponibles en SQLContext y en conjuntos de DataFrame regulares.  Esto puede confundirlo, porque algunos de los m√©todos mencionados en la documentaci√≥n de Java no se pueden usar desde el c√≥digo de Java, pero en tales casos se proporcionan funciones con los mismos nombres para llamar desde Java. <br><br>  Las funciones definidas por el usuario (UDF) en el lenguaje Java, y para el caso, en la mayor√≠a de los otros idiomas, excepto Scala, requieren especificar el tipo del valor devuelto por la funci√≥n, ya que no se puede deducir l√≥gicamente, de forma similar a c√≥mo se realiza en el lenguaje Scala (ejemplo 7.4) . <br><br>  Ejemplo 7.4.  Muestra UDF para Java <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">sqlContext</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.udf</span></span>() <span class="hljs-selector-class"><span class="hljs-selector-class">.register</span></span>("<span class="hljs-selector-tag"><span class="hljs-selector-tag">strlen</span></span>", (<span class="hljs-selector-tag"><span class="hljs-selector-tag">String</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span>) <span class="hljs-selector-tag"><span class="hljs-selector-tag">-</span></span>&gt; <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.length</span></span>(), <span class="hljs-selector-tag"><span class="hljs-selector-tag">DataTypes</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.StringType</span></span>);</code> </pre> <br>  Aunque los tipos requeridos por las API de Scala y Java son diferentes, envolver los tipos de colecci√≥n Java no requiere copia adicional.  En el caso de los iteradores, la conversi√≥n de tipo requerida para el adaptador se realiza de manera retardada a medida que se accede a los elementos, lo que permite que el marco de Spark descargue datos si es necesario (como se discuti√≥ en la secci√≥n "Realizaci√≥n de transformaciones de iterador-iterador usando la funci√≥n mapPartitions" en la p√°gina 121).  Esto es muy importante porque para muchas operaciones simples el costo de copiar datos puede ser mayor que el costo del c√°lculo en s√≠. <br><br><h3>  M√°s all√° de Scala y JVM </h3><br>  Si no se limita a la JVM, entonces la cantidad de lenguajes de programaci√≥n disponibles para trabajar aumenta dram√°ticamente.  Sin embargo, con la arquitectura actual de Spark, trabajar fuera de la JVM, especialmente en los nodos de trabajo, puede generar un aumento significativo de los costos debido a la copia de datos en los nodos de trabajo entre la JVM y el c√≥digo de idioma de destino.  En operaciones complejas, la parte del costo de copiar datos es relativamente peque√±a, pero en operaciones simples puede conducir f√°cilmente a una duplicaci√≥n del costo computacional total. <br><br>  El primer lenguaje de programaci√≥n no JVM que se admite directamente fuera de Spark es Python, su API e interfaz se han convertido en el modelo en el que se basan las implementaciones para otros lenguajes de programaci√≥n no JVM. <br><br><h3>  C√≥mo funciona PySpark </h3><br>  PySpark se conecta a JVM Spark utilizando una combinaci√≥n de canales en los trabajadores y Py4J, una biblioteca especializada que proporciona interacci√≥n Python / Java, en el controlador.  Bajo esto, a primera vista, la arquitectura simple esconde muchos matices complejos, gracias a los cuales funciona PySpark, como se muestra en la Fig.  7.1.  Uno de los principales problemas: incluso cuando los datos se copian de un trabajador de Python a la JVM, no es en la forma en que una m√°quina virtual puede analizar f√°cilmente.  Se requieren esfuerzos especiales tanto del trabajador de Python como de Java para garantizar que la JVM tenga suficiente informaci√≥n para operaciones como la partici√≥n. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/10/ez/wf/10ezwfv-1jvl1gxwsansnexwvj4.png" alt="imagen"></div><br><h3>  Kits PySpark RDD </h3><br>  El costo de los recursos para transferir datos hacia y desde la JVM, as√≠ como para ejecutar el ejecutor de Python, es significativo.  Puede evitar muchos problemas de rendimiento con las API de PySpark RDD Suite utilizando las API DataFrame / Dataset, porque los datos permanecen en la JVM durante el mayor tiempo posible. <br><br>  La copia de datos de la JVM a Python se realiza mediante sockets y bytes serializados.  Una versi√≥n m√°s general para interactuar con programas en otros idiomas est√° disponible a trav√©s de la interfaz PipedRDD, cuya aplicaci√≥n se muestra en la subsecci√≥n "Uso de la tuber√≠a". <br><br>  La organizaci√≥n de canales para el intercambio de datos (en dos direcciones) para cada transformaci√≥n ser√≠a demasiado costosa.  Como resultado, PySpark organiza (si es posible) la tuber√≠a de transformaci√≥n de Python dentro del int√©rprete de Python, encadenando la operaci√≥n de filtro, y luego el mapa, en el iterador de objetos de Python usando la clase especializada PipelinedRDD.  Incluso cuando necesite mezclar datos y PySpark no pueda encadenar conversiones en la m√°quina virtual de un trabajador individual, puede reutilizar el int√©rprete de Python, por lo que el costo de iniciar el int√©rprete no disminuir√° a√∫n m√°s. <br><br>  Esto es solo una parte del rompecabezas.  Los PipedRDD normales funcionan con el tipo String, que no es tan f√°cil de mezclar debido a la falta de una clave natural.  En PySpark, y en su imagen y similitud en las bibliotecas vinculadas a muchos otros lenguajes de programaci√≥n, se utiliza un tipo especial de PairwiseRDD, donde la clave es un entero largo, y su deserializaci√≥n se realiza mediante c√≥digo de usuario en el lenguaje Scala, destinado a analizar los valores de Python.  El costo de esta deserializaci√≥n no es demasiado alto, pero demuestra que Scala en el marco de Spark b√°sicamente considera que los resultados del c√≥digo Python funcionan como conjuntos de bytes "opacos". <br><br>  Para toda su simplicidad, este enfoque de integraci√≥n funciona sorprendentemente bien, y la mayor√≠a de las operaciones en conjuntos Scala RDD est√°n disponibles en Python.  En algunos de los lugares m√°s dif√≠ciles del c√≥digo, se accede a las bibliotecas, por ejemplo, MLlib, as√≠ como a cargar / guardar datos de varias fuentes. <br><br>  Trabajar con varios formatos de datos tambi√©n impone sus limitaciones, ya que una parte importante del c√≥digo para cargar / guardar datos del marco de Spark se basa en las interfaces Java de Hadoop.  Esto significa que todos los datos cargados se cargan primero en la JVM, y solo luego se mueven a Python. <br><br>  Por lo general, se utilizan dos enfoques para interactuar con MLlib: PySpark usa un tipo de datos especializado con conversiones de tipo Scala o el algoritmo se vuelve a implementar en Python.  Estos problemas se pueden evitar con el paquete Spark ML, que utiliza la interfaz DataFrame / Dataset, que generalmente almacena datos en la JVM. <br><br><h3>  PySpark DataFrame y kits de conjuntos de datos </h3><br>  Los conjuntos de DataFrame y Dataset no tienen muchos problemas de rendimiento con las API de Python RDD Set porque almacenan datos en la JVM durante el mayor tiempo posible.  La misma prueba de rendimiento que realizamos para ilustrar la superioridad de los conjuntos de DataFrame sobre los conjuntos RDD (consulte la Figura 3.1) muestra diferencias significativas cuando se ejecuta en Python (Figura 7.2). <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d9/mk/tl/d9mktl7qhe3hg8z2e9lnyanlgde.png" alt="imagen"></div><br>  Para muchas operaciones con conjuntos de DataFrame y Dataset, es posible que no necesite mover datos de la JVM, aunque usar varias expresiones lambda UDF, UDAF y Python naturalmente requiere mover algunos de los datos a la JVM.  Esto lleva al siguiente esquema simplificado para muchas operaciones, que se parece al que se muestra en la Fig.  7.3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4e/3q/el/4e3qel6hamrvb5ipzycqh9sftcg.png" alt="imagen"></div><br><h3>  Acceso a objetos Java subyacentes y c√≥digo mixto en Scala </h3><br>  Una consecuencia importante de la arquitectura PySpark es que muchas de las clases de framework de Spark Python son en realidad adaptadores para traducir llamadas del c√≥digo de Python a una forma JVM comprensible. <br><br>  Si trabaja con desarrolladores de Scala / Java y desea interactuar con su c√≥digo, de antemano no habr√° adaptadores para acceder a su c√≥digo, pero puede registrar su UDF de Java / Scala y usarlos desde el c√≥digo Python.  Comenzando con Spark 2.1, esto se puede hacer usando el m√©todo registerJavaFunction del objeto sqlContext. <br><br>  A veces, estos adaptadores no tienen todos los mecanismos necesarios, y dado que Python no tiene una protecci√≥n s√≥lida contra la llamada a m√©todos privados, puede recurrir inmediatamente a la JVM.  La misma t√©cnica le permitir√° acceder a su propio c√≥digo en la JVM y, con poco esfuerzo, volver a convertir los resultados en objetos de Python. <br><br>  En la subsecci√≥n "Grandes planes de consulta y algoritmos iterativos" en la p√°g.  91 notamos la importancia de usar la versi√≥n JVM de los conjuntos DataFrame y RDD para reducir el plan de consulta.  Esta es una soluci√≥n alternativa, porque cuando los planes de consulta se vuelven demasiado grandes para ser procesados ‚Äã‚Äãpor el optimizador Spark SQL, el optimizador SQL, al colocar el conjunto RDD en el medio, pierde la capacidad de mirar m√°s all√° del momento en que los datos aparecen en RDD.  Lo mismo se puede lograr con la ayuda de las API p√∫blicas de Python, sin embargo, muchas de las ventajas de los conjuntos de DataFrame se perder√°n, porque todos los datos tendr√°n que ir y venir a trav√©s de los nodos de trabajo de Python.  En cambio, puede reducir el gr√°fico de origen si contin√∫a almacenando datos en la JVM (como se muestra en el Ejemplo 7.5). <br><br>  Ejemplo 7.5  Recortar un plan de consulta grande para un DataFrame usando Python <br><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cutLineage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    DataFrame ‚Äî     .. :              &gt;&gt;&gt; df = RDD.toDF() &gt;&gt;&gt; cutDf = cutLineage(df) &gt;&gt;&gt; cutDf.count() 3 """</span></span> jRDD = df._jdf.toJavaRDD() jSchema = df._jdf.schema() jRDD.cache() sqlCtx = df.sql_ctx <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx newJavaDF = javaSqlCtx.createDataFrame(jRDD, jSchema) newDF = DataFrame(newJavaDF, sqlCtx) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> newDF</code> </pre> <br>  En t√©rminos generales, por convenci√≥n, la sintaxis _j [nombre_ abreviado] se utiliza para acceder a las versiones internas de Java de la mayor√≠a de los objetos de Python.  Entonces, por ejemplo, el objeto SparkContext tiene _jsc, que le permite obtener el objeto interno SparkContext Java.  Esto solo es posible en el programa controlador, por lo que cuando env√≠e objetos PySpark a nodos de trabajo, no podr√° acceder al componente interno de Java y la mayor√≠a de las API no funcionar√°n. <br><br>  Para acceder a la clase Spark en la JVM, que no tiene un adaptador Python, puede usar la puerta de enlace Py4J en el controlador.  El objeto SparkContext contiene un enlace a la puerta de enlace en la propiedad _gateway.  La sintaxis sc._gateway.jvm. [Full_class_name_in_JVM] permitir√° el acceso a cualquier objeto Java. <br><br>  Una t√©cnica similar funcionar√° para sus propias clases Scala si est√°n organizadas de acuerdo con el classpath.  Puede agregar archivos JAR al classpath usando el comando spark-submit con el par√°metro --jars o estableciendo las propiedades de configuraci√≥n spark.driver.extraClassPath.  Ejemplo 7.6, que ayud√≥ a generar arroz.  7.2, est√° dise√±ado intencionalmente para generar datos para pruebas de rendimiento utilizando el c√≥digo Scala existente. <br><br>  Ejemplo 7.6  Llamando a clases que no son Spark-JVM usando Py4J <br><br><pre> <code class="hljs pgsql">sc = sqlCtx._sc #  <span class="hljs-keyword"><span class="hljs-keyword">SQL</span></span> Context,   <span class="hljs-number"><span class="hljs-number">2.1</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>   , #  <span class="hljs-number"><span class="hljs-number">2.0</span></span>, ‚Äî  ,   :p try: try: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._jwrapped jsc = sc._jsc scalasc = jsc.sc() gateway = sc._gateway #  java-,   RDD JVM- # <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> (<span class="hljs-type"><span class="hljs-type">Int</span></span>, <span class="hljs-type"><span class="hljs-type">Double</span></span>).   RDD  Python   #  RDD  Java (   <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>),   # ,      . #   Java-RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> ‚Äî     #    DataFrame,     #    RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>. java_rdd = (gateway.jvm.com.highperformancespark.examples. tools.GenerateScalingData. generateMiniScaleRows(scalasc, <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span>, numCols)) #     <span class="hljs-type"><span class="hljs-type">JSON</span></span>     . #  Python-     Java-. schema = StructType([ StructField("zip", IntegerType()), StructField("fuzzyness", DoubleType())]) #   <span class="hljs-number"><span class="hljs-number">2.1</span></span> /  <span class="hljs-number"><span class="hljs-number">2.1</span></span> try: jschema = javaSqlCtx.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: jschema = sqlCtx._jsparkSession.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) #  RDD (Java)  DataFrame (Java) java_dataframe = javaSqlCtx.createDataFrame(java_rdd, jschema) #  DataFrame (Java)  DataFrame (Python) python_dataframe = DataFrame(java_dataframe, sqlCtx) #  DataFrame (Python)   RDD pairRDD = python_dataframe.rdd.map(lambda <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>: (<span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (python_dataframe, pairRDD)</code> </pre> <br><br>  Aunque muchas clases de Python son simplemente adaptadores de objetos de Java, no todos los objetos de Java pueden envolverse en objetos de Python y luego usarse en Spark.  Por ejemplo, los objetos en los conjuntos PySpark RDD se representan como cadenas serializadas, que solo se pueden analizar f√°cilmente en el c√≥digo Python.  Afortunadamente, los objetos DataFrame est√°n estandarizados entre diferentes lenguajes de programaci√≥n, por lo que si puede convertir sus datos en conjuntos de DataFrame, puede envolverlos en objetos Python y usarlos directamente como un Python DataFrame o convertir un Python DataFrame en un RDD de este mismo idioma <br><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br>  20% de descuento en cupones para pulverizadores - <b>Spark</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es414525/">https://habr.com/ru/post/es414525/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es414513/index.html">27 excelentes herramientas de desarrollo web de c√≥digo abierto</a></li>
<li><a href="../es414515/index.html">Lecci√≥n de optimizaci√≥n del servidor de aplicaciones web</a></li>
<li><a href="../es414517/index.html">Cient√≠ficos de Oxford: la probabilidad de que estemos solos en la parte previsible del universo es mucho mayor que cero</a></li>
<li><a href="../es414519/index.html">¬øC√≥mo convertir 15 minutos de reuniones de Scrum en una casa llena?</a></li>
<li><a href="../es414523/index.html">Comparaci√≥n de quadcopters DJI Mavic Pro y Mavic Air</a></li>
<li><a href="../es414527/index.html">Lo que nos espera en Highload ++ Siberia, excepto los osos pintados</a></li>
<li><a href="../es414531/index.html">Nadie sabe qu√© pasar√° con las compras en l√≠nea a partir del 1 de julio</a></li>
<li><a href="../es414535/index.html">Manchester: el lugar de nacimiento del desaliento, el post-punk y dos famosos clubes de f√∫tbol</a></li>
<li><a href="../es414537/index.html">C√≥mo hicimos uno de los mejores juegos de AR del mundo sin tr√°fico pagado</a></li>
<li><a href="../es414539/index.html">Los cibercriminales roban cada vez m√°s datos personales de los rusos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>