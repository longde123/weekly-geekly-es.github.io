<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßúüèª ü§π ‚ò£Ô∏è Integra√ß√£o do Spark Streaming e Kafka üë©üèº‚Äç‚öñÔ∏è üíáüèΩ üöü</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° colegas! Lembramos que, h√° pouco tempo, publicamos um livro sobre o Spark e, atualmente, um livro sobre Kafka est√° passando pela revis√£o mais rece...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Integra√ß√£o do Spark Streaming e Kafka</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/417123/">  Ol√° colegas!  Lembramos que, h√° pouco tempo, publicamos um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livro sobre o Spark</a> e, atualmente, um <a href="">livro sobre Kafka</a> est√° passando pela revis√£o mais recente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/it/cf/nj/itcfnjaffoo8apwikyd7_yfym5s.jpeg"></div><br>  Esperamos que esses livros tenham √™xito o suficiente para continuar o t√≥pico - por exemplo, para a tradu√ß√£o e publica√ß√£o de literatura sobre o Spark Streaming.  Quer√≠amos oferecer hoje uma tradu√ß√£o sobre a integra√ß√£o dessa tecnologia ao Kafka. <br><a name="habracut"></a><br>  <b>1. Justifica√ß√£o</b> <br><br>  O Apache Kafka + Spark Streaming √© uma das melhores combina√ß√µes para criar aplicativos em tempo real.  Neste artigo, discutiremos em detalhes os detalhes dessa integra√ß√£o.  Al√©m disso, veremos um exemplo com o Spark Streaming-Kafka.  Em seguida, discutimos a ‚Äúabordagem de destinat√°rio‚Äù e a op√ß√£o de integra√ß√£o direta do Kafka e Spark Streaming.  Ent√£o, vamos come√ßar a integrar o Kafka e o Spark Streaming. <br><br><img src="https://habrastorage.org/webt/8x/jl/cp/8xjlcpzhwdwi4w2g87iifbquvr0.jpeg"><br><br>  <b>2. Integra√ß√£o do Kafka e Spark Streaming</b> <br><br>  Ao integrar o Apache Kafka e o Spark Streaming, existem duas abordagens poss√≠veis para configurar o Spark Streaming para receber dados do Kafka - ou seja,  duas abordagens para integrar o Kafka e o Spark Streaming.  Primeiro, voc√™ pode usar os destinat√°rios e a API Kafka de alto n√≠vel.  A segunda abordagem (mais recente) √© o trabalho sem destinat√°rios.  Existem diferentes modelos de programa√ß√£o para ambas as abordagens, diferindo, por exemplo, em termos de desempenho e garantias sem√¢nticas. <br><br><img src="https://habrastorage.org/webt/91/mn/sk/91mnsklu_81q0nx9aadnjgya4fc.png"><br><br>  Vamos considerar essas abordagens em mais detalhes. <br><br>  <i><b>a.</b></i>  <i><b>Abordagem Baseada em Destinat√°rio</b></i> <br><br>  Nesse caso, a recep√ß√£o de dados √© fornecida pelo destinat√°rio.  Portanto, usando a API de alto n√≠vel de consumo fornecida pela Kafka, implementamos o Destinat√°rio.  Al√©m disso, os dados recebidos s√£o armazenados no Spark Artists.  Em seguida, os trabalhos s√£o iniciados no Kafka - Spark Streaming, no qual os dados s√£o processados. <br><br>  No entanto, ao usar essa abordagem, o risco de perda de dados em caso de falha (com a configura√ß√£o padr√£o) permanece.  Conseq√ºentemente, ser√° necess√°rio incluir adicionalmente um log write-ahead no Kafka - Spark Streaming para eliminar a perda de dados.  Assim, todos os dados recebidos do Kafka s√£o armazenados de forma s√≠ncrona no log write-ahead em um sistema de arquivos distribu√≠do.  √â por isso que, mesmo ap√≥s uma falha do sistema, todos os dados podem ser restaurados. <br><br>  A seguir, veremos como usar essa abordagem com os destinat√°rios em um aplicativo com o Kafka - Spark Streaming. <br><br>  <i>eu</i>  <i>Encaderna√ß√£o</i> <br><br>  Agora, conectaremos nosso aplicativo de streaming ao artefato a seguir para aplicativos Scala / Java, usaremos as defini√ß√µes do projeto para o SBT / Maven. <br><br><pre><code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  No entanto, ao implantar nosso aplicativo, teremos que adicionar a biblioteca mencionada e suas depend√™ncias, isso ser√° necess√°rio para aplicativos Python. <br><br>  <i>ii.</i>  <i>Programa√ß√£o</i> <br><br>  Em seguida, crie um fluxo de entrada <code>KafkaUtils</code> importando <code>KafkaUtils</code> para o c√≥digo do aplicativo de fluxo: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])</code> </pre> <br>  Al√©m disso, usando as op√ß√µes createStream, voc√™ pode especificar classes de chave e classes de valor, bem como as classes correspondentes para sua decodifica√ß√£o. <br><br>  <i>iii.</i>  <i>Implanta√ß√£o</i> <br><br>  Como em qualquer aplicativo Spark, o comando spark-submit √© usado para iniciar.  No entanto, os detalhes s√£o ligeiramente diferentes nos aplicativos Scala / Java e nos aplicativos Python. <br><br>  Al√©m disso, com <code>‚Äìpackages</code> voc√™ pode adicionar <code>spark-streaming-Kafka-0-8_2.11</code> e suas depend√™ncias diretamente ao <code>spark-submit</code> , isso √© √∫til para aplicativos Python em que √© imposs√≠vel gerenciar projetos usando o SBT / Maven. <br><br><pre> <code class="java hljs">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span>:<span class="hljs-number"><span class="hljs-number">2.2</span></span>.0 ...</code> </pre> <br>  Voc√™ tamb√©m pode fazer o download do arquivo JAR do artefato Maven <code>spark-streaming-Kafka-0-8-assembly</code> no reposit√≥rio Maven.  Em seguida, adicione-o para <code>spark-submit</code> com - <code>jars</code> . <br><br>  <i>b.</i>  <i>Abordagem direta (sem destinat√°rios)</i> <br><br>  Ap√≥s a abordagem usando os destinat√°rios, uma abordagem mais recente foi desenvolvida - a "direta".  Ele fornece garantias completas e confi√°veis.  Nesse caso, perguntamos periodicamente √† Kafka sobre as compensa√ß√µes de compensa√ß√µes para cada t√≥pico / se√ß√£o e n√£o organizamos a entrega de dados pelos destinat√°rios.  Al√©m disso, o tamanho do fragmento de leitura √© determinado, isto √© necess√°rio para o processamento correto de cada pacote.  Por fim, uma API de consumo simples √© usada para ler intervalos com dados do Kafka com os deslocamentos especificados, especialmente quando os trabalhos de processamento de dados s√£o iniciados.  Todo o processo √© como ler arquivos de um sistema de arquivos. <br><br>  Nota: Esse recurso apareceu no Spark 1.3 para Scala e na API Java, bem como no Spark 1.4 para a API Python. <br><br>  Agora vamos discutir como aplicar essa abordagem em nosso aplicativo de streaming. <br>  A API do consumidor √© descrita em mais detalhes no seguinte link: <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Consumidor Apache Kafka |</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Exemplos de Consumidor Kafka</a> <br><br>  eu  Encaderna√ß√£o <br><br>  √â verdade que essa abordagem √© suportada apenas em aplicativos Scala / Java.  Com o artefato a seguir, construa o projeto SBT / Maven. <br><br><pre> <code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  <i>ii.</i>  <i>Programa√ß√£o</i> <br><br>  Em seguida, importe o KafkaUtils e crie um <code>DStream</code> entrada no c√≥digo do aplicativo de fluxo: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val directKafkaStream = KafkaUtils.createDirectStream[ [key <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">key</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">] ]( </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">streamingContext</span></span></span><span class="hljs-class">, [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">map</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Kafka</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">parameters</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">set</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">topics</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">to</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">consume</span></span></span><span class="hljs-class">])</span></span></code> </pre> <br>  Nos par√¢metros Kafka, voc√™ precisar√° especificar <code>metadata.broker.list</code> ou <code>bootstrap.servers</code> .  Portanto, por padr√£o, consumiremos dados a partir do √∫ltimo deslocamento em cada se√ß√£o do Kafka.  No entanto, se voc√™ deseja que a leitura comece no menor fragmento, nos par√¢metros Kafka, √© necess√°rio definir a op√ß√£o de configura√ß√£o <code>auto.offset.reset</code> . <br><br>  Al√©m disso, trabalhando com as op√ß√µes <code>KafkaUtils.createDirectStream</code> , voc√™ pode come√ßar a ler de um deslocamento arbitr√°rio.  Em seguida, faremos o seguinte, o que nos permitir√° acessar os fragmentos Kafka consumidos em cada pacote. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      ,        var offsetRanges = Array.empty[OffsetRange] directKafkaStream.transform { rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd }.map { ... }.foreachRDD { rdd =&gt; for (o &lt;- offsetRanges) { println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}") } ... }</span></span></code> </pre> <br>  Se queremos organizar o monitoramento do Kafka com base no Zookeeper usando ferramentas especiais, podemos atualizar o Zookeeper por conta pr√≥pria com a ajuda deles. <br><br>  <i>iii.</i>  <i>Implanta√ß√£o</i> <br><br>  O processo de implanta√ß√£o neste caso se assemelha ao processo de implanta√ß√£o na variante com o destinat√°rio. <br><br>  <b>3. Os benef√≠cios de uma abordagem direta</b> <br><br>  A segunda abordagem para integrar o Spark Streaming ao Kafka supera a primeira pelos seguintes motivos: <br><br>  <b><i>a.</i></b>  <b><i>Concorr√™ncia simplificada</i></b> <br><br>  Nesse caso, voc√™ n√£o precisa criar muitos fluxos de entrada Kafka e combin√°-los.  No entanto, o Kafka - Spark Streaming criar√° tantos segmentos RDD quanto haver√° segmentos Kafka para consumo.  Todos esses dados Kafka ser√£o lidos em paralelo.  Portanto, podemos dizer que teremos uma correspond√™ncia individual entre os segmentos Kafka e RDD, e esse modelo √© mais compreens√≠vel e mais f√°cil de configurar. <br><br>  <i><b>b.</b></i>  <i><b>Efic√°cia</b></i> <br><br>  Para eliminar completamente a perda de dados durante a primeira abordagem, as informa√ß√µes precisavam ser armazenadas em um log dos principais registros e, em seguida, replicadas.  De fato, isso √© ineficiente porque os dados s√£o replicados duas vezes: a primeira vez pelo pr√≥prio Kafka e a segunda pelo log de grava√ß√£o antecipada.  Na segunda abordagem, esse problema √© eliminado, pois n√£o h√° destinat√°rio e, portanto, nenhum di√°rio de grava√ß√£o principal √© necess√°rio.  Se tivermos um armazenamento de dados suficientemente longo no Kafka, voc√™ poder√° recuperar mensagens diretamente do Kafka. <br><br>  <b><i>s</i></b>  <b><i>Sem√¢ntica Exatamente Uma Vez</i></b> <br><br>  Basicamente, usamos a API Kafka de alto n√≠vel na primeira abordagem para armazenar fragmentos de leitura consumidos no Zookeeper.  No entanto, esse √© o costume de consumir dados do Kafka.  Embora a perda de dados possa ser eliminada com seguran√ßa, h√° uma pequena chance de que, em algumas falhas, os registros individuais possam ser consumidos duas vezes.  O ponto principal √© a inconsist√™ncia entre o mecanismo confi√°vel de transfer√™ncia de dados no Kafka - Spark Streaming e a leitura de fragmentos que ocorrem no Zookeeper.  Portanto, na segunda abordagem, usamos a API Kafka simples, que n√£o requer recurso ao Zookeeper.  Aqui, os fragmentos de leitura s√£o rastreados no Kafka - Spark Streaming, para isso, s√£o utilizados pontos de controle.  Nesse caso, a inconsist√™ncia entre o Spark Streaming e o Zookeeper / Kafka √© eliminada. <br><br>  Portanto, mesmo em caso de falhas, o Spark Streaming recebe cada registro estritamente uma vez.  Aqui, precisamos garantir que nossa opera√ß√£o de sa√≠da, na qual os dados s√£o armazenados no armazenamento externo, seja idempotente ou uma transa√ß√£o at√¥mica na qual os resultados e as compensa√ß√µes sejam armazenados.  √â assim que a sem√¢ntica exata √© alcan√ßada na deriva√ß√£o de nossos resultados. <br><br>  Embora haja uma desvantagem: as compensa√ß√µes no Zookeeper n√£o s√£o atualizadas.  Portanto, as ferramentas de monitoramento do Kafka baseadas no Zookeeper n√£o permitem acompanhar o progresso. <br>  No entanto, ainda podemos nos referir a compensa√ß√µes, se o processamento for organizado dessa maneira - recorremos a cada pacote e atualizamos o Zookeeper. <br><br>  √â tudo o que queremos falar sobre a integra√ß√£o do Apache Kafka e Spark Streaming.  Esperamos que voc√™ tenha gostado. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417123/">https://habr.com/ru/post/pt417123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt417111/index.html">Confer√™ncias online: streaming vs webinar</a></li>
<li><a href="../pt417113/index.html">Impressora 3D italiana na R√∫ssia: Raise3D N1 Dual - modelagem e prototipagem</a></li>
<li><a href="../pt417115/index.html">Enterrar ou queimar Flutter.io?</a></li>
<li><a href="../pt417117/index.html">Engenharia reversa do emulador NES no jogo para GameCube</a></li>
<li><a href="../pt417119/index.html">Pagina√ß√£o no Vue.js</a></li>
<li><a href="../pt417125/index.html">RTC Meetup .Net: convite para a primeira reuni√£o</a></li>
<li><a href="../pt417127/index.html">Tesla assina acordo para construir o Gigafactory 3 na China</a></li>
<li><a href="../pt417129/index.html">Universo da mente</a></li>
<li><a href="../pt417131/index.html">Como sentir as transa√ß√µes no MongoDB agora</a></li>
<li><a href="../pt417135/index.html">Unity3D: como descobrir o grau de ilumina√ß√£o de um ponto em uma cena?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>