<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🚒 👩🏾‍🤝‍👩🏼 🥡 Introdução à Aprendizagem Reforçada 👨🏾‍🤝‍👨🏽 🌼 👨🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá pessoal! 

 Abrimos um novo fluxo para o curso Machine Learning . Portanto, aguarde no futuro próximo artigos relacionados a essa disciplina, por ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introdução à Aprendizagem Reforçada</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Olá pessoal! <br><br>  Abrimos um novo fluxo para o curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning</a> . Portanto, aguarde no futuro próximo artigos relacionados a essa disciplina, por assim dizer.  Bem, é claro, seminários abertos.  Agora, vejamos o que é aprendizado por reforço. <br><br>  O aprendizado reforçado é uma forma importante de aprendizado de máquina, em que um agente aprende a se comportar em um ambiente executando ações e vendo resultados. <br><br>  Nos últimos anos, vimos muitos sucessos nesse fascinante campo de pesquisa.  Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind e Deep Q Learning Architecture</a> em 2014, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vitória sobre o go go com AlphaGo</a> em 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI e PPO</a> em 2017, entre outros. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  Nesta série de artigos, focaremos o estudo das diferentes arquiteturas usadas hoje para resolver o problema do aprendizado reforçado.  Estes incluem Q-learning, Deep Q-learning, Gradientes de Política, Critic Actor e PPO. <br><br>  Neste artigo, você aprenderá: <br><br><ul><li>  O que é aprendizado por reforço e por que recompensas são uma ideia central </li><li>  Três abordagens de aprendizado por reforço </li><li>  O que "profundo" significa na aprendizagem por reforço profundo </li></ul><br>  É muito importante dominar esses aspectos antes de mergulhar na implementação de agentes de aprendizado por reforço. <br><br>  A idéia do treinamento por reforço é que o agente aprenda com o ambiente interagindo com ele e recebendo recompensas por executar ações. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  Aprender através da interação com o meio ambiente vem de nossa experiência natural.  Imagine que você é uma criança na sala de estar.  Você vê a lareira e vai até ela. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  Perto quente, você se sente bem (recompensa positiva +1).  Você entende que o fogo é uma coisa positiva. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Mas então você tenta tocar o fogo.  Ai!  Ele queimou a mão (recompensa negativa -1).  Você acabou de perceber que o fogo é positivo quando você está a uma distância suficiente porque produz calor.  Mas se você se aproximar dele, você se queimará. <br><br>  É assim que as pessoas aprendem através da interação.  O aprendizado reforçado é simplesmente uma abordagem computacional do aprendizado através da ação. <br><br>  <b>Processo de aprendizagem por reforço</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Como exemplo, imagine um agente aprendendo a jogar Super Mario Bros.  O processo de Aprendizagem por Reforço (RL) pode ser modelado como um ciclo que funciona da seguinte maneira: <br><br><ul><li>  O agente recebe o estado S0 do ambiente (no nosso caso, obtemos o primeiro quadro do jogo (estado) de Super Mario Bros (ambiente)) </li><li>  Com base nesse estado S0, o agente executa a ação A0 (o agente se moverá para a direita) </li><li>  O ambiente passa para um novo estado S1 (novo quadro) </li><li>  O ambiente dá alguma recompensa ao agente R1 (não está morto: +1) </li></ul><br>  Esse ciclo RL produz uma sequência de <b>estados, ações e recompensas.</b> <br>  O objetivo do agente é maximizar as recompensas acumuladas esperadas. <br><br>  <b>Hipóteses de recompensa da ideia central</b> <br><br>  Por que o objetivo de um agente é maximizar as recompensas acumuladas esperadas?  Bem, o aprendizado por reforço é baseado na idéia de uma hipótese de recompensa.  Todas as metas podem ser descritas maximizando as recompensas acumuladas esperadas. <br><br>  <b>Portanto, no treinamento de reforço, para alcançar o melhor comportamento, precisamos maximizar as recompensas acumuladas esperadas.</b> <br><br>  A recompensa acumulada em cada etapa t pode ser escrita como: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Isso é equivalente a: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  No entanto, na realidade, não podemos simplesmente adicionar essas recompensas.  As recompensas que chegarem mais cedo (no início do jogo) são mais prováveis, pois são mais previsíveis do que as recompensas no futuro. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Suponha que seu agente seja um mouse pequeno e seu oponente seja um gato.  Seu objetivo é comer a quantidade máxima de queijo antes que o gato coma você.  Como podemos ver no diagrama, é mais provável que um rato coma queijo próximo a si mesmo do que queijo perto de um gato (quanto mais próximos estamos dele, mais perigoso é). <br><br>  Como resultado, a recompensa de um gato, mesmo que seja maior (mais queijo), será reduzida.  Não temos certeza de que podemos comê-lo.  Para reduzir a remuneração, fazemos o seguinte: <br><br><ul><li>  Determinamos a taxa de desconto chamada gama.  Deve estar entre 0 e 1. </li><li>  Quanto maior a gama, menor o desconto.  Isso significa que o agente de aprendizado está mais preocupado com recompensas de longo prazo. </li><li>  Por outro lado, quanto menor a gama, maior o desconto.  Isso significa que é dada prioridade às recompensas de curto prazo (queijo mais próximo). </li></ul><br>  A contraprestação esperada acumulada, considerando o desconto, é a seguinte: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  Grosso modo, cada recompensa será reduzida usando a gama no indicador de tempo.  À medida que o tempo aumenta, o gato se aproxima de nós, e a recompensa futura se torna cada vez menos provável. <br><br>  <b>Tarefas ocasionais ou contínuas</b> <br><br>  Uma tarefa é uma instância do problema de aprendizagem com reforço.  Podemos ter dois tipos de tarefas: episódica e contínua. <br><br>  <b>Tarefa episódica</b> <br><br>  Nesse caso, temos um ponto inicial e um ponto final <b>(estado terminal).</b>  <b>Isso cria um episódio</b> : uma lista de estados, ações, recompensas e novos estados. <br>  Veja o Super Mario Bros, por exemplo: o episódio começa com o lançamento do novo Mario e termina quando você é morto ou chega ao final do nível. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>O início de um novo episódio</i> <br><br>  <b>Tarefas contínuas</b> <br><br>  <b>Essas são tarefas que duram para sempre (sem um estado terminal)</b> .  Nesse caso, o agente deve aprender a escolher as melhores ações e ao mesmo tempo interagir com o ambiente. <br><br>  Por exemplo, um agente que realiza negociação automatizada de ações.  Não há ponto inicial nem estado terminal para esta tarefa.  <b>O agente continua trabalhando até que decidimos detê-lo.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Método Monte Carlo vs. Diferença de Tempo</b> <br><br>  Existem duas maneiras de aprender: <br><br><ul><li>  Coletando recompensas no final do episódio e calculando as recompensas futuras máximas esperadas - abordagem Monte Carlo </li><li>  Avaliação das recompensas a cada passo - uma diferença temporária </li></ul><br>  <b>Monte Carlo</b> <br><br>  Quando o episódio termina (o agente atinge um "estado terminal"), o agente analisa a recompensa acumulada total para ver o quão bem ele se saiu.  Na abordagem de Monte Carlo, as recompensas são recebidas apenas no final do jogo. <br><br>  Então começamos um novo jogo com conhecimento aumentado.  <b>O agente toma as melhores decisões com cada iteração.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Aqui está um exemplo: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Se tomarmos o labirinto como um ambiente: <br><br><ul><li>  Sempre começamos do mesmo ponto de partida. </li><li>  Paramos o episódio se o gato nos comer ou avançarmos&gt; 20 passos. </li><li>  No final do episódio, temos uma lista de estados, ações, recompensas e novos estados. </li><li>  O agente resume a recompensa total de Gt (para ver como ele se saiu). </li><li>  Em seguida, ele atualiza V (st) de acordo com a fórmula acima. </li><li>  Então, um novo jogo começa com novos conhecimentos. </li></ul><br>  Executando mais e mais episódios, o <b>agente aprenderá a jogar cada vez melhor.</b> <br><br>  <b>Diferenças horárias: aprendendo a cada passo</b> <br><br>  O método Temporal Difference Learning (TD) não aguardará o final do episódio para atualizar a maior recompensa possível.  Ele atualizará o V dependendo da experiência adquirida. <br><br>  Esse método é chamado de TD (0) ou <b>TD gradual (atualiza a função do utilitário após uma única etapa).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  Os métodos TD esperam apenas que a próxima <b>etapa atualize os valores.</b>  No tempo t + 1 <b>, um alvo TD é formado usando a recompensa Rt + 1 e a classificação atual V (St + 1).</b> <br><br>  A meta de TD é uma estimativa do esperado: na verdade, você atualiza a pontuação V (St) anterior para a meta em uma etapa. <br><br>  <b>Exploração / Operação de Compromisso</b> <br><br>  Antes de considerar várias estratégias para resolver problemas de treinamento de reforço, devemos considerar outro tópico muito importante: o compromisso entre exploração e exploração. <br><br><ul><li>  A inteligência encontra mais informações sobre o ambiente. </li><li>  A exploração usa informações conhecidas para maximizar as recompensas. </li></ul><br>  Lembre-se de que o objetivo do nosso agente de RL é maximizar as recompensas acumuladas esperadas.  No entanto, podemos cair em uma armadilha comum. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  Neste jogo, nosso mouse pode ter um número infinito de pequenos pedaços de queijo (+1 cada).  Mas no topo do labirinto há um pedaço gigante de queijo (+1000).  No entanto, se focarmos apenas em recompensas, nosso agente nunca alcançará um pedaço gigantesco.  Em vez disso, ele usará apenas a fonte de recompensas mais próxima, mesmo que essa fonte seja pequena (exploração).  Mas se nosso agente reconhecer um pouco, ele será capaz de encontrar uma grande recompensa. <br><br>  É o que chamamos de compromisso entre exploração e exploração.  Devemos definir uma regra que ajude a lidar com esse compromisso.  Nos próximos artigos, você aprenderá maneiras diferentes de fazer isso. <br><br>  <b>Três abordagens de aprendizado por reforço</b> <br><br>  Agora que identificamos os principais elementos do aprendizado por reforço, passemos a três abordagens para solucionar o aprendizado reforçado: baseado em custos, baseado em políticas e baseado em modelos. <br><br>  <b>Com base no custo</b> <br><br>  Na RL baseada em custo, o objetivo é otimizar a função de utilidade V (s). <br>  Uma função de utilitário é uma função que nos informa da recompensa máxima esperada que um agente receberá em cada estado. <br><br>  O valor de cada estado é o valor total da recompensa que o agente pode esperar acumular no futuro, a partir desse estado. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  O agente usará essa função de utilitário para decidir qual estado escolher em cada etapa.  O agente seleciona o estado com o valor mais alto. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  No exemplo do labirinto, em cada etapa, obteremos o valor mais alto: -7, depois -6, depois -5 (etc.) para atingir a meta. <br><br>  <b>Baseado em políticas</b> <br><br>  Na RL baseada em política, queremos otimizar diretamente a função de política π (s) sem usar a função de utilitário.  Uma política é o que determina o comportamento de um agente em um determinado momento. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>ação = política (estado)</i> <br>  Estudamos a função da política.  Isso nos permite correlacionar cada estado com a melhor ação apropriada. <br><br>  Existem dois tipos de políticas: <br><br><ul><li>  Determinista: a política em um determinado estado sempre retornará a mesma ação. </li><li>  Estocástico: exibe a probabilidade de distribuição por ação. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Como você pode ver, a política indica diretamente a melhor ação para cada etapa. <br><br>  <b>Baseado no modelo</b> <br><br>  Na RL baseada em modelo, modelamos o ambiente.  Isso significa que estamos criando um modelo de comportamento ambiental.  O problema é que cada ambiente precisará de uma visão diferente do modelo.  É por isso que não focaremos muito esse tipo de treinamento nos artigos a seguir. <br><br>  <b>Introdução ao aprendizado por reforço profundo</b> <br><br>  O aprendizado profundo por reforço introduz redes neurais profundas para resolver os problemas do aprendizado reforçado - daí o nome "profundo". <br>  Por exemplo, no próximo artigo, trabalharemos no Q-Learning (aprendizado de reforço clássico) e no Deep Q-Learning. <br><br>  Você verá a diferença de que, na primeira abordagem, usamos o algoritmo tradicional para criar a tabela Q, o que nos ajuda a encontrar as ações a serem tomadas para cada estado. <br><br>  Na segunda abordagem, usaremos uma rede neural (para aproximar as recompensas baseadas no estado: valor q). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Udacity Inspired Q Design Chart</i> <i><br></i> <br><br>  Isso é tudo.  Como sempre, estamos aguardando seus comentários ou perguntas aqui, ou você pode pedir ao professor do curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arthur Kadurin</a> em sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lição aberta</a> sobre networking. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt429090/">https://habr.com/ru/post/pt429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt429078/index.html">Como criar arte processual em menos de 100 linhas de código</a></li>
<li><a href="../pt429082/index.html">Tailândia sem estereótipos</a></li>
<li><a href="../pt429084/index.html">A segunda vida do forno elétrico "Kharkov"</a></li>
<li><a href="../pt429086/index.html">Festa de cerveja alternativa</a></li>
<li><a href="../pt429088/index.html">Executando consultas GraphQL com OdataToEntity</a></li>
<li><a href="../pt429092/index.html">Por que a furtividade no espaço ainda está lá</a></li>
<li><a href="../pt429094/index.html">Som direcional: tecnologia que pode substituir os fones de ouvido - como funciona</a></li>
<li><a href="../pt429096/index.html">Antiguidades: ZX Spectrum, programas de cassetes e alta definição</a></li>
<li><a href="../pt429102/index.html">Vendas de veículos elétricos no Canadá</a></li>
<li><a href="../pt429104/index.html">Dados altos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>