<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüöí üë©üèæ‚Äçü§ù‚Äçüë©üèº ü•° Introdu√ß√£o √† Aprendizagem Refor√ßada üë®üèæ‚Äçü§ù‚Äçüë®üèΩ üåº üë®üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal! 

 Abrimos um novo fluxo para o curso Machine Learning . Portanto, aguarde no futuro pr√≥ximo artigos relacionados a essa disciplina, por ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introdu√ß√£o √† Aprendizagem Refor√ßada</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Ol√° pessoal! <br><br>  Abrimos um novo fluxo para o curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Machine Learning</a> . Portanto, aguarde no futuro pr√≥ximo artigos relacionados a essa disciplina, por assim dizer.  Bem, √© claro, semin√°rios abertos.  Agora, vejamos o que √© aprendizado por refor√ßo. <br><br>  O aprendizado refor√ßado √© uma forma importante de aprendizado de m√°quina, em que um agente aprende a se comportar em um ambiente executando a√ß√µes e vendo resultados. <br><br>  Nos √∫ltimos anos, vimos muitos sucessos nesse fascinante campo de pesquisa.  Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DeepMind e Deep Q Learning Architecture</a> em 2014, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vit√≥ria sobre o go go com AlphaGo</a> em 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI e PPO</a> em 2017, entre outros. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  Nesta s√©rie de artigos, focaremos o estudo das diferentes arquiteturas usadas hoje para resolver o problema do aprendizado refor√ßado.  Estes incluem Q-learning, Deep Q-learning, Gradientes de Pol√≠tica, Critic Actor e PPO. <br><br>  Neste artigo, voc√™ aprender√°: <br><br><ul><li>  O que √© aprendizado por refor√ßo e por que recompensas s√£o uma ideia central </li><li>  Tr√™s abordagens de aprendizado por refor√ßo </li><li>  O que "profundo" significa na aprendizagem por refor√ßo profundo </li></ul><br>  √â muito importante dominar esses aspectos antes de mergulhar na implementa√ß√£o de agentes de aprendizado por refor√ßo. <br><br>  A id√©ia do treinamento por refor√ßo √© que o agente aprenda com o ambiente interagindo com ele e recebendo recompensas por executar a√ß√µes. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  Aprender atrav√©s da intera√ß√£o com o meio ambiente vem de nossa experi√™ncia natural.  Imagine que voc√™ √© uma crian√ßa na sala de estar.  Voc√™ v√™ a lareira e vai at√© ela. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  Perto quente, voc√™ se sente bem (recompensa positiva +1).  Voc√™ entende que o fogo √© uma coisa positiva. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Mas ent√£o voc√™ tenta tocar o fogo.  Ai!  Ele queimou a m√£o (recompensa negativa -1).  Voc√™ acabou de perceber que o fogo √© positivo quando voc√™ est√° a uma dist√¢ncia suficiente porque produz calor.  Mas se voc√™ se aproximar dele, voc√™ se queimar√°. <br><br>  √â assim que as pessoas aprendem atrav√©s da intera√ß√£o.  O aprendizado refor√ßado √© simplesmente uma abordagem computacional do aprendizado atrav√©s da a√ß√£o. <br><br>  <b>Processo de aprendizagem por refor√ßo</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Como exemplo, imagine um agente aprendendo a jogar Super Mario Bros.  O processo de Aprendizagem por Refor√ßo (RL) pode ser modelado como um ciclo que funciona da seguinte maneira: <br><br><ul><li>  O agente recebe o estado S0 do ambiente (no nosso caso, obtemos o primeiro quadro do jogo (estado) de Super Mario Bros (ambiente)) </li><li>  Com base nesse estado S0, o agente executa a a√ß√£o A0 (o agente se mover√° para a direita) </li><li>  O ambiente passa para um novo estado S1 (novo quadro) </li><li>  O ambiente d√° alguma recompensa ao agente R1 (n√£o est√° morto: +1) </li></ul><br>  Esse ciclo RL produz uma sequ√™ncia de <b>estados, a√ß√µes e recompensas.</b> <br>  O objetivo do agente √© maximizar as recompensas acumuladas esperadas. <br><br>  <b>Hip√≥teses de recompensa da ideia central</b> <br><br>  Por que o objetivo de um agente √© maximizar as recompensas acumuladas esperadas?  Bem, o aprendizado por refor√ßo √© baseado na id√©ia de uma hip√≥tese de recompensa.  Todas as metas podem ser descritas maximizando as recompensas acumuladas esperadas. <br><br>  <b>Portanto, no treinamento de refor√ßo, para alcan√ßar o melhor comportamento, precisamos maximizar as recompensas acumuladas esperadas.</b> <br><br>  A recompensa acumulada em cada etapa t pode ser escrita como: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Isso √© equivalente a: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  No entanto, na realidade, n√£o podemos simplesmente adicionar essas recompensas.  As recompensas que chegarem mais cedo (no in√≠cio do jogo) s√£o mais prov√°veis, pois s√£o mais previs√≠veis do que as recompensas no futuro. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Suponha que seu agente seja um mouse pequeno e seu oponente seja um gato.  Seu objetivo √© comer a quantidade m√°xima de queijo antes que o gato coma voc√™.  Como podemos ver no diagrama, √© mais prov√°vel que um rato coma queijo pr√≥ximo a si mesmo do que queijo perto de um gato (quanto mais pr√≥ximos estamos dele, mais perigoso √©). <br><br>  Como resultado, a recompensa de um gato, mesmo que seja maior (mais queijo), ser√° reduzida.  N√£o temos certeza de que podemos com√™-lo.  Para reduzir a remunera√ß√£o, fazemos o seguinte: <br><br><ul><li>  Determinamos a taxa de desconto chamada gama.  Deve estar entre 0 e 1. </li><li>  Quanto maior a gama, menor o desconto.  Isso significa que o agente de aprendizado est√° mais preocupado com recompensas de longo prazo. </li><li>  Por outro lado, quanto menor a gama, maior o desconto.  Isso significa que √© dada prioridade √†s recompensas de curto prazo (queijo mais pr√≥ximo). </li></ul><br>  A contrapresta√ß√£o esperada acumulada, considerando o desconto, √© a seguinte: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  Grosso modo, cada recompensa ser√° reduzida usando a gama no indicador de tempo.  √Ä medida que o tempo aumenta, o gato se aproxima de n√≥s, e a recompensa futura se torna cada vez menos prov√°vel. <br><br>  <b>Tarefas ocasionais ou cont√≠nuas</b> <br><br>  Uma tarefa √© uma inst√¢ncia do problema de aprendizagem com refor√ßo.  Podemos ter dois tipos de tarefas: epis√≥dica e cont√≠nua. <br><br>  <b>Tarefa epis√≥dica</b> <br><br>  Nesse caso, temos um ponto inicial e um ponto final <b>(estado terminal).</b>  <b>Isso cria um epis√≥dio</b> : uma lista de estados, a√ß√µes, recompensas e novos estados. <br>  Veja o Super Mario Bros, por exemplo: o epis√≥dio come√ßa com o lan√ßamento do novo Mario e termina quando voc√™ √© morto ou chega ao final do n√≠vel. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>O in√≠cio de um novo epis√≥dio</i> <br><br>  <b>Tarefas cont√≠nuas</b> <br><br>  <b>Essas s√£o tarefas que duram para sempre (sem um estado terminal)</b> .  Nesse caso, o agente deve aprender a escolher as melhores a√ß√µes e ao mesmo tempo interagir com o ambiente. <br><br>  Por exemplo, um agente que realiza negocia√ß√£o automatizada de a√ß√µes.  N√£o h√° ponto inicial nem estado terminal para esta tarefa.  <b>O agente continua trabalhando at√© que decidimos det√™-lo.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>M√©todo Monte Carlo vs. Diferen√ßa de Tempo</b> <br><br>  Existem duas maneiras de aprender: <br><br><ul><li>  Coletando recompensas no final do epis√≥dio e calculando as recompensas futuras m√°ximas esperadas - abordagem Monte Carlo </li><li>  Avalia√ß√£o das recompensas a cada passo - uma diferen√ßa tempor√°ria </li></ul><br>  <b>Monte Carlo</b> <br><br>  Quando o epis√≥dio termina (o agente atinge um "estado terminal"), o agente analisa a recompensa acumulada total para ver o qu√£o bem ele se saiu.  Na abordagem de Monte Carlo, as recompensas s√£o recebidas apenas no final do jogo. <br><br>  Ent√£o come√ßamos um novo jogo com conhecimento aumentado.  <b>O agente toma as melhores decis√µes com cada itera√ß√£o.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Aqui est√° um exemplo: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Se tomarmos o labirinto como um ambiente: <br><br><ul><li>  Sempre come√ßamos do mesmo ponto de partida. </li><li>  Paramos o epis√≥dio se o gato nos comer ou avan√ßarmos&gt; 20 passos. </li><li>  No final do epis√≥dio, temos uma lista de estados, a√ß√µes, recompensas e novos estados. </li><li>  O agente resume a recompensa total de Gt (para ver como ele se saiu). </li><li>  Em seguida, ele atualiza V (st) de acordo com a f√≥rmula acima. </li><li>  Ent√£o, um novo jogo come√ßa com novos conhecimentos. </li></ul><br>  Executando mais e mais epis√≥dios, o <b>agente aprender√° a jogar cada vez melhor.</b> <br><br>  <b>Diferen√ßas hor√°rias: aprendendo a cada passo</b> <br><br>  O m√©todo Temporal Difference Learning (TD) n√£o aguardar√° o final do epis√≥dio para atualizar a maior recompensa poss√≠vel.  Ele atualizar√° o V dependendo da experi√™ncia adquirida. <br><br>  Esse m√©todo √© chamado de TD (0) ou <b>TD gradual (atualiza a fun√ß√£o do utilit√°rio ap√≥s uma √∫nica etapa).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  Os m√©todos TD esperam apenas que a pr√≥xima <b>etapa atualize os valores.</b>  No tempo t + 1 <b>, um alvo TD √© formado usando a recompensa Rt + 1 e a classifica√ß√£o atual V (St + 1).</b> <br><br>  A meta de TD √© uma estimativa do esperado: na verdade, voc√™ atualiza a pontua√ß√£o V (St) anterior para a meta em uma etapa. <br><br>  <b>Explora√ß√£o / Opera√ß√£o de Compromisso</b> <br><br>  Antes de considerar v√°rias estrat√©gias para resolver problemas de treinamento de refor√ßo, devemos considerar outro t√≥pico muito importante: o compromisso entre explora√ß√£o e explora√ß√£o. <br><br><ul><li>  A intelig√™ncia encontra mais informa√ß√µes sobre o ambiente. </li><li>  A explora√ß√£o usa informa√ß√µes conhecidas para maximizar as recompensas. </li></ul><br>  Lembre-se de que o objetivo do nosso agente de RL √© maximizar as recompensas acumuladas esperadas.  No entanto, podemos cair em uma armadilha comum. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  Neste jogo, nosso mouse pode ter um n√∫mero infinito de pequenos peda√ßos de queijo (+1 cada).  Mas no topo do labirinto h√° um peda√ßo gigante de queijo (+1000).  No entanto, se focarmos apenas em recompensas, nosso agente nunca alcan√ßar√° um peda√ßo gigantesco.  Em vez disso, ele usar√° apenas a fonte de recompensas mais pr√≥xima, mesmo que essa fonte seja pequena (explora√ß√£o).  Mas se nosso agente reconhecer um pouco, ele ser√° capaz de encontrar uma grande recompensa. <br><br>  √â o que chamamos de compromisso entre explora√ß√£o e explora√ß√£o.  Devemos definir uma regra que ajude a lidar com esse compromisso.  Nos pr√≥ximos artigos, voc√™ aprender√° maneiras diferentes de fazer isso. <br><br>  <b>Tr√™s abordagens de aprendizado por refor√ßo</b> <br><br>  Agora que identificamos os principais elementos do aprendizado por refor√ßo, passemos a tr√™s abordagens para solucionar o aprendizado refor√ßado: baseado em custos, baseado em pol√≠ticas e baseado em modelos. <br><br>  <b>Com base no custo</b> <br><br>  Na RL baseada em custo, o objetivo √© otimizar a fun√ß√£o de utilidade V (s). <br>  Uma fun√ß√£o de utilit√°rio √© uma fun√ß√£o que nos informa da recompensa m√°xima esperada que um agente receber√° em cada estado. <br><br>  O valor de cada estado √© o valor total da recompensa que o agente pode esperar acumular no futuro, a partir desse estado. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  O agente usar√° essa fun√ß√£o de utilit√°rio para decidir qual estado escolher em cada etapa.  O agente seleciona o estado com o valor mais alto. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  No exemplo do labirinto, em cada etapa, obteremos o valor mais alto: -7, depois -6, depois -5 (etc.) para atingir a meta. <br><br>  <b>Baseado em pol√≠ticas</b> <br><br>  Na RL baseada em pol√≠tica, queremos otimizar diretamente a fun√ß√£o de pol√≠tica œÄ (s) sem usar a fun√ß√£o de utilit√°rio.  Uma pol√≠tica √© o que determina o comportamento de um agente em um determinado momento. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>a√ß√£o = pol√≠tica (estado)</i> <br>  Estudamos a fun√ß√£o da pol√≠tica.  Isso nos permite correlacionar cada estado com a melhor a√ß√£o apropriada. <br><br>  Existem dois tipos de pol√≠ticas: <br><br><ul><li>  Determinista: a pol√≠tica em um determinado estado sempre retornar√° a mesma a√ß√£o. </li><li>  Estoc√°stico: exibe a probabilidade de distribui√ß√£o por a√ß√£o. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Como voc√™ pode ver, a pol√≠tica indica diretamente a melhor a√ß√£o para cada etapa. <br><br>  <b>Baseado no modelo</b> <br><br>  Na RL baseada em modelo, modelamos o ambiente.  Isso significa que estamos criando um modelo de comportamento ambiental.  O problema √© que cada ambiente precisar√° de uma vis√£o diferente do modelo.  √â por isso que n√£o focaremos muito esse tipo de treinamento nos artigos a seguir. <br><br>  <b>Introdu√ß√£o ao aprendizado por refor√ßo profundo</b> <br><br>  O aprendizado profundo por refor√ßo introduz redes neurais profundas para resolver os problemas do aprendizado refor√ßado - da√≠ o nome "profundo". <br>  Por exemplo, no pr√≥ximo artigo, trabalharemos no Q-Learning (aprendizado de refor√ßo cl√°ssico) e no Deep Q-Learning. <br><br>  Voc√™ ver√° a diferen√ßa de que, na primeira abordagem, usamos o algoritmo tradicional para criar a tabela Q, o que nos ajuda a encontrar as a√ß√µes a serem tomadas para cada estado. <br><br>  Na segunda abordagem, usaremos uma rede neural (para aproximar as recompensas baseadas no estado: valor q). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Udacity Inspired Q Design Chart</i> <i><br></i> <br><br>  Isso √© tudo.  Como sempre, estamos aguardando seus coment√°rios ou perguntas aqui, ou voc√™ pode pedir ao professor do curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arthur Kadurin</a> em sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">li√ß√£o aberta</a> sobre networking. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt429090/">https://habr.com/ru/post/pt429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt429078/index.html">Como criar arte processual em menos de 100 linhas de c√≥digo</a></li>
<li><a href="../pt429082/index.html">Tail√¢ndia sem estere√≥tipos</a></li>
<li><a href="../pt429084/index.html">A segunda vida do forno el√©trico "Kharkov"</a></li>
<li><a href="../pt429086/index.html">Festa de cerveja alternativa</a></li>
<li><a href="../pt429088/index.html">Executando consultas GraphQL com OdataToEntity</a></li>
<li><a href="../pt429092/index.html">Por que a furtividade no espa√ßo ainda est√° l√°</a></li>
<li><a href="../pt429094/index.html">Som direcional: tecnologia que pode substituir os fones de ouvido - como funciona</a></li>
<li><a href="../pt429096/index.html">Antiguidades: ZX Spectrum, programas de cassetes e alta defini√ß√£o</a></li>
<li><a href="../pt429102/index.html">Vendas de ve√≠culos el√©tricos no Canad√°</a></li>
<li><a href="../pt429104/index.html">Dados altos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>