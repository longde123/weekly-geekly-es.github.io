<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üà¥ ‚úùÔ∏è üë®üèº‚ÄçüöÄ Wie fehlertolerante Webarchitektur in der Mail.ru Cloud Solutions-Plattform implementiert wird ‚öíÔ∏è üññüèΩ üöë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Ich bin Artyom Karamyshev, Leiter des Systemadministrationsteams bei Mail.Ru Cloud Solutions (MCS) . Im vergangenen Jahr haben wir viele ne...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie fehlertolerante Webarchitektur in der Mail.ru Cloud Solutions-Plattform implementiert wird</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/474180/"><img src="https://habrastorage.org/webt/gd/wp/de/gdwpdevye3ploqkbmd4rwjqkvva.jpeg"><br><br>  Hallo habr  Ich bin Artyom Karamyshev, Leiter des Systemadministrationsteams bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mail.Ru Cloud Solutions (MCS)</a> .  Im vergangenen Jahr haben wir viele neue Produkte auf den Markt gebracht.  Wir wollten, dass die API-Services einfach skaliert werden k√∂nnen, fehlertolerant sind und f√ºr eine schnelle Erh√∂hung der Benutzerlast bereit sind.  Unsere Plattform ist auf OpenStack implementiert, und ich m√∂chte Ihnen sagen, welche Probleme der Fehlertoleranz von Komponenten wir schlie√üen mussten, um ein fehlertolerantes System zu erhalten.  Ich denke, das wird f√ºr diejenigen interessant sein, die auch Produkte auf OpenStack entwickeln. <br><br>  Die Gesamtfehlertoleranz der Plattform besteht in der Stabilit√§t ihrer Komponenten.  Wir werden also schrittweise alle Ebenen durchlaufen, auf denen wir die Risiken entdeckt und geschlossen haben. <br><br>  Eine Videoversion dieser Geschichte, deren Quelle ein Bericht auf der von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ITSumma</a> organisierten Uptime Day 4-Konferenz war, kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf dem YouTube-Kanal der Uptime Community angesehen werden</a> . <br><a name="habracut"></a><br>
<h2>  Fehlertoleranz der physischen Architektur </h2><br>  Der √∂ffentliche Teil der MCS-Cloud befindet sich jetzt in zwei Tier III-Rechenzentren. Zwischen diesen befindet sich eine eigene dunkle Faser, die auf verschiedenen Wegen auf der physischen Schicht reserviert ist und einen Durchsatz von 200 Gbit / s aufweist.  Die Stufe III bietet die erforderliche Ausfallsicherheit der physischen Infrastruktur. <br><br>  Dunkle Fasern sind sowohl auf physischer als auch auf logischer Ebene reserviert.  Der Kanalreservierungsprozess war iterativ, es traten Probleme auf und wir verbessern st√§ndig die Kommunikation zwischen Rechenzentren. <br><br><blockquote>  Zum Beispiel hat vor nicht allzu langer Zeit, als ein Bagger in einem Brunnen neben einem der Rechenzentren arbeitete, ein Rohr gestanzt. In diesem Rohr befand sich sowohl ein optisches Haupt- als auch ein Ersatzkabel.  Unser fehlertoleranter Kommunikationskanal mit dem Rechenzentrum erwies sich an einer Stelle im Bohrloch als anf√§llig.  Dementsprechend haben wir einen Teil der Infrastruktur verloren.  Wir haben Schlussfolgerungen gezogen und eine Reihe von Ma√ünahmen ergriffen, einschlie√ülich der Verlegung zus√§tzlicher Optiken entlang eines benachbarten Brunnens. </blockquote><br>  In den Rechenzentren gibt es Pr√§senzpunkte von Kommunikationsanbietern, an die wir unsere Pr√§fixe √ºber BGP senden.  F√ºr jede Netzwerkrichtung wird die beste Metrik ausgew√§hlt, mit der verschiedene Kunden die beste Verbindungsqualit√§t erzielen k√∂nnen.  Wenn die Kommunikation √ºber einen Anbieter unterbrochen wird, bauen wir unser Routing √ºber verf√ºgbare Anbieter neu auf. <br><br>  Bei einem Providerausfall wechseln wir automatisch zum n√§chsten.  Im Falle eines Ausfalls eines der Rechenzentren haben wir eine Spiegelkopie unserer Dienste im zweiten Rechenzentrum, die die gesamte Last auf sich nehmen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d0/m8/ly/d0m8lykvifmc-gum-h9mbppyn3g.jpeg"></div><br>  <i>Ausfallsicherheit der physischen Infrastruktur</i> <br><br><h2>  Was wir f√ºr die Fehlertoleranz auf Anwendungsebene verwenden </h2><br>  Unser Service basiert auf einer Reihe von Open Source-Komponenten. <br><br>  <b>ExaBGP</b> ist ein Dienst, der eine Reihe von Funktionen mithilfe des auf BGP basierenden dynamischen Routing-Protokolls implementiert.  Wir verwenden es aktiv, um unsere wei√üen IP-Adressen bekannt zu geben, √ºber die Benutzer Zugriff auf die API erhalten. <br><br>  <b>HAProxy</b> ist ein hoch geladener Balancer, mit dem Sie sehr flexible Regeln f√ºr den Datenverkehrsausgleich auf verschiedenen Ebenen des OSI-Modells konfigurieren k√∂nnen.  Wir verwenden es, um alle Dienste auszugleichen: Datenbanken, Nachrichtenbroker, API-Dienste, Webdienste, unsere internen Projekte - alles steckt hinter HAProxy. <br><br>  <b>API-Anwendung</b> - eine in Python geschriebene Webanwendung, mit der der Benutzer seine Infrastruktur und seinen Dienst steuert. <br><br>  <b>Worker-Anwendung</b> (im Folgenden einfach als Worker bezeichnet) - In OpenStack-Diensten handelt es sich um einen Infrastruktur-Daemon, mit dem Sie API-Befehle in die Infrastruktur √ºbersetzen k√∂nnen.  Beispielsweise wird im Worker eine Festplatte erstellt, und eine Anforderung zum Erstellen befindet sich in der Anwendungs-API. <br><br><h2>  Standard OpenStack-Anwendungsarchitektur </h2><br>  Die meisten Dienste, die f√ºr OpenStack entwickelt wurden, versuchen, einem einzigen Paradigma zu folgen.  Ein Service besteht normalerweise aus zwei Teilen: API und Workern (Backend-Executoren).  In der Regel ist eine API eine Python-WSGI-Anwendung, die entweder als eigenst√§ndiger Prozess (Daemon) oder unter Verwendung eines vorgefertigten Nginx-Webservers, Apache, ausgef√ºhrt wird.  Die API verarbeitet die Benutzeranforderung und leitet weitere Anweisungen an die Worker-Anwendung weiter.  Die √úbertragung erfolgt √ºber einen Nachrichtenbroker, normalerweise RabbitMQ, der Rest wird schlecht unterst√ºtzt.  Wenn Nachrichten an den Broker gelangen, werden sie von den Mitarbeitern verarbeitet und geben bei Bedarf eine Antwort zur√ºck. <br><br>  Dieses Paradigma impliziert isolierte h√§ufige Fehlerquellen: RabbitMQ und die Datenbank.  RabbitMQ ist jedoch innerhalb eines Dienstes isoliert und kann theoretisch f√ºr jeden Dienst individuell sein.  Deshalb teilen wir bei MCS diese Dienste so weit wie m√∂glich. F√ºr jedes einzelne Projekt erstellen wir eine separate Datenbank, einen separaten RabbitMQ.  Dieser Ansatz ist gut, da im Falle eines Unfalls an einigen gef√§hrdeten Stellen nicht alle Dienstpausen, sondern nur ein Teil davon unterbrochen werden. <br><br>  Die Anzahl der Worker-Anwendungen ist unbegrenzt, sodass die API problemlos horizontal hinter den Balancern skaliert werden kann, um die Produktivit√§t und Fehlertoleranz zu erh√∂hen. <br><br><blockquote>  Einige Dienste erfordern eine Koordination innerhalb des Dienstes - wenn komplexe sequentielle Vorg√§nge zwischen APIs und Mitarbeitern auftreten.  In diesem Fall wird ein einzelnes Koordinierungszentrum verwendet, ein Clustersystem wie Redis, Memcache usw., mit dem ein Mitarbeiter dem anderen mitteilen kann, dass diese Aufgabe ihm zugewiesen ist ("Bitte nicht √ºbernehmen").  Wir verwenden etcd.  In der Regel kommunizieren Mitarbeiter aktiv mit der Datenbank, schreiben und lesen dort Informationen.  Als Datenbank verwenden wir Mariadb, das wir im Multimaster-Cluster haben. <br></blockquote><br>  Ein solcher klassischer Einzelbenutzerdienst ist in einer f√ºr OpenStack allgemein akzeptierten Weise organisiert.  Es kann als geschlossenes System betrachtet werden, f√ºr das die Methoden der Skalierung und Fehlertoleranz ziemlich offensichtlich sind.  F√ºr die Fehlertoleranz der API reicht es beispielsweise aus, einen Balancer vor sie zu stellen.  Die Skalierung der Arbeitnehmer wird durch die Erh√∂hung ihrer Zahl erreicht. <br><br>  Die Schwachstellen im gesamten Schema sind RabbitMQ und MariaDB.  Ihre Architektur verdient einen separaten Artikel. In diesem Artikel m√∂chte ich mich auf die Fehlertoleranz der API konzentrieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1a/ab/i1/1aabi1ew0ctxnlrcm2j78nbhefk.jpeg"></div><br>  <i>Openstack-Anwendungsarchitektur</i>  <i>Ausgleich und Ausfallsicherheit der Cloud-Plattform</i> <br><br><h2>  HAProxy Balancer mit ExaBGP widerstandsf√§hig machen </h2><br>  Um unsere APIs skalierbar, schnell und fehlertolerant zu machen, setzen wir einen Balancer vor sie.  Wir haben uns f√ºr HAProxy entschieden.  Meiner Meinung nach weist es alle notwendigen Merkmale f√ºr unsere Aufgabe auf: Balancing auf mehreren OSI-Ebenen, Verwaltungsschnittstelle, Flexibilit√§t und Skalierbarkeit, eine gro√üe Anzahl von Balancing-Methoden, Unterst√ºtzung f√ºr Sitzungstabellen. <br><br>  Das erste Problem, das gel√∂st werden musste, war die Fehlertoleranz des Balancers selbst.  Allein die Installation des Balancers f√ºhrt zu einer Fehlerquelle: Der Balancer bricht ab - der Dienst wird unterbrochen.  Um dies zu verhindern, haben wir HAProxy zusammen mit ExaBGP verwendet. <br><br>  Mit ExaBGP k√∂nnen Sie einen Mechanismus zum √úberpr√ºfen des Status eines Dienstes implementieren.  Wir haben diesen Mechanismus verwendet, um die Funktionalit√§t von HAProxy zu √ºberpr√ºfen und bei Problemen den HAProxy-Dienst von BGP aus zu deaktivieren. <br><br>  <b>ExaBGP + HAProxy-Schema</b> <br><br><ol><li>  Wir installieren die erforderliche Software auf drei Servern, ExaBGP und HAProxy. </li><li>  Auf jedem der Server erstellen wir eine Loopback-Schnittstelle. </li><li>  Auf allen drei Servern weisen wir dieser Schnittstelle dieselbe wei√üe IP-Adresse zu. </li><li>  Eine wei√üe IP-Adresse wird im Internet √ºber ExaBGP angek√ºndigt. </li></ol><br>  Die Fehlertoleranz wird erreicht, indem von allen drei Servern dieselbe IP-Adresse angek√ºndigt wird.  Aus Netzwerksicht ist dieselbe Adresse aus drei verschiedenen n√§chsten Hoffnungen zug√§nglich.  Der Router sieht drei identische Routen, w√§hlt die h√∂chste Priorit√§t anhand seiner eigenen Metrik aus (dies ist normalerweise dieselbe Option), und der Datenverkehr wird nur an einen der Server geleitet. <br><br>  Bei Problemen mit dem HAProxy-Betrieb oder einem Serverausfall beendet ExaBGP die Ank√ºndigung der Route und der Datenverkehr wird reibungslos auf einen anderen Server umgeschaltet. <br><br>  Damit haben wir die Fehlertoleranz des Balancers erreicht. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ij/c_/cz/ijc_cz1jvhlug0axiwiiqjqpcww.jpeg"></div><br>  <i>Fehlertoleranz von HAProxy-Balancern</i> <br><br>  Das Schema stellte sich als unvollkommen heraus: Wir haben gelernt, wie man HAProxy reserviert, aber nicht, wie man die Last innerhalb der Dienste verteilt.  Aus diesem Grund haben wir dieses Schema ein wenig erweitert: Wir haben den Ausgleich zwischen mehreren wei√üen IP-Adressen vorgenommen. <br><br><h2>  DNS Based Balancing Plus BGP </h2><br>  Das Problem des Lastausgleichs vor unserem HAProxy blieb ungel√∂st.  Trotzdem kann es ganz einfach gel√∂st werden, wie wir es zu Hause getan haben. <br><br>  Um die drei Server auszugleichen, ben√∂tigen Sie 3 wei√üe IP-Adressen und einen guten alten DNS.  Jede dieser Adressen wird auf der Loopback-Schnittstelle jedes HAProxy definiert und im Internet angek√ºndigt. <br><br>  OpenStack verwendet einen Dienstkatalog zum Verwalten von Ressourcen, wodurch die Endpunkt-API eines Dienstes festgelegt wird.  In diesem Verzeichnis schreiben wir einen Domainnamen vor - public.infra.mail.ru, der √ºber DNS mit drei verschiedenen IP-Adressen aufgel√∂st wird.  Als Ergebnis erhalten wir einen Lastenausgleich zwischen den drei Adressen √ºber DNS. <br><br>  Da wir bei der Ank√ºndigung von wei√üen IP-Adressen die Priorit√§ten f√ºr die Serverauswahl nicht steuern, ist dies bisher kein Ausgleich.  In der Regel wird nur ein Server nach Vorrang der IP-Adresse ausgew√§hlt, und die anderen beiden sind inaktiv, da in BGP keine Metriken angegeben sind. <br><br>  Wir haben damit begonnen, Routen durch ExaBGP mit verschiedenen Metriken anzugeben.  Jeder Balancer k√ºndigt alle drei wei√üen IP-Adressen an, aber eine von ihnen, die Hauptadresse f√ºr diesen Balancer, wird mit einer Mindestmetrik angek√ºndigt.  W√§hrend also alle drei Balancer in Betrieb sind, fallen Anrufe an die erste IP-Adresse auf den ersten Balancer, Anrufe an die zweite an die zweite, an die dritte an die dritte. <br><br>  Was passiert, wenn einer der Balancer f√§llt?  Im Falle eines Ausfalls eines Balancers aufgrund seiner Basis wird die Adresse weiterhin von den beiden anderen angek√ºndigt, und der Verkehr zwischen ihnen wird neu verteilt.  Somit geben wir dem Benutzer √ºber das DNS mehrere IP-Adressen gleichzeitig.  Durch das Balancieren auf DNS und verschiedenen Metriken erhalten wir eine gleichm√§√üige Lastverteilung auf alle drei Balancer.  Gleichzeitig verlieren wir nicht die Fehlertoleranz. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ek/xc/3z/ekxc3zsz5oazwdiqziwp_idfk7a.jpeg"></div><br>  <i>HAProxy Balancing basierend auf DNS + BGP</i> <br><br><h2>  Wechselwirkung zwischen ExaBGP und HAProxy </h2><br>  Daher haben wir die Fehlertoleranz f√ºr den Fall implementiert, dass der Server den Server verl√§sst, basierend auf der Beendigung der Ank√ºndigung von Routen.  HAProxy kann jedoch auch aus anderen Gr√ºnden als einem Serverausfall getrennt werden: Verwaltungsfehler, Dienstausf√§lle.  Wir wollen den kaputten Balancer unter der Last entfernen und in diesen F√§llen brauchen wir einen anderen Mechanismus. <br><br>  Aus diesem Grund haben wir zur Erweiterung des vorherigen Schemas einen Heartbeat zwischen ExaBGP und HAProxy implementiert.  Dies ist eine Softwareimplementierung der Interaktion zwischen ExaBGP und HAProxy, wenn ExaBGP benutzerdefinierte Skripts verwendet, um den Status von Anwendungen zu √ºberpr√ºfen. <br><br>  Dazu m√ºssen Sie in der ExaBGP-Konfiguration einen Integrit√§tspr√ºfer konfigurieren, der den Status von HAProxy √ºberpr√ºfen kann.  In unserem Fall haben wir das Integrit√§ts-Backend in HAProxy konfiguriert und von der Seite von ExaBGP aus mit einer einfachen GET-Anforderung √ºberpr√ºft.  Wenn die Ank√ºndigung nicht mehr erfolgt, funktioniert HAProxy h√∂chstwahrscheinlich nicht und es ist nicht erforderlich, sie anzuk√ºndigen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w0/ac/5c/w0ac5cvqsvjtki2cqzcgtgyk4x4.jpeg"></div><br>  <i>HAProxy Health Check</i> <br><br><h2>  HAProxy Peers: Sitzungssynchronisation </h2><br>  Als n√§chstes mussten die Sitzungen synchronisiert werden.  Bei der Arbeit mit verteilten Balancern ist es schwierig, die Speicherung von Informationen zu Client-Sitzungen zu organisieren.  HAProxy ist jedoch einer der wenigen Balancer, die dies aufgrund der Peers-Funktionalit√§t tun k√∂nnen - der M√∂glichkeit, Sitzungstabellen zwischen verschiedenen HAProxy-Prozessen zu √ºbertragen. <br><br>  Es gibt verschiedene Ausgleichsmethoden: Einfach, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Round-Robin</a> , und Erweitert, wenn eine Clientsitzung gespeichert wird und jedes Mal, wenn sie auf denselben Server wie zuvor gelangt.  Wir wollten die zweite Option implementieren. <br><br>  HAProxy verwendet Stick-Tabellen, um Client-Sitzungen f√ºr diesen Mechanismus zu speichern.  Sie speichern die Quell-IP-Adresse des Clients, die ausgew√§hlte Zieladresse (Backend) und einige Dienstinformationen.  In der Regel werden Stick-Tabellen verwendet, um das Quell-IP + Ziel-IP-Paar zu speichern. Dies ist besonders n√ºtzlich f√ºr Anwendungen, die den Kontext einer Benutzersitzung nicht √ºbertragen k√∂nnen, wenn sie zu einem anderen Balancer wechseln, z. B. im RoundRobin-Ausgleichsmodus. <br><br>  Wenn der Stick-Tabelle beigebracht wird, sich zwischen verschiedenen HAProxy-Prozessen zu bewegen (zwischen denen das Balancing stattfindet), k√∂nnen unsere Balancer mit einem Pool von Stick-Tischen arbeiten.  Auf diese Weise kann das Client-Netzwerk nahtlos gewechselt werden, wenn einer der Balancer ausf√§llt. Die Arbeit mit Client-Sitzungen wird auf denselben Backends fortgesetzt, die zuvor ausgew√§hlt wurden. <br><br>  F√ºr einen ordnungsgem√§√üen Betrieb muss die Quell-IP-Adresse des Balancers aufgel√∂st werden, von dem aus die Sitzung eingerichtet wird.  In unserem Fall ist dies eine dynamische Adresse auf der Loopback-Schnittstelle. <br><br>  Der korrekte Betrieb von Peers wird nur unter bestimmten Bedingungen erreicht.  Das hei√üt, die TCP-Zeit√ºberschreitungen m√ºssen gro√ü genug sein oder der Switch sollte schnell genug sein, damit die TCP-Sitzung keine Zeit zum Unterbrechen hat.  Dies erm√∂glicht jedoch ein nahtloses Umschalten. <br><br>  Wir bei IaaS haben einen Service, der auf der gleichen Technologie basiert.  Dies ist ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Load Balancer als Service f√ºr OpenStack</a> namens Octavia.  Es basiert auf zwei HAProxy-Prozessen und umfasste urspr√ºnglich die Unterst√ºtzung von Peers.  Sie haben sich in diesem Service bew√§hrt. <br><br>  Das Bild zeigt schematisch die Bewegung von Peers-Tabellen zwischen drei HAProxy-Instanzen. Es wird eine Konfiguration vorgeschlagen, wie diese konfiguriert werden kann: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ov/ol/wr/ovolwrmp-gzrvyybotjjagtb-re.jpeg"></div><br>  <i>HAProxy Peers (Sitzungssynchronisation)</i> <br><br>  Wenn Sie dasselbe Schema implementieren, muss seine Arbeit sorgf√§ltig getestet werden.  Nicht die Tatsache, dass dies in 100% der F√§lle auf die gleiche Weise funktioniert.  Zumindest verlieren Sie jedoch keine Stick-Tabellen, wenn Sie sich die Quell-IP des Clients merken m√ºssen. <br><br><h2>  Begrenzung der Anzahl gleichzeitiger Anforderungen vom selben Client </h2><br>  Alle gemeinfreien Dienste, einschlie√ülich unserer APIs, k√∂nnen Lawinen von Anfragen unterliegen.  Die Gr√ºnde daf√ºr k√∂nnen v√∂llig unterschiedlich sein, von Benutzerfehlern bis hin zu gezielten Angriffen.  Wir sind regelm√§√üig DDoS an IP-Adressen.  Kunden machen oft Fehler in ihren Skripten, sie machen uns zu Mini-DDoSs. <br><br>  Auf die eine oder andere Weise muss zus√§tzlicher Schutz bereitgestellt werden.  Die naheliegende L√∂sung besteht darin, die Anzahl der API-Anforderungen zu begrenzen und keine CPU-Zeit f√ºr die Verarbeitung b√∂swilliger Anforderungen zu verschwenden. <br><br>  Um solche Einschr√§nkungen zu implementieren, verwenden wir Ratenlimits, die auf der Basis von HAProxy organisiert sind und dieselben Stick-Tabellen verwenden.  Die Grenzwerte sind recht einfach konfiguriert und erm√∂glichen es Ihnen, den Benutzer durch die Anzahl der Anforderungen an die API zu begrenzen.  Der Algorithmus merkt sich die Quell-IP, von der aus die Anforderungen gestellt werden, und begrenzt die Anzahl gleichzeitiger Anforderungen von einem Benutzer.  Nat√ºrlich haben wir das durchschnittliche API-Lastprofil f√ºr jeden Service berechnet und den Grenzwert auf das 10-fache dieses Werts festgelegt.  Bis jetzt beobachten wir die Situation weiterhin genau und halten den Finger am Puls der Zeit. <br><br>  Wie sieht es in der Praxis aus?  Wir haben Kunden, die st√§ndig unsere automatischen APIs verwenden.  Sie erstellen ungef√§hr zwei- oder dreihundert virtuelle Maschinen n√§her am Morgen und l√∂schen sie n√§her am Abend.  Erstellen Sie f√ºr OpenStack eine virtuelle Maschine, auch mit PaaS-Diensten, mindestens 1000 API-Anforderungen, da die Interaktion zwischen den Diensten auch √ºber die API erfolgt. <br><br>  Ein solches Aufgabenwerfen verursacht eine ziemlich gro√üe Last.  Wir haben diese Belastung gesch√§tzt, t√§gliche Spitzenwerte gesammelt, sie verzehnfacht, und dies wurde zu unserer Ratengrenze.  Wir bleiben am Puls der Zeit.  Wir sehen oft Bots, Scanner, die versuchen, uns anzusehen. Haben wir CGA-Skripte, die ausgef√ºhrt werden k√∂nnen, schneiden wir sie aktiv aus. <br><br><h2>  So aktualisieren Sie die Codebasis diskret f√ºr Benutzer </h2><br>  Wir implementieren auch Fehlertoleranz auf der Ebene der Codebereitstellungsprozesse.  W√§hrend des Rollouts kommt es zu Abst√ºrzen, deren Auswirkungen auf die Verf√ºgbarkeit von Diensten k√∂nnen jedoch minimiert werden. <br><br>  Wir aktualisieren st√§ndig unsere Dienste und sollten sicherstellen, dass die Codebasis ohne Auswirkungen f√ºr die Benutzer aktualisiert wird.  Wir haben es geschafft, dieses Problem mithilfe der Funktionen des HAProxy-Managements und der Implementierung von Graceful Shutdown in unseren Diensten zu l√∂sen. <br><br>  Um dieses Problem zu l√∂sen, war es notwendig, eine Balancer-Steuerung und das ‚Äûkorrekte‚Äú Herunterfahren von Diensten bereitzustellen: <br><br><ul><li>  Im Fall von HAProxy erfolgt die Steuerung √ºber die Statistikdatei, die im Wesentlichen ein Socket ist und in der HAProxy-Konfiguration definiert ist.  Sie k√∂nnen Befehle √ºber stdio an ihn senden.  Unser Hauptwerkzeug zur Konfigurationssteuerung ist jedoch ansibel und verf√ºgt √ºber ein integriertes Modul zur Verwaltung von HAProxy.  Was wir aktiv nutzen. </li><li>  Die meisten unserer API- und Engine-Services unterst√ºtzen ordnungsgem√§√üe Abschalttechnologien: Beim Herunterfahren warten sie auf den Abschluss der aktuellen Aufgabe, sei es eine http-Anforderung oder eine Dienstprogrammaufgabe.  Das gleiche passiert mit dem Arbeiter.  Er kennt alle Aufgaben, die er erledigt, und endet, wenn er alles erfolgreich abgeschlossen hat. </li></ul><br>  Dank dieser beiden Punkte lautet der sichere Algorithmus unserer Bereitstellung wie folgt. <br><br><ol><li>  Der Entwickler erstellt ein neues Codepaket (wir haben RPM), testet in der Entwicklungsumgebung, testet in der Phase und bel√§sst es im Stage-Repository. </li><li>  Der Entwickler stellt die Aufgabe mit der detailliertesten Beschreibung der "Artefakte" auf die Bereitstellung: die Version des neuen Pakets, eine Beschreibung der neuen Funktionalit√§t und gegebenenfalls weitere Details zur Bereitstellung. </li><li>  Der Systemadministrator startet das Upgrade.  Startet das Ansible-Playbook, das wiederum Folgendes ausf√ºhrt: <br><ul><li>  Es nimmt ein Paket aus dem Stage-Repository und aktualisiert damit die Paketversion im Produkt-Repository. </li><li>  Erstellt eine Liste der Backends des aktualisierten Dienstes. </li><li>  Deaktiviert den ersten aktualisierten Dienst in HAProxy und wartet auf das Ende seiner Prozesse.  Dank des ordnungsgem√§√üen Herunterfahrens sind wir zuversichtlich, dass alle aktuellen Clientanforderungen erfolgreich abgeschlossen werden. </li><li>  Nachdem die API, Worker und HAProxy vollst√§ndig gestoppt wurden, wird der Code aktualisiert. </li><li>  Ansible startet Dienste. </li><li>  F√ºr jeden Dienst werden bestimmte ‚ÄûStifte‚Äú gezogen, die Unit-Tests f√ºr eine Reihe vordefinierter Schl√ºsseltests durchf√ºhren.  Eine grundlegende √úberpr√ºfung des neuen Codes erfolgt. </li><li>  Wenn im vorherigen Schritt keine Fehler gefunden wurden, wird das Backend aktiviert. </li><li>  Gehe zum n√§chsten Backend. </li></ul></li><li>  Nach dem Aktualisieren aller Backends werden Funktionstests gestartet.  Wenn sie nicht ausreichen, pr√ºft der Entwickler alle neuen Funktionen, die er ausgef√ºhrt hat. </li></ol><br>  Damit ist die Bereitstellung abgeschlossen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-b/km/dt/-bkmdt98ituj53jetxiaay4uf4c.jpeg"></div><br>  <i>Service-Update-Zyklus</i> <br><br>  Dieses Schema w√ºrde nicht funktionieren, wenn wir keine einzige Regel h√§tten.  Wir unterst√ºtzen sowohl die alte als auch die neue Version im Kampf.  In der Phase der Softwareentwicklung wird im Voraus festgelegt, dass selbst bei √Ñnderungen in der Servicedatenbank der vorherige Code nicht besch√§digt wird.  Infolgedessen wird die Codebasis schrittweise aktualisiert. <br><br><h2>  Fazit </h2><br>  Ich teile meine eigenen Gedanken √ºber die fehlertolerante WEB-Architektur und m√∂chte noch einmal die wichtigsten Punkte hervorheben: <br><br><ul><li>  physikalische Fehlertoleranz; </li><li>  Netzwerkfehlertoleranz (Balancer, BGP); </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fehlertoleranz der verwendeten und entwickelten Software. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alles stabile Betriebszeit! </font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de474180/">https://habr.com/ru/post/de474180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de474166/index.html">Titelindizes f√ºr GiST</a></li>
<li><a href="../de474170/index.html">Design Confession - 15. November, Moskau, DI Telegraph</a></li>
<li><a href="../de474172/index.html">Eine Geldstrafe von 30.000 Euro f√ºr die illegale Verwendung von Cookies</a></li>
<li><a href="../de474176/index.html">11 Videos vom ersten Tag des DevFest 2019 in Kaliningrad</a></li>
<li><a href="../de474178/index.html">IVR auf Webhook</a></li>
<li><a href="../de474184/index.html">Wir bestehen die Herausforderung von Callum Macrae zu 100%</a></li>
<li><a href="../de474186/index.html">Mythen widerlegen: Echte IT-Praktiken in Armenien</a></li>
<li><a href="../de474192/index.html">Warum bin ich von UX zu PM und dann zu Lead PM gewechselt und was hat sich ge√§ndert?</a></li>
<li><a href="../de474194/index.html">Kompass-Team</a></li>
<li><a href="../de474196/index.html">Die 10 wichtigsten Meilensteine ‚Äã‚Äãin der heutigen KI-Entwicklung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>