<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¥üèæ ‚õΩÔ∏è ü§∞üèº Implantar o Kubernetes Cluster no OpenStack com o Kubespray üë∏üèæ üë®‚Äçüë©‚Äçüë¶ üì¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O Kubernetes tornou-se rapidamente o padr√£o para entregar e dimensionar aplicativos em cont√™ineres e gerenci√°-los. Esta √© uma solu√ß√£o de c√≥digo aberto...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implantar o Kubernetes Cluster no OpenStack com o Kubespray</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/456792/"><p><img src="https://habrastorage.org/webt/oz/5b/xn/oz5bxnm9_ej5v-po5gjbk93otk0.jpeg"></p><br><p>  O Kubernetes tornou-se rapidamente o padr√£o para entregar e dimensionar aplicativos em cont√™ineres e gerenci√°-los.  Esta √© uma solu√ß√£o de c√≥digo aberto muito flex√≠vel e vers√°til.  Possui extensa documenta√ß√£o e nem sempre √© f√°cil encontrar a se√ß√£o correta nela.  Portanto, o Kubernetes √© t√£o dif√≠cil de dominar.  Depois de planejar o cluster, voc√™ ainda precisar√° instal√°-lo, mas tamb√©m aqui tudo n√£o est√° indo bem.  Portanto, existem ferramentas de implanta√ß√£o, como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubespray,</a> que simplificam o trabalho.  Vou falar sobre a implanta√ß√£o autom√°tica de um cluster Kubernetes usando o Kubespray em uma nuvem do OpenStack (Open Telekom Cloud). </p><a name="habracut"></a><br><p>  Para implanta√ß√£o autom√°tica do Kubernetes, o Kubespray usa a ferramenta Ansible de inicializa√ß√£o, configura√ß√£o e entrega de aplicativos.  E o Kubespray fornece uma biblioteca para inicializar recursos em diferentes plataformas na nuvem.  Para fazer isso, use a ferramenta "infrastructure as code" Terraform.  O projeto Kubespray agora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">suporta Terraform</a> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nuvens</a> da AWS, OpenStack e Packet.  Essa ferramenta √© usada em conjunto com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a biblioteca OpenStack</a> para fornecer a infraestrutura neste cen√°rio. </p><br><h2 id="trebovaniya">  Exig√™ncias </h2><br><p>  Primeiro, vamos examinar os pr√©-requisitos de implanta√ß√£o.  Eles s√£o divididos em duas categorias: requisitos para o Kubespray e requisitos para a biblioteca do provedor. </p><br><p>  O Kubespray precisa dos seguintes componentes: </p><br><ul><li>  Python 2.7 (ou superior) </li><li>  Resolvido 2.7 (ou superior) </li><li>  Jinja 2.9 (ou superior) </li></ul><br><p>  Requisitos da biblioteca do provedor OpenStack: </p><br><ul><li>  Terraform 0.11 (ou superior) </li></ul><br><p>  Para instalar o Terraform, voc√™ precisa fazer o download do pacote apropriado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no site da Hashicorp</a> e descompact√°-lo.  Em seguida, o caminho para o arquivo descompactado deve ser salvo na vari√°vel PATH.  Use o comando terraform para verificar se tudo est√° instalado.  Saiba mais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui.</a> </p><br><p>  Dependendo do sistema operacional, o Ansible pode ser instalado com alguns comandos.  Veja a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o Ansible.</a>  Aqui eu uso o Ubuntu e instalo o Ansible da seguinte maneira. </p><br><pre><code class="plaintext hljs">sudo apt update sudo apt install ansible</code> </pre> <br><p>  Ent√£o voc√™ precisa instalar as depend√™ncias do Kubespray.  Isso √© feito pelo seguinte comando.  Mas primeiro voc√™ precisa clonar o reposit√≥rio. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-sigs/kubespray sudo pip install -r requirements.txt</code> </pre> <br><p>  Para usar o Open Telekom Cloud, configure os dados de acesso usando .ostackrc no diret√≥rio raiz e carregue as vari√°veis ‚Äã‚Äãde ambiente. </p><br><h2 id="planirovanie-klastera">  Planejamento de cluster </h2><br><p>  O Kubernetes √© muito flex√≠vel, portanto o cluster pode ser adaptado √†s suas necessidades.  Aqui n√£o consideraremos op√ß√µes diferentes para clusters.  Voc√™ pode ler sobre isso na documenta√ß√£o do Kubernetes em Criando um cluster personalizado a partir do zero.  Por exemplo, criaremos um cluster a partir do assistente com o etcd e dois n√≥s de trabalho.  O cluster n√£o ter√° um IP flutuante, portanto n√£o estar√° dispon√≠vel na Internet. </p><br><p>  Tamb√©m precisamos escolher CNI (Container Network Interface).  Existem v√°rias op√ß√µes (c√≠lio, chita, flanela, rede de tecer, etc.), mas usaremos uma flanela, que n√£o precisa ser configurada.  O Calico funcionar√°, mas voc√™ precisar√° <a href="">configurar as portas do OpenStack Neutron</a> para sub-redes de servi√ßos e pods. </p><br><p>  Para gerenciar os clusters no painel do Kubernetes ap√≥s a implanta√ß√£o, precisamos instalar esse painel. </p><br><h2 id="nastroyka-konfiguracii-klastera">  Configura√ß√£o de Cluster </h2><br><p>  Execute os seguintes comandos no diret√≥rio do reposit√≥rio, especificando o nome desejado na vari√°vel $ CLUSTER. </p><br><pre> <code class="plaintext hljs">cp -LRp contrib/terraform/openstack/sample-inventory \ inventory/$CLUSTER cd inventory/$CLUSTER ln -s ../../contrib/terraform/openstack/hosts ln -s ../../contrib</code> </pre> <br><p>  Ap√≥s executar os comandos, edite o arquivo invent√°rio / $ CLUSTER / cluster.tf. </p><br><pre> <code class="plaintext hljs"># your Kubernetes cluster name here cluster_name = "k8s-test-cluster" az_list=["eu-de-01", "eu-de-02"] dns_nameservers=["100.125.4.25", "8.8.8.8"] # SSH key to use for access to nodes public_key_path = "~/.ssh/id_rsa.pub" # image to use for bastion, masters, standalone etcd instances, and nodes image = "Standard_CentOS_7_latest" # user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.) ssh_user = "linux" # 0|1 bastion nodes number_of_bastions = 0 flavor_bastion = "s2.xlarge.4" # standalone etcds number_of_etcd = 0 flavor_etcd = "s2.xlarge.4" # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = "s2.xlarge.4" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 2 flavor_k8s_node = "s2.xlarge.4" # GlusterFS # either 0 or more than one #number_of_gfs_nodes_no_floating_ip = 1 #gfs_volume_size_in_gb = 150 # Container Linux does not support GlusterFS image_gfs = "Standard_CentOS_7_latest" # May be different from other nodes #ssh_user_gfs = "linux" #flavor_gfs_node = "s2.xlarge.4" # networking network_name = "k8s-test-network" external_net = "Externel_Network_ID" subnet_cidr = "192.168.100.0/24" floatingip_pool = "admin_external_net" bastion_allowed_remote_ips = ["0.0.0.0/0"]</code> </pre> <br><p>  Descri√ß√£o das vari√°veis ‚Äã‚Äãlidas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui.</a>  Neste exemplo, criaremos um cluster com um mestre e dois n√≥s de trabalho do Kubernetes com base na vers√£o mais recente do CentOS 7 e s2.xlarge.4.  O etcd tamb√©m instala no assistente. </p><br><h2 id="razvertyvanie-infrastruktury">  Implanta√ß√£o de infraestrutura </h2><br><p>  Agora estamos prontos para implantar a infraestrutura de cluster usando o Terraform.  A figura mostra a apar√™ncia da infraestrutura ap√≥s a implanta√ß√£o.  Detalhes abaixo. </p><br><p><img src="https://habrastorage.org/webt/vk/2o/aw/vk2oawuj3cgahv3v3i7ockh_fbe.png"></p><br><p>  Para iniciar a implanta√ß√£o do Terraform, v√° para o diret√≥rio invent√°rio / $ CLUSTER / e execute os seguintes comandos.  Primeiro, instale os plugins necess√°rios.  Para fazer isso, precisamos do argumento init e do caminho para os plugins. </p><br><pre> <code class="plaintext hljs">terraform init ../../contrib/terraform/openstack</code> </pre> <br><p>  Esta opera√ß√£o √© muito r√°pida.  Agora, o Terraform est√° pronto para implantar a infraestrutura com o argumento apply. </p><br><pre> <code class="plaintext hljs">terraform apply -var-file=cluster.tf ../../contrib/terraform/openstack</code> </pre> <br><p>  Ap√≥s alguns segundos, o Terraform deve mostrar um resultado semelhante e as inst√¢ncias estar√£o dispon√≠veis para o trabalho. </p><br><pre> <code class="plaintext hljs">Apply complete! Resources: 3 added, 0 changed, 0 destroyed.</code> </pre> <br><p>  Para verificar a disponibilidade do servidor, execute o seguinte comando Ansible e iremos para a pasta raiz do reposit√≥rio. </p><br><pre> <code class="plaintext hljs">$ ansible -i inventory/$CLUSTER/hosts -m ping all example-k8s_node-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" } example-etcd-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" } example-k8s-master-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" }</code> </pre> <br><h2 id="razvertyvanie-klastera-kubernetes">  Implanta√ß√£o de Cluster Kubernetes </h2><br><p>  A infraestrutura est√° implantada, agora voc√™ precisa instalar o cluster Kubernetes.  Primeiro, configure as vari√°veis ‚Äã‚Äãde cluster no invent√°rio de arquivos / $ CLUSTER / group_vars / all / all.yml.  Aqui voc√™ precisa definir o valor de cloud_provider como openstack e, para bin_dir, o caminho em que os arquivos ser√£o instalados.  No nosso exemplo, usamos a seguinte configura√ß√£o. </p><br><pre> <code class="plaintext hljs">## Directory where etcd data stored etcd_data_dir: /var/lib/etcd ## Directory where the binaries will be installed bin_dir: /usr/local/bin ## The access_ip variable is used to define how other nodes should access ## the node. This is used in flannel to allow other flannel nodes to see ## this node for example. The access_ip is really useful AWS and Google ## environments where the nodes are accessed remotely by the "public" ip, ## but don't know about that address themselves. #access_ip: 1.1.1.1 ## External LB example config ## apiserver_loadbalancer_domain_name: "elb.some.domain" #loadbalancer_apiserver: # address: 1.2.3.4 # port: 1234 ## Internal loadbalancers for apiservers #loadbalancer_apiserver_localhost: true ## Local loadbalancer should use this port instead, if defined. ## Defaults to kube_apiserver_port (6443) #nginx_kube_apiserver_port: 8443 ### OTHER OPTIONAL VARIABLES ## For some things, kubelet needs to load kernel modules. For example, dynamic kernel services are needed ## for mounting persistent volumes into containers. These may not be loaded by preinstall kubernetes ## processes. For example, ceph and rbd backed volumes. Set to true to allow kubelet to load kernel ## modules. #kubelet_load_modules: false ## Upstream dns servers used by dnsmasq #upstream_dns_servers: # - 8.8.8.8 # - 8.8.4.4 ## There are some changes specific to the cloud providers ## for instance we need to encapsulate packets with some network plugins ## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external' ## When openstack is used make sure to source in the openstack credentials ## like you would do when using nova-client before starting the playbook. ## Note: The 'external' cloud provider is not supported. ## TODO(riverzhang): https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager cloud_provider: openstack ## Set these proxy values in order to update package manager and docker daemon to use proxies #http_proxy: "" #https_proxy: "" ## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy #no_proxy: "" ## Some problems may occur when downloading files over https proxy due to ansible bug ## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable ## SSL validation of get_url module. Note that kubespray will still be performing checksum validation. #download_validate_certs: False ## If you need exclude all cluster nodes from proxy and other resources, add other resources here. #additional_no_proxy: "" ## Certificate Management ## This setting determines whether certs are generated via scripts. ## Chose 'none' if you provide your own certificates. ## Option is "script", "none" ## note: vault is removed #cert_management: script ## Set to true to allow pre-checks to fail and continue deployment #ignore_assert_errors: false ## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable. #kube_read_only_port: 10255 ## Set true to download and cache container download_container: false ## Deploy container engine # Set false if you want to deploy container engine manually. #deploy_container_engine: true ## Set Pypi repo and cert accordingly #pyrepo_index: https://pypi.example.com/simple #pyrepo_cert: /etc/ssl/certs/ca-certificates.crt</code> </pre> <br><p>  Agora configure o arquivo invent√°rio / $ CLUSTER / group_vars / k8s-cluster / k8s-cluster.yml.  Para a vari√°vel kube_network_plugin, configure flannel ou chita (voc√™ precisa <a href="">configurar as portas do OpenStack Neutron</a> ).  Teremos essa flanela, que n√£o precisa ser configurada.  Para a vari√°vel resolvconf_mode, configure docker_dns.  Este valor informa ao Kubespray para definir os <a href="">par√¢metros do daemon do Docker.</a>  Abaixo, voc√™ v√™ um exemplo de configura√ß√£o para nosso cluster. </p><br><pre> <code class="plaintext hljs"># Kubernetes configuration dirs and system namespace. # Those are where all the additional config stuff goes # the kubernetes normally puts in /srv/kubernetes. # This puts them in a sane location and namespace. # Editing those values will almost surely break something. kube_config_dir: /etc/kubernetes kube_script_dir: "{{ bin_dir }}/kubernetes-scripts" kube_manifest_dir: "{{ kube_config_dir }}/manifests" # This is where all the cert scripts and certs will be located kube_cert_dir: "{{ kube_config_dir }}/ssl" # This is where all of the bearer tokens will be stored kube_token_dir: "{{ kube_config_dir }}/tokens" # This is where to save basic auth file kube_users_dir: "{{ kube_config_dir }}/users" kube_api_anonymous_auth: true ## Change this to use another Kubernetes version, eg a current beta release kube_version: v1.13.3 # kubernetes image repo define kube_image_repo: "gcr.io/google-containers" # Where the binaries will be downloaded. # Note: ensure that you've enough disk space (about 1G) local_release_dir: "/tmp/releases" # Random shifts for retrying failed ops like pushing/downloading retry_stagger: 5 # This is the group that the cert creation scripts chgrp the # cert files to. Not really changeable... kube_cert_group: kube-cert # Cluster Loglevel configuration kube_log_level: 2 # Directory where credentials will be stored credentials_dir: "{{ inventory_dir }}/credentials" # Users to create for basic auth in Kubernetes API via HTTP # Optionally add groups for user kube_api_pwd: "{{ lookup('password', credentials_dir + '/kube_user.creds length=15 chars=ascii_letters,digits') }}" kube_users: kube: pass: "{{kube_api_pwd}}" role: admin groups: - system:masters ## It is possible to activate / deactivate selected authentication methods (basic auth, static token auth) #kube_oidc_auth: false #kube_basic_auth: false #kube_token_auth: false ## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/ ## To use OpenID you have to deploy additional an OpenID Provider (eg Dex, Keycloak, ...) # kube_oidc_url: https:// ... # kube_oidc_client_id: kubernetes ## Optional settings for OIDC # kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem" # kube_oidc_username_claim: sub # kube_oidc_username_prefix: oidc: # kube_oidc_groups_claim: groups # kube_oidc_groups_prefix: oidc: # Choose network plugin (cilium, calico, contiv, weave or flannel) # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing kube_network_plugin: flannel # Setting multi_networking to true will install Multus: https://github.com/intel/multus-cni kube_network_plugin_multus: false # Kubernetes internal network for services, unused block of space. kube_service_addresses: 10.233.0.0/18 # internal network. When used, it will assign IP # addresses from this range to individual pods. # This network must be unused in your network infrastructure! kube_pods_subnet: 10.233.64.0/18 # internal network node size allocation (optional). This is the size allocated # to each node on your network. With these defaults you should have # room for 4096 nodes with 254 pods per node. kube_network_node_prefix: 24 # The port the API Server will be listening on. kube_apiserver_ip: "{{ kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') }}" kube_apiserver_port: 6443 # (https) #kube_apiserver_insecure_port: 8080 # (http) # Set to 0 to disable insecure port - Requires RBAC in authorization_modes and kube_api_anonymous_auth: true kube_apiserver_insecure_port: 0 # (disabled) # Kube-proxy proxyMode configuration. # Can be ipvs, iptables kube_proxy_mode: ipvs # A string slice of values which specify the addresses to use for NodePorts. # Values may be valid IP blocks (eg 1.2.3.0/24, 1.2.3.4/32). # The default empty string slice ([]) means to use all local addresses. # kube_proxy_nodeport_addresses_cidr is retained for legacy config kube_proxy_nodeport_addresses: &gt;- {%- if kube_proxy_nodeport_addresses_cidr is defined -%} [{{ kube_proxy_nodeport_addresses_cidr }}] {%- else -%} [] {%- endif -%} # If non-empty, will use this string as identification instead of the actual hostname #kube_override_hostname: &gt;- # {%- if cloud_provider is defined and cloud_provider in [ 'aws' ] -%} # {%- else -%} # {{ inventory_hostname }} # {%- endif -%} ## Encrypting Secret Data at Rest (experimental) kube_encrypt_secret_data: false # DNS configuration. # Kubernetes cluster name, also will be used as DNS domain cluster_name: cluster.local # Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods ndots: 2 # Can be dnsmasq_kubedns, kubedns, coredns, coredns_dual, manual or none dns_mode: coredns # Set manual server if using a custom cluster DNS server #manual_dns_server: 10.xxx # Enable nodelocal dns cache enable_nodelocaldns: False nodelocaldns_ip: 169.254.25.10 # Can be docker_dns, host_resolvconf or none resolvconf_mode: docker_dns # Deploy netchecker app to verify DNS resolve as an HTTP service deploy_netchecker: false # Ip address of the kubernetes skydns service skydns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') }}" skydns_server_secondary: "{{ kube_service_addresses|ipaddr('net')|ipaddr(4)|ipaddr('address') }}" dnsmasq_dns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') }}" dns_domain: "{{ cluster_name }}" ## Container runtime ## docker for docker and crio for cri-o. container_manager: docker ## Settings for containerized control plane (etcd/kubelet/secrets) etcd_deployment_type: docker kubelet_deployment_type: host helm_deployment_type: host # K8s image pull policy (imagePullPolicy) k8s_image_pull_policy: IfNotPresent # audit log for kubernetes kubernetes_audit: false # dynamic kubelet configuration dynamic_kubelet_configuration: false # define kubelet config dir for dynamic kubelet #kubelet_config_dir: default_kubelet_config_dir: "{{ kube_config_dir }}/dynamic_kubelet_dir" dynamic_kubelet_configuration_dir: "{{ kubelet_config_dir | default(default_kubelet_config_dir) }}" # pod security policy (RBAC must be enabled either by having 'RBAC' in authorization_modes or kubeadm enabled) podsecuritypolicy_enabled: false # Make a copy of kubeconfig on the host that runs Ansible in {{ inventory_dir }}/artifacts # kubeconfig_localhost: false # Download kubectl onto the host that runs Ansible in {{ bin_dir }} # kubectl_localhost: false # dnsmasq # dnsmasq_upstream_dns_servers: # - /resolvethiszone.with/10.0.4.250 # - 8.8.8.8 # Enable creation of QoS cgroup hierarchy, if true top level QoS and pod cgroups are created. (default true) # kubelet_cgroups_per_qos: true # A comma separated list of levels of node allocatable enforcement to be enforced by kubelet. # Acceptable options are 'pods', 'system-reserved', 'kube-reserved' and ''. Default is "". # kubelet_enforce_node_allocatable: pods ## Supplementary addresses that can be added in kubernetes ssl keys. ## That can be useful for example to setup a keepalived virtual IP # supplementary_addresses_in_ssl_keys: [10.0.0.1, 10.0.0.2, 10.0.0.3] ## Running on top of openstack vms with cinder enabled may lead to unschedulable pods due to NoVolumeZoneConflict restriction in kube-scheduler. ## See https://github.com/kubernetes-sigs/kubespray/issues/2141 ## Set this variable to true to get rid of this issue volume_cross_zone_attachment: false # Add Persistent Volumes Storage Class for corresponding cloud provider ( OpenStack is only supported now ) persistent_volumes_enabled: false ## Container Engine Acceleration ## Enable container acceleration feature, for example use gpu acceleration in containers # nvidia_accelerator_enabled: true ## Nvidia GPU driver install. Install will by done by a (init) pod running as a daemonset. ## Important: if you use Ubuntu then you should set in all.yml 'docker_storage_options: -s overlay2' ## Array with nvida_gpu_nodes, leave empty or comment if you dont't want to install drivers. ## Labels and taints won't be set to nodes if they are not in the array. # nvidia_gpu_nodes: # - kube-gpu-001 # nvidia_driver_version: "384.111" ## flavor can be tesla or gtx # nvidia_gpu_flavor: gtx</code> </pre> <br><p>  Por fim, edite o arquivo invent√°rio / $ CLUSTER / group_vars / k8s-cluster / addons.yml e defina dashboard_enabled como true para configurar o painel.  Exemplo de configura√ß√£o: </p><br><pre> <code class="plaintext hljs"># Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled: true # Helm deployment helm_enabled: false # Registry deployment registry_enabled: false # registry_namespace: kube-system # registry_storage_class: "" # registry_disk_size: "10Gi" # Metrics Server deployment metrics_server_enabled: false # metrics_server_kubelet_insecure_tls: true # metrics_server_metric_resolution: 60s # metrics_server_kubelet_preferred_address_types: "InternalIP" # Local volume provisioner deployment local_volume_provisioner_enabled: false # local_volume_provisioner_namespace: kube-system # local_volume_provisioner_storage_classes: # local-storage: # host_dir: /mnt/disks # mount_dir: /mnt/disks # fast-disks: # host_dir: /mnt/fast-disks # mount_dir: /mnt/fast-disks # block_cleaner_command: # - "/scripts/shred.sh" # - "2" # volume_mode: Filesystem # fs_type: ext4 # CephFS provisioner deployment cephfs_provisioner_enabled: false # cephfs_provisioner_namespace: "cephfs-provisioner" # cephfs_provisioner_cluster: ceph # cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789" # cephfs_provisioner_admin_id: admin # cephfs_provisioner_secret: secret # cephfs_provisioner_storage_class: cephfs # cephfs_provisioner_reclaim_policy: Delete # cephfs_provisioner_claim_root: /volumes # cephfs_provisioner_deterministic_names: true # Nginx ingress controller deployment ingress_nginx_enabled: false # ingress_nginx_host_network: false # ingress_nginx_nodeselector: # node.kubernetes.io/node: "" # ingress_nginx_tolerations: # - key: "node.kubernetes.io/master" # operator: "Equal" # value: "" # effect: "NoSchedule" # ingress_nginx_namespace: "ingress-nginx" # ingress_nginx_insecure_port: 80 # ingress_nginx_secure_port: 443 # ingress_nginx_configmap: # map-hash-bucket-size: "128" # ssl-protocols: "SSLv2" # ingress_nginx_configmap_tcp_services: # 9000: "default/example-go:8080" # ingress_nginx_configmap_udp_services: # 53: "kube-system/kube-dns:53" # Cert manager deployment cert_manager_enabled: false # cert_manager_namespace: "cert-manager"</code> </pre> <br><p>  Ap√≥s alterar a configura√ß√£o, execute o ansible-playbook com nossa configura√ß√£o, executando o seguinte comando. </p><br><pre> <code class="plaintext hljs">ansible-playbook --become -i inventory/$CLUSTER/hosts cluster.yml</code> </pre> <br><p>  O Ansible executa v√°rias opera√ß√µes e, se todas forem conclu√≠das com √™xito, o cluster ser√° semelhante a esta figura. </p><br><p><img src="https://habrastorage.org/webt/2d/sg/tz/2dsgtzytttaugaufa1aolqy06wy.png"></p><br><h2 id="testirovanie">  Teste </h2><br><p>  Para testar o cluster, v√° para o assistente, alterne para o usu√°rio root e, no kubectl, execute o comando kubectl cluster-info para obter informa√ß√µes do cluster.  Voc√™ ver√° informa√ß√µes sobre o terminal do assistente e servi√ßos no cluster.  Se tudo estiver bem com o cluster, crie o usu√°rio do painel Kubernetes usando os seguintes comandos. </p><br><pre> <code class="plaintext hljs"># Create service account kubectl create serviceaccount cluster-admin-dashboard-sa # Bind ClusterAdmin role to the service account kubectl create clusterrolebinding cluster-admin-dashboard-sa \ --clusterrole=cluster-admin \ --serviceaccount=default:cluster-admin-dashboard-sa # Parse the token kubectl describe secret $(kubectl -n kube-system get secret | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}'</code> </pre> <br><p>  Agora voc√™ pode entrar no painel usando o token.  Primeiro, voc√™ precisa criar um encapsulamento para o assistente do Kubernetes, porque o painel ainda est√° aberto para o host local na porta 8001. Depois disso, voc√™ pode acessar o painel usando a URL localhost: 8001.  Agora selecione Token, insira o token e fa√ßa o login. </p><br><p><img src="https://habrastorage.org/webt/vr/gh/k2/vrghk2vujok6oexyyg-reeygrwk.png"></p><br><p>  Voc√™ est√° pronto para come√ßar a trabalhar no cluster Kubernetes.  Neste artigo, voc√™ viu como √© f√°cil implantar e configurar um cluster Kubernetes na nuvem do OpenStack. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456792/">https://habr.com/ru/post/pt456792/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456772/index.html">Fus√µes e Aquisi√ß√µes para equipes de projeto: como gerenciar dados e processos do projeto de maneira eficaz?</a></li>
<li><a href="../pt456774/index.html">O que sinto falta em Java depois de trabalhar com Kotlin / Scala</a></li>
<li><a href="../pt456780/index.html">9 maneiras de aumentar a efici√™ncia do desenvolvedor de aplicativos m√≥veis</a></li>
<li><a href="../pt456782/index.html">Projeto orientado a modelo - como n√£o repetir Chernobyl</a></li>
<li><a href="../pt456790/index.html">Resumo de not√≠cias do PostgreSQL. Edi√ß√£o No.16</a></li>
<li><a href="../pt456794/index.html">Arquitetura da interface do usu√°rio da Web: um passado de madeira, um presente estranho e um futuro brilhante</a></li>
<li><a href="../pt456796/index.html">Svalbard - novo nome para o projeto Fui Pwned antes da venda</a></li>
<li><a href="../pt456798/index.html">Tutoriais SDL 2: Li√ß√£o 5 - Texturas</a></li>
<li><a href="../pt456804/index.html">Siga o dinheiro: como o grupo RTM come√ßou a ocultar os endere√ßos dos servidores C&C em uma carteira criptogr√°fica</a></li>
<li><a href="../pt456806/index.html">Um bot de todas as preocupa√ß√µes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>