<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🤝‍👨🏻 🚶🏻 🆒 Comment fonctionne kubectl exec? 👨🏻‍💻 😼 🌇</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : L'auteur de l'article, Erkan Erol, un ingénieur de SAP, partage son étude des mécanismes de fonctionnement de la commande kubectl ex...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment fonctionne kubectl exec?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/466093/"> <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: L'auteur de l'article, Erkan Erol, un ingénieur de SAP, partage son étude des mécanismes de fonctionnement de la commande <code>kubectl exec</code> , si familière à tous ceux qui travaillent avec Kubernetes.</i>  <i>Il accompagne l'ensemble de l'algorithme avec des listes de code source Kubernetes (et des projets associés), qui vous permettent de comprendre le sujet aussi profondément que nécessaire.</i> <br><br><img src="https://habrastorage.org/webt/z7/je/ja/z7jejaxnf5kisqkvemay56t6_pe.png"><br><br>  Un vendredi, un collègue est venu me voir et m'a demandé comment exécuter une commande dans le pod à l'aide de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">client-go</a> .  Je n'ai pas pu lui répondre et j'ai soudain réalisé que je ne savais rien du mécanisme de travail des <code>kubectl exec</code> de <code>kubectl exec</code> .  Oui, j'avais certaines idées sur son appareil, mais je n'étais pas sûr à 100% de leur exactitude et j'ai donc décidé de résoudre ce problème.  Après avoir étudié les blogs, la documentation et le code source, j'ai appris beaucoup de nouvelles choses et dans cet article, je veux partager mes découvertes et ma compréhension.  Si quelque chose ne va pas, veuillez me contacter sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Twitter</a> . <a name="habracut"></a><br><br><h2>  La préparation </h2><br>  Pour créer un cluster sur un MacBook, j'ai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cloné ecomm-integration-ballerina / kubernetes-cluster</a> .  Il a ensuite corrigé les adresses IP des nœuds dans la configuration de kubelet, car les paramètres par défaut ne permettaient pas à <code>kubectl exec</code> d'être <code>kubectl exec</code> .  Vous pouvez en savoir plus sur la raison principale de cela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br><ul><li>  Toute voiture = mon MacBook </li><li>  IP maître = 192.168.205.10 </li><li>  IP hôte de travail = 192.168.205.11 </li><li>  Port du serveur API = 6443 </li></ul><br><h2>  Composants </h2><br><img src="https://habrastorage.org/webt/of/q5/dv/ofq5dvsbuqvr8bp6gryhxsficfk.png"><br><br><ul><li>  <b>Processus kubectl exec</b> : lorsque nous exécutons «kubectl exec ...», le processus démarre.  Vous pouvez le faire sur n'importe quelle machine ayant accès au serveur API K8s.  <i>Remarque</i>  <i>trans.: Plus loin dans les listes de consoles, l'auteur utilise le commentaire "n'importe quelle machine", ce qui implique que les commandes suivantes peuvent être exécutées sur toutes ces machines avec accès à Kubernetes.</i> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>serveur api</b></a> : un composant sur le maître qui donne accès à l'API Kubernetes.  Ceci est l'interface du plan de contrôle dans Kubernetes. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>kubelet</b></a> : un agent qui s'exécute sur chaque nœud du cluster.  Il fournit des conteneurs en pod'e. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>runtime conteneur</b></a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b>runtime conteneur</b></a> ): logiciel responsable du fonctionnement des conteneurs.  Exemples: Docker, CRI-O, containerd ... </li><li>  <b>noyau</b> : <b>noyau du</b> système d'exploitation sur le nœud de travail;  responsable de la gestion des processus. </li><li>  <b>conteneur</b> <b>cible</b> : conteneur qui fait partie d'un pod et fonctionne sur l'un des nœuds de travail. </li></ul><br><h2>  Ce que j'ai découvert </h2><br><h3>  1. Activité côté client </h3><br>  Créez un pod dans l'espace de noms <code>default</code> : <br><br><pre> <code class="bash hljs">// any machine $ kubectl run <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>-test-nginx --image=nginx</code> </pre> <br>  Ensuite, nous exécutons la commande exec et attendons 5000 secondes pour d'autres observations: <br><br><pre> <code class="bash hljs">// any machine $ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>-test-nginx-6558988d5-fgxgg -- sh <span class="hljs-comment"><span class="hljs-comment"># sleep 5000</span></span></code> </pre> <br>  Le processus kubectl apparaît (avec pid = 8507 dans notre cas): <br><br><pre> <code class="bash hljs">// any machine $ ps -ef |grep kubectl 501 8507 8409 0 7:19PM ttys000 0:00.13 kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>-test-nginx-6558988d5-fgxgg -- sh</code> </pre> <br>  Si nous vérifions l'activité réseau du processus, nous constatons qu'il a des connexions avec le serveur api (192.168.205.10.6443): <br><br><pre> <code class="bash hljs">// any machine $ netstat -atnv |grep 8507 tcp4 0 0 192.168.205.1.51673 192.168.205.10.6443 ESTABLISHED 131072 131768 8507 0 0x0102 0x00000020 tcp4 0 0 192.168.205.1.51672 192.168.205.10.6443 ESTABLISHED 131072 131768 8507 0 0x0102 0x00000028</code> </pre> <br>  Regardons le code.  Kubectl crée une requête POST avec la sous-ressource exec et envoie une requête REST: <br><br><pre> <code class="go hljs"> req := restClient.Post(). Resource(<span class="hljs-string"><span class="hljs-string">"pods"</span></span>). Name(pod.Name). Namespace(pod.Namespace). SubResource(<span class="hljs-string"><span class="hljs-string">"exec"</span></span>) req.VersionedParams(&amp;corev1.PodExecOptions{ Container: containerName, Command: p.Command, Stdin: p.Stdin, Stdout: p.Out != <span class="hljs-literal"><span class="hljs-literal">nil</span></span>, Stderr: p.ErrOut != <span class="hljs-literal"><span class="hljs-literal">nil</span></span>, TTY: t.Raw, }, scheme.ParameterCodec) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> p.Executor.Execute(<span class="hljs-string"><span class="hljs-string">"POST"</span></span>, req.URL(), p.Config, p.In, p.Out, p.ErrOut, t.Raw, sizeQueue)</code> </pre> <br>  <i>( <a href="">kubectl / pkg / cmd / exec / exec.go</a> )</i> <br><br><img src="https://habrastorage.org/webt/og/nn/cg/ognncgumyrwb7cr6l8wq5alo2je.png"><br><h3>  2. Activité sur le côté du nœud maître </h3><br>  On peut également observer la requête côté serveur api: <br><br><pre> <code class="bash hljs">handler.go:143] kube-apiserver: POST <span class="hljs-string"><span class="hljs-string">"/api/v1/namespaces/default/pods/exec-test-nginx-6558988d5-fgxgg/exec"</span></span> satisfied by gorestful with webservice /api/v1 upgradeaware.go:261] Connecting to backend proxy (intercepting redirects) https://192.168.205.11:10250/<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>/default/<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>-test-nginx-6558988d5-fgxgg/<span class="hljs-built_in"><span class="hljs-built_in">exec</span></span>-test-nginx?<span class="hljs-built_in"><span class="hljs-built_in">command</span></span>=sh&amp;input=1&amp;output=1&amp;tty=1 Headers: map[Connection:[Upgrade] Content-Length:[0] Upgrade:[SPDY/3.1] User-Agent:[kubectl/v1.12.10 (darwin/amd64) kubernetes/e3c1340] X-Forwarded-For:[192.168.205.1] X-Stream-Protocol-Version:[v4.channel.k8s.io v3.channel.k8s.io v2.channel.k8s.io channel.k8s.io]]</code> </pre> <br>  <i>Notez que la demande HTTP inclut une demande de changement de protocole.</i>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SPDY</a> vous permet de multiplexer des flux d'erreur stdin / stdout / stderr / spdy individuels via une seule connexion TCP.</i> <br><br>  Le serveur API reçoit la demande et la convertit en <code>PodExecOptions</code> : <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// PodExecOptions is the query options to a Pod's remote exec call type PodExecOptions struct { metav1.TypeMeta // Stdin if true indicates that stdin is to be redirected for the exec call Stdin bool // Stdout if true indicates that stdout is to be redirected for the exec call Stdout bool // Stderr if true indicates that stderr is to be redirected for the exec call Stderr bool // TTY if true indicates that a tty will be allocated for the exec call TTY bool // Container in which to execute the command. Container string // Command is the remote command to execute; argv array; not executed within a shell. Command []string }</span></span></code> </pre> <br>  <i>( <a href="">pkg / apis / core / types.go</a> )</i> <br><br>  Pour effectuer les actions requises, api-server doit savoir quel pod il doit contacter: <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// ExecLocation returns the exec URL for a pod container. If opts.Container is blank // and only one container is present in the pod, that container is used. func ExecLocation( getter ResourceGetter, connInfo client.ConnectionInfoGetter, ctx context.Context, name string, opts *api.PodExecOptions, ) (*url.URL, http.RoundTripper, error) { return streamLocation(getter, connInfo, ctx, name, opts, opts.Container, "exec") }</span></span></code> </pre> <br>  <i>( <a href="">pkg / registry / core / pod / strategy.go</a> )</i> <br><br>  Bien sûr, les données des points d'extrémité sont extraites des informations de l'hôte: <br><br><pre> <code class="go hljs"> nodeName := types.NodeName(pod.Spec.NodeName) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(nodeName) == <span class="hljs-number"><span class="hljs-number">0</span></span> { <span class="hljs-comment"><span class="hljs-comment">// If pod has not been assigned a host, return an empty location return nil, nil, errors.NewBadRequest(fmt.Sprintf("pod %s does not have a host assigned", name)) } nodeInfo, err := connInfo.GetConnectionInfo(ctx, nodeName)</span></span></code> </pre> <br>  <i>( <a href="">pkg / registry / core / pod / strategy.go</a> )</i> <br><br>  Hourra!  Kubelet dispose désormais d'un port ( <code>node.Status.DaemonEndpoints.KubeletEndpoint.Port</code> ) auquel le serveur API peut se connecter: <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// GetConnectionInfo retrieves connection info from the status of a Node API object. func (k *NodeConnectionInfoGetter) GetConnectionInfo(ctx context.Context, nodeName types.NodeName) (*ConnectionInfo, error) { node, err := k.nodes.Get(ctx, string(nodeName), metav1.GetOptions{}) if err != nil { return nil, err } // Find a kubelet-reported address, using preferred address type host, err := nodeutil.GetPreferredNodeAddress(node, k.preferredAddressTypes) if err != nil { return nil, err } // Use the kubelet-reported port, if present port := int(node.Status.DaemonEndpoints.KubeletEndpoint.Port) if port &lt;= 0 { port = k.defaultPort } return &amp;ConnectionInfo{ Scheme: k.scheme, Hostname: host, Port: strconv.Itoa(port), Transport: k.transport, }, nil }</span></span></code> </pre> <br>  <i>( <a href="">pkg / kubelet / client / kubelet_client.go</a> )</i> <br><br><blockquote>  De la documentation de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Communication Master-Node&gt; Master to Cluster&gt; apiserver to kubelet</a> : <br><br>  Ces connexions sont fermées sur le point de terminaison HTTPS de kubelet.  Par défaut, apiserver ne vérifie pas le certificat du kubelet, ce qui rend la connexion vulnérable aux «attaques intermédiaires» (MITM) et <i><b>peu sûre</b></i> pour travailler sur des réseaux non fiables et / ou publics. </blockquote><br>  Maintenant, le serveur API connaît le point final et établit une connexion: <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Connect returns a handler for the pod exec proxy func (r *ExecREST) Connect(ctx context.Context, name string, opts runtime.Object, responder rest.Responder) (http.Handler, error) { execOpts, ok := opts.(*api.PodExecOptions) if !ok { return nil, fmt.Errorf("invalid options object: %#v", opts) } location, transport, err := pod.ExecLocation(r.Store, r.KubeletConn, ctx, name, execOpts) if err != nil { return nil, err } return newThrottledUpgradeAwareProxyHandler(location, transport, false, true, true, responder), nil }</span></span></code> </pre> <br>  <i>( <a href="">pkg / registry / core / pod / rest / subresources.go</a> )</i> <br><br>  Voyons ce qui se passe sur le nœud maître. <br><br>  Nous découvrons d'abord l'IP du nœud de travail.  Dans notre cas, c'est 192.168.205.11: <br><br><pre> <code class="bash hljs">// any machine $ kubectl get nodes k8s-node-1 -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-node-1 Ready &lt;none&gt; 9h v1.15.3 192.168.205.11 &lt;none&gt; Ubuntu 16.04.6 LTS 4.4.0-159-generic docker://17.3.3</code> </pre> <br>  Installez ensuite le port kubelet (10250 dans notre cas): <br><br><pre> <code class="bash hljs">// any machine $ kubectl get nodes k8s-node-1 -o jsonpath=<span class="hljs-string"><span class="hljs-string">'{.status.daemonEndpoints.kubeletEndpoint}'</span></span> map[Port:10250]</code> </pre> <br>  Il est maintenant temps de vérifier le réseau.  Y a-t-il une connexion au nœud de travail (192.168.205.11)?  C'est là!  Si vous tuez le processus d' <code>exec</code> , il disparaîtra, donc je sais que la connexion a été établie par le serveur api à la suite de la commande exécutée. <br><br><pre> <code class="bash hljs">// master node $ netstat -atn |grep 192.168.205.11 tcp 0 0 192.168.205.10:37870 192.168.205.11:10250 ESTABLISHED …</code> </pre> <br><img src="https://habrastorage.org/webt/mp/-l/ql/mp-lqlocn8io490w9r9xenbawwg.png"><br><br>  La connexion entre kubectl et le serveur api est toujours ouverte.  De plus, il existe une autre connexion reliant api-server et kubelet. <br><br><h3>  3. Activité sur le nœud de travail </h3><br>  Maintenant, connectons-nous au nœud de travail et voyons ce qui s'y passe. <br><br>  Tout d'abord, nous voyons que la connexion avec elle est également établie (deuxième ligne);  192.168.205.10 est l'IP du nœud maître: <br><br><pre> <code class="bash hljs"> // worker node $ netstat -atn |grep 10250 tcp6 0 0 :::10250 :::* LISTEN tcp6 0 0 192.168.205.11:10250 192.168.205.10:37870 ESTABLISHED</code> </pre> <br>  Et notre équipe de <code>sleep</code> ?  Hourra, elle est aussi présente! <br><br><pre> <code class="bash hljs"> // worker node $ ps -afx ... 31463 ? Sl 0:00 \_ docker-containerd-shim 7d974065bbb3107074ce31c51f5ef40aea8dcd535ae11a7b8f2dd180b8ed583a /var/run/docker/libcontainerd/7d974065bbb3107074ce31c51 31478 pts/0 Ss 0:00 \_ sh 31485 pts/0 S+ 0:00 \_ sleep 5000 …</code> </pre> <br>  Mais attendez: comment le kubelet a-t-il démarré cela?  Il y a un démon dans kubelet qui permet d'accéder à l'API via le port pour les requêtes du serveur api: <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Server is the library interface to serve the stream requests. type Server interface { http.Handler // Get the serving URL for the requests. // Requests must not be nil. Responses may be nil iff an error is returned. GetExec(*runtimeapi.ExecRequest) (*runtimeapi.ExecResponse, error) GetAttach(req *runtimeapi.AttachRequest) (*runtimeapi.AttachResponse, error) GetPortForward(*runtimeapi.PortForwardRequest) (*runtimeapi.PortForwardResponse, error) // Start the server. // addr is the address to serve on (address:port) stayUp indicates whether the server should // listen until Stop() is called, or automatically stop after all expected connections are // closed. Calling Get{Exec,Attach,PortForward} increments the expected connection count. // Function does not return until the server is stopped. Start(stayUp bool) error // Stop the server, and terminate any open connections. Stop() error }</span></span></code> </pre> <br>  <i>( <a href="">pkg / kubelet / server / streaming / server.go</a> )</i> <br><br>  Kubelet calcule le point de terminaison de la réponse pour les demandes d'exécution: <br><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(s *server)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">GetExec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(req *runtimeapi.ExecRequest)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*runtimeapi.ExecResponse, error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err := validateExecRequest(req); err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-literal"><span class="hljs-literal">nil</span></span>, err } token, err := s.cache.Insert(req) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-literal"><span class="hljs-literal">nil</span></span>, err } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> &amp;runtimeapi.ExecResponse{ Url: s.buildURL(<span class="hljs-string"><span class="hljs-string">"exec"</span></span>, token), }, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> }</code> </pre> <br>  <i>( <a href="">pkg / kubelet / server / streaming / server.go</a> )</i> <br><br>  Ne confondez pas.  Il ne renvoie pas le résultat de la commande, mais le point de terminaison de la communication: <br><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> ExecResponse <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { <span class="hljs-comment"><span class="hljs-comment">// Fully qualified URL of the exec streaming server. Url string `protobuf:"bytes,1,opt,name=url,proto3" json:"url,omitempty"` XXX_NoUnkeyedLiteral struct{} `json:"-"` XXX_sizecache int32 `json:"-"` }</span></span></code> </pre> <br>  <i>( <a href="">cri-api / pkg / apis / runtime / v1alpha2 / api.pb.go</a> )</i> <br><br>  Kubelet implémente l'interface <code>RuntimeServiceClient</code> , qui fait partie de l'interface Container Runtime <i>(nous avons écrit plus à ce sujet, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> - environ Transl.)</i> : <br><br><div class="spoiler">  <b class="spoiler_title">Liste longue de cri-api à kubernetes / kubernetes</b> <div class="spoiler_text"><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream. type RuntimeServiceClient interface { // Version returns the runtime name, runtime version, and runtime API version. Version(ctx context.Context, in *VersionRequest, opts ...grpc.CallOption) (*VersionResponse, error) // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. RunPodSandbox(ctx context.Context, in *RunPodSandboxRequest, opts ...grpc.CallOption) (*RunPodSandboxResponse, error) // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (eg, IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. StopPodSandbox(ctx context.Context, in *StopPodSandboxRequest, opts ...grpc.CallOption) (*StopPodSandboxResponse, error) // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. RemovePodSandbox(ctx context.Context, in *RemovePodSandboxRequest, opts ...grpc.CallOption) (*RemovePodSandboxResponse, error) // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. PodSandboxStatus(ctx context.Context, in *PodSandboxStatusRequest, opts ...grpc.CallOption) (*PodSandboxStatusResponse, error) // ListPodSandbox returns a list of PodSandboxes. ListPodSandbox(ctx context.Context, in *ListPodSandboxRequest, opts ...grpc.CallOption) (*ListPodSandboxResponse, error) // CreateContainer creates a new container in specified PodSandbox CreateContainer(ctx context.Context, in *CreateContainerRequest, opts ...grpc.CallOption) (*CreateContainerResponse, error) // StartContainer starts the container. StartContainer(ctx context.Context, in *StartContainerRequest, opts ...grpc.CallOption) (*StartContainerResponse, error) // StopContainer stops a running container with a grace period (ie, timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">TODO:</span></span></span><span class="hljs-comment"> what must the runtime do after the grace period is reached? StopContainer(ctx context.Context, in *StopContainerRequest, opts ...grpc.CallOption) (*StopContainerResponse, error) // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. RemoveContainer(ctx context.Context, in *RemoveContainerRequest, opts ...grpc.CallOption) (*RemoveContainerResponse, error) // ListContainers lists all containers by filters. ListContainers(ctx context.Context, in *ListContainersRequest, opts ...grpc.CallOption) (*ListContainersResponse, error) // ContainerStatus returns status of the container. If the container is not // present, returns an error. ContainerStatus(ctx context.Context, in *ContainerStatusRequest, opts ...grpc.CallOption) (*ContainerStatusResponse, error) // UpdateContainerResources updates ContainerConfig of the container. UpdateContainerResources(ctx context.Context, in *UpdateContainerResourcesRequest, opts ...grpc.CallOption) (*UpdateContainerResourcesResponse, error) // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. ReopenContainerLog(ctx context.Context, in *ReopenContainerLogRequest, opts ...grpc.CallOption) (*ReopenContainerLogResponse, error) // ExecSync runs a command in a container synchronously. ExecSync(ctx context.Context, in *ExecSyncRequest, opts ...grpc.CallOption) (*ExecSyncResponse, error) // Exec prepares a streaming endpoint to execute a command in the container. Exec(ctx context.Context, in *ExecRequest, opts ...grpc.CallOption) (*ExecResponse, error) // Attach prepares a streaming endpoint to attach to a running container. Attach(ctx context.Context, in *AttachRequest, opts ...grpc.CallOption) (*AttachResponse, error) // PortForward prepares a streaming endpoint to forward ports from a PodSandbox. PortForward(ctx context.Context, in *PortForwardRequest, opts ...grpc.CallOption) (*PortForwardResponse, error) // ContainerStats returns stats of the container. If the container does not // exist, the call returns an error. ContainerStats(ctx context.Context, in *ContainerStatsRequest, opts ...grpc.CallOption) (*ContainerStatsResponse, error) // ListContainerStats returns stats of all running containers. ListContainerStats(ctx context.Context, in *ListContainerStatsRequest, opts ...grpc.CallOption) (*ListContainerStatsResponse, error) // UpdateRuntimeConfig updates the runtime configuration based on the given request. UpdateRuntimeConfig(ctx context.Context, in *UpdateRuntimeConfigRequest, opts ...grpc.CallOption) (*UpdateRuntimeConfigResponse, error) // Status returns the status of the runtime. Status(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error) }</span></span></code> </pre> <br>  <i>( <a href="">cri-api / pkg / apis / runtime / v1alpha2 / api.pb.go</a> )</i> </div></div><br>  Il utilise simplement gRPC pour appeler une méthode via l'interface Container Runtime: <br><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> runtimeServiceClient <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { cc *grpc.ClientConn }</code> </pre> <br>  <i>( <a href="">cri-api / pkg / apis / runtime / v1alpha2 / api.pb.go</a> )</i> <br><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *runtimeServiceClient)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Exec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ctx context.Context, in *ExecRequest, opts ...grpc.CallOption)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*ExecResponse, error)</span></span></span></span> { out := <span class="hljs-built_in"><span class="hljs-built_in">new</span></span>(ExecResponse) err := c.cc.Invoke(ctx, <span class="hljs-string"><span class="hljs-string">"/runtime.v1alpha2.RuntimeService/Exec"</span></span>, in, out, opts...) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-literal"><span class="hljs-literal">nil</span></span>, err } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> out, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> }</code> </pre> <br>  <i>( <a href="">cri-api / pkg / apis / runtime / v1alpha2 / api.pb.go</a> )</i> <br><br>  Le Container Runtime est responsable de l'implémentation du <code>RuntimeServiceServer</code> : <br><br><div class="spoiler">  <b class="spoiler_title">Liste longue de cri-api à kubernetes / kubernetes</b> <div class="spoiler_text"><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// RuntimeServiceServer is the server API for RuntimeService service. type RuntimeServiceServer interface { // Version returns the runtime name, runtime version, and runtime API version. Version(context.Context, *VersionRequest) (*VersionResponse, error) // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. RunPodSandbox(context.Context, *RunPodSandboxRequest) (*RunPodSandboxResponse, error) // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (eg, IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. StopPodSandbox(context.Context, *StopPodSandboxRequest) (*StopPodSandboxResponse, error) // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. RemovePodSandbox(context.Context, *RemovePodSandboxRequest) (*RemovePodSandboxResponse, error) // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. PodSandboxStatus(context.Context, *PodSandboxStatusRequest) (*PodSandboxStatusResponse, error) // ListPodSandbox returns a list of PodSandboxes. ListPodSandbox(context.Context, *ListPodSandboxRequest) (*ListPodSandboxResponse, error) // CreateContainer creates a new container in specified PodSandbox CreateContainer(context.Context, *CreateContainerRequest) (*CreateContainerResponse, error) // StartContainer starts the container. StartContainer(context.Context, *StartContainerRequest) (*StartContainerResponse, error) // StopContainer stops a running container with a grace period (ie, timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">TODO:</span></span></span><span class="hljs-comment"> what must the runtime do after the grace period is reached? StopContainer(context.Context, *StopContainerRequest) (*StopContainerResponse, error) // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. RemoveContainer(context.Context, *RemoveContainerRequest) (*RemoveContainerResponse, error) // ListContainers lists all containers by filters. ListContainers(context.Context, *ListContainersRequest) (*ListContainersResponse, error) // ContainerStatus returns status of the container. If the container is not // present, returns an error. ContainerStatus(context.Context, *ContainerStatusRequest) (*ContainerStatusResponse, error) // UpdateContainerResources updates ContainerConfig of the container. UpdateContainerResources(context.Context, *UpdateContainerResourcesRequest) (*UpdateContainerResourcesResponse, error) // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. ReopenContainerLog(context.Context, *ReopenContainerLogRequest) (*ReopenContainerLogResponse, error) // ExecSync runs a command in a container synchronously. ExecSync(context.Context, *ExecSyncRequest) (*ExecSyncResponse, error) // Exec prepares a streaming endpoint to execute a command in the container. Exec(context.Context, *ExecRequest) (*ExecResponse, error) // Attach prepares a streaming endpoint to attach to a running container. Attach(context.Context, *AttachRequest) (*AttachResponse, error) // PortForward prepares a streaming endpoint to forward ports from a PodSandbox. PortForward(context.Context, *PortForwardRequest) (*PortForwardResponse, error) // ContainerStats returns stats of the container. If the container does not // exist, the call returns an error. ContainerStats(context.Context, *ContainerStatsRequest) (*ContainerStatsResponse, error) // ListContainerStats returns stats of all running containers. ListContainerStats(context.Context, *ListContainerStatsRequest) (*ListContainerStatsResponse, error) // UpdateRuntimeConfig updates the runtime configuration based on the given request. UpdateRuntimeConfig(context.Context, *UpdateRuntimeConfigRequest) (*UpdateRuntimeConfigResponse, error) // Status returns the status of the runtime. Status(context.Context, *StatusRequest) (*StatusResponse, error) }</span></span></code> </pre> <br>  <i>( <a href="">cri-api / pkg / apis / runtime / v1alpha2 / api.pb.go</a> )</i> </div></div><br><img src="https://habrastorage.org/webt/ud/wx/mb/udwxmbczx1kgrhy_etiwnmkhlha.png"><br><br>  Si c'est le cas, nous devrions voir une connexion entre le kubelet et le runtime du conteneur, non?  Voyons ça. <br><br>  Exécutez cette commande avant et après la commande exec et examinez les différences.  Dans mon cas, la différence est la suivante: <br><br><pre> <code class="bash hljs">// worker node $ ss -a -p |grep kubelet ... u_str ESTAB 0 0 * 157937 * 157387 users:((<span class="hljs-string"><span class="hljs-string">"kubelet"</span></span>,pid=5714,fd=33)) ...</code> </pre> <br>  Hmmm ... Une nouvelle connexion via des sockets unix entre kubelet (pid = 5714) et quelque chose d'inconnu.  Qu'est-ce que ça pourrait être?  C'est vrai, c'est Docker (pid = 1186)! <br><br><pre> <code class="bash hljs">// worker node $ ss -a -p |grep 157387 ... u_str ESTAB 0 0 * 157937 * 157387 users:((<span class="hljs-string"><span class="hljs-string">"kubelet"</span></span>,pid=5714,fd=33)) u_str ESTAB 0 0 /var/run/docker.sock 157387 * 157937 users:((<span class="hljs-string"><span class="hljs-string">"dockerd"</span></span>,pid=1186,fd=14)) ...</code> </pre> <br>  Comme vous vous en souvenez, il s'agit d'un processus de démon docker (pid = 1186) qui exécute notre commande: <br><br><pre> <code class="bash hljs">// worker node $ ps -afx ... 1186 ? Ssl 0:55 /usr/bin/dockerd -H fd:// 17784 ? Sl 0:00 \_ docker-containerd-shim 53a0a08547b2f95986402d7f3b3e78702516244df049ba6c5aa012e81264aa3c /var/run/docker/libcontainerd/53a0a08547b2f95986402d7f3 17801 pts/2 Ss 0:00 \_ sh 17827 pts/2 S+ 0:00 \_ sleep 5000 ...</code> </pre> <br><h3>  4. Activité dans le runtime du conteneur </h3><br>  Examinons le code source du CRI-O pour comprendre ce qui se passe.  Dans Docker, la logique est similaire. <br><br>  Il existe un serveur chargé d'implémenter le <code>RuntimeServiceServer</code> : <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Server implements the RuntimeService and ImageService type Server struct { config libconfig.Config seccompProfile *seccomp.Seccomp stream StreamService netPlugin ocicni.CNIPlugin hostportManager hostport.HostPortManager appArmorProfile string hostIP string bindAddress string *lib.ContainerServer monitorsChan chan struct{} defaultIDMappings *idtools.IDMappings systemContext *types.SystemContext // Never nil updateLock sync.RWMutex seccompEnabled bool appArmorEnabled bool }</span></span></code> </pre> <br>  <i>( <a href="">cri-o / server / server.go</a> )</i> <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Exec prepares a streaming endpoint to execute a command in the container. func (s *Server) Exec(ctx context.Context, req *pb.ExecRequest) (resp *pb.ExecResponse, err error) { const operation = "exec" defer func() { recordOperation(operation, time.Now()) recordError(operation, err) }() resp, err = s.getExec(req) if err != nil { return nil, fmt.Errorf("unable to prepare exec endpoint: %v", err) } return resp, nil }</span></span></code> </pre> <br>  <i>( <a href="">cri-o / erver / container_exec.go</a> )</i> <br><br>  À la fin de la chaîne, le runtime du conteneur exécute une commande sur le nœud de travail: <br><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// ExecContainer prepares a streaming endpoint to execute a command in the container. func (r *runtimeOCI) ExecContainer(c *Container, cmd []string, stdin io.Reader, stdout, stderr io.WriteCloser, tty bool, resize &lt;-chan remotecommand.TerminalSize) error { processFile, err := prepareProcessExec(c, cmd, tty) if err != nil { return err } defer os.RemoveAll(processFile.Name()) args := []string{rootFlag, r.root, "exec"} args = append(args, "--process", processFile.Name(), c.ID()) execCmd := exec.Command(r.path, args...) if v, found := os.LookupEnv("XDG_RUNTIME_DIR"); found { execCmd.Env = append(execCmd.Env, fmt.Sprintf("XDG_RUNTIME_DIR=%s", v)) } var cmdErr, copyError error if tty { cmdErr = ttyCmd(execCmd, stdin, stdout, resize) } else { if stdin != nil { // Use an os.Pipe here as it returns true *os.File objects. // This way, if you run 'kubectl exec &lt;pod&gt; -i bash' (no tty) and type 'exit', // the call below to execCmd.Run() can unblock because its Stdin is the read half // of the pipe. r, w, err := os.Pipe() if err != nil { return err } go func() { _, copyError = pools.Copy(w, stdin) }() execCmd.Stdin = r } if stdout != nil { execCmd.Stdout = stdout } if stderr != nil { execCmd.Stderr = stderr } cmdErr = execCmd.Run() } if copyError != nil { return copyError } if exitErr, ok := cmdErr.(*exec.ExitError); ok { return &amp;utilexec.ExitErrorWrapper{ExitError: exitErr} } return cmdErr }</span></span></code> </pre> <br>  <i>( <a href="">cri-o / internal / oci / runtime_oci.go</a> )</i> <br><br><img src="https://habrastorage.org/webt/hm/dt/jk/hmdtjkw28fyrng0zsjghbw7tq5g.png"><br><br>  Enfin, le noyau exécute les commandes: <br><br><img src="https://habrastorage.org/webt/f2/7n/jk/f27njkgk1lqwo0ik-9vkj-kocic.png"><br><br><h2>  Rappels </h2><br><ul><li>  API Server peut également initier une connexion à kubelet. </li><li>  Les connexions suivantes sont maintenues jusqu'à la fin de la session d'exécution interactive: <ul><li>  entre kubectl et api-server; </li><li>  entre api-server et kubectl; </li><li>  entre le kubelet et l'exécution du conteneur. </li></ul></li><li>  Kubectl ou api-server ne peuvent rien exécuter sur les nœuds de production.  Kubelet peut démarrer, mais pour ces actions, il interagit également avec le runtime du conteneur. </li></ul><br><h2>  Les ressources </h2><br><ul><li>  La discussion " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment" kubectl exec "fonctionne</a> " dans kubernetes-dev; </li><li>  Article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jouer avec le kubeadm dans les machines vagabondes, partie 2</a> "; </li><li>  La discussion " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment trouver une autre fin de connexion socket Unix?</a>  "On Server Fault. </li></ul><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  "Que se passe-t-il dans Kubernetes lorsque l'exécution de kubectl démarre?"  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partie 2</a> ; </li><li>  « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alors, qu'est-ce qu'un pod dans Kubernetes?</a>  "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment fonctionne le planificateur Kubernetes?"</a>  "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment la haute disponibilité chez Kubernetes est assurée</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr466093/">https://habr.com/ru/post/fr466093/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr466071/index.html">10,3 secondes par hachage: extraction sur l'ordinateur de contrôle embarqué du vaisseau spatial Apollo</a></li>
<li><a href="../fr466075/index.html">Comment j'ai quitté la science fondamentale dans une startup</a></li>
<li><a href="../fr466077/index.html">Journée Techdir de diffusion à Saint-Pétersbourg</a></li>
<li><a href="../fr466081/index.html">3 septembre</a></li>
<li><a href="../fr466089/index.html">Algorithme de pensée et de conscience, partie 2</a></li>
<li><a href="../fr466097/index.html">Surveillance des applications .NET</a></li>
<li><a href="../fr466099/index.html">Caractéristiques du test d'une application Web pour le service vidéo</a></li>
<li><a href="../fr466103/index.html">Surveillance de la sécurité du cloud</a></li>
<li><a href="../fr466105/index.html">Overclocking de l'API Magento Rest avec RoadRunner</a></li>
<li><a href="../fr466107/index.html">Système de maison intelligente à faire soi-même</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>