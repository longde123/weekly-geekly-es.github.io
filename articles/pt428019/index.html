<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ™Ô∏è üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø üë®üèº‚Äçü§ù‚Äçüë®üèª Jogue Mortal Kombat com TensorFlow.js üöè üêú ü¶ï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Experimentando melhorias para o modelo de previs√£o do Guess.js. , comecei a examinar de perto o aprendizado profundo: redes neurais recorrentes (RNNs)...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jogue Mortal Kombat com TensorFlow.js</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/">  Experimentando melhorias para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo de</a> previs√£o do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Guess.js.</a> , comecei a examinar de perto o aprendizado profundo: redes neurais recorrentes (RNNs), em particular LSTMs, devido √† sua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"efic√°cia irracional"</a> na √°rea em que o Guess.js. trabalha.  Ao mesmo tempo, comecei a brincar com redes neurais convolucionais (CNNs), que tamb√©m s√£o frequentemente usadas para s√©ries temporais.  As CNNs s√£o comumente usadas para classificar, reconhecer e detectar imagens. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">Gerenciando o <a href="">MK.js</a> com o TensorFlow.js</font></i> <br><br><blockquote>  O c√≥digo fonte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deste artigo</a> e do <a href="">MK.js</a> est√£o no meu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GitHub</a> .  N√£o publiquei um conjunto de dados de treinamento, mas voc√™ pode criar seu pr√≥prio e treinar o modelo conforme descrito abaixo! </blockquote><a name="habracut"></a><br>  Depois de jogar na CNN, lembrei-me de um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">experimento</a> que conduzi h√° v√°rios anos quando os desenvolvedores de navegadores lan√ßaram a API <code>getUserMedia</code> .  Nele, a c√¢mera do usu√°rio serviu como um controlador para reproduzir o pequeno clone JavaScript do Mortal Kombat 3. Voc√™ pode encontrar esse jogo no <a href="">reposit√≥rio do GitHub</a> .  Como parte do experimento, implementei um algoritmo de posicionamento b√°sico que classifica a imagem nas seguintes classes: <br><br><ul><li>  Soco esquerdo ou direito </li><li>  Chute esquerdo ou direito </li><li>  Passos √† esquerda e √† direita </li><li>  Agachamento </li><li>  Nenhuma das op√ß√µes acima </li></ul><br>  O algoritmo √© t√£o simples que eu posso explicar em algumas frases: <br><br><blockquote>  O algoritmo fotografa o plano de fundo.  Assim que o usu√°rio aparece no quadro, o algoritmo calcula a diferen√ßa entre o plano de fundo e o quadro atual com o usu√°rio.  Portanto, determina a posi√ß√£o da figura do usu√°rio.  O pr√≥ximo passo √© exibir o corpo do usu√°rio em branco ou preto.  Depois disso, s√£o constru√≠dos histogramas verticais e horizontais, somando os valores para cada pixel.  Com base nesse c√°lculo, o algoritmo determina a posi√ß√£o atual do corpo. </blockquote><br>  O v√≠deo mostra como o programa funciona.  C√≥digo fonte do <a href="">GitHub</a> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Embora o pequeno clone MK tenha funcionado com sucesso, o algoritmo est√° longe de ser perfeito.  √â necess√°ria uma moldura com fundo.  Para uma opera√ß√£o adequada, o plano de fundo deve ter a mesma cor durante a execu√ß√£o do programa.  Essa limita√ß√£o significa que mudan√ßas na luz, sombra e outras coisas interferem e d√£o um resultado impreciso.  Finalmente, o algoritmo n√£o reconhece a a√ß√£o;  ele apenas classifica o novo quadro como a posi√ß√£o do corpo a partir de um conjunto predefinido. <br><br>  Agora, gra√ßas ao progresso na API da web, ou seja, WebGL, decidi voltar a esta tarefa aplicando o TensorFlow.js. <br><br><h1>  1. Introdu√ß√£o </h1><br>  Neste artigo, compartilharei minha experi√™ncia na cria√ß√£o de um algoritmo para classificar posi√ß√µes corporais usando o TensorFlow.js e o MobileNet.  Considere os seguintes t√≥picos: <br><br><ul><li>  Coleta de dados de treinamento para classifica√ß√£o de imagens </li><li>  Aumento de dados com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">imgaug</a> </li><li>  Transfira o aprendizado com o MobileNet </li><li>  Classifica√ß√£o bin√°ria e classifica√ß√£o N-prim√°ria </li><li>  Treinando o modelo de classifica√ß√£o de imagem TensorFlow.js no Node.js e usando-o em um navegador </li><li>  Algumas palavras sobre a classifica√ß√£o de a√ß√µes com LSTM </li></ul><br>  Neste artigo, reduziremos o problema para determinar a posi√ß√£o do corpo com base em um quadro, em contraste com o reconhecimento de a√ß√µes por uma sequ√™ncia de quadros.  Desenvolveremos um modelo de aprendizado profundo com um professor que, com base na imagem da webcam do usu√°rio, determina os movimentos de uma pessoa: chute, perna ou nada disso. <br><br>  At√© o final do artigo, poderemos criar um modelo para jogar o <a href="">MK.js</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  Para uma melhor compreens√£o do artigo, o leitor deve estar familiarizado com os conceitos fundamentais de programa√ß√£o e JavaScript.  Um entendimento b√°sico da aprendizagem profunda tamb√©m √© √∫til, mas n√£o necess√°rio. <br><br><h1>  Coleta de dados </h1><br>  A precis√£o do modelo de aprendizado profundo depende muito da qualidade dos dados.  Precisamos nos esfor√ßar para coletar um extenso conjunto de dados, como na produ√ß√£o. <br><br>  Nosso modelo deve ser capaz de reconhecer socos e chutes.  Isso significa que devemos coletar imagens de tr√™s categorias: <br><br><ul><li>  Chutes </li><li>  Chutes </li><li>  Outros </li></ul><br>  Nesta experi√™ncia, dois volunt√°rios ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">@lili_vs</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">@gsamokovarov</a> ) me ajudaram a coletar fotos.  Gravamos 5 v√≠deos do QuickTime no meu MacBook Pro, cada um contendo 2-4 chutes e 2-4 chutes. <br><br>  Em seguida, usamos o ffmpeg para extrair quadros individuais dos v√≠deos e salv√°-los como imagens <code>jpg</code> : <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  Para executar o comando acima, voc√™ primeiro precisa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">instalar o</a> <code>ffmpeg</code> no computador. <br><br>  Se quisermos treinar o modelo, devemos fornecer os dados de entrada e os dados de sa√≠da correspondentes, mas, neste est√°gio, temos apenas um monte de imagens de tr√™s pessoas em poses diferentes.  Para estruturar os dados, voc√™ precisa classificar os quadros em tr√™s categorias: socos, chutes e outros.  Para cada categoria, um diret√≥rio separado √© criado onde todas as imagens correspondentes s√£o movidas. <br><br>  Assim, em cada diret√≥rio deve haver cerca de 200 imagens semelhantes √†s abaixo: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Observe que haver√° muito mais imagens no diret√≥rio Outros, porque relativamente poucos quadros cont√™m fotos de socos e pontap√©s, e nos quadros restantes as pessoas andam, viram ou controlam o v√≠deo.  Se tivermos muitas imagens de uma classe, corremos o risco de ensinar o modelo tendencioso a essa classe em particular.  Nesse caso, ao classificar uma imagem com impacto, a rede neural ainda pode determinar a classe ‚ÄúOutro‚Äù.  Para reduzir esse vi√©s, voc√™ pode remover algumas fotos do diret√≥rio Outros e treinar o modelo em um n√∫mero igual de imagens de cada categoria. <br><br>  Por conveni√™ncia, atribu√≠mos os n√∫meros nos n√∫meros de cat√°logos de <code>1</code> a <code>190</code> , para que a primeira imagem seja <code>1.jpg</code> , a segunda <code>2.jpg</code> , etc. <br><br>  Se treinarmos o modelo em apenas 600 fotografias tiradas no mesmo ambiente e com as mesmas pessoas, n√£o obteremos um n√≠vel muito alto de precis√£o.  Para aproveitar ao m√°ximo nossos dados, √© melhor gerar algumas amostras extras usando o aumento de dados. <br><br><h1>  Aumento de Dados </h1><br>  O aumento de dados √© uma t√©cnica que aumenta o n√∫mero de pontos de dados sintetizando novos pontos de um conjunto existente.  Normalmente, o aumento √© usado para aumentar o tamanho e a diversidade do conjunto de treinamento.  Transferimos as imagens originais para o pipeline de transforma√ß√µes que criam novas imagens.  Voc√™ n√£o pode abordar as transforma√ß√µes com muita agressividade: apenas outros socos de m√£o devem ser gerados a partir de um soco. <br><br>  As transforma√ß√µes aceit√°veis ‚Äã‚Äãs√£o rota√ß√£o, invers√£o de cores, desfoque, etc. Existem excelentes ferramentas de c√≥digo aberto para aumento de dados.  No momento em que escrevi este artigo em JavaScript, n√£o havia muitas op√ß√µes, ent√£o usei a biblioteca implementada no Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">imgaug</a> .  Possui um conjunto de aumentadores que podem ser aplicados probabilisticamente. <br><br>  Aqui est√° a l√≥gica de aumento de dados para esta experi√™ncia: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  Este script usa o m√©todo <code>main</code> com tr√™s <code>for</code> loops - um para cada categoria de imagem.  Em cada itera√ß√£o, em cada um dos loops, chamamos o m√©todo <code>draw_single_sequential_images</code> : o primeiro argumento √© o nome do arquivo, o segundo √© o caminho, o terceiro √© o diret√≥rio em que o resultado ser√° salvo. <br><br>  Depois disso, lemos a imagem do disco e aplicamos uma s√©rie de transforma√ß√µes.  Eu documentei a maioria das transforma√ß√µes no trecho de c√≥digo acima, portanto n√£o o repetiremos. <br><br>  Para cada imagem, outras 16 imagens s√£o criadas.  Aqui est√° um exemplo de como eles se parecem: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Observe que, no script acima, dimensionamos imagens para <code>100x56</code> pixels.  Fazemos isso para reduzir a quantidade de dados e, consequentemente, o n√∫mero de c√°lculos que nosso modelo executa durante o treinamento e a avalia√ß√£o. <br><br><h1>  Constru√ß√£o de modelo </h1><br>  Agora construa um modelo para classifica√ß√£o! <br><br>  Como estamos lidando com imagens, usamos uma rede neural convolucional (CNN).  Essa arquitetura de rede √© conhecida por ser adequada para reconhecimento de imagens, detec√ß√£o de objetos e classifica√ß√£o. <br><br><h3>  Transfer√™ncia de Aprendizado </h3><br>  A imagem abaixo mostra o popular CNN VGG-16, usado para classificar imagens. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  A Rede Neural VGG-16 reconhece 1000 classes de imagens.  Possui 16 camadas (sem contar as camadas de pool e sa√≠da).  Essa rede multicamada √© dif√≠cil de treinar na pr√°tica.  Isso exigir√° um grande conjunto de dados e muitas horas de treinamento. <br><br>  As camadas ocultas da CNN treinada reconhecem v√°rios elementos das imagens do conjunto de treinamento, come√ßando pelas bordas, passando para elementos mais complexos, como formas, objetos individuais e assim por diante.  Uma CNN treinada no estilo do VGG-16 para reconhecer um grande conjunto de imagens deve ter camadas ocultas que aprenderam muitos recursos do conjunto de treinamento.  Esses recursos ser√£o comuns √† maioria das imagens e, consequentemente, reutilizados em diferentes tarefas. <br><br>  A transfer√™ncia de aprendizado permite reutilizar uma rede existente e treinada.  Podemos pegar a sa√≠da de qualquer uma das camadas da rede existente e transferi-la como entrada para a nova rede neural.  Assim, ensinando a rede neural rec√©m-criada, com o tempo, ela pode ser ensinada a reconhecer novos recursos de um n√≠vel superior e a classificar corretamente imagens de classes que o modelo original nunca havia visto antes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  Para nossos prop√≥sitos, use a rede neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MobileNet do pacote @ tensorflow-models / mobilenet</a> .  O MobileNet √© t√£o poderoso quanto o VGG-16, mas √© muito menor, o que acelera a distribui√ß√£o direta, ou seja, a propaga√ß√£o de rede (propaga√ß√£o direta) e reduz o tempo de download no navegador.  A MobileNet treinou no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conjunto de dados de</a> classifica√ß√£o de imagem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ILSVRC-2012-CLS</a> . <br><br>  Ao desenvolver um modelo com uma transfer√™ncia de aprendizado, temos duas op√ß√µes: <br><br><ol><li>  A sa√≠da da qual camada do modelo de origem usar como entrada para o modelo de destino. </li><li>  Quantas camadas do modelo de destino vamos treinar, se houver. </li></ol><br>  O primeiro ponto √© muito significativo.  Dependendo da camada selecionada, obteremos recursos em um n√≠vel mais baixo ou mais alto de abstra√ß√£o como entrada para nossa rede neural. <br><br>  N√£o vamos treinar nenhuma camada do MobileNet.  <code>global_average_pooling2d_1</code> sa√≠da de <code>global_average_pooling2d_1</code> e a passamos como entrada para o nosso pequeno modelo.  Por que eu escolhi essa camada em particular?  Empiricamente.  Fiz alguns testes e essa camada funciona muito bem. <br><br><h3>  Defini√ß√£o do modelo </h3><br>  A tarefa inicial era classificar a imagem em tr√™s classes: m√£o, p√© e outros movimentos.  Primeiro, vamos resolver o problema menor: determinaremos se h√° um golpe de m√£o no quadro ou n√£o.  Este √© um problema t√≠pico de classifica√ß√£o bin√°ria.  Para esse fim, podemos definir o seguinte modelo: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  Esse c√≥digo define um modelo simples, uma camada com <code>1024</code> unidades e ativa√ß√£o da <code>ReLU</code> , bem como uma unidade de sa√≠da que passa pela <code>sigmoid</code> ativa√ß√£o <code>sigmoid</code> .  Este √∫ltimo fornece um n√∫mero de <code>0</code> a <code>1</code> , dependendo da probabilidade de um golpe de m√£o nesse quadro. <br><br>  Por que escolhi <code>1024</code> unidades para o segundo n√≠vel e uma velocidade de treinamento de <code>1e-6</code> ?  Bem, tentei v√°rias op√ß√µes diferentes e vi que essas op√ß√µes funcionam melhor.  O m√©todo Spear n√£o parece ser a melhor abordagem, mas, em grande parte, √© assim que funcionam as configura√ß√µes de hiperpar√¢metro no aprendizado profundo - com base em nossa compreens√£o do modelo, usamos a intui√ß√£o para atualizar par√¢metros ortogonais e verificar empiricamente como o modelo funciona. <br><br>  O m√©todo de <code>compile</code> compila as camadas juntas, preparando o modelo para treinamento e avalia√ß√£o.  Aqui anunciamos que queremos usar o algoritmo de otimiza√ß√£o do <code>adam</code> .  Tamb√©m declaramos que calcularemos a perda (perda) da entropia cruzada e indicamos que queremos avaliar a precis√£o do modelo.  O TensorFlow.js calcula a precis√£o usando a f√≥rmula: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  Se voc√™ transferir o treinamento do modelo MobileNet original, dever√° primeiro fazer o download.  Como n√£o √© pr√°tico treinar nosso modelo em mais de 3.000 imagens em um navegador, usaremos o Node.js e carregaremos a rede neural a partir do arquivo. <br><br>  Fa√ßa o download do MobileNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  O cat√°logo cont√©m o arquivo <code>model.json</code> , que cont√©m a arquitetura do modelo - camadas, ativa√ß√µes etc.  Os arquivos restantes cont√™m par√¢metros do modelo.  Voc√™ pode carregar o modelo de um arquivo usando este c√≥digo: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Observe que no m√©todo <code>loadModel</code> retornamos uma fun√ß√£o que aceita um tensor unidimensional como entrada e retorna <code>mn.infer(input, Layer)</code> .  O m√©todo <code>infer</code> leva um tensor e uma camada como argumentos.  A camada determina de qual camada oculta queremos a sa√≠da.  Se voc√™ abrir <a href="">model.json</a> e <code>global_average_pooling2d_1</code> , encontrar√° esse nome em uma das camadas. <br><br>  Agora voc√™ precisa criar um conjunto de dados para treinar o modelo.  Para fazer isso, devemos passar todas as imagens pelo m√©todo <code>infer</code> no MobileNet e atribuir-lhes r√≥tulos: <code>1</code> para imagens com tra√ßos e <code>0</code> para imagens sem tra√ßos: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  No c√≥digo acima, primeiro lemos os arquivos em diret√≥rios com e sem ocorr√™ncias.  Em seguida, determinamos o tensor unidimensional que cont√©m os r√≥tulos de sa√≠da.  Se tivermos <code>n</code> imagens com tra√ßos e <code>m</code> outras imagens, o tensor ter√° <code>n</code> elementos com o valor 1 e <code>m</code> elementos com o valor 0. <br><br>  Em <code>xs</code> <code>infer</code> resultados da chamada do m√©todo <code>infer</code> para imagens individuais.  Observe que, para cada imagem, chamamos o m√©todo <code>readInput</code> .  Aqui est√° a sua implementa√ß√£o: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> chama primeiro a fun√ß√£o <code>readImage</code> e depois delega sua chamada para <code>imageToInput</code> .  A fun√ß√£o <code>readImage</code> l√™ uma imagem do disco e decodifica jpg do buffer usando o pacote <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">jpeg-js</a> .  Em <code>imageToInput</code> convertemos a imagem em um tensor tridimensional. <br><br>  Como resultado, para cada <code>i</code> de <code>0</code> a <code>TotalImages</code> deve ser <code>ys[i]</code> igual a <code>1</code> se <code>xs[i]</code> corresponde √† imagem com um hit e <code>0</code> caso contr√°rio. <br><br><h1>  Modelo de treinamento </h1><br>  Agora o modelo est√° pronto para o treinamento!  Chame o m√©todo de <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  As chamadas de c√≥digo acima se <code>fit</code> em tr√™s argumentos: <code>xs</code> , ys e o objeto de configura√ß√£o.  No objeto de configura√ß√£o, definimos quantas Eras o modelo, o tamanho do pacote e o retorno de chamada que o TensorFlow.js gerar√° ap√≥s o processamento de cada pacote ser√£o treinados. <br><br>  O tamanho do pacote determina <code>xs</code> e <code>ys</code> para treinar o modelo em uma era.  Para cada era, o TensorFlow.js seleciona um subconjunto de <code>xs</code> e os elementos correspondentes de <code>ys</code> , executa uma distribui√ß√£o direta, recebe a sa√≠da da camada com ativa√ß√£o <code>sigmoid</code> e, com base na perda, executa a otimiza√ß√£o usando o algoritmo <code>adam</code> . <br><br>  Ap√≥s iniciar o script de treinamento, voc√™ ver√° um resultado semelhante ao abaixo: <br><br><pre>  Custo: 0,84212, precis√£o: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 perda = 0,84 Custo: 0,79740, precis√£o: 1,00000
 eta = 0,2 =&gt; --------- acc = perda de 1,00 = 0,80 Custo: 0,81533, precis√£o: 1,00000
 eta = 0,2 ==&gt; -------- acc = 1,00 perda = 0,82 Custo: 0,64303, precis√£o: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 perda = 0,64 Custo: 0,51377, precis√£o: 0,00000
 eta = 0,2 ====&gt; ------ acc = 0,00 perda = 0,51 Custo: 0,46473, precis√£o: 0,50000
 eta = 0,1 =====&gt; ----- acc = 0,50 perda = 0,46 Custo: 0,50872, precis√£o: 0,00000
 eta = 0,1 ======&gt; ---- acc = 0,00 perda = 0,51 Custo: 0,62556, precis√£o: 1,00000
 eta = 0,1 =======&gt; --- acc = perda de 1,00 = 0,63 Custo: 0,65133, precis√£o: 0,50000
 eta = 0,1 ========&gt; - acc = 0,50 perda = 0,65 Custo: 0,63824, precis√£o: 0,50000
 eta = 0,0 ===========&gt;
 293ms 14675us / etapa - acc = 0,60 perda = 0,65
 √âpoca 3/50
 Custo: 0,44661, precis√£o: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 perda = 0,45 Custo: 0,78060, precis√£o: 1,00000
 eta = 0,3 =&gt; --------- acc = perda de 1,00 = 0,78 Custo: 0,79208, precis√£o: 1,00000
 eta = 0,3 ==&gt; -------- acc = 1,00 perda = 0,79 Custo: 0,49072, precis√£o: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 perda = 0,49 Custo: 0,62232, precis√£o: 1,00000
 eta = 0,2 ====&gt; ------ acc = perda de 1,00 = 0,62 Custo: 0,82899, precis√£o: 1,00000
 eta = 0,2 =====&gt; ----- acc = 1,00 perda = 0,83 Custo: 0,67629, precis√£o: 0,50000
 eta = 0,1 ======&gt; ---- acc = 0,50 perda = 0,68 Custo: 0,62621, precis√£o: 0,50000
 eta = 0,1 =======&gt; --- acc = 0,50 perda = 0,63 Custo: 0,46077, precis√£o: 1,00000
 eta = 0,1 ========&gt; - acc = perda de 1,00 = 0,46 Custo: 0,62076, precis√£o: 1,00000
 eta = 0,0 ===========&gt;
 304ms 15221us / etapa - acc = 0,85 perda = 0,63 </pre><br>  Observe como a precis√£o aumenta com o tempo e a perda diminui. <br><br>  No meu conjunto de dados, o modelo ap√≥s o treinamento mostrou uma precis√£o de 92%.  Lembre-se de que a precis√£o pode n√£o ser muito alta devido ao pequeno conjunto de dados de treinamento. <br><br><h1>  Executando o Modelo em um Navegador </h1><br>  Na se√ß√£o anterior, treinamos o modelo de classifica√ß√£o bin√°ria.  Agora execute-o em um navegador e conecte-se ao <a href="">MK.js</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  Existem v√°rias declara√ß√µes no c√≥digo acima: <br><br><ul><li> <code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cont√©m um link para um item </font></font><code>HTML5 video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">na p√°gina</font></font></li><li> <code>Layer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cont√©m o nome da camada do MobileNet a partir da qual queremos obter a sa√≠da e pass√°-la como entrada para o nosso modelo </font></font></li><li> <code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- uma fun√ß√£o que pega uma inst√¢ncia do MobileNet e retorna outra fun√ß√£o. </font><font style="vertical-align: inherit;">A fun√ß√£o retornada aceita entrada e retorna a sa√≠da correspondente da camada MobileNet especificada.</font></font></li><li> <code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">indica o elemento </font></font><code>HTML5 canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que usaremos para extrair quadros do v√≠deo</font></font></li><li> <code>scale</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- outro </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que √© usado para escalar quadros individuais</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depois disso, obtemos o fluxo de v√≠deo da c√¢mera do usu√°rio e o definimos como a fonte do elemento </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A pr√≥xima etapa √© implementar um filtro em escala de cinza que aceite </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e converta seu conte√∫do:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Como pr√≥ximo passo, conectaremos o modelo ao MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No c√≥digo acima, primeiro carregamos o modelo que treinamos acima e baixamos o MobileNet. </font><font style="vertical-align: inherit;">Passamos o MobileNet para o m√©todo </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para calcular a sa√≠da da camada de rede oculta. </font><font style="vertical-align: inherit;">Depois disso, chamamos o m√©todo </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">com duas redes como argumentos.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A parte mais interessante come√ßa no m√©todo </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! Primeiro, executamos um intervalo em que todos </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">chamam uma fun√ß√£o an√¥nima. Nele, o </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">v√≠deo com o quadro atual √© renderizado </font><font style="vertical-align: inherit;">primeiro em cima dele </font><font style="vertical-align: inherit;">. Em seguida, reduzimos o tamanho do quadro </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e aplicamos um filtro em escala de cinza a ele. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O pr√≥ximo passo √© transferir o quadro para o MobileNet, obter a sa√≠da da camada oculta desejada e transferi-la como entrada para o m√©todo do </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nosso modelo. Isso retorna um tensor com um elemento. Usando, </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">obtemos o valor do tensor e o atribu√≠mos a uma constante </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, verificamos: se a probabilidade de um golpe de m√£o exceder </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, chamamos o m√©todo de </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">objeto global </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. O MK.js fornece um objeto global com tr√™s m√©todos:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que pode ser usado para controlar um dos personagens.</font></font><br><br>  Feito!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aqui est√° o resultado! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconhecimento de chute e bra√ßo com classifica√ß√£o N </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Na pr√≥xima se√ß√£o, criaremos um modelo mais inteligente: uma rede neural que reconhece socos, chutes e outras imagens. </font><font style="vertical-align: inherit;">Desta vez, vamos come√ßar preparando o conjunto de treinamento:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como antes, lemos primeiro os cat√°logos com imagens de socos √† m√£o, p√© e outras imagens. Depois disso, diferentemente da √∫ltima vez, formamos o resultado esperado na forma de um tensor bidimensional, e n√£o unidimensional. Se tivermos </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> imagens com um chute, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> imagens com um chute </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ek</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> outras imagens, o tensor </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ter√° </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos com um valor </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos com um valor </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos com um valor </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um vetor de </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos em que existem </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elementos com um valor </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e um elemento com um valor </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, chamamos de vetor unit√°rio (vetor quente). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depois disso, formamos o tensor de entrada</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">empilhamento da sa√≠da de cada imagem do MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aqui voc√™ deve atualizar a defini√ß√£o do modelo:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As √∫nicas duas diferen√ßas em rela√ß√£o ao modelo anterior s√£o: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O n√∫mero de unidades na camada de sa√≠da </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ativa√ß√µes na camada de sa√≠da </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Existem tr√™s unidades na camada de sa√≠da, porque temos tr√™s categorias diferentes de imagens: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Golpe de m√£o </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kick </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Outros </font></font></li></ul><br>       <code>softmax</code> ,         .      ?           : <code>00</code> , <code>01</code> , <code>10</code> .   ,  <code>softmax</code> ,  1,       00,        . <br><br>      <code>500</code>      92%!  ,   ,       . <br><br>   ‚Äî    !          ,    ,        : <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Primeiro chamamos o MobileNet com uma moldura reduzida em tons de cinza, depois transferimos o resultado do nosso modelo treinado. </font><font style="vertical-align: inherit;">O modelo retorna um tensor unidimensional, que convertemos em </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Na pr√≥xima etapa, usamos </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para converter uma matriz digitada em uma matriz JavaScript. </font><font style="vertical-align: inherit;">Em seguida, extra√≠mos as probabilidades de que um tiro com a m√£o, um chute ou nada esteja presente no quadro. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se a probabilidade do terceiro resultado exceder </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, retornamos. </font><font style="vertical-align: inherit;">Caso contr√°rio, se a probabilidade de um chute for maior </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, enviamos um comando de chute ao MK.js. </font><font style="vertical-align: inherit;">Se a probabilidade de um chute for maior </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e maior que a probabilidade de um chute, enviaremos a a√ß√£o de um chute. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em geral, √© tudo! </font><font style="vertical-align: inherit;">O resultado √© mostrado abaixo:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconhecimento de a√ß√£o </font></font></h1><br>         ,     ,    ,      .    ?              :      (back kick). <br><br>     ,           : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br>     ,    : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br>       ,    ? <br><br>          ,     (RNN). , RNN       : <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Processamento de linguagem natural (PNL), em que cada palavra depende de palavras anteriores e subsequentes </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Prever a pr√≥xima p√°gina com base no seu hist√≥rico de navega√ß√£o </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Reconhecimento de quadros </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A implementa√ß√£o desse modelo est√° al√©m do escopo deste artigo, mas vamos examinar um exemplo de arquitetura para ter uma id√©ia de como tudo isso funcionar√° em conjunto. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> O poder da RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O diagrama abaixo mostra o modelo de reconhecimento de a√ß√µes: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pegamos os √∫ltimos </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">quadros do v√≠deo e os transferimos para a CNN. </font><font style="vertical-align: inherit;">A sa√≠da CNN para cada quadro √© transmitida como RNN de entrada. </font><font style="vertical-align: inherit;">Uma rede neural recorrente determinar√° as rela√ß√µes entre quadros individuais e reconhecer√° a que a√ß√£o eles correspondem.</font></font><br><br><h1>  Conclus√£o </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neste artigo, desenvolvemos um modelo de classifica√ß√£o de imagens. Para isso, coletamos um conjunto de dados: extra√≠mos os quadros de v√≠deo e os dividimos manualmente em tr√™s categorias. Em seguida, os dados foram </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aumentados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> adicionando imagens usando o </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">imgaug</font></a><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depois disso, explicamos o que √© a transfer√™ncia de aprendizado e usamos o modelo MobileNet treinado do pacote </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">@ tensorflow-models / mobilenet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para nossos prop√≥sitos </font><font style="vertical-align: inherit;">. Carregamos o MobileNet a partir de um arquivo no processo Node.js e treinamos uma camada densa adicional na qual os dados foram alimentados a partir da camada oculta do MobileNet. Ap√≥s o treinamento, alcan√ßamos uma precis√£o de mais de 90%! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para usar esse modelo em um navegador, o baixamos junto com o MobileNet e come√ßamos a categorizar os quadros da webcam do usu√°rio a cada 100 ms. Conectamos o modelo ao jogo</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e usou a sa√≠da do modelo para controlar um dos caracteres. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, vimos como melhorar o modelo combinando-o com uma rede neural recorrente para reconhecer a√ß√µes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que voc√™ tenha gostado desse pequeno projeto, tanto quanto eu! </font><font style="vertical-align: inherit;">‚Äç</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt428019/">https://habr.com/ru/post/pt428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt428001/index.html">Problemas conhecidos, bugs e corre√ß√µes nas atualiza√ß√µes do Windows 10 para outubro de 2018</a></li>
<li><a href="../pt428003/index.html">Design responsivo: mantendo a forma dos elementos de marca√ß√£o</a></li>
<li><a href="../pt428005/index.html">Tr√™s maneiras eficazes de exacerbar um desastre de rela√ß√µes p√∫blicas</a></li>
<li><a href="../pt428009/index.html">Equifax: um ano ap√≥s o maior vazamento de dados</a></li>
<li><a href="../pt428011/index.html">Can√ß√µes de zumbis espa√ßo</a></li>
<li><a href="../pt428021/index.html">Selos contra a rede neural. Ou selecione e execute uma rede neural para reconhecer objetos no Raspberry Zero</a></li>
<li><a href="../pt428023/index.html">No√ß√µes b√°sicas de seguran√ßa el√©trica no design de dispositivos eletr√¥nicos</a></li>
<li><a href="../pt428025/index.html">Conectando um arquivo de troca (SWAP) no MAC OS X ao usar uma unidade SSD externa como sistema</a></li>
<li><a href="../pt428027/index.html">Como tentei criar um analisador est√°tico GLSL (e o que deu errado)</a></li>
<li><a href="../pt428029/index.html">Eventos digitais em Moscou, de 29 de outubro a 4 de novembro</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>