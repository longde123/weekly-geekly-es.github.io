<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöß üè∞ üë®üèæ‚Äçüé§ Python adalah asisten dalam mencari penerbangan murah bagi mereka yang suka bepergian üõåüèΩ ‚úä ‚ò∫Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Penulis artikel, terjemahan yang kami terbitkan hari ini, mengatakan bahwa tujuannya adalah untuk berbicara tentang mengembangkan scraper web Python m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python adalah asisten dalam mencari penerbangan murah bagi mereka yang suka bepergian</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/451872/">  Penulis artikel, terjemahan yang kami terbitkan hari ini, mengatakan bahwa tujuannya adalah untuk berbicara tentang mengembangkan scraper web Python menggunakan Selenium, yang mencari harga tiket pesawat.  Saat mencari tiket, tanggal fleksibel digunakan (+ - 3 hari relatif terhadap tanggal yang ditentukan).  Scraper menyimpan hasil pencarian dalam file Excel dan mengirim ke orang yang meluncurkannya email dengan informasi umum tentang apa yang berhasil ia temukan.  Tujuan dari proyek ini adalah untuk membantu wisatawan menemukan penawaran terbaik. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/webt/xl/jo/rr/xljorr2xue-q63wegfrfyt5uxu4.jpeg"></a> <br><br>  Jika Anda, ketika berhadapan dengan materi, merasa bahwa Anda tersesat - lihat artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> . <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Apa yang kita cari</font> </h2><br>  Anda bebas menggunakan sistem yang dijelaskan di sini seperti yang Anda inginkan.  Misalnya, saya menggunakannya untuk mencari tur akhir pekan dan tiket ke kota asal saya.  Jika Anda serius menemukan tiket yang menguntungkan, Anda dapat menjalankan skrip di server (server sederhana, untuk 130 rubel sebulan, sangat cocok untuk ini) dan membuatnya berjalan sekali atau dua kali sehari.  Hasil pencarian akan diemailkan kepada Anda.  Selain itu, saya sarankan Anda mengkonfigurasi semuanya sehingga skrip menyimpan file Excel dengan hasil pencarian di folder Dropbox, yang memungkinkan Anda untuk melihat file seperti itu dari mana saja dan kapan saja. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d39/b7b/f3b/d39b7bf3b4f8c11fa9617ae308f01247.png"></div><br>  <i><font color="#999999">Saya belum menemukan tarif dengan kesalahan, tapi saya percaya ini mungkin</font></i> <br><br>  Saat mencari, seperti yang telah dikatakan, "tanggal fleksibel" digunakan, skrip menemukan penawaran yang berada dalam tiga hari dari tanggal yang diberikan.  Meskipun ketika memulai skrip, ia mencari penawaran hanya dalam satu arah, mudah untuk memperbaikinya sehingga dapat mengumpulkan data di beberapa arah penerbangan.  Dengan bantuannya, Anda bahkan dapat mencari tarif yang salah, penemuan semacam itu bisa sangat menarik. <br><br><h2>  <font color="#3AC1EF">Mengapa saya perlu pengikis web lain?</font> </h2><br>  Ketika saya pertama kali mulai menggores web, sejujurnya, itu tidak terlalu menarik.  Saya ingin melakukan lebih banyak proyek di bidang pemodelan prediktif, analisis keuangan, dan, mungkin, di bidang analisis pewarnaan emosional teks.  Tapi ternyata itu sangat menarik - untuk mencari cara membuat program yang mengumpulkan data dari situs web.  Ketika saya mempelajari topik ini, saya menyadari bahwa pengikisan web adalah "mesin" Internet. <br><br>  Anda dapat memutuskan bahwa ini adalah pernyataan yang terlalu berani.  Tetapi pikirkan tentang bagaimana Google memulai dengan scraper web yang Larry Page buat menggunakan Java dan Python.  Googlebots telah meneliti dan menjelajahi Internet, mencoba memberikan jawaban terbaik kepada pengguna mereka untuk pertanyaan mereka.  Scraping web memiliki jumlah aplikasi yang tak terbatas, dan bahkan jika Anda, di bidang Ilmu Data, tertarik pada sesuatu yang lain, maka untuk mendapatkan data untuk dianalisis, Anda akan memerlukan beberapa keterampilan scraping. <br><br>  Beberapa trik yang digunakan di sini saya temukan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">buku yang</a> bagus tentang pengikisan web, yang baru-baru ini saya peroleh.  Di dalamnya Anda dapat menemukan banyak contoh dan ide sederhana tentang aplikasi praktis yang dipelajari.  Selain itu, ada bab yang sangat menarik tentang uji bypass reCaptcha.  Bagi saya, ini adalah berita, karena saya tidak tahu bahwa ada alat khusus dan bahkan seluruh layanan untuk menyelesaikan masalah seperti itu. <br><br><h2>  <font color="#3AC1EF">Apakah Anda suka bepergian ?!</font> </h2><br>  Terhadap pertanyaan sederhana dan agak tidak berbahaya yang diajukan dalam judul bagian ini, orang sering dapat mendengar jawaban positif, diberikan beberapa kisah perjalanan dari orang yang kepadanya ia ditanyai.  Sebagian besar dari kita akan setuju bahwa bepergian adalah cara yang bagus untuk menyelam ke lingkungan budaya baru dan memperluas wawasan kita.  Namun, jika Anda bertanya kepada seseorang tentang apakah dia suka mencari tiket pesawat, saya yakin jawabannya tidak jauh dari positif.  Sebagai soal fakta, di sini Python datang untuk menyelamatkan. <br><br>  Tugas pertama yang perlu kita selesaikan dalam cara menciptakan sistem pencarian informasi tentang tiket pesawat adalah pemilihan platform yang sesuai dengan mana kita akan mengambil informasi.  Solusi untuk masalah ini tidak mudah bagi saya, tetapi pada akhirnya saya memilih layanan Kayak.  Saya mencoba layanan Momondo, Skyscanner, Expedia, dan lainnya, tetapi mekanisme perlindungan terhadap robot pada sumber daya ini tidak bisa ditembus.  Setelah beberapa upaya, di mana, dalam upaya meyakinkan sistem bahwa saya adalah manusia, saya harus berurusan dengan lampu lalu lintas, penyeberangan pejalan kaki dan sepeda, saya memutuskan bahwa Kayak cocok untuk saya yang terbaik, meskipun di sini juga, jika memuat terlalu banyak halaman dalam waktu singkat, cek juga dimulai.  Saya berhasil membuat bot mengirim permintaan ke situs dengan interval 4 hingga 6 jam, dan semuanya bekerja dengan baik.  Kesulitan timbul dari waktu ke waktu ketika bekerja dengan Kayak, tetapi jika Anda mulai terganggu oleh cek, maka Anda perlu mengatasinya secara manual, lalu mulai bot, atau tunggu beberapa jam, dan cek harus berhenti.  Jika perlu, Anda dapat dengan baik mengadaptasi kode untuk platform lain, dan jika Anda melakukannya, Anda dapat melaporkannya di komentar. <br><br>  Jika Anda baru memulai dengan pengikisan web, dan tidak tahu mengapa beberapa situs web kesulitan melakukannya, maka sebelum Anda memulai proyek pertama Anda di area ini, bantulah diri Anda sendiri dan cari kata-kata Google "Etiket kikisan web".  Eksperimen Anda mungkin berakhir lebih cepat daripada yang Anda pikirkan jika Anda terlibat dalam pengikisan web secara tidak masuk akal. <br><br><h2>  <font color="#3AC1EF">Memulai</font> </h2><br>  Berikut ini adalah gambaran umum tentang apa yang akan terjadi dalam kode scraper web kami: <br><br><ul><li>  Impor perpustakaan yang diperlukan. </li><li>  Buka tab Google Chrome. </li><li>  Memanggil fungsi yang meluncurkan bot, melewatinya kota dan tanggal, yang akan digunakan saat mencari tiket. </li><li>  Fungsi ini menerima hasil pencarian pertama, diurutkan berdasarkan kriteria yang paling menarik (terbaik), dan menekan tombol untuk memuat hasil tambahan. </li><li>  Fungsi lain mengumpulkan data dari seluruh halaman dan mengembalikan bingkai data. </li><li>  Dua langkah sebelumnya dilakukan dengan menggunakan tipe sortir berdasarkan harga tiket (murah) dan menurut kecepatan penerbangan (tercepat). </li><li>  Email dikirim ke pengguna skrip yang berisi ringkasan singkat harga tiket (tiket termurah dan harga rata-rata), dan kerangka data dengan informasi yang diurutkan berdasarkan tiga indikator tersebut disimpan sebagai file Excel. </li><li>  Semua tindakan di atas dilakukan dalam satu siklus setelah periode waktu tertentu. </li></ul><br>  Perlu dicatat bahwa setiap proyek Selenium dimulai dengan driver web.  Saya menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Chromedriver</a> , bekerja dengan Google Chrome, tetapi ada opsi lain.  Yang juga populer adalah PhantomJS dan Firefox.  Setelah memuat driver, Anda harus meletakkannya di folder yang sesuai, ini melengkapi persiapan untuk penggunaannya.  Baris pertama skrip kami membuka tab Chrome baru. <br><br>  Ingatlah bahwa, dalam cerita saya, saya tidak mencoba membuka cakrawala baru untuk menemukan penawaran menguntungkan pada tiket pesawat.  Ada banyak teknik yang lebih canggih untuk menemukan penawaran seperti itu.  Saya hanya ingin menawarkan kepada pembaca materi ini cara sederhana namun praktis untuk menyelesaikan masalah ini. <br><br>  Ini kode yang kita bicarakan di atas. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep, strftime <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> randint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> webdriver <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> selenium.webdriver.common.keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Keys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> smtplib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> email.mime.multipart <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MIMEMultipart <span class="hljs-comment"><span class="hljs-comment">#      chromedriver! chromedriver_path = 'C:/{YOUR PATH HERE}/chromedriver_win32/chromedriver.exe' driver = webdriver.Chrome(executable_path=chromedriver_path) #     Chrome sleep(2)</span></span></code> </pre> <br>  Di awal kode, Anda dapat melihat perintah impor paket yang digunakan di seluruh proyek kami.  Jadi, <code>randint</code> digunakan sehingga bot akan "tertidur" selama beberapa detik secara acak sebelum memulai operasi pencarian baru.  Biasanya tidak satu bot bisa melakukannya tanpa itu.  Jika Anda menjalankan kode di atas, jendela Chrome akan terbuka, yang akan digunakan bot untuk bekerja dengan situs. <br><br>  Mari kita lakukan percobaan kecil dan buka situs web kayak.com di jendela terpisah.  Pilih kota dari mana kami akan terbang, dan kota yang ingin kami tuju, serta tanggal penerbangan.  Saat memilih tanggal, kami akan memverifikasi bahwa rentangnya + -3 hari.  Saya menulis kode dengan mempertimbangkan apa yang dihasilkan situs dalam menanggapi permintaan tersebut.  Jika, misalnya, Anda perlu mencari tiket hanya untuk tanggal tertentu, maka sangat mungkin Anda harus memodifikasi kode bot.  Berbicara tentang kode, saya membuat penjelasan yang tepat, tetapi jika Anda merasa bahwa Anda bingung - beri tahu saya. <br><br>  Sekarang klik tombol mulai pencarian dan lihat tautan di bilah alamat.  Seharusnya terlihat seperti tautan yang saya gunakan dalam contoh di bawah ini, di mana variabel <code>kayak</code> yang menyimpan URL dinyatakan, dan metode <code>get</code> driver web digunakan.  Setelah mengklik tombol pencarian, hasilnya akan muncul di halaman. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae6/ac7/1f7/ae6ac71f71c92c6ff76d5dd6a8fcfa25.png"></div><br>  Ketika saya menggunakan perintah <code>get</code> lebih dari dua hingga tiga kali dalam beberapa menit, saya diminta untuk lulus tes menggunakan reCaptcha.  Anda dapat melalui pemeriksaan ini secara manual dan melanjutkan percobaan sampai sistem memutuskan untuk mengatur pemeriksaan baru.  Ketika saya menguji skrip, saya merasa bahwa sesi pencarian pertama selalu berjalan tanpa masalah, jadi jika Anda ingin bereksperimen dengan kode, Anda hanya perlu memeriksanya secara manual dan membiarkan kode untuk dieksekusi menggunakan interval panjang antara sesi pencarian.  Ya, dan jika Anda memikirkannya, seseorang tidak mungkin memerlukan informasi tentang harga tiket yang diterima pada interval 10 menit antara operasi pencarian. <br><br><h2>  <font color="#3AC1EF">Bekerja dengan halaman menggunakan XPath</font> </h2><br>  Jadi, kami membuka jendela dan memuat situs.  Untuk mendapatkan harga dan informasi lainnya, kita perlu menggunakan teknologi XPath atau penyeleksi CSS.  Saya memutuskan untuk memikirkan XPath dan tidak merasa perlu menggunakan penyeleksi CSS, tetapi sangat mungkin untuk bekerja seperti itu.  Bergerak di sekitar halaman menggunakan XPath bisa menjadi tugas yang menakutkan, dan bahkan jika Anda menggunakan metode yang saya jelaskan dalam artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> , yang menggunakan menyalin pengidentifikasi yang sesuai dari kode halaman, saya menyadari bahwa ini, pada kenyataannya, bukan cara terbaik untuk mengakses elemen yang diperlukan.  Ngomong-ngomong, dalam buku <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> Anda dapat menemukan deskripsi yang sangat baik tentang dasar-dasar bekerja dengan halaman menggunakan pemilih XPath dan CSS.  Inilah yang tampak seperti metode driver web yang sesuai. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa6/5c0/5c0/fa65c05c0385c658a4eee0c08a6274ff.png"></div><br>  Jadi, kami terus bekerja pada bot.  Manfaatkan program ini untuk memilih tiket termurah.  Pada gambar berikut, kode pemilih XPath disorot dengan warna merah.  Untuk melihat kode, Anda perlu mengklik kanan pada elemen halaman yang Anda minati dan pilih perintah Inspect di menu yang muncul.  Perintah ini dapat dipanggil untuk elemen halaman yang berbeda, kode yang akan ditampilkan dan disorot di jendela tampilan kode. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c4/eee/9dc/2c4eee9dc28a8ef0ff540e1c01d3eda5.png"></div><br>  <i><font color="#999999">Lihat Kode Halaman</font></i> <br><br>  Untuk mendapatkan konfirmasi atas alasan saya tentang kelemahan menyalin pemilih dari kode, perhatikan fitur-fitur berikut. <br><br>  Inilah yang Anda dapatkan saat menyalin kode: <br><br><pre> <code class="python hljs">//*[@id=<span class="hljs-string"><span class="hljs-string">"wtKI-price_aTab"</span></span>]/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/div/div[<span class="hljs-number"><span class="hljs-number">1</span></span>]/div/span/span</code> </pre> <br>  Untuk menyalin sesuatu yang serupa, Anda perlu mengklik kanan pada bagian kode yang Anda minati dan pilih Salin&gt; Salin XPath dari menu yang muncul. <br><br>  Inilah yang saya gunakan untuk mendefinisikan tombol termurah: <br><br><pre> <code class="python hljs">cheap_results = <span class="hljs-string"><span class="hljs-string">'//a[@data-code = "price"]'</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9c9/4e3/8a4/9c94e38a436fa588ade8a2f92d97aa2d.png"></div><br>  <i><font color="#999999">Salin&gt; Salin Perintah XPath</font></i> <br><br>  Cukup jelas bahwa opsi kedua terlihat jauh lebih sederhana.  Saat menggunakannya, ia mencari elemen a, yang memiliki atribut <code>data-code</code> sama dengan <code>price</code> .  Menggunakan opsi pertama, elemen <code>id</code> dicari yaitu <code>wtKI-price_aTab</code> , dan path XPath ke elemen tersebut terlihat seperti <code>/div[1]/div/div/div[1]/div/span/span</code> .  Permintaan XPath yang mirip dengan halaman akan melakukan trik, tetapi hanya sekali.  Saya dapat mengatakan sekarang bahwa <code>id</code> akan berubah pada saat halaman dibuka.  <code>wtKI</code> karakter <code>wtKI</code> berubah secara dinamis setiap kali halaman dibuka, sebagai hasilnya, kode yang digunakan akan tidak berguna setelah halaman berikutnya dimuat ulang.  Jadi luangkan waktu untuk mencari tahu XPath.  Pengetahuan ini akan membantu Anda dengan baik. <br><br>  Namun, perlu dicatat bahwa menyalin pemilih XPath dapat berguna ketika bekerja dengan situs yang cukup sederhana, dan jika ini cocok untuk Anda, tidak ada yang salah dengan itu. <br><br>  Sekarang mari kita pikirkan apa yang harus dilakukan jika Anda perlu mendapatkan semua hasil pencarian dalam beberapa baris, di dalam daftar.  Sangat sederhana.  Setiap hasil berada di dalam objek dengan kelas <code>resultWrapper</code> .  Mengunduh semua hasil dapat dilakukan dalam satu lingkaran yang menyerupai yang ditunjukkan di bawah ini. <br><br>  Perlu dicatat bahwa jika Anda memahami hal di atas, maka Anda harus dengan mudah memahami sebagian besar kode yang akan kami uraikan.  Dalam perjalanan pekerjaan kode ini, kita beralih ke apa yang kita butuhkan (pada kenyataannya, ini adalah elemen di mana hasilnya dibungkus) menggunakan beberapa mekanisme untuk menunjukkan path (XPath).  Ini dilakukan untuk mendapatkan teks elemen dan menempatkannya di objek dari mana data dapat dibaca (pertama gunakan <code>flight_containers</code> , kemudian <code>flights_list</code> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/3fc/b7f/0b23fcb7f32738fd6012541c40f68b97.png"></div><br>  Tiga baris pertama ditampilkan dan kita dapat dengan jelas melihat semua yang kita butuhkan.  Namun, kami memiliki cara yang lebih menarik untuk mendapatkan informasi.  Kita perlu mengambil data dari setiap elemen secara terpisah. <br><br><h2>  <font color="#3AC1EF">Untuk bekerja!</font> </h2><br>  Paling mudah menulis fungsi untuk memuat hasil tambahan, jadi mari kita mulai dengan itu.  Saya ingin memaksimalkan jumlah penerbangan yang menerima informasi tentang program, dan pada saat yang sama tidak menimbulkan kecurigaan dalam layanan yang mengarah ke verifikasi, jadi saya klik tombol Muat lebih banyak hasil sekali setiap kali halaman ditampilkan.  Dalam kode ini, Anda harus memperhatikan blok <code>try</code> , yang saya tambahkan karena kadang-kadang tombol tidak memuat secara normal.  Jika Anda juga menjumpai ini, beri komentar panggilan ke fungsi ini dalam kode fungsi <code>start_kayak</code> , yang akan kita bahas di bawah ini. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,      def load_more():   try:       more_results = '//a[@class = "moreButton"]'       driver.find_element_by_xpath(more_results).click()       #            ,          print('sleeping.....')       sleep(randint(45,60))   except:       pass</span></span></code> </pre> <br>  Sekarang, setelah analisis panjang fungsi ini (kadang-kadang saya bisa terbawa), kami siap untuk mendeklarasikan fungsi yang akan menangani pengikisan halaman. <br><br>  Saya telah mengumpulkan sebagian besar dari apa yang dibutuhkan dalam fungsi selanjutnya yang disebut <code>page_scrape</code> .  Kadang-kadang data yang dikembalikan tentang tahapan jalan ternyata digabungkan, untuk pemisahan mereka saya menggunakan metode sederhana.  Sebagai contoh, pertama kali saya menggunakan variabel <code>section_a_list</code> dan <code>section_b_list</code> .  Fungsi kami mengembalikan frame data <code>flights_df</code> , ini memungkinkan kami untuk memisahkan hasil yang diperoleh dengan menggunakan metode penyortiran data yang berbeda, dan kemudian menggabungkannya. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">page_scrape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""This function takes care of the scraping part"""</span></span>     xp_sections = <span class="hljs-string"><span class="hljs-string">'//*[@class="section duration"]'</span></span>   sections = driver.find_elements_by_xpath(xp_sections)   sections_list = [value.text <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sections]   section_a_list = sections_list[::<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-comment"><span class="hljs-comment">#          section_b_list = sections_list[1::2]     #     reCaptcha,    - .   #  ,  -   ,     ,       #   if        -   #    ,           #    SystemExit           if section_a_list == []:       raise SystemExit     #     A     B     a_duration = []   a_section_names = []   for n in section_a_list:       #         a_section_names.append(''.join(n.split()[2:5]))       a_duration.append(''.join(n.split()[0:2]))   b_duration = []   b_section_names = []   for n in section_b_list:       #         b_section_names.append(''.join(n.split()[2:5]))       b_duration.append(''.join(n.split()[0:2]))   xp_dates = '//div[@class="section date"]'   dates = driver.find_elements_by_xpath(xp_dates)   dates_list = [value.text for value in dates]   a_date_list = dates_list[::2]   b_date_list = dates_list[1::2]   #      a_day = [value.split()[0] for value in a_date_list]   a_weekday = [value.split()[1] for value in a_date_list]   b_day = [value.split()[0] for value in b_date_list]   b_weekday = [value.split()[1] for value in b_date_list]     #     xp_prices = '//a[@class="booking-link"]/span[@class="price option-text"]'   prices = driver.find_elements_by_xpath(xp_prices)   prices_list = [price.text.replace('$','') for price in prices if price.text != '']   prices_list = list(map(int, prices_list))   # stops -   ,         ,   -     xp_stops = '//div[@class="section stops"]/div[1]'   stops = driver.find_elements_by_xpath(xp_stops)   stops_list = [stop.text[0].replace('n','0') for stop in stops]   a_stop_list = stops_list[::2]   b_stop_list = stops_list[1::2]   xp_stops_cities = '//div[@class="section stops"]/div[2]'   stops_cities = driver.find_elements_by_xpath(xp_stops_cities)   stops_cities_list = [stop.text for stop in stops_cities]   a_stop_name_list = stops_cities_list[::2]   b_stop_name_list = stops_cities_list[1::2]     #   -,          xp_schedule = '//div[@class="section times"]'   schedules = driver.find_elements_by_xpath(xp_schedule)   hours_list = []   carrier_list = []   for schedule in schedules:       hours_list.append(schedule.text.split('\n')[0])       carrier_list.append(schedule.text.split('\n')[1])   #          a  b   a_hours = hours_list[::2]   a_carrier = carrier_list[1::2]   b_hours = hours_list[::2]   b_carrier = carrier_list[1::2]     cols = (['Out Day', 'Out Time', 'Out Weekday', 'Out Airline', 'Out Cities', 'Out Duration', 'Out Stops', 'Out Stop Cities',           'Return Day', 'Return Time', 'Return Weekday', 'Return Airline', 'Return Cities', 'Return Duration', 'Return Stops', 'Return Stop Cities',           'Price'])   flights_df = pd.DataFrame({'Out Day': a_day,                              'Out Weekday': a_weekday,                              'Out Duration': a_duration,                              'Out Cities': a_section_names,                              'Return Day': b_day,                              'Return Weekday': b_weekday,                              'Return Duration': b_duration,                              'Return Cities': b_section_names,                              'Out Stops': a_stop_list,                              'Out Stop Cities': a_stop_name_list,                              'Return Stops': b_stop_list,                              'Return Stop Cities': b_stop_name_list,                              'Out Time': a_hours,                              'Out Airline': a_carrier,                              'Return Time': b_hours,                              'Return Airline': b_carrier,                                                     'Price': prices_list})[cols]     flights_df['timestamp'] = strftime("%Y%m%d-%H%M") #      return flights_df</span></span></code> </pre> <br>  Saya mencoba memberi nama variabel sehingga kode akan menjadi jelas.  Ingat bahwa variabel yang dimulai dengan merujuk pada langkah pertama dari path, dan <code>b</code> ke yang kedua.  Pergi ke fungsi berikutnya. <br><br><h2>  <font color="#3AC1EF">Mekanisme bantu</font> </h2><br>  Sekarang kami memiliki fungsi yang memungkinkan Anda memuat hasil pencarian tambahan dan fungsi untuk memproses hasil ini.  Artikel ini dapat diselesaikan pada ini, karena dua fungsi ini menyediakan semua yang diperlukan untuk memo halaman yang dapat dibuka secara mandiri.  Tetapi kami belum mempertimbangkan beberapa mekanisme tambahan yang dibahas di atas.  Sebagai contoh, ini adalah kode untuk mengirim email dan beberapa hal lainnya.  Semua ini dapat ditemukan di fungsi <code>start_kayak</code> , yang sekarang kita pertimbangkan. <br><br>  Untuk menggunakan fungsi ini, Anda memerlukan informasi tentang kota dan tanggal.  Dengan menggunakan informasi ini, ia membentuk tautan dalam variabel <code>kayak</code> , yang digunakan untuk membuka halaman yang akan berisi hasil pencarian yang diurutkan berdasarkan kecocokan terbaik mereka.  Setelah sesi pengikisan pertama, kami akan bekerja dengan harga pada tabel di bagian atas halaman.  Yaitu, kami menemukan harga tiket minimum dan harga rata-rata.  Semua ini, bersama dengan prediksi yang dikeluarkan oleh situs, akan dikirim melalui email.  Di halaman, tabel yang sesuai harus di sudut kiri atas.  Mengatasi tabel ini, dapat menyebabkan kesalahan saat mencari menggunakan tanggal yang tepat, karena dalam kasus ini tabel tidak ditampilkan pada halaman. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">start_kayak</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(city_from, city_to, date_start, date_end)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-string"><span class="hljs-string">"""City codes - it's the IATA codes!   Date format -  YYYY-MM-DD"""</span></span>     kayak = (<span class="hljs-string"><span class="hljs-string">'https://www.kayak.com/flights/'</span></span> + city_from + <span class="hljs-string"><span class="hljs-string">'-'</span></span> + city_to +            <span class="hljs-string"><span class="hljs-string">'/'</span></span> + date_start + <span class="hljs-string"><span class="hljs-string">'-flexible/'</span></span> + date_end + <span class="hljs-string"><span class="hljs-string">'-flexible?sort=bestflight_a'</span></span>)   driver.get(kayak)   sleep(randint(<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))     <span class="hljs-comment"><span class="hljs-comment">#    ,           try   try:       xp_popup_close = '//button[contains(@id,"dialog-close") and contains(@class,"Button-No-Standard-Style close ")]'       driver.find_elements_by_xpath(xp_popup_close)[5].click()   except Exception as e:       pass   sleep(randint(60,95))   print('loading more.....')  #     load_more()     print('starting first scrape.....')   df_flights_best = page_scrape()   df_flights_best['sort'] = 'best'   sleep(randint(60,80))     #      ,        matrix = driver.find_elements_by_xpath('//*[contains(@id,"FlexMatrixCell")]')   matrix_prices = [price.text.replace('$','') for price in matrix]   matrix_prices = list(map(int, matrix_prices))   matrix_min = min(matrix_prices)   matrix_avg = sum(matrix_prices)/len(matrix_prices)     print('switching to cheapest results.....')   cheap_results = '//a[@data-code = "price"]'   driver.find_element_by_xpath(cheap_results).click()   sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting second scrape.....')   df_flights_cheap = page_scrape()   df_flights_cheap['sort'] = 'cheap'   sleep(randint(60,80))     print('switching to quickest results.....')   quick_results = '//a[@data-code = "duration"]'   driver.find_element_by_xpath(quick_results).click()    sleep(randint(60,90))   print('loading more.....')  #     load_more()     print('starting third scrape.....')   df_flights_fast = page_scrape()   df_flights_fast['sort'] = 'fast'   sleep(randint(60,80))     #     Excel-,         final_df = df_flights_cheap.append(df_flights_best).append(df_flights_fast)   final_df.to_excel('search_backups//{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(strftime("%Y%m%d-%H%M"),                                                                                  city_from, city_to,                                                                                  date_start, date_end), index=False)   print('saved df.....')     #    ,  ,  ,      xp_loading = '//div[contains(@id,"advice")]'   loading = driver.find_element_by_xpath(xp_loading).text   xp_prediction = '//span[@class="info-text"]'   prediction = driver.find_element_by_xpath(xp_prediction).text   print(loading+'\n'+prediction)     #    loading   , , ,        #    -    "Not Sure"   weird = '¬Ø\\_(„ÉÑ)_/¬Ø'   if loading == weird:       loading = 'Not sure'     username = 'YOUREMAIL@hotmail.com'   password = 'YOUR PASSWORD'   server = smtplib.SMTP('smtp.outlook.com', 587)   server.ehlo()   server.starttls()   server.login(username, password)   msg = ('Subject: Flight Scraper\n\n\ Cheapest Flight: {}\nAverage Price: {}\n\nRecommendation: {}\n\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\n'+prediction)))   message = MIMEMultipart()   message['From'] = 'YOUREMAIL@hotmail.com'   message['to'] = 'YOUROTHEREMAIL@domain.com'   server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)   print('sent email.....')</span></span></code> </pre> <br>  Saya menguji skrip ini menggunakan akun Outlook (hotmail.com).  Saya tidak memeriksanya untuk operasi yang benar dari akun Gmail, sistem surat ini sangat populer, tetapi ada banyak opsi yang memungkinkan.  Jika Anda menggunakan akun Hotmail, maka agar semuanya berfungsi, Anda hanya perlu memasukkan data Anda dalam kode. <br><br>  Jika Anda ingin memahami apa yang sebenarnya dilakukan di bagian terpisah dari kode fungsi ini, Anda dapat menyalinnya dan bereksperimen dengannya.  Eksperimen kode adalah satu-satunya cara untuk memahaminya. <br><br><h2>  <font color="#3AC1EF">Sistem siap</font> </h2><br>  Sekarang semua yang kita bicarakan sudah selesai, kita dapat membuat loop sederhana di mana fungsi kita dipanggil.  Skrip meminta pengguna untuk data tentang kota dan tanggal.  Saat menguji dengan restart skrip yang konstan, Anda tidak mungkin ingin memasukkan data ini secara manual setiap kali, sehingga baris yang sesuai, selama durasi pengujian, dapat dikomentari dengan menghapus komentar orang-orang di bawahnya di mana data yang diperlukan untuk skrip dikode dengan keras. <br><br><pre> <code class="python hljs">city_from = input(<span class="hljs-string"><span class="hljs-string">'From which city? '</span></span>) city_to = input(<span class="hljs-string"><span class="hljs-string">'Where to? '</span></span>) date_start = input(<span class="hljs-string"><span class="hljs-string">'Search around which departure date? Please use YYYY-MM-DD format only '</span></span>) date_end = input(<span class="hljs-string"><span class="hljs-string">'Return when? Please use YYYY-MM-DD format only '</span></span>) <span class="hljs-comment"><span class="hljs-comment"># city_from = 'LIS' # city_to = 'SIN' # date_start = '2019-08-21' # date_end = '2019-09-07' for n in range(0,5):   start_kayak(city_from, city_to, date_start, date_end)   print('iteration {} was complete @ {}'.format(n, strftime("%Y%m%d-%H%M")))     #  4    sleep(60*60*4)   print('sleep finished.....')</span></span></code> </pre> <br>  Ini adalah uji coba skrip. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44c/305/023/44c305023996a98a3dec745493dce7a7.png"></div><br>  <i><font color="#999999">Script uji coba</font></i> <br><br><h2>  <font color="#3AC1EF">Ringkasan</font> </h2><br>  Jika Anda sampai pada titik ini - selamat!  Sekarang Anda memiliki scraper web yang berfungsi, meskipun saya sudah melihat banyak cara untuk memperbaikinya.  Misalnya, dapat diintegrasikan dengan Twilio sehingga, alih-alih email, ia mengirim pesan teks.  Anda dapat menggunakan VPN atau yang lainnya untuk secara bersamaan menerima hasil dari beberapa server.  Ada juga masalah berulang dengan memeriksa pengguna situs apakah dia orang, tetapi masalah ini juga bisa diselesaikan.  Bagaimanapun, sekarang Anda memiliki basis yang dapat Anda kembangkan jika diinginkan.  Misalnya, untuk memastikan bahwa file Excel dikirim kepada pengguna sebagai lampiran email. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id451872/">https://habr.com/ru/post/id451872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id451860/index.html">Matematikawan telah menemukan cara sempurna untuk melipatgandakan angka</a></li>
<li><a href="../id451862/index.html">Musikal Petir Joe Diprim: Insinyur otodidak membuat gulungan Tesla untuk hiburan dan penghasilan</a></li>
<li><a href="../id451864/index.html">Kerentanan RCE kritis tingkat EternalBlue terdeteksi di OS Windows</a></li>
<li><a href="../id451866/index.html">Pilih node terdekat di jaringan</a></li>
<li><a href="../id451870/index.html">Fitur C ++ modern yang perlu diketahui oleh semua programmer</a></li>
<li><a href="../id451874/index.html">Tren SEO Teratas di Google</a></li>
<li><a href="../id451876/index.html">Pusat data Frankfurt: Pusat data Telehouse</a></li>
<li><a href="../id451878/index.html">Streaming langsung video stereo ke kacamata VR (Oculus Go)</a></li>
<li><a href="../id451880/index.html">DevPRO'19: pemandangan dari bilik Wrike</a></li>
<li><a href="../id451884/index.html">Tujuh tahun bekerja sebagai pengembang: pelajaran apa yang telah saya pelajari</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>