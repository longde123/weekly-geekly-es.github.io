<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍🎨 🐸 👦🏿 Restaurando fotos usando redes neurais 🏿 📷 👩🏿‍🚒</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá pessoal, trabalho como programador de pesquisa na equipe de visão computacional do Mail.ru Group. Para o Dia da Vitória deste ano, decidimos fazer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Restaurando fotos usando redes neurais</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/453872/"><img src="https://habrastorage.org/getpro/habr/post_images/333/44a/c78/33344ac788b63200841180799417f934.jpg"><br><br>  Olá pessoal, trabalho como programador de pesquisa na equipe de visão computacional do Mail.ru Group.  Para o Dia da Vitória deste ano, decidimos fazer um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto para a restauração de fotografias militares</a> .  O que é restauração de fotos?  Consiste em três etapas: <br><br><ul><li>  encontramos todos os defeitos da imagem: rupturas, arranhões, buracos; <br></li><li>  pinte os defeitos encontrados com base nos valores de pixel ao seu redor; <br></li><li>  colorir a imagem. <br></li></ul><br>  Neste artigo, analisarei cada um dos estágios da restauração em detalhes e mostrarei como e para onde levamos os dados, quais redes aprendemos, o que fizemos e em que etapas pisamos. <br><a name="habracut"></a><br><h1>  Pesquisa de defeitos </h1><br>  Queremos encontrar todos os pixels relacionados a defeitos na foto carregada.  Primeiro, precisamos entender que tipo de fotografias dos anos de guerra as pessoas enviarão.  Nos voltamos para os organizadores do projeto Immortal Regiment, que compartilharam dados conosco.  Depois de analisá-los, percebemos que as pessoas frequentemente enviam retratos, únicos ou em grupo, com um número moderado ou grande de defeitos. <br><br>  Então foi necessário coletar uma amostra de treinamento.  A amostra de treinamento para a tarefa de segmentação é uma imagem e uma máscara na qual todos os defeitos são marcados.  A maneira mais fácil é dar fotos aos marcadores.  Obviamente, as pessoas são boas em encontrar defeitos, mas o problema é que a marcação é um processo muito longo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5d/a19/892/d5da1989220f10bcaac3944d27d64276.png"><br><br>  Pode levar de uma hora a um dia útil inteiro para marcar os pixels relacionados aos defeitos em uma foto, por isso é difícil coletar uma amostra de mais de 100 fotos em poucas semanas.  Portanto, tentamos de alguma forma complementar nossos dados e escrevemos defeitos: tiramos uma foto limpa, aplicamos defeitos artificiais e obtivemos uma máscara nos mostrando quais partes específicas da imagem estavam danificadas.  A parte principal de nossa amostra de treinamento foi de 79 fotos marcadas manualmente, das quais 11 foram transferidas para a amostra de teste. <br><br>  A abordagem mais popular para o problema de segmentação: leve o Unet com um codificador pré-treinado e minimize a quantidade <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>c</mi><mi>e</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-65" x="1193" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi><mi>c</mi><mi>e</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> Bce </script>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entropia cruzada binária</a> ) e <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mi>I</mi><mi>C</mi><mi>E</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.638ex" height="2.057ex" viewBox="0 -780.1 2858 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-44" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-49" x="828" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-43" x="1333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-45" x="2093" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mi>I</mi><mi>C</mi><mi>E</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> DICE </script>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sørensen - coeficiente de dados</a> ). <br><br>  Que problemas surgem com essa abordagem no problema de segmentação de defeitos? <br><br><ul><li>  Mesmo que pareça que há muitos defeitos na foto, que ela está muito suja e muito esfarrapada pelo tempo, a área ocupada por defeitos ainda é muito menor que a parte não danificada da imagem.  Para resolver esse problema, você pode aumentar o peso da classe positiva em <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>c</mi><mi>e</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-65" x="1193" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi><mi>c</mi><mi>e</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> Bce </script>  , e o peso ideal será a proporção entre o número de todos os pixels puros e o número de pixels pertencentes aos defeitos. <br></li><li>  O segundo problema é que, se usarmos o Unet imediatamente com um codificador pré-treinado, por exemplo Albunet-18, perderemos muitas informações posicionais.  A primeira camada de Albunet-18 consiste em uma convolução com um núcleo de 5 e passo igual a dois.  Isso permite que a rede trabalhe rapidamente.  Sacrificamos o tempo de operação da rede para melhor localização de defeitos: removemos o pool máximo após a primeira camada, reduzimos o passo para 1 e reduzimos o núcleo de convolução para 3. <br></li><li>  Se trabalharmos com imagens pequenas, por exemplo, compactando uma imagem para 256 x 256 ou 512 x 512, pequenos defeitos simplesmente desaparecerão devido à interpolação.  Portanto, você precisa trabalhar com uma imagem grande.  Agora, na produção, estamos segmentando defeitos em fotografias de 1024 x 1024. Portanto, era necessário treinar a rede neural em grandes quantidades de imagens grandes.  E por isso, há problemas com o tamanho pequeno do lote em uma placa de vídeo. <br></li><li>  Durante o treinamento, temos cerca de 20 fotos colocadas em um cartão.  Por esse motivo, a estimativa da média e da variação nas camadas BatchNorm é imprecisa.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O BatchNorm no local</a> nos ajuda a resolver esse problema, que primeiro economiza memória e, em segundo lugar, possui uma versão do BatchNorm Sincronizado, que sincroniza as estatísticas entre todos os cartões.  Agora, consideramos a média e a variação não em 20 fotos em um cartão, mas em 80 fotos em 4 cartões.  Isso melhora a convergência de rede. <br></li></ul><br>  No final, aumentando o peso <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mi>c</mi><mi>e</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.854ex" height="2.057ex" viewBox="0 -780.1 1659.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-63" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-65" x="1193" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>B</mi><mi>c</mi><mi>e</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> Bce </script>  Alterando a arquitetura e usando o In-place BatchNorm, começamos a procurar defeitos na foto.  Mas, de maneira barata, você pode se sair ainda melhor adicionando o aumento do tempo de teste.  Podemos executar a rede uma vez na imagem de entrada, espelhá-la e executar a rede novamente, isso pode nos ajudar a encontrar pequenos defeitos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c75/95b/702/c7595b702dcb921dac612b0218ce13e4.png"><br><br>  Como resultado, nossa rede convergiu em quatro GeForce 1080Ti em 18 horas.  A inferência leva 290 ms.  Acontece por tempo suficiente, mas é uma taxa pelo fato de estarmos procurando por pequenos defeitos.  Validação <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi><mi>I</mi><mi>C</mi><mi>E</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.638ex" height="2.057ex" viewBox="0 -780.1 2858 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-44" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-49" x="828" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-43" x="1333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-45" x="2093" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mi>I</mi><mi>C</mi><mi>E</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> DICE </script>  igual a 0,35, e <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi><mi>O</mi><mi>C</mi><mi>A</mi><mi>U</mi><mi>C</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.596ex" height="2.057ex" viewBox="0 -780.1 4562 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-52" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-4F" x="759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-43" x="1523" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-41" x="2283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-55" x="3034" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/company/mailru/blog/453872/&amp;usg=ALkJrhiZ9n-eqNV3K2jf2eeFFTAC-ByBGA#MJMATHI-43" x="3801" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mi>O</mi><mi>C</mi><mi>A</mi><mi>U</mi><mi>C</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> ROCAUC </script>  - 0,93. <br><br><h1>  Restauração de fragmentos </h1><br>  A Unet nos ajudou a resolver esse problema novamente.  Na entrada, demos a ele a imagem original e uma máscara na qual marcamos espaços limpos com unidades e os pixels que queremos pintar com zeros.  Coletamos os dados da seguinte maneira: tiramos da Internet um grande conjunto de dados com imagens, por exemplo, o OpenImagesV4, e adicionamos defeitos artificialmente semelhantes em forma aos encontrados na vida real.  E depois disso, eles treinaram a rede para reparar as peças ausentes. <br><br>  Como podemos modificar o Unet para esta tarefa? <br><br>  Você pode usar Convolução Parcial em vez da convolução usual.  A ideia dela é que, quando reduzimos uma região de uma imagem com um kernel, não levamos em conta os valores de pixel relacionados aos defeitos.  Isso ajuda a tornar a pintura mais precisa.  Um exemplo de um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo da NVIDIA</a> .  Na imagem central, eles usaram Unet com a convolução usual e, à direita - com Convolução Parcial: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5a5/280/a4b/5a5280a4be71695d16ab08af876c5d23.png"><br><br>  Treinamos a rede por 5 dias.  No último dia, congelamos o BatchNorm, o que ajudou a tornar menos perceptíveis as bordas da parte pintada da imagem. <br><br>  A rede processa uma imagem de 512 x 512 em 50 ms.  A validação PSNR é 26.4.  No entanto, as métricas não podem ser confiáveis ​​incondicionalmente nesta tarefa.  Portanto, primeiro executamos vários bons modelos em nossos dados, anonimizamos os resultados e depois votamos nos que mais gostamos.  Então escolhemos o modelo final. <br><br>  Mencionei que adicionamos defeitos artificialmente para limpar imagens.  Ao treinar, você precisa monitorar cuidadosamente o tamanho máximo dos defeitos sobrepostos, porque, com defeitos muito grandes que a rede nunca viu no processo de aprendizado, ela fantasiará muito e fornecerá um resultado absolutamente inaplicável.  Portanto, se você precisar pintar sobre grandes defeitos, aplique também grandes defeitos durante o treinamento. <br><br>  Aqui está um exemplo do algoritmo: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/abd/6cb/c8c/abd6cbc8c05396042bf83c00516b630e.png"><br><br><h1>  Coloração </h1><br>  Nós segmentamos os defeitos e os pintamos, o terceiro passo é a reconstrução da cor.  Deixe-me lembrá-lo de que, entre as fotografias do "Regimento Imortal", existem muitos retratos individuais ou em grupo.  E queríamos que nossa rede funcionasse bem com eles.  Decidimos fazer nossa própria coloração, porque nenhum dos serviços conhecidos por nós retrata retratos de maneira rápida e eficaz. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/015/bce/bf1/015bcebf15ef1ee069fdf08f16e00542.png"><br><br>  O GitHub tem um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">repositório</a> popular para colorir fotos.  Em média, ele faz bem esse trabalho, mas ele tem vários problemas.  Por exemplo, ele gosta de pintar roupas de azul.  Portanto, também a rejeitamos. <br><br>  Então, decidimos fazer uma rede neural para colorir.  A idéia mais óbvia: tire uma imagem em preto e branco e preveja três canais, vermelho, verde e azul.  Mas, de um modo geral, podemos simplificar nosso trabalho.  Podemos trabalhar não com a representação RGB da cor, mas com a representação YCbCr.  O componente Y é o brilho (luma).  A imagem em preto e branco baixada é o canal Y, vamos reutilizá-la.  Restava prever Cb e Cr: Cb é a diferença na cor azul e brilho, e Cr é a diferença na cor vermelha e brilho. <br><br><img src="https://habrastorage.org/webt/k8/ke/l_/k8kel_xrjco6euolh8ypkchjm8g.jpeg"><br><br>  Por que escolhemos a visualização YCbCr?  O olho humano é mais suscetível a mudanças no brilho do que a mudanças de cor.  Portanto, reutilizamos o componente Y (brilho), algo ao qual o olho é inicialmente bem receptivo, e prevemos Cb e Cr, nos quais podemos cometer um pouco mais de erro, já que as pessoas percebem menos cores "falsas".  Esse recurso começou a ser usado ativamente no início da televisão em cores, quando a largura de banda do canal não era suficiente para transmitir todas as cores na íntegra.  A imagem foi transferida para YCbCr, transferida para o componente Y inalterada e Cb e Cr foram compactados duas vezes. <br><br><h1>  Como montar a linha de base </h1><br>  Você pode novamente usar o Unet com um codificador pré-treinado e minimizar a perda de L1 entre o CbCr real e o previsto.  Queremos retratos em cores, portanto, além das fotos do OpenImages, precisamos adicionar fotos específicas à nossa tarefa. <br><br>  Onde posso obter fotografias coloridas de pessoas em uniforme militar?  Existem pessoas na Internet que pintam fotografias antigas como hobby ou por encomenda.  Eles fazem isso com muito cuidado, tentando cumprir totalmente todas as nuances.  Pintando o uniforme, dragonas, medalhas, eles recorrem a materiais de arquivo, para que o resultado de seu trabalho seja confiável.  No total, foram utilizadas 200 fotografias pintadas à mão.  A segunda fonte de dados útil é o site do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Exército Vermelho</a> dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Trabalhadores e Camponeses</a> .  Um de seus criadores foi fotografado em quase todas as variantes possíveis de um uniforme militar durante a Grande Guerra Patriótica. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f72/999/7d9/f729997d9fd02755fd95967ff09d27ca.png"><br><br>  Em algumas fotografias, ele repetiu as poses de pessoas de famosas fotografias de arquivo.  É especialmente bom que ele tenha filmado em um fundo branco. Isso nos permitiu aumentar muito bem os dados, adicionando vários objetos naturais ao fundo.  Também usamos retratos modernos comuns de pessoas, complementando-os com insígnias e outros atributos de roupas de guerra. <br><br>  Nós treinamos o AlbuNet-50 - este é o Unet, no qual o AlbuNet-50 é usado como um codificador.  A rede começou a dar resultados adequados: a pele é rosada, os olhos são azul acinzentados, as alças são amareladas.  Mas o problema é que ela pintou as figuras com manchas.  Isso se deve ao fato de que, do ponto de vista do erro L1, às vezes é mais lucrativo não fazer nada do que tentar prever alguma cor. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/435/4a9/63c/4354a963c4cd2789c434002e44fdda26.png"><br>  <i>Estamos comparando nosso resultado com uma foto de Ground Truth - coloração manual do artista sob o apelido <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Klimbim</a></i> <br><br>  Como resolver este problema?  Precisamos de um discriminador: uma rede neural, para a qual forneceremos imagens à entrada, e dirá quão realista essa imagem parece.  Abaixo, uma fotografia é pintada à mão e a segunda por uma rede neural.  Qual você acha? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/253/86a/abd/25386aabd9080c14daf71b60d9968453.png"><br><br><div class="spoiler">  <b class="spoiler_title">A resposta</b> <div class="spoiler_text">  A foto esquerda é pintada manualmente. <br></div></div><br>  Como discriminador, usamos o discriminador do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Self-Attention GAN</a> .  Trata-se de uma pequena rede convolucional, nas últimas camadas, na qual está embutida a chamada Auto-Atenção.  Ele permite que você "preste atenção" aos detalhes da imagem.  Também usamos normalização espectral.  A explicação exata e a motivação podem ser encontradas no artigo.  Nós treinamos uma rede com uma combinação de perda de L1 e o erro retornado pelo discriminador.  Agora a rede pinta os detalhes da imagem melhor e o fundo é mais consistente.  Outro exemplo: à esquerda é o resultado da rede treinada apenas com perda L1, à direita - com perda L1 e um erro discriminador. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa4/724/ada/aa4724ada4e0bbdc52dbcaadd5bb3fdc.png"><br><br>  Em quatro Geforce 1080Ti, o treinamento levou dois dias.  A rede funcionou em 30 ms na imagem 512 x 512. A validação MSE foi 34.4.  Como no problema da pintura, as métricas não podem ser totalmente confiáveis.  Portanto, selecionamos 6 modelos que tinham as melhores métricas para validação e votamos cegamente no melhor modelo. <br><br>  Depois de lançar o modelo em produção, continuamos os experimentos e chegamos à conclusão de que é melhor minimizar a perda de L1 por pixel, mas a perda de percepção.  Para calculá-lo, é necessário executar a previsão de rede e a foto de origem através da rede VGG-16, pegar os mapas de atributos nas camadas inferiores e compará-los de acordo com o MSE.  Essa abordagem pinta mais áreas e ajuda a obter uma imagem mais colorida. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db9/303/509/db93035094c4906cf82c246151432cbc.jpg"><br><br><h1>  Conclusões e Conclusão </h1><br>  Unet é um modelo legal.  No primeiro problema de segmentação, encontramos um problema no treinamento e no trabalho com imagens de alta resolução; portanto, usamos o In-Place BatchNorm.  Na segunda tarefa (Pintura), em vez da convolução usual, usamos Convolução Parcial, que ajudou a obter melhores resultados.  No problema de coloração da Unet, adicionamos uma pequena rede de discriminadores que multou o gerador por uma imagem com aparência irreal e usou perda perceptiva. <br><br>  A segunda conclusão é que os acessadores são importantes.  E não apenas na etapa de marcar as figuras antes do treinamento, mas também na validação do resultado final, porque em problemas de defeitos ou coloração da pintura, você ainda precisa validar o resultado com a ajuda de uma pessoa.  Damos ao usuário três fotos: a original com os defeitos removidos, colorida com os defeitos removidos e apenas a foto colorida, caso o algoritmo de pesquisa e pintura de defeitos esteja errado. <br><br>  Tiramos algumas fotos do projeto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Álbum Militar</a> e as processamos com nossas redes neurais.  Aqui estão os resultados obtidos: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a51/ade/15e/a51ade15e9cee54ec8269dc1e22df0af.jpg"><br><br>  E <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> você pode vê-los na resolução original e em cada estágio do processamento. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt453872/">https://habr.com/ru/post/pt453872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt453862/index.html">Por que você deve usar o pathlib</a></li>
<li><a href="../pt453864/index.html">Usar um mouse e teclado em consoles é trapaça?</a></li>
<li><a href="../pt453866/index.html">Solicitação de API com ganchos de reação, HOC ou prop de renderização</a></li>
<li><a href="../pt453868/index.html">Mini interruptor sensível ao toque com painel de vidro no nRF52832</a></li>
<li><a href="../pt453870/index.html">Escrevemos o proxy reverso socks5 no PowerShell.</a></li>
<li><a href="../pt453874/index.html">Da roleta russa ao LOTO seguro: como proteger o pessoal do data center</a></li>
<li><a href="../pt453876/index.html">Como no Yandex.Practicum, o front-end desync venceu: um número acrobático com Redux-Saga, postMessage e Jupyter</a></li>
<li><a href="../pt453882/index.html">Um ótimo guia sobre a profissão de arquiteto de soluções (+ lista de links úteis)</a></li>
<li><a href="../pt453884/index.html">Substituição da câmera HYIP ou DSLR?</a></li>
<li><a href="../pt453886/index.html">Trabalhos do programa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>