<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëåüèª ‚ÑπÔ∏è üö± Depuraci√≥n de retrasos en la red en Kubernetes üòó üç≠ ‚ùî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hace un par de a√±os, Kubernetes ya se discuti√≥ en el blog oficial de GitHub. Desde entonces, se ha convertido en la tecnolog√≠a est√°ndar para implement...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Depuraci√≥n de retrasos en la red en Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/477390/"><img src="https://habrastorage.org/getpro/habr/post_images/c82/5b1/413/c825b1413d9c59cf78c51e6e2c8f8049.png"><br><br>  Hace un par de a√±os, Kubernetes <a href="https://github.blog/2017-08-16-kubernetes-at-github/">ya se discuti√≥</a> en el blog oficial de GitHub.  Desde entonces, se ha convertido en la tecnolog√≠a est√°ndar para implementar servicios.  Kubernetes ahora gestiona una parte importante de los servicios internos y p√∫blicos.  A medida que nuestros grupos crecieron y los requisitos de rendimiento se volvieron m√°s estrictos, comenzamos a notar que algunos servicios en Kubernetes muestran espor√°dicamente demoras que no pueden explicarse por la carga de la aplicaci√≥n en s√≠. <br><br>  De hecho, en las aplicaciones, se produce un retraso de red aleatorio de hasta 100 ms o m√°s, lo que conduce a tiempos de espera o reintentos.  Se esperaba que los servicios pudieran responder a solicitudes mucho m√°s r√°pido que 100 ms.  Pero esto no es posible si la conexi√≥n en s√≠ misma toma tanto tiempo.  Por separado, observamos consultas MySQL muy r√°pidas, que se supon√≠a que tomar√≠an milisegundos, y MySQL realmente se manej√≥ en milisegundos, pero desde el punto de vista de la aplicaci√≥n solicitante, la respuesta tom√≥ 100 ms o m√°s. <br><a name="habracut"></a><br>  De inmediato se hizo evidente que el problema solo ocurre cuando se conecta al host Kubernetes, incluso si la llamada proviene de fuera de Kubernetes.  La forma m√°s f√°cil de reproducir el problema es en la prueba <a href="https://github.com/tsenart/vegeta">Vegeta</a> , que se ejecuta desde cualquier host interno, prueba el servicio Kubernetes en un puerto espec√≠fico y registra espor√°dicamente un gran retraso.  En este art√≠culo, veremos c√≥mo logramos localizar la causa de este problema. <br><br><h1>  Eliminar la complejidad innecesaria en la cadena de fallas </h1><br>  Despu√©s de reproducir el mismo ejemplo, quer√≠amos reducir el enfoque del problema y eliminar las capas adicionales de complejidad.  Inicialmente, hab√≠a demasiados elementos en la secuencia entre Vegeta y las vainas en Kubernetes.  Para identificar un problema de red m√°s profundo, debe excluir algunos de ellos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/488/8c1/d29/4888c1d29a8fc1b4a1194c4c3a14c9ff.png"><br><br>  El cliente (Vegeta) crea una conexi√≥n TCP con cualquier nodo en el cl√∫ster.  Kubernetes act√∫a como una red superpuesta (encima de la red del centro de datos existente) que usa <a href="https://en.wikipedia.org/wiki/IP_in_IP">IPIP</a> , es decir, encapsula los paquetes IP de la red superpuesta dentro de los paquetes IP del centro de datos.  Cuando se conecta al primer nodo, la <a href="https://en.wikipedia.org/wiki/Network_address_translation">traducci√≥n de la direcci√≥n de red de Network Address Translation</a> (NAT) se realiza con monitoreo de estado para convertir la direcci√≥n IP y el puerto del host Kubernetes en la direcci√≥n IP y el puerto en la red superpuesta (en particular, el pod con la aplicaci√≥n).  Para los paquetes recibidos, se realiza la secuencia inversa.  Este es un sistema complejo con muchos estados y muchos elementos que se actualizan y cambian constantemente a medida que los servicios se implementan y mueven. <br><br>  La utilidad <code>tcpdump</code> en la prueba Vegeta genera un retraso durante el protocolo de enlace TCP (entre SYN y SYN-ACK).  Para eliminar esta complejidad innecesaria, puede usar <code>hping3</code> para "pings" simples con paquetes SYN.  Compruebe si hay un retraso en el paquete de respuesta y luego restablezca la conexi√≥n.  Podemos filtrar los datos al incluir solo paquetes de m√°s de 100 ms, y obtener una opci√≥n m√°s simple para reproducir el problema que la prueba completa de nivel de red 7 en Vegeta.  Estos son los "pings" del host Kubernetes que utilizan TCP SYN / SYN-ACK en el host "puerto" del servicio (30927) con un intervalo de 10 ms, filtrado por las respuestas m√°s lentas: <br><br> <code>theojulienne@shell ~ $ sudo hping3 172.16.47.27 -S -p 30927 -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1485 win=29200 rtt=127.1 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1486 win=29200 rtt=117.0 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1487 win=29200 rtt=106.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1488 win=29200 rtt=104.1 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=5024 win=29200 rtt=109.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=5231 win=29200 rtt=109.2 ms</code> <br> <br>  Inmediatamente puede hacer la primera observaci√≥n.  Los n√∫meros de serie y los tiempos muestran que no se trata de congesti√≥n de una sola vez.  La demora a menudo se acumula y finalmente se procesa. <br><br>  A continuaci√≥n, queremos averiguar qu√© componentes pueden estar involucrados en la aparici√≥n de congesti√≥n.  ¬øQuiz√°s estos son algunos de los cientos de reglas de iptables en NAT?  ¬øO algunos problemas con el t√∫nel IPIP en la red?  Una forma de verificar esto es verificar cada paso del sistema excluy√©ndolo.  Qu√© sucede si elimina la l√≥gica de NAT y firewall, dejando solo una parte de IPIP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b3/e2a/cff/5b3e2acff2ef9f1f8c7c527356741d92.png"><br><br>  Afortunadamente, Linux facilita el acceso directo a la capa de superposici√≥n de IP si la m√°quina est√° en la misma red: <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 10.125.20.64 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7346 win=0 rtt=127.3 ms <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7347 win=0 rtt=117.3 ms <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7348 win=0 rtt=107.2 ms</code> <br> <br>  A juzgar por los resultados, ¬°el problema a√∫n persiste!  Esto excluye iptables y NAT.  Entonces, ¬øel problema est√° en TCP?  Veamos qu√© tan regular es el ping ICMP: <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 10.125.20.64 --icmp -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=28 ip=10.125.20.64 ttl=64 id=42594 icmp_seq=104 rtt=110.0 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49448 icmp_seq=4022 rtt=141.3 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49449 icmp_seq=4023 rtt=131.3 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49450 icmp_seq=4024 rtt=121.2 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49451 icmp_seq=4025 rtt=111.2 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49452 icmp_seq=4026 rtt=101.1 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50023 icmp_seq=4343 rtt=126.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50024 icmp_seq=4344 rtt=116.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50025 icmp_seq=4345 rtt=106.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=59727 icmp_seq=9836 rtt=106.1 ms</code> <br> <br>  Los resultados muestran que el problema no ha desaparecido.  Tal vez este es un t√∫nel IPIP?  Simplifiquemos la prueba: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/267/ff6/137/267ff613754b99f8cc1bb1d89119206e.png"><br><br>  ¬øSe env√≠an todos los paquetes entre estos dos hosts? <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 172.16.47.27 --icmp -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41127 icmp_seq=12564 rtt=140.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41128 icmp_seq=12565 rtt=130.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41129 icmp_seq=12566 rtt=120.8 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41130 icmp_seq=12567 rtt=110.8 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41131 icmp_seq=12568 rtt=100.7 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9062 icmp_seq=31443 rtt=134.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9063 icmp_seq=31444 rtt=124.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9064 icmp_seq=31445 rtt=114.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9065 icmp_seq=31446 rtt=104.2 ms</code> <br> <br>  Simplificamos la situaci√≥n a dos hosts Kubernetes que se env√≠an cualquier paquete, incluso el ping ICMP.  Todav√≠a ven un retraso si el host objetivo es "malo" (algunos peores que otros). <br><br>  Ahora la √∫ltima pregunta: ¬øpor qu√© la demora solo ocurre en los servidores de nodo kube?  ¬øY sucede cuando kube-node es el emisor o receptor?  Afortunadamente, esto tambi√©n es bastante f√°cil de resolver enviando un paquete desde un host fuera de Kubernetes, pero con el mismo destinatario "conocido malo".  Como puede ver, el problema no ha desaparecido: <br><br> <code>theojulienne@shell ~ $ sudo hping3 172.16.47.27 -p 9876 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=312 win=0 rtt=108.5 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=5903 win=0 rtt=119.4 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=6227 win=0 rtt=139.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=7929 win=0 rtt=131.2 ms</code> <br> <br>  Luego realizamos las mismas solicitudes desde el nodo kube de origen anterior al host externo (que excluye el host original, ya que el ping incluye los componentes RX y TX): <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 172.16.33.44 -p 9876 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> ^C <br> --- 172.16.33.44 hping statistic --- <br> 22352 packets transmitted, 22350 packets received, 1% packet loss <br> round-trip min/avg/max = 0.2/7.6/1010.6 ms</code> <br> <br>  Despu√©s de examinar las capturas de paquetes retrasadas, obtuvimos informaci√≥n adicional.  En particular, que el remitente (abajo) ve este tiempo de espera, pero el receptor (arriba) no lo ve; vea la columna Delta (en segundos): <br><br> <a href=""><img src="https://habrastorage.org/webt/4m/-t/dj/4m-tdjzws9lrhnva3xcxijel7eg.png"></a> <br><br>  Adem√°s, si observa la diferencia en el orden de los paquetes TCP e ICMP (por n√∫meros de serie) en el lado del destinatario, los paquetes ICMP siempre llegan en la misma secuencia en la que se enviaron, pero con un tiempo diferente.  Al mismo tiempo, los paquetes TCP a veces se alternan y algunos se atascan.  En particular, si examinamos los puertos de los paquetes SYN, en el lado del remitente van en orden, pero en el lado del destinatario no. <br><br>  Existe una sutil diferencia en c√≥mo <a href="https://en.wikipedia.org/wiki/Network_address_translation">las tarjetas de red de</a> los servidores modernos (como en nuestro centro de datos) procesan los paquetes que contienen TCP o ICMP.  Cuando llega un paquete, el adaptador de red "lo pasa por la conexi√≥n", es decir, intenta romper las conexiones por turnos y enviar cada cola a un n√∫cleo de procesador separado.  Para TCP, este hash incluye la direcci√≥n IP y el puerto de origen y destino.  En otras palabras, cada conexi√≥n es hash (potencialmente) de manera diferente.  Para ICMP, solo se codifican las direcciones IP, ya que no hay puertos. <br><br>  Otra observaci√≥n nueva: durante este per√≠odo, vemos retrasos en ICMP en todas las comunicaciones entre los dos hosts, pero TCP no.  Esto nos dice que la raz√≥n probablemente se deba al hash de las colas RX: es casi seguro que la congesti√≥n se produce en el procesamiento de paquetes RX, en lugar de enviar respuestas. <br><br>  Esto excluye el env√≠o de paquetes de la lista de posibles razones.  Ahora sabemos que el problema con el procesamiento de paquetes est√° en el lado receptor en algunos servidores de nodo kube. <br><br><h1>  Comprensi√≥n del procesamiento de paquetes en el kernel de Linux </h1><br>  Para entender por qu√© ocurre el problema con el destinatario en algunos servidores de nodo kube, veamos c√≥mo el kernel de Linux maneja los paquetes. <br><br>  Volviendo a la implementaci√≥n tradicional m√°s simple, la tarjeta de red recibe el paquete y env√≠a una <a href="https://en.wikipedia.org/wiki/Interrupt">interrupci√≥n</a> al kernel de Linux, que es el paquete que debe procesarse.  El kernel detiene otra operaci√≥n, cambia el contexto al controlador de interrupciones, procesa el paquete y luego vuelve a las tareas actuales. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a2/3c3/4ee/1a23c34eea2236294913fd09a25aa1e4.png"><br><br>  Este cambio de contexto es lento: la latencia puede no haber sido notable en las tarjetas de red de 10 megabytes en la d√©cada de 1990, pero en las tarjetas 10G modernas con un rendimiento m√°ximo de 15 millones de paquetes por segundo, cada n√∫cleo de un peque√±o servidor de ocho n√∫cleos puede interrumpirse millones de veces por segundo. <br><br>  Para no lidiar constantemente con el manejo de interrupciones, hace muchos a√±os Linux agreg√≥ <a href="https://en.wikipedia.org/wiki/New_API">NAPI</a> : una API de red que todos los controladores modernos usan para aumentar el rendimiento a altas velocidades.  A bajas velocidades, el n√∫cleo a√∫n acepta las interrupciones de la tarjeta de red de la manera anterior.  Tan pronto como llega un n√∫mero suficiente de paquetes que excede el umbral, el n√∫cleo deshabilita las interrupciones y en su lugar comienza a sondear el adaptador de red y a tomar paquetes en lotes.  El procesamiento se realiza en softirq, es decir, en el <a href="https://www.kernel.org/doc/htmldocs/kernel-hacking/basics-softirqs.html">contexto de las interrupciones de software</a> despu√©s de las llamadas al sistema y las interrupciones de hardware cuando el n√∫cleo (a diferencia del espacio del usuario) ya se est√° ejecutando. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/22a/50d/ee1/22a50dee1fffdbc20614db2b1db28fc4.png"><br><br>  Esto es mucho m√°s r√°pido, pero causa un problema diferente.  Si hay demasiados paquetes, todo el tiempo que lleva procesar paquetes desde la tarjeta de red y los procesos de espacio de usuario no tienen tiempo para vaciar realmente estas colas (lectura de conexiones TCP, etc.).  Al final, las colas se llenan y comenzamos a soltar paquetes.  Intentando encontrar un equilibrio, el n√∫cleo establece un presupuesto para el n√∫mero m√°ximo de paquetes procesados ‚Äã‚Äãen el contexto softirq.  Una vez que se supera este presupuesto, se <code>ksoftirqd</code> subproceso <code>ksoftirqd</code> separado (ver√° uno de ellos en <code>ps</code> para cada n√∫cleo), que procesa estos softirqs fuera de la ruta normal de llamada / interrupci√≥n.  Este hilo se planifica utilizando un planificador de procesos est√°ndar que intenta distribuir los recursos de manera justa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d0f/1e6/f0c/d0f1e6f0c54d45c24d62cb2bcf90c674.png"><br><br>  Despu√©s de examinar c√≥mo procesa los paquetes el n√∫cleo, puede ver que existe una cierta probabilidad de congesti√≥n.  Si las llamadas de softirq se reciben con menos frecuencia, los paquetes tendr√°n que esperar un tiempo para procesarse en la cola RX en la tarjeta de red.  Quiz√°s esto se deba a que alguna tarea bloquea el n√∫cleo del procesador, o algo m√°s impide que el n√∫cleo inicie softirq. <br><br><h1>  Limitamos el procesamiento al n√∫cleo o m√©todo </h1><br>  Los retrasos de Softirq son solo una suposici√≥n.  Pero tiene sentido, y sabemos que estamos viendo algo muy similar.  Por lo tanto, el siguiente paso es confirmar esta teor√≠a.  Y si se confirma, busque el motivo de los retrasos. <br><br>  De vuelta a nuestros paquetes lentos: <br><br> <code>len=46 ip=172.16.53.32 ttl=61 id=29573 icmp_seq=1953 rtt=99.3 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29574 icmp_seq=1954 rtt=89.3 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29575 icmp_seq=1955 rtt=79.2 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29576 icmp_seq=1956 rtt=69.1 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29577 icmp_seq=1957 rtt=59.1 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29790 icmp_seq=2070 rtt=75.7 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29791 icmp_seq=2071 rtt=65.6 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29792 icmp_seq=2072 rtt=55.5 ms</code> <br> <br>  Como se discuti√≥ anteriormente, estos paquetes ICMP se combinan en una sola cola NIC RX y son procesados ‚Äã‚Äãpor un solo n√∫cleo de CPU.  Si queremos entender c√≥mo funciona Linux, es √∫til saber d√≥nde (en qu√© n√∫cleo de CPU) y c√≥mo (softirq, ksoftirqd) se procesan estos paquetes para rastrear el proceso. <br><br>  Ahora es el momento de usar herramientas que permitan el monitoreo en tiempo real del kernel de Linux.  Aqu√≠ usamos <a href="https://github.com/iovisor/bcc">bcc</a> .  Este kit de herramientas le permite escribir peque√±os programas en C que interceptan funciones arbitrarias en el n√∫cleo y los eventos del b√∫fer en un programa Python en el espacio del usuario que puede procesarlos y devolverle el resultado.  Los ganchos para funciones arbitrarias en el n√∫cleo son complejos, pero la utilidad est√° dise√±ada para la m√°xima seguridad y est√° dise√±ada para rastrear con precisi√≥n los problemas de producci√≥n que no son f√°ciles de reproducir en un entorno de prueba o desarrollo. <br><br>  El plan aqu√≠ es simple: sabemos que el n√∫cleo procesa estos pings ICMP, por lo que <a href="">conectamos la funci√≥n del</a> n√∫cleo <a href="">icmp_echo</a> , que recibe el paquete ICMP de solicitud de eco entrante e inicia el env√≠o de la respuesta ICMP de respuesta de eco.  Podemos identificar el paquete aumentando el n√∫mero icmp_seq, que muestra <code>hping3</code> arriba. <br><br>  El c√≥digo de <a href="https://gist.github.com/theojulienne/9d78a0cb68dbe56f19a2ae6316bc6846">script bcc</a> parece complicado, pero no da tanto miedo como parece.  La funci√≥n <code>icmp_echo</code> pasa <code>struct sk_buff *skb</code> : este es el paquete con la solicitud "echo request".  Podemos rastrearlo, extraer la secuencia <code>echo.sequence</code> (que se asigna a <code>icmp_seq</code> desde hping3 <code></code> ) y enviarla al espacio del usuario.  Tambi√©n es conveniente capturar el nombre / identificador actual del proceso.  A continuaci√≥n se muestran los resultados que vemos directamente durante el procesamiento de paquetes por el n√∫cleo: <br><br><pre>  NOMBRE DEL PROCESO TGID PID ICMP_SEQ
 0 0 swapper / 11,770
 0 0 swapper / 11,771
 0 0 swapper / 11 772
 0 0 swapper / 11 773
 0 0 swapper / 11,774
 20041 20086 prometeo 775
 0 0 swapper / 11,776
 0 0 swapper / 11,777
 0 0 swapper / 11 778
 4512 4542 informe-radios-s 779 </pre><br>  Cabe se√±alar aqu√≠ que en el contexto de <code>softirq</code> procesos que hacen que las llamadas al sistema aparezcan como "procesos", aunque en realidad este n√∫cleo procesa paquetes de forma segura en el contexto del n√∫cleo. <br><br>  Con esta herramienta podemos establecer la conexi√≥n de procesos espec√≠ficos con paquetes espec√≠ficos que muestran un retraso en <code>hping3</code> .  Hacemos un <code>grep</code> simple en esta captura para valores espec√≠ficos de <code>icmp_seq</code> .  Los paquetes correspondientes a los valores icmp_seq anteriores se marcaron con su RTT, que observamos anteriormente (entre par√©ntesis se encuentran los valores RTT esperados para los paquetes que filtramos debido a valores RTT inferiores a 50 ms): <br><br><pre>  TGID PID NOMBRE DEL PROCESO ICMP_SEQ ** RTT
 -
 10137 10436 cadvisor 1951
 10137 10436 cadvisor 1952
 76 76 ksoftirqd / 11 1953 ** 99ms
 76 76 ksoftirqd / 11 1954 ** 89ms
 76 76 ksoftirqd / 11 1955 ** 79 ms
 76 76 ksoftirqd / 11 1956 ** 69ms
 76 76 ksoftirqd / 11 1957 ** 59 ms
 76 76 ksoftirqd / 11 1958 ** (49 ms)
 76 76 ksoftirqd / 11 1959 ** (39 ms)
 76 76 ksoftirqd / 11 1960 ** (29 ms)
 76 76 ksoftirqd / 11 1961 ** (19 ms)
 76 76 ksoftirqd / 11 1962 ** (9 ms)
 -
 10137 10436 cadvisor 2068
 10137 10436 cadvisor 2069
 76 76 ksoftirqd / 11 2070 ** 75 ms
 76 76 ksoftirqd / 11 2071 ** 65 ms
 76 76 ksoftirqd / 11 2072 ** 55 ms
 76 76 ksoftirqd / 11 2073 ** (45 ms)
 76 76 ksoftirqd / 11 2074 ** (35 ms)
 76 76 ksoftirqd / 11 2075 ** (25 ms)
 76 76 ksoftirqd / 11 2076 ** (15 ms)
 76 76 ksoftirqd / 11 2077 ** (5 ms) </pre><br>  Los resultados nos dicen algunas cosas.  Primero, el contexto <code>ksoftirqd/11</code> maneja todos estos paquetes.  Esto significa que para este par particular de m√°quinas, los paquetes ICMP se procesaron en el n√∫cleo 11 en el extremo receptor.  Tambi√©n vemos que en cada atasco de tr√°fico hay paquetes que se procesan en el contexto de la <code>cadvisor</code> sistema de <code>cadvisor</code> .  Entonces <code>ksoftirqd</code> asume la tarea y cumple con la cola acumulada: exactamente el n√∫mero de paquetes que se acumularon despu√©s del <code>cadvisor</code> . <br><br>  El hecho de que un <code>cadvisor</code> siempre trabaje inmediatamente antes de esto implica su participaci√≥n en el problema.  Ir√≥nicamente, el prop√≥sito de <a href="https://github.com/google/cadvisor">cadvisor</a> es "analizar la utilizaci√≥n de recursos y las caracter√≠sticas de rendimiento de los contenedores en ejecuci√≥n", en lugar de causar este problema de rendimiento. <br><br>  Al igual que con otros aspectos del manejo de contenedores, todas estas son herramientas extremadamente avanzadas de las cuales se pueden esperar problemas de rendimiento en algunas circunstancias imprevistas. <br><br><h1>  ¬øQu√© hace cadvisor que ralentiza la cola de paquetes? </h1><br>  Ahora tenemos una buena comprensi√≥n de c√≥mo ocurre la falla, qu√© proceso la causa y en qu√© CPU.  Vemos que debido al bloqueo duro, el kernel de Linux no tiene tiempo para programar <code>ksoftirqd</code> .  Y vemos que los paquetes se procesan en el contexto de <code>cadvisor</code> .  Es l√≥gico suponer que <code>cadvisor</code> inicia una llamada syscall lenta, despu√©s de lo cual se procesan todos los paquetes acumulados en este momento: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6fd/6fb/970/6fd6fb970f2d27943039910db9b41743.png"><br><br>  Esta es una teor√≠a, pero ¬øc√≥mo probarla?  Lo que podemos hacer es rastrear el funcionamiento del n√∫cleo de la CPU a lo largo de este proceso, encontrar el punto donde se excede el presupuesto por la cantidad de paquetes y se llama a ksoftirqd, y luego mirar un poco antes: qu√© funcion√≥ exactamente en el n√∫cleo de la CPU justo antes de ese momento.  Es como una radiograf√≠a de una CPU cada pocos milisegundos.  Se ver√° m√°s o menos as√≠: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/44a/954/6e8/44a9546e8de19e43cb125eb8a03a8f47.png"><br><br>  Convenientemente, todo esto se puede hacer con las herramientas existentes.  Por ejemplo, el <a href="https://perf.wiki.kernel.org/index.php/Tutorial">registro de rendimiento</a> verifica el n√∫cleo de CPU especificado con la frecuencia indicada y puede generar una programaci√≥n de llamadas a un sistema en ejecuci√≥n, que incluye tanto el espacio de usuario como el kernel de Linux.  Puede tomar este registro y procesarlo utilizando una peque√±a bifurcaci√≥n del programa <a href="https://github.com/brendangregg/FlameGraph">FlameGraph</a> de Brendan Gregg, que conserva el orden de seguimiento de la pila.  Podemos guardar trazas de pila de una l√≠nea cada 1 ms, y luego seleccionar y guardar la muestra durante 100 milisegundos antes de que <code>ksoftirqd</code> entre en la traza: <br><br> <code># record 999 times a second, or every 1ms with some offset so not to align exactly with timers <br> sudo perf record -C 11 -g -F 999 <br> # take that recording and make a simpler stack trace. <br> sudo perf script 2&gt;/dev/null | ./FlameGraph/stackcollapse-perf-ordered.pl | grep ksoftir -B 100</code> <br> <br>  Aqu√≠ est√°n los resultados: <br><br> <code>( ,   ) <br> <br> cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_iter cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages ksoftirqd/11;ret_from_fork;kthread;kthread;smpboot_thread_fn;smpboot_thread_fn;run_ksoftirqd;__do_softirq;net_rx_action;ixgbe_poll;ixgbe_clean_rx_irq;napi_gro_receive;netif_receive_skb_internal;inet_gro_receive;bond_handle_frame;__netif_receive_skb_core;ip_rcv_finish;ip_rcv;ip_forward_finish;ip_forward;ip_finish_output;nf_iterate;ip_output;ip_finish_output2;__dev_queue_xmit;dev_hard_start_xmit;ipip_tunnel_xmit;ip_tunnel_xmit;iptunnel_xmit;ip_local_out;dst_output;__ip_local_out;nf_hook_slow;nf_iterate;nf_conntrack_in;generic_packet;ipt_do_table;set_match_v4;ip_set_test;hash_net4_kadt;ixgbe_xmit_frame_ring;swiotlb_dma_mapping_error;hash_net4_test ksoftirqd/11;ret_from_fork;kthread;kthread;smpboot_thread_fn;smpboot_thread_fn;run_ksoftirqd;__do_softirq;net_rx_action;gro_cell_poll;napi_gro_receive;netif_receive_skb_internal;inet_gro_receive;__netif_receive_skb_core;ip_rcv_finish;ip_rcv;ip_forward_finish;ip_forward;ip_finish_output;nf_iterate;ip_output;ip_finish_output2;__dev_queue_xmit;dev_hard_start_xmit;dev_queue_xmit_nit;packet_rcv;tpacket_rcv;sch_direct_xmit;validate_xmit_skb_list;validate_xmit_skb;netif_skb_features;ixgbe_xmit_frame_ring;swiotlb_dma_mapping_error;__dev_queue_xmit;dev_hard_start_xmit;__bpf_prog_run;__bpf_prog_run</code> <br> <br>  Aqu√≠ hay muchas cosas, pero lo principal es que encontramos la plantilla "cadvisor antes de ksoftirqd" que vimos anteriormente en el trazador ICMP.  ¬øQu√© significa esto? <br><br>  Cada l√≠nea es un rastro de la CPU en un punto particular en el tiempo.  Cada llamada hacia abajo en la pila en una l√≠nea est√° separada por un punto y coma.  En el medio de las l√≠neas vemos syscall llamado: <code>read(): .... ;do_syscall_64;sys_read; ...</code>  <code>read(): .... ;do_syscall_64;sys_read; ...</code>  Por lo tanto, cadvisor pasa mucho tiempo en la llamada al sistema <code>read()</code> , relacionada con las funciones <code>mem_cgroup_*</code> (parte superior de la pila de llamadas / final de l√≠nea). <br><br>  En el seguimiento de llamadas, es inconveniente ver qu√© se est√° leyendo exactamente, as√≠ que ejecute <code>strace</code> y vea qu√© hace el asesor, y encuentre llamadas del sistema de m√°s de 100 ms: <br><br> <code>theojulienne@kube-node-bad ~ $ sudo strace -p 10137 -T -ff 2&gt;&amp;1 | egrep '&lt;0\.[1-9]' <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.156784&gt; <br> [pid 10432] &lt;... futex resumed&gt; ) = 0 &lt;0.258285&gt; <br> [pid 10137] &lt;... futex resumed&gt; ) = 0 &lt;0.678382&gt; <br> [pid 10384] &lt;... futex resumed&gt; ) = 0 &lt;0.762328&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 154234880\nrss 507904\nrss_h"..., 4096) = 658 &lt;0.179438&gt; <br> [pid 10384] &lt;... futex resumed&gt; ) = 0 &lt;0.104614&gt; <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.175936&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 577 &lt;0.228091&gt; <br> [pid 10427] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 577 &lt;0.207334&gt; <br> [pid 10411] &lt;... epoll_ctl resumed&gt; ) = 0 &lt;0.118113&gt; <br> [pid 10382] &lt;... pselect6 resumed&gt; ) = 0 (Timeout) &lt;0.117717&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 154234880\nrss 507904\nrss_h"..., 4096) = 660 &lt;0.159891&gt; <br> [pid 10417] &lt;... futex resumed&gt; ) = 0 &lt;0.917495&gt; <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.208172&gt; <br> [pid 10417] &lt;... futex resumed&gt; ) = 0 &lt;0.190763&gt; <br> [pid 10417] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 576 &lt;0.154442&gt;</code> <br> <br>  Como es de esperar, aqu√≠ vemos llamadas de <code>read()</code> lenta <code>read()</code> .  A partir del contenido de las operaciones de lectura y el contexto <code>mem_cgroup</code> , <code>mem_cgroup</code> puede ver que estas llamadas <code>read()</code> se refieren al archivo <code>memory.stat</code> , que muestra el uso de la memoria y las limitaciones de cgroup (tecnolog√≠a de aislamiento de recursos de Docker).  La herramienta cadvisor sondea este archivo para obtener informaci√≥n sobre el uso de recursos para contenedores.  Veamos si este n√∫cleo o asesor hace algo inesperado: <br><br> <code>theojulienne@kube-node-bad ~ $ time cat /sys/fs/cgroup/memory/memory.stat &gt;/dev/null <br> <br> real 0m0.153s <br> user 0m0.000s <br> sys 0m0.152s <br> theojulienne@kube-node-bad ~ $</code> <br> <br>  Ahora podemos reproducir el error y comprender que el kernel de Linux se enfrenta a una patolog√≠a. <br><br><h1>  ¬øQu√© hace que leer sea tan lento? </h1><br>  En este punto, es mucho m√°s f√°cil encontrar mensajes de otros usuarios sobre problemas similares.  Al final result√≥ que, en el rastreador de cadvisor, este error se inform√≥ como un <a href="https://github.com/google/cadvisor/issues/1774">problema de uso excesivo de la CPU</a> , pero nadie not√≥ que el retraso tambi√©n se reflejaba al azar en la pila de la red.  De hecho, se not√≥ que cadvisor consume m√°s tiempo de procesador de lo esperado, pero esto no se le dio mucha importancia, ya que nuestros servidores tienen muchos recursos de procesador, por lo que no estudiamos cuidadosamente el problema. <br><br>  El problema es que los grupos de control (cgroups) tienen en cuenta el uso de memoria dentro del espacio de nombres (contenedor).  Cuando todos los procesos en este cgroup finalizan, Docker libera un grupo de control de memoria.  Sin embargo, "memoria" no es solo una memoria de proceso.  Aunque la memoria de proceso en s√≠ ya no se usa, resulta que el n√∫cleo tambi√©n asigna contenido en cach√©, como dentries e inodes (directorio y metadatos de archivos), que se almacenan en cach√© en cgroup de memoria.  De la descripci√≥n del problema: <br><br><blockquote>  cgroups zombies: grupos de control en los que no hay procesos y se eliminan, pero para los cuales todav√≠a se asigna memoria (en mi caso, desde el cach√© de dentry, pero tambi√©n se puede asignar desde el cach√© de p√°gina o tmpfs). </blockquote><br>  Verificar por el n√∫cleo todas las p√°ginas en el cach√© cuando se libera cgroup puede ser muy lento, por lo que se elige el proceso diferido: espere hasta que estas p√°ginas se soliciten nuevamente, e incluso cuando la memoria sea realmente necesaria, finalmente borre cgroup.  Hasta ahora, cgroup todav√≠a se tiene en cuenta al recopilar estad√≠sticas. <br><br>  En t√©rminos de rendimiento, sacrificaron memoria por rendimiento: aceleraron la limpieza inicial debido al hecho de que queda un poco de memoria en cach√©.  Esto es normal  Cuando el kernel usa la √∫ltima parte de la memoria en cach√©, cgroup finalmente se borra, por lo que esto no se puede llamar una "fuga".  Desafortunadamente, la implementaci√≥n espec√≠fica del mecanismo de b√∫squeda <code>memory.stat</code> en esta versi√≥n del kernel (4.9), combinada con la gran cantidad de memoria en nuestros servidores, lleva al hecho de que lleva mucho m√°s tiempo restaurar los √∫ltimos datos almacenados en cach√© y eliminar zombies de cgroup. <br><br>  Resulta que hab√≠a tantos zombis cgroup en algunos de nuestros nodos que la lectura y la latencia excedieron un segundo. <br><br>  Una soluci√≥n alternativa para el problema de cadvisor es borrar inmediatamente las cach√©s de dentries / inodes en todo el sistema, lo que elimina inmediatamente la latencia de lectura y la latencia de red en el host, ya que la eliminaci√≥n de la cach√© incluye p√°ginas en cach√© cgroup zombie, y tambi√©n se liberan.  Esta no es una soluci√≥n, pero confirma la causa del problema. <br><br>  Result√≥ que las versiones m√°s nuevas del kernel (4.19+) mejoraron el rendimiento de la llamada <code>memory.stat</code> , por lo que cambiar a este kernel solucion√≥ el problema.  Al mismo tiempo, ten√≠amos herramientas para detectar nodos problem√°ticos en los grupos de Kubernetes, dren√°ndolos con gracia y reiniciando.  Revisamos todos los grupos, encontramos los nodos con un retraso suficientemente alto y los reiniciamos.  Esto nos dio tiempo para actualizar el sistema operativo en el resto de los servidores. <br><br><h1>  Para resumir </h1><br>  Dado que este error detuvo el procesamiento de las colas NIC RX durante cientos de milisegundos, simult√°neamente caus√≥ un gran retraso en las conexiones cortas y un retraso en el medio de la conexi√≥n, por ejemplo, entre las consultas MySQL y los paquetes de respuesta. <br><br>       ,   Kubernetes,            .    Kubernetes    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/477390/">https://habr.com/ru/post/477390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../477378/index.html">√Ågil mixto: enfoque de cascada al implementar aplicaciones comerciales (tambi√©n conocido como Agile-like)</a></li>
<li><a href="../477382/index.html">Esports: obtener ganancias: Mercedes, meg√°fono, apuestas y branding para esports</a></li>
<li><a href="../477384/index.html">Conferencia ‚ÄúSeguridad de la informaci√≥n. Amenazas del presente y del futuro ‚Äù</a></li>
<li><a href="../477386/index.html">Semana de la seguridad 48: fuga de datos gigantesca y vulnerabilidad de Whatsapp</a></li>
<li><a href="../477388/index.html">NILFS2 - sistema de archivos a prueba de balas para / home</a></li>
<li><a href="../477392/index.html">Micr√≥fono abierto: backend. Invitamos oradores</a></li>
<li><a href="../477396/index.html">C√≥mo inscribirse en un curso y ... ir al final</a></li>
<li><a href="../477400/index.html">Sobre la profesi√≥n de gerente de producto: ¬øc√≥mo lograr el ideal?</a></li>
<li><a href="../477402/index.html">Implementaci√≥n de Keras Deep Learning Model como una aplicaci√≥n web de Python</a></li>
<li><a href="../477404/index.html">El problema de crear y eliminar objetos con frecuencia en C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>