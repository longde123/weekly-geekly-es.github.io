<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∫ üàπ üëèüèº Weiterentwicklung der Datenbankinfrastruktur: Von Datenbank und Anwendung auf einem Server zur Streaming-Replikation üë™ üôáüèæ üëÇüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! 

 Mein Name ist Anton Markelov, ich bin Ops-Ingenieur bei United Traders. Wir sind auf die eine oder andere Weise an Projekten beteiligt,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Weiterentwicklung der Datenbankinfrastruktur: Von Datenbank und Anwendung auf einem Server zur Streaming-Replikation</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/utex/blog/443530/"><img src="https://habrastorage.org/webt/9o/uy/bk/9ouybky85gzjccvemkc882t32ws.png"><br><br>  Hallo Habr! <br><br>  Mein Name ist Anton Markelov, ich bin Ops-Ingenieur bei United Traders.  Wir sind auf die eine oder andere Weise an Projekten beteiligt, die mit Investitionen, B√∂rsen und anderen finanziellen Angelegenheiten verbunden sind.  Wir sind kein sehr gro√ües Unternehmen, etwa 30 Entwicklungsingenieure, die Waage ist angemessen - etwas weniger als hundert Server.  W√§hrend des quantitativen und qualitativen Wachstums unserer Infrastruktur hat die klassische L√∂sung ‚ÄûWir halten sowohl die Anwendung als auch ihre Datenbank auf demselben Server‚Äú in Bezug auf Zuverl√§ssigkeit und Geschwindigkeit nicht mehr zu uns passen.  Seitens der Analysten mussten datenbank√ºbergreifende Abfragen erstellt werden. Die Betriebsabteilung hatte es satt, mit der Sicherung und √úberwachung einer gro√üen Anzahl von Datenbankservern herumzuspielen.  Dar√ºber hinaus hat das Speichern des Status auf demselben Computer wie die Anwendung selbst die Flexibilit√§t der Ressourcenplanung und die Ausfallsicherheit der Infrastruktur erheblich verringert. <br><br>  Der √úbergang zur aktuellen Architektur war evolution√§r. Verschiedene L√∂sungen wurden getestet, um Entwicklern und Analysten eine bequeme Schnittstelle zu bieten und die Zuverl√§ssigkeit und Verwaltbarkeit dieser gesamten Wirtschaft zu erh√∂hen.  Ich m√∂chte √ºber die Hauptphasen der Modernisierung unseres DBMS sprechen, zu welchem ‚Äã‚ÄãRechen wir gekommen sind und zu welchen Entscheidungen wir als Ergebnis einer fehlertoleranten unabh√§ngigen Umgebung gekommen sind, die Betriebsingenieuren, Entwicklern und Analysten bequeme Interaktionsm√∂glichkeiten bietet.  Ich hoffe, dass unsere Erfahrung Ingenieuren aus Unternehmen unserer Gr√∂√üenordnung von Nutzen sein wird. <br><br>  Dieser Artikel ist eine Zusammenfassung meines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berichts</a> auf der UPTIMEDAY-Konferenz. Vielleicht ist das Videoformat f√ºr jemanden angenehmer, obwohl der Autor mit meinen H√§nden etwas besser ist als ein Mundsprecher. <br><br>  Der ‚ÄûSchneeflockenmann‚Äú mit KDPV wurde schamlos von Maxim Dorofeev <i>entlehnt</i> . <br><a name="habracut"></a><br><h2>  Wachstumskrankheiten </h2><br>  Wir haben eine Microservice-Architektur, Services werden haupts√§chlich in Java oder Kotlin unter Verwendung des Spring-Frameworks geschrieben.  Neben jedem Microservice befindet sich eine PostgreSQL-Basis. Alles wird von Nginx abgedeckt, um den Zugriff zu erm√∂glichen.  Ein typischer Mikroservice ist eine Anwendung in Spring Boot, die ihre Daten in PostgreSQL (gleichzeitig Teil der Anwendungen und in ClickHouse) schreibt, √ºber Kafka mit Nachbarn kommuniziert und √ºber einige REST- oder GraphQL-Endpunkte f√ºr die Kommunikation mit der Au√üenwelt verf√ºgt. <br><br><img src="https://habrastorage.org/webt/43/s1/aq/43s1aq-9ggmakbk3dygl8ox9bna.png"><br><br>  Fr√ºher, als wir noch sehr klein waren, hatten wir nur mehrere Server in DigitalOcean, Kafka war noch nicht da, die gesamte Kommunikation erfolgte √ºber REST.  Das hei√üt, wir haben ein Droplet genommen, dort Java, PostgreSQL und Nginx installiert und dort Zabbix gestartet, um die Serverressourcen und die Verf√ºgbarkeit von Service-Endpunkten zu √ºberwachen.  Sie stellten alles mit Hilfe von Ansible bereit, wir hatten standardisierte Playbooks, vier bis f√ºnf Rollen rollten den gesamten Service aus.  Solange wir relativ gesehen 6 Server in der Produktion und 3 im Test hatten, konnte man irgendwie damit leben. <br>  Dann begann die aktive Entwicklungsphase, die Anzahl der Anwendungen wuchs, zehn Microservices wurden zu vierzig, ihre Funktionalit√§t begann sich zu √§ndern, und die Integration mit externen Systemen wie CRM, Kundenstandorten und dergleichen erschien.  Wir haben den ersten Schmerz.  Einige Anwendungen verbrauchten mehr Ressourcen, drangen nicht mehr in vorhandene Server ein, wir bekamen Tr√∂pfchen, zogen Anwendungen hin und her und w√§hlten viele H√§nde.  Es tat ziemlich weh - niemand mag dumme mechanische Arbeit - ich wollte mich schnell entscheiden.  Also haben wir uns auf den Weg gemacht - wir haben nur 3 gro√üe dedizierte Server anstelle von 10 Cloud-Tr√∂pfchen genommen.  Dies schloss das Problem f√ºr eine Weile, aber es wurde offensichtlich, dass es Zeit war, Optionen f√ºr eine Art Orchestrierung und Server-Rebalancing auszuarbeiten.  Wir haben begonnen, L√∂sungen wie DC / OS und Kubernetes genau zu untersuchen und unser Fachwissen in diesem Bereich schrittweise zu erweitern. <br><br>  Etwa zur gleichen Zeit hatten wir eine analytische Abteilung, die regelm√§√üig schwierige Anfragen stellen, Berichte erstellen und sch√∂ne Dashboards erstellen musste, was uns einen zweiten Schmerz bereitete.  Erstens haben Analysten die Basis stark belastet, und zweitens ben√∂tigten sie datenbank√ºbergreifende Abfragen, weil  Jeder Mikroservice behielt eine ziemlich enge Datenscheibe.  Wir haben mehrere Systeme getestet. Zuerst haben wir versucht, alles durch Replikation auf Tabellenebene zu l√∂sen (es war wieder im neunten PostgreSQL, es gab keine sofort einsatzbereite logische Replikation), aber die daraus resultierenden Handwerke, die auf pglogical, Presto, Slony-I und Bucardo basierten, haben dies nicht vollst√§ndig getan arrangiert.  Zum Beispiel hat pglogical die Migration nicht unterst√ºtzt - eine neue Version des Microservices wurde eingef√ºhrt, die Struktur der Datenbank wurde ge√§ndert, Java selbst hat die Struktur mithilfe von Flyway ge√§ndert, und bei Replikaten in pglogical muss alles manuell ge√§ndert werden.  Ansonsten fehlte entweder etwas oder es war zu schwierig. <br><br><h2>  Super Sklave </h2><br>  Als Ergebnis der Forschung wurde eine einfache und brutale L√∂sung namens Superslave geboren: Wir nahmen einen separaten Server, konfigurierten darauf einen Slave f√ºr jeden Produktionsserver an verschiedenen Ports und erstellten eine virtuelle Datenbank, die die Datenbanken der Slaves √ºber postgres_fdw (Foreign Data Wrapper) kombiniert.  Das hei√üt, all dies wurde einfach und zuverl√§ssig mit Standard-Postgres implementiert, ohne dass zus√§tzliche Entit√§ten eingef√ºhrt wurden: Mit einer einzigen Anfrage konnten Daten aus mehreren Datenbanken abgerufen werden.  Wir haben diese virtuelle Basis an Analysten weitergegeben.  Ein zus√§tzliches Plus ist, dass das schreibgesch√ºtzte Replikat selbst bei einem Fehler mit Zugriffsrechten dort nichts schreiben konnte. <br><br><img src="https://habrastorage.org/webt/uf/j7/2i/ufj72i581hu3b_s5cep3cy_dk60.png"><br><br>  Wir haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redash</a> zur Visualisierung verwendet, es wei√ü, wie man Diagramme zeichnet, Anforderungen nach einem Zeitplan ausf√ºhrt, zum Beispiel einmal am Tag, und verf√ºgt √ºber ein gewichtiges System von Rechten, sodass wir Analysten und Entwickler dorthin gehen lassen. <br><br><img src="https://habrastorage.org/webt/wq/jo/gj/wqjogjc5ovjcfxz9b6nqa_g71nm.png"><br><br>  Parallel dazu setzte sich das Wachstum fort, Kafka erschien in der Infrastruktur als Bus und ClickHouse f√ºr Analytics-Speicher.  Sie sind leicht aus der Schachtel zu gruppieren, unser Supersklave vor ihrem Hintergrund sah aus wie ein ungeschicktes Fossil.  Au√üerdem blieb PostgreSQL der einzige Status, der nach der Anwendung gezogen werden musste (wenn er noch auf einen anderen Server √ºbertragen werden musste), und wir wollten unbedingt eine zustandslose Anwendung, um eng mit Kubernetes und ihm zu experimentieren √§hnliche Plattformen. <br><br>  Wir haben nach einer L√∂sung gesucht, die die folgenden Anforderungen erf√ºllt: <br><br><ul><li>  Fehlertoleranz: Wenn N Server ausfallen, funktioniert der Cluster weiter. </li><li>  F√ºr Anwendungen sollte alles wie zuvor bleiben, keine √Ñnderungen im Code; </li><li>  einfache Bereitstellung und Verwaltung; </li><li>  weniger Abstraktionsebenen gegen√ºber regul√§rem PostgreSQL; </li><li>  Im Idealfall sollte der Lastenausgleich so erfolgen, dass nicht alle Anforderungen an einen Server gesendet werden. </li><li>  Idealerweise ist es in einer vertrauten Sprache geschrieben. </li></ul><br>  Es gab nicht viele Kandidaten: <br><br><ul><li>  Standard-Streaming-Replikation (repmgr, Patroni, Stolon); </li><li>  Trigger-basierte Replikation (Londiste, Slony); </li><li>  Abfrage-Replikation der mittleren Schicht (pgpool-II); </li><li>  synchrone Replikation mit mehreren Kernservern (Bucardo). </li></ul><br>  Zum gro√üen Teil hatten wir bereits schlechte Erfahrungen beim Bau der Querbasis, so dass Patroni und Stolon blieben.  Patroni ist in Python geschrieben, Stolon in Go, wir haben genug Erfahrung in beiden Sprachen.  Dar√ºber hinaus haben sie eine √§hnliche Architektur und Funktionalit√§t, sodass die Wahl aus subjektiven Gr√ºnden getroffen wurde: Patroni wurde von Zalando entwickelt, und wir haben einmal versucht, mit ihrem Nakadi-Projekt (REST-API f√ºr Kafka) zu arbeiten, bei dem wir auf einen schwerwiegenden Mangel an Dokumentation stie√üen. <br><br><h2>  Stolon </h2><br><img src="https://habrastorage.org/webt/z5/af/hf/z5afhfobk43vpt0x2dmryf0gt50.png"><br><br>  Die Architektur von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stolon ist</a> recht einfach: Es gibt N Server, mit Hilfe von etcd / consul wird ein Leader ausgew√§hlt, PostgreSQL wird darin im Assistentenmodus gestartet und auf andere Server repliziert.  Dann gehen Stolon-Proxys zu diesem PostgreSQL-Master und geben vor, Anwendungen mit gew√∂hnlichen Postgres zu sein, und Clients gehen zu diesen Proxys.  Im Falle des Verschwindens eines Meisters finden Wiederwahlen statt, jemand anderes wird Meister, der Rest wird bereit.  Es gibt nur wenige Abstraktionsebenen. PostgreSQL wird wie gewohnt installiert. Die einzige Einschr√§nkung besteht darin, dass die PostgreSQL-Konfiguration in etcd gespeichert ist und etwas anders konfiguriert ist. <br><br>  Beim Testen des Clusters sind einige Probleme aufgetreten: <br><br><ul><li>  Stolon wei√ü nicht, wie man auf ZooKeeper arbeitet, nur Konsul oder etcd; </li><li>  etcd ist sehr empfindlich gegen√ºber E / A.  Wenn Sie PostgreSQL und etcd auf demselben Server behalten, ben√∂tigen Sie auf jeden Fall schnelle SSDs. </li><li>  Selbst auf SSDs m√ºssen usw. Timeouts konfiguriert werden, da sonst unter Last alles kaputt geht. Der Cluster glaubt, dass der Master heruntergefallen ist, und unterbricht st√§ndig die Verbindungen. </li><li>  Standardm√§√üig ist max_connections unter PostgreSQL klein (200). Sie m√ºssen es an Ihre Anforderungen anpassen. </li><li>  Ein Cluster von drei etcd √ºberlebt den Tod von nur einem Server. Idealerweise ben√∂tigen Sie eine Konfiguration, zum Beispiel 5 etcd + 3 Stolon. </li><li>  Nach dem Auspacken gehen alle Verbindungen zum Master, die Slaves sind f√ºr die Verbindung nicht zug√§nglich. </li></ul><br>  Da alle Verbindungen zu PostgreSQL zum Assistenten gehen, sto√üen wir erneut auf ein Problem mit umfangreichen Analyseanforderungen.  etcd reagierte manchmal schmerzhaft auf die hohe Belastung des Masters und schaltete ihn um.  Durch das Umschalten des Assistenten werden immer die Verbindungen unterbrochen.  Die Anfrage wurde neu gestartet, alles begann von vorne.  Zur Umgehung dieses Problems wurde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein Python-Skript</a> geschrieben, das stolonctl-Adressen von Live-Slaves anforderte und eine Konfiguration f√ºr HAProxy generierte, um Anforderungen an diese umzuleiten. <br><br><img src="https://habrastorage.org/webt/k6/er/ds/k6erds-_gyudfmyruuzpmw0oudm.png"><br><br>  Das folgende Bild stellte sich heraus: Anforderungen von Anwendungen gehen an den Stolon-Proxy-Port, der sie an den Master weiterleitet, und Anforderungen von Analysten (sie sind immer schreibgesch√ºtzt) gehen an den HAProxy-Port, der sie an einen Slave weiterleitet. <br><br>  Au√üerdem wurde heute buchst√§blich eine PR bei Stolon verabschiedet, die es erm√∂glichte, Informationen √ºber Stolon-Instanzen an eine Dienstentdeckung eines Drittanbieters zu senden. <br><br><img src="https://habrastorage.org/webt/o-/al/k_/o-alk_xqmlva498z1yav5nbooze.png"><br><br>  Gemessen an den Metriken f√ºr die Reaktionsgeschwindigkeit der Anwendung hatte der √úbergang zu einem Remote-Cluster keinen wesentlichen Einfluss auf die Leistung. Die durchschnittliche Antwortzeit hat sich nicht ge√§ndert.  Die daraus resultierende Netzwerklatenz wurde offenbar dadurch kompensiert, dass sich die Datenbank jetzt auf einem dedizierten Server befindet. <br><br>  Stolon √ºberlebt ohne Probleme einen Assistentenabsturz (Serververlust, Netzwerkverlust, Festplattenverlust), wenn der Server zum Leben erweckt wird - er setzt das Replikat automatisch zur√ºck.  Der schw√§chste Punkt in Stolon ist etcd, Fehler darin setzen den Cluster.  Wir hatten einen typischen Unfall: Eine Gruppe von drei Knoten usw., zwei wurden abgeholzt.  Alles, das Quorum war kaputt, etcd ging in einen ungesunden Zustand, der Stolon-Cluster akzeptiert keine Verbindungen, einschlie√ülich Anfragen von stolonctl.  Wiederherstellungsschema: Verwandeln Sie etcd auf dem √ºberlebenden Server in einen Einzelknotencluster und f√ºgen Sie die Mitglieder wieder hinzu.  Fazit: Um den Tod von zwei Servern zu √ºberleben, m√ºssen mindestens 5 Instanzen usw. vorhanden sein. <br><br><h2>  Fehler √ºberwachen und abfangen </h2><br>  Mit dem Wachstum der Infrastruktur und der Komplexit√§t von Microservices wollte ich mehr Informationen dar√ºber sammeln, was in der Anwendung und auf der Java-Maschine geschieht.  Wir konnten Zabbix nicht an die neue Umgebung anpassen: Dies ist unter den Bedingungen einer sich √§ndernden Infrastruktur sehr unpraktisch.  Ich musste entweder Kr√ºcken durch die API schleifen oder mit meinen H√§nden hineinklettern, was noch schlimmer ist.  Die Datenbank ist schlecht an schwere Lasten angepasst, und im Allgemeinen ist es sehr unpraktisch, all dies in eine relationale Datenbank zu stellen. <br><br>  Aus diesem Grund haben wir Prometheus f√ºr die √úberwachung ausgew√§hlt.  Er hat einen sofort einsatzbereiten Aktuator f√ºr Spring-Anwendungen zur Bereitstellung von Metriken. F√ºr Kafka haben sie JMX Exporter geschraubt, der auch Metriken auf komfortable Weise bereitstellt.  Die Exporteure, die nicht "in der Box" gefunden wurden, haben wir uns in Python geschrieben, es gibt ungef√§hr zehn von ihnen.  Wir visualisieren Grafana, sammeln die Protokolle mit Graylog (da er jetzt Beats unterst√ºtzt). <br><br>  Wir verwenden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sentry</a> , um Fehler zu sammeln.  Er schreibt alles in strukturierter Form, zeichnet Grafiken, zeigt, was √∂fter und seltener passiert ist.  Normalerweise wenden sich Entwickler sofort nach der Bereitstellung an Sentry, um festzustellen, ob es Spitzenwerte gibt oder ob ein Rollback dringend erforderlich ist.  Es stellt sich heraus, dass Fehler schnell erkannt werden, ohne dass die Protokolle ausgew√§hlt werden m√ºssen. <br><br>  Das ist alles f√ºr den Moment, wenn das Format der Artikel den Lesern passt, werden wir weiter √ºber unsere Infrastruktur sprechen, es macht immer noch viel Spa√ü: Kafka und Analysel√∂sungen f√ºr Ereignisse, CI / CD-Kanal f√ºr Windows-Anwendungen und Abenteuer mit Openshift. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443530/">https://habr.com/ru/post/de443530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443520/index.html">W√§hlen Sie ein Auto f√ºr einen IT-Spezialisten oder Tipps f√ºr Teekannen aus einer Teekanne</a></li>
<li><a href="../de443522/index.html">Hosting: Optionen, Vergleiche, Benutzerstatistiken</a></li>
<li><a href="../de443524/index.html">Do-it-yourself-Flash-Animationen in Unity3D. Erster Teil, Lyrisch</a></li>
<li><a href="../de443526/index.html">Von Algorithmen zu Krebs: Vorlesungen an der School of Bioinformatics</a></li>
<li><a href="../de443528/index.html">Amazon hat Open Distro f√ºr Elasticsearch ver√∂ffentlicht</a></li>
<li><a href="../de443532/index.html">5 Merkmale von Metallpulvern f√ºr den 3D-Druck</a></li>
<li><a href="../de443534/index.html">Compute Express Link - Verbindung f√ºr Big Data</a></li>
<li><a href="../de443542/index.html">Kostenloser Check Point Erste Schritte R80.20 Kostenloser Kurs</a></li>
<li><a href="../de443544/index.html">Speicherprofilerstellung auf STM32 und anderen Mikrocontrollern: Statische Stapelgr√∂√üenanalyse</a></li>
<li><a href="../de443546/index.html">Toyota und JAXA planen, 2029 einen bemannten Rover auf dem Mond zu haben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>