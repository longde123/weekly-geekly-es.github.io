<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïâÔ∏è ü§• üõÇ VShard - escala horizontal en Tarantool üññüèø üë®üèΩ‚Äçüîß üê¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola, me llamo Vladislav y soy miembro del equipo de desarrollo de Tarantool . Tarantool es un DBMS y un servidor de aplicaciones todo en uno. Hoy voy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - escala horizontal en Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442782/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Hola, me llamo Vladislav y soy miembro del equipo de desarrollo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tarantool</a> .  Tarantool es un DBMS y un servidor de aplicaciones todo en uno.  Hoy voy a contar la historia de c√≥mo implementamos el escalado horizontal en Tarantool mediante el m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VShard</a> . <br><br>  Algunos conocimientos b√°sicos primero. <br><br>  Hay dos tipos de escalado: horizontal y vertical.  Y hay dos tipos de escalado horizontal: replicaci√≥n y fragmentaci√≥n.  La replicaci√≥n asegura el escalado computacional mientras que el fragmentaci√≥n se usa para el escalado de datos. <br><br>  Sharding tambi√©n se subdivide en dos tipos: sharding basado en rango y sharding basado en hash. <br><br>  El particionamiento basado en rango implica que se calcula alguna clave de fragmento para cada registro de cl√∫ster.  Las teclas de fragmentos se proyectan en una l√≠nea recta que se separa en rangos y se asigna a diferentes nodos f√≠sicos. <br><br>  La fragmentaci√≥n basada en hash es menos complicada: se calcula una funci√≥n hash para cada registro en un cl√∫ster;  los registros con la misma funci√≥n hash se asignan al mismo nodo f√≠sico. <br><br>  Me centrar√© en el escalado horizontal utilizando el fragmentaci√≥n basada en hash. <br><a name="habracut"></a><br><h2>  Implementaci√≥n anterior </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tarantool Shard</a> fue nuestro m√≥dulo original para escalado horizontal.  Utiliz√≥ fragmentos simples basados ‚Äã‚Äãen hash y claves de fragmentos calculados por clave principal para todos los registros en un cl√∫ster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Pero eventualmente Tarantool Shard se volvi√≥ incapaz de abordar nuevas tareas. <br><br>  Primero, uno de nuestros eventuales requisitos se convirti√≥ en la <b>localidad</b> garantizada <b>de datos relacionados l√≥gicamente</b> .  En otras palabras, cuando tenemos datos relacionados l√≥gicamente, siempre queremos almacenarlos en un solo nodo f√≠sico, independientemente de la topolog√≠a del cl√∫ster y los cambios de equilibrio.  Fragmento de Tarantool no puede garantizar eso.  Calculaba hashes solo con claves primarias, por lo que el reequilibrio podr√≠a causar la separaci√≥n temporal de registros con el mismo hash porque los cambios no se realizan at√≥micamente. <br><br>  Esta falta de localidad de datos fue el principal problema para nosotros.  Aqu√≠ hay un ejemplo.  Digamos que hay un banco donde un cliente ha abierto una cuenta.  La informaci√≥n sobre la cuenta y el cliente debe almacenarse f√≠sicamente juntos para que pueda recuperarse en una sola solicitud o modificarse en una sola transacci√≥n, por ejemplo, durante una transferencia de dinero.  Si utilizamos el fragmentaci√≥n tradicional de Tarantool Shard, habr√° diferentes valores de funci√≥n hash para cuentas y clientes.  Los datos podr√≠an terminar en nodos f√≠sicos separados.  Esto realmente complica tanto la lectura como la transacci√≥n con los datos de un cliente. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  En el ejemplo anterior, los campos de identificaci√≥n de las cuentas y el cliente pueden ser inconsistentes.  Est√°n conectados por el campo customer_id de la cuenta y el campo id del cliente.  El mismo campo de identificaci√≥n violar√≠a la restricci√≥n de unicidad de la clave primaria de la cuenta.  Y Shard no puede realizar fragmentaci√≥n de ninguna otra manera. <br><br>  Otro problema fue el <b>lento reacomodamiento</b> , que es el problema fundamental de todos los fragmentos hash.  La conclusi√≥n es que al cambiar los componentes del cl√∫ster, la funci√≥n de fragmento cambia porque generalmente depende del n√∫mero de nodos.  Entonces, cuando la funci√≥n cambia, es necesario revisar todos los registros del cl√∫ster y volver a calcular la funci√≥n.  Tambi√©n puede ser necesario transferir algunos registros.  Y durante la transferencia de datos, ¬øni siquiera sabemos si el registro requerido?  En la solicitud, los datos ya se han transferido o se est√°n transfiriendo en este momento.  Por lo tanto, durante la reorganizaci√≥n, es necesario realizar solicitudes de lectura con funciones de fragmento antiguas y nuevas.  Las solicitudes se manejan dos veces m√°s lento, y esto es inaceptable. <br><br>  Otro problema con Tarantool Shard fue la baja disponibilidad de lecturas en el caso de falla de nodo en un conjunto de r√©plicas. <br><br><h2>  Nueva soluci√≥n </h2><br>  Creamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tarantool VShard</a> para resolver los tres problemas mencionados anteriormente.  Su diferencia clave es que su nivel de almacenamiento de datos est√° virtualizado, es decir, los almacenamientos f√≠sicos alojan almacenamientos virtuales, y los registros de datos se asignan sobre los virtuales.  Estos almacenamientos se llaman <i>cubos</i> .  El usuario no tiene que preocuparse por lo que se encuentra en un nodo f√≠sico dado.  Un cubo es una unidad de datos indivisible at√≥mica, como una tupla en el fragmentaci√≥n tradicional.  VShard siempre almacena un dep√≥sito completo en un nodo f√≠sico, y durante la reorganizaci√≥n migra at√≥micamente todos los datos de un dep√≥sito.  Este m√©todo asegura la localidad de los datos.  Simplemente colocamos los datos en un dep√≥sito y siempre podemos estar seguros de que no se separar√°n durante los cambios del cl√∫ster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  ¬øC√≥mo ponemos los datos en un cubo?  Agreguemos un nuevo campo de identificaci√≥n de dep√≥sito a la tabla para nuestro cliente bancario.  Si este valor de campo es el mismo para datos relacionados, todos los registros estar√°n en un dep√≥sito.  La ventaja es que podemos almacenar registros con la misma identificaci√≥n del dep√≥sito en diferentes espacios, e incluso en diferentes motores.  La localidad de los datos basada en la identificaci√≥n del dep√≥sito est√° garantizada independientemente del m√©todo de almacenamiento. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  ¬øPor qu√© es esto tan importante?  Cuando se usa el fragmentaci√≥n tradicional, los datos se extender√≠an a varios almacenamientos f√≠sicos existentes.  Para nuestro ejemplo bancario, tendr√≠amos que contactar a cada nodo cuando solicitemos todas las cuentas para un cliente determinado.  Entonces obtenemos una complejidad de lectura O (N), donde N es el n√∫mero de almacenamientos f√≠sicos.  Es exasperantemente lento. <br><br>  El uso de cubos y la localidad por ID de cubo hace posible leer los datos necesarios de un nodo utilizando una solicitud, independientemente del tama√±o del cl√∫ster. <br><br><img src="https://habrastorage.org/webt/t7/_r/fm/t7_rfmxoroosmaoqbe8cskpsr0k.png"><br><br>  En VShard, calcula su id. De cubo y lo asigna.  Para algunas personas, esto es una ventaja, mientras que otras lo consideran una desventaja.  Creo que la posibilidad de elegir su propia funci√≥n para el c√°lculo del id del dep√≥sito es una ventaja. <br><br>  ¬øCu√°l es la diferencia clave entre los fragmentos tradicionales y los fragmentos virtuales con cubos? <br><br>  En el primer caso, cuando cambiamos los componentes del cl√∫ster, tenemos dos estados: el actual (antiguo) y el nuevo que se implementar√°.  En el proceso de transici√≥n, es necesario no solo migrar datos, sino tambi√©n recalcular la funci√≥n hash para cada registro.  Esto no es muy conveniente porque en un momento dado no sabemos si los datos requeridos ya se han migrado o no.  Adem√°s, este m√©todo no es confiable y los cambios no son at√≥micos, ya que la migraci√≥n at√≥mica del conjunto de registros con el mismo valor de funci√≥n hash requerir√≠a un almacenamiento persistente del estado de migraci√≥n en caso de que sea necesaria la recuperaci√≥n.  Como resultado, hay conflictos y errores, y la operaci√≥n debe reiniciarse varias veces. <br><br>  El fragmentaci√≥n virtual es mucho m√°s simple.  No tenemos dos estados de cl√∫ster diferentes;  solo tenemos el estado del cubo.  El cl√∫ster es m√°s flexible, se mueve suavemente de un estado a otro.  ¬øHay m√°s de dos estados ahora?  (claro)  Con la transici√≥n suave, es posible cambiar el equilibrio sobre la marcha o eliminar almacenamientos reci√©n agregados.  Es decir, el control de equilibrio ha aumentado considerablemente y se ha vuelto m√°s granular. <br><br><h2>  Uso </h2><br>  Digamos que hemos seleccionado una funci√≥n para nuestra identificaci√≥n de dep√≥sito y hemos cargado tantos datos en el cl√∫ster que no queda espacio.  Ahora nos gustar√≠a agregar algunos nodos y mover datos autom√°ticamente a ellos.  As√≠ es como lo hacemos en VShard: primero, iniciamos nuevos nodos y ejecutamos Tarantool all√≠, luego actualizamos nuestra configuraci√≥n de VShard.  Contiene informaci√≥n sobre cada componente del cl√∫ster, cada r√©plica, conjuntos de r√©plicas, maestros, URI asignados y mucho m√°s.  Ahora agregamos nuestros nuevos nodos al archivo de configuraci√≥n y lo aplicamos a todos los nodos del cl√∫ster usando VShard.storage.cfg. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Como recordar√°, al cambiar el n√∫mero de nodos en el fragmentaci√≥n tradicional, la funci√≥n de fragmentaci√≥n tambi√©n cambia.  Esto no sucede en VShard.  Aqu√≠ tenemos un n√∫mero fijo de almacenamientos virtuales, o cubos.  Esta es una constante que elige al iniciar el cl√∫ster.  Puede parecer que la escalabilidad es, por lo tanto, limitada, pero realmente no lo es.  Puede especificar una gran cantidad de cubos, decenas y cientos de miles.  Lo importante es saber que debe haber al menos dos √≥rdenes de magnitud m√°s dep√≥sitos que el n√∫mero m√°ximo de conjuntos de r√©plicas que tendr√° en el cl√∫ster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Dado que el n√∫mero de almacenamientos virtuales no cambia, y la funci√≥n de fragmento depende solo de este valor, podemos agregar tantos almacenamientos f√≠sicos como quisi√©ramos sin volver a calcular la funci√≥n de fragmento. <br><br>  Entonces, ¬øc√≥mo se asignan los cubos a los almacenamientos f√≠sicos?  Si se llama VShard.storage.cfg, un proceso de reequilibrio se activa en uno de los nodos.  Este es un proceso anal√≠tico que calcula el equilibrio perfecto para el cl√∫ster.  El proceso va a cada nodo f√≠sico y recupera su n√∫mero de cubos, y luego construye rutas de sus movimientos para equilibrar la asignaci√≥n.  Luego, el reequilibrador env√≠a las rutas a los almacenes sobrecargados, que a su vez comienzan a enviar cubos.  Un poco m√°s tarde, el grupo est√° equilibrado. <br><br>  En proyectos del mundo real, un equilibrio perfecto puede no alcanzarse tan f√°cilmente.  Por ejemplo, un conjunto de r√©plicas podr√≠a contener menos datos que el otro porque tiene menos capacidad de almacenamiento.  En este caso, VShard puede pensar que todo est√° equilibrado pero, de hecho, el primer almacenamiento est√° a punto de sobrecargarse.  Para contrarrestar esto, hemos proporcionado un mecanismo para corregir las reglas de equilibrio mediante pesas.  Se puede asignar un peso a cualquier conjunto de r√©plica o almacenamiento.  Cuando el reequilibrador decide cu√°ntos cubos se deben enviar y d√≥nde, considera las <b>relaciones</b> de todos los pares de pesos. <br><br>  Por ejemplo, si un almacenamiento pesa 100 y el otro 200, el segundo almacenar√° el doble de cubos que el primero.  Tenga en cuenta que estoy hablando espec√≠ficamente de las <b>relaciones de</b> peso.  Los valores absolutos no tienen influencia alguna.  Usted elige pesos basados ‚Äã‚Äãen una distribuci√≥n del 100% en un cl√∫ster: por lo tanto, el 30% para un almacenamiento producir√≠a el 70% para el otro.  Puede tomar la capacidad de almacenamiento en gigabytes como base, o puede medir el peso en la cantidad de cubos.  Lo m√°s importante es mantener la proporci√≥n necesaria. <br><br><img src="https://habrastorage.org/webt/sz/0v/gi/sz0vgicyunfvpamx3ic8enwsl58.png"><br><br>  Este m√©todo tiene un efecto secundario interesante: si se asigna un peso cero a un almacenamiento, el reequilibrador har√° que este almacenamiento redistribuya todos sus dep√≥sitos.  A partir de entonces, puede eliminar todo el conjunto de r√©plicas de la configuraci√≥n. <br><br><h2>  Migraci√≥n de cubo at√≥mico </h2><br>  Tenemos un balde;  acepta algunas lecturas y escrituras, y en un momento dado, el reequilibrador solicita su migraci√≥n a otro almacenamiento.  El dep√≥sito deja de aceptar solicitudes de escritura; de lo contrario, se actualizar√≠a durante la migraci√≥n, luego se actualizar√≠a nuevamente durante la migraci√≥n de actualizaci√≥n, luego la actualizaci√≥n se actualizar√≠a, y as√≠ sucesivamente.  Por lo tanto, las solicitudes de escritura est√°n bloqueadas, pero a√∫n es posible leer desde el dep√≥sito.  Los datos ahora se est√°n migrando a la nueva ubicaci√≥n.  Cuando se completa la migraci√≥n, el dep√≥sito comienza a aceptar solicitudes nuevamente.  Todav√≠a existe en la ubicaci√≥n anterior, pero est√° marcado como basura, y m√°s tarde el recolector de basura lo elimina pieza por pieza. <br><br>  Hay algunos metadatos almacenados f√≠sicamente en el disco que est√°n asociados con cada dep√≥sito.  Todos los pasos descritos anteriormente se almacenan en el disco, y no importa lo que pase con el almacenamiento, el estado del dep√≥sito se restaurar√° autom√°ticamente. <br><br>  Puede tener algunas preguntas siguientes: <br><br><ul><li>  <b>¬øQu√© sucede con las solicitudes que funcionan con el dep√≥sito cuando comienza la migraci√≥n?</b> <br><br>  Hay dos tipos de referencias en los metadatos de cada segmento: RO y RW.  Cuando un usuario realiza una solicitud a un dep√≥sito, indica si el trabajo debe estar en modo de solo lectura o en modo de lectura y escritura.  Para cada solicitud, se incrementa el contador de referencia correspondiente. <br><br>  ¬øPor qu√© necesitamos contadores de referencia para solicitudes de escritura?  Digamos que se est√° migrando un cubo y, de repente, el recolector de basura quiere eliminarlo.  El recolector de basura reconoce que el contador de referencia est√° por encima de cero y, por lo tanto, el dep√≥sito no se eliminar√°.  Cuando se completan todas las solicitudes, el recolector de basura puede hacer su trabajo. <br><br>  El contador de referencia para escrituras tambi√©n asegura que la migraci√≥n del dep√≥sito no se iniciar√° si hay al menos una solicitud de escritura en proceso.  Pero, de nuevo, las solicitudes de escritura podr√≠an venir una tras otra, y el dep√≥sito nunca se migrar√≠a.  Entonces, si el reequilibrador desea mover el dep√≥sito, el sistema bloquea las nuevas solicitudes de escritura mientras espera que se completen las solicitudes actuales durante un cierto per√≠odo de tiempo de espera.  Si las solicitudes no se completan dentro del tiempo de espera especificado, el sistema comenzar√° a aceptar nuevas solicitudes de escritura nuevamente mientras pospone la migraci√≥n del dep√≥sito.  De esta manera, el reequilibrador intentar√° migrar el dep√≥sito hasta que la migraci√≥n se realice correctamente. <br><br>  VShard tiene una API bucket_ref de bajo nivel en caso de que necesite algo m√°s que capacidades de alto nivel.  Si realmente desea hacer algo usted mismo, consulte esta API. </li><li>  <b>¬øEs posible dejar los registros desbloqueados?</b> <br><br>  No  Si el dep√≥sito contiene datos cr√≠ticos y requiere acceso de escritura permanente, entonces tendr√° que bloquear su migraci√≥n por completo.  Tenemos una funci√≥n bucket_pin para hacer precisamente eso.  Conecta el dep√≥sito al conjunto de r√©plica actual para que el reequilibrador no pueda migrar el dep√≥sito.  En este caso, los cubos adyacentes podr√°n moverse sin restricciones. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Un bloqueo de conjunto de r√©plica es una herramienta a√∫n m√°s fuerte que bucket_pin.  Esto ya no se hace en el c√≥digo sino en la configuraci√≥n.  Un bloqueo de conjunto de r√©plicas deshabilita la migraci√≥n de cualquier dep√≥sito dentro / fuera del conjunto de r√©plica.  Por lo tanto, todos los datos estar√°n disponibles permanentemente para escrituras. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h2>  VShard.router </h2><br>  VShard consta de dos subm√≥dulos: VShard.storage y VShard.router.  Podemos crear y escalar estos de forma independiente en una sola instancia.  Al solicitar un cl√∫ster, no sabemos d√≥nde se encuentra un dep√≥sito determinado, y VShard.router lo buscar√° por ID de dep√≥sito para nosotros. <br><br>  Volvamos a nuestro ejemplo, el grupo bancario con cuentas de clientes.  Me gustar√≠a poder obtener todas las cuentas de un determinado cliente del cl√∫ster.  Esto requiere una funci√≥n est√°ndar para la b√∫squeda local: <br><br><img src="https://habrastorage.org/webt/q4/om/pp/q4omppscsww5c-cshatc6bnvgky.png"><br><br>  Busca todas las cuentas del cliente por su id.  Ahora tengo que decidir d√≥nde debo ejecutar la funci√≥n.  Para este prop√≥sito, calculo la identificaci√≥n del dep√≥sito por el identificador del cliente en mi solicitud y le pido a VShard.router que llame a la funci√≥n en el almacenamiento donde se encuentra el dep√≥sito con la identificaci√≥n del dep√≥sito objetivo.  El subm√≥dulo tiene una tabla de enrutamiento que describe las ubicaciones de los dep√≥sitos en los conjuntos de r√©plica.  VShard.router redirige mi solicitud. <br><br>  Ciertamente, puede suceder que el fragmentaci√≥n comience en este momento exacto y que los cubos comiencen a moverse.  El enrutador en segundo plano actualiza gradualmente la tabla en fragmentos grandes al solicitar tablas de dep√≥sito actuales de los almacenes. <br><br>  Incluso podemos solicitar un dep√≥sito recientemente migrado, por el cual el enrutador a√∫n no ha actualizado su tabla de enrutamiento.  En este caso, solicitar√° el almacenamiento anterior, que redirigir√° el enrutador a otro almacenamiento o simplemente responder√° que no tiene los datos necesarios.  Luego, el enrutador pasar√° por cada almacenamiento en busca del cubo requerido.  Y ni siquiera notaremos un error en la tabla de enrutamiento. <br><br><h2>  Leer conmutaci√≥n por error </h2><br>  Recordemos nuestros problemas iniciales: <br><br><ul><li>  Sin localidad de datos.  Resuelto mediante cubos. </li><li>  Proceso de reacomodamiento empantanado y reteniendo todo.  Implementamos la transferencia de datos at√≥micos mediante cubos y eliminamos el rec√°lculo de la funci√≥n de fragmento. </li><li>  Lea la conmutaci√≥n por error. </li></ul><br>  VShard.router resuelve el √∫ltimo problema, compatible con el subsistema de conmutaci√≥n por error de lectura autom√°tica. <br><br>  De vez en cuando, el enrutador hace ping a los almacenamientos especificados en la configuraci√≥n.  Digamos, por ejemplo, que el enrutador no puede hacer ping a uno de ellos.  El enrutador tiene una conexi√≥n de respaldo en caliente para cada r√©plica, por lo que si la r√©plica actual no responde, simplemente cambia a otra.  Las solicitudes de lectura se procesar√°n normalmente porque podemos leer en las r√©plicas (pero no escribir).  Y podemos especificar la prioridad para las r√©plicas como un factor para que el enrutador elija la conmutaci√≥n por error para las lecturas.  Esto se hace por medio de la zonificaci√≥n. <br><br><img src="https://habrastorage.org/webt/aw/iz/ry/awizryylhzk9h2rct_kxo1-jvpc.png"><br><br>  Asignamos un n√∫mero de zona a cada r√©plica y cada enrutador y especificamos una tabla donde indicamos la distancia entre cada par de zonas.  Cuando el enrutador decide a d√≥nde debe enviar una solicitud de lectura, selecciona una r√©plica en la zona m√°s cercana. <br><br>  As√≠ es como se ve en la configuraci√≥n: <br><br><img src="https://habrastorage.org/webt/2w/jx/cu/2wjxcuidtcghobukd3mxu02ms2y.png"><br><br>  En general, puede solicitar cualquier r√©plica, pero si el cl√∫ster es grande, complejo y altamente distribuido, la zonificaci√≥n puede ser muy √∫til.  Se pueden seleccionar diferentes racks de servidores como zonas para que el tr√°fico no sobrecargue la red.  Alternativamente, se pueden seleccionar puntos geogr√°ficamente aislados. <br><br>  La zonificaci√≥n tambi√©n ayuda cuando las r√©plicas demuestran diferentes comportamientos.  Por ejemplo, cada conjunto de r√©plicas tiene una r√©plica de respaldo que no debe aceptar solicitudes, sino que solo debe almacenar una copia de los datos.  En este caso, lo colocamos en una zona lejos de todos los enrutadores de la tabla para que el enrutador no aborde esta r√©plica a menos que sea absolutamente necesario. <br><br><h2>  Escribir failover </h2><br>  Ya hemos hablado sobre la conmutaci√≥n por error de lectura.  ¬øQu√© pasa con la escritura de conmutaci√≥n por error al cambiar el maestro?  En VShard, la imagen no es tan optimista como antes: la selecci√≥n maestra no est√° implementada, por lo que tendremos que hacerlo nosotros mismos.  Cuando de alguna manera hemos designado un maestro, la instancia designada ahora deber√≠a asumir el control como maestro.  Luego, actualizamos la configuraci√≥n especificando master = false para el maestro anterior y master = true para el nuevo, aplicamos la configuraci√≥n mediante VShard.storage.cfg y la compartimos con cada almacenamiento.  Todo lo dem√°s se hace autom√°ticamente.  El viejo maestro deja de aceptar solicitudes de escritura e inicia la sincronizaci√≥n con el nuevo, porque puede haber datos que ya se han aplicado en el viejo maestro pero no en el nuevo.  Despu√©s de eso, el nuevo maestro est√° a cargo y comienza a aceptar solicitudes, y el viejo maestro es una r√©plica.  As√≠ es como funciona la conmutaci√≥n por error de escritura en VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><br><h2>  ¬øC√≥mo rastreamos estos diversos eventos? </h2><br>  VShard.storage.info y VShard.router.info son suficientes. <br><br>  VShard.storage.info muestra informaci√≥n en varias secciones. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  La primera secci√≥n es para replicaci√≥n.  Aqu√≠ puede ver el estado del conjunto de r√©plicas donde se llama la funci√≥n: su retraso de replicaci√≥n, sus conexiones disponibles y no disponibles, su configuraci√≥n maestra, etc. <br><br>  En la secci√≥n de dep√≥sito, puede ver en tiempo real el n√∫mero de dep√≥sitos que se migran hacia / desde el conjunto de r√©plica actual, el n√∫mero de dep√≥sitos que funcionan en modo normal, el n√∫mero de dep√≥sitos marcados como basura y el n√∫mero de dep√≥sitos anclados. <br><br>  La secci√≥n Alertas muestra los problemas que VShard pudo determinar por s√≠ mismo: "el maestro no est√° configurado", "hay un nivel de redundancia insuficiente", "el maestro est√° all√≠, pero todas las r√©plicas fallaron", etc. <br><br>  Y la √∫ltima secci√≥n (q: ¬øes este "estado"?) Es una luz que se vuelve roja cuando todo sale mal.  Es un n√∫mero de cero a tres, por lo que un n√∫mero mayor es peor. <br><br>  VShard.router.info tiene las mismas secciones, pero su significado es algo diferente. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  La primera secci√≥n es para la replicaci√≥n, aunque no contiene informaci√≥n sobre los retrasos de replicaci√≥n, sino m√°s bien informaci√≥n sobre disponibilidad: conexiones de enrutador a un conjunto de r√©plica;  conexi√≥n en caliente y conexi√≥n de respaldo en caso de que falle el maestro;  el maestro seleccionado;  y el n√∫mero de dep√≥sitos RW disponibles y dep√≥sitos RO en cada conjunto de r√©plicas. <br><br>  La secci√≥n del dep√≥sito muestra el n√∫mero total de dep√≥sitos de lectura-escritura y solo lectura disponibles actualmente para este enrutador;  el n√∫mero de cubos con una ubicaci√≥n desconocida;  y el n√∫mero de dep√≥sitos con una ubicaci√≥n conocida pero sin conexi√≥n con el conjunto de r√©plicas necesario. <br><br>  La secci√≥n de alertas describe principalmente conexiones, eventos de conmutaci√≥n por error y dep√≥sitos no identificados. <br><br>  Finalmente, tambi√©n est√° el estado simple?  Indicador de cero a tres. <br><br><h2>  ¬øQu√© necesitas para usar VShard? </h2><br>  Primero debe seleccionar un n√∫mero constante de cubos.  ¬øPor qu√© no simplemente configurarlo en int32_max?  Debido a que los metadatos se almacenan junto con cada dep√≥sito, 30 bytes en almacenamiento y 16 bytes en el enrutador.  Cuantos m√°s cubos tenga, m√°s espacio ocupar√°n los metadatos.  Pero al mismo tiempo, el tama√±o del dep√≥sito ser√° m√°s peque√±o, lo que significa una mayor granularidad del cl√∫ster y una mayor velocidad de migraci√≥n por dep√≥sito.  Por lo tanto, debe elegir qu√© es m√°s importante para usted y el nivel de escalabilidad que es necesario. <br><br>  En segundo lugar, debe seleccionar una funci√≥n de fragmento para calcular la identificaci√≥n del dep√≥sito.  Las reglas son las mismas que cuando se selecciona una funci√≥n de fragmento en el fragmento tradicional, ya que un dep√≥sito aqu√≠ es el mismo que el n√∫mero fijo de almacenamientos en el fragmento tradicional.  La funci√≥n debe distribuir uniformemente los valores de salida, de lo contrario, el crecimiento del tama√±o de la cubeta no se equilibrar√° y VShard solo opera con la cantidad de cubetas.  Si no equilibra su funci√≥n de fragmento, deber√° migrar los datos de un dep√≥sito a otro y cambiar la funci√≥n de fragmento.  Por lo tanto, debe elegir con cuidado. <br><br><h2>  Resumen </h2><br>  VShard asegura: <br><br><ul><li>  localidad de datos </li><li>  reabastecimiento at√≥mico </li><li>  mayor flexibilidad de cl√∫ster </li><li>  failover de lectura autom√°tica </li><li>  Controladores de cubos m√∫ltiples. </li></ul><br>  VShard est√° en desarrollo activo.  Algunas tareas planificadas ya se est√°n implementando.  La primera tarea es <b>el equilibrio de carga del enrutador</b> .  Si hay muchas solicitudes de lectura, no siempre se recomienda dirigirlas al maestro.  El enrutador debe equilibrar las solicitudes de diferentes r√©plicas de lectura por s√≠ mismo. <br><br>  La segunda tarea es la <b>migraci√≥n del dep√≥sito sin bloqueo</b> .  Ya se ha implementado un algoritmo que ayuda a mantener los cubos desbloqueados incluso durante la migraci√≥n.  El dep√≥sito se bloquear√° solo al final para documentar la migraci√≥n en s√≠. <br><br>  La tercera tarea es <b>la aplicaci√≥n at√≥mica de la configuraci√≥n</b> .  No es conveniente ni at√≥mico aplicar la configuraci√≥n por separado porque es posible que parte del almacenamiento no est√© disponible, y si la configuraci√≥n no se aplica, ¬øqu√© hacemos a continuaci√≥n?  Es por eso que estamos trabajando en un mecanismo para la transferencia autom√°tica de configuraci√≥n. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442782/">https://habr.com/ru/post/442782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442770/index.html">Descripci√≥n general de los esc√°neres de c√≥digo de barras JavaScript</a></li>
<li><a href="../442772/index.html">Matem√°ticas para el cient√≠fico de datos: secciones necesarias</a></li>
<li><a href="../442776/index.html">√çndices en PostgreSQL - 3 (Hash)</a></li>
<li><a href="../442778/index.html">Learning Go: una selecci√≥n de informes en video</a></li>
<li><a href="../442780/index.html">Conceptos err√≥neos m√°s comunes en f√≠sica popular</a></li>
<li><a href="../442784/index.html">Secuestro de BGP al agregar la v√≠ctima AS al AS-SET del atacante</a></li>
<li><a href="../442786/index.html">7 consejos √∫tiles para usar la habitaci√≥n</a></li>
<li><a href="../442788/index.html">¬øPor qu√© necesitamos un sistema de monitoreo en un chip?</a></li>
<li><a href="../442790/index.html">El registro est√° abierto para Allure Server Meetup en San Petersburgo</a></li>
<li><a href="../442794/index.html">Te invitamos a la conferencia ‚ÄúArquitecto (de TI) en proyectos y organizaciones de TI‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>