<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôçÔ∏è üÜó üïπÔ∏è Wie AWS seine belastbaren Services zusammenstellt. Netzwerkskalierung üçó ü¶Ç üßö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Amazon Web Services-Netzwerk verf√ºgt √ºber 69 Standorte weltweit in 22 Regionen: USA, Europa, Asien, Afrika und Australien. In jeder Zone gibt es b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie AWS seine belastbaren Services zusammenstellt. Netzwerkskalierung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/471688/"> Das Amazon Web Services-Netzwerk verf√ºgt √ºber 69 Standorte weltweit in 22 Regionen: USA, Europa, Asien, Afrika und Australien.  In jeder Zone gibt es bis zu 8 Rechenzentren - Datenverarbeitungszentren.  Jedes Rechenzentrum verf√ºgt √ºber Tausende oder Hunderttausende von Servern.  Das Netzwerk ist so aufgebaut, dass alle unwahrscheinlichen Ausfallszenarien ber√ºcksichtigt werden.  Beispielsweise sind alle Regionen voneinander isoliert und die Zugangszonen sind mehrere Kilometer voneinander entfernt.  Selbst wenn Sie das Kabel abschneiden, wechselt das System zu Sicherungskan√§len, und der Informationsverlust betr√§gt Einheiten von Datenpaketen.  √úber welche anderen Prinzipien das Netzwerk aufgebaut ist und wie es aufgebaut ist, wird Vasily Pantyukhin sagen. <br><br><img src="https://habrastorage.org/webt/5p/_u/v_/5p_uv_g6etdeiay-nwb0r6v8ns0.png"><br><br>  <b>Vasily Pantyukhin</b> begann als Unix-Administrator in .ru-Unternehmen, verbrachte 6 Jahre in den gro√üen Dr√ºsen des Sun Microsystems und predigte 11 Jahre lang bei EMC die Datenzentriertheit der Welt.  Nat√ºrlich zu privaten Clouds entwickelt, dann an die √ñffentlichkeit gegangen.  Als Architekt von Amazon Web Services k√∂nnen Sie mit technischen Ratschl√§gen in der AWS-Cloud leben und wachsen. <br><br>  Im vorherigen Teil der AWS-Ger√§tetrilogie befasste sich Vasily mit dem Ger√§t der physischen Server und der Datenbankskalierung.  Nitro-Karten, benutzerdefinierter Hypervisor basierend auf KVM, Amazon Aurora-Datenbank - all dies im Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wie AWS" seine elastischen Dienste kocht ".</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Server- und Datenbankskalierung</a> . ‚Äú  Lesen Sie, um in den Kontext einzutauchen, oder sehen Sie sich ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video der</a> Pr√§sentation an. <br><br>  In diesem Teil konzentrieren wir uns auf die Netzwerkskalierung - eines der komplexesten Systeme in AWS.  Entwicklung von einem flachen Netzwerk zu Virtual Private Cloud und seinem Ger√§t, internen Blackfoot- und HyperPlane-Diensten, dem Problem eines lauten Nachbarn und letztendlich der Gr√∂√üe des Netzwerks, des Backbones und der physischen Kabel.  √úber all das unter dem Schnitt. <br><br>  <i>Haftungsausschluss: Alles unten ist Vasilys pers√∂nliche Meinung und stimmt m√∂glicherweise nicht mit der Position von Amazon Web Services √ºberein.</i> <br><a name="habracut"></a><br><h2>  Netzwerkskalierung </h2><br>  AWS Cloud wurde 2006 gestartet.  Sein Netzwerk war ziemlich primitiv - mit einer flachen Struktur.  Der Bereich der privaten Adressen war allen Mandanten der Cloud gemeinsam.  Wenn Sie eine neue virtuelle Maschine starten, haben Sie versehentlich eine verf√ºgbare IP-Adresse aus diesem Bereich erhalten. <br><br><img src="https://habrastorage.org/webt/kj/u9/jw/kju9jwz69aeoldlbn6yn_vmtzom.jpeg"><br><br>  Dieser Ansatz war einfach zu implementieren, beschr√§nkte jedoch die Nutzung der Cloud grundlegend.  Insbesondere war es ziemlich schwierig, hybride L√∂sungen zu entwickeln, die private Netzwerke vor Ort und in AWS kombinierten.  Das h√§ufigste Problem war der Schnittpunkt von IP-Adressbereichen. <br><br><img src="https://habrastorage.org/webt/lw/fu/tg/lwfutg75jtwalgbliyv-rmdhfbc.jpeg"><br><br><h3>  Virtuelle private Cloud </h3><br>  Die Cloud war gefragt.  Es ist Zeit, √ºber die Skalierbarkeit und die M√∂glichkeit der Nutzung durch zig Millionen Mieter nachzudenken.  Flaches Netzwerk ist zu einem gro√üen Hindernis geworden.  Daher haben wir uns √ºberlegt, wie Benutzer auf Netzwerkebene voneinander isoliert werden k√∂nnen, damit sie IP-Bereiche unabh√§ngig ausw√§hlen k√∂nnen. <br><br><img src="https://habrastorage.org/webt/fl/tw/co/fltwcomu7sr802932c3wyyaucve.jpeg"><br><br>  Was f√§llt Ihnen zuerst ein, wenn Sie an Netzwerkisolation denken?  Nat√ºrlich sind <b>VLAN</b> und <b>VRF Virtual Routing und Forwarding</b> . <br><br>  Dies hat leider nicht funktioniert.  Die VLAN-ID betr√§gt nur 12 Bit, wodurch wir nur 4096 isolierte Segmente erhalten.  Selbst in den gr√∂√üten Switches k√∂nnen Sie maximal 1-2 Tausend VRF verwenden.  Die kombinierte Verwendung von VRF und VLAN gibt uns nur wenige Millionen Subnetze.  Dies ist definitiv nicht genug f√ºr zig Millionen Mieter, von denen jeder mehrere Subnetze nutzen kann. <br><br>  Trotzdem k√∂nnen wir es uns einfach nicht leisten, die erforderliche Anzahl gro√üer Kartons zu kaufen, beispielsweise von Cisco oder Juniper.  Es gibt zwei Gr√ºnde: Es ist unglaublich teuer und wir m√∂chten nicht von deren Entwicklungs- und Patch-Richtlinien abh√§ngig werden. <br><br><blockquote>  Es gibt nur eine Schlussfolgerung - Ihre eigene Entscheidung zu treffen. </blockquote><br>  2009 haben wir <b>VPC</b> - <b>Virtual Private Cloud</b> angek√ºndigt.  Der Name hat Wurzeln geschlagen und wird jetzt auch von vielen Cloud-Anbietern verwendet. <br><br>  VPC ist ein virtuelles <b>SDN-</b> Netzwerk (Software Defined Network).  Wir haben beschlossen, keine speziellen Protokolle auf den Ebenen L2 und L3 zu erfinden.  Das Netzwerk l√§uft √ºber Standard-Ethernet und IP.  F√ºr die √úbertragung √ºber ein Netzwerk wird der Datenverkehr der virtuellen Maschine in einem Wrapper unseres eigenen Protokolls gekapselt.  Es gibt die ID an, die zur Mandanten-VPC geh√∂rt. <br><br><img src="https://habrastorage.org/webt/x7/yh/nc/x7yhncwzn9xfi677tpy65s3td18.jpeg"><br><br>  Das klingt einfach.  Es ist jedoch notwendig, mehrere schwerwiegende technische Probleme zu l√∂sen.  Beispiel: Wo und wie werden Zuordnungsdaten f√ºr virtuelle MAC / IP-Adressen, VPC-IDs und entsprechende physische MAC / IP-Adressen gespeichert?  Auf einer AWS-Skala ist dies eine riesige Tabelle, die mit minimaler Latenz arbeiten sollte.  Verantwortlich daf√ºr ist der <b>Mapping-Service</b> , der im gesamten Netzwerk mit einer d√ºnnen Schicht verschmiert ist. <br><br>  In Maschinen neuer Generationen erfolgt die Einkapselung durch Nitro-Karten auf Eisenebene.  In √§lteren F√§llen Software-Kapselung und -Entkapselung. <br><br><img src="https://habrastorage.org/webt/6q/pt/zr/6qptzrmyqowtaimwlbu6mqobr44.jpeg"><br><br>  Mal sehen, wie das allgemein funktioniert.  Beginnen wir mit Level L2.  Angenommen, wir haben eine virtuelle Maschine mit IP 10.0.0.2 auf einem physischen Server 192.168.0.3.  Es sendet Daten an eine virtuelle Maschine 10.0.0.3, die unter 192.168.1.4 ausgef√ºhrt wird.  Es wird eine ARP-Anfrage generiert, die auf die Nitro-Netzwerkkarte f√§llt.  Der Einfachheit halber glauben wir, dass beide virtuellen Maschinen in derselben ‚Äûblauen‚Äú VPC leben. <br><br><img src="https://habrastorage.org/webt/vl/u5/hv/vlu5hvvmaufe2e2jirp0mz14ugg.png"><br><br>  Die Karte ersetzt die Quelladresse durch eine eigene und sendet den ARP-Frame an den Zuordnungsdienst. <br><br><img src="https://habrastorage.org/webt/0j/d9/lx/0jd9lx41a5744ptq64tdrmgzxya.png"><br><br>  Der Zuordnungsdienst gibt die Informationen zur√ºck, die f√ºr die √úbertragung √ºber das physische L2-Netzwerk erforderlich sind. <br><br><img src="https://habrastorage.org/webt/3e/ir/nt/3eirntgsopwiccgdaaneovvgqxe.png"><br><br>  Die Nitrokarte in der ARP-Antwort ersetzt den MAC im physischen Netzwerk durch die Adresse in der VPC. <br><br><img src="https://habrastorage.org/webt/fv/uo/kh/fvuokhk6mswvl5i8ifguxgxrhee.png"><br><br>  Bei der Daten√ºbertragung verpacken wir den logischen MAC und die IP in einen VPC-Wrapper.  All dies wird √ºber das physische Netzwerk mit den entsprechenden IP-Nitro-Karten der Quelle und des Ziels √ºbertragen. <br><br><img src="https://habrastorage.org/webt/4k/4i/u-/4k4iu-ei5cp9cdk-vfqmmf8vtig.png"><br><br>  Die physische Maschine, auf der das Paket √úberpr√ºfungen durchf√ºhren soll.  Dies soll die M√∂glichkeit von Spoofing verhindern.  Der Computer sendet eine spezielle Anforderung an den Zuordnungsdienst und fragt: ‚ÄûVom physischen Computer 192.168.0.3 habe ich ein Paket erhalten, das f√ºr 10.0.0.3 in der blauen VPC ausgelegt ist.  Ist er legitim? " <br><br><img src="https://habrastorage.org/webt/vv/qn/0e/vvqn0ecobvoxvxw8-1u3hgc8k48.png"><br><br>  Der Zuordnungsdienst √ºberpr√ºft seine Ressourcenzuordnungstabelle und erlaubt oder verweigert den Durchgang des Pakets.  In allen neuen F√§llen wird eine zus√§tzliche Validierung in Nitro-Karten eingef√ºgt.  Es ist unm√∂glich, sich auch theoretisch fortzubewegen.  Daher funktioniert das Spoofing von Ressourcen in einer anderen VPC nicht. <br><br><img src="https://habrastorage.org/webt/xx/tr/wa/xxtrwaqlrsbhdledrligrt0mfjc.png"><br><br>  Anschlie√üend werden die Daten an die virtuelle Maschine gesendet, f√ºr die sie bestimmt sind. <br><br><img src="https://habrastorage.org/webt/1i/hb/py/1ihbpywnhbngzsuzs8376qf0i8g.png"><br><br>  Der Zuordnungsdienst fungiert auch als logischer Router zum √úbertragen von Daten zwischen virtuellen Maschinen in verschiedenen Subnetzen.  Dort ist konzeptionell alles einfach, ich werde es nicht im Detail analysieren. <br><br><img src="https://habrastorage.org/webt/rr/rj/9n/rrrj9nvl-jwgk54pzowtmqm6ynm.png"><br><br>  Es stellt sich heraus, dass die Server w√§hrend der √úbertragung jedes Pakets auf den Zuordnungsdienst zugreifen.  Wie gehe ich mit unvermeidlichen Verz√∂gerungen um?  <b>Caching</b> nat√ºrlich. <br><br>  Der ganze Reiz ist, dass Sie nicht den gesamten riesigen Tisch zwischenspeichern m√ºssen.  Virtuelle Maschinen von einer relativ kleinen Anzahl von VPCs leben auf einem physischen Server.  Informationen m√ºssen nur √ºber diese VPCs zwischengespeichert werden.  Das √úbertragen von Daten auf andere VPCs in der "Standard" -Konfiguration ist immer noch nicht legitim.  Wenn Funktionen wie VPC-Peering verwendet werden, werden zus√§tzlich Informationen zu den entsprechenden VPCs in den Cache geladen. <br><br><img src="https://habrastorage.org/webt/cj/vu/uh/cjvuuhb_xbbsrhjdzxpf0x7lnck.jpeg"><br><br>  Mit der √úbertragung der Daten an die VPC herausgefunden. <br><br><h3>  Blackfoot </h3><br>  Was tun, wenn der Datenverkehr au√üerhalb des Unternehmens √ºbertragen werden muss, z. B. im Internet oder √ºber ein VPN zum Boden?  Hier hilft uns <b>Blackfoot</b> , der interne AWS-Service.  Es wurde von unserem s√ºdafrikanischen Team entworfen.  Daher ist der Dienst nach dem in S√ºdafrika lebenden Pinguin benannt. <br><br><img src="https://habrastorage.org/webt/af/7s/gf/af7sgf9jhvniudixr94rqhwgdy0.jpeg"><br><br>  Blackfoot entkapselt den Verkehr und macht damit, was es braucht.  Internetdaten werden unver√§ndert gesendet. <br><br><img src="https://habrastorage.org/webt/fi/93/02/fi9302-pumvdpx70gqpo7ufaaqw.png"><br><br>  Bei Verwendung eines VPN werden die Daten entkapselt und erneut in einen IPSec-Wrapper eingeschlossen. <br><br><img src="https://habrastorage.org/webt/jk/ag/iu/jkagiufrbqprn50vjuxqy_hwbe8.png"><br><br>  Bei Verwendung von Direct Connect wird der Datenverkehr markiert und an das entsprechende VLAN √ºbertragen. <br><br><img src="https://habrastorage.org/webt/yj/xj/i0/yjxji0wexg4cugs-cztvnv_wvxs.png"><br><br><h3>  HyperPlane </h3><br>  Dies ist ein interner Flusskontrolldienst.  Viele Netzwerkdienste erfordern die √úberwachung des <b>Status des Datenstroms</b> .  Wenn Sie beispielsweise NAT verwenden, sollte die Flusskontrolle sicherstellen, dass jedes Paar ‚ÄûIP: Zielport‚Äú einen eindeutigen ausgehenden Port hat.  Beim <b>NLB-</b> Balancer - <b>Network Load Balancer</b> sollte der Datenstrom immer an dieselbe virtuelle Zielmaschine geleitet werden.  Sicherheitsgruppen ist eine Stateful Firewall.  Es √ºberwacht den eingehenden Verkehr und √∂ffnet implizit Ports f√ºr den ausgehenden Paketstrom. <br><br><img src="https://habrastorage.org/webt/wq/pa/kk/wqpakkyp8v_rdhyuclzte2e2y6w.jpeg"><br><br>  In der AWS-Cloud sind die Anforderungen an die √úbertragungslatenz extrem hoch.  Daher ist <b>HyperPlane</b> f√ºr den <b>Zustand</b> des gesamten Netzwerks von entscheidender Bedeutung. <br><br><img src="https://habrastorage.org/webt/ov/cr/wm/ovcrwmosat9fxborz1tmkxkjt_4.jpeg"><br><br>  Hyperplane basiert auf virtuellen EC2-Maschinen.  Hier gibt es keine Magie, nur List.  Der Trick ist, dass es sich um virtuelle Maschinen mit gro√üem RAM handelt.  Transaktionen sind Transaktionen und werden ausschlie√ülich im Speicher ausgef√ºhrt.  Dies erm√∂glicht Verz√∂gerungen von nur einigen zehn Mikrosekunden.  Das Arbeiten mit einer Festplatte w√ºrde die gesamte Leistung beeintr√§chtigen. <br><br>  Hyperplane ist ein verteiltes System aus einer gro√üen Anzahl solcher EC2-Maschinen.  Jede virtuelle Maschine hat eine Bandbreite von 5 GB / s.  Dies ergibt im gesamten regionalen Netzwerk wilde Terabit-Bandbreite und erm√∂glicht es Ihnen, <b>Millionen von Verbindungen pro Sekunde zu verarbeiten</b> . <br><br>  HyperPlane funktioniert nur mit Threads.  Die VPC-Paketkapselung ist f√ºr ihn v√∂llig transparent.  Die potenzielle Sicherheitsanf√§lligkeit in diesem internen Dienst l√§sst die VPC-Isolation weiterhin nicht zu.  F√ºr die Sicherheit sind die folgenden Ebenen verantwortlich. <br><br><h3>  Lauter Nachbar </h3><br>  Es gibt auch das <b>Problem mit</b> <b>lauten Nachbarn</b> .  Angenommen, wir haben 8 Knoten.  Diese Knoten verarbeiten die Threads aller Cloud-Benutzer.  Alles scheint in Ordnung zu sein und die Last sollte gleichm√§√üig auf alle Knoten verteilt sein.  Die Knoten sind sehr leistungsf√§hig und schwer zu √ºberlasten. <br><br>  Aber wir bauen unsere Architektur auf selbst unwahrscheinlichen Szenarien auf. <br><br><blockquote>  Geringe Wahrscheinlichkeit bedeutet nicht Unm√∂glichkeit. </blockquote><br>  Wir k√∂nnen uns eine Situation vorstellen, in der ein oder mehrere Benutzer zu viel Last erzeugen.  Alle HyperPlane-Knoten sind an der Verarbeitung dieser Last beteiligt, und andere Benutzer k√∂nnen m√∂glicherweise eine Leistungsverschlechterung sp√ºren.  Dies zerst√∂rt das Konzept der Cloud, in der sich die Mieter nicht gegenseitig beeinflussen k√∂nnen. <br><br><img src="https://habrastorage.org/webt/gc/ni/_l/gcni_lqe59zlatesmuodjxmcxcm.png"><br><br>  Wie kann man das Problem eines lauten Nachbarn l√∂sen?  Das erste, was mir in den Sinn kommt, ist Scherben.  Unsere 8 Knoten sind logisch in 4 Shards mit jeweils 2 Knoten unterteilt.  Jetzt wird ein lauter Nachbar nur von einem Viertel aller Benutzer behindert, aber viel mehr. <br><br><img src="https://habrastorage.org/webt/e7/vz/-v/e7vz-vablrrhawvz2xbvb1psfxu.png"><br><br>  Lass es uns anders machen.  Jedem Benutzer sind nur 3 Knoten zugeordnet. <br><br><img src="https://habrastorage.org/webt/md/su/px/mdsupxahouehxh6y-jffnyhjpyy.png"><br><br>  Der Trick besteht darin, Knoten zuf√§llig verschiedenen Benutzern zuzuweisen.  In der Abbildung unten schneidet der blaue Benutzer die Knoten mit einem der beiden anderen Benutzer - gr√ºn und orange. <br><br><img src="https://habrastorage.org/webt/xw/ee/yz/xweeyztmogrwbeomfqi_sbec0u0.png"><br><br>  Bei 8 Knoten und 3 Benutzern betr√§gt die Wahrscheinlichkeit, dass ein lauter Nachbar mit einem der Benutzer √ºberquert, 54%.  Mit dieser Wahrscheinlichkeit wirkt sich der blaue Benutzer auf andere Mieter aus.  Dar√ºber hinaus nur ein Teil seiner Last.  In unserem Beispiel wird dieser Einfluss zumindest nicht f√ºr alle sp√ºrbar sein, sondern nur f√ºr ein Drittel aller Benutzer.  Das ist schon ein gutes Ergebnis. <br><div class="scrollable-table"><table><tbody><tr><td>  Die Anzahl der Benutzer, die sich √ºberschneiden <br></td><td>  Wahrscheinlichkeit in Prozent <br></td></tr><tr><td>  0 <br></td><td>  18% <br></td></tr><tr><td>  1 <br></td><td>  54% <br></td></tr><tr><td>  2 <br></td><td>  26% <br></td></tr><tr><td>  3 <br></td><td>  2% <br></td></tr></tbody></table></div><br>  Lassen Sie uns die Situation n√§her an die reale bringen - nehmen Sie 100 Knoten und 5 Benutzer auf 5 Knoten.  In diesem Fall schneidet sich keiner der Knoten mit einer Wahrscheinlichkeit von 77%. <br><div class="scrollable-table"><table><tbody><tr><td>  Die Anzahl der Benutzer, die sich √ºberschneiden <br></td><td>  Wahrscheinlichkeit in Prozent <br></td></tr><tr><td>  0 <br></td><td>  77% <br></td></tr><tr><td>  1 <br></td><td>  21% <br></td></tr><tr><td>  2 <br></td><td>  1,8% <br></td></tr><tr><td>  3 <br></td><td>  0,06% <br></td></tr><tr><td>  4 <br></td><td>  0,0006% <br></td></tr><tr><td>  5 <br></td><td>  0,00000013% <br></td></tr></tbody></table></div><br>  In einer realen Situation mit einer gro√üen Anzahl von HyperPlane-Knoten und -Benutzern ist die potenzielle Auswirkung eines lauten Nachbarn auf andere Benutzer minimal.  Diese Methode wird als <b>Shuffle-Sharding bezeichnet</b> .  Es minimiert die negativen Auswirkungen eines Knotenausfalls. <br><br>  Viele Dienste basieren auf HyperPlane: Netzwerklastenausgleich, NAT-Gateway, Amazon EFS, AWS PrivateLink und AWS Transit Gateway. <br><br><h3>  Netzwerkskala </h3><br>  Lassen Sie uns nun √ºber die Gr√∂√üe des Netzwerks selbst sprechen.  F√ºr Oktober 2019 bietet AWS seine Dienste in <b>22 Regionen an</b> , 9 weitere sind geplant. <br><br><ul><li>  Jede Region enth√§lt mehrere Verf√ºgbarkeitszonen.  Es gibt 69 von ihnen auf der Welt. <br></li><li>  Jede AZ besteht aus Datenverarbeitungszentren.  Es gibt nicht mehr als 8 von ihnen. <br></li><li>  Im Rechenzentrum gibt es eine gro√üe Anzahl von Servern, einige bis zu 300.000. <br></li></ul><br>  Jetzt wird alles gemittelt, multipliziert und erh√§lt eine beeindruckende Zahl, die die <b>Gr√∂√üe der Amazon-Cloud</b> anzeigt. <br><br>  Zwischen den Zugangszonen und dem Rechenzentrum sind viele optische Kan√§le verlegt.  In einer unserer gr√∂√üten Regionen wurden nur 388 Kan√§le f√ºr die Verbindung zwischen AZ und Kommunikationszentren mit anderen Regionen (Transitzentren) eingerichtet.  Insgesamt ergibt dies verr√ºckte <b>5000 Tbit</b> . <br><br><img src="https://habrastorage.org/webt/zi/nj/bj/zinjbjkov6298ccr_kuiux7z89k.jpeg"><br><br>  Backbone AWS wurde speziell f√ºr die Cloud entwickelt und f√ºr die Arbeit mit dieser optimiert.  Wir bauen es auf <b>100 GB / s-</b> Kan√§len.  Wir kontrollieren sie vollst√§ndig, mit Ausnahme der Regionen in China.  Der Datenverkehr wird nicht mit den Lasten anderer Unternehmen geteilt. <br><br><img src="https://habrastorage.org/webt/e-/ko/hs/e-kohsjs0wax_tmb0zd3l5xh5ys.png"><br><br>  Nat√ºrlich sind wir nicht der einzige Cloud-Anbieter mit einem privaten Backbone-Netzwerk.  Immer mehr gro√üe Unternehmen gehen diesen Weg.  Dies wird beispielsweise von unabh√§ngigen Forschern aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Telegeographie best√§tigt</a> . <br><br><img src="https://habrastorage.org/webt/my/en/ad/myenadxrsqlm4a58uyc6a_us7kq.jpeg"><br><br>  Die Grafik zeigt, dass der Anteil von Inhaltsanbietern und Cloud-Anbietern w√§chst.  Aus diesem Grund nimmt der Anteil des Internetverkehrs von Backbone-Anbietern st√§ndig ab. <br><br>  Ich werde erkl√§ren, warum dies passiert.  Bisher waren die meisten Webdienste verf√ºgbar und wurden direkt aus dem Internet genutzt.  Jetzt befinden sich immer mehr Server in der Cloud und sind √ºber das <b>CDN</b> - <b>Content Distribution Network</b> verf√ºgbar.  Um auf die Ressource zuzugreifen, geht der Benutzer √ºber das Internet nur zum n√§chsten CDN PoP - <b>Point of Presence</b> .  Meistens ist es irgendwo in der N√§he.  Dann verl√§sst er das √∂ffentliche Internet und fliegt beispielsweise √ºber ein privates Backbone durch den Atlantik und gelangt direkt zur Ressource. <br><br>  Ich frage mich, wie sich das Internet in 10 Jahren √§ndern wird, wenn sich dieser Trend fortsetzt. <br><br><h3>  Physische Kan√§le </h3><br>  Wissenschaftler haben noch nicht herausgefunden, wie die Lichtgeschwindigkeit im Universum erh√∂ht werden kann, haben jedoch gro√üe Fortschritte bei den Methoden zur √úbertragung durch Lichtwellenleiter erzielt.  Wir verwenden derzeit 6912 Glasfaserkabel.  Dies hilft, die Kosten ihrer Installation erheblich zu optimieren. <br><br>  In einigen Regionen m√ºssen spezielle Kabel verwendet werden.  In der Region Sydney verwenden wir beispielsweise Kabel mit einer speziellen Beschichtung gegen Termiten. <br><br><img src="https://habrastorage.org/webt/qc/vn/wh/qcvnwhnrrbnil48u0qqgblljgna.jpeg"><br><br>  Niemand ist vor Problemen sicher und manchmal sind unsere Kan√§le besch√§digt.  Das Foto rechts zeigt optische Kabel in einer der amerikanischen Regionen, die von Bauherren zerrissen wurden.  Infolge des Unfalls gingen nur 13 Datenpakete verloren, was √ºberraschend ist.  Noch einmal - nur 13!  Das System wechselte buchst√§blich sofort zu Backup-Kan√§len - die Waage funktioniert. <br><br>  Wir galoppierten √ºber einige Amazon Cloud-Dienste und -Technologien.  Ich hoffe, Sie haben zumindest eine Vorstellung von der Gr√∂√üe der Aufgaben, die unsere Ingenieure l√∂sen m√ºssen.  Ich pers√∂nlich interessiere mich sehr daf√ºr. <br><br><blockquote>  Dies ist der letzte Teil der Trilogie von Vasily Pantyukhin √ºber das AWS-Ger√§t.  Der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste</a> Teil beschreibt die Serveroptimierung und Datenbankskalierung, und der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweite Teil</a> beschreibt serverlose Funktionen und Firecracker. <br><br>  Bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HighLoad ++</a> im November wird Vasily Pantyukhin neue Amazon-Ger√§tedetails ver√∂ffentlichen.  Er <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wird</a> √ºber die Fehlerursachen und das Design verteilter Systeme bei Amazon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sprechen</a> .  Am 24. Oktober k√∂nnen Sie noch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein</a> Ticket zu einem guten Preis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">buchen</a> und sp√§ter bezahlen.  Wir warten bei HighLoad ++ auf Sie, kommen Sie und reden Sie! </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de471688/">https://habr.com/ru/post/de471688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de471670/index.html">Versuchen Sie Jetpack Compose im Kampf?</a></li>
<li><a href="../de471676/index.html">Telefonbetr√ºger. Die zweite Aktion, bei der ich zusammenbreche und zum n√§chsten Geldautomaten renne</a></li>
<li><a href="../de471678/index.html">Tragen Sie Dienstleistungen auf Anfrage</a></li>
<li><a href="../de471684/index.html">Warum m√ºssen Sie Module f√ºr Nginx erstellen?</a></li>
<li><a href="../de471686/index.html">Wie AWS seine belastbaren Services zusammenstellt. Server- und Datenbankskalierung</a></li>
<li><a href="../de471700/index.html">Wie ich einen technologischen Stack mit einer Grundlage f√ºr die Zukunft gew√§hlt habe</a></li>
<li><a href="../de471702/index.html">Cyber-erweiterte Webanwendungen</a></li>
<li><a href="../de471704/index.html">Das Buch ‚ÄûEgoistische Mitochondrien. Wie man die Gesundheit erh√§lt und das Alter bewegt "</a></li>
<li><a href="../de471706/index.html">9 typische Netzwerkprobleme, die mithilfe der NetFlow-Analyse erkannt werden k√∂nnen (am Beispiel Flowmon)</a></li>
<li><a href="../de471708/index.html">Storypoints sind gef√§hrlich f√ºr die Entwicklung von Client-Server-Anwendungen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>