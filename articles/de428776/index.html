<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😵 🚵🏾 🙋🏾 Eine neue Erkenntnis der Neugier in der KI. Training mit einer Belohnung, die von der Schwierigkeit abhängt, das Ergebnis vorherzusagen 📏 ⭐️ 🔖</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fortschritte im Spiel "Montezumas Rache" wurden von vielen als Synonym für Erfolge bei der Erforschung unbekannter Umgebungen angesehen 

 Wir haben e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Eine neue Erkenntnis der Neugier in der KI. Training mit einer Belohnung, die von der Schwierigkeit abhängt, das Ergebnis vorherzusagen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">Fortschritte im Spiel "Montezumas Rache" wurden von vielen als Synonym für Erfolge bei der Erforschung unbekannter Umgebungen angesehen</font></i> <br><br>  Wir haben eine prädiktive RND-Methode (Random Network Distillation) entwickelt, die verstärkte Lernagenten dazu ermutigt, die Umwelt durch Neugierde zu erkunden.  Diese Methode übertraf zum ersten Mal die durchschnittlichen menschlichen Ergebnisse im Computerspiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Montezumas Rache"</a> (mit Ausnahme der anonymen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anwendung</a> im ICLR, bei der das Ergebnis schlechter ist als bei uns).  <b>RND demonstriert hochmoderne Effizienz, findet regelmäßig alle 24 Räume und passiert das erste Level ohne vorläufige Demonstration und ohne Zugang zum Grundzustand des Spiels.</b> <br><a name="habracut"></a><br>  Die RND-Methode stimuliert den Übergang eines Agenten in unbekannte Zustände, indem die Komplexität der Vorhersage des Ergebnisses der Überlagerung eines zufälligen zufälligen neuronalen Netzwerks mit Zustandsdaten gemessen wird.  Wenn der Zustand unbekannt ist, ist das Endergebnis schwer vorherzusagen, was bedeutet, dass die Belohnung hoch ist.  Die Methode kann auf jeden Verstärkungslernalgorithmus angewendet werden, ist einfach zu implementieren und für die Skalierung effektiv.  Unten finden Sie einen Link zur Implementierung von RND, der die Ergebnisse unseres Artikels wiedergibt. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Text eines wissenschaftlichen Artikels</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Ergebnisse in Montezumas Rache </h1><br>  Um das gewünschte Ziel zu erreichen, muss der Agent zunächst untersuchen, welche Aktionen in der Umgebung möglich sind und was den Fortschritt in Richtung des Ziels ausmacht.  Viele Belohnungssignale in Spielen bieten einen Lehrplan, sodass bereits einfache Forschungsstrategien ausreichen, um das Ziel zu erreichen.  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Arbeit mit der DQN-Präsentation war Montezumas</a> Revenge das <b>einzige Spiel, in dem DQN das Ergebnis von 0% der durchschnittlichen menschlichen Punktzahl (4700) zeigte</b> .  Es ist unwahrscheinlich, dass einfache Intelligenzstrategien Belohnungen sammeln und nicht mehr als ein paar Räume auf dem Level finden.  Seitdem wurde der Fortschritt im Spiel Montezumas Rache von vielen als Synonym für Fortschritte bei der Erforschung unbekannter Umgebungen angesehen. <br><br>  Signifikante Fortschritte wurden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2016</a> erzielt, indem DQN mit einem Bonus auf dem Schalter kombiniert wurde. Dadurch gelang es dem Agenten, 15 Zimmer zu finden und die höchste Punktzahl von 6600 mit einem Durchschnitt von etwa 3700 zu erzielen. Seitdem werden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">signifikante</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbesserungen des</a> Ergebnisses nur durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Demonstrationen</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experten</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experten</a> erzielt durch Zugriff auf die Basiszustände des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Emulators</a> . <br><br>  Wir haben ein groß angelegtes RND-Experiment mit 1024 Arbeitern durchgeführt, wobei ein <b>Durchschnittsergebnis von 10.000 über 9 Starts</b> und ein <b>bestes Durchschnittsergebnis von 14.500 erzielt wurden</b> .  In jedem Fall fand der Agent 20-22 Zimmer.  Darüber hinaus <b>beträgt</b> das <b>maximale Ergebnis</b> bei einem kleineren, aber längeren Start (von 10) <b>17.500, was dem Bestehen der ersten Ebene und dem Auffinden aller 24 Räume entspricht</b> .  Die folgende Grafik vergleicht diese beiden Experimente und zeigt den Durchschnittswert in Abhängigkeit von den Aktualisierungsparametern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  Die folgende Visualisierung zeigt den Fortschritt des Experiments in kleinerem Maßstab.  Der Agent öffnet unter dem Einfluss der Neugier neue Räume und findet Wege, um Punkte zu sammeln. Während des Trainings zwingt ihn diese externe Belohnung, später in diese Räume zurückzukehren. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Die vom Agenten entdeckten Räume und das durchschnittliche Ergebnis während des Trainings.</font></i>  <i><font color="gray">Der Transparenzgrad des Raums entspricht, wie oft von 10 Durchgängen des Agenten es erkannt wurde.</font></i>  <i><font color="gray"><a href="">Video</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neugierdebasierte groß angelegte Lernstudie</a> </h1><br>  Vor der Entwicklung von RND haben wir zusammen mit Mitarbeitern der University of California in Berkeley das Lernen ohne Umweltbelohnungen untersucht.  Neugier bietet eine einfachere Möglichkeit, Agenten die Interaktion mit <i>jeder</i> Umgebung beizubringen, als eine speziell entwickelte Belohnungsfunktion für eine bestimmte Aufgabe zu verwenden, die noch nicht der Lösung des Problems entspricht.  In Projekten wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Universum</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Malmö</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fitnessstudio</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fitnessstudio Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CommAI</a> wird eine große Anzahl simulierter Umgebungen für den Agenten über eine standardisierte Schnittstelle geöffnet.  Ein Agent, der eine allgemeine Belohnungsfunktion verwendet, die nicht für eine bestimmte Umgebung spezifisch ist, kann in einer Vielzahl von Umgebungen ein grundlegendes Kompetenzniveau erwerben.  Dies ermöglicht es ihm, nützliches Verhalten auch ohne aufwändige Belohnungen zu bestimmen. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Text eines wissenschaftlichen Artikels</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> </blockquote><br>  In Standardtrainingseinstellungen mit Verstärkung zu jedem einzelnen Zeitschritt sendet der Agent die Aktion an die Umgebung und reagiert, wobei er dem Agenten eine neue Beobachtung, eine Belohnung für den Übergang und einen Indikator für das Ende der Episode gibt.  In unserem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel haben</a> wir die Umgebung so eingerichtet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, dass</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nur die</a> folgende Beobachtung erzeugt wird.  Dort untersucht der Agent anhand seiner Erfahrung das Prädiktormodell des nächsten Zustands und verwendet den Vorhersagefehler als interne Belohnung.  Infolgedessen ist er von Unvorhersehbarkeit angezogen.  Beispielsweise wird eine Änderung des Spielkontos nur belohnt, wenn das Konto auf dem Bildschirm angezeigt wird und die Änderung schwer vorherzusagen ist.  Ein Agent findet in der Regel nützliche Interaktionen mit neuen Objekten, da die Ergebnisse solcher Interaktionen normalerweise schwieriger vorherzusagen sind als andere Aspekte der Umgebung. <br><br>  Wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">andere</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Forscher</a> haben wir versucht, die Modellierung aller Aspekte der Umgebung zu vermeiden, unabhängig davon, ob sie relevant sind oder nicht, und die Beobachtungsmerkmale für die Modellierung ausgewählt.  Überraschenderweise haben wir festgestellt, dass auch zufällige Funktionen gut funktionieren. <br><br><h1>  Was machen neugierige Agenten? </h1><br>  Wir haben unseren Agenten in mehr als 50 verschiedenen Umgebungen getestet und eine Reihe von Kompetenzen beobachtet, von scheinbar zufälligen Aktionen bis hin zu bewusster Interaktion mit der Umgebung.  Zu unserer Überraschung gelang es dem Agenten in einigen Fällen, durch das Spiel zu kommen, obwohl er nicht durch eine externe Belohnung über das Tor informiert wurde. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Interne Vergütung zu Beginn der Ausbildung</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Der Sprung in die interne Belohnung beim ersten Durchgang des Levels</font></i> <br><br>  <b>Breakout</b> - Springt in die interne Belohnung ein, wenn der Agent in einem frühen Stadium des Trainings eine neue Konfiguration von Blöcken sieht und wenn das Level nach mehreren Stunden Training zum ersten Mal bestanden wird. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - Wir haben den Agenten geschult, beide Plattformen gleichzeitig zu steuern, und er hat gelernt, den Ball im Spiel zu halten, was zu langwierigen Kämpfen führte.  Selbst beim Training gegen KI im Spiel versuchte der Agent, das Spiel zu maximieren und nicht zu gewinnen. <br><br>  <b><a href="">Bowling</a></b> - Der Agent hat gelernt, das Spiel besser zu spielen als andere Agenten, die direkt geschult wurden, um die externe Belohnung zu maximieren.  Wir glauben, dass dies passiert, weil der Agent von dem kaum vorhersehbaren Blinken der Anzeigetafel nach den Würfen angezogen wird. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - Die interne Belohnung passt besonders gut zum Ziel des Spiels: Levelfortschritt.  Der Agent wird für die Suche nach neuen Bereichen belohnt, da die Details des neu gefundenen Bereichs nicht vorhergesagt werden können.  Infolgedessen entdeckte der Agent 11 Levels, fand geheime Räume und besiegte sogar Bosse. <br><br><h1>  Lautes TV-Problem </h1><br>  Als Spieler an einem Spielautomaten, der von zufälligen Ergebnissen angezogen wird, gerät der Agent aufgrund des „lauten TV-Problems“ manchmal in die Falle seiner Neugier.  Der Agent findet eine Quelle der Zufälligkeit in der Umgebung und beobachtet sie weiterhin, wobei er für solche Übergänge immer eine hohe interne Belohnung erhält.  Ein Beispiel für eine solche Falle ist das Fernsehen, das statisches Rauschen erzeugt.  Wir demonstrieren dies buchstäblich, indem wir den Agenten mit einem Fernseher, der zufällige Kanäle wiedergibt, in das Unity-Labyrinth stellen. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in einem Labyrinth mit einem lauten Fernseher</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in einem Labyrinth ohne lauten Fernseher</font></i> <br><br>  Theoretisch ist das Problem eines lauten Fernsehgeräts sehr ernst, aber wir haben dennoch erwartet, dass in vielen deterministischen Umgebungen wie Montezumas Rache die Neugier den Agenten dazu veranlassen würde, Räume zu finden und mit Objekten zu interagieren.  Wir haben verschiedene Optionen ausprobiert, um den nächsten Zustand basierend auf Neugier vorherzusagen, indem wir einen Forschungsbonus mit einem Spielkonto kombiniert haben. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterstützt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  In diesen Experimenten steuert der Agent die Umgebung über einen Geräuschregler, der mit einiger Wahrscheinlichkeit die letzte Aktion anstelle der aktuellen Aktion wiederholt.  Diese Einstellung mit wiederholbaren „klebrigen“ Aktionen wurde als bewährte Methode für Trainingsagenten in vollständig deterministischen Spielen wie Atari vorgeschlagen, um das Auswendiglernen zu verhindern.  "Sticky" -Aktionen machen den Übergang von Raum zu Raum unvorhersehbar. <br><br><h1>  Zufällige Netzdestillation </h1><br>  Da die Vorhersage des nächsten Zustands von Natur aus anfällig für das Problem eines verrauschten Fernsehgeräts ist, haben wir die folgenden relevanten Ursachen für Vorhersagefehler identifiziert: <br><br><ul><li>  <b>Faktor 1</b> .  Der Prognosefehler ist hoch, wenn der Prädiktor die zuvor betrachteten Beispiele nicht verallgemeinert.  Neue Erfahrungen entsprechen einem hohen Vorhersagefehler. </li><li>  <b>Faktor 2</b> .  Der Prognosefehler ist aufgrund des stochastischen Prognoseziels hoch. </li><li>  <b>Faktor 3</b> .  Der Prognosefehler ist hoch, weil keine Informationen für die Prognose erforderlich sind oder weil die Klasse des Prädiktormodells zu begrenzt ist, um der Komplexität der Zielfunktion zu entsprechen. </li></ul><br>  Wir haben festgestellt, dass Faktor 1 eine nützliche Fehlerquelle ist, da er die Neuheit der Erfahrung quantifiziert, während die Faktoren 2 und 3 zum Problem eines lauten Fernsehgeräts führen.  Um die Faktoren 2 und 3 zu vermeiden, haben wir RND entwickelt - einen neuen Forschungsbonus, der auf der <b>Vorhersage der Ausgabe eines konstanten und zufällig initialisierten neuronalen Netzwerks im nächsten Zustand</b> basiert <b>und den folgenden Zustand selbst berücksichtigt</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  Die Intuition legt nahe, dass Vorhersagemodelle einen geringen Fehler bei der Vorhersage der Bedingungen aufweisen, unter denen sie trainiert wurde.  Insbesondere sind die Vorhersagen des Agenten über die Ausgabe eines zufällig initialisierten neuronalen Netzwerks in den neuen Zuständen weniger genau als in den Zuständen, die der Agent zuvor häufig getroffen hat.  Der Vorteil der Verwendung des synthetischen Prognoseproblems besteht darin, dass es deterministisch sein kann (Faktor 2 umgehen) und der Prädiktor innerhalb der Funktionsklasse einen Prädiktor mit derselben Architektur wie das Zielnetzwerk auswählen kann (Faktor 3 umgehen).  Dies beseitigt das RND-Problem eines lauten Fernsehgeräts. <br><br>  Wir haben den Forschungsbonus mit externen Belohnungen durch eine Variante der engsten Richtlinienoptimierung kombiniert - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Proximal Policy Optimization</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PPO</a> ), bei der <b>zwei Wertwerte für zwei Belohnungsflüsse verwendet werden</b> .  Auf diese Weise können Sie unterschiedliche Rabatte für unterschiedliche Belohnungen verwenden und episodische und nicht-episodische Belohnungen kombinieren.  <b>Aufgrund dieser zusätzlichen Flexibilität findet unser bester Agent häufig 22 von 24 Zimmern auf der ersten Ebene in Montezumas Rache und passiert manchmal die erste Ebene, nachdem er die verbleibenden zwei Zimmer gefunden hat.</b>  Die gleiche Methode demonstriert die Rekordleistung in Venture- und Gravitar-Spielen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  Die folgende Visualisierung zeigt eine grafische Darstellung der internen Belohnung in der Episode "Montezumas Rache", in der der Agent zuerst die Fackel findet. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  Eine kompetente Umsetzung ist wichtig </h1><br>  Um einen guten Algorithmus auszuwählen, ist es wichtig, allgemeine Überlegungen zu berücksichtigen, z. B. die Anfälligkeit für das Problem eines lauten Fernsehgeräts.  Wir haben jedoch festgestellt, dass scheinbar sehr kleine Änderungen an unserem einfachen Algorithmus seine Effektivität stark beeinflussen: von einem Agenten, der den ersten Raum nicht verlassen kann, zu einem Agenten, der die erste Ebene durchläuft.  Um dem Training Stabilität zu verleihen, haben wir die Sättigung von Merkmalen vermieden und interne Belohnungen auf einen vorhersehbaren Bereich gebracht.  Wir haben auch <b>jedes Mal, wenn wir einen Fehler gefunden und behoben haben, signifikante Verbesserungen in der Effektivität von RND festgestellt</b> (unser Favorit war das zufällige Nullstellen des Arrays, was dazu führte, dass externe Belohnungen als nicht episodisch angesehen wurden; wir haben dies erst erkannt, nachdem wir über die externe Wertefunktion nachgedacht hatten , die verdächtig periodisch aussah).  Das Korrigieren dieser Details ist zu einem wichtigen Bestandteil der Erzielung einer hohen Leistung geworden, selbst wenn Algorithmen verwendet werden, die konzeptionell denen früherer Arbeiten ähneln.  Dies ist einer der Gründe, warum es am besten ist, wenn immer möglich, einfache Algorithmen zu wählen. <br><br><h1>  Zukünftige Arbeit </h1><br>  Wir bieten folgende Bereiche für die weitere Forschung an: <br><br><ul><li>  Analyse der Vorteile verschiedener Forschungsmethoden und Suche nach neuen Kombinationsmöglichkeiten. </li><li>  Trainieren Sie einen neugierigen Agenten in vielen verschiedenen Umgebungen ohne Belohnungen und lernen Sie, mit Belohnungen in eine Zielumgebung zu wechseln. </li><li>  Globale Intelligenz, einschließlich koordinierter Lösungen über lange Zeiträume. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428776/">https://habr.com/ru/post/de428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428766/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 337 (29. Oktober - 4. November 2018)</a></li>
<li><a href="../de428768/index.html">In drei Artikeln über kleinste Quadrate: Bildungsprogramm zur Wahrscheinlichkeitstheorie</a></li>
<li><a href="../de428770/index.html">Tastaturmakros für alltägliche Aufgaben</a></li>
<li><a href="../de428772/index.html">Demokratisierung von Uber-Daten</a></li>
<li><a href="../de428774/index.html">GPS-Firewall für Rechenzentren - warum wird sie benötigt und wie funktioniert sie?</a></li>
<li><a href="../de428778/index.html">Sehen Sie das Unsichtbare. Nahinfrarot (0,9-1,7 μm)</a></li>
<li><a href="../de428786/index.html">Quantenprozessor basierend auf Spinresonanz und Manipulationen mit einem Singulett-Triplett-System</a></li>
<li><a href="../de428788/index.html">Unter der Haube von Bitfury Clarke - wie unser neuer Mining-Chip funktioniert</a></li>
<li><a href="../de428790/index.html">Wir schreiben einen Bot-Chat für VKontakte in Python mit Longpoll. Teil Zwei Doppelschleifen, Ausnahmen und andere Häresien</a></li>
<li><a href="../de428792/index.html">Der neue Apple T2-Chip macht es schwierig, über das eingebaute Mikrofon des Laptops zu hören</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>