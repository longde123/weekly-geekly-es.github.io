<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòµ üöµüèæ üôãüèæ Eine neue Erkenntnis der Neugier in der KI. Training mit einer Belohnung, die von der Schwierigkeit abh√§ngt, das Ergebnis vorherzusagen üìè ‚≠êÔ∏è üîñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fortschritte im Spiel "Montezumas Rache" wurden von vielen als Synonym f√ºr Erfolge bei der Erforschung unbekannter Umgebungen angesehen 

 Wir haben e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Eine neue Erkenntnis der Neugier in der KI. Training mit einer Belohnung, die von der Schwierigkeit abh√§ngt, das Ergebnis vorherzusagen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">Fortschritte im Spiel "Montezumas Rache" wurden von vielen als Synonym f√ºr Erfolge bei der Erforschung unbekannter Umgebungen angesehen</font></i> <br><br>  Wir haben eine pr√§diktive RND-Methode (Random Network Distillation) entwickelt, die verst√§rkte Lernagenten dazu ermutigt, die Umwelt durch Neugierde zu erkunden.  Diese Methode √ºbertraf zum ersten Mal die durchschnittlichen menschlichen Ergebnisse im Computerspiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Montezumas Rache"</a> (mit Ausnahme der anonymen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anwendung</a> im ICLR, bei der das Ergebnis schlechter ist als bei uns).  <b>RND demonstriert hochmoderne Effizienz, findet regelm√§√üig alle 24 R√§ume und passiert das erste Level ohne vorl√§ufige Demonstration und ohne Zugang zum Grundzustand des Spiels.</b> <br><a name="habracut"></a><br>  Die RND-Methode stimuliert den √úbergang eines Agenten in unbekannte Zust√§nde, indem die Komplexit√§t der Vorhersage des Ergebnisses der √úberlagerung eines zuf√§lligen zuf√§lligen neuronalen Netzwerks mit Zustandsdaten gemessen wird.  Wenn der Zustand unbekannt ist, ist das Endergebnis schwer vorherzusagen, was bedeutet, dass die Belohnung hoch ist.  Die Methode kann auf jeden Verst√§rkungslernalgorithmus angewendet werden, ist einfach zu implementieren und f√ºr die Skalierung effektiv.  Unten finden Sie einen Link zur Implementierung von RND, der die Ergebnisse unseres Artikels wiedergibt. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Text eines wissenschaftlichen Artikels</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Ergebnisse in Montezumas Rache </h1><br>  Um das gew√ºnschte Ziel zu erreichen, muss der Agent zun√§chst untersuchen, welche Aktionen in der Umgebung m√∂glich sind und was den Fortschritt in Richtung des Ziels ausmacht.  Viele Belohnungssignale in Spielen bieten einen Lehrplan, sodass bereits einfache Forschungsstrategien ausreichen, um das Ziel zu erreichen.  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Arbeit mit der DQN-Pr√§sentation war Montezumas</a> Revenge das <b>einzige Spiel, in dem DQN das Ergebnis von 0% der durchschnittlichen menschlichen Punktzahl (4700) zeigte</b> .  Es ist unwahrscheinlich, dass einfache Intelligenzstrategien Belohnungen sammeln und nicht mehr als ein paar R√§ume auf dem Level finden.  Seitdem wurde der Fortschritt im Spiel Montezumas Rache von vielen als Synonym f√ºr Fortschritte bei der Erforschung unbekannter Umgebungen angesehen. <br><br>  Signifikante Fortschritte wurden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2016</a> erzielt, indem DQN mit einem Bonus auf dem Schalter kombiniert wurde. Dadurch gelang es dem Agenten, 15 Zimmer zu finden und die h√∂chste Punktzahl von 6600 mit einem Durchschnitt von etwa 3700 zu erzielen. Seitdem werden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">signifikante</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbesserungen des</a> Ergebnisses nur durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Demonstrationen</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experten</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experten</a> erzielt durch Zugriff auf die Basiszust√§nde des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Emulators</a> . <br><br>  Wir haben ein gro√ü angelegtes RND-Experiment mit 1024 Arbeitern durchgef√ºhrt, wobei ein <b>Durchschnittsergebnis von 10.000 √ºber 9 Starts</b> und ein <b>bestes Durchschnittsergebnis von 14.500 erzielt wurden</b> .  In jedem Fall fand der Agent 20-22 Zimmer.  Dar√ºber hinaus <b>betr√§gt</b> das <b>maximale Ergebnis</b> bei einem kleineren, aber l√§ngeren Start (von 10) <b>17.500, was dem Bestehen der ersten Ebene und dem Auffinden aller 24 R√§ume entspricht</b> .  Die folgende Grafik vergleicht diese beiden Experimente und zeigt den Durchschnittswert in Abh√§ngigkeit von den Aktualisierungsparametern. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  Die folgende Visualisierung zeigt den Fortschritt des Experiments in kleinerem Ma√üstab.  Der Agent √∂ffnet unter dem Einfluss der Neugier neue R√§ume und findet Wege, um Punkte zu sammeln. W√§hrend des Trainings zwingt ihn diese externe Belohnung, sp√§ter in diese R√§ume zur√ºckzukehren. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Die vom Agenten entdeckten R√§ume und das durchschnittliche Ergebnis w√§hrend des Trainings.</font></i>  <i><font color="gray">Der Transparenzgrad des Raums entspricht, wie oft von 10 Durchg√§ngen des Agenten es erkannt wurde.</font></i>  <i><font color="gray"><a href="">Video</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neugierdebasierte gro√ü angelegte Lernstudie</a> </h1><br>  Vor der Entwicklung von RND haben wir zusammen mit Mitarbeitern der University of California in Berkeley das Lernen ohne Umweltbelohnungen untersucht.  Neugier bietet eine einfachere M√∂glichkeit, Agenten die Interaktion mit <i>jeder</i> Umgebung beizubringen, als eine speziell entwickelte Belohnungsfunktion f√ºr eine bestimmte Aufgabe zu verwenden, die noch nicht der L√∂sung des Problems entspricht.  In Projekten wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Universum</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Malm√∂</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fitnessstudio</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fitnessstudio Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CommAI</a> wird eine gro√üe Anzahl simulierter Umgebungen f√ºr den Agenten √ºber eine standardisierte Schnittstelle ge√∂ffnet.  Ein Agent, der eine allgemeine Belohnungsfunktion verwendet, die nicht f√ºr eine bestimmte Umgebung spezifisch ist, kann in einer Vielzahl von Umgebungen ein grundlegendes Kompetenzniveau erwerben.  Dies erm√∂glicht es ihm, n√ºtzliches Verhalten auch ohne aufw√§ndige Belohnungen zu bestimmen. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Text eines wissenschaftlichen Artikels</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> </blockquote><br>  In Standardtrainingseinstellungen mit Verst√§rkung zu jedem einzelnen Zeitschritt sendet der Agent die Aktion an die Umgebung und reagiert, wobei er dem Agenten eine neue Beobachtung, eine Belohnung f√ºr den √úbergang und einen Indikator f√ºr das Ende der Episode gibt.  In unserem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel haben</a> wir die Umgebung so eingerichtet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, dass</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nur die</a> folgende Beobachtung erzeugt wird.  Dort untersucht der Agent anhand seiner Erfahrung das Pr√§diktormodell des n√§chsten Zustands und verwendet den Vorhersagefehler als interne Belohnung.  Infolgedessen ist er von Unvorhersehbarkeit angezogen.  Beispielsweise wird eine √Ñnderung des Spielkontos nur belohnt, wenn das Konto auf dem Bildschirm angezeigt wird und die √Ñnderung schwer vorherzusagen ist.  Ein Agent findet in der Regel n√ºtzliche Interaktionen mit neuen Objekten, da die Ergebnisse solcher Interaktionen normalerweise schwieriger vorherzusagen sind als andere Aspekte der Umgebung. <br><br>  Wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">andere</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Forscher</a> haben wir versucht, die Modellierung aller Aspekte der Umgebung zu vermeiden, unabh√§ngig davon, ob sie relevant sind oder nicht, und die Beobachtungsmerkmale f√ºr die Modellierung ausgew√§hlt.  √úberraschenderweise haben wir festgestellt, dass auch zuf√§llige Funktionen gut funktionieren. <br><br><h1>  Was machen neugierige Agenten? </h1><br>  Wir haben unseren Agenten in mehr als 50 verschiedenen Umgebungen getestet und eine Reihe von Kompetenzen beobachtet, von scheinbar zuf√§lligen Aktionen bis hin zu bewusster Interaktion mit der Umgebung.  Zu unserer √úberraschung gelang es dem Agenten in einigen F√§llen, durch das Spiel zu kommen, obwohl er nicht durch eine externe Belohnung √ºber das Tor informiert wurde. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Interne Verg√ºtung zu Beginn der Ausbildung</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Der Sprung in die interne Belohnung beim ersten Durchgang des Levels</font></i> <br><br>  <b>Breakout</b> - Springt in die interne Belohnung ein, wenn der Agent in einem fr√ºhen Stadium des Trainings eine neue Konfiguration von Bl√∂cken sieht und wenn das Level nach mehreren Stunden Training zum ersten Mal bestanden wird. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - Wir haben den Agenten geschult, beide Plattformen gleichzeitig zu steuern, und er hat gelernt, den Ball im Spiel zu halten, was zu langwierigen K√§mpfen f√ºhrte.  Selbst beim Training gegen KI im Spiel versuchte der Agent, das Spiel zu maximieren und nicht zu gewinnen. <br><br>  <b><a href="">Bowling</a></b> - Der Agent hat gelernt, das Spiel besser zu spielen als andere Agenten, die direkt geschult wurden, um die externe Belohnung zu maximieren.  Wir glauben, dass dies passiert, weil der Agent von dem kaum vorhersehbaren Blinken der Anzeigetafel nach den W√ºrfen angezogen wird. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - Die interne Belohnung passt besonders gut zum Ziel des Spiels: Levelfortschritt.  Der Agent wird f√ºr die Suche nach neuen Bereichen belohnt, da die Details des neu gefundenen Bereichs nicht vorhergesagt werden k√∂nnen.  Infolgedessen entdeckte der Agent 11 Levels, fand geheime R√§ume und besiegte sogar Bosse. <br><br><h1>  Lautes TV-Problem </h1><br>  Als Spieler an einem Spielautomaten, der von zuf√§lligen Ergebnissen angezogen wird, ger√§t der Agent aufgrund des ‚Äûlauten TV-Problems‚Äú manchmal in die Falle seiner Neugier.  Der Agent findet eine Quelle der Zuf√§lligkeit in der Umgebung und beobachtet sie weiterhin, wobei er f√ºr solche √úberg√§nge immer eine hohe interne Belohnung erh√§lt.  Ein Beispiel f√ºr eine solche Falle ist das Fernsehen, das statisches Rauschen erzeugt.  Wir demonstrieren dies buchst√§blich, indem wir den Agenten mit einem Fernseher, der zuf√§llige Kan√§le wiedergibt, in das Unity-Labyrinth stellen. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in einem Labyrinth mit einem lauten Fernseher</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in einem Labyrinth ohne lauten Fernseher</font></i> <br><br>  Theoretisch ist das Problem eines lauten Fernsehger√§ts sehr ernst, aber wir haben dennoch erwartet, dass in vielen deterministischen Umgebungen wie Montezumas Rache die Neugier den Agenten dazu veranlassen w√ºrde, R√§ume zu finden und mit Objekten zu interagieren.  Wir haben verschiedene Optionen ausprobiert, um den n√§chsten Zustand basierend auf Neugier vorherzusagen, indem wir einen Forschungsbonus mit einem Spielkonto kombiniert haben. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Ihr Browser unterst√ºtzt kein HTML5-Video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  In diesen Experimenten steuert der Agent die Umgebung √ºber einen Ger√§uschregler, der mit einiger Wahrscheinlichkeit die letzte Aktion anstelle der aktuellen Aktion wiederholt.  Diese Einstellung mit wiederholbaren ‚Äûklebrigen‚Äú Aktionen wurde als bew√§hrte Methode f√ºr Trainingsagenten in vollst√§ndig deterministischen Spielen wie Atari vorgeschlagen, um das Auswendiglernen zu verhindern.  "Sticky" -Aktionen machen den √úbergang von Raum zu Raum unvorhersehbar. <br><br><h1>  Zuf√§llige Netzdestillation </h1><br>  Da die Vorhersage des n√§chsten Zustands von Natur aus anf√§llig f√ºr das Problem eines verrauschten Fernsehger√§ts ist, haben wir die folgenden relevanten Ursachen f√ºr Vorhersagefehler identifiziert: <br><br><ul><li>  <b>Faktor 1</b> .  Der Prognosefehler ist hoch, wenn der Pr√§diktor die zuvor betrachteten Beispiele nicht verallgemeinert.  Neue Erfahrungen entsprechen einem hohen Vorhersagefehler. </li><li>  <b>Faktor 2</b> .  Der Prognosefehler ist aufgrund des stochastischen Prognoseziels hoch. </li><li>  <b>Faktor 3</b> .  Der Prognosefehler ist hoch, weil keine Informationen f√ºr die Prognose erforderlich sind oder weil die Klasse des Pr√§diktormodells zu begrenzt ist, um der Komplexit√§t der Zielfunktion zu entsprechen. </li></ul><br>  Wir haben festgestellt, dass Faktor 1 eine n√ºtzliche Fehlerquelle ist, da er die Neuheit der Erfahrung quantifiziert, w√§hrend die Faktoren 2 und 3 zum Problem eines lauten Fernsehger√§ts f√ºhren.  Um die Faktoren 2 und 3 zu vermeiden, haben wir RND entwickelt - einen neuen Forschungsbonus, der auf der <b>Vorhersage der Ausgabe eines konstanten und zuf√§llig initialisierten neuronalen Netzwerks im n√§chsten Zustand</b> basiert <b>und den folgenden Zustand selbst ber√ºcksichtigt</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  Die Intuition legt nahe, dass Vorhersagemodelle einen geringen Fehler bei der Vorhersage der Bedingungen aufweisen, unter denen sie trainiert wurde.  Insbesondere sind die Vorhersagen des Agenten √ºber die Ausgabe eines zuf√§llig initialisierten neuronalen Netzwerks in den neuen Zust√§nden weniger genau als in den Zust√§nden, die der Agent zuvor h√§ufig getroffen hat.  Der Vorteil der Verwendung des synthetischen Prognoseproblems besteht darin, dass es deterministisch sein kann (Faktor 2 umgehen) und der Pr√§diktor innerhalb der Funktionsklasse einen Pr√§diktor mit derselben Architektur wie das Zielnetzwerk ausw√§hlen kann (Faktor 3 umgehen).  Dies beseitigt das RND-Problem eines lauten Fernsehger√§ts. <br><br>  Wir haben den Forschungsbonus mit externen Belohnungen durch eine Variante der engsten Richtlinienoptimierung kombiniert - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Proximal Policy Optimization</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PPO</a> ), bei der <b>zwei Wertwerte f√ºr zwei Belohnungsfl√ºsse verwendet werden</b> .  Auf diese Weise k√∂nnen Sie unterschiedliche Rabatte f√ºr unterschiedliche Belohnungen verwenden und episodische und nicht-episodische Belohnungen kombinieren.  <b>Aufgrund dieser zus√§tzlichen Flexibilit√§t findet unser bester Agent h√§ufig 22 von 24 Zimmern auf der ersten Ebene in Montezumas Rache und passiert manchmal die erste Ebene, nachdem er die verbleibenden zwei Zimmer gefunden hat.</b>  Die gleiche Methode demonstriert die Rekordleistung in Venture- und Gravitar-Spielen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  Die folgende Visualisierung zeigt eine grafische Darstellung der internen Belohnung in der Episode "Montezumas Rache", in der der Agent zuerst die Fackel findet. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  Eine kompetente Umsetzung ist wichtig </h1><br>  Um einen guten Algorithmus auszuw√§hlen, ist es wichtig, allgemeine √úberlegungen zu ber√ºcksichtigen, z. B. die Anf√§lligkeit f√ºr das Problem eines lauten Fernsehger√§ts.  Wir haben jedoch festgestellt, dass scheinbar sehr kleine √Ñnderungen an unserem einfachen Algorithmus seine Effektivit√§t stark beeinflussen: von einem Agenten, der den ersten Raum nicht verlassen kann, zu einem Agenten, der die erste Ebene durchl√§uft.  Um dem Training Stabilit√§t zu verleihen, haben wir die S√§ttigung von Merkmalen vermieden und interne Belohnungen auf einen vorhersehbaren Bereich gebracht.  Wir haben auch <b>jedes Mal, wenn wir einen Fehler gefunden und behoben haben, signifikante Verbesserungen in der Effektivit√§t von RND festgestellt</b> (unser Favorit war das zuf√§llige Nullstellen des Arrays, was dazu f√ºhrte, dass externe Belohnungen als nicht episodisch angesehen wurden; wir haben dies erst erkannt, nachdem wir √ºber die externe Wertefunktion nachgedacht hatten , die verd√§chtig periodisch aussah).  Das Korrigieren dieser Details ist zu einem wichtigen Bestandteil der Erzielung einer hohen Leistung geworden, selbst wenn Algorithmen verwendet werden, die konzeptionell denen fr√ºherer Arbeiten √§hneln.  Dies ist einer der Gr√ºnde, warum es am besten ist, wenn immer m√∂glich, einfache Algorithmen zu w√§hlen. <br><br><h1>  Zuk√ºnftige Arbeit </h1><br>  Wir bieten folgende Bereiche f√ºr die weitere Forschung an: <br><br><ul><li>  Analyse der Vorteile verschiedener Forschungsmethoden und Suche nach neuen Kombinationsm√∂glichkeiten. </li><li>  Trainieren Sie einen neugierigen Agenten in vielen verschiedenen Umgebungen ohne Belohnungen und lernen Sie, mit Belohnungen in eine Zielumgebung zu wechseln. </li><li>  Globale Intelligenz, einschlie√ülich koordinierter L√∂sungen √ºber lange Zeitr√§ume. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428776/">https://habr.com/ru/post/de428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428766/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 337 (29. Oktober - 4. November 2018)</a></li>
<li><a href="../de428768/index.html">In drei Artikeln √ºber kleinste Quadrate: Bildungsprogramm zur Wahrscheinlichkeitstheorie</a></li>
<li><a href="../de428770/index.html">Tastaturmakros f√ºr allt√§gliche Aufgaben</a></li>
<li><a href="../de428772/index.html">Demokratisierung von Uber-Daten</a></li>
<li><a href="../de428774/index.html">GPS-Firewall f√ºr Rechenzentren - warum wird sie ben√∂tigt und wie funktioniert sie?</a></li>
<li><a href="../de428778/index.html">Sehen Sie das Unsichtbare. Nahinfrarot (0,9-1,7 Œºm)</a></li>
<li><a href="../de428786/index.html">Quantenprozessor basierend auf Spinresonanz und Manipulationen mit einem Singulett-Triplett-System</a></li>
<li><a href="../de428788/index.html">Unter der Haube von Bitfury Clarke - wie unser neuer Mining-Chip funktioniert</a></li>
<li><a href="../de428790/index.html">Wir schreiben einen Bot-Chat f√ºr VKontakte in Python mit Longpoll. Teil Zwei Doppelschleifen, Ausnahmen und andere H√§resien</a></li>
<li><a href="../de428792/index.html">Der neue Apple T2-Chip macht es schwierig, √ºber das eingebaute Mikrofon des Laptops zu h√∂ren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>