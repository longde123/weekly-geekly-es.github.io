<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤷🏾 🙏🏼 👵 使用神经网络自动检测文本对话中的情绪 🚔 👩🏿‍🎨 🔟</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="对话系统的主要任务之一不仅是提供用户所需的信息，而且还要生成尽可能多的人为答案。 识别对话者的情绪不再只是一个很酷的功能，而是必不可少的。 在本文中，我们将研究用于确定文本对话中的情绪的递归神经网络的体系结构，该体系结构参加了SemEval-2019任务3“ EmoContext” ，这是计算机语言...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用神经网络自动检测文本对话中的情绪</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/463045/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/t6/sr/jr/t6srjrmjjmm6qn8gpld9emy4txu.gif"></div><br> 对话系统的主要任务之一不仅是提供用户所需的信息，而且还要生成尽可能多的人为答案。 识别对话者的情绪不再只是一个很酷的功能，而是必不可少的。 在本文中，我们将研究<b>用于确定文本对话中的情绪的递归神经网络</b>的<b>体系结构，</b>该<b>体系结构</b>参加了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">SemEval-2019任务3“ EmoContext”</a> ，这是计算机语言学的年度竞赛。 任务是通过聊天机器人和一个人参加的三个评论来对情绪（“快乐”，“悲伤”，“愤怒”和“其他”）进行分类。 <br><br> 在本文的第一部分中，我们将考虑EmoContext中设置的任务以及组织者提供的数据。 在第二部分和第三部分中，我们分析了文本的初步处理以及单词的矢量表示方式。 在第四部分中，我们描述了我们在比赛中使用的LSTM体系结构。 该代码使用Keras库以Python编写。 <br><a name="habracut"></a><br><h2>  1.培训数据 </h2><br>  SemEval-2019的曲目“ EmoContext”致力于在文本对话中定义情感，同时考虑了对应的上下文。 在这种情况下，上下文是对话参与者的几个连续的发言。 对话中有两个参与者：一个匿名用户（他拥有第一个和第三个副本）和一个聊天机器人<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Ruuh</a> （他拥有第二个副本）。 基于三个副本，有必要确定用户在向聊天机器人编写答案时经历了什么情绪（表1）。 总体而言，数据集的标记包含四种情绪：“快乐”，“悲伤”，“愤怒”或“其他”（表1）。 此处提供了详细描述:( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Chatterjee等，2019</a> ）。 <br><br>  <i>表1.来自EmoContext数据集的示例（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Chatterjee等，2019</a> ）</i> <br><div class="scrollable-table"><table><tbody><tr><th> 用户（阶段1） </th><th> 互动机器人（第一阶段） </th><th> 用户（阶段2） </th><th> 真正的课 </th></tr><tr><td> 我刚刚有资格参加纳巴德实习 </td><td> 哇！ 那真是个好消息。 恭喜你！ </td><td> 我开始哭了 </td><td> 幸福感 </td></tr><tr><td> 你怎么敢打我的孩子 </td><td> 如果你把我的车弄坏了，我也会帮你的 </td><td> 只需尝试一次 </td><td> 怒气 </td></tr><tr><td> 我更被你伤害了 </td><td> 你不是这个意思 </td><td> 说你爱我 </td><td> 悲伤 </td></tr><tr><td> 我会做的夜晚。 </td><td> 好吧 让我保持循环。 </td><td> 没有给出WhatsApp编号。 </td><td> 其他 </td></tr></tbody></table></div><br> 比赛期间，组织者提供了一些数据集。 训练数据集（训练）由30,160个手动标记的文本组成。 在这些文本中，大约有5000个属于“快乐”，“悲伤”和“愤怒”类别的对象，以及来自“其他”类别的15000个文本（表2）。 <br><br> 组织者还提供了用于开发（Dev）和测试（Test）的数据集，其中与训练数据集不同，情绪类别的分布与现实生活相对应：“快乐”，“悲伤”和“生气”，剩下的就是班上的“他人”。 由Microsoft提供的数据，您可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在LinkedIn上</a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方组中</a>下载。 <br><br>  <i>表2.情感类别标签在数据集中的分布（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Chatterjee et al。，2019</a> ）。</i> <br><div class="scrollable-table"><table><tbody><tr><th> 数据中心 </th><th> 幸福感 </th><th> 悲伤 </th><th> 怒气 </th><th> 其他 </th><th>合计 </th></tr><tr><td> 培训课程 <br></td><td>  14.07％ <br></td><td>  18.11％ <br></td><td>  18.26％ <br></td><td>  49.56％ <br></td><td>  30160 <br></td></tr><tr><td> 发展 <br></td><td>  5.15％ <br></td><td>  4.54％ <br></td><td>  5.45％ <br></td><td>  84.86％ <br></td><td>  2755 <br></td></tr><tr><td> 测验 <br></td><td>  5.16％ <br></td><td>  4.54％ <br></td><td>  5.41％ <br></td><td>  84.90％ <br></td><td>  5509 <br></td></tr><tr><td> 遥控 <br></td><td>  33.33％ <br></td><td>  33.33％ <br></td><td>  33.33％ <br></td><td>  0％ <br></td><td>  90万 <br></td></tr></tbody></table></div><br> 除了这些数据，我们还从Twitter收集了90万条英语消息，以创建一个远程数据集（每种情感30万条推文）。 创建它时，我们遵循Go等人的策略。  （2009年），在该框架中，消息仅与与情绪相关的单词的存在相关联，例如＃生气，＃烦恼，＃happy，＃sad，＃surprised等。 术语列表基于SemEval-2018 AIT DISC（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Duppada等人，2018</a> ）中的术语。 <br><br>  EmoContext竞赛中的主要质量指标是三种情绪类别（即“快乐”，“悲伤”和“愤怒”类别）的平均F1度量。 <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocessData</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataFilePath, mode)</span></span></span><span class="hljs-function">:</span></span> conversations = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(dataFilePath, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> finput: finput.readline() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> finput: line = line.strip().split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>): line[i] = tokenize(line[i]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: labels.append(emotion2label[line[<span class="hljs-number"><span class="hljs-number">4</span></span>]]) conv = line[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">4</span></span>] conversations.append(conv) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> mode == <span class="hljs-string"><span class="hljs-string">"train"</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations), np.array(labels) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(conversations) texts_train, labels_train = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/train.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_dev, labels_dev = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/dev.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>) texts_test, labels_test = preprocessData(<span class="hljs-string"><span class="hljs-string">'./starterkitdata/test.txt'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">"train"</span></span>)</code> </pre> <br><h2>  2.文本预处理 </h2><br> 在训练之前，我们使用Ekphrasis工具对文本进行了预处理（Baziotis等，2017）。 它有助于纠正拼写，规范单词，句段，并确定应使用特殊标签删除，规范或注释哪些标记。 在预处理阶段，我们执行了以下操作： <br><br><ul><li>  URL和邮件，日期和时间，昵称，百分比，货币和数字已替换为相应的标签。 </li><li> 重复，删节，拉长的大写术语，并附有适当的标签。 </li><li> 伸长的单词已自动更正。 </li></ul><br> 此外，Emphasis包含一个令牌识别器，可以识别大多数表情符号，表情符号和复杂表达式，以及日期，时间，货币和首字母缩写词。 <br><br>  <i>表3.文本预处理的示例。</i> <br><div class="scrollable-table"><table><tbody><tr><th> 源文字 </th><th> 预处理文字 </th></tr><tr><td> 我觉得你...我要分解成百万片 <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td><td>  &lt;allcaps&gt;我感觉到你&lt;/ allcaps&gt;。  &lt;重复&gt;我要分解成百万个 <img src="https://habrastorage.org/webt/2n/p4/l5/2np4l5uym3fkohcwlijjcma8eaw.png" width="100"></td></tr><tr><td> 累了，我也想念你:-( </td><td> 累了，我也想念你&lt;sad&gt; </td></tr><tr><td> 您应该使用以下<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网址</a> ： <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">www.youtube.com/watch?</a> v=99myH1orbs4 </td><td> 您应该听&lt;elongated&gt;这样：&lt;url&gt; </td></tr><tr><td> 我的公寓照顾它。 我的房租大约是650美元。 </td><td> 我的公寓照顾它。 我的租金在&lt;money&gt;左右。 </td></tr></tbody></table></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.preprocessor <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextPreProcessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.classes.tokenizer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SocialTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ekphrasis.dicts.emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> emoticons <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> io label2emotion = {<span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-string"><span class="hljs-string">"happy"</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-string"><span class="hljs-string">"sad"</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>: <span class="hljs-string"><span class="hljs-string">"angry"</span></span>} emotion2label = {<span class="hljs-string"><span class="hljs-string">"others"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">"happy"</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">"sad"</span></span>: <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-string"><span class="hljs-string">"angry"</span></span>: <span class="hljs-number"><span class="hljs-number">3</span></span>} emoticons_additional = { <span class="hljs-string"><span class="hljs-string">'(^・^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑c'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'=‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑)"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑('</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':\\/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'d=&lt;'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">':‑/'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">';‑]'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'(^ ^)'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'angru'</span></span>: <span class="hljs-string"><span class="hljs-string">'angry'</span></span>, <span class="hljs-string"><span class="hljs-string">"d‑':"</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":'‑("</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;sad&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">":‑["</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;annoyed&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'( ? )'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;happy&gt;'</span></span>, <span class="hljs-string"><span class="hljs-string">'x‑d'</span></span>: <span class="hljs-string"><span class="hljs-string">'&lt;laugh&gt;'</span></span>, } text_processor = TextPreProcessor( <span class="hljs-comment"><span class="hljs-comment"># terms that will be normalized normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'url', 'date', 'number'], # terms that will be annotated annotate={"hashtag", "allcaps", "elongated", "repeated", 'emphasis', 'censored'}, fix_html=True, # fix HTML tokens # corpus from which the word statistics are going to be used # for word segmentation segmenter="twitter", # corpus from which the word statistics are going to be used # for spell correction corrector="twitter", unpack_hashtags=True, # perform word segmentation on hashtags unpack_contractions=True, # Unpack contractions (can't -&gt; can not) spell_correct_elong=True, # spell correction for elongated words # select a tokenizer. You can use SocialTokenizer, or pass your own # the tokenizer, should take as input a string and return a list of tokens tokenizer=SocialTokenizer(lowercase=True).tokenize, # list of dictionaries, for replacing tokens extracted from the text, # with other expressions. You can pass more than one dictionaries. dicts=[emoticons, emoticons_additional] ) def tokenize(text): text = " ".join(text_processor.pre_process_doc(text)) return text</span></span></code> </pre><br><h2>  3.单词的向量表示 </h2><br> 向量表示已成为使用深度学习创建NLP系统的大多数方法的组成部分。 为了确定最合适的矢量映射模型，我们尝试使用Word2Vec（ <a href="">Mikolov等，2013</a> ），GloVe（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Pennington等，2014</a> ）和FastText（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Joulin等，2017</a> ），以及预先训练的DataStories向量（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Baziotis等。 。，2017</a> ）。  Word2Vec通过假设在相似的上下文中发现了语义相关的单词来查找单词之间的关系。  Word2Vec尝试预测目标单词（CBOW体系结构）或上下文（Skip-Gram体系结构），即最小化损失函数，而GloVe计算单词向量，从而减小邻接矩阵的维数。  FastText的逻辑与Word2Vec的逻辑相似，不同之处在于它使用符号n-gram来构建单词向量，从而可以解决未知单词的问题。 <br><br> 对于所有提到的模型，我们使用作者提供的默认训练参数。 我们基于这些向量表示中的每一个训练了一个简单的LSTM模型（dim = 64），并使用交叉验证比较了分类效率。 预先训练的DataStories向量显示了F1措施中的最佳结果。 <br><br> 为了丰富选择的向量映射以使词具有情感色彩，我们决定使用自动标记的Distant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">数据集</a>对向量进行微调（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Deriu et al。，2017</a> ）。 我们使用Distant数据集训练了一个简单的LSTM网络，以对“邪恶”，“悲伤”和“快乐”消息进行分类。 嵌入层在训练的第一个迭代过程中被冻结，以避免矢量权重的强烈变化，并且在接下来的五个迭代中，解冻该层。 训练后，“延迟的”向量将被保存以供以后在神经网络中使用，并被<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">共享</a> 。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(file)</span></span></span><span class="hljs-function">:</span></span> embeddingsIndex = {} dim = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> io.open(file, encoding=<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: values = line.split() word = values[<span class="hljs-number"><span class="hljs-number">0</span></span>] embeddingVector = np.asarray(values[<span class="hljs-number"><span class="hljs-number">1</span></span>:], dtype=<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) embeddingsIndex[word] = embeddingVector dim = len(embeddingVector) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingsIndex, dim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEmbeddingMatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(wordIndex, embeddings, dim)</span></span></span><span class="hljs-function">:</span></span> embeddingMatrix = np.zeros((len(wordIndex) + <span class="hljs-number"><span class="hljs-number">1</span></span>, dim)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wordIndex.items(): embeddingMatrix[i] = embeddings.get(word) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> embeddingMatrix <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer embeddings, dim = getEmbeddings(<span class="hljs-string"><span class="hljs-string">'emosense.300d.txt'</span></span>) tokenizer = Tokenizer(filters=<span class="hljs-string"><span class="hljs-string">''</span></span>) tokenizer.fit_on_texts([<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(list(embeddings.keys()))]) wordIndex = tokenizer.word_index print(<span class="hljs-string"><span class="hljs-string">"Found %s unique tokens."</span></span> % len(wordIndex)) embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim)</code> </pre><br><h2>  4.神经网络架构 </h2><br> 递归神经网络（RNN）是专门处理一系列事件的一系列神经网络。 与传统的神经网络不同，RNN被设计为使用内部天平处理序列。 为此，计算图RNN包含反映事件序列中先前信息对当前事件的影响的周期。  LSTM神经网络（长期短期记忆）是RNN在1997年的扩展（ <a href="">Hochreiter和Schmidhuber，1997年</a> ）。 连接LSTM递归池可避免爆发和褪色问题。 传统LSTM仅在沿一个方向处理序列时保留过去的信息。 双向运行的双向LSTM结合了两个隐藏的LSTM层的输出，它们在相反的方向上传输信息-一个在时间上，另一个在相反的方向上-从而同时从过去和将来的状态接收数据（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Schuster和Paliwal，1997年</a> ）。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bdf/d46/a41/bdfd46a41a20ba916382a57bb7c17e19.png"><br>  <i>图1：简化的体系结构版本。</i>  <i>LSTM模块在第一阶段和第三阶段使用相同的权重。</i> <br><br> 图1给出了所描述方法的简化表示。神经网络的体系结构由一个嵌入层和两个双向LTSM模块（dim = 64）组成。 第一LTSM模块分析第一用户的单词（即会话的第一和第三副本），第二模块分析第二用户的单词（第二副本）。 在第一阶段，使用预先训练的矢量表示的每个用户的单词被馈送到相应的双向LTSM模块中。 然后将得到的三个特征图组合成一个平面特征向量，然后转移到完全连接的隐藏层（dim = 30），该层分析提取的特征之间的相互作用。 最后，使用softmax激活函数在输出层中处理这些特性，以确定最终的类别标签。 为了减少过度拟合，在向量表示的各层之后，添加了具有高斯噪声的正则化层，并为每个LTSM模块（p = 0.2）和一个隐藏的全连接层（p = 0.1）添加了缺失层（ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Srivastava et al。，2014</a> ） <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Dense, Embedding, Concatenate, Activation, \ Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">buildModel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, noise=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout_lstm=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dropout=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> turn1_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn2_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) turn3_input = Input(shape=(sequence_length,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) embedding_dim = embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] embeddingLayer = Embedding(embeddings_matrix.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], embedding_dim, weights=[embeddings_matrix], input_length=sequence_length, trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) turn1_branch = embeddingLayer(turn1_input) turn2_branch = embeddingLayer(turn2_input) turn3_branch = embeddingLayer(turn3_input) turn1_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn1_branch) turn2_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn2_branch) turn3_branch = GaussianNoise(noise, input_shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, sequence_length, embedding_dim))(turn3_branch) lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm)) turn1_branch = lstm1(turn1_branch) turn2_branch = lstm2(turn2_branch) turn3_branch = lstm1(turn3_branch) x = Concatenate(axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)([turn1_branch, turn2_branch, turn3_branch]) x = Dropout(dropout)(x) x = Dense(hidden_layer_dim, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) output = Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_layer_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, num_classes=<span class="hljs-number"><span class="hljs-number">4</span></span>)</code> </pre> <br><h2>  5.结果 </h2><br> 在寻找最佳架构时，我们不仅尝试了层中神经元的数量，激活函数和规则化参数，还尝试了神经网络本身的架构。 这在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">原始工作</a>中有更详细的描述。 <br><br> 上一节中描述的体系结构在Train数据集上进行训练并在Dev数据集上进行验证时显示出最佳结果，因此已在比赛的最后阶段使用。 在最后一个测试数据集中，该模型显示的平均F1测度为72.59％，所有参与者中最大的成功率为79.59％。 但是，我们的结果远远高于组织者设定的基准值58.68％。 <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">单词的模型和矢量表示的源代码</a>可在GitHub上获得。 <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文章的完整版本</a>以及<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">与任务描述一起使用的信息</a>均位于ACL Anthology网站上。 <br> 可以从官方LinkedIn组下载<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">培训数据集</a> 。 <br><br> 报价单： <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@inproceedings{smetanin-2019-emosense, title = "{E}mo{S}ense at {S}em{E}val-2019 Task 3: Bidirectional {LSTM} Network for Contextual Emotion Detection in Textual Conversations", author = "Smetanin, Sergey", booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation", year = "2019", address = "Minneapolis, Minnesota, USA", publisher = "Association for Computational Linguistics", url = "https://www.aclweb.org/anthology/S19-2034", pages = "210--214", }</span></span></code> </pre> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN463045/">https://habr.com/ru/post/zh-CN463045/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN463031/index.html">生活和学习。 第3部分。继续教育或永恒的学生年龄</a></li>
<li><a href="../zh-CN463035/index.html">OpenStreetMap第471号世界的新闻（07.23.2019-29.07.2019）</a></li>
<li><a href="../zh-CN463037/index.html">寻找灵感，或如何使自己摆脱F</a></li>
<li><a href="../zh-CN463039/index.html">自己动手修指甲吸尘器</a></li>
<li><a href="../zh-CN463041/index.html">定义还是未定义？ 在JavaScript中创建数组的细节</a></li>
<li><a href="../zh-CN463055/index.html">关于公司内部的管理员，开发人员，无休止的混乱和DevOps转型</a></li>
<li><a href="../zh-CN463057/index.html">Yii Framework 2自定义权限</a></li>
<li><a href="../zh-CN463059/index.html">IT中的三者</a></li>
<li><a href="../zh-CN463061/index.html">在Figma中准备布局的规则</a></li>
<li><a href="../zh-CN463063/index.html">我们在Go中处理接口</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>