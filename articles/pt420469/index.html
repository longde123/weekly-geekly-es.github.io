<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüë©‚Äçüëß‚Äçüë¶ ‚èÆÔ∏è üë©üèº‚Äçüåæ Recursos do uso do Druid no exemplo de colegas de classe üà≥ üé± ü§ï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Meu nome √© Yuri Nevinitsin e estou envolvido no sistema de estat√≠sticas internas em OK. Quero falar sobre como transferimos um sistema anal√≠tico de 50...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Recursos do uso do Druid no exemplo de colegas de classe</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/420469/"><img src="https://habrastorage.org/webt/jd/0-/fd/jd0-fdngd2psn0-j-hkq1gmcm_a.jpeg"><br><br>  Meu nome √© Yuri Nevinitsin e estou envolvido no sistema de estat√≠sticas internas em OK.  Quero falar sobre como transferimos um sistema anal√≠tico de 50 terabytes em tempo real, no qual bilh√µes de eventos s√£o registrados diariamente, do Microsoft SQL para uma base de colunas chamada Druid.  E ao mesmo tempo, voc√™ aprender√° algumas receitas para usar o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Druid</a> . <br><a name="habracut"></a><br><h2>  Por que precisamos de estat√≠sticas? </h2><br>  Queremos saber tudo sobre o nosso site, para registrar n√£o apenas o comportamento de discos, processadores, etc., mas tamb√©m toda a√ß√£o do usu√°rio, toda intera√ß√£o entre subsistemas e todos os processos internos de quase todos os nossos sistemas.  O sistema estat√≠stico est√° intimamente integrado ao processo de desenvolvimento. <br><br>  Com base nos dados do sistema estat√≠stico, nossos gerentes estabelecem metas para as equipes, acompanham seus resultados e indicadores-chave.  Administradores e desenvolvedores monitoram a opera√ß√£o de todos os sistemas, investigam incidentes e anomalias.  O monitoramento autom√°tico monitora constantemente e, em um est√°gio inicial, identifica problemas, faz previs√µes de limites excedentes.  Al√©m disso, recursos e experimentos s√£o constantemente lan√ßados, atualiza√ß√µes e altera√ß√µes s√£o feitas.  E monitoramos o efeito de todas essas a√ß√µes atrav√©s do sistema de estat√≠sticas.  Se ela recusar, n√£o poderemos fazer altera√ß√µes no site. <br><br>  Nossas estat√≠sticas s√£o apresentadas principalmente na forma de gr√°ficos.  Normalmente, o gr√°fico √© exibido v√°rios dias ao mesmo tempo, para que a din√¢mica seja clara.  Aqui est√° um exemplo de minhas experi√™ncias com o Druid.  Aqui est√° um gr√°fico do carregamento de dados (linhas / 5 min). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9b4/66b/1b1/9b466b1b191463f97dd3d192a8c4da42.jpg"><br><br>  Reduzi a velocidade do download (o gr√°fico vermelho cai para zero), esperei um pouco, reiniciei o download e observei a rapidez com que o Druid podia carregar os dados acumulados (picos ap√≥s falhas). <br><br>  Qualquer programa√ß√£o pode ser expandida por qualquer par√¢metro, por exemplo, por host, tabela, opera√ß√£o, etc.  Tamb√©m temos gr√°ficos de longo prazo com din√¢mica anual.  Por exemplo, abaixo est√° um gr√°fico do aumento di√°rio no n√∫mero de entradas no Druid. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d9d/b52/45d/d9db5245dca91cc7cbdeee071b3529fa.png"><br><br>  Tamb√©m podemos combinar v√°rios gr√°ficos em pain√©is separados (pain√©is), o que acabou sendo muito conveniente.  E mesmo que o usu√°rio precise ver apenas algumas centenas de gr√°ficos, ele ainda os abre n√£o individualmente, mas no painel, o que aumenta a carga no sistema. <br><br><h2>  O problema </h2><br>  Enquanto o volume de dados era pequeno, lidamos muito bem com o SQL.  Mas √† medida que o volume de dados aumentou, a produ√ß√£o de gr√°ficos diminuiu.  E, no final, as estat√≠sticas na hora do rush come√ßaram a diminuir em meia hora, e o tempo m√©dio de resposta de um gr√°fico chegou a 6 segundos.  Ou seja, algu√©m recebeu a programa√ß√£o em 2 segundos, algu√©m em 10-20 e algu√©m em um minuto.  (Voc√™ pode ler sobre o desenvolvimento do sistema em SQL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ) <br><br>  Quando voc√™ investiga uma anomalia ou incidente, geralmente √© necess√°rio abrir e ver uma d√∫zia de gr√°ficos, cada um dos quais segue o anterior, eles n√£o podem ser abertos ao mesmo tempo.  Eu tive que esperar 10 vezes por 10-20 segundos.  Foi muito chato. <br><br><h2>  A migra√ß√£o </h2><br>  Voc√™ ainda pode extrair algo do sistema, adicionar servidores ... Mas, ao mesmo tempo, a Microsoft mudou sua pol√≠tica de licenciamento.  Se continu√°ssemos usando o SQL Server, ter√≠amos que doar milh√µes de d√≥lares.  Portanto, eles decidiram migrar. <br><br>  Os requisitos foram os seguintes: <br><br><ul><li>  As estat√≠sticas n√£o devem demorar (mais de 2 minutos). </li><li>  O gr√°fico deve abrir em n√£o mais de 2 segundos. </li><li>  O painel inteiro deve abrir em n√£o mais que 10 segundos. </li><li>  O sistema deve ser tolerante a falhas, capaz de sobreviver √† perda de um data center. </li><li>  O sistema deve ser facilmente escal√°vel. </li><li>  O sistema deve ser f√°cil de modificar, por isso, quer√≠amos que fosse em Java. </li></ul><br>  Tudo isso nos foi oferecido apenas por Druid.  Ele tamb√©m possui agrega√ß√£o preliminar, que permite economizar um pouco mais de volume e indexa√ß√£o durante a inser√ß√£o de dados.  O Druid suporta todos os tipos de consultas necess√°rias para nossas estat√≠sticas.  Portanto, parecia que poder√≠amos substituir facilmente o Druid pelo SQL Server. <br><br>  Obviamente, consideramos n√£o apenas o druida o papel de candidato √† mudan√ßa.  Meu primeiro pensamento foi substituir o Microsoft SQL Server pelo PostgreSQL.  No entanto, isso apenas resolveria o problema dos custos financeiros, mas n√£o ajudaria na acessibilidade e no dimensionamento. <br><br>  Tamb√©m analisamos o Influx, mas verificou-se que a parte respons√°vel pela alta disponibilidade e escalabilidade est√° fechada.  O Prometheus, com todo o devido respeito ao seu desempenho, √© mais otimizado para monitoramento e n√£o pode se orgulhar de alta disponibilidade ou simples escalabilidade.  O OpenTSDB tamb√©m √© mais adequado para monitoramento, pois n√£o possui √≠ndices para todos os campos.  N√£o consideramos a Click House, pois na √©poca ela n√£o estava l√°. <br><br>  Coloque Druida.  Terabytes de dados migrados.  E imediatamente ap√≥s a mudan√ßa do SQL Server para o Druid, o n√∫mero de visualiza√ß√µes de gr√°ficos foi aumentado 5 vezes.  Ent√£o eles come√ßaram a executar estat√≠sticas ‚Äúpesadas‚Äù, que eles tinham medo de executar mais cedo, porque  SQL dificilmente lidaria com isso. <br><br>  Agora, o Druid de 12 n√≥s (40 n√∫cleos, 196 GB de RAM) recebe 500 mil eventos por segundo por hora de pico, enquanto h√° uma grande margem de seguran√ßa (coluna MAX: quase cinco vezes a margem da CPU). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/833/336/d09/833336d095db6e9d32da19dae981d474.png" width="500"><br><br>  Esses n√∫meros s√£o baseados em dados de produ√ß√£o.  Vou contar como conseguimos isso, mas primeiro descreverei o Druid com mais detalhes. <br><br><h2>  Druida </h2><br>  Este √© um sistema OLAP de s√©ries temporais de colunas distribu√≠das.  Sua documenta√ß√£o n√£o cont√©m os conceitos usuais do mundo SQL para uma tabela (fonte de dados) ou uma string (evento), mas eu os utilizarei para facilitar a descri√ß√£o. <br><br>  O Druid √© baseado em v√°rias suposi√ß√µes de dados (limita√ß√µes): <br><br><ul><li>  cada linha de dados tem um carimbo de data / hora que cresce monotonamente (em uma janela de 10 minutos por padr√£o). </li><li>  os dados n√£o mudam, insira apenas (opera√ß√£o de atualiza√ß√£o n√£o). </li></ul><br>  Isso permite que voc√™ corte dados nos chamados segmentos de tempo.  Um segmento √© uma "parti√ß√£o" indivis√≠vel e invari√°vel m√≠nima de uma tabela por um determinado per√≠odo de tempo.  Todas as opera√ß√µes de dados, todas as consultas s√£o realizadas segmento por segmento. <br><br>  Cada segmento √© auto-suficiente: al√©m da tabela principal, escrita em forma de coluna, tamb√©m cont√©m diret√≥rios e √≠ndices necess√°rios para a execu√ß√£o da consulta.  Podemos dizer que um segmento √© um banco de dados somente leitura de coluna pequena (uma descri√ß√£o mais detalhada do dispositivo de segmento ser√° fornecida abaixo). <br><br>  Por sua vez, isso resulta em "distribui√ß√£o": a capacidade de dividir uma grande quantidade de dados em pequenos segmentos para realizar c√°lculos em paralelo (em uma m√°quina e em v√°rias ao mesmo tempo). <br><br>  Se voc√™ precisar "atualizar" pelo menos uma linha, precisar√° recarregar o segmento inteiro novamente.  √â poss√≠vel e est√° tudo pronto para isso.  Cada segmento tem uma vers√£o, e um segmento com uma vers√£o mais recente substitui automaticamente o segmento com a vers√£o antiga (no entanto, se a atualiza√ß√£o for necess√°ria regularmente, vale a pena reavaliar se o Druid √© adequado para essa caso de uso). <br><br>  Para descrever o segmento do dispositivo, consideramos um exemplo simples na forma tabular usual: <br><br><img src="https://habrastorage.org/webt/7l/pb/hw/7lpbhwalmq7ogn5qnaq2gjrkzua.png" width="600"><br><br>  Nesta tabela, o n√∫mero de chamadas em dois cinco minutos de quatro hosts (observe que, para o host web1, existem duas linhas em cada per√≠odo de cinco minutos). <br><br>  Todas as c√©lulas de dados do ponto de vista do druida s√£o divididas em tr√™s tipos: <br><br><ul><li>  registro de data e hora - registro de data e hora UTC em ms (no exemplo, √© hora). </li><li>  m√©tricas √© o que voc√™ precisa calcular (soma, min, max, contagem, ...) e precisa conhec√™-las com anteced√™ncia para cada tabela (no exemplo, s√£o chamadas e calcularemos a soma). </li><li>  dimens√µes - √© isso que voc√™ pode agrupar e filtrar (voc√™ n√£o precisa conhec√™-las com anteced√™ncia e pode ser alterado rapidamente) (no exemplo, esse √© Host). </li></ul><br>  Ao inserir, todas as linhas s√£o agrupadas pelo conjunto completo de dimens√µes + registro de data e hora e, se corresponderem a cada uma das m√©tricas, a fun√ß√£o de agrega√ß√£o "its" ser√° aplicada (como resultado, n√£o haver√° linhas com o mesmo conjunto de dimens√µes + registro de data e hora).  Assim, nosso exemplo ap√≥s a inser√ß√£o no druida ficar√° assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb8/8b9/e64/eb88b9e64bdafc870ca39646e4c0a4fb.png" width="500"><br><br>  O carimbo de data e hora e todas as m√©tricas (no nosso caso, s√£o horas e chamadas) ser√£o gravadas como matrizes de n√∫meros do tipo long (float e double tamb√©m s√£o suportados).  Para cada uma das dimens√µes (no nosso caso, √© Host), um dicion√°rio ser√° criado - um conjunto classificado de strings (com nomes de host).  A pr√≥pria coluna host ser√° escrita como uma matriz int, indicando os n√∫meros no dicion√°rio. <br><br>  Observe que, ap√≥s a inser√ß√£o no druida, pares de linhas para o host web1 com o mesmo registro de data e hora foram agregados e a quantidade total foi registrada nas chamadas (√© imposs√≠vel extrair os dados iniciais do druida). <br><br>  Os √≠ndices s√£o necess√°rios para a filtragem r√°pida de dados, porque pode haver milh√µes de linhas e milhares de hosts.  Os √≠ndices s√£o bitmaps, um para cada linha no dicion√°rio. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f32/bb5/edd/f32bb5eddba3c14b6ef241057f061724.png" width="500"><br><br>  As unidades indicam os n√∫meros de linha nos quais esse host participa.  Para filtrar dois hosts, voc√™ precisa pegar dois bitmaps, combin√°-los atrav√©s de OR e selecionar os n√∫meros de linha em unidades do bitmap resultante. <br><br><h2>  Um druida √© composto de muitos componentes. </h2><br>  Em primeiro lugar, possui v√°rias depend√™ncias externas. <br><br><img src="https://habrastorage.org/webt/rd/zs/uo/rdzsuo03og7v4whzfi5rz4m6qss.png" width="400"><br><br><ol><li>  Armazenamento  L√°, o Druid simplesmente armazena os segmentos em uma forma compactada.  Pode ser um diret√≥rio local, HDFS, Amazon S3.  Somente o espa√ßo √© usado aqui, nenhum c√°lculo √© feito. </li><li>  Meta: um banco de dados para informa√ß√µes meta.  Esse banco de dados armazena o mapa de dados completo: quais segmentos s√£o relevantes, quais est√£o desatualizados, qual caminho est√° armazenado. </li><li>  Usando o ZooKeeper, o sistema realiza a descoberta e anuncia em quais n√≥s do druida quais segmentos est√£o dispon√≠veis para consulta. </li><li>  Cache de solicita√ß√µes executadas, ele pode ser armazenado em cache ou local armazenado em cache no java. </li></ol><br>  Em segundo lugar, o pr√≥prio Druid consiste em v√°rios tipos de componentes. <br><br><ol><li>  Os n√≥s em tempo real carregam o fluxo de dados atualizados na ordem em que s√£o recebidos e atendem a solicita√ß√µes. </li><li>  Os n√≥s hist√≥ricos cont√™m toda a massa de dados e atendem a solicita√ß√µes.  Quando dizemos que temos um cluster de 300 TB, queremos dizer n√≥s hist√≥ricos. </li><li>  O Broker √© respons√°vel por distribuir c√°lculos entre n√≥s hist√≥ricos e em tempo real. </li><li>  O coordenador √© respons√°vel pela aloca√ß√£o de segmentos nos n√≥s hist√≥ricos e pela replica√ß√£o. </li><li>  Servi√ßo de indexa√ß√£o, que permite (re) carregar dados em lotes, por exemplo, para "atualizar" parte dos dados. </li></ol><br><h2>  Fluxo de dados </h2><br><img src="https://habrastorage.org/webt/gh/yg/pc/ghygpcnsakfac9x6dqrjtjjagdc.png" width="400"><br>  <i>Setas em negrito indicam um fluxo de dados, setas finas indicam um fluxo de metadados.</i> <br><br>  Um n√≥ em tempo real obt√©m dados, indexa e corta segmentos por hora, por exemplo, por dia. <br><br>  Cada novo segmento de um n√≥ em tempo real grava no armazenamento e deixa uma c√≥pia para atender a solicita√ß√µes.  Em seguida, ele registra os metadados que um novo segmento apareceu no reposit√≥rio ao longo desse caminho. <br><br>  Esta informa√ß√£o √© recebida pelo coordenador, relendo periodicamente a base de metadados.  Quando ele encontra um novo segmento, (atrav√©s do ZooKeeper) solicita v√°rios n√≥s hist√≥ricos para fazer o download desse segmento.  Eles fazem o download e (atrav√©s do ZooKeeper) anunciam que t√™m um novo segmento.  Quando um n√≥ em tempo real recebe essa mensagem (via ZooKeeper), ele exclui sua c√≥pia para liberar espa√ßo para novos dados. <br><br><h2>  Processamento de solicita√ß√£o </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c1f/8f5/e8f/c1f8f5e8fcc11c157c7ab10311ae977a.png"><br><br>  Tr√™s tipos de n√≥s participam do processamento de solicita√ß√µes: intermedi√°rio, em tempo real e hist√≥rico.  A solicita√ß√£o chega ao broker, que sabe em quais n√≥s quais segmentos est√£o localizados.  Ele distribui a solicita√ß√£o por n√≥s hist√≥ricos (e em tempo real) que armazenam os segmentos desejados.  Os n√≥s hist√≥ricos tamb√©m paralelizam os c√°lculos o m√°ximo poss√≠vel, enviam os resultados ao intermedi√°rio e ele os fornece ao cliente.  Ao combinar esse esquema com o armazenamento de dados da coluna, o Druid pode processar grandes quantidades de informa√ß√µes muito rapidamente. <br><br><h2>  Alta disponibilidade </h2><br>  Como voc√™ se lembra, o Druid na lista de depend√™ncias tem uma base para metadados, que podem ser MySQL ou PostgreSQL.  O Apache Derby tamb√©m √© mencionado, mas este produto n√£o pode ser usado para produ√ß√£o, apenas para desenvolvimento (como eu o entendo, o derby √© usado em uma forma incorporada, para n√£o aumentar o mysql / pgsql em um ambiente de virgem). <br><br>  O que acontecer√° se essa base falhar (e / ou armazenamento e / ou o coordenador)?  Um n√≥ em tempo real n√£o pode gravar metadados (e / ou segmentos).  O coordenador n√£o poder√° l√™-los novamente e n√£o encontrar√° um novo segmento.  O n√≥ Hist√≥rico n√£o far√° o download e o n√≥ em tempo real n√£o excluir√° sua c√≥pia, mas continuar√° a baixar os dados mais recentes.  Como resultado, os dados come√ßar√£o a se acumular nos n√≥s em tempo real.  Isso n√£o pode continuar indefinidamente.  No entanto, sabe-se quais recursos est√£o dispon√≠veis nos n√≥s em tempo real e que tipo de fluxo de dados temos.  Portanto, temos um tempo previs√≠vel pelo qual podemos consertar a base com falha (e / ou armazenamento e / ou coordenador). <br><br>  Como o mysql / pgsql suportado n√£o garante alta disponibilidade imediata, decidimos jogar com seguran√ßa e usamos nossa pr√≥pria solu√ß√£o (pronta) baseada em Cassandra, uma vez que pronta para uso fornece alta disponibilidade (voc√™ pode ler mais sobre isso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ). <br><br>  Al√©m disso, finalizamos os n√≥s em tempo real de forma que, com acumula√ß√£o excessiva, os dados mais antigos sejam exclu√≠dos, liberando espa√ßo para novos.  Isso √© muito importante para n√≥s, porque a situa√ß√£o em que n√£o podemos aumentar a base com falha (e / ou armazenamento e / ou o coordenador) por um longo tempo e muitos dados acumulados √© provavelmente uma consequ√™ncia de um grande acidente.  E, neste momento, os dados mais recentes s√£o mais importantes. <br><br><h2>  Druida e ZooKeeper </h2><br>  Com o ZooKeeper, tudo fica melhor e pior.  Melhor porque o ZooKeeper em si √© tolerante a falhas, possui replica√ß√£o pronta para uso.  Parece que isso poderia acontecer? <br><br>  De um modo geral, este cap√≠tulo n√£o √© mais relevante.  E isso n√£o √© uma hist√≥ria de sucesso, √© uma dor que (n√≥s e o novo druida) decidimos remover radicalmente quase todos os dados do ZooKeeper, e agora os n√≥s do druida os solicitam diretamente um do outro via HTTP. <br><br>  O ZooKeeper tem dois tipos de tempos limite.  O tempo limite da conex√£o √© um tempo limite da rede simples, ap√≥s o qual o cliente se reconecta ao ZooKeeper e tenta restaurar sua sess√£o.  E o tempo limite da sess√£o, ap√≥s o qual a sess√£o √© exclu√≠da e todos os dados <i>ef√™meros</i> criados nessa sess√£o tamb√©m s√£o exclu√≠dos (pelo pr√≥prio ZooKeeper), que √© notificado a todos os outros clientes do ZooKeeper. <br><br>  Com base nisso, a descoberta no druid funciona: na inicializa√ß√£o, cada n√≥ cria uma nova sess√£o no ZooKeeper e registra dados <i>ef√™meros</i> sobre si: host: porta, tipo de n√≥ (broker / realtime / hist√≥rico / ...), registro de data e hora da conex√£o, etc. ... Outros n√≥s de druida recebem notifica√ß√µes do ZooKeeper e leem esses dados, para saberem que um novo n√≥ de druida aumentou e que tipo de n√≥ √©.  Se algum n√≥ do druida cair ap√≥s o tempo limite da sess√£o, os dados sobre ele ser√£o exclu√≠dos pelo ZooKeeper, e os outros n√≥s do druida saber√£o sobre ele.  Para que eles aprendam mais r√°pido, preferimos colocar um pequeno tempo limite de sess√£o. <br><br>  Quando um n√≥ em tempo real ou hist√≥rico aumenta, ele, al√©m de dados sobre si mesmo, tamb√©m grava no ZooKeeper uma lista de segmentos que possui (tamb√©m s√£o dados <i>ef√™meros</i> ).  Mais adiante, segmentos em tempo real e n√≥s hist√≥ricos s√£o criados, novos e antigos s√£o exclu√≠dos, e cada n√≥ reflete isso em sua lista no ZooKeeper.  Essa lista pode ser grande e, portanto, √© dividida em partes, para que n√£o a lista inteira seja substitu√≠da, mas apenas a parte modificada. <br>  O Broker, por sua vez, quando v√™ um novo n√≥ em tempo real ou hist√≥rico, tamb√©m subtrai sua lista de segmentos do ZooKeeper para distribuir solicita√ß√µes para esse n√≥.  Os n√≥s em tempo real leem esta lista para remover sua c√≥pia do segmento que apareceu no n√≥ hist√≥rico.  Como a lista √© dividida em partes e √© sobrescrita em partes, o ZooKeeper informar√° qual parte foi alterada, apenas ser√° lida novamente. <br><br>  Como eu disse, essa lista pode ser longa.  Quando h√° muitos dados no ZooKeeper, verifica-se que ele n√£o √© mais t√£o est√°vel.  No nosso caso, problemas √≥bvios come√ßaram quando o n√∫mero de segmentos alcan√ßou cerca de 7 milh√µes, o snapshot do ZooKeeper ocupou 6 GB. <br>  O que acontece se um n√≥ druida perde contato com o ZooKeeper? <br><br>  O Druid trabalha com o ZooKeeper de tal maneira que, no caso de um tempo limite de sess√£o, cada n√≥ cria uma nova sess√£o e grava todos os seus dados l√° e rel√™ os dados de outros n√≥s.  Como existem muitos dados, o tr√°fego decola no ZooKeeper.  Isso pode levar a um tempo limite em outros n√≥s do druida, e eles tamb√©m come√ßam a reescrever e reler.  Assim, o tr√°fego cresce como uma avalanche at√© o ponto em que o ZooKeeper perde a sincroniza√ß√£o entre suas inst√¢ncias e come√ßa a gerar instant√¢neos para frente e para tr√°s. <br><br>  O que o usu√°rio v√™ neste momento? <br><br>  Quando um broker perde contato com o ZooKeeper (e ocorre um tempo limite de sess√£o), ele n√£o sabe mais quais segmentos em que n√≥s hist√≥ricos est√£o.  E d√° respostas vazias.  Ou seja, se o ZooKeeper estiver inativo, o Druid n√£o funcionar√°.  √â completamente imposs√≠vel "cur√°-lo", mas √© poss√≠vel espalhar canudos em alguns lugares. <br>  Em primeiro lugar, voc√™ pode excluir dados do ZooKeeper.  Tudo bem se eles se perderem: o Druid simplesmente os substituir√°.  Se o problema com o ZooKeeper j√° tiver come√ßado, ent√£o, para obter a solu√ß√£o mais r√°pida, √© recomend√°vel desativar o ZooKeeper, excluir os dados e aument√°-los vazios, e n√£o espere que ele se resolva. <br><br>  Agora estamos aumentando o tempo limite da sess√£o.  O que acontece neste caso? <br><br>  Digamos que o n√≥ hist√≥rico n√£o foi reiniciado corretamente e n√£o excluiu a sess√£o antiga do ZooKeeper, criando um novo e gravando um monte de dados l√°.  Enquanto a sess√£o antiga ainda est√° ativa e o tempo limite n√£o passou, duas c√≥pias dos dados s√£o armazenadas no ZooKeeper.  Se houver muitos desses n√≥s reiniciados imediatamente, muitos dados ser√£o duplicados.  Portanto, voc√™ precisa manter um suprimento de mem√≥ria para o ZooKeeper para que ele n√£o se esgote e o ZooKeeper n√£o pare de funcionar.  Por que n√£o foi poss√≠vel excluir os dados da sess√£o antiga? <br><br>  Pelo mesmo motivo, √© necess√°rio concluir corretamente a opera√ß√£o dos n√≥s hist√≥ricos, pois nesse momento eles excluem seus dados do ZooKeeper e podem fazer isso por um longo tempo.  A conclus√£o dos n√≥s hist√≥ricos leva cerca de meia hora. <br><br>  N√≥s hist√≥ricos t√™m mais um recurso.  Quando eles iniciam, examinam quais segmentos est√£o armazenados neles e as informa√ß√µes sobre isso s√£o gravadas no ZooKeeper.  E como os dados est√£o espalhados de maneira mais ou menos uniforme entre os n√≥s hist√≥ricos, se voc√™ os executar ao mesmo tempo, eles come√ßar√£o a gravar no ZooKeeper aproximadamente ao mesmo tempo.  Isso aumenta novamente a probabilidade de crescimento e tempos limite do tr√°fego semelhante a ondas.  Portanto, voc√™ precisa executar n√≥s hist√≥ricos em seq√º√™ncia para distribuir as sess√µes de grava√ß√£o no ZooKeeper no tempo. <br><br>  Tamb√©m fizemos mais duas otimiza√ß√µes: <br><br><ul><li>  N√≥s reprogramamos o trabalho com o ZooKeeper um pouco para que apenas os n√≥s que precisam deles sejam lidos no Druid.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E eles s√£o necess√°rios apenas em tempo real, o intermedi√°rio e o coordenador, mas n√£o por n√≥s hist√≥ricos. </font><font style="vertical-align: inherit;">Eles n√£o precisam saber quais outros n√≥s hist√≥ricos t√™m segmentos. </font><font style="vertical-align: inherit;">Al√©m disso, tudo isso n√£o √© necess√°rio para o servi√ßo de indexa√ß√£o e seus funcion√°rios, que podem ser muitos.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dos dados gravados no ZooKeeper, eles removeram tudo sup√©rfluo e deixaram apenas o necess√°rio para atender √†s solicita√ß√µes. </font><font style="vertical-align: inherit;">Isso reduziu a quantidade de dados no ZooKeeper de 6 GB para 2 GB (esse √© o tamanho do instant√¢neo).</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como resultado, o volume de tr√°fego em forte crescimento diminuiu cerca de 8 vezes; </font><font style="vertical-align: inherit;">assim, minimizamos a probabilidade de tempos limite dos f√£s.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Carregar no Druid </font></font></h2><br><img src="https://habrastorage.org/getpro/habr/post_images/2f4/71b/9a9/2f471b9a9927b4075e3dcc9bba8333cc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No processo de carregamento de dados em tempo real, o n√≥ periodicamente libera mem√≥ria liberando dados em partes para o disco. Tecnicamente, essas partes s√£o minissegmentos (cada um possui uma tabela, diret√≥rios, √≠ndices). E para processar solicita√ß√µes com base nesses dados, elas s√£o acessadas usando o MMAP (al√©m de segmentos completos). Ao final do carregamento, um segmento dessas pe√ßas est√° se acumulando bastante. Dois pontos est√£o conectados com isso. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Primeiro, um n√≥ em tempo real pode corromper os dados, n√£o apenas durante uma falha na JVM ou uma reinicializa√ß√£o inesperada do servidor, mas mesmo durante uma reinicializa√ß√£o correta.</font></font><br><br><img src="https://habrastorage.org/webt/on/uk/w1/onukw1xyab_eojgwxynlsxjr6qc.png" width="500"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√â por isso que isso est√° acontecendo. O processo de libera√ß√£o de dados no disco consiste em duas partes: 1) libera√ß√£o direta de dados e 2) manuten√ß√£o da posi√ß√£o a partir da qual iniciar ap√≥s uma reinicializa√ß√£o. Esses dois tipos de dados s√£o gravados de forma completamente independente, eles n√£o sabem nada um do outro. E, claro, n√£o atomicamente. E, dependendo do que exatamente √© perdido, temos perda ou duplica√ß√£o de dados. (No momento, no druida original, isso est√° sendo reparado ativamente, mas n√£o reparado). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Esse problema pode ser resolvido se voc√™ n√£o usar n√≥s em tempo real e carregar dados usando o servi√ßo de indexa√ß√£o ou us√°-los em pares, porque o servi√ßo de indexa√ß√£o n√£o salva a posi√ß√£o, carrega o segmento inteiro ou descarta o que n√£o foi carregado (por qualquer motivo).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O segundo ponto √© a degrada√ß√£o do desempenho sob demanda. </font><font style="vertical-align: inherit;">Quanto mais essas partes se acumulam no disco, especialmente em dados e consultas pesadas, pior. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para entender esse problema, precisamos retornar ao segmento de dispositivo no Druid e nosso exemplo. </font><font style="vertical-align: inherit;">Como mostrei anteriormente, depois de carregar os dados do exemplo no druida, o segmento resultante parecer√° um conjunto de colunas, um dicion√°rio e √≠ndices para ele. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f32/bb5/edd/f32bb5eddba3c14b6ef241057f061724.png" width="500"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora vamos ver como funciona a consulta neste segmento. </font><font style="vertical-align: inherit;">Suponha que voc√™ precise calcular o n√∫mero total de chamadas para v√°rias classes de hosts (web%, api%).</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O Druid primeiro pega o primeiro filtro, uma express√£o regular. </font><font style="vertical-align: inherit;">Com sua ajuda, processa o dicion√°rio inteiro e encontra hosts que correspondem ao filtro.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ele pegar√° os bitmaps apropriados, mesclar√° e salvar√° em um bitmap intermedi√°rio. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ent√£o o Druid far√° a segunda temporada regular, o segundo filtro, fa√ßa o mesmo: percorra o dicion√°rio, pegue os bitmaps, combine, obtenha o segundo bitmap intermedi√°rio. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> No final do Druid, o pacote resultante de bitmaps intermedi√°rios √© combinado em um bitmap final, que mostra de quais linhas precisamos somar as chamadas. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Usando o criador de perfil, descobri que, ao processar uma solicita√ß√£o, 5% do tempo √© gasto no c√°lculo da quantidade e 95% s√£o gastos na filtragem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora vamos ver o que acontece quando um n√≥ em tempo real libera os dados aos poucos no disco no momento da inicializa√ß√£o. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/76b/30d/618/76b30d618474a0cd9afdf6aa40af6b90.png" width="500"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Come√ßamos a baixar dados, despejamos parte (√†s 10:45) no disco. </font><font style="vertical-align: inherit;">O resultado foi um mini-segmento com tr√™s colunas, um dicion√°rio e √≠ndices de bitmap. </font><font style="vertical-align: inherit;">Fazemos o download adicional, redefinimos a segunda parte (√†s 10:50) para o disco, novamente temos um mini-segmento.</font></font> E assim por diante<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se analisarmos em partes, perceberemos que as colunas "chamadas", "hora" e "host" nessas partes foram cortadas proporcionalmente. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas com o dicion√°rio e os √≠ndices, sai de maneira diferente. Cada host uma vez a cada cinco minutos limpa seus dados, para que todos os hosts sejam "marcados" em cada parte do fluxo para o disco. O dicion√°rio √© igualmente grande, n√£o √© cortado de forma alguma e existem tantos √≠ndices nele. Ao processar uma solicita√ß√£o, a itera√ß√£o no dicion√°rio e a combina√ß√£o de bitmaps (que levam 95% do tempo) precisam ser executadas para cada uma das partes; portanto, a depend√™ncia √© quase linear: quanto mais partes, mais a solicita√ß√£o dura. Isso quase n√£o √© percept√≠vel enquanto estiver no dicion√°rio at√© 100 valores e ficar√° muito percept√≠vel desacelerar quando houver mais de 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O que pode ser feito sobre isso? Voc√™ pode controlar o n√∫mero de pe√ßas liberadas para o disco. Por exemplo, se voc√™ tiver um segmento di√°rio e as consultas estiverem diminuindo a velocidade nos n√≥s em tempo real, reduza-o para hora a hora. Em seguida, o n√∫mero de partes ser√° reduzido proporcionalmente (j√° que os dados ser√£o movidos mais rapidamente para os n√≥s hist√≥ricos e exclu√≠dos dos n√≥s em tempo real) e diminuir√£o proporcionalmente menos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Existem tamb√©m dois par√¢metros que permitem controlar a frequ√™ncia de libera√ß√£o dessas pe√ßas no disco: o n√∫mero m√°ximo de linhas na mem√≥ria e o intervalo de libera√ß√£o no disco. Por exemplo, voc√™ pode redefini-lo n√£o a cada cinco minutos, mas a cada meia hora. E n√£o a cada 100 mil linhas, mas a cada milh√£o. Ent√£o as pe√ßas ficar√£o menores e tudo funcionar√° muitas vezes mais r√°pido.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ainda h√° um ponto importante. </font><font style="vertical-align: inherit;">√Äs vezes, 80% do tempo gasto na filtragem leva um dicion√°rio a passar por express√µes regulares em vez de combinar bitmaps. </font><font style="vertical-align: inherit;">N√£o sab√≠amos disso e durante a migra√ß√£o todos os filtros foram feitos express√µes regulares. </font><font style="vertical-align: inherit;">Isto n√£o √© necess√°rio. </font><font style="vertical-align: inherit;">Quando filtramos pelo valor exato, devemos usar um filtro do tipo seletor, pois ele encontrar√° o valor desejado pela pesquisa bin√°ria e obter√° imediatamente o bitmap. </font><font style="vertical-align: inherit;">Isso funciona mil vezes mais r√°pido que o regex.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Otimiza√ß√£o da faixa de op√ß√µes </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como voc√™ sabe, em qualquer rede social, h√° um feed de eventos que coleta conte√∫do criado por todas as equipes de desenvolvimento. Claro, todas essas equipes querem assistir e escrever estat√≠sticas. Temos estat√≠sticas de fita escritas em um prato, 8 bilh√µes de linhas por dia. Ela freou mesmo em druida. E o pior √© que, quando diminuiu a velocidade, sobrecarregou todo o druida, ou seja, tudo diminuiu para todos. Nestas estat√≠sticas, havia um campo combinado, que consiste em v√°rias palavras conectadas atrav√©s de um ponto. Algo assim: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6d/917/10e/c6d91710e3436dc0feede42b3138baa0.png" width="500"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Podemos apreciar a foto principal, no √°lbum, no grupo. O mesmo vale para v√≠deo e m√∫sica. Tamb√©m podemos compartilhar fotos, v√≠deos e m√∫sicas na p√°gina principal, no √°lbum e no grupo. E podemos comentar sobre tudo. Um total de 27 combina√ß√µes de eventos. Assim, o dicion√°rio ter√° 27 linhas, 27 bitmaps.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Queremos calcular quantas curtidas houve. </font><font style="vertical-align: inherit;">Essa consulta passa por uma express√£o regular de 27 valores no dicion√°rio, seleciona 9 deles, obt√©m 9 bitmaps, combina e passa a contar. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora vamos cortar em tr√™s partes.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/710/585/fcb/710585fcb1dcf69e39a4a8e2faacc62f.png" width="500"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A primeira √© a a√ß√£o: curtir, compartilhar, comentar. </font><font style="vertical-align: inherit;">A segunda parte √© um objeto: foto, v√≠deo, m√∫sica. </font><font style="vertical-align: inherit;">A terceira parte √© o local: na parte principal, no √°lbum, no grupo. </font><font style="vertical-align: inherit;">Em seguida, a consulta entrar√° em apenas um dicion√°rio - uma a√ß√£o na qual existem apenas tr√™s valores e tr√™s bitmaps. </font><font style="vertical-align: inherit;">Para a pureza do experimento, suponha que essa tamb√©m seja uma express√£o regular. </font><font style="vertical-align: inherit;">Ou seja, neste caso, haver√° tr√™s express√µes regulares e, no anterior, havia 27. Havia 9 bitmaps, agora existe um. </font><font style="vertical-align: inherit;">Como resultado, reduzimos a passagem do dicion√°rio e a combina√ß√£o de bitmaps (que leva 95% do tempo) em 9 vezes. </font><font style="vertical-align: inherit;">E acabamos de cortar um dicion√°rio de 27 linhas em tr√™s.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Na realidade, tivemos 14 mil combina√ß√µes. </font><font style="vertical-align: inherit;">Assim, em nosso dicion√°rio havia 14 mil valores e 14 mil bitmaps. </font><font style="vertical-align: inherit;">Como resultado, quando cortamos esse campo em pequenas partes, de acordo com as palavras, a velocidade das estat√≠sticas da fita aumentou 10 vezes e o tamanho dos dados foi reduzido pela metade. </font><font style="vertical-align: inherit;">Agora tudo funciona r√°pido.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Prioridades de solicita√ß√£o </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas aqui vem o usu√°rio e quer ver as estat√≠sticas do ano, isto √© 2 TB. No nosso cluster, voc√™ precisa aumentar 11 GB do disco, isso levar√° 74 segundos. O usu√°rio sabe que est√° solicitando dados pesados ‚Äã‚Äãe est√° pronto para aguardar. Mas o que os outros usu√°rios far√£o nesses 74 segundos? Para dizer o m√≠nimo, eles ficar√£o nervosos e perguntar√£o por que os gr√°ficos n√£o funcionam. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O Druid permite priorizar solicita√ß√µes. Tentamos diminuir a prioridade para dados pesados, ficou mais f√°cil, mas ainda diminuiu, porque as prioridades funcionam no n√≠vel da fila. Isso significa que, se parte da solicita√ß√£o pesada j√° tiver sido processada, todos ter√£o que esperar. Em seguida, solicita√ß√µes leves e r√°pidas avan√ßam e, novamente, solicita√ß√µes pesadas ocupam todos os recursos. H√° uma sensa√ß√£o de que o sistema est√° trabalhando duro, at√© o limite.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aproveitamos o fato de o Druid ter todas as informa√ß√µes sobre a solicita√ß√£o e os dados. Eles implementaram uma prioriza√ß√£o simples, que define a prioridade para o n√∫mero (em megabytes) de dados que essa solicita√ß√£o passar√°. Ao mesmo tempo, fizemos cinco filas: uma para as solicita√ß√µes mais dif√≠ceis, uma para as mais leves e tr√™s intermedi√°rias. Eles espalharam solicita√ß√µes para a prioridade calculada. Cada fila tem uma prioridade no n√≠vel do sistema operacional (definido por meios padr√£o e configura√ß√µes de java), portanto, solicita√ß√µes r√°pidas excluem as mais pesadas. Agora, finalmente, o Druid ganhou como voc√™ espera dele.</font></font><br><br><h2>  Sum√°rio </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementamos um sistema r√°pido, distribu√≠do e tolerante a falhas, em vez do SQL Server, e n√£o doamos v√°rios milh√µes de d√≥lares para a Microsoft. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temos f√°cil escalabilidade horizontal, voc√™ s√≥ precisa adicionar discos e / ou servidores, conforme necess√°rio. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temos uma grande margem de desempenho para inserir dados e consultas e a capacidade de detalhar nossas estat√≠sticas por uma ordem de magnitude. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Atualmente, temos mais de 20 tabelas, cada uma das quais grava mais de um bilh√£o de linhas por dia, a maior grava 18 bilh√µes de linhas por dia. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nosso druida √© quase totalmente transferido para uma nuvem ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://habr.com/company/odnoklassniki/blog/346868/</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), o que simplifica ainda mais o processo de dimensionamento.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt420469/">https://habr.com/ru/post/pt420469/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt420459/index.html">√çcone com um contador na barra de ferramentas superior: um exemplo de uma variedade de abordagens para uma tarefa</a></li>
<li><a href="../pt420461/index.html">10 cita√ß√µes de maus designers</a></li>
<li><a href="../pt420463/index.html">ICO merecidamente em decl√≠nio, mas eles t√™m a chance de mudar</a></li>
<li><a href="../pt420465/index.html">Vari√°veis ‚Äã‚ÄãNginx com njs: simples, indolor e via JavaScript</a></li>
<li><a href="../pt420467/index.html">Wrapper C ++ para "todos" sistemas operacionais em tempo real para CortexM4</a></li>
<li><a href="../pt420471/index.html">Tr√™s relat√≥rios t√©cnicos do RIT 2018 da Plesk</a></li>
<li><a href="../pt420473/index.html">Livros para executivos iniciantes ou por que √© t√£o importante ler</a></li>
<li><a href="../pt420475/index.html">Compara√ß√£o das m√°quinas de corte a laser Raylogic 11G e Raylogic V12</a></li>
<li><a href="../pt420477/index.html">Revis√£o do HyperX Cloud Stinger Core: um headset leve e resistente para personagens</a></li>
<li><a href="../pt420479/index.html">Inje√ß√£o de depend√™ncia no servi√ßo Apache Ignite.NET</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>