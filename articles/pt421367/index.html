<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëèüèæ üë≤üèº ü§Ø AI, curso pr√°tico. Configurando o modelo e os hiperpar√¢metros para reconhecer emo√ß√µes nas imagens üòû üé† üî£</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nos artigos anteriores desta s√©rie de treinamentos, foram descritas poss√≠veis op√ß√µes para a prepara√ß√£o dos dados: pr√©-processamento e adi√ß√£o de dados ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI, curso pr√°tico. Configurando o modelo e os hiperpar√¢metros para reconhecer emo√ß√µes nas imagens</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/421367/"><img src="https://habrastorage.org/webt/zq/7s/el/zq7selxswjsrmplg_pxw2xilqi4.jpeg"><br><br>  Nos artigos anteriores desta s√©rie de treinamentos, foram descritas poss√≠veis op√ß√µes para a prepara√ß√£o dos dados: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√©-processamento e adi√ß√£o de dados com imagens</a> ; nesses artigos, tamb√©m foi constru√≠do o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo Base para reconhecimento de emo√ß√µes com</a> base em imagens de uma rede neural convolucional. <br>  Neste artigo, construiremos um modelo de rede neural convolucional aprimorado para reconhecer emo√ß√µes em imagens usando uma t√©cnica chamada <i>aprendizado indutivo</i> . <br><a name="habracut"></a><br>  Primeiro, voc√™ precisa se familiarizar com o artigo sobre o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Modelo b√°sico para reconhecer emo√ß√µes em imagens</a> , tamb√©m pode consult√°-lo durante a leitura, pois algumas se√ß√µes, incluindo o estudo dos dados de origem e a descri√ß√£o dos indicadores de rede, n√£o ser√£o fornecidas aqui em detalhes. <br><br><h2>  <font color="#0071c5">Dados</font> </h2><br>  O conjunto de dados cont√©m 1630 imagens com emo√ß√µes de duas classes: <i>Negativo</i> (classe 0) e <i>Positivo</i> (classe 1).  Alguns exemplos dessas imagens s√£o apresentados abaixo. <br><br>  <b>Negativo</b> <br><img src="https://habrastorage.org/webt/zr/pf/ki/zrpfkiqvgcxw6kpsv777v9d1t6w.jpeg"><br><br><img src="https://habrastorage.org/webt/52/8x/s6/528xs6k7jnimfgvhagwolru8mi4.jpeg"><br><br><img src="https://habrastorage.org/webt/no/p0/vb/nop0vbapr4ep7o5dps1wvpkncoa.jpeg"><br><br>  <b>Positivo</b> <br><img src="https://habrastorage.org/webt/uc/ua/oh/ucuaohklcwcgotqf4uryopnt_qg.jpeg"><br><br><img src="https://habrastorage.org/webt/u5/hp/rb/u5hprb1cjig28b4xk8urdljb8lm.jpeg"><br><br><img src="https://habrastorage.org/webt/ru/_a/xo/ru_axoahw4h4xytx_-yphxk56ii.jpeg"><br><br>  Alguns dos exemplos cont√™m emo√ß√µes positivas ou negativas √≥bvias, enquanto outros podem n√£o ser categorizados - mesmo com o envolvimento humano.  Com base em uma inspe√ß√£o visual desses casos, estimamos que a precis√£o m√°xima poss√≠vel seja de cerca de 80%.  Observe que um classificador aleat√≥rio fornece aproximadamente 53% de precis√£o devido a um pequeno desequil√≠brio nas classes. <br><br>  Para treinar o modelo, usamos a t√©cnica de <i>reter parte das amostras</i> e dividir o conjunto de dados inicial em duas partes, uma das quais (20% do conjunto inicial) ser√° usada por n√≥s para verifica√ß√£o.  O particionamento √© realizado usando <i>estratifica√ß√£o</i> : isso significa que o equil√≠brio entre as classes √© mantido nos conjuntos de treinamento e teste. <br><br><h2>  <font color="#0071c5">Resolvendo Insufici√™ncia de Dados</font> </h2><br>  O modelo b√°sico apresentou resultados apenas ligeiramente melhores que as previs√µes aleat√≥rias da classe de imagens.  Pode haver muitos motivos poss√≠veis para esse comportamento.  Acreditamos que o principal motivo √© que a quantidade de dados dispon√≠vel √© decididamente insuficiente para esse treinamento da parte convolucional da rede que permitiria obter caracter√≠sticas caracter√≠sticas baseadas na imagem de entrada. <br>  Existem muitas maneiras diferentes de resolver o problema da insufici√™ncia de dados.  Aqui est√£o alguns deles: <br><br><ul><li>  <b>Buscar novamente</b> .  A id√©ia do m√©todo √© avaliar a distribui√ß√£o de dados e selecionar <i>novos exemplos</i> dessa distribui√ß√£o. </li><li>  <b>Aprendendo sem professor</b> .  Todos podem encontrar grandes quantidades de dados da mesma natureza que exemplos marcados em um determinado conjunto de dados.  Por exemplo, podem ser filmes para reconhecimento de v√≠deo ou audiolivros para reconhecimento de fala.  O pr√≥ximo passo √© usar esses dados para pr√©-treinar o modelo (por exemplo, usando codificadores autom√°ticos). </li><li>  <b>Aumento de dados</b> .  Durante esse processo, os dados de amostra s√£o modificados aleatoriamente usando um determinado conjunto de transforma√ß√µes. </li><li>  <b>Aprendizagem indutiva</b> .  Este t√≥pico √© de grande interesse para n√≥s, ent√£o vamos nos familiarizar com ele em mais detalhes. </li></ul><br><h2>  <font color="#0071c5">Aprendizagem indutiva</font> </h2><br>  O termo <i>treinamento indutivo</i> refere-se a um conjunto de t√©cnicas usando modelos (geralmente muito grandes) treinados em diferentes conjuntos de dados de aproximadamente a mesma natureza. <br><br><img src="https://habrastorage.org/webt/wl/jb/qi/wljbqidbfmpj1yfddtvjlkqt9ma.png"><br><br><img src="https://habrastorage.org/webt/bq/ji/ah/bqjiahabshkakrv2jcilp5pa1y8.png"><br><br>  Compara√ß√£o de m√©todos tradicionais de aprendizado de m√°quina e aprendizado indutivo.  Imagem retirada da entrada de blog de S. Ruder <i>"O que √© aprendizado indutivo?"</i>  . <br>  Existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tr√™s</a> cen√°rios principais para o uso da aprendizagem indutiva: <br><br><ul><li>  <b>Modelos pr√©-treinados</b> .  Qualquer usu√°rio pode simplesmente pegar um modelo treinado por outra pessoa e us√°-lo para suas tarefas.  Esse cen√°rio √© poss√≠vel se as tarefas forem muito semelhantes. </li><li>  <b>Bloqueie a sele√ß√£o de sinais</b> .  Nesse ponto, sabemos que a arquitetura do modelo pode ser dividida em duas partes principais: a <i>unidade de extra√ß√£o de Recursos</i> , respons√°vel por extrair recursos dos dados de entrada, e o <i>m√≥dulo de classifica√ß√£o</i> , que classifica exemplos com base nos recursos recebidos.  Normalmente, o bloco de extra√ß√£o de recursos √© a parte principal do modelo.  A id√©ia do m√©todo √© usar um bloco para distinguir recursos de um modelo treinado em outro problema, fixar seus coeficientes de peso (torn√°-los destreinados) e, ent√£o, criar novos m√≥dulos de classifica√ß√£o para o problema em considera√ß√£o.  O m√≥dulo de classifica√ß√£o geralmente n√£o √© muito profundo e consiste em v√°rias camadas totalmente conectadas, portanto esse modelo √© muito mais f√°cil de treinar. </li><li>  <b>Ajuste preciso e profundo</b> .  Este m√©todo √© como um cen√°rio usando um bloco de extra√ß√£o de recurso.  As mesmas a√ß√µes s√£o executadas, com exce√ß√£o do "congelamento" do bloco de extra√ß√£o do recurso.  Por exemplo, voc√™ pode considerar a rede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VGG</a> como um bloco de extra√ß√£o de recursos e "congelar" nela apenas os tr√™s primeiros (em quatro) blocos convolucionais.  Nesse caso, a unidade de extra√ß√£o de recursos pode se adaptar melhor √† tarefa atual.  Para obter mais informa√ß√µes, consulte a postagem no blog de F. Chollet.Crie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelos poderosos de classifica√ß√£o de imagens usando uma quantidade muito pequena de dados</a> . </li></ul><br>  Uma descri√ß√£o detalhada dos cen√°rios para o uso da aprendizagem indutiva pode ser encontrada nas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">redes neurais convolucionais do</a> curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CS231n</a> da Universidade de Stanford <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">para reconhecimento visual</a> por Fei-Fei Li e nas entradas do blog de S. Ruder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A aprendizagem indutiva √© a pr√≥xima fronteira em desenvolvimento aprendizado de m√°quina</a> (t√≥picos discutidos de forma mais abrangente). <br><br>  Voc√™ pode ter perguntas: por que todos esses m√©todos s√£o necess√°rios e por que eles podem funcionar?  Vamos tentar responder a eles. <br><br><ul><li>  Benef√≠cios do uso de grandes conjuntos de dados.  Por exemplo, podemos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">usar</a> o bloco de extra√ß√£o de recursos de um modelo treinado em 14 milh√µes de imagens contidas no conjunto de dados do concurso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ImageNet</a> .  Esses modelos s√£o complexos o suficiente para permitir a <i>extra√ß√£o de recursos de alta qualidade</i> dos dados de entrada. </li><li>  Considera√ß√µes relacionadas ao tempo.  O treinamento de modelos grandes pode levar semanas ou at√© meses.  Nesse caso, todos podem <i>economizar uma quantidade enorme de tempo e recursos de computa√ß√£o</i> . </li><li>  Uma suposi√ß√£o importante subjacente √† raz√£o pela qual tudo isso pode funcionar √© a seguinte: Os atributos obtidos com o treinamento em uma tarefa podem ser √∫teis e adequados para outra tarefa.  Em outras palavras, os recursos t√™m a propriedade de invari√¢ncia em rela√ß√£o ao problema.  Observe que o <i>dom√≠nio da</i> nova tarefa deve ser semelhante ao dom√≠nio da tarefa original.  Caso contr√°rio, a unidade de extra√ß√£o de recursos pode at√© piorar os resultados. </li></ul><br><h2>  <font color="#0071c5">Arquitetura de modelo aprimorada</font> </h2><br>  Agora estamos familiarizados com o conceito de aprendizado indutivo.  Tamb√©m sabemos que o ImageNet √© um evento importante, no qual quase todas as arquiteturas modernas de redes neurais convolucionais avan√ßadas foram testadas.  Vamos tentar tirar o bloco de extra√ß√£o de recursos de uma dessas redes. <br><br>  Felizmente, a biblioteca Keras fornece <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">v√°rios</a> modelos pr√©-treinados (atrav√©s do ImageNet) que foram criados dentro desta plataforma.  Importamos e usamos um desses modelos. <br><br><img src="https://habrastorage.org/webt/jz/3t/ry/jz3trysk11d88hbmbxvlnoungxy.png"><br><br>  Nesse caso, usaremos uma rede com arquitetura VGG.  Para selecionar apenas a unidade de extra√ß√£o do recurso, exclu√≠mos o m√≥dulo de classifica√ß√£o (as tr√™s principais camadas totalmente conectadas) da rede, definindo o par√¢metro <i>include_top</i> como <i>False</i> .  Tamb√©m queremos inicializar nossa rede usando os pesos da rede treinada no ImageNet.  O par√¢metro final √© o tamanho da entrada. <br><br>  Observe que o tamanho das imagens originais no concurso ImageNet √© (224, 224, 3), enquanto nossas imagens s√£o (400, 500, 3).  No entanto, usamos camadas convolucionais - isso significa que os pesos da rede s√£o os pesos dos n√∫cleos em movimento na opera√ß√£o de convolu√ß√£o.  Juntamente com a propriedade da separa√ß√£o de par√¢metros (uma discuss√£o sobre isso √© encontrada em nosso artigo te√≥rico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Vis√£o geral das redes neurais convolucionais para classificar imagens</a> ), isso leva ao fato de que o tamanho dos dados de entrada pode ser quase arbitr√°rio, uma vez que a convolu√ß√£o √© realizada por meio de uma janela deslizante, e essa janela pode deslizar ao longo imagem de qualquer tamanho.  A √∫nica limita√ß√£o √© que o tamanho dos dados de entrada deve ser grande o suficiente para n√£o colapsar em um ponto (medi√ß√µes espaciais) em alguma camada intermedi√°ria, porque, caso contr√°rio, ser√° imposs√≠vel fazer c√°lculos adicionais. <br><br>  Outro truque que usamos √© o <i>cache</i> .  VGG √© uma rede muito grande.  Uma passagem direta para todas as imagens (1630 exemplos) pela unidade de extra√ß√£o de recursos leva aproximadamente 50 segundos.  No entanto, deve-se lembrar que os pesos da unidade de extra√ß√£o de recursos s√£o fixos e uma passagem direta sempre fornece o mesmo resultado para a mesma imagem.  Podemos usar esse fato para executar uma passagem direta pela unidade de extra√ß√£o de recursos apenas <i>uma vez</i> e depois armazenar em cache os resultados em uma matriz intermedi√°ria.  Para implementar esse cen√°rio, primeiro criamos uma inst√¢ncia da classe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ImageDataGenerator</a> para carregar arquivos diretamente do disco r√≠gido (para obter mais informa√ß√µes, consulte o artigo base <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Modelo b√°sico para reconhecer emo√ß√µes nas imagens</a> ). <br><br><img src="https://habrastorage.org/webt/o-/wo/7h/o-wo7hel-_kgurcvwcs2v2smecc.png"><br><br>  No pr√≥ximo est√°gio, usamos no modo de previs√£o o bloco de extra√ß√£o de recurso criado anteriormente como parte do modelo para obter recursos de imagem. <br><br><img src="https://habrastorage.org/webt/gl/no/p7/glnop717aagtytzitgpdtbl_11c.png"><br><br>  Demora cerca de 50 segundos.  Agora, podemos usar os resultados para um treinamento muito r√°pido da parte de classifica√ß√£o superior do modelo - uma era dura cerca de 1 segundo para n√≥s.  Imagine agora que cada √©poca dura 50 segundos a mais.  Assim, essa t√©cnica simples de cache nos permitiu acelerar o processo de treinamento em rede em 50 vezes!  Nesse cen√°rio, salvamos todos os sinais para todos os exemplos na RAM, pois seu volume √© suficiente para isso.  Ao usar um conjunto de dados maior, voc√™ pode calcular as propriedades, grav√°-las no disco r√≠gido e l√™-las usando a mesma abordagem associada √† classe do gerador. <br><br>  Por fim, considere a arquitetura da parte de classifica√ß√£o do modelo: <br><br><img src="https://habrastorage.org/webt/6x/hq/qb/6xhqqbgjol6dfxyn47rufbuc_ag.png"><br><br><img src="https://habrastorage.org/webt/ss/4v/3t/ss4v3to-rinafik2lthu5ecszt8.png"><br><br>  Lembre-se de que, na sa√≠da do bloco de extra√ß√£o de recursos da rede neural convolucional, √© emitido um tensor quadridimensional (exemplos, altura, largura e canais) e uma camada totalmente conectada para classifica√ß√£o utiliza um tensor bidimensional (exemplos, recursos).  Uma maneira de transformar um tensor quadridimensional com recursos √© simplesmente alinh√°-lo nos tr√™s √∫ltimos eixos (usamos uma t√©cnica semelhante no modelo base).  Nesse cen√°rio, usamos uma abordagem diferente, denominada <i>subamostragem de valor m√©dio global</i> (GAP).  Em vez de alinhar os vetores quadridimensionais, usaremos o valor m√©dio com base em duas dimens√µes espaciais.  De fato, pegamos um mapa de atributos e simplesmente medimos todos os valores nele.  O m√©todo GAP foi introduzido pela primeira vez no excelente trabalho da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Rede</a> Min Lin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na Internet</a> (vale a pena conhec√™-lo, pois ele discute alguns conceitos importantes - por exemplo, convolu√ß√µes 1 √ó 1).  Uma vantagem √≥bvia da abordagem GAP √© uma redu√ß√£o significativa no n√∫mero de par√¢metros.  Usando o GAP, obtemos apenas 512 recursos para cada exemplo. Ao alinhar os dados brutos, o n√∫mero de recursos ser√° 15 √ó 12 √ó 512 = 92 160. Isso pode levar a uma sobrecarga s√©ria, pois nesse caso a parte de classifica√ß√£o do modelo ter√° cerca de 50 milh√£o de par√¢metros!  Outros elementos da parte de classifica√ß√£o do modelo, como camadas totalmente conectadas e camadas que implementam o m√©todo de exclus√£o, s√£o discutidos em detalhes no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Modelo b√°sico para reconhecer emo√ß√µes em imagens</a> . <br><br><h2>  <font color="#0071c5">Configura√ß√µes e op√ß√µes de treinamento</font> </h2><br>  Depois de prepararmos a arquitetura do nosso modelo usando o Keras, voc√™ precisar√° configurar o modelo inteiro para treinamento usando o m√©todo de compila√ß√£o. <br><br><img src="https://habrastorage.org/webt/dl/x5/by/dlx5byabpmih_ocdviw_8ngvatw.png"><br><br>  Nesse caso, usamos configura√ß√µes quase semelhantes √†s do modelo base, com exce√ß√£o da escolha do otimizador.  Para otimizar o aprendizado, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a entropia cruzada bin√°ria</a> ser√° usada como uma fun√ß√£o de perda e uma m√©trica de precis√£o ser√° rastreada adicionalmente.  Usamos o m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Adam</a> como um otimizador.  Adam √© um tipo de algoritmo estoc√°stico de descida de gradiente com um momento e <i>velocidade de aprendizado</i> adapt√°vel (para obter mais informa√ß√µes, consulte a entrada de blog de S. Ruder. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Vis√£o geral dos algoritmos de otimiza√ß√£o de descida de gradiente</a> ). <br><br>  A velocidade de aprendizado √© um hiperpar√¢metro otimizador que deve ser configurado para garantir que o modelo esteja operacional.  Lembre-se de que a f√≥rmula para a descida do gradiente "baunilha" n√£o cont√©m funcionalidade adicional: <br><br><img src="https://habrastorage.org/webt/qy/iu/ny/qyiunyjwn2bjmvzkkd_jg5050ay.png"><br><br>  Œò √© o vetor de par√¢metros do modelo (no nosso caso, esses s√£o os coeficientes de pondera√ß√£o da rede neural), - √© a fun√ß√£o objetivo, ‚àá √© o operador de gradiente (calculado usando o algoritmo de propaga√ß√£o de erro), Œ± √© a velocidade de aprendizado.  Assim, o gradiente da fun√ß√£o objetivo representa a dire√ß√£o da etapa de otimiza√ß√£o no espa√ßo de par√¢metros e a velocidade de aprendizado √© o seu tamanho.  Ao usar uma velocidade de aprendizado excessivamente alta, existe a possibilidade de um escorregamento constante do ponto ideal devido ao tamanho da etapa muito grande.  Por outro lado, se a velocidade de aprendizado for muito baixa, a otimiza√ß√£o levar√° muito tempo e poder√° garantir a converg√™ncia apenas para m√≠nimos locais de baixa qualidade, em vez de um extremo global.  Portanto, em cada situa√ß√£o espec√≠fica, √© necess√°rio buscar um compromisso adequado.  Usar as configura√ß√µes padr√£o do algoritmo Adam √© um bom ponto de partida para come√ßar. <br><br>  No entanto, nesta tarefa, as configura√ß√µes padr√£o do Adam mostram resultados ruins.  Precisamos reduzir a taxa de aprendizado inicial para 0,0001.  Caso contr√°rio, o treinamento n√£o ser√° capaz de garantir a converg√™ncia. <br><br>  Por fim, podemos come√ßar a aprender mais de 100 √©pocas e depois salvar o modelo em si e a hist√≥ria do aprendizado.  O comando <i>% time</i> √© um comando m√°gico Ipython * que permite medir o tempo de execu√ß√£o do c√≥digo. <br><br><img src="https://habrastorage.org/webt/tp/qr/h3/tpqrh3zk0u7oxnxg77cbsmigxc8.png"><br><br><h2>  <font color="#0071c5">Classifica√ß√£o</font> </h2><br><br><img src="https://habrastorage.org/webt/ij/w6/a-/ijw6a-rdyl9fd5rhlsuybmu24vm.png"><br><br>  Vamos avaliar a efic√°cia do modelo durante o treinamento.  No nosso caso, a precis√£o da verifica√ß√£o √© de 73% (em compara√ß√£o com 55% usando o modelo base).  Este resultado √© muito melhor que o resultado do modelo base. <br><br>  Vejamos tamb√©m a distribui√ß√£o de erros usando a matriz de imprecis√µes.  Os erros s√£o distribu√≠dos quase uniformemente entre as classes com um leve vi√©s em dire√ß√£o a exemplos negativos classificados incorretamente (c√©lula superior esquerda da matriz de imprecis√µes).  Isso pode ser explicado por um <i>pequeno desequil√≠brio no conjunto de dados em</i> rela√ß√£o √† classe positiva. <br><br>  Outra m√©trica que rastreamos √© a curva de desempenho do receptor (curva ROC) e a √°rea sob essa curva (AUC).  Para uma descri√ß√£o detalhada dessas m√©tricas, consulte o artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Modelo b√°sico para reconhecer emo√ß√µes nas imagens</a> . <br><br><img src="https://habrastorage.org/webt/if/lx/cf/iflxcf-2pxzdcuicg8im4us-9yg.png"><br><br>  Quanto mais pr√≥xima a curva ROC estiver do ponto superior esquerdo do gr√°fico e maior a √°rea abaixo dela (m√©trica da AUC), melhor o classificador funcionar√°.  Esta figura mostra claramente que um modelo aprimorado e pr√©-treinado mostra melhores resultados em compara√ß√£o com o modelo b√°sico criado a partir do zero.  O valor da AUC para o modelo pr√©-treinado √© 0,82, o que √© um bom resultado. <br><br><img src="https://habrastorage.org/webt/7o/dy/hu/7odyhuvi5j09ajzquknecv_ch2s.png"><br><br><h2>  <font color="#0071c5">Conclus√£o</font> </h2><br>  Neste artigo, encontramos uma t√©cnica poderosa - aprendizado indutivo.  Tamb√©m constru√≠mos um classificador de rede neural convolucional usando uma unidade de extra√ß√£o de recursos pr√©-treinada, baseada na arquitetura VGG.  Esse classificador superou em suas caracter√≠sticas de desempenho o modelo convolucional b√°sico, treinado a partir do zero.  O aumento na precis√£o foi de 18% e o aumento na m√©trica da AUC foi de 0,25, o que demonstra um aumento muito significativo na qualidade do sistema. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt421367/">https://habr.com/ru/post/pt421367/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt421355/index.html">Lan√ßamento do 1.11 - WebAssembly e m√≥dulos nativos</a></li>
<li><a href="../pt421357/index.html">Para a quest√£o do imposs√≠vel. Parte 3</a></li>
<li><a href="../pt421359/index.html">O festival √© como um jogo. Taxonomia de pessoas de TI</a></li>
<li><a href="../pt421361/index.html">A AMD abriu o c√≥digo-fonte para o V-EZ, uma API Vulkan de shell de baixo n√≠vel e plataforma cruzada</a></li>
<li><a href="../pt421365/index.html">A evolu√ß√£o de uma startup. √Ågil de Yaytselov para Chiken Invaders</a></li>
<li><a href="../pt421369/index.html">O que os estagi√°rios da ABBYY realmente fazem</a></li>
<li><a href="../pt421371/index.html">Agulhas invis√≠veis: cientistas desenvolveram uma maneira de mascarar nanossensores para √≥ptica e biomedicina</a></li>
<li><a href="../pt421373/index.html">Python disponibiliza a programa√ß√£o para um amplo p√∫blico</a></li>
<li><a href="../pt421375/index.html">Como a incerteza mata o com√©rcio</a></li>
<li><a href="../pt421377/index.html">7 equ√≠vocos de um gerente de projeto iniciante no gamedev</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>