<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëØ üôà üèçÔ∏è Mudan√ßas recentes na pilha de E / S do Linux do ponto de vista do DBA üßô üë®üèª‚Äçüíº üòä</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Os principais problemas do trabalho com o banco de dados est√£o relacionados aos recursos do dispositivo do sistema operacional no qual o banco de dado...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mudan√ßas recentes na pilha de E / S do Linux do ponto de vista do DBA</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/459444/">  Os principais problemas do trabalho com o banco de dados est√£o relacionados aos recursos do dispositivo do sistema operacional no qual o banco de dados trabalha.  O Linux agora √© o principal sistema operacional para bancos de dados.  Solaris, Microsoft e at√© HPUX ainda s√£o usados ‚Äã‚Äãna empresa, mas nunca ocupam o primeiro lugar, mesmo combinados.  O Linux est√° ganhando terreno com confian√ßa, porque h√° cada vez mais bancos de dados de c√≥digo aberto.  Portanto, a quest√£o da intera√ß√£o do banco de dados com o sistema operacional √© obviamente sobre bancos de dados Linux.  Isso √© sobreposto ao eterno problema do banco de dados - desempenho de E / S.  √â bom que, nos √∫ltimos anos, o Linux tenha passado por uma grande reforma na pilha de E / S e haja esperan√ßa de ilumina√ß√£o. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0o7uNUOS-Ho" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Ilya Kosmodemyansky ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">hydrobiont</a> ) trabalha para a Data Egret, uma empresa que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">consulta</a> e oferece suporte ao PostgreSQL, e sabe muito sobre a intera√ß√£o entre o SO e os bancos de dados.  Em um relat√≥rio sobre o HighLoad ++, Ilya falou sobre a intera√ß√£o de IO e bancos de dados usando o exemplo do PostgreSQL, mas tamb√©m mostrou como outros bancos de dados funcionam com IO.  Eu olhei para a pilha de E / S do Linux, que coisas novas e boas apareciam nela e por que nem tudo √© como era h√° alguns anos atr√°s.  Como um lembrete √∫til - uma lista de verifica√ß√£o das configura√ß√µes do PostgreSQL e Linux para obter o desempenho m√°ximo do subsistema IO nos novos kernels. <br><a name="habracut"></a><br>  <i>O v√≠deo do relat√≥rio cont√©m muito ingl√™s, a maioria dos quais traduzimos no artigo.</i> <br><br><h2>  Por que falar sobre IO? </h2><br>  <strong>E / S r√°pida √© a coisa mais cr√≠tica para os administradores de banco de dados</strong> .  Todo mundo sabe o que pode ser alterado ao trabalhar com a CPU, que a mem√≥ria pode ser expandida, mas a E / S pode estragar tudo.  Se estiver ruim com discos e com E / S em excesso, o banco de dados ir√° gemer.  IO se tornar√° um gargalo. <br><br><blockquote>  Para que tudo funcione bem, voc√™ precisa configurar tudo. </blockquote><br>  N√£o apenas o banco de dados ou apenas o hardware - √© isso.  At√© o Oracle de alto n√≠vel, que em si √© um sistema operacional em alguns lugares, requer configura√ß√£o.  Lemos as instru√ß√µes no "Guia de instala√ß√£o" da Oracle: altere esses par√¢metros do kernel, alteramos outros - existem muitas configura√ß√µes.  Al√©m do fato de que no Unbreakable Kernel, muito j√° est√° por padr√£o conectado ao Oracle Linux. <br><br>  Para PostgreSQL e MySQL, s√£o necess√°rias ainda mais altera√ß√µes.  Isso ocorre porque essas tecnologias contam com mecanismos do SO.  Um DBA que funcione com PostgreSQL, MySQL ou NoSQL moderno deve ser um engenheiro de opera√ß√µes do Linux e girar nozes diferentes do SO. <br><br>  Todo mundo que quer lidar com as configura√ß√µes do kernel, vira para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LWN</a> .  O recurso √© engenhoso, minimalista, cont√©m muitas informa√ß√µes √∫teis, mas foi <strong>escrito por desenvolvedores de kernel para desenvolvedores de kernel</strong> .  O que os desenvolvedores do kernel escrevem bem?  O n√∫cleo, n√£o o artigo, como us√°-lo.  Portanto, tentarei explicar tudo para os desenvolvedores e permitir que eles escrevam o kernel. <br><br>  Tudo √© complicado muitas vezes pelo fato de que inicialmente o desenvolvimento do kernel Linux e o processamento de sua pilha estavam atrasados, e nos √∫ltimos anos eles foram muito r√°pidos.  Nem ferro nem desenvolvedores com artigos atr√°s dele acompanham. <br><br><h2>  Banco de dados t√≠pico </h2><br>  Vamos come√ßar com os exemplos do PostgreSQL - aqui est√° a E / S em buffer.  Ele possui mem√≥ria compartilhada, que √© alocada no <strong>espa√ßo</strong> do <strong>usu√°rio</strong> do ponto de vista do sistema operacional e possui o mesmo cache no cache do kernel no <strong>espa√ßo do kernel</strong> . <br><br><img src="https://habrastorage.org/webt/jd/dh/d2/jddhd25l97pocaqtqqf6ccygcxi.jpeg"><br><br>  <strong>A principal tarefa de um banco de dados moderno</strong> : <br><br><ul><li>  pegue as p√°ginas do disco na mem√≥ria; </li><li>  quando ocorrer uma altera√ß√£o, marque as p√°ginas como sujas; </li><li>  gravar no Write-Ahead Log; </li><li>  depois sincronize a mem√≥ria para que fique consistente com o disco. </li></ul><br>  Em uma situa√ß√£o do PostgreSQL, esta √© uma viagem de ida e volta constante: da mem√≥ria compartilhada que o PostgreSQL controla no kernel do cache de p√°gina e depois no disco atrav√©s de toda a pilha do Linux.  Se voc√™ usar um banco de dados em um sistema de arquivos, ele funcionar√° neste algoritmo com qualquer sistema semelhante ao UNIX e com qualquer banco de dados.  As diferen√ßas s√£o, mas insignificantes. <br><br>  O uso do Oracle ASM ser√° diferente - o pr√≥prio Oracle interage com o disco.  Mas o princ√≠pio √© o mesmo: com o Direct IO ou com o cache de p√°gina, mas a tarefa √© <strong>desenhar p√°ginas por toda a pilha de E / S o mais r√°pido poss√≠vel</strong> , seja ele qual for.  E problemas podem surgir em todas as etapas. <br><br><h3>  Dois problemas de IO </h3><br>  Embora tudo seja <strong>somente leitura</strong> , n√£o h√° problemas.  Eles l√™em e, se houver mem√≥ria suficiente, todos os dados que precisam ser lidos s√£o colocados na RAM.  O fato de que, no caso do PostgreSQL no <strong>Buffer Cache,</strong> √© o mesmo, n√£o estamos muito preocupados. <br><br><img src="https://habrastorage.org/webt/mh/0j/sm/mh0jsmmdrh5bosmamuaiko9l-jw.jpeg"><br><br>  <strong>O primeiro problema com o IO √© a sincroniza√ß√£o de cache.</strong>  Ocorre quando a grava√ß√£o √© necess√°ria.  Nesse caso, voc√™ ter√° que dirigir para frente e para tr√°s muito mais mem√≥ria. <br><br><img src="https://habrastorage.org/webt/tq/mw/u7/tqmwu7fy-wlwqjx6nxec6jfcyrk.jpeg"><br><br>  Portanto, voc√™ precisa configurar o PostgreSQL ou MySQL para que tudo chegue ao disco a partir da mem√≥ria compartilhada.  No caso do PostgreSQL - voc√™ ainda precisa ajustar a trapa√ßa em segundo plano das p√°ginas sujas no Linux para enviar tudo para o disco. <br><br>  <strong>O segundo problema comum √© a falha de grava√ß√£o do log Write-Ahead</strong> .  Aparece quando a carga √© t√£o poderosa que at√© um log gravado seq√ºencialmente fica no disco.  Nesta situa√ß√£o, ele tamb√©m precisa ser gravado rapidamente. <br><br>  A situa√ß√£o n√£o √© muito diferente da <strong>sincroniza√ß√£o de cache</strong> .  No PostgreSQL, trabalhamos com um grande n√∫mero de buffers compartilhados, o banco de dados possui mecanismos para grava√ß√£o eficiente de registros Write-Ahead Log, otimizados at√© o limite.  A √∫nica coisa que pode ser feita para tornar o log mais eficiente √© alterar as configura√ß√µes do Linux. <br><br><h2>  Os principais problemas de trabalhar com o banco de dados </h2><br>  <strong>O segmento de mem√≥ria compartilhada pode ser muito grande</strong> .  Comecei a falar sobre isso em confer√™ncias em 2012.  Ent√£o eu disse que a mem√≥ria caiu de pre√ßo, mesmo existem servidores com 32 GB de RAM.  Em 2019, j√° pode haver mais em laptops, com mais e mais frequ√™ncia nos servidores 128, 256 etc. <br><br>  <strong>Realmente muita mem√≥ria</strong> .  A grava√ß√£o banal leva tempo e recursos, e as <strong>tecnologias que usamos para isso s√£o conservadoras</strong> .  Os bancos de dados s√£o antigos, foram desenvolvidos por um longo tempo, est√£o evoluindo lentamente.  Os mecanismos nos bancos de dados n√£o est√£o exatamente corretos com a tecnologia mais recente. <br><br>  <strong>Sincronizar p√°ginas na mem√≥ria com o disco resulta em grandes opera√ß√µes de E / S.</strong>  Quando sincronizamos os caches, um grande fluxo de E / S surge e outro problema surge - <strong>n√£o podemos distorcer algo e observar o efeito.</strong>  Em um experimento cient√≠fico, os pesquisadores alteram um par√¢metro - obt√©m o efeito, o segundo - obt√©m o efeito, o terceiro.  N√≥s n√£o teremos sucesso.  Torcemos alguns par√¢metros no PostgreSQL, configuramos pontos de verifica√ß√£o - n√£o vimos o efeito.  Em seguida, configure novamente a pilha inteira para obter pelo menos algum resultado.  Torcer um par√¢metro n√£o funciona - somos for√ßados a configurar tudo de uma vez. <br><br>  A maioria das E / S do PostgreSQL gera sincroniza√ß√£o de p√°gina: pontos de verifica√ß√£o e outros mecanismos de sincroniza√ß√£o.  Se voc√™ trabalhou com o PostgreSQL, pode ter visto picos nos pontos de verifica√ß√£o quando uma ‚Äúserra‚Äù aparece periodicamente nos gr√°ficos.  Anteriormente, muitos enfrentavam esse problema, mas agora existem manuais sobre como corrigi-lo, ficou mais f√°cil. <br><br>  Hoje, os SSDs salvam bastante a situa√ß√£o.  No PostgreSQL, algo raramente repousa diretamente no registro de valor.  Tudo depende da sincroniza√ß√£o: quando ocorre um ponto de verifica√ß√£o, o fsync √© chamado e existe um tipo de "acertar" um ponto de verifica√ß√£o no outro.  Muito IO.  Um ponto de verifica√ß√£o ainda n√£o terminou, n√£o concluiu todos os seus fsyncs, mas j√° ganhou outro ponto de verifica√ß√£o e come√ßou! <br><br>  O PostgreSQL possui um recurso exclusivo - <strong>v√°cuo</strong> autom√°tico.  Este √© um longo hist√≥rico de muletas para arquitetura de banco de dados.  Se o autovacuum falhar, eles geralmente o configuram para que ele funcione de forma agressiva e n√£o interfira com o resto: h√° muitos trabalhadores de autovacuum, frequentemente acionando um pouco, processando tabelas rapidamente.  Caso contr√°rio, haver√° problemas com DDL e com bloqueios. <br><br><blockquote>  Mas quando o Autovacuum √© agressivo, ele come√ßa a mastigar IO. </blockquote><br>  Se o v√°cuo autom√°tico for sobreposto nos pontos de verifica√ß√£o, na maioria das vezes os discos s√£o quase 100% reciclados, e essa √© a fonte dos problemas. <br><br>  Curiosamente, h√° um problema de <strong>recarga de cache</strong> .  Ela √© geralmente menos conhecida por DBA.  Um exemplo t√≠pico: o banco de dados foi iniciado e, por algum tempo, tudo diminui tristemente.  Portanto, mesmo se voc√™ tiver muita RAM, compre bons discos para que a pilha aque√ßa o cache. <br><br>  Tudo isso afeta seriamente o desempenho.  Os problemas come√ßam n√£o imediatamente ap√≥s a reinicializa√ß√£o do banco de dados, mas posteriormente.  Por exemplo, o ponto de verifica√ß√£o passou e muitas p√°ginas est√£o sujas em todo o banco de dados.  Eles s√£o copiados para o disco porque voc√™ precisa sincroniz√°-los.  Em seguida, os pedidos solicitam uma nova vers√£o das p√°ginas do disco e o banco de dados afunda.  Os gr√°ficos mostrar√£o como o refil do cache ap√≥s cada ponto de verifica√ß√£o contribui com uma certa porcentagem para a carga. <br><br>  A coisa mais desagrad√°vel na entrada / sa√≠da do banco de dados √© o <strong>IO do Trabalhador.</strong>  Quando cada trabalhador que voc√™ solicita, come√ßa a gerar seu IO.  No Oracle, √© mais f√°cil, mas no PostgreSQL √© um problema. <br><br>  H√° muitos motivos para problemas com o <strong>Worker IO</strong> : n√£o h√° cache suficiente para "postar" novas p√°ginas do disco.  Por exemplo, acontece que todos os buffers s√£o compartilhados, todos sujos, os pontos de verifica√ß√£o ainda n√£o foram.  Para que o trabalhador execute a sele√ß√£o mais simples, voc√™ precisa levar o cache de algum lugar.  Para fazer isso, primeiro voc√™ precisa salvar tudo em disco.  Voc√™ n√£o possui um processo especializado de checkpointer, e o trabalhador inicia o fsync para liber√°-lo e preench√™-lo com algo novo. <br><br>  Isso levanta um problema ainda maior: o trabalhador √© uma coisa n√£o especializada e todo o processo n√£o √© otimizado.  √â poss√≠vel otimizar em algum lugar no n√≠vel do Linux, mas no PostgreSQL isso √© uma medida de emerg√™ncia. <br><br><h2>  Problema de E / S principal para DB </h2><br>  <strong>Que problema resolvemos quando montamos algo?</strong>  Queremos maximizar o deslocamento de p√°ginas sujas entre o disco e a mem√≥ria. <br><br>  Mas muitas vezes acontece que essas coisas n√£o tocam diretamente o disco.  Um caso t√≠pico - voc√™ v√™ uma m√©dia de carga muito grande.  Porque  Porque algu√©m est√° esperando pelo disco e todos os outros processos tamb√©m est√£o esperando.  Parece que n√£o h√° utiliza√ß√£o expl√≠cita do disco, apenas algo bloqueou o disco e o problema est√° na entrada / sa√≠da. <br><br><blockquote>  Os problemas de E / S do banco de dados nem sempre dizem respeito apenas a discos. </blockquote><br>  Tudo est√° envolvido neste problema: discos, mem√≥ria, CPU, Agendadores de E / S, sistemas de arquivos e configura√ß√µes de banco de dados.  Agora vamos examinar a pilha, ver o que fazer com ela e que coisas boas foram inventadas no Linux para que tudo funcione melhor. <br><br><h3>  Discos </h3><br>  Por muitos anos, os discos ficaram terrivelmente lentos e ningu√©m esteve envolvido na lat√™ncia ou otimiza√ß√£o dos est√°gios de transi√ß√£o.  Otimizar fsyncs n√£o fazia sentido.  O disco estava girando, as cabe√ßas se movendo ao longo dele como um registro fonogr√°fico, e o fsyncs era t√£o longo que os problemas n√£o surgiram. <br><br><h3>  A mem√≥ria </h3><br>  √â in√∫til olhar para as principais consultas sem ajustar o banco de dados.  Voc√™ configurar√° uma quantidade suficiente de mem√≥ria compartilhada, etc., e ter√° uma nova consulta superior - precisar√° configur√°-la novamente.  Aqui est√° a mesma hist√≥ria.  Toda a pilha do Linux foi feita a partir deste c√°lculo. <br><br><h3>  Largura de banda e lat√™ncia </h3><br>  <strong>Maximizar o desempenho de IO maximizando o rendimento √© f√°cil at√© certo ponto.</strong>  Um processo auxiliar do PageWriter foi inventado no PostgreSQL que descarregava o ponto de verifica√ß√£o.  O trabalho tornou-se paralelo, mas ainda h√° bases para a adi√ß√£o do paralelismo.  E minimizar a lat√™ncia √© a tarefa da √∫ltima milha, para a qual s√£o necess√°rias super tecnologias. <br><br>  Essas super tecnologias s√£o SSDs.  Quando eles apareceram, a lat√™ncia caiu acentuadamente.  Mas em todos os outros est√°gios da pilha, surgiram problemas: tanto do lado dos fabricantes de bancos de dados quanto dos fabricantes de Linux.  Problemas precisam ser resolvidos. <br><br>  O desenvolvimento do banco de dados se concentrou em maximizar a produtividade, assim como o desenvolvimento do kernel do Linux.  Muitos m√©todos para otimizar a era de E / S de discos girat√≥rios n√£o s√£o t√£o bons para SSDs. <br><br>  No meio, fomos for√ßados a fazer backup da infraestrutura atual do Linux, mas com novos discos.  Assistimos a testes de desempenho do fabricante com um grande n√∫mero de IOPS diferentes e o banco de dados n√£o melhorou, porque o banco de dados n√£o √© apenas e nem tanto sobre IOPS.  Muitas vezes acontece que podemos pular 50.000 IOPS por segundo, o que √© bom.  Mas se n√£o sabemos a lat√™ncia, n√£o sabemos sua distribui√ß√£o, n√£o podemos dizer nada sobre desempenho.  Em algum momento, o banco de dados come√ßar√° a ponto de verifica√ß√£o e a lat√™ncia aumentar√° dramaticamente. <br><br>  Por um longo tempo, como agora, esse tem sido um grande problema de desempenho em bancos de dados virtuais.  O IO virtual √© caracterizado por lat√™ncia desigual, o que, √© claro, tamb√©m envolve problemas. <br><br><h2>  Pilha de E / S.  Como era antes </h2><br><img src="https://habrastorage.org/webt/yl/3v/oz/yl3vozgbt2ltrkqo8lbey-wzdfo.jpeg"><br><br>  H√° espa√ßo para o usu√°rio - essa mem√≥ria, que √© gerenciada pelo pr√≥prio banco de dados.  Em um banco de dados configurado para que tudo funcionasse como deveria.  Isso pode ser feito em um relat√≥rio separado, e nem em um.  Ent√£o tudo passa inevitavelmente pelo cache de p√°gina ou pela interface Direct IO que entra na <strong>camada Block Input / Output</strong> . <br><br>  Imagine uma interface do sistema de arquivos.  As p√°ginas que estavam no cache de buffer, como estavam originalmente no banco de dados, ou seja, os blocos, saem dele.  A camada IO do bloco lida com o seguinte.  H√° uma estrutura C que descreve um bloco no kernel.  A estrutura pega esses blocos e coleta deles vetores (matrizes) de solicita√ß√µes de entrada ou sa√≠da.  Abaixo da camada BIO, est√° a camada solicitante.  Os vetores s√£o coletados nessa camada e v√£o al√©m. <br><br>  Por um longo tempo, essas duas camadas no Linux foram aprimoradas para grava√ß√£o eficiente em discos magn√©ticos.  Era imposs√≠vel ficar sem uma transi√ß√£o.  Existem blocos convenientes para gerenciar no banco de dados.  √â necess√°rio montar esses blocos em vetores que s√£o convenientemente gravados no disco para que fiquem em algum lugar pr√≥ximo.  Para que isso funcionasse efetivamente, eles criaram Elevadores, ou Agendadores IO. <br><br><h2>  Elevadores </h2><br>  Os elevadores estavam envolvidos principalmente na combina√ß√£o e na classifica√ß√£o de vetores.  Tudo em ordem para que o driver SD do bloco - o driver quase-disco - os blocos de grava√ß√£o cheguem na ordem conveniente para ele.  O driver foi traduzido dos blocos para seus setores e gravado no disco. <br><br>  O problema era que era necess√°rio fazer v√°rias transi√ß√µes e, a cada uma delas, implementar sua pr√≥pria l√≥gica do processo √≥timo. <br><br><h3>  Elevadores: at√© o kernel 2.6 </h3><br>  <strong>Antes do kernel 2.6, havia o Linus Elevator</strong> - o IO Scheduler mais primitivo, escrito por voc√™ adivinha quem.  Por um longo tempo, ele foi considerado absolutamente inabal√°vel e bom, at√© que eles desenvolveram algo novo. <br><br>  O Linus Elevator teve muitos problemas.  <strong>Ele combinou e classificou de</strong> <strong>acordo com a forma de gravar com mais efici√™ncia</strong> .  No caso de discos mec√¢nicos rotativos, isso levou ao surgimento de " <strong>fome"</strong> : uma situa√ß√£o em que a efici√™ncia da grava√ß√£o depende da rota√ß√£o do disco.  Se de repente voc√™ precisar ler efetivamente ao mesmo tempo, mas j√° estiver errado, ser√° mal lido nesse disco. <br><br>  Gradualmente, ficou claro que essa √© uma maneira ineficiente.  Portanto, a partir do kernel 2.6, um zool√≥gico inteiro de agendadores come√ßou a aparecer, destinado a tarefas diferentes. <br><br><h3>  Elevadores: entre 2,6 e 3 </h3><br>  Muitas pessoas confundem esses agendadores com agendadores de sistema operacional porque eles t√™m nomes semelhantes.  <strong>CFQ - Enfileiramento completamente justo</strong> n√£o √© o mesmo que agendadores de SO.  Apenas os nomes s√£o semelhantes.  Foi cunhado como um agendador universal. <br><br>  <strong>O que √© um agendador universal?</strong>  Voc√™ acha que tem uma carga m√©dia ou, pelo contr√°rio, uma carga √∫nica?  Os bancos de dados t√™m uma versatilidade muito baixa.  A carga universal pode ser imaginada como um laptop comum.  Tudo acontece l√°: ouvimos m√∫sica, tocamos, digitamos texto.  Para isso, apenas agendadores universais foram escritos. <br><br>  <strong>A principal tarefa do planejador universal:</strong> no caso do Linux, para cada terminal e processo virtual, crie uma fila de solicita√ß√µes.  Quando queremos ouvir m√∫sica em um reprodutor de √°udio, as entradas e sa√≠das do reprodutor ficam em uma fila.  Se queremos fazer backup de algo usando o comando cp, algo mais est√° envolvido. <br><br>  No caso de bancos de dados, ocorre um problema.  Como regra, um banco de dados √© um processo iniciado e, durante a opera√ß√£o, surgiram processos paralelos que sempre terminam na mesma fila de E / S.  O motivo √© que esse √© o mesmo aplicativo, o mesmo processo pai.  Para cargas muito pequenas, essa programa√ß√£o era adequada; para o resto, n√£o fazia sentido.  Era mais f√°cil desligar e n√£o usar, se poss√≠vel. <br><br>  Gradualmente, o <strong>agendador de prazos</strong> apareceu - ele funciona de maneira mais esperta, mas basicamente √© mesclagem e classifica√ß√£o para discos girat√≥rios.  Dado o design de um subsistema de disco espec√≠fico, coletamos vetores de blocos para escrev√™-los da maneira ideal.  Ele tinha menos problemas com a <strong>fome</strong> , mas eles estavam l√°. <br><br>  Portanto, mais perto do terceiro kernel do Linux apareceu <strong>noop</strong> ou <strong>none</strong> , o que funcionou muito melhor com a expans√£o dos SSDs.  Incluindo o planejador noop, na verdade desabilitamos o planejamento: n√£o h√° classifica√ß√µes, mesclagens e coisas semelhantes que o CFQ e o prazo fizeram. <br><br>  Isso funciona melhor com os SSDs, porque eles s√£o inerentemente paralelos: possui c√©lulas de mem√≥ria.  Quanto mais desses elementos forem amontoados em uma placa PCIe, mais eficiente ele funcionar√°. <br><br>  O agendador de algumas de suas considera√ß√µes de outro mundo, do ponto de vista do SSD, coleta alguns vetores e os envia para algum lugar.  Tudo termina com um funil.  Portanto, matamos a simultaneidade de SSDs, n√£o os usamos ao m√°ximo.  Portanto, um desligamento simples, quando os vetores v√£o aleatoriamente sem nenhuma classifica√ß√£o, funcionou melhor em termos de desempenho.  Por esse motivo, acredita-se que leituras aleat√≥rias e grava√ß√µes aleat√≥rias sejam melhores em SSDs. <br><br><h3>  Elevadores: 3.13 em diante </h3><br>  Come√ßando com o kernel 3.13, o <strong>blk-mq apareceu</strong> .  Um pouco antes, havia um prot√≥tipo, mas na vers√£o 3.13, uma vers√£o de trabalho apareceu pela primeira vez. <br><br>  <strong>O Blk-mq come√ßou</strong> como um agendador, mas √© dif√≠cil cham√°-lo de agendador - ele fica sozinho arquitetonicamente.  Esta √© uma substitui√ß√£o para a camada de solicita√ß√£o no kernel.  Lentamente, o desenvolvimento do blk-mq levou a uma grande revis√£o de toda a pilha de E / S do Linux. <br><br>  A id√©ia √© a seguinte: vamos usar a capacidade nativa dos SSDs para fazer simultaneidade eficiente para E / S.    ,    /  ,   ,      as is  SSD.   CPU     . <br><br>    <strong>blk-mq</strong>    .     .   ,  4  ,  <strong>blk-mq</strong>   ‚Äî  5-10%,   . <br><br><blockquote> blk-mq ‚Äî ,      SSD. </blockquote><br>    <strong>blk-mq</strong>    <strong>NVMe</strong> driver Linux.      Linux,     Microsoft.     <strong>blk-mq</strong>  NVMe driver ‚Äî      Linux,      . <br><br>       ,   .     -     PCIe SSD.      ,    . <br><br><blockquote>  blk-mq  NVMe ,  .       . </blockquote><br>    ,  ,   .  NVMe ,     ,     . <br><br><h3>    elevators </h3><br><img src="https://habrastorage.org/webt/ya/82/ou/ya82oun8cusxylg0bsuz5y0e1la.jpeg"><br><br>  :  CPU,   ,   -   . <br><br>   Elevators  -.   CPU   . - , ,    ,       , IO      . <br><br><h3>    elevators </h3><br> blk-mq ‚Äî    .  CPU,  NUMA-   /   .     ,  ,  ,    .   SD ,    , . <br><br><img src="https://habrastorage.org/webt/zf/12/7n/zf127n1eqhli2qioregzrsiftp8.jpeg"><br><br>   .  -    RAID-   ,     RAID.   SSD ‚Äî   .    SD-   ,  blq-mq. <br><br><h2>    blk-mq </h2><br>      . <br><br><img src="https://habrastorage.org/webt/me/ma/3e/mema3e595wllaoub6thn61q8kvu.jpeg"><br><br>    . ,   . /    ,  ,   Block IO-.     <strong>blk-mq</strong> ,    ,   scheduler. <br><br>   3.13      ,       .    schedulers  <strong>blk-mq</strong> ,      .     Linux    schedulers IO ‚Äî  Kyber  BFQ.         <strong>blk-mq</strong> . <br><br> <strong>BFQ</strong> <strong>‚Äî Budget Fair Queueing ‚Äî </strong> <strong></strong> <strong></strong> <strong>FQ</strong> .   ,     . BFQ ‚Äî  scheduler   .         IO.     IO,  / .       ,   .     ‚Äî  .   BFQ,   ,    . <br><br> <strong>Kyber ‚Äî  </strong> .   BFQ,   .  Kyber  scheduler   .    ‚Äî   CPU  . Kyber    . <br><br>      ‚Äî <strong>blk-mq    SD-</strong> .      ,    ,  ,    IO-.  blk-mq  NVMe driver      .     . <br><br>        ‚Äî   latency,      .   SSD,    ‚Äî     .      -, ,    NVMe-,   blk-mq    ,    .    . <br><br><h2>   Linux IO </h2><br>         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> / Linux. <br><br><img src="https://habrastorage.org/webt/ak/05/qb/ak05qbritsgbiaeybq3dmz2mpyi.png"><br><br>   ,    ,   ,  Elevators,   . <br><br>   ,           ,    . <br><br><h2>  NVM Express </h2><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NVM Express ou NVMe √© uma especifica√ß√£o, um conjunto de padr√µes que ajudam voc√™ a usar melhor os SSDs. </font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A especifica√ß√£o est√° bem implementada no Linux. </font><font style="vertical-align: inherit;">O Linux √© uma das for√ßas motrizes do padr√£o. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Agora em produ√ß√£o √© a terceira vers√£o. </font><font style="vertical-align: inherit;">O driver desta vers√£o, de acordo com a especifica√ß√£o, pode passar em torno de 20 GB / s por bloco SSD e o NVMe da quinta vers√£o, que ainda n√£o est√° dispon√≠vel, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">at√© 32 GB / s</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">O driver SD n√£o possui interfaces nem mecanismos internos para fornecer essa largura de banda.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Essa especifica√ß√£o √© significativamente mais r√°pida do que qualquer coisa que era antes. </font></font></blockquote><script type="text/javascript">function gtElInit() {var lib = new google.translate.TranslateService();lib.translatePage('ru', 'pt', function () {});}</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=gtElInit&amp;client=wt"></script><br>  Depois que os bancos de dados foram escritos para discos rotativos e orientados a eles - eles t√™m √≠ndices na forma de uma √°rvore B, por exemplo.  Surge a pergunta: <strong>os bancos de dados est√£o prontos para o NVMe</strong> ?  Os bancos de dados s√£o capazes de mastigar essa carga? <br><br>  Ainda n√£o, mas eles est√£o se adaptando.  A lista de discuss√£o do PostgreSQL recentemente teve alguns <code>pwrite()</code> e coisas semelhantes.  Os desenvolvedores do PostgreSQL e MySQL interagem com os desenvolvedores do kernel.  Claro, eu gostaria de mais intera√ß√£o. <br><br><h2>  Desenvolvimentos Recentes </h2><br>  No √∫ltimo ano e meio, o NVMe adicionou <strong>pesquisas de E /</strong> S. <br><br>  No in√≠cio, havia discos girat√≥rios com alta lat√™ncia.  Depois vieram os SSDs, que s√£o muito mais r√°pidos.  Mas havia um batente: o fsync continua, a grava√ß√£o √© iniciada e, em um n√≠vel muito baixo - no fundo do driver, uma solicita√ß√£o √© enviada diretamente para o hardware - anote-a. <br><br>  O mecanismo era simples - eles enviaram e esperamos at√© que a interrup√ß√£o seja processada.  Aguardar o processamento da interrup√ß√£o n√£o √© um problema comparado √† grava√ß√£o em um disco girat√≥rio.  Demorou tanto tempo para esperar que, assim que a grava√ß√£o terminou, a interrup√ß√£o funcionou. <br><br>  Desde que o SSD grava muito rapidamente, apareceu um mecanismo para pesquisar o hardware sobre a grava√ß√£o.  Nas primeiras vers√µes, o aumento na velocidade de E / S atingiu 50% devido ao fato de n√£o estarmos esperando uma interrup√ß√£o, mas estamos perguntando ativamente ao ferro sobre o registro.  <strong>Esse mecanismo √© chamado de pesquisa de entrada / sa√≠da</strong> . <br><br>  Foi introduzido em vers√µes recentes.  Na vers√£o 4.12, os <strong>agendadores de E</strong> / <strong>S</strong> apareceram, especialmente aprimorados para trabalhar com <strong>blk-mq</strong> e NVMe, sobre os quais eu disse <strong>Kyber e BFQ</strong> .  Eles j√° est√£o oficialmente no kernel, eles podem ser usados. <br><br>  Agora, de uma forma utiliz√°vel, existe a chamada <strong>marca√ß√£o IO</strong> .  Principalmente os fabricantes de nuvens e m√°quinas virtuais contribuir√£o para esse desenvolvimento.  Grosso modo, a entrada de um aplicativo espec√≠fico pode ser abordada e dar prioridade a ele.  Os bancos de dados ainda n√£o est√£o prontos para isso, mas fique atento.  Eu acho que ser√° mainstream em breve. <br><br><h2>  Notas diretas de IO </h2><br>  <strong>O PostgreSQL n√£o oferece suporte ao Direct IO, e h√° v√°rios problemas que dificultam a ativa√ß√£o do suporte</strong> .  Agora, isso √© suportado apenas por valor e somente se a replica√ß√£o n√£o estiver ativada.  √â necess√°rio <strong>escrever muito c√≥digo espec√≠fico do SO e</strong> , por enquanto, todos se abst√™m disso. <br><br>  Apesar do fato de o Linux jurar pesadamente a id√©ia do Direct IO e como ele √© implementado, todos os bancos de dados v√£o para l√°.  No Oracle e MySQL, o Direct IO √© muito usado.  O PostgreSQL √© o √∫nico banco de dados que o Direct IO n√£o tolera. <br><br><h2>  Lista de verifica√ß√£o </h2><br>  Como se proteger de surpresas com o fsync no PostgreSQL: <br><br><ul><li>  Configure pontos de verifica√ß√£o para serem menos frequentes e maiores. </li><li>  Configure o gravador de plano de fundo para ajudar no ponto de verifica√ß√£o. </li><li>  Puxe o v√°cuo autom√°tico para que n√£o haja E / S falsa desnecess√°ria. </li></ul><br><blockquote>  Segundo a tradi√ß√£o, em novembro, aguardamos desenvolvedores profissionais de servi√ßos altamente carregados no Skolkovo no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HighLoad ++</a> .  Ainda h√° um m√™s para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">solicitar</a> um relat√≥rio, mas j√° aceitamos os primeiros relat√≥rios para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">programa</a> .  Assine nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">newsletter</a> e aprenda sobre novos t√≥picos em primeira m√£o. </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt459444/">https://habr.com/ru/post/pt459444/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt459430/index.html">Aplica√ß√£o m√≥vel com gera√ß√£o autom√°tica de formul√°rios: nosso caso</a></li>
<li><a href="../pt459432/index.html">RD-180: os EUA podem fabricar motores de foguete?</a></li>
<li><a href="../pt459434/index.html">React Hook Router Uma alternativa moderna ao roteador React</a></li>
<li><a href="../pt459438/index.html">Os dados ainda s√£o mais importantes</a></li>
<li><a href="../pt459442/index.html">5 sistemas de gerenciamento de eventos de seguran√ßa de c√≥digo aberto</a></li>
<li><a href="../pt459446/index.html">Cinco tend√™ncias assustadoras do design moderno</a></li>
<li><a href="../pt459450/index.html">A vulnerabilidade do software de teleconfer√™ncia com zoom permite que qualquer site espie os usu√°rios via webcam</a></li>
<li><a href="../pt459452/index.html">Agro-rob√¥ com IA aprendeu a coletar cuidadosamente apenas salada amadurecida do jardim</a></li>
<li><a href="../pt459454/index.html">Como foi a primeira hackathon no The Standoff</a></li>
<li><a href="../pt459456/index.html">Dagaz: Epis√≥dios (Parte 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>