<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïî üåú ‚ò¢Ô∏è La red neuronal LipNet lee los labios con una precisi√≥n del 93,4% üôåüèº üëà üë®üèª‚Äçüè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El comandante Dave Bowman y el copiloto Frank Poole, que no confiaban en la computadora, decidieron desconectarla del control de la nave. Para hacer e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La red neuronal LipNet lee los labios con una precisi√≥n del 93,4%</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/398901/"><img src="https://habrastorage.org/files/f5b/c98/b66/f5bc98b66e6b4c54a8b456037259caab.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El comandante Dave Bowman y el copiloto Frank Poole, que no confiaban en la computadora, decidieron desconectarla del control de la nave. Para hacer esto, se re√∫nen en una habitaci√≥n insonorizada, pero HAL 9000 lee su conversaci√≥n en los labios. </font></font><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tomada de la pel√≠cula "Space Odyssey of 2001" La</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
lectura de labios juega un papel importante en la comunicaci√≥n. M√°s experimentos en 1976 mostraron que las personas "escuchan" </font><font style="vertical-align: inherit;">fonemas </font><font style="vertical-align: inherit;">completamente </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diferentes</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> si pones el sonido incorrecto en el movimiento de los labios (ver </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Escuchar labios y ver voces"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Nature 264, 746-748, 23 de diciembre de 1976, doi: 10.1038 / 264746a0) .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desde un punto de vista pr√°ctico, la lectura de labios es una habilidad importante y √∫til. Puede entender al interlocutor sin apagar la m√∫sica en los auriculares, leer las conversaciones de todas las personas en el campo de visi√≥n (por ejemplo, todos los pasajeros en la sala de espera), escuchar a las personas a trav√©s de binoculares o un telescopio. El alcance de la habilidad es muy amplio. Un profesional que lo domine encontrar√° f√°cilmente un trabajo bien remunerado. Por ejemplo, en el campo de la seguridad o la inteligencia competitiva.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los sistemas autom√°ticos de lectura de labios tambi√©n tienen un gran potencial pr√°ctico. Estos son los aud√≠fonos m√©dicos de nueva generaci√≥n con reconocimiento de voz, sistemas para conferencias silenciosas en lugares p√∫blicos, identificaci√≥n biom√©trica, sistemas para la transmisi√≥n secreta de informaci√≥n para espionaje, reconocimiento de voz por video desde c√°maras de vigilancia, etc. Al final, las computadoras del futuro tambi√©n leer√°n los labios, como el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HAL 9000</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, los cient√≠ficos han intentado durante muchos a√±os desarrollar sistemas autom√°ticos de lectura de labios, pero sin mucho √©xito. Incluso para un ingl√©s relativamente simple, en el que la cantidad de fonemas es mucho menor que en ruso, la precisi√≥n del reconocimiento es baja.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Comprender el habla basada en expresiones faciales humanas es una tarea desalentadora. Las personas que han dominado esta habilidad intentan reconocer docenas de fonemas consonantes, muchos de los cuales son muy similares en apariencia. Es especialmente dif√≠cil para una persona no capacitada distinguir entre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cinco categor√≠as de fonemas visuales</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (es decir, visemas) del idioma ingl√©s. En otras palabras, distinguir la pronunciaci√≥n de algunas consonantes por los labios es casi imposible. No es sorprendente que a las personas les vaya muy mal la lectura precisa de los labios. Incluso las mejores personas con discapacidad auditiva muestran una precisi√≥n de solo </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">17 ¬± 12% de 30 monos√≠labos o 21 ¬± 11% de palabras polisil√°bicas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (en adelante, los resultados para el idioma ingl√©s).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La lectura autom√°tica de labios es una de las tareas de la visi√≥n artificial, que se reduce al procesamiento cuadro por cuadro de una secuencia de video. La tarea se complica en gran medida por la baja calidad de la mayor√≠a de los materiales de video pr√°cticos, que no permiten una lectura precisa del espacio-temporal, es decir, las caracter√≠sticas espacio-temporales de una persona durante una conversaci√≥n. Las caras se mueven y giran en diferentes direcciones. Los desarrollos recientes en el campo de la visi√≥n artificial est√°n tratando de rastrear el movimiento de la cara en el marco para resolver este problema. A pesar de los √©xitos, hasta hace poco, solo pod√≠an reconocer palabras individuales, pero no oraciones. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Los desarrolladores de la Universidad de Oxford lograron un avance significativo en esta √°rea. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El LipNet que entrenaron</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">se convirti√≥ en el primero en el mundo en reconocer con √©xito los labios a nivel de oraciones completas, procesando im√°genes de video. </font><i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">Mapas de relevancia</font></a></i></font><br>
<br>
<img src="https://habrastorage.org/files/c91/f89/dae/c91f89daeefd4e0da9a55c41073a5285.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cuadro por cuadro </font><font style="vertical-align: inherit;">para las palabras en ingl√©s "please" (arriba) y "lay" (abajo) cuando son procesadas por una red neuronal que lee los labios, destacando las caracter√≠sticas m√°s llamativas (salientes) de</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LipNet, una red neuronal recurrente del tipo LSTM (memoria a largo plazo). La arquitectura se muestra en la ilustraci√≥n. La red neuronal se entren√≥ utilizando el m√©todo de Clasificaci√≥n Temporal Connectionist (CTC), que se usa ampliamente en los sistemas modernos de reconocimiento de voz, ya que elimina la necesidad de entrenamiento en un conjunto de datos de entrada sincronizados con el resultado correcto.</font></font><br>
<br>
<img src="https://habrastorage.org/files/856/4ac/a35/8564aca35bea4280bce7c669cce37fe2.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arquitectura de red neuronal LipNet. </font><font style="vertical-align: inherit;">En la entrada, se suministra una secuencia de tramas T, que luego son procesadas por tres capas de la red neuronal convolucional espacio-temporal (espacio-temporal) (STCNN), cada una de las cuales est√° acompa√±ada por una capa de muestreo espacial. </font><font style="vertical-align: inherit;">Para las caracter√≠sticas extra√≠das, la frecuencia de muestreo en la l√≠nea de tiempo (muestreo ascendente) aumenta, y luego se procesan con doble LTSM. </font><font style="vertical-align: inherit;">Cada paso de tiempo en la salida LTSM es procesado por una red de distribuci√≥n directa de dos capas y la √∫ltima capa SoftMax.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
En un paquete especial de oferta GRID, la red neuronal muestra una precisi√≥n de reconocimiento del 93.4%. </font><font style="vertical-align: inherit;">Esto no solo excede la precisi√≥n de reconocimiento de otros desarrollos de software (que se indican en la tabla a continuaci√≥n), sino que tambi√©n excede la eficiencia de la lectura en boca de personas especialmente capacitadas.</font></font><br>
<br>
<table>
<tbody><tr>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">M√©todo</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjunto de datos</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tama√±o</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Problema</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Precisi√≥n</font></font></th>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fu y col. </font><font style="vertical-align: inherit;">(2008)</font></font></td>
<td>AVICAR</td>
<td>851</td>
<td></td>
<td>37,9%</td>
</tr>
<tr>
<td>Zhao et al. (2009)</td>
<td>AVLetter</td>
<td>78</td>
<td></td>
<td>43,5%</td>
</tr>
<tr>
<td>Papandreou et al. (2009)</td>
<td>CUAVE</td>
<td>1800</td>
<td></td>
<td>83,0%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016a)</td>
<td>OuluVS1</td>
<td>200</td>
<td></td>
<td>91,4%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016b)</td>
<td>OuluVS2</td>
<td>520</td>
<td></td>
<td>94,1%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016a)</td>
<td>BBC TV</td>
<td>&gt;400000</td>
<td></td>
<td>65,4%</td>
</tr>
<tr>
<td>Wand et al. (2016)</td>
<td>GRID</td>
<td>9000</td>
<td></td>
<td>79,6%</td>
</tr>
<tr>
<td>LipNet</td>
<td>GRID</td>
<td>28853</td>
<td><b></b></td>
<td><b>93,4%</b></td>
</tr>
</tbody></table><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El caso especial de GRID se compone de acuerdo con la siguiente plantilla: </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comando (4) + color (4) + preposici√≥n (4) + letra (25) + d√≠gito (10) + adverbio (4),</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
donde el n√∫mero corresponde al n√∫mero de variantes de palabras para cada una de las seis categor√≠as verbales . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En otras palabras, la precisi√≥n del 93,4% sigue siendo el resultado obtenido en condiciones de laboratorio de invernadero. </font><font style="vertical-align: inherit;">Por supuesto, con el reconocimiento del discurso humano arbitrario, el resultado ser√° mucho peor. </font><font style="vertical-align: inherit;">Sin mencionar el an√°lisis de datos de video real, donde la cara de una persona no se toma de cerca con excelente iluminaci√≥n y alta resoluci√≥n. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El funcionamiento de la red neuronal LipNet se muestra en el video de demostraci√≥n.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fa5QGremQf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El art√≠culo cient√≠fico fue preparado para la conferencia ICLR 2017 y </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> el 4 de noviembre de 2016 en el dominio p√∫blico.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es398901/">https://habr.com/ru/post/es398901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es398889/index.html">Publicado el informe de prueba de √©xito EmDrive de la NASA</a></li>
<li><a href="../es398893/index.html">Estaci√≥n base GSM oculta en una impresora de oficina</a></li>
<li><a href="../es398895/index.html">C√≥mo reprogram√© mi cerebro para comenzar a entender las matem√°ticas</a></li>
<li><a href="../es398897/index.html">Yandex ha lanzado el servicio de salud</a></li>
<li><a href="../es398899/index.html">Masterkeys Pro L: personalizar a todos los campos</a></li>
<li><a href="../es398903/index.html">La belleza perdurable del cosmos</a></li>
<li><a href="../es398905/index.html">TP-LINK HS110: ¬øun asistente dom√©stico u otra toma de corriente con Wi-Fi?</a></li>
<li><a href="../es398907/index.html">M√∫sica sinf√≥nica: problemas de calidad de reproducci√≥n, elecci√≥n de formato y equipo</a></li>
<li><a href="../es398911/index.html">Qu√© dispositivos de control de resonancia podr√≠an ser: ejemplo de la marca Cold Ray</a></li>
<li><a href="../es398913/index.html">C√≥mo Valve intenta ense√±ar cortes√≠a a los jugadores</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>