<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèæ üôçüèø ‚ñ´Ô∏è A seguran√ßa dos algoritmos de aprendizado de m√°quina. Protegendo e testando modelos usando Python üë®üèΩ üñïüèΩ ‚Ü™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No artigo anterior, falamos sobre um problema de aprendizado de m√°quina, como exemplos adversos e alguns tipos de ataques que permitem que eles sejam ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>A seguran√ßa dos algoritmos de aprendizado de m√°quina. Protegendo e testando modelos usando Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dsec/blog/438644/"><p><img src="https://habrastorage.org/webt/wo/o_/u2/woo_u2i8ll_fqrqvt3o-typrlue.jpeg" alt="imagem"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">No artigo anterior,</a> falamos sobre um problema de aprendizado de m√°quina, como exemplos adversos e alguns tipos de ataques que permitem que eles sejam gerados.  Este artigo se concentrar√° nos algoritmos de prote√ß√£o contra esse tipo de efeito e nas recomenda√ß√µes para modelos de teste. </p><a name="habracut"></a><br><h2 id="zaschita">  Protec√ß√£o </h2><br><p>  Antes de tudo, vamos explicar imediatamente um ponto - √© imposs√≠vel se defender completamente contra esse efeito, e isso √© bastante natural.  De fato, se resolv√™ssemos o problema de exemplos adversos completamente, resolver√≠amos simultaneamente o problema de construir um hiperplano ideal, o que, √© claro, n√£o pode ser feito sem um conjunto de dados geral. </p><br><p>  Existem dois est√°gios para defender um modelo de aprendizado de m√°quina: </p><br><p>  <strong>Aprendizado</strong> - Ensinamos nosso algoritmo a responder corretamente a exemplos adversos. </p><br><p>  <strong>Opera√ß√£o</strong> - estamos tentando detectar um exemplo contradit√≥rio durante a fase de opera√ß√£o do modelo. </p><br><p>  Vale a pena dizer imediatamente que voc√™ pode trabalhar com os m√©todos de prote√ß√£o apresentados neste artigo usando o IBM <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Adversarial Robustness Toolbox</a> . </p><br><h3 id="adversarial-training">  Treinamento advers√°rio </h3><br><p><img src="https://habrastorage.org/webt/4w/t_/lm/4wt_lmm-cbcdye9rabryki0jj70.png" alt="imagem"></p><br><p>  Se voc√™ perguntar a uma pessoa que acabou de se familiarizar com o problema Adversarial com exemplos, a pergunta: "Como se proteger desse efeito?", 9 de 10 pessoas dir√£o: "Vamos adicionar os objetos gerados ao conjunto de treinamento".  Essa abordagem foi proposta imediatamente no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Propriedades intrigantes de redes neurais</a> em 2013.  Foi neste artigo que esse problema foi descrito pela primeira vez e o ataque de L-BFGS, permitindo obter exemplos adversos. </p><br><p>  Este m√©todo √© muito simples.  Geramos exemplos de Adversarial usando v√°rios tipos de ataques e os adicionamos ao conjunto de treinamento a cada itera√ß√£o, aumentando assim a ‚Äúresist√™ncia‚Äù do modelo Adversarial aos exemplos. </p><br><p>  A desvantagem desse m√©todo √© bastante √≥bvia: a cada itera√ß√£o do treinamento, para cada exemplo, podemos gerar um n√∫mero muito grande de exemplos, respectivamente, e o tempo para modelar o treinamento aumenta muitas vezes. </p><br><p>  Voc√™ pode aplicar esse m√©todo usando a biblioteca ART-IBM da seguinte maneira. </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.adversarial_trainer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AdversarialTrainer trainer = AdversarialTrainer(model, attacks) trainer.fit(x_train, y_train)</code> </pre> <br><h3 id="gaussian-data-augmentation">  Aumento Gaussiano de Dados </h3><br><p><img src="https://habrastorage.org/webt/jf/9d/ko/jf9dkoia9fom1rtqkvgwe-7aulo.png" alt="imagem"></p><br><p>  O m√©todo a seguir, descrito no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Efficient Defenses Against Adversarial Attacks</a> , usa uma l√≥gica semelhante: tamb√©m sugere adicionar objetos adicionais ao conjunto de treinamento, mas, diferentemente do Adversarial Training, esses objetos n√£o s√£o exemplos de Adversarial, mas objetos de conjunto de treinamento um pouco barulhentos (Gaussian √© usado como ru√≠do ru√≠do, da√≠ o nome do m√©todo).  E, de fato, isso parece muito l√≥gico, porque o principal problema dos modelos √© justamente a falta de imunidade a ru√≠dos. </p><br><p>  Este m√©todo mostra resultados semelhantes ao Treinamento Adversarial, enquanto gasta muito menos tempo na gera√ß√£o de objetos para treinamento. </p><br><p>  Voc√™ pode aplicar esse m√©todo usando a classe GaussianAugmentation no ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.gaussian_augmentation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianAugmentation GDA = GaussianAugmentation() new_x = GDA(x_train)</code> </pre> <br><h3 id="label-smoothing">  Suaviza√ß√£o de etiquetas </h3><br><p>  O m√©todo de suaviza√ß√£o de etiqueta √© muito simples de implementar, mas ainda assim carrega muito significado probabil√≠stico.  N√£o entraremos em detalhes da interpreta√ß√£o probabil√≠stica desse m√©todo; voc√™ pode encontr√°-lo no artigo original <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Repensando a arquitetura de cria√ß√£o para vis√£o computacional</a> .  Mas, resumindo, Label Smoothing √© um tipo adicional de regulariza√ß√£o do modelo no problema de classifica√ß√£o, o que o torna mais resistente ao ru√≠do. </p><br><p>  De fato, esse m√©todo suaviza os r√≥tulos de classe.  Fazendo-os, digamos, n√£o 1, mas 0,9.  Assim, os modelos de treinamento s√£o multados por uma "confian√ßa" muito maior no r√≥tulo de um objeto espec√≠fico. </p><br><p>  A aplica√ß√£o deste m√©todo em Python pode ser vista abaixo. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.label_smoothing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelSmoothing LS = LabelSmoothing() new_x, new_y = LS(train_x, train_y)</code> </pre> <br><h3 id="bounded-relu">  Relu limitado </h3><br><p><img src="https://habrastorage.org/webt/dw/mw/sz/dwmwszowk1t9l6byacxcscvmvh4.png" alt="imagem"></p><br><p>  Quando falamos sobre ataques, muitos podem perceber que alguns ataques (JSMA, OnePixel) dependem da for√ßa do gradiente em um ponto ou outro na imagem de entrada.  O m√©todo simples e "barato" (em termos de custos computacionais e de tempo) do ReLU vinculado est√° tentando lidar com esse problema. </p><br><p>  A ess√™ncia do m√©todo √© a seguinte.  Vamos substituir a fun√ß√£o de ativa√ß√£o do ReLU em uma rede neural pela mesma, que √© limitada n√£o apenas por baixo, mas tamb√©m por cima, suavizando mapas de gradiente e, em pontos espec√≠ficos, n√£o ser√° poss√≠vel obter um splash, o que n√£o permitir√° que voc√™ engane o algoritmo alterando um pixel da imagem. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"></script></p><br>  \ begin {equa√ß√£o *} f (x) = <br>  \ begin {cases} <br>  0, x &lt;0 <br>  \\ <br>  x, 0 \ leq x \ leq t <br>  \\ <br>  t, x&gt; t <br>  \ end {cases} <br>  \ end {equa√ß√£o *} <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"></script></p><br><p>  Esse m√©todo tamb√©m foi descrito no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Defesas eficientes contra ataques adversos</a> </p><br><h3 id="postroenie-ansambley-modeley">  Conjuntos de modelos de constru√ß√£o </h3><br><p><img src="https://habrastorage.org/webt/cq/2i/pg/cq2ipgeru_vavdrnk-usoydtlxw.png" alt="imagem"><br>  N√£o √© dif√≠cil enganar um modelo treinado.  Enganar dois modelos ao mesmo tempo com um objeto √© ainda mais dif√≠cil.  E se houver N tais modelos?  √â sobre isso que o m√©todo de conjunto de modelos se baseia.  Simplesmente constru√≠mos N modelos diferentes e agregamos sua sa√≠da em uma √∫nica resposta.  Se os modelos tamb√©m s√£o representados por algoritmos diferentes, √© extremamente dif√≠cil enganar esse sistema, mas √© extremamente dif√≠cil! </p><br><p>  √â bastante natural que a implementa√ß√£o de conjuntos de modelos seja uma abordagem puramente arquitet√¥nica, com muitas perguntas (quais modelos b√°sicos adotar? Como agregar as sa√≠das de modelos b√°sicos? Existe uma rela√ß√£o entre modelos? E assim por diante).  Por esse motivo, essa abordagem n√£o √© implementada no ART-IBM </p><br><h3 id="feature-squeezing">  Aperto de recurso </h3><br><p><img src="https://habrastorage.org/webt/sn/wy/wp/snwywpuqqae7pun4njrlmluseeg.png" alt="imagem"><br>  Este m√©todo, descrito em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aperto de recursos: detec√ß√£o de exemplos adversos em redes neurais profundas</a> , funciona durante a fase operacional do modelo.  Permite detectar exemplos contradit√≥rios. </p><br><p>  A id√©ia por tr√°s desse m√©todo √© a seguinte: se voc√™ treinar n modelos nos mesmos dados, mas com taxas de compacta√ß√£o diferentes, os resultados do trabalho ainda ser√£o semelhantes.  Ao mesmo tempo, o exemplo Adversarial, que funciona na rede de origem, provavelmente falhar√° em redes adicionais.  Assim, considerando a diferen√ßa de pares entre as sa√≠das da rede neural inicial e as adicionais, escolhendo o m√°ximo delas e comparando-a com um limiar pr√©-selecionado, podemos afirmar que o objeto de entrada √© Adversarial ou absolutamente v√°lido. </p><br><p>  A seguir, √© apresentado um m√©todo para obter objetos compactados usando ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.feature_squeezing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FeatureSqueezing FS = FeatureSqueezing() new_x = FS(train_x)</code> </pre> <br><p>  Terminaremos com m√©todos de prote√ß√£o.  Mas seria errado n√£o entender um ponto importante.  Se um invasor n√£o tiver acesso √†s entradas e sa√≠das do modelo, ele n√£o entender√° como os dados brutos s√£o processados ‚Äã‚Äãdentro do seu sistema antes de entrar no modelo.  Ent√£o, e somente ent√£o, todos os seus ataques ser√£o reduzidos √† classifica√ß√£o aleat√≥ria dos valores de entrada, o que √© naturalmente improv√°vel que leve ao resultado desejado. </p><br><h2 id="testirovanie">  Teste </h2><br><p>  Agora vamos falar sobre o teste de algoritmos para combater os exemplos contradit√≥rios.  Aqui, antes de tudo, √© necess√°rio entender como testaremos nosso modelo.  Se assumirmos que, de alguma maneira, um invasor pode obter acesso total a todo o modelo, √© necess√°rio testar nosso modelo usando m√©todos de ataque WhiteBox. <br><img src="https://habrastorage.org/webt/vj/pm/-w/vjpm-wuwle8c5sngov5ksw5iahq.png" alt="imagem"></p><br><p>  Em outro caso, assumimos que um invasor nunca ter√° acesso ao "interior" do nosso modelo, mas poder√°, ainda que indiretamente, influenciar os dados de entrada e ver o resultado do modelo.  Ent√£o voc√™ deve aplicar os m√©todos de ataques do BlackBox. <br><img src="https://habrastorage.org/webt/xc/h9/wo/xch9wo0pweqhlf33pzgrgdiihqm.png" alt="imagem"></p><br><p>  O algoritmo de teste geral pode ser descrito com o seguinte exemplo: </p><br><p><img src="https://habrastorage.org/webt/1d/p_/lx/1dp_lxdocm0zkd2ssmbg_fmnvba.jpeg" alt="imagem"></p><br><p>  Que haja uma rede neural treinada escrita em TensorFlow (TF NN).  Reivindicamos habilmente que nossa rede pode cair nas m√£os de um invasor ao penetrar no sistema em que o modelo est√° localizado.  Nesse caso, precisamos realizar ataques do WhiteBox.  Para isso, definimos um pool de ataques e estruturas (FoolBox - FB, CleverHans - CH, Adversarial robustness toolbox - ART) que permitem a implementa√ß√£o desses ataques.  Depois, contando quantos ataques foram bem-sucedidos, calculamos a taxa de sucesso (SR).  Se o SR nos conv√©m, terminamos o teste, caso contr√°rio, usamos um dos m√©todos de prote√ß√£o, por exemplo, implementado no ART-IBM.  Ent√£o, novamente, realizamos ataques e consideramos SR.  Fazemos essa opera√ß√£o ciclicamente, at√© que a SR nos convenha. </p><br><h2 id="vyvody">  Conclus√µes </h2><br><p>  Gostaria de terminar aqui com informa√ß√µes gerais sobre ataques, defesas e modelos de aprendizado de m√°quina de teste.  Resumindo os dois artigos, podemos concluir o seguinte: </p><br><ol><li>  N√£o acredite no aprendizado de m√°quina como uma esp√©cie de milagre que pode resolver todos os seus problemas. </li><li>  Ao aplicar algoritmos de aprendizado de m√°quina em suas tarefas, pense em qu√£o resistente esse algoritmo √© a uma amea√ßa como os exemplos adversos. </li><li>  Voc√™ pode proteger o algoritmo do lado do aprendizado de m√°quina e do lado do sistema em que este modelo √© operado. </li><li>  Teste seus modelos, especialmente nos casos em que o resultado do modelo afeta diretamente a decis√£o </li><li>  Bibliotecas como FoolBox, CleverHans, ART-IBM fornecem uma interface conveniente para atacar e defender modelos de aprendizado de m√°quina. </li></ol><br><p>  Tamb√©m neste artigo, gostaria de resumir o trabalho com as bibliotecas FoolBox, CleverHans e ART-IBM: </p><br><p>  O FoolBox √© uma biblioteca simples e compreens√≠vel para atacar redes neurais, suportando muitas estruturas diferentes. </p><br><p>  O CleverHans √© uma biblioteca que permite realizar ataques alterando muitos par√¢metros do ataque, um pouco mais complicado que o FoolBox, suporta menos estruturas. </p><br><p>  O ART-IBM √© a √∫nica biblioteca acima, que permite trabalhar com m√©todos de seguran√ßa, at√© o momento suporta apenas o TensorFlow e o Keras, mas est√° se desenvolvendo mais rapidamente que outros. </p><br><p>  Aqui, vale a pena dizer que existe outra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">biblioteca</a> para trabalhar com exemplos adversos do Baidu, mas, infelizmente, √© adequada apenas para pessoas que falam chin√™s. </p><br><p>  No pr√≥ximo artigo sobre este t√≥pico, analisaremos parte da tarefa que foi proposta para ser resolvida durante o ZeroNights HackQuest 2018, enganando uma rede neural t√≠pica usando a biblioteca FoolBox. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt438644/">https://habr.com/ru/post/pt438644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt438634/index.html">Nos escrit√≥rios, √© muito quente ou muito frio: existe uma maneira melhor de ajustar a temperatura?</a></li>
<li><a href="../pt438636/index.html">Incorpora√ß√£o defeituosa de fun√ß√µes no Go</a></li>
<li><a href="../pt438638/index.html">Analisamos o protocolo de mensagens de pager POCSAG, parte 2</a></li>
<li><a href="../pt438640/index.html">Moeda eletr√¥nica aberta de alta velocidade</a></li>
<li><a href="../pt438642/index.html">No√ß√µes b√°sicas de programa√ß√£o reativa usando RxJS</a></li>
<li><a href="../pt438646/index.html">Sobre a cria√ß√£o de imagens est√©reo de or√ßamento nos dedos (estereograma, anaglyph, estereosc√≥pio)</a></li>
<li><a href="../pt438648/index.html">Compara√ß√£o de sistemas de BI (Tableau, Power BI, Oracle, Qlik)</a></li>
<li><a href="../pt438650/index.html">Foguete 9M729. Algumas palavras sobre o "violador" do Tratado INF</a></li>
<li><a href="../pt438652/index.html">IDA de Portabeliza√ß√£o</a></li>
<li><a href="../pt438654/index.html">OpenSceneGraph: Integra√ß√£o com o Qt Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>