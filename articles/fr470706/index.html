<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â™’ï¸ ğŸ‘¨ğŸ»â€âš–ï¸ âœ’ï¸ Python + Keras + LSTM: faites un traducteur de texte en une demi-heure ğŸ™‹ğŸ» ğŸ» ğŸ‘§ğŸ»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, Habr. 

 Dans la partie prÃ©cÃ©dente, j'ai envisagÃ© de crÃ©er une simple reconnaissance de texte basÃ©e sur un rÃ©seau neuronal. Aujourd'hui, nous u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + Keras + LSTM: faites un traducteur de texte en une demi-heure</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470706/">  Salut, Habr. <br><br>  Dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">partie prÃ©cÃ©dente,</a> j'ai envisagÃ© de crÃ©er une simple reconnaissance de texte basÃ©e sur un rÃ©seau neuronal.  Aujourd'hui, nous utiliserons une approche similaire et rÃ©digerons un traducteur automatique de textes de l'anglais vers l'allemand. <br><br><img src="https://habrastorage.org/webt/gf/ft/jx/gfftjxwflb7yxrwtffish1hkqsc.jpeg"><br><br>  Pour ceux qui sont intÃ©ressÃ©s par la faÃ§on dont cela fonctionne, les dÃ©tails sont sous la coupe. <br><a name="habracut"></a><br>  <i>Remarque</i> : ce projet d'utilisation d'un rÃ©seau de neurones pour la traduction est exclusivement Ã©ducatif, donc la question Â«pourquoiÂ» n'est pas considÃ©rÃ©e.  Juste pour le plaisir.  Je ne cherche pas Ã  prouver que telle ou telle mÃ©thode est meilleure ou pire, c'Ã©tait juste intÃ©ressant de vÃ©rifier ce qui se passe.  La mÃ©thode utilisÃ©e ci-dessous est bien sÃ»r simplifiÃ©e, mais j'espÃ¨re que personne n'espÃ¨re que nous Ã©crirons un deuxiÃ¨me Lingvo dans une demi-heure. <br><br><h2>  Collecte de donnÃ©es </h2><br>  Un fichier trouvÃ© sur le rÃ©seau contenant des phrases anglaises et allemandes sÃ©parÃ©es par des tabulations a Ã©tÃ© utilisÃ© comme ensemble de donnÃ©es source.  Un ensemble de phrases ressemble Ã  ceci: <br><br><pre><code class="python hljs">Hi. Hallo! Hi. GrÃ¼ÃŸ Gott! Run! Lauf! Wow! Potzdonner! Wow! Donnerwetter! Fire! Feuer! Help! Hilfe! Help! Zu HÃ¼lf! Stop! Stopp! Wait! Warte! Go on. Mach weiter. Hello! Hallo! I ran. Ich rannte. I see. Ich verstehe. ...</code> </pre> <br>  Le fichier contient 192 000 lignes et a une taille de 13 Mo.  Nous chargeons le texte en mÃ©moire et divisons les donnÃ©es en deux blocs, pour les mots anglais et allemands. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, mode=<span class="hljs-string"><span class="hljs-string">'rt'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> file: text = file.read() sents = text.strip().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.split(<span class="hljs-string"><span class="hljs-string">'\t'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sents] data = read_text(<span class="hljs-string"><span class="hljs-string">"deutch.txt"</span></span>) deu_eng = np.array(data) deu_eng = deu_eng[:<span class="hljs-number"><span class="hljs-number">30000</span></span>,:] print(<span class="hljs-string"><span class="hljs-string">"Dictionary size:"</span></span>, deu_eng.shape) <span class="hljs-comment"><span class="hljs-comment"># Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower()</span></span></code> </pre><br>  Nous avons Ã©galement converti tous les mots en minuscules et supprimÃ© les signes de ponctuation. <br><br>  L'Ã©tape suivante consiste Ã  prÃ©parer les donnÃ©es pour le rÃ©seau neuronal.  Le rÃ©seau ne sait pas ce que sont les mots et fonctionne exclusivement avec des nombres.  Heureusement pour nous, keras a dÃ©jÃ  intÃ©grÃ© la classe Tokenizer, qui remplace les mots dans les phrases par des codes numÃ©riques. <br><br>  Son utilisation est simplement illustrÃ©e par un exemple: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences s = <span class="hljs-string"><span class="hljs-string">"To be or not to be"</span></span> eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts([s]) seq = eng_tokenizer.texts_to_sequences([s]) seq = pad_sequences(seq, maxlen=<span class="hljs-number"><span class="hljs-number">8</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'post'</span></span>) print(seq)</code> </pre><br>  L'expression Â«Ãªtre ou ne pas ÃªtreÂ» sera remplacÃ©e par le tableau [1 2 3 4 1 2 0 0], oÃ¹ il n'est pas difficile de deviner, 1 = Ã , 2 = Ãªtre, 3 = ou, 4 = pas.  Nous pouvons dÃ©jÃ  soumettre ces donnÃ©es au rÃ©seau neuronal. <br><br><h2>  Formation au rÃ©seau de neurones </h2><br>  Nos donnÃ©es sont prÃªtes numÃ©riquement.  Nous divisons le tableau en deux blocs pour les donnÃ©es d'entrÃ©e (lignes anglaises) et de sortie (lignes allemandes).  Nous prÃ©parerons Ã©galement une unitÃ© distincte pour valider le processus d'apprentissage. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1])</span></span></code> </pre><br>  Nous pouvons maintenant crÃ©er un modÃ¨le de rÃ©seau neuronal et commencer sa formation.  Comme vous pouvez le voir, le rÃ©seau neuronal contient des couches LSTM ayant des cellules mÃ©moire.  Bien que cela fonctionnerait probablement sur un rÃ©seau Â«rÃ©gulierÂ», ceux qui le souhaitent peuvent vÃ©rifier par eux-mÃªmes. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in_vocab, out_vocab, in_timesteps, out_timesteps, n)</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(n)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(out_vocab, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(optimizer=optimizers.RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model eng_vocab_size = len(eng_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> deu_vocab_size = len(deu_tokenizer.word_index) + <span class="hljs-number"><span class="hljs-number">1</span></span> eng_length, deu_length = <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span> model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, <span class="hljs-number"><span class="hljs-number">512</span></span>) num_epochs = <span class="hljs-number"><span class="hljs-number">40</span></span> model.fit(trainX, trainY.reshape(trainY.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], trainY.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>), epochs=num_epochs, batch_size=<span class="hljs-number"><span class="hljs-number">512</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, callbacks=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>)</code> </pre><br>  La formation elle-mÃªme ressemble Ã  ceci: <br><br><img src="https://habrastorage.org/webt/fj/xl/b4/fjxlb4yorszz5ojzpcih4ixlria.png"><br><br>  Le processus, comme vous pouvez le voir, n'est pas rapide et prend environ une demi-heure sur un Core i7 + GeForce 1060 pour un ensemble de 30 000 lignes.  Ã€ la fin de la formation (elle ne doit Ãªtre effectuÃ©e qu'une seule fois), le modÃ¨le est enregistrÃ© dans un fichier, puis il peut Ãªtre rÃ©utilisÃ©. <br><br>  Pour obtenir la traduction, nous utilisons la fonction Predict_classes, dont nous soumettons quelques phrases simples en entrÃ©e.  La fonction get_word est utilisÃ©e pour inverser les mots en nombres. <br><br><pre> <code class="python hljs">model = load_model(<span class="hljs-string"><span class="hljs-string">'en-de-model.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_word</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n, tokenizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word, index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.word_index.items(): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index == n: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> phrs_enc = encode_sequences(eng_tokenizer, eng_length, [<span class="hljs-string"><span class="hljs-string">"the weather is nice today"</span></span>, <span class="hljs-string"><span class="hljs-string">"my name is tom"</span></span>, <span class="hljs-string"><span class="hljs-string">"how old are you"</span></span>, <span class="hljs-string"><span class="hljs-string">"where is the nearest shop"</span></span>]) preds = model.predict_classes(phrs_enc) print(<span class="hljs-string"><span class="hljs-string">"Preds:"</span></span>, preds.shape) print(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer)) print(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>]) print(get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>], deu_tokenizer), get_word(preds[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">3</span></span>], deu_tokenizer))</code> </pre><br><h2>  RÃ©sultats </h2><br>  Maintenant, en fait, la chose la plus curieuse, ce sont les rÃ©sultats.  Il est intÃ©ressant de voir comment le rÃ©seau neuronal apprend et Â«se souvientÂ» de la correspondance entre les phrases anglaises et allemandes.  J'ai spÃ©cifiquement pris 2 phrases plus faciles et 2 plus difficiles Ã  voir la diffÃ©rence. <br><br>  <b>5 minutes de formation</b> <br><br>  Â«Il fait beau aujourd'huiÂ» - Â«das ist ist tomÂ» <br>  "Mon nom est tom" - "wie fÃ¼r tom tom" <br>  "Quel Ã¢ge avez-vous" - "wie geht ist es" <br>  "OÃ¹ est le magasin le plus proche" - "wo ist der" <br><br>  Comme vous pouvez le voir, il n'y a pour l'instant que peu de Â«hitsÂ».  Un fragment de l'expression Â«quel Ã¢ge avez-vousÂ» a confondu le rÃ©seau neuronal avec l'expression Â«comment allez-vousÂ» et a produit la traduction Â«wie geht ist esÂ» (comment allez-vous?).  Dans la phrase Â«oÃ¹ est ...Â», le rÃ©seau neuronal n'a identifiÃ© que le verbe oÃ¹ et a produit la traduction Â«wo ist derÂ» (oÃ¹ est-il?), Qui, en principe, n'est pas dÃ©nuÃ© de sens.  En gÃ©nÃ©ral, Ã  peu prÃ¨s la mÃªme chose que traduit en allemand un nouveau venu dans le groupe A1;) <br><br>  <b>10 minutes de formation</b> <br><br>  Â«Il fait beau aujourd'huiÂ» - Â«das haus ist bereitÂ» <br>  Â«Je m'appelle TomÂ» - Â«mein heiÃŸe heiÃŸe tomÂ» <br>  "Quel Ã¢ge avez-vous" - "wie alt sind sie" <br>  "OÃ¹ est le magasin le plus proche" - "wo ist paris" <br><br>  Des progrÃ¨s sont visibles.  La premiÃ¨re phrase est complÃ¨tement hors de propos.  Dans la deuxiÃ¨me phrase, le rÃ©seau neuronal a Â«apprisÂ» le verbe heiÃŸen (appelÃ©), mais Â«mein heiÃŸe heiÃŸe tomÂ» est toujours incorrect, bien que vous puissiez dÃ©jÃ  en deviner le sens.  La troisiÃ¨me phrase est dÃ©jÃ  correcte.  Dans le quatriÃ¨me, la premiÃ¨re partie correcte est Â«wo istÂ», mais le magasin le plus proche a Ã©tÃ© pour une raison quelconque remplacÃ© par paris. <br><br>  <b>30 minutes de formation</b> <br><br>  Â«Il fait beau aujourd'huiÂ» - Â«das ist ist ausÂ» <br>  "Je m'appelle Tom" - "" Tom "ist mein name" <br>  "Quel Ã¢ge avez-vous" - "wie alt sind sie" <br>  "OÃ¹ est le magasin le plus proche" - "wo ist der" <br><br>  Comme vous pouvez le voir, la deuxiÃ¨me phrase est devenue correcte, bien que le design semble quelque peu inhabituel.  La troisiÃ¨me phrase est correcte, mais les premiÃ¨re et quatriÃ¨me phrases n'ont pas encore Ã©tÃ© Â«apprisesÂ».  Avec cela <s>afin d'Ã©conomiser de l'Ã©lectricitÃ©, j'ai</s> terminÃ© le processus. <br><br><h2>  Conclusion </h2><br>  Comme vous pouvez le voir, cela fonctionne en principe.  Je voudrais mÃ©moriser une nouvelle langue avec une telle vitesse :) Bien sÃ»r, le rÃ©sultat n'est pas parfait jusqu'Ã  prÃ©sent, mais la formation sur un ensemble complet de 190 mille lignes prendrait plus d'une heure. <br><br>  Pour ceux qui veulent expÃ©rimenter par eux-mÃªmes, le code source est sous le spoiler.  Le programme peut thÃ©oriquement utiliser n'importe quelle paire de langues, pas seulement l'anglais et l'allemand (le fichier doit Ãªtre au format UTF-8).  La question de la qualitÃ© de la traduction reste Ã©galement ouverte, il y a quelque chose Ã  tester. <br><br><div class="spoiler">  <b class="spoiler_title">keras_translate.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Force CPU os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed import string import re import numpy as np import pandas as pd from keras.models import Sequential from keras.layers import Dense, LSTM, Embedding, RepeatVector from keras.preprocessing.text import Tokenizer from keras.callbacks import ModelCheckpoint from keras.preprocessing.sequence import pad_sequences from keras.models import load_model from keras import optimizers from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', 200) # Read raw text file def read_text(filename): with open(filename, mode='rt', encoding='utf-8') as file: text = file.read() sents = text.strip().split('\n') return [i.split('\t') for i in sents] data = read_text("deutch.txt") deu_eng = np.array(data) deu_eng = deu_eng[:30000,:] print("Dictionary size:", deu_eng.shape) # Remove punctuation deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]] deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]] # Convert text to lowercase for i in range(len(deu_eng)): deu_eng[i,0] = deu_eng[i,0].lower() deu_eng[i,1] = deu_eng[i,1].lower() # Prepare English tokenizer eng_tokenizer = Tokenizer() eng_tokenizer.fit_on_texts(deu_eng[:, 0]) eng_vocab_size = len(eng_tokenizer.word_index) + 1 eng_length = 8 # Prepare Deutch tokenizer deu_tokenizer = Tokenizer() deu_tokenizer.fit_on_texts(deu_eng[:, 1]) deu_vocab_size = len(deu_tokenizer.word_index) + 1 deu_length = 8 # Encode and pad sequences def encode_sequences(tokenizer, length, lines): # integer encode sequences seq = tokenizer.texts_to_sequences(lines) # pad sequences with 0 values seq = pad_sequences(seq, maxlen=length, padding='post') return seq # Split data into train and test set train, test = train_test_split(deu_eng, test_size=0.2, random_state=12) # Prepare training data trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0]) trainY = encode_sequences(deu_tokenizer, deu_length, train[:, 1]) # Prepare validation data testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0]) testY = encode_sequences(deu_tokenizer, deu_length, test[:, 1]) # Build NMT model def make_model(in_vocab, out_vocab, in_timesteps, out_timesteps, n): model = Sequential() model.add(Embedding(in_vocab, n, input_length=in_timesteps, mask_zero=True)) model.add(LSTM(n)) model.add(Dropout(0.3)) model.add(RepeatVector(out_timesteps)) model.add(LSTM(n, return_sequences=True)) model.add(Dropout(0.3)) model.add(Dense(out_vocab, activation='softmax')) model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy') return model print("deu_vocab_size:", deu_vocab_size, deu_length) print("eng_vocab_size:", eng_vocab_size, eng_length) # Model compilation (with 512 hidden units) model = make_model(eng_vocab_size, deu_vocab_size, eng_length, deu_length, 512) # Train model num_epochs = 250 history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=num_epochs, batch_size=512, validation_split=0.2, callbacks=None, verbose=1) # plt.plot(history.history['loss']) # plt.plot(history.history['val_loss']) # plt.legend(['train','validation']) # plt.show() model.save('en-de-model.h5') # Load model model = load_model('en-de-model.h5') def get_word(n, tokenizer): if n == 0: return "" for word, index in tokenizer.word_index.items(): if index == n: return word return "" phrs_enc = encode_sequences(eng_tokenizer, eng_length, ["the weather is nice today", "my name is tom", "how old are you", "where is the nearest shop"]) print("phrs_enc:", phrs_enc.shape) preds = model.predict_classes(phrs_enc) print("Preds:", preds.shape) print(preds[0]) print(get_word(preds[0][0], deu_tokenizer), get_word(preds[0][1], deu_tokenizer), get_word(preds[0][2], deu_tokenizer), get_word(preds[0][3], deu_tokenizer)) print(preds[1]) print(get_word(preds[1][0], deu_tokenizer), get_word(preds[1][1], deu_tokenizer), get_word(preds[1][2], deu_tokenizer), get_word(preds[1][3], deu_tokenizer)) print(preds[2]) print(get_word(preds[2][0], deu_tokenizer), get_word(preds[2][1], deu_tokenizer), get_word(preds[2][2], deu_tokenizer), get_word(preds[2][3], deu_tokenizer)) print(preds[3]) print(get_word(preds[3][0], deu_tokenizer), get_word(preds[3][1], deu_tokenizer), get_word(preds[3][2], deu_tokenizer), get_word(preds[3][3], deu_tokenizer)) print()</span></span></code> </pre><br></div></div><br>  Le dictionnaire lui-mÃªme est trop volumineux pour Ãªtre attachÃ© Ã  l'article, le lien est dans les commentaires. <br><br>  Comme d'habitude, toutes les expÃ©riences rÃ©ussies. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr470706/">https://habr.com/ru/post/fr470706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr470688/index.html">Ce que l'on sait sur VMworld 2019</a></li>
<li><a href="../fr470692/index.html">Comment nous avons crÃ©Ã© un nouveau site Web Rosbank et ce qui en est ressorti</a></li>
<li><a href="../fr470694/index.html">Choisir une plate-forme de marketing par e-mail: que faire attention aux entreprises russes</a></li>
<li><a href="../fr470696/index.html">Pourquoi Kaldi est-il bon pour la reconnaissance vocale? (mis Ã  jour le 12.25.2019)</a></li>
<li><a href="../fr470700/index.html">Dessus de table. MÃ©tallique Silencieux Le vÃ´tre</a></li>
<li><a href="../fr470710/index.html">Apprentissage automatique pour votre chasse Ã  plat. 2e partie</a></li>
<li><a href="../fr470714/index.html">Comment je suis allÃ© Ã  la finale de la percÃ©e numÃ©rique</a></li>
<li><a href="../fr470718/index.html">"Effets algÃ©briques" dans le langage humain</a></li>
<li><a href="../fr470720/index.html">Comment Ã©crire un contrat intelligent avec Python sur l'ontologie? Partie 2: API de stockage</a></li>
<li><a href="../fr470722/index.html">Comment Ã©crire un contrat intelligent avec Python sur l'ontologie? Partie 3: API d'exÃ©cution</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>