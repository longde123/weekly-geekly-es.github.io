<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüéì üëî üßúüèæ Almacenamiento confiable con DRBD9 y Proxmox (Parte 2: iSCSI + LVM) üÜì üç∞ üîâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En un art√≠culo anterior, examin√© la posibilidad de crear un servidor NFS tolerante a fallas usando DRBD y Proxmox. Result√≥ bastante bien, pero no desc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Almacenamiento confiable con DRBD9 y Proxmox (Parte 2: iSCSI + LVM)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417597/"><p><img src="https://habrastorage.org/getpro/habr/post_images/101/70c/524/10170c52443d67bd757a09ef22ba39e2.jpg" alt="imagen"></p><br><p>  En un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior,</a> examin√© la posibilidad de crear un servidor NFS tolerante a fallas usando DRBD y Proxmox.  Result√≥ bastante bien, pero no descansaremos en nuestros laureles y ahora trataremos de "exprimir todos los jugos" de nuestro almacenamiento. </p><br><p> En este art√≠culo, le dir√© c√≥mo crear un objetivo iSCSI tolerante a fallas de esta manera, que usaremos LVM para cortar en trozos peque√±os y usarlo para m√°quinas virtuales. </p><br><p>  Es este enfoque el que reducir√° la carga y aumentar√° la velocidad de acceso a los datos varias veces, esto es especialmente beneficioso cuando no se requiere un acceso competitivo a los datos, por ejemplo, en el caso de que necesite organizar el almacenamiento para m√°quinas virtuales. </p><a name="habracut"></a><br><h2 id="para-slov-o-drbd">  Algunas palabras sobre DRBD </h2><br><p>  DRBD es una soluci√≥n bastante simple y madura, el c√≥digo de la octava versi√≥n se adopta como parte del kernel de Linux.  De hecho, representa un espejo de red RAID1.  La novena versi√≥n introdujo soporte para qu√≥rum y replicaci√≥n con m√°s de dos nodos. </p><br><p>  De hecho, le permite combinar dispositivos de bloque en varios nodos f√≠sicos en una red compartida com√∫n. </p><br><p>  Usando DRBD puedes lograr configuraciones muy interesantes.  Hoy hablaremos sobre iSCSI y LVM. </p><br><p>  Puede obtener m√°s informaci√≥n al respecto leyendo mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior</a> , donde describ√≠ esta soluci√≥n en detalle. </p><br><h2 id="para-slov-ob-iscsi">  Algunas palabras sobre iSCSI </h2><br><p>  iSCSI es un protocolo de entrega de dispositivos de bloque a trav√©s de una red. </p><br><p>  A diferencia de NBD, admite autorizaci√≥n, resuelve fallas de red sin problemas y admite muchas otras funciones √∫tiles, y lo m√°s importante es que muestra un muy buen rendimiento. </p><br><p>  Hay una gran cantidad de sus implementaciones, algunas de ellas tambi√©n est√°n incluidas en el kernel y no requieren dificultades especiales para su configuraci√≥n y conexi√≥n. </p><br><h2 id="para-slov-ob-lvm">  Algunas palabras sobre LVM </h2><br><p>  Vale la pena mencionar que LINBIT tiene su propia soluci√≥n para Proxmox, deber√≠a funcionar de inmediato y permitir lograr un resultado similar, pero en este art√≠culo no me gustar√≠a centrarme solo en Proxmox y describir alguna soluci√≥n m√°s universal que sea adecuada tanto para Proxmox como para Cualquier otra cosa, en este ejemplo, proxmox se usa solo como un medio de orquestaci√≥n de contenedores, de hecho, puede reemplazarlo con otra soluci√≥n, por ejemplo, lanzar contenedores con un objetivo en Kubernetes. </p><br><p>  En cuanto a Proxmox espec√≠ficamente, funciona bien con LUN y LVM compartidos, utilizando solo sus propios controladores est√°ndar. </p><br><p>  Las ventajas de LVM incluyen el hecho de que su uso no es algo revolucionario nuevo e insuficientemente utilizado, sino que, por el contrario, muestra estabilidad en seco, que generalmente se requiere del almacenamiento.  Vale la pena mencionar que LVM se usa bastante activamente en otros entornos, por ejemplo, en OpenNebula o Kubernetes, y es bastante compatible all√≠. </p><br><p>  Por lo tanto, recibir√° un almacenamiento universal que se puede usar en diferentes sistemas (no solo en proxmox), utilizando solo controladores listos para usar, sin ninguna necesidad especial de modificarlo con un archivo. </p><br><p>  Desafortunadamente, al elegir una soluci√≥n para el almacenamiento, siempre tiene que hacer algunos compromisos.  Entonces, aqu√≠, esta soluci√≥n no le dar√° la misma flexibilidad que, por ejemplo, Ceph. <br>  El tama√±o del disco virtual est√° limitado por el tama√±o del grupo LVM, y el √°rea marcada para un disco virtual espec√≠fico se reasignar√° necesariamente; esto mejora en gran medida la velocidad de acceso a los datos, pero no permite Thin-Provisioning (cuando el disco virtual ocupa menos espacio de lo que realmente es).  Vale la pena mencionar que el rendimiento de LVM disminuye bastante cuando se usan instant√°neas, y por lo tanto, la posibilidad de su uso gratuito a menudo se elimina. </p><br><p>  S√≠, LVM admite grupos de Thin-Provision que carecen de este inconveniente, pero desafortunadamente su uso solo es posible en el contexto de un nodo y no hay forma de compartir un grupo de Thin-Provision para varios nodos en el cl√∫ster. </p><br><p>  Pero a pesar de estas deficiencias, debido a su simplicidad, LVM todav√≠a no permite a los competidores evitarlo y sacarlo completamente del campo de batalla. </p><br><p>  Con una sobrecarga bastante peque√±a, LVM a√∫n proporciona una soluci√≥n muy r√°pida, estable y bastante flexible. </p><br><h1 id="obschaya-shema">  Esquema general </h1><br><ul><li>  Tenemos <strong>tres nodos</strong> </li><li>  Cada nodo tiene un <strong>dispositivo drbd</strong> distribuido. </li><li>  En la parte superior del dispositivo drbd, se inicia un <strong>contenedor LXC</strong> con un objetivo iSCSI. </li><li>  El objetivo est√° conectado a los tres nodos. </li><li>  Se <strong>ha</strong> creado un <strong>grupo LVM</strong> en el objetivo conectado. </li><li>  Si es necesario, el <strong>contenedor LXC</strong> puede moverse a otro nodo, junto con el <strong>objetivo iSCSI</strong> </li></ul><br><h1 id="nastroyka">  Personalizaci√≥n </h1><br><p>  Descubrimos la idea ahora, pasemos a la implementaci√≥n. </p><br><p>  Por defecto <strong>, la octava versi√≥n de drbd</strong> se suministra <strong>con el kernel de Linux</strong> , desafortunadamente <strong>no</strong> nos <strong>conviene</strong> y necesitamos instalar la novena versi√≥n del m√≥dulo. </p><br><p>  Conecte el repositorio de LINBIT e instale todo lo que necesita: </p><br><pre><code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> : <code>pve-headers</code> kernel necesarios para compilar el m√≥dulo </li><li>  <code>drbd-dkms</code> - m√≥dulo de kernel en formato DKMS </li><li>  <code>drbd-utils</code> - utilidades b√°sicas de gesti√≥n de DRBD </li><li>  <code>drbdtop</code> es una herramienta interactiva como top solo para DRBD </li></ul><br><p>  Despu√©s de instalar el <strong>m√≥dulo,</strong> verifique si todo est√° bien con √©l: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Si ve la <strong>octava versi√≥n</strong> en la salida del comando, entonces algo sali√≥ mal y se carga el m√≥dulo del n√∫cleo <strong>en el √°rbol</strong> .  Compruebe el <code>dkms status</code> averiguar cu√°l es el motivo. </p><br><p>  Cada nodo que tengamos tendr√° el mismo <strong>dispositivo drbd</strong> ejecut√°ndose sobre las particiones regulares.  Primero necesitamos preparar esta secci√≥n para drbd en cada nodo. </p><br><p>  Dicha partici√≥n puede ser cualquier <strong>dispositivo de bloque</strong> , puede ser lvm, zvol, una partici√≥n de disco o todo el disco.  En este art√≠culo usar√© un disco nvme separado con una partici√≥n en drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  Vale la pena se√±alar que los nombres de los dispositivos tienden a cambiar a veces, por lo que es mejor tomarlo inmediatamente como un h√°bito para usar un enlace simb√≥lico constante al dispositivo. </p><br><p>  Puede encontrar un enlace simb√≥lico para <code>/dev/nvme1n1p1</code> esta manera: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Describimos nuestro recurso en los tres nodos: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/tgt1.res resource tgt1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Es recomendable utilizar una <strong>red separada</strong> para la sincronizaci√≥n drbd. </p><br><p>  Ahora cree los metadatos para drbd y ejec√∫telo: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md tgt1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up tgt1</span></span></code> </pre> <br><p>  Repita estos pasos en los tres nodos y verifique el estado: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status tgt1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Ahora nuestro disco <strong>inconsistente est√°</strong> en los tres nodos, esto se debe a que drbd no sabe qu√© disco debe tomarse como el original.  Debemos marcar uno de ellos como <strong>Primario</strong> para que su estado se sincronice con los otros nodos: </p><br><pre> <code class="bash hljs">drbdadm primary --force tgt1 drbdadm secondary tgt1</code> </pre> <br><p>  Inmediatamente despu√©s de esto, comenzar√° la <strong>sincronizaci√≥n</strong> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status tgt1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  No tenemos que esperar a que termine y podemos llevar a cabo m√°s pasos en paralelo.  Se pueden ejecutar en <strong>cualquier nodo</strong> , independientemente de su estado actual del disco local en DRBD.  Todas las solicitudes ser√°n redirigidas autom√°ticamente al dispositivo con el estado <strong>UpToDate</strong> . </p><br><p>  No olvide activar la <strong>ejecuci√≥n autom√°tica del</strong> servicio drbd en los nodos: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configurar un contenedor LXC </h2><br><p>  Omitiremos la parte de configuraci√≥n del <strong>cl√∫ster Proxmox</strong> de tres nodos, esta parte est√° bien descrita en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">wiki oficial</a> </p><br><p>  Como dije antes, nuestro <strong>objetivo iSCSI</strong> funcionar√° en un <strong>contenedor LXC</strong> .  Mantendremos el contenedor en el dispositivo <code>/dev/drbd100</code> que acabamos de crear. </p><br><p>  Primero necesitamos crear un <strong>sistema de archivos</strong> en √©l: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> por defecto incluye <strong>protecci√≥n multimount</strong> a nivel del sistema de archivos, en principio, podemos prescindir de √©l, porque  DRBD tiene su propia protecci√≥n por defecto, simplemente proh√≠be el segundo <strong>primario</strong> para el dispositivo, pero la precauci√≥n no nos perjudica. </p><br><p>  Ahora descargue la plantilla de Ubuntu: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Y crea nuestro contenedor a partir de √©l: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=tgt1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  En este comando, indicamos que el <strong>sistema ra√≠z de</strong> nuestro contenedor estar√° en el dispositivo <code>/dev/drbd100</code> y agregaremos el par√°metro <code>shared=1</code> para permitir la <strong>migraci√≥n del</strong> contenedor entre nodos. </p><br><p>  Si algo sali√≥ mal, siempre puede solucionarlo a trav√©s de la interfaz <strong>Proxmox</strong> o en la <code>/etc/pve/lxc/101.conf</code> contenedor <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox desempaquetar√° la plantilla y preparar√° <strong>el sistema ra√≠z del</strong> contenedor para nosotros.  Despu√©s de eso podemos lanzar nuestro contenedor: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-iscsi-targeta">  Configurar un objetivo iSCSI. </h2><br><p>  De todo el conjunto de <strong>objetivos</strong> , eleg√≠ <strong>istgt</strong> , ya que tiene el mayor rendimiento y funciona en el espacio del usuario. </p><br><p>  Ahora iniciemos sesi√≥n en nuestro contenedor: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Instalar actualizaciones e <strong>istgt</strong> : </p><br><pre> <code class="hljs sql">apt-get <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">upgrade</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> istgt</code> </pre> <br><p>  Cree un archivo que le entregaremos a trav√©s de la red: </p><br><pre> <code class="hljs powershell">mkdir <span class="hljs-literal"><span class="hljs-literal">-p</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> fallocate <span class="hljs-literal"><span class="hljs-literal">-l</span></span> <span class="hljs-number"><span class="hljs-number">740</span></span>G /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/target1.img</code> </pre> <br><p>  Ahora necesitamos escribir una configuraci√≥n para <strong>istgt</strong> <code>/etc/istgt/istgt.conf</code> : </p><br><pre> <code class="hljs sql">[Global] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Global section"</span></span> NodeBase <span class="hljs-string"><span class="hljs-string">"iqn.2018-07.org.example.tgt1"</span></span> PidFile /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/run/istgt.pid AuthFile /etc/istgt/auth.conf MediaDirectory /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/istgt LogFacility <span class="hljs-string"><span class="hljs-string">"local7"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Timeout</span></span> <span class="hljs-number"><span class="hljs-number">30</span></span> NopInInterval <span class="hljs-number"><span class="hljs-number">20</span></span> DiscoveryAuthMethod <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> MaxSessions <span class="hljs-number"><span class="hljs-number">16</span></span> MaxConnections <span class="hljs-number"><span class="hljs-number">4</span></span> MaxR2T <span class="hljs-number"><span class="hljs-number">32</span></span> MaxOutstandingR2T <span class="hljs-number"><span class="hljs-number">16</span></span> DefaultTime2Wait <span class="hljs-number"><span class="hljs-number">2</span></span> DefaultTime2Retain <span class="hljs-number"><span class="hljs-number">60</span></span> FirstBurstLength <span class="hljs-number"><span class="hljs-number">262144</span></span> MaxBurstLength <span class="hljs-number"><span class="hljs-number">1048576</span></span> MaxRecvDataSegmentLength <span class="hljs-number"><span class="hljs-number">262144</span></span> InitialR2T Yes ImmediateData Yes DataPDUInOrder Yes DataSequenceInOrder Yes ErrorRecoveryLevel <span class="hljs-number"><span class="hljs-number">0</span></span> [UnitControl] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Internal Logical Unit Controller"</span></span> AuthMethod CHAP Mutual AuthGroup AuthGroup10000 Portal UC1 <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:<span class="hljs-number"><span class="hljs-number">3261</span></span> Netmask <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> [PortalGroup1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"SINGLE PORT TEST"</span></span> Portal DA1 <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.11</span></span>:<span class="hljs-number"><span class="hljs-number">3260</span></span> [InitiatorGroup1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Initiator Group1"</span></span> InitiatorName <span class="hljs-string"><span class="hljs-string">"ALL"</span></span> Netmask <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> [LogicalUnit1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Hard Disk Sample"</span></span> TargetName disk1 TargetAlias <span class="hljs-string"><span class="hljs-string">"Data Disk1"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Mapping</span></span> PortalGroup1 InitiatorGroup1 AuthMethod <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> AuthGroup AuthGroup1 UseDigest <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> UnitType Disk LUN0 <span class="hljs-keyword"><span class="hljs-keyword">Storage</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/target1.img <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span></code> </pre> <br><p>  Reiniciar istgt: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> istgt</code> </pre> <br><p>  Esto completa la configuraci√≥n del objetivo </p><br><h2 id="nastroyka-ha">  Configuraci√≥n HA </h2><br><p>  Ahora podemos pasar a la configuraci√≥n de <strong>HA-manager</strong> .  Creemos un grupo HA separado para nuestro dispositivo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd tgt1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Nuestro <strong>recurso</strong> solo funcionar√° en los nodos especificados para este grupo.  Agregue nuestro contenedor a este grupo: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=tgt1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><h2 id="rekomendacii-i-tyuning">  Recomendaciones y puesta a punto </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Como se√±al√© anteriormente, siempre es recomendable usar una red separada para la replicaci√≥n.  Es muy recomendable utilizar <strong>adaptadores de red de 10 gigabits</strong> , de lo contrario, se encontrar√° con la velocidad del puerto. <br>  Si la replicaci√≥n parece lo suficientemente lenta, pruebe algunas de las opciones para <strong>DRBD</strong> .  Aqu√≠ est√° la configuraci√≥n, que en mi opini√≥n es √≥ptima para mi <strong>red 10G</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Puede obtener m√°s informaci√≥n sobre cada par√°metro de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial de DRBD.</a> </p><br><h5 id="open-iscsi">  Abrir iSCSI </h5><br><p>  Como no utilizamos rutas m√∫ltiples, en nuestro caso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se recomienda</a> deshabilitar las comprobaciones de conexi√≥n peri√≥dicas en los clientes, as√≠ como aumentar los tiempos de espera para la recuperaci√≥n de la sesi√≥n en <code>/etc/iscsi/iscsid.conf</code> . </p><br><pre> <code class="hljs powershell">node.conn[<span class="hljs-number"><span class="hljs-number">0</span></span>].timeo.noop_out_interval = <span class="hljs-number"><span class="hljs-number">0</span></span> node.conn[<span class="hljs-number"><span class="hljs-number">0</span></span>].timeo.noop_out_timeout = <span class="hljs-number"><span class="hljs-number">0</span></span> node.session.timeo.replacement_timeout = <span class="hljs-number"><span class="hljs-number">86400</span></span></code> </pre> <br><h2 id="ispolzovanie">  Uso </h2><br><h4 id="proxmox">  Proxmox </h4><br><p>  El <strong>objetivo iSCSI</strong> resultante puede conectarse inmediatamente a Proxmox, sin olvidar desmarcar <strong>Usar LUN directamente</strong> . </p><br><p><img src="https://habrastorage.org/webt/uw/j3/pu/uwj3pusr-nf9bc7neisd5x-fcsg.png"></p><br><p>  Inmediatamente despu√©s de eso, ser√° posible crear LVM encima, no olvides marcar la casilla <strong>compartida</strong> : </p><br><p><img src="https://habrastorage.org/webt/j1/ob/mw/j1obmwcwhz-e6krjix72pmiz118.png"></p><br><h4 id="drugie-sredy">  Otros ambientes </h4><br><p>  Si planea usar esta soluci√≥n en un entorno diferente, es posible que necesite instalar una extensi√≥n de cl√∫ster para LVM en el momento en que haya dos implementaciones.  <strong>CLVM</strong> y <strong>lvmlockd</strong> . </p><br><p>  La configuraci√≥n de <strong>CLVM</strong> no <strong>es</strong> trivial y requiere un administrador de cl√∫ster que funcione. <br>  <strong>Mientras que</strong> como segundo m√©todo, <strong>lvmlockd</strong> a√∫n no se ha probado completamente y apenas comienza a aparecer en repositorios estables. </p><br><p>  Recomiendo leer un excelente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>art√≠culo sobre bloqueo en LVM</strong></a> </p><br><p>  Cuando se usa <strong>LVM</strong> con <strong>Proxmox,</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">no se requiere la</a> adici√≥n de cl√∫ster, ya que proxmox proporciona la administraci√≥n del volumen, que actualiza y monitorea los metadatos de LVM de forma independiente.  Lo mismo se aplica a <strong>OpenNebula</strong> , como lo indica claramente la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n oficial</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es417597/">https://habr.com/ru/post/es417597/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es417587/index.html">Medios de comunicaci√≥n: los ataques cibern√©ticos a gran escala aceleraron el crecimiento de la capitalizaci√≥n de las empresas de la industria de seguridad de la informaci√≥n</a></li>
<li><a href="../es417589/index.html">Siete reglas simples para hacer que Internet sea accesible para todos</a></li>
<li><a href="../es417591/index.html">C√≥mo "aprender" ingl√©s en un a√±o por su cuenta o un art√≠culo para aquellos que no trabajaron con ingl√©s</a></li>
<li><a href="../es417593/index.html">NewSQL = NoSQL + ACID</a></li>
<li><a href="../es417595/index.html">Antig√ºedades: Palm OS, c√≥digo eficiente y fotos repugnantes</a></li>
<li><a href="../es417599/index.html">Fintech digest: los reguladores financieros necesitan IA para trabajar en condiciones modernas</a></li>
<li><a href="../es417601/index.html">Elige un servidor. ¬øQu√© buscar? Lista de verificaci√≥n</a></li>
<li><a href="../es417603/index.html">Anuncio de un mitap m√≥vil: ¬øQu√© hacer cuando la aplicaci√≥n se ha vuelto grande?</a></li>
<li><a href="../es417605/index.html">Conceptos b√°sicos de modelado 3D para impresi√≥n 3D</a></li>
<li><a href="../es417607/index.html">Las pruebas A / B no funcionan. Comprueba lo que est√°s haciendo mal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>