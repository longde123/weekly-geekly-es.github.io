<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游끨 游꽄 游댲 RedisPipe - M치s diversi칩n juntos 游뱗 游녪游낖 游땾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cuando pienso en c칩mo funcionan los clientes ingenuos de RPC, recuerdo una broma: 
 El tribunal 
 "Demandado, 쯣or qu칠 mataste a una mujer?" 
 - Estoy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RedisPipe - M치s diversi칩n juntos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/joom/blog/438026/"><p>  Cuando pienso en c칩mo funcionan los clientes ingenuos de RPC, recuerdo una broma: </p><br><blockquote>  El tribunal <br>  "Demandado, 쯣or qu칠 mataste a una mujer?" <br>  - Estoy en el autob칰s, el conductor se acerca a la mujer y le exige que compre un boleto.  La mujer abri칩 su bolso, sac칩 su bolso, cerr칩 su bolso, abri칩 su bolso, cerr칩 su bolso, abri칩 su bolso, puso su bolso all칤, cerr칩 su bolso, abri칩 su bolso, tom칩 su dinero, abri칩 su bolso, tom칩 su bolso, cerr칩 su bolso, abri칩 su bolso y puso su bolso all칤 , cerr칩 su bolso, abri칩 su bolso, puso su bolso all칤. <br>  - Y que? <br>  - El controlador le dio un boleto.  Una mujer abri칩 su bolso, sac칩 su bolso, cerr칩 su bolso, abri칩 su bolso, cerr칩 su bolso, abri칩 su bolso, puso su bolso all칤, cerr칩 su bolso, abri칩 su bolso, coloc칩 su boleto, cerr칩 su bolso, abri칩 su bolso, sac칩 su bolso, cerr칩 su bolso, abri칩 su bolso , puse el bolso all칤, cerr칠 el bolso, abr칤 el bolso, puse el bolso all칤, cerr칠 el bolso. <br>  "Toma el cambio", lleg칩 la voz del controlador.  La mujer ... abri칩 su bolso ... <br>  "S칤, no es suficiente matarla", el fiscal no puede soportar. <br>  "As칤 que lo hice". <br>  춸 S. Altov </blockquote><p><img src="https://habrastorage.org/webt/qx/ne/-b/qxne-bocftxvmk99ouizmn3iqyo.jpeg"></p><a name="habracut"></a><br><p>  Aproximadamente lo mismo sucede en el proceso de solicitud-respuesta, si lo aborda fr칤volamente: </p><br><ul><li>  el proceso del usuario escribe una solicitud serializada en el socket, de hecho la copia en el buffer del socket en el nivel del sistema operativo; <br>  Esta es una operaci칩n bastante dif칤cil, porque  es necesario hacer un cambio de contexto (incluso si puede ser "f치cil"); </li><li>  cuando al sistema operativo le parece que se puede escribir algo en la red, se forma un paquete (la solicitud se copia nuevamente) y se env칤a a la tarjeta de red; </li><li>  la tarjeta de red escribe el paquete en la red (posiblemente despu칠s del almacenamiento en b칰fer); </li><li>  (en el camino, un paquete puede almacenarse varias veces en los enrutadores); </li><li>  finalmente, el paquete llega al host de destino y se almacena en la tarjeta de red; </li><li>  la tarjeta de red env칤a una notificaci칩n al sistema operativo, y cuando el sistema operativo encuentra la hora, copia el paquete en su b칰fer y establece el indicador de listo en el descriptor de archivo; </li><li>  (a칰n debe recordar enviar el ACK en respuesta); </li><li>  despu칠s de un tiempo, la aplicaci칩n del servidor se da cuenta de que el descriptor est치 listo (usando epoll) y alg칰n d칤a copia la solicitud en el b칰fer de la aplicaci칩n; </li><li>  y finalmente, la aplicaci칩n del servidor procesa la solicitud. </li></ul><br><p>  Como saben, la respuesta se transmite exactamente de la misma manera solo en la direcci칩n opuesta.  Por lo tanto, cada solicitud pasa un tiempo notable en el sistema operativo para su mantenimiento, y cada respuesta pasa el mismo tiempo nuevamente. </p><br><p>  Esto se hizo especialmente notable despu칠s de Meltdown / Spectre, ya que los parches lanzados condujeron a un aumento significativo en el costo de las llamadas al sistema.  A principios de enero de 2018, nuestro cl칰ster Redis de repente comenz칩 a consumir una CPU y media o dos veces m치s, porque  Amazon ha aplicado parches de kernel apropiados para cubrir estas vulnerabilidades.  (Es cierto que Amazon aplic칩 una nueva versi칩n del parche un poco m치s tarde, y el consumo de CPU cay칩 casi a los niveles anteriores. Pero el conector ya ha comenzado a nacer). </p><br><p> Desafortunadamente, todos los conectores Go ampliamente conocidos para Redis y Memcached funcionan de la siguiente manera: el conector crea un grupo de conexiones, y cuando necesita enviar una solicitud, extrae una conexi칩n del grupo, escribe una solicitud y luego espera una respuesta.  (Es especialmente triste que el conector a Memcached haya sido escrito por el propio Brad Fitzpatrick). Y algunos conectores tienen una implementaci칩n tan infructuosa de este grupo que el proceso de eliminar la conexi칩n del grupo se convierte en una botnet en s칤 mismo. </p><br><p>  Hay dos formas de facilitar este arduo trabajo de reenv칤o de solicitud / respuesta: </p><br><ol><li>  Utilice el acceso directo a la tarjeta de red: DPDK, netmap, PF_RING, etc. </li><li>  No env칤e cada solicitud / respuesta como un paquete separado, sino comb칤nelas, si es posible, en paquetes m치s grandes, es decir, extienda la sobrecarga de trabajar con la red para varias solicitudes.  Juntos m치s divertido! </li></ol><br><p>  La primera opci칩n, por supuesto, es posible.  Pero, en primer lugar, esto es para los valientes, ya que debe escribir la implementaci칩n de TCP / IP usted mismo (por ejemplo, como en ScyllaDB).  Y en segundo lugar, de esta manera facilitamos la situaci칩n solo en un lado, en el que nosotros mismos escribimos.  No quiero volver a escribir Redis (todav칤a), por lo que los servidores consumir치n la misma cantidad, incluso si el cliente usa el genial DPDK. </p><br><p>  La segunda opci칩n es mucho m치s simple y, lo m치s importante, facilita la situaci칩n inmediatamente en el cliente y en el servidor.  Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una base de datos en memoria se</a> jacta de que puede servir a millones de RPS, mientras que Redis no puede atender a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">par de cientos de miles</a> .  Sin embargo, este 칠xito no es tanto la implementaci칩n de esa base en memoria como la decisi칩n una vez aceptada de que el protocolo ser치 completamente as칤ncrono, y los clientes deben usar esta asincron칤a siempre que sea posible.  Lo que muchos clientes (especialmente los utilizados en los puntos de referencia) implementan con 칠xito enviando solicitudes a trav칠s de una conexi칩n TCP y, si es posible, envi치ndolas juntas a la red. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Un art칤culo bien conocido</a> muestra que Redis tambi칠n puede entregar un mill칩n de respuestas por segundo si se utiliza la canalizaci칩n.  La experiencia personal en el desarrollo de historias en memoria tambi칠n indica que la canalizaci칩n reduce significativamente el consumo de CPU SYS y le permite usar el procesador y la red de manera mucho m치s eficiente. </p><br><p>  La 칰nica pregunta es c칩mo usar la canalizaci칩n, si en la solicitud las solicitudes en Redis a menudo llegan de una en una.  Y si falta un servidor y Redis Cluster se usa con una gran cantidad de fragmentos, incluso cuando se encuentra un paquete de solicitudes, se divide en solicitudes individuales para cada fragmento. </p><br><p>  La respuesta, por supuesto, es "obvia": realice un trazado de tuber칤a impl칤cito, recopile las solicitudes de todas las gorutinas que se ejecutan en paralelo a un servidor Redis y las env칤e a trav칠s de una conexi칩n. </p><br><p>  Por cierto, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tendido</a> impl칤cito de tuber칤as no es tan raro en los conectores en otros idiomas: nodejs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">node_redis</a> , C # <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RedisBoost</a> , python <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aioredis</a> y muchos otros.  Muchos de estos conectores est치n escritos en la parte superior de los bucles de eventos y, por lo tanto, parece natural recopilar solicitudes de "flujos de c치lculo" paralelos.  En Go, se promueve el uso de interfaces s칤ncronas y, aparentemente, porque pocas personas deciden organizar su propio "bucle". </p><br><p>  Quer칤amos usar Redis de la manera m치s eficiente posible y, por lo tanto, decidimos escribir un nuevo conector "mejor" (tm): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RedisPipe</a> . </p><br><h2 id="kak-sdelat-neyavnyy-payplayning">  쮺칩mo hacer un tendido de tuber칤a impl칤cito? </h2><br><p>  El esquema b치sico: </p><br><ul><li>  Las rutinas de la l칩gica de la aplicaci칩n no escriben solicitudes directamente en la red, sino que las pasan al recolector de rutinas; </li><li>  si es posible, el recolector recopila un mont칩n de solicitudes, las escribe en la red y las pasa al lector de rutina; </li><li>  Goroutine-reader lee las respuestas de la red, las compara con las solicitudes correspondientes y notifica a los goroutines de la l칩gica acerca de la respuesta recibida. </li></ul><br><p> Algo debe ser notificado sobre la respuesta.  Un astuto programador de Go sin duda dir치: "춰A trav칠s del canal!" <br>  Pero esta no es la 칰nica primitiva de sincronizaci칩n posible y no la m치s eficiente incluso en el entorno Go.  Y dado que diferentes personas tienen diferentes necesidades, haremos que el mecanismo sea extensible, permitiendo al usuario implementar la interfaz (llam칠mosla <code>Future</code> ): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Future <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> { Resolve(val <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span>{}) }</code> </pre> <br><p>  Y luego el esquema b치sico se ver치 as칤: </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> future <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { req Request fut Future } <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Conn <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c net.Conn futmtx sync.Mutex wfutures []future futtimer *time.Timer rfutures <span class="hljs-keyword"><span class="hljs-keyword">chan</span></span> []future } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r Request, f Future)</span></span></span></span> { c.futmtx.Lock() <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> c.futmtx.Unlock() c.wfutures = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(c.wfutures, future{req: r, fut: f}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(c.wfutures) == <span class="hljs-number"><span class="hljs-number">1</span></span> { futtimer.Reset(<span class="hljs-number"><span class="hljs-number">100</span></span>*time.Microsecond) } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">writer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> c.futtimer.C { c.futmtx.Lock() futures, c.wfutures = c.wfutures, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> c.futmtx.Unlock() <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> b []<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, ft := <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> futures { b = AppendRequest(b, ft.req) } _, _err := ccWrite(b) c.rfutures &lt;- futures } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reader</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { rd := bufio.NewReader(cc) <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> futures []future <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> { response, _err := ParseResponse(rd) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(futures) == <span class="hljs-number"><span class="hljs-number">0</span></span> { futures = &lt;- c.rfutures } futures[<span class="hljs-number"><span class="hljs-number">0</span></span>].fut.Resolve(response) futures = futures[<span class="hljs-number"><span class="hljs-number">1</span></span>:] } }</code> </pre> <br><p>  Por supuesto, este es un c칩digo muy simplificado.  Omitido </p><br><ul><li>  establecimiento de conexi칩n; </li><li>  Tiempos de espera de E / S </li><li>  manejo de errores de lectura / escritura; </li><li>  restablecer la conexi칩n; </li><li>  la capacidad de cancelar la solicitud antes de enviarla a la red; </li><li>  optimizando la asignaci칩n de memoria (reutilizando el buffer y las matrices de futuros). </li></ul><br><p>  Cualquier error de E / S (incluido un tiempo de espera) en el c칩digo real conduce a una resoluci칩n de todos los errores futuros correspondientes a las solicitudes enviadas y en espera de ser enviadas. <br>  La capa de conexi칩n no est치 involucrada en el reintento de la solicitud, y si es necesario (y posible) volver a intentar la solicitud, se puede hacer en un nivel superior de abstracci칩n (por ejemplo, en la implementaci칩n del soporte de Redis Cluster que se describe a continuaci칩n). </p><br><p>  Observaci칩n  Inicialmente, el circuito parec칤a un poco m치s complicado.  Pero en el proceso de experimentos simplificado a tal opci칩n. </p><br><p>  Observaci칩n 2. Se imponen requisitos muy estrictos al futuro. M칠todo de resoluci칩n: debe ser lo m치s r치pido posible, pr치cticamente sin bloqueo y en ning칰n caso de p치nico.  Esto se debe al hecho de que se llama sincr칩nicamente en el ciclo del lector, y cualquier "freno" inevitablemente conducir치 a la degradaci칩n.  La implementaci칩n de Future.Resolve debe hacer el m칤nimo necesario de acciones lineales: despertar al expectante;  tal vez maneje el error y env칤e un reintento asincr칩nico (utilizado en la implementaci칩n del soporte de cl칰ster). </p><br><h2 id="effekt">  Efecto </h2><br><p>  춰Un buen punto de referencia es la mitad del art칤culo! </p><br><p>  Un buen punto de referencia es uno que est칠 lo m치s cerca posible de combatir el uso en t칠rminos de efectos observados.  Y esto no es f치cil de hacer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La opci칩n de referencia</a> , que me parece bastante similar a la real: </p><br><ul><li>  el "script" principal emula 5 clientes paralelos, </li><li>  en cada "cliente", por cada 300-1000 rps "deseadas", se activa la gorutina (se activan 3 gorutinas por 1000 rps, se ejecutan 124 gorutinas por 128000 rps), </li><li>  gorutin usa una instancia separada del limitador de velocidad y env칤a solicitudes en series aleatorias, de 5 a 15 solicitudes. </li></ul><br><p>  La aleatoriedad de la serie de consultas nos permite lograr una distribuci칩n aleatoria de la serie en la l칤nea de tiempo, que refleja m치s correctamente la carga real. </p><br><div class="spoiler">  <b class="spoiler_title">Texto oculto</b> <div class="spoiler_text"><p>  Las opciones incorrectas fueron: <br>  a) utilice un limitador de velocidad para todas las gorutinas del "cliente" y recurra a 칠l para cada solicitud; esto lleva a un consumo excesivo de CPU por el propio limitador de velocidad, as칤 como a una mayor rotaci칩n de tiempo de goroutin, lo que degrada el rendimiento de RedisPipe a rps medias (pero inexplicablemente mejora a la altura); <br>  b) utilice un limitador de velocidad para todas las gorutinas del "cliente" y env칤e solicitudes en serie: el limitador de velocidad no est치 consumiendo tanto la CPU, pero la alternancia de gorutinas en el tiempo solo aumenta; <br>  c) use un limitador de velocidad para cada goroutine, pero env칤e la misma serie de 10 solicitudes; en este escenario, las goroutines se despiertan demasiado simult치neamente, lo que mejora injustamente los resultados de RedisPipe. </p></div></div><br><p>  Las pruebas se realizaron en una instancia de AWS c5-2xlarge de cuatro n칰cleos.  La versi칩n de Redis es 5.0. </p><br><p>  La relaci칩n de la intensidad de consulta deseada, la intensidad total resultante y consumida por el r치bano de la CPU: </p><br><table><thead><tr><th>  rps previstos </th><th>  redigo <br>  rps /% cpu </th><th>  redispipe no espere <br>  rps /% cpu </th><th>  redispipe 50췃s <br>  rps /% cpu </th><th>  redispipe 150췃s <br>  rps /% cpu </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  5015/7% </td><td>  5015/6% </td><td>  5015/6% </td><td>  5015/6% </td></tr><tr><td>  2000 * 5 </td><td>  10022/11% </td><td>  10022/10% </td><td>  10022/10% </td><td>  10022/10% </td></tr><tr><td>  4000 * 5 </td><td>  20036/21% </td><td>  20036/18% </td><td>  20035/17% </td><td>  20035/15% </td></tr><tr><td>  8000 * 5 </td><td>  40020/45% </td><td>  40062/37% </td><td>  40060/26% </td><td>  40056/19% </td></tr><tr><td>  16000 * 5 </td><td>  79994/71% </td><td>  80102/58% </td><td>  80096/33% </td><td>  80090/23% </td></tr><tr><td>  32000 * 5 </td><td>  159590/96% </td><td>  160 180/80% </td><td>  160,167 / 39% </td><td>  160150/29% </td></tr><tr><td>  64000 * 5 </td><td>  187774/99% </td><td>  320 313/98% </td><td>  320,283 / 47% </td><td>  320,258 / 37% </td></tr><tr><td>  92000 * 5 </td><td>  183206/99% </td><td>  480,443 / 97% </td><td>  480,407 / 52% </td><td>  480 366/42% </td></tr><tr><td>  128000 * 5 </td><td>  179744/99% </td><td>  640,484 / 97% </td><td>  640,488 / 55% </td><td>  640,428 / 46% </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/g-/d7/7k/g-d77kznsmwmgejiyplpipwv7t4.png" alt="Solicitar tarifa"><img src="https://habrastorage.org/webt/cs/xf/lz/csxflztfjbnmzc0kuhpnqddeokq.png" alt="CPU Redis"></p><br><p>  Puede notar que con un conector que funciona de acuerdo con el esquema cl치sico (solicitud / respuesta + grupo de conexiones), Redis consume r치pidamente el n칰cleo del procesador, despu칠s de lo cual se convierte en una tarea imposible obtener m치s de 190 krps. </p><br><p>  RedisPipe le permite exprimir toda la potencia requerida de Redis.  Y cuanto m치s nos detenemos para recopilar solicitudes paralelas, menos Redis consume CPU.  Ya se obtiene un beneficio tangible a 4krps del cliente (20krps en total) si se usa una pausa de 150 microsegundos. </p><br><p>  Incluso si la pausa no se usa expl칤citamente cuando Redis descansa en la CPU, el retraso aparece por s칤 solo.  Adem치s, las solicitudes comienzan a ser almacenadas por el sistema operativo.  Esto permite que RedisPipe aumente el n칰mero de solicitudes ejecutadas con 칠xito cuando el conector cl치sico ya est치 bajando sus patas. </p><br><p>  Este es el resultado principal, para lo cual fue necesario crear un nuevo conector. </p><br><p>  쯈u칠 sucede entonces con el consumo de CPU en el cliente y con las solicitudes retrasadas? </p><br><table><thead><tr><th>  rps previstos </th><th>  redigo <br>  % cpu / ms </th><th>  redispipe nowait <br>  % cpu / ms </th><th>  redispipe 50ms <br>  % cpu / ms </th><th>  redispipe 150ms <br>  % cpu / ms </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  13 / 0.03 </td><td>  20 / 0.04 </td><td>  46 / 0.16 </td><td>  44 / 0.26 </td></tr><tr><td>  2000 * 5 </td><td>  25 / 0.03 </td><td>  33 / 0.04 </td><td>  77 / 0.16 </td><td>  71 / 0.26 </td></tr><tr><td>  4000 * 5 </td><td>  48 / 0.03 </td><td>  60 / 0.04 </td><td>  124 / 0.16 </td><td>  107 / 0.26 </td></tr><tr><td>  8000 * 5 </td><td>  94 / 0.03 </td><td>  119 / 0.04 </td><td>  178 / 0.15 </td><td>  141 / 0.26 </td></tr><tr><td>  16000 * 5 </td><td>  184 / 0.04 </td><td>  206 / 0.04 </td><td>  228 / 0.15 </td><td>  177 / 0.25 </td></tr><tr><td>  32000 * 5 </td><td>  341 / 0.08 </td><td>  322 / 0.05 </td><td>  280 / 0.15 </td><td>  226 / 0.26 </td></tr><tr><td>  64000 * 5 </td><td>  316 / 1.88 </td><td>  469 / 0.08 </td><td>  345 / 0.16 </td><td>  307 / 0.26 </td></tr><tr><td>  92000 * 5 </td><td>  313 / 2,88 </td><td>  511 / 0.16 </td><td>  398 / 0.17 </td><td>  366 / 0.27 </td></tr><tr><td>  128000 * 5 </td><td>  312 / 3.54 </td><td>  509 / 0.37 </td><td>  441 / 0,19 </td><td>  418 / 0.29 </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/fp/ws/ra/fpwsra-pgtexl6ua7crpc4vt5fa.png" alt="CPU del cliente"><img src="https://habrastorage.org/webt/kw/kl/lk/kwkllkjyf-vfpv77miu4jnnlrqm.png" alt="latencia"></p><br><p>  Puede notar que en peque침os rps RedisPipe consume m치s CPU que el "competidor", especialmente si se usa una peque침a pausa.  Esto se debe principalmente a la implementaci칩n de temporizadores en Go y la implementaci칩n de las llamadas del sistema utilizadas en el sistema operativo (en Linux es futexsleep), ya que en el modo "sin pausa" la diferencia es significativamente menor. </p><br><p>  A medida que aumenta el rps, la sobrecarga para los temporizadores se compensa con una sobrecarga menor para la conectividad de red, y despu칠s de 16 krps por cliente, el uso de RedisPipe con una pausa de 150 microsegundos comienza a brindar beneficios tangibles. </p><br><p>  Por supuesto, despu칠s de que Redis descansaba en la CPU, la latencia de las solicitudes que usan el conector cl치sico comienza a aumentar.  Sin embargo, no estoy seguro de que, en la pr치ctica, a menudo alcances 180 krps en una instancia de Redis.  Pero si es as칤, tenga en cuenta que puede tener problemas. </p><br><p>  Mientras Redis no se ejecute en la CPU, la latencia de la solicitud, por supuesto, sufre el uso de una pausa.  Este compromiso se establece intencionalmente en el conector.  Sin embargo, esta compensaci칩n solo se nota si Redis y el cliente est치n en el mismo host f칤sico.  Dependiendo de la topolog칤a de la red, un viaje de ida y vuelta a un host vecino puede ser de cien microsegundos a un milisegundo.  En consecuencia, la diferencia en el retraso ya en lugar de nueve veces (0.26 / 0.03) se convierte en tres veces (0.36 / 0.13) o se mide solo por un par de decenas de por ciento (1.26 / 1.03). </p><br><p>  En nuestra carga de trabajo, cuando Redis se usa como cach칠, la expectativa total de respuestas de la base de datos con una p칠rdida de cach칠 es mayor que la expectativa total de respuestas de Redis, porque se cree que el aumento en el retraso no es significativo. </p><br><p>  El principal resultado positivo es una tolerancia al crecimiento de la carga: si de repente la carga en el servicio aumenta N veces, Redis no consumir치 la CPU la misma N veces m치s.  Para soportar cuadruplicar la carga de 160 krps a 640 krps, Redis gast칩 solo 1.6 veces m치s CPU, aumentando el consumo del 29 al 46%.  Esto nos permite no tener miedo de que Redis se doblegue de repente.  La escalabilidad de la aplicaci칩n tampoco estar치 determinada por el funcionamiento del conector y el costo de la conectividad de la red (l칠ase: costos de CPU de SYS). </p><br><p>  Observaci칩n  El c칩digo de referencia opera con valores peque침os.  Para despejar mi conciencia, repet칤 la prueba con valores de tama침o 768 bytes.  El consumo de CPU por r치bano aument칩 notablemente (hasta un 66% en una pausa de 150 췃s), y el techo para un conector cl치sico cae a 170 krps.  Pero todas las proporciones observadas permanecieron iguales, y de ah칤 las conclusiones. </p><br><h2 id="klaster">  Racimo </h2><br><p>  Para escalar usamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Redis Cluster</a> .  Esto nos permite usar Redis no solo como cach칠, sino tambi칠n como almacenamiento vol치til y, al mismo tiempo, no perder datos al expandir / comprimir un cl칰ster. </p><br><p>  Redis Cluster utiliza el principio de cliente inteligente, es decir  el cliente debe monitorear el estado del cl칰ster y tambi칠n responder a los errores auxiliares que devuelve el "r치bano" cuando el "ramo" se mueve de una instancia a otra. </p><br><p>  En consecuencia, el cliente debe mantener conexiones a todas las instancias de Redis en el cl칰ster y emitir una conexi칩n a la requerida para cada solicitud.  Y fue en este lugar que el cliente us칩 antes (no se침alaremos con el dedo) que se fastidi칩 mucho.  El autor, que sobrestim칩 la comercializaci칩n de Go (CSP, canales, gorutinas), implement칩 la sincronizaci칩n del trabajo con el estado del cl칰ster enviando devoluciones de llamada a la gorutina central.  Esto se ha convertido en un botnek serio para nosotros.  Como parche temporal, tuvimos que lanzar cuatro clientes en un cl칰ster, cada uno, a su vez, elevando hasta cientos de conexiones en el grupo para cada instancia de Redis. </p><br><p>  En consecuencia, el nuevo conector se encarg칩 de evitar este error.  Toda interacci칩n con el estado del cl칰ster en la ruta de ejecuci칩n de la consulta se realiza de la manera m치s libre posible: </p><br><ul><li>  el estado del cl칰ster se hace pr치cticamente inmutable, y no hay numerosas mutaciones aromatizadas por 치tomos </li><li>  el acceso al estado se produce mediante atomic.StorePointer / atomic.LoadPointer y, por lo tanto, se puede obtener sin bloquear. </li></ul><br><p>  Por lo tanto, incluso durante la actualizaci칩n del estado del cl칰ster, las solicitudes pueden usar el estado anterior sin temor a esperar en un bloqueo. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// storeConfig atomically stores config func (c *Cluster) storeConfig(cfg *clusterConfig) { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) atomic.StorePointer(p, unsafe.Pointer(cfg)) } // getConfig loads config atomically func (c *Cluster) getConfig() *clusterConfig { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) return (*clusterConfig)(atomic.LoadPointer(p)) } func (cfg *clusterConfig) slot2shardno(slot uint16) uint16 { return uint16(atomic.LoadUint32(&amp;cfg.slots[slot])) } func (cfg *clusterConfig) slotSetShard(slot, shard uint16) { atomic.StoreUint32(&amp;cfg.slots[slot], shard) }</span></span></code> </pre> <br><p>  El estado del cl칰ster se actualiza cada 5 segundos.  Pero si hay una sospecha de inestabilidad del cl칰ster, la actualizaci칩n es forzada: </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">control</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { t := time.NewTicker(c.opts.CheckInterval) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> t.Stop() <span class="hljs-comment"><span class="hljs-comment">// main control loop for { select { case &lt;-c.ctx.Done(): // cluster closed, exit control loop c.report(LogContextClosed{Error: c.ctx.Err()}) return case cmd := &lt;-c.commands: // execute some asynchronous "cluster-wide" actions c.execCommand(cmd) continue case &lt;-forceReload: // forced mapping reload c.reloadMapping() case &lt;-tC: // regular mapping reload c.reloadMapping() } } } func (c *Cluster) ForceReloading() { select { case c.forceReload &lt;- struct{}{}: default: } }</span></span></code> </pre> <br><p>  Si la respuesta MOVED o ASK recibida del r치bano contiene una direcci칩n desconocida, se inicia su adici칩n asincr칩nica a la configuraci칩n.  (Lo siento, no descubr칤 c칩mo simplificar el c칩digo, porque <a href="">aqu칤 est치 el enlace</a> ). No fue sin el uso de bloqueos, pero se toman por un corto per칤odo de tiempo.  La expectativa principal se realiza al guardar la devoluci칩n de llamada en la matriz: la misma vista lateral futura. </p><br><p>  Se establecen conexiones con todas las instancias de Redis, con maestros y esclavos.  Dependiendo de la pol칤tica preferida y el tipo de solicitud (lectura o escritura), la solicitud puede enviarse tanto al maestro como al esclavo.  Esto tiene en cuenta la "vivacidad" de la instancia, que consiste tanto en la informaci칩n obtenida al actualizar el estado del cl칰ster como en el estado actual de la conexi칩n. </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connForSlot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(slot </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint16</span></span></span></span><span class="hljs-function"><span class="hljs-params">, policy ReplicaPolicyEnum)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*redisconn.Connection, *errorx.Error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conn *redisconn.Connection cfg := c.getConfig() shard := cfg.slot2shard(slot) nodes := cfg.nodes <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> addr <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> policy { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> MasterOnly: addr = shard.addr[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">// master is always first node := nodes[addr] if conn = node.getConn(c.opts.ConnHostPolicy, needConnected); conn == nil { conn = node.getConn(c.opts.ConnHostPolicy, mayBeConnected) } case MasterAndSlaves: n := uint32(len(shard.addr)) off := c.opts.RoundRobinSeed.Current() for _, needState := range []int{needConnected, mayBeConnected} { mask := atomic.LoadUint32(&amp;shard.good) // load health information for ; mask != 0; off++ { bit := 1 &lt;&lt; (off % n) if mask&amp;bit == 0 { // replica isn't healthy, or already viewed continue } mask &amp;^= bit addr = shard.addr[k] if conn = nodes[addr].getConn(c.opts.ConnHostPolicy, needState); conn != nil { return conn, nil } } } } if conn == nil { c.ForceReloading() return nil, c.err(ErrNoAliveConnection) } return conn, nil } func (n *node) getConn(policy ConnHostPolicyEnum, liveness int) *redisconn.Connection { for _, conn := range n.conns { switch liveness { case needConnected: if c.ConnectedNow() { return conn } case mayBeConnected: if c.MayBeConnected() { return conn } } } return nil }</span></span></code> </pre> <br><p>  Hay un cr칤ptico <code>RoundRobinSeed.Current()</code> .  Esto, por un lado, es una fuente de aleatoriedad, y por otro, aleatoriedad que no cambia con frecuencia.  Si selecciona una nueva conexi칩n para cada solicitud, esto degrada la eficiencia de la canalizaci칩n.  Es por eso que la implementaci칩n predeterminada cambia el valor de Current cada pocas decenas de milisegundos.  Para tener menos superposiciones en el tiempo, cada host selecciona su propio intervalo. </p><br><p>  Como recordar치, la conexi칩n utiliza el concepto de Futuro para solicitudes asincr칩nicas.  El cl칰ster utiliza el mismo concepto: un futuro personalizado se envuelve en uno agrupado, y ese se alimenta a la conexi칩n. </p><br><p>  쯇or qu칠 envolver el futuro personalizado?  En primer lugar, en el modo de cl칰ster, "r치bano" devuelve maravillosos "errores" MOVIDO y PREGUNTA con la informaci칩n a d칩nde ir para obtener la clave que necesita, y, despu칠s de recibir dicho error, debe repetir la solicitud a otro host.  En segundo lugar, dado que todav칤a necesitamos implementar la l칩gica de redireccionamiento, 쯣or qu칠 no incrustar la solicitud para volver a intentarlo con un error de E / S (por supuesto, solo si la solicitud de lectura): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c *Cluster req Request cb Future slot <span class="hljs-keyword"><span class="hljs-keyword">uint16</span></span> policy ReplicaPolicyEnum mayRetry <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SendWithPolicy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy ReplicaPolicyEnum, req Request, cb Future)</span></span></span></span> { slot := redisclusterutil.ReqSlot(req) policy = c.fixPolicy(slot, req, policy) conn, err := c.connForSlot(slot, policy, <span class="hljs-literal"><span class="hljs-literal">nil</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { cb.Resolve(err) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } r := &amp;request{ c: c, req: req, cb: cb, slot: slot, policy: policy, mayRetry: policy != MasterOnly || redis.ReplicaSafe(req.Cmd), } conn.Send(req, r, <span class="hljs-number"><span class="hljs-number">0</span></span>) } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r *request)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Resolve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(res </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">interface</span></span></span></span><span class="hljs-function"><span class="hljs-params">{}, _ </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { err := redis.AsErrorx(res) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { r.resolve(res) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> err.IsOfType(redis.ErrIO): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> !r.mayRetry { <span class="hljs-comment"><span class="hljs-comment">// It is not safe to retry read-write operation r.resolve(err) return } fallthrough case err.HasTrait(redis.ErrTraitNotSent): // It is request were not sent at all, it is safe to retry both readonly and write requests. conn, err := rcconnForSlot(r.slot, r.policy, r.seen) if err != nil { r.resolve(err) return } conn.Send(r.req, r) return case err.HasTrait(redis.ErrTraitClusterMove): addr := movedTo(err) ask := err.IsOfType(redis.ErrAsk) rcensureConnForAddress(addr, func(conn *redisconn.Connection, cerr error) { if cerr != nil { r.resolve(cerr) } else { r.lastconn = conn conn.SendAsk(r.req, r, ask) } }) return default: // All other errors: just resolve. r.resolve(err) } }</span></span></code> </pre> <br><p>  Este tambi칠n es un c칩digo simplificado.  Se omite la restricci칩n en el n칰mero de reintentos, la memorizaci칩n de conexiones problem치ticas, etc. </p><br><h2 id="komfort">  Comodidad </h2><br><p>  Solicitudes asincr칩nicas, 춰Future es una superkule!  Pero terriblemente inc칩modo. </p><br><p>  La interfaz es lo m치s importante.  Puedes vender cualquier cosa si tiene una buena interfaz.  Es por eso que Redis y MongoDB han ganado popularidad. </p><br><p>  Por lo tanto, es necesario convertir nuestras solicitudes asincr칩nicas en s칤ncronas. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Sync provides convenient synchronous interface over asynchronous Sender. type Sync struct { S Sender } // Do is convenient method to construct and send request. // Returns value that could be either result or error. func (s Sync) Do(cmd string, args ...interface{}) interface{} { return s.Send(Request{cmd, args}) } // Send sends request to redis. // Returns value that could be either result or error. func (s Sync) Send(r Request) interface{} { var res syncRes res.Add(1) sSSend(r, &amp;res) res.Wait() return res.r } type syncRes struct { r interface{} sync.WaitGroup } // Resolve implements Future.Resolve func (s *syncRes) Resolve(res interface{}) { sr = res s.Done() } // Usage func get(s redis.Sender, key string) (interface{}, error) { res := redis.Sync{s}.Do("GET", key) if err := redis.AsError(res); err != nil { return nil, err } return res, nil }</span></span></code> </pre> <br><p>  <code>AsError</code> no se ve como un Go-way nativo para obtener un error.  Pero me gusta porque  desde mi punto de vista, el resultado es <code>Result&lt;T,Error&gt;</code> y <code>AsError</code> es un patr칩n de coincidencia ersatz. </p><br><h2 id="nedostatki">  Desventajas </h2><br><p>  Pero, desafortunadamente, hay una mosca en la pomada en este bienestar. </p><br><p>  El protocolo Redis no implica reordenar solicitudes.  Y al mismo tiempo, tiene solicitudes de bloqueo como BLPOP, BRPOP. </p><br><p>  Esto es un fracaso </p><br><p>  Como sabe, si dicha solicitud est치 bloqueada, bloquear치 todas las solicitudes que la sigan.  Y no hay nada que hacer al respecto. </p><br><p>  Despu칠s de una larga discusi칩n, se decidi칩 prohibir el uso de estas solicitudes en RedisPipe. </p><br><p>  Por supuesto, si realmente lo necesita, puede: poner el par치metro <code>ScriptMode: true</code> , y eso est치 en su conciencia. </p><br><h2 id="alternativy">  Alternativas </h2><br><p>  De hecho, todav칤a hay una alternativa que no mencion칠, pero que los lectores expertos pensaron, es el rey de la twemproxy de cach칠s de cl칰ster. </p><br><p>  칄l hace por Redis lo que hace nuestro conector: transforma una "solicitud / respuesta" grosera y desalmada en una suave "colocaci칩n de tuber칤as". </p><br><p>  Pero la propia twemproxy sufrir치 el hecho de que tendr치 que trabajar en un sistema de "solicitud / respuesta".  Esta vez  Y en segundo lugar, usamos "r치bano" y "almacenamiento poco confiable" y, a veces, cambiamos el tama침o del cl칰ster.  Y twemproxy no facilita la tarea de reequilibrio de ninguna manera y, adem치s, requiere un reinicio al cambiar la configuraci칩n del cl칰ster. </p><br><h2 id="vliyanie">  Influencia </h2><br><p>  No tuve tiempo de escribir un art칤culo, y las olas de RedisPipe ya se han ido.  Se ha adoptado un parche en Radix.v3 que agrega canalizaci칩n a su Pool: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Consulte RedisPipe y descubra si se puede incorporar su estrategia de canalizaci칩n / procesamiento por lotes impl칤cito</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Canalizaci칩n autom치tica para comandos en Pool</a> </p><br><p>  Son ligeramente inferiores en velocidad (a juzgar por sus puntos de referencia, pero no lo dir칠 con certeza).  Pero su ventaja es que pueden enviar comandos de bloqueo a otras conexiones desde el grupo. </p><br><h2 id="zaklyuchenie">  Conclusi칩n </h2><br><p>  Ya hace un a침o que RedisPipe contribuye a la efectividad de nuestro servicio. <br>  Y en previsi칩n de cualquier "d칤a caluroso", uno de los recursos, cuya capacidad no causa preocupaci칩n, es la CPU en los servidores Redis. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repositorio</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Punto de referencia</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/438026/">https://habr.com/ru/post/438026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../438012/index.html">En Alemania, en 2018 las fuentes de energ칤a renovables lideradas por el viento proporcionaron m치s energ칤a que el carb칩n.</a></li>
<li><a href="../438016/index.html">Nasdaq y Citi invierten millones de d칩lares en startup para introducir blockchain en los mercados financieros</a></li>
<li><a href="../438018/index.html">CNC en un taller de hobby (parte 2)</a></li>
<li><a href="../438020/index.html">Caterpillar presenta una excavadora el칠ctrica de 26 toneladas con una bater칤a gigante de 300 kWh</a></li>
<li><a href="../438024/index.html">Pagos r치pidos: de qu칠 deben preocuparse los bancos</a></li>
<li><a href="../438028/index.html">No necesita blockchain: 8 casos de usuarios populares y por qu칠 no funcionan</a></li>
<li><a href="../438032/index.html">C칩digo abierto popular - segunda parte: 5 herramientas de administraci칩n en la nube</a></li>
<li><a href="../438034/index.html">Android, Rx y Kotlin, o c칩mo hacer que una garra de Lego se encoja. Parte 1</a></li>
<li><a href="../438036/index.html">3blue1brown y MIT en ruso</a></li>
<li><a href="../438038/index.html">Carrera de esteroides. Historias reales</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>