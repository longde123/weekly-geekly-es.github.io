<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏉 🍺 🔸 RedisPipe - Más diversión juntos 🤬 👏🏼 😸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cuando pienso en cómo funcionan los clientes ingenuos de RPC, recuerdo una broma: 
 El tribunal 
 "Demandado, ¿por qué mataste a una mujer?" 
 - Estoy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>RedisPipe - Más diversión juntos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/joom/blog/438026/"><p>  Cuando pienso en cómo funcionan los clientes ingenuos de RPC, recuerdo una broma: </p><br><blockquote>  El tribunal <br>  "Demandado, ¿por qué mataste a una mujer?" <br>  - Estoy en el autobús, el conductor se acerca a la mujer y le exige que compre un boleto.  La mujer abrió su bolso, sacó su bolso, cerró su bolso, abrió su bolso, cerró su bolso, abrió su bolso, puso su bolso allí, cerró su bolso, abrió su bolso, tomó su dinero, abrió su bolso, tomó su bolso, cerró su bolso, abrió su bolso y puso su bolso allí , cerró su bolso, abrió su bolso, puso su bolso allí. <br>  - Y que? <br>  - El controlador le dio un boleto.  Una mujer abrió su bolso, sacó su bolso, cerró su bolso, abrió su bolso, cerró su bolso, abrió su bolso, puso su bolso allí, cerró su bolso, abrió su bolso, colocó su boleto, cerró su bolso, abrió su bolso, sacó su bolso, cerró su bolso, abrió su bolso , puse el bolso allí, cerré el bolso, abrí el bolso, puse el bolso allí, cerré el bolso. <br>  "Toma el cambio", llegó la voz del controlador.  La mujer ... abrió su bolso ... <br>  "Sí, no es suficiente matarla", el fiscal no puede soportar. <br>  "Así que lo hice". <br>  © S. Altov </blockquote><p><img src="https://habrastorage.org/webt/qx/ne/-b/qxne-bocftxvmk99ouizmn3iqyo.jpeg"></p><a name="habracut"></a><br><p>  Aproximadamente lo mismo sucede en el proceso de solicitud-respuesta, si lo aborda frívolamente: </p><br><ul><li>  el proceso del usuario escribe una solicitud serializada en el socket, de hecho la copia en el buffer del socket en el nivel del sistema operativo; <br>  Esta es una operación bastante difícil, porque  es necesario hacer un cambio de contexto (incluso si puede ser "fácil"); </li><li>  cuando al sistema operativo le parece que se puede escribir algo en la red, se forma un paquete (la solicitud se copia nuevamente) y se envía a la tarjeta de red; </li><li>  la tarjeta de red escribe el paquete en la red (posiblemente después del almacenamiento en búfer); </li><li>  (en el camino, un paquete puede almacenarse varias veces en los enrutadores); </li><li>  finalmente, el paquete llega al host de destino y se almacena en la tarjeta de red; </li><li>  la tarjeta de red envía una notificación al sistema operativo, y cuando el sistema operativo encuentra la hora, copia el paquete en su búfer y establece el indicador de listo en el descriptor de archivo; </li><li>  (aún debe recordar enviar el ACK en respuesta); </li><li>  después de un tiempo, la aplicación del servidor se da cuenta de que el descriptor está listo (usando epoll) y algún día copia la solicitud en el búfer de la aplicación; </li><li>  y finalmente, la aplicación del servidor procesa la solicitud. </li></ul><br><p>  Como saben, la respuesta se transmite exactamente de la misma manera solo en la dirección opuesta.  Por lo tanto, cada solicitud pasa un tiempo notable en el sistema operativo para su mantenimiento, y cada respuesta pasa el mismo tiempo nuevamente. </p><br><p>  Esto se hizo especialmente notable después de Meltdown / Spectre, ya que los parches lanzados condujeron a un aumento significativo en el costo de las llamadas al sistema.  A principios de enero de 2018, nuestro clúster Redis de repente comenzó a consumir una CPU y media o dos veces más, porque  Amazon ha aplicado parches de kernel apropiados para cubrir estas vulnerabilidades.  (Es cierto que Amazon aplicó una nueva versión del parche un poco más tarde, y el consumo de CPU cayó casi a los niveles anteriores. Pero el conector ya ha comenzado a nacer). </p><br><p> Desafortunadamente, todos los conectores Go ampliamente conocidos para Redis y Memcached funcionan de la siguiente manera: el conector crea un grupo de conexiones, y cuando necesita enviar una solicitud, extrae una conexión del grupo, escribe una solicitud y luego espera una respuesta.  (Es especialmente triste que el conector a Memcached haya sido escrito por el propio Brad Fitzpatrick). Y algunos conectores tienen una implementación tan infructuosa de este grupo que el proceso de eliminar la conexión del grupo se convierte en una botnet en sí mismo. </p><br><p>  Hay dos formas de facilitar este arduo trabajo de reenvío de solicitud / respuesta: </p><br><ol><li>  Utilice el acceso directo a la tarjeta de red: DPDK, netmap, PF_RING, etc. </li><li>  No envíe cada solicitud / respuesta como un paquete separado, sino combínelas, si es posible, en paquetes más grandes, es decir, extienda la sobrecarga de trabajar con la red para varias solicitudes.  Juntos más divertido! </li></ol><br><p>  La primera opción, por supuesto, es posible.  Pero, en primer lugar, esto es para los valientes, ya que debe escribir la implementación de TCP / IP usted mismo (por ejemplo, como en ScyllaDB).  Y en segundo lugar, de esta manera facilitamos la situación solo en un lado, en el que nosotros mismos escribimos.  No quiero volver a escribir Redis (todavía), por lo que los servidores consumirán la misma cantidad, incluso si el cliente usa el genial DPDK. </p><br><p>  La segunda opción es mucho más simple y, lo más importante, facilita la situación inmediatamente en el cliente y en el servidor.  Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una base de datos en memoria se</a> jacta de que puede servir a millones de RPS, mientras que Redis no puede atender a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">par de cientos de miles</a> .  Sin embargo, este éxito no es tanto la implementación de esa base en memoria como la decisión una vez aceptada de que el protocolo será completamente asíncrono, y los clientes deben usar esta asincronía siempre que sea posible.  Lo que muchos clientes (especialmente los utilizados en los puntos de referencia) implementan con éxito enviando solicitudes a través de una conexión TCP y, si es posible, enviándolas juntas a la red. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Un artículo bien conocido</a> muestra que Redis también puede entregar un millón de respuestas por segundo si se utiliza la canalización.  La experiencia personal en el desarrollo de historias en memoria también indica que la canalización reduce significativamente el consumo de CPU SYS y le permite usar el procesador y la red de manera mucho más eficiente. </p><br><p>  La única pregunta es cómo usar la canalización, si en la solicitud las solicitudes en Redis a menudo llegan de una en una.  Y si falta un servidor y Redis Cluster se usa con una gran cantidad de fragmentos, incluso cuando se encuentra un paquete de solicitudes, se divide en solicitudes individuales para cada fragmento. </p><br><p>  La respuesta, por supuesto, es "obvia": realice un trazado de tubería implícito, recopile las solicitudes de todas las gorutinas que se ejecutan en paralelo a un servidor Redis y las envíe a través de una conexión. </p><br><p>  Por cierto, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tendido</a> implícito de tuberías no es tan raro en los conectores en otros idiomas: nodejs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">node_redis</a> , C # <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RedisBoost</a> , python <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aioredis</a> y muchos otros.  Muchos de estos conectores están escritos en la parte superior de los bucles de eventos y, por lo tanto, parece natural recopilar solicitudes de "flujos de cálculo" paralelos.  En Go, se promueve el uso de interfaces síncronas y, aparentemente, porque pocas personas deciden organizar su propio "bucle". </p><br><p>  Queríamos usar Redis de la manera más eficiente posible y, por lo tanto, decidimos escribir un nuevo conector "mejor" (tm): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RedisPipe</a> . </p><br><h2 id="kak-sdelat-neyavnyy-payplayning">  ¿Cómo hacer un tendido de tubería implícito? </h2><br><p>  El esquema básico: </p><br><ul><li>  Las rutinas de la lógica de la aplicación no escriben solicitudes directamente en la red, sino que las pasan al recolector de rutinas; </li><li>  si es posible, el recolector recopila un montón de solicitudes, las escribe en la red y las pasa al lector de rutina; </li><li>  Goroutine-reader lee las respuestas de la red, las compara con las solicitudes correspondientes y notifica a los goroutines de la lógica acerca de la respuesta recibida. </li></ul><br><p> Algo debe ser notificado sobre la respuesta.  Un astuto programador de Go sin duda dirá: "¡A través del canal!" <br>  Pero esta no es la única primitiva de sincronización posible y no la más eficiente incluso en el entorno Go.  Y dado que diferentes personas tienen diferentes necesidades, haremos que el mecanismo sea extensible, permitiendo al usuario implementar la interfaz (llamémosla <code>Future</code> ): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Future <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> { Resolve(val <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span>{}) }</code> </pre> <br><p>  Y luego el esquema básico se verá así: </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> future <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { req Request fut Future } <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Conn <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c net.Conn futmtx sync.Mutex wfutures []future futtimer *time.Timer rfutures <span class="hljs-keyword"><span class="hljs-keyword">chan</span></span> []future } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r Request, f Future)</span></span></span></span> { c.futmtx.Lock() <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> c.futmtx.Unlock() c.wfutures = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(c.wfutures, future{req: r, fut: f}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(c.wfutures) == <span class="hljs-number"><span class="hljs-number">1</span></span> { futtimer.Reset(<span class="hljs-number"><span class="hljs-number">100</span></span>*time.Microsecond) } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">writer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> c.futtimer.C { c.futmtx.Lock() futures, c.wfutures = c.wfutures, <span class="hljs-literal"><span class="hljs-literal">nil</span></span> c.futmtx.Unlock() <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> b []<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _, ft := <span class="hljs-keyword"><span class="hljs-keyword">range</span></span> futures { b = AppendRequest(b, ft.req) } _, _err := ccWrite(b) c.rfutures &lt;- futures } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Conn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reader</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { rd := bufio.NewReader(cc) <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> futures []future <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> { response, _err := ParseResponse(rd) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">len</span></span>(futures) == <span class="hljs-number"><span class="hljs-number">0</span></span> { futures = &lt;- c.rfutures } futures[<span class="hljs-number"><span class="hljs-number">0</span></span>].fut.Resolve(response) futures = futures[<span class="hljs-number"><span class="hljs-number">1</span></span>:] } }</code> </pre> <br><p>  Por supuesto, este es un código muy simplificado.  Omitido </p><br><ul><li>  establecimiento de conexión; </li><li>  Tiempos de espera de E / S </li><li>  manejo de errores de lectura / escritura; </li><li>  restablecer la conexión; </li><li>  la capacidad de cancelar la solicitud antes de enviarla a la red; </li><li>  optimizando la asignación de memoria (reutilizando el buffer y las matrices de futuros). </li></ul><br><p>  Cualquier error de E / S (incluido un tiempo de espera) en el código real conduce a una resolución de todos los errores futuros correspondientes a las solicitudes enviadas y en espera de ser enviadas. <br>  La capa de conexión no está involucrada en el reintento de la solicitud, y si es necesario (y posible) volver a intentar la solicitud, se puede hacer en un nivel superior de abstracción (por ejemplo, en la implementación del soporte de Redis Cluster que se describe a continuación). </p><br><p>  Observación  Inicialmente, el circuito parecía un poco más complicado.  Pero en el proceso de experimentos simplificado a tal opción. </p><br><p>  Observación 2. Se imponen requisitos muy estrictos al futuro. Método de resolución: debe ser lo más rápido posible, prácticamente sin bloqueo y en ningún caso de pánico.  Esto se debe al hecho de que se llama sincrónicamente en el ciclo del lector, y cualquier "freno" inevitablemente conducirá a la degradación.  La implementación de Future.Resolve debe hacer el mínimo necesario de acciones lineales: despertar al expectante;  tal vez maneje el error y envíe un reintento asincrónico (utilizado en la implementación del soporte de clúster). </p><br><h2 id="effekt">  Efecto </h2><br><p>  ¡Un buen punto de referencia es la mitad del artículo! </p><br><p>  Un buen punto de referencia es uno que esté lo más cerca posible de combatir el uso en términos de efectos observados.  Y esto no es fácil de hacer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La opción de referencia</a> , que me parece bastante similar a la real: </p><br><ul><li>  el "script" principal emula 5 clientes paralelos, </li><li>  en cada "cliente", por cada 300-1000 rps "deseadas", se activa la gorutina (se activan 3 gorutinas por 1000 rps, se ejecutan 124 gorutinas por 128000 rps), </li><li>  gorutin usa una instancia separada del limitador de velocidad y envía solicitudes en series aleatorias, de 5 a 15 solicitudes. </li></ul><br><p>  La aleatoriedad de la serie de consultas nos permite lograr una distribución aleatoria de la serie en la línea de tiempo, que refleja más correctamente la carga real. </p><br><div class="spoiler">  <b class="spoiler_title">Texto oculto</b> <div class="spoiler_text"><p>  Las opciones incorrectas fueron: <br>  a) utilice un limitador de velocidad para todas las gorutinas del "cliente" y recurra a él para cada solicitud; esto lleva a un consumo excesivo de CPU por el propio limitador de velocidad, así como a una mayor rotación de tiempo de goroutin, lo que degrada el rendimiento de RedisPipe a rps medias (pero inexplicablemente mejora a la altura); <br>  b) utilice un limitador de velocidad para todas las gorutinas del "cliente" y envíe solicitudes en serie: el limitador de velocidad no está consumiendo tanto la CPU, pero la alternancia de gorutinas en el tiempo solo aumenta; <br>  c) use un limitador de velocidad para cada goroutine, pero envíe la misma serie de 10 solicitudes; en este escenario, las goroutines se despiertan demasiado simultáneamente, lo que mejora injustamente los resultados de RedisPipe. </p></div></div><br><p>  Las pruebas se realizaron en una instancia de AWS c5-2xlarge de cuatro núcleos.  La versión de Redis es 5.0. </p><br><p>  La relación de la intensidad de consulta deseada, la intensidad total resultante y consumida por el rábano de la CPU: </p><br><table><thead><tr><th>  rps previstos </th><th>  redigo <br>  rps /% cpu </th><th>  redispipe no espere <br>  rps /% cpu </th><th>  redispipe 50µs <br>  rps /% cpu </th><th>  redispipe 150µs <br>  rps /% cpu </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  5015/7% </td><td>  5015/6% </td><td>  5015/6% </td><td>  5015/6% </td></tr><tr><td>  2000 * 5 </td><td>  10022/11% </td><td>  10022/10% </td><td>  10022/10% </td><td>  10022/10% </td></tr><tr><td>  4000 * 5 </td><td>  20036/21% </td><td>  20036/18% </td><td>  20035/17% </td><td>  20035/15% </td></tr><tr><td>  8000 * 5 </td><td>  40020/45% </td><td>  40062/37% </td><td>  40060/26% </td><td>  40056/19% </td></tr><tr><td>  16000 * 5 </td><td>  79994/71% </td><td>  80102/58% </td><td>  80096/33% </td><td>  80090/23% </td></tr><tr><td>  32000 * 5 </td><td>  159590/96% </td><td>  160 180/80% </td><td>  160,167 / 39% </td><td>  160150/29% </td></tr><tr><td>  64000 * 5 </td><td>  187774/99% </td><td>  320 313/98% </td><td>  320,283 / 47% </td><td>  320,258 / 37% </td></tr><tr><td>  92000 * 5 </td><td>  183206/99% </td><td>  480,443 / 97% </td><td>  480,407 / 52% </td><td>  480 366/42% </td></tr><tr><td>  128000 * 5 </td><td>  179744/99% </td><td>  640,484 / 97% </td><td>  640,488 / 55% </td><td>  640,428 / 46% </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/g-/d7/7k/g-d77kznsmwmgejiyplpipwv7t4.png" alt="Solicitar tarifa"><img src="https://habrastorage.org/webt/cs/xf/lz/csxflztfjbnmzc0kuhpnqddeokq.png" alt="CPU Redis"></p><br><p>  Puede notar que con un conector que funciona de acuerdo con el esquema clásico (solicitud / respuesta + grupo de conexiones), Redis consume rápidamente el núcleo del procesador, después de lo cual se convierte en una tarea imposible obtener más de 190 krps. </p><br><p>  RedisPipe le permite exprimir toda la potencia requerida de Redis.  Y cuanto más nos detenemos para recopilar solicitudes paralelas, menos Redis consume CPU.  Ya se obtiene un beneficio tangible a 4krps del cliente (20krps en total) si se usa una pausa de 150 microsegundos. </p><br><p>  Incluso si la pausa no se usa explícitamente cuando Redis descansa en la CPU, el retraso aparece por sí solo.  Además, las solicitudes comienzan a ser almacenadas por el sistema operativo.  Esto permite que RedisPipe aumente el número de solicitudes ejecutadas con éxito cuando el conector clásico ya está bajando sus patas. </p><br><p>  Este es el resultado principal, para lo cual fue necesario crear un nuevo conector. </p><br><p>  ¿Qué sucede entonces con el consumo de CPU en el cliente y con las solicitudes retrasadas? </p><br><table><thead><tr><th>  rps previstos </th><th>  redigo <br>  % cpu / ms </th><th>  redispipe nowait <br>  % cpu / ms </th><th>  redispipe 50ms <br>  % cpu / ms </th><th>  redispipe 150ms <br>  % cpu / ms </th></tr></thead><tbody><tr><td>  1000 * 5 </td><td>  13 / 0.03 </td><td>  20 / 0.04 </td><td>  46 / 0.16 </td><td>  44 / 0.26 </td></tr><tr><td>  2000 * 5 </td><td>  25 / 0.03 </td><td>  33 / 0.04 </td><td>  77 / 0.16 </td><td>  71 / 0.26 </td></tr><tr><td>  4000 * 5 </td><td>  48 / 0.03 </td><td>  60 / 0.04 </td><td>  124 / 0.16 </td><td>  107 / 0.26 </td></tr><tr><td>  8000 * 5 </td><td>  94 / 0.03 </td><td>  119 / 0.04 </td><td>  178 / 0.15 </td><td>  141 / 0.26 </td></tr><tr><td>  16000 * 5 </td><td>  184 / 0.04 </td><td>  206 / 0.04 </td><td>  228 / 0.15 </td><td>  177 / 0.25 </td></tr><tr><td>  32000 * 5 </td><td>  341 / 0.08 </td><td>  322 / 0.05 </td><td>  280 / 0.15 </td><td>  226 / 0.26 </td></tr><tr><td>  64000 * 5 </td><td>  316 / 1.88 </td><td>  469 / 0.08 </td><td>  345 / 0.16 </td><td>  307 / 0.26 </td></tr><tr><td>  92000 * 5 </td><td>  313 / 2,88 </td><td>  511 / 0.16 </td><td>  398 / 0.17 </td><td>  366 / 0.27 </td></tr><tr><td>  128000 * 5 </td><td>  312 / 3.54 </td><td>  509 / 0.37 </td><td>  441 / 0,19 </td><td>  418 / 0.29 </td></tr></tbody></table><br><p><img src="https://habrastorage.org/webt/fp/ws/ra/fpwsra-pgtexl6ua7crpc4vt5fa.png" alt="CPU del cliente"><img src="https://habrastorage.org/webt/kw/kl/lk/kwkllkjyf-vfpv77miu4jnnlrqm.png" alt="latencia"></p><br><p>  Puede notar que en pequeños rps RedisPipe consume más CPU que el "competidor", especialmente si se usa una pequeña pausa.  Esto se debe principalmente a la implementación de temporizadores en Go y la implementación de las llamadas del sistema utilizadas en el sistema operativo (en Linux es futexsleep), ya que en el modo "sin pausa" la diferencia es significativamente menor. </p><br><p>  A medida que aumenta el rps, la sobrecarga para los temporizadores se compensa con una sobrecarga menor para la conectividad de red, y después de 16 krps por cliente, el uso de RedisPipe con una pausa de 150 microsegundos comienza a brindar beneficios tangibles. </p><br><p>  Por supuesto, después de que Redis descansaba en la CPU, la latencia de las solicitudes que usan el conector clásico comienza a aumentar.  Sin embargo, no estoy seguro de que, en la práctica, a menudo alcances 180 krps en una instancia de Redis.  Pero si es así, tenga en cuenta que puede tener problemas. </p><br><p>  Mientras Redis no se ejecute en la CPU, la latencia de la solicitud, por supuesto, sufre el uso de una pausa.  Este compromiso se establece intencionalmente en el conector.  Sin embargo, esta compensación solo se nota si Redis y el cliente están en el mismo host físico.  Dependiendo de la topología de la red, un viaje de ida y vuelta a un host vecino puede ser de cien microsegundos a un milisegundo.  En consecuencia, la diferencia en el retraso ya en lugar de nueve veces (0.26 / 0.03) se convierte en tres veces (0.36 / 0.13) o se mide solo por un par de decenas de por ciento (1.26 / 1.03). </p><br><p>  En nuestra carga de trabajo, cuando Redis se usa como caché, la expectativa total de respuestas de la base de datos con una pérdida de caché es mayor que la expectativa total de respuestas de Redis, porque se cree que el aumento en el retraso no es significativo. </p><br><p>  El principal resultado positivo es una tolerancia al crecimiento de la carga: si de repente la carga en el servicio aumenta N veces, Redis no consumirá la CPU la misma N veces más.  Para soportar cuadruplicar la carga de 160 krps a 640 krps, Redis gastó solo 1.6 veces más CPU, aumentando el consumo del 29 al 46%.  Esto nos permite no tener miedo de que Redis se doblegue de repente.  La escalabilidad de la aplicación tampoco estará determinada por el funcionamiento del conector y el costo de la conectividad de la red (léase: costos de CPU de SYS). </p><br><p>  Observación  El código de referencia opera con valores pequeños.  Para despejar mi conciencia, repetí la prueba con valores de tamaño 768 bytes.  El consumo de CPU por rábano aumentó notablemente (hasta un 66% en una pausa de 150 µs), y el techo para un conector clásico cae a 170 krps.  Pero todas las proporciones observadas permanecieron iguales, y de ahí las conclusiones. </p><br><h2 id="klaster">  Racimo </h2><br><p>  Para escalar usamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Redis Cluster</a> .  Esto nos permite usar Redis no solo como caché, sino también como almacenamiento volátil y, al mismo tiempo, no perder datos al expandir / comprimir un clúster. </p><br><p>  Redis Cluster utiliza el principio de cliente inteligente, es decir  el cliente debe monitorear el estado del clúster y también responder a los errores auxiliares que devuelve el "rábano" cuando el "ramo" se mueve de una instancia a otra. </p><br><p>  En consecuencia, el cliente debe mantener conexiones a todas las instancias de Redis en el clúster y emitir una conexión a la requerida para cada solicitud.  Y fue en este lugar que el cliente usó antes (no señalaremos con el dedo) que se fastidió mucho.  El autor, que sobrestimó la comercialización de Go (CSP, canales, gorutinas), implementó la sincronización del trabajo con el estado del clúster enviando devoluciones de llamada a la gorutina central.  Esto se ha convertido en un botnek serio para nosotros.  Como parche temporal, tuvimos que lanzar cuatro clientes en un clúster, cada uno, a su vez, elevando hasta cientos de conexiones en el grupo para cada instancia de Redis. </p><br><p>  En consecuencia, el nuevo conector se encargó de evitar este error.  Toda interacción con el estado del clúster en la ruta de ejecución de la consulta se realiza de la manera más libre posible: </p><br><ul><li>  el estado del clúster se hace prácticamente inmutable, y no hay numerosas mutaciones aromatizadas por átomos </li><li>  el acceso al estado se produce mediante atomic.StorePointer / atomic.LoadPointer y, por lo tanto, se puede obtener sin bloquear. </li></ul><br><p>  Por lo tanto, incluso durante la actualización del estado del clúster, las solicitudes pueden usar el estado anterior sin temor a esperar en un bloqueo. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// storeConfig atomically stores config func (c *Cluster) storeConfig(cfg *clusterConfig) { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) atomic.StorePointer(p, unsafe.Pointer(cfg)) } // getConfig loads config atomically func (c *Cluster) getConfig() *clusterConfig { p := (*unsafe.Pointer)(unsafe.Pointer(&amp;c.config)) return (*clusterConfig)(atomic.LoadPointer(p)) } func (cfg *clusterConfig) slot2shardno(slot uint16) uint16 { return uint16(atomic.LoadUint32(&amp;cfg.slots[slot])) } func (cfg *clusterConfig) slotSetShard(slot, shard uint16) { atomic.StoreUint32(&amp;cfg.slots[slot], shard) }</span></span></code> </pre> <br><p>  El estado del clúster se actualiza cada 5 segundos.  Pero si hay una sospecha de inestabilidad del clúster, la actualización es forzada: </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">control</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> { t := time.NewTicker(c.opts.CheckInterval) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> t.Stop() <span class="hljs-comment"><span class="hljs-comment">// main control loop for { select { case &lt;-c.ctx.Done(): // cluster closed, exit control loop c.report(LogContextClosed{Error: c.ctx.Err()}) return case cmd := &lt;-c.commands: // execute some asynchronous "cluster-wide" actions c.execCommand(cmd) continue case &lt;-forceReload: // forced mapping reload c.reloadMapping() case &lt;-tC: // regular mapping reload c.reloadMapping() } } } func (c *Cluster) ForceReloading() { select { case c.forceReload &lt;- struct{}{}: default: } }</span></span></code> </pre> <br><p>  Si la respuesta MOVED o ASK recibida del rábano contiene una dirección desconocida, se inicia su adición asincrónica a la configuración.  (Lo siento, no descubrí cómo simplificar el código, porque <a href="">aquí está el enlace</a> ). No fue sin el uso de bloqueos, pero se toman por un corto período de tiempo.  La expectativa principal se realiza al guardar la devolución de llamada en la matriz: la misma vista lateral futura. </p><br><p>  Se establecen conexiones con todas las instancias de Redis, con maestros y esclavos.  Dependiendo de la política preferida y el tipo de solicitud (lectura o escritura), la solicitud puede enviarse tanto al maestro como al esclavo.  Esto tiene en cuenta la "vivacidad" de la instancia, que consiste tanto en la información obtenida al actualizar el estado del clúster como en el estado actual de la conexión. </p><br><pre> <code class="go hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connForSlot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(slot </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint16</span></span></span></span><span class="hljs-function"><span class="hljs-params">, policy ReplicaPolicyEnum)</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*redisconn.Connection, *errorx.Error)</span></span></span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conn *redisconn.Connection cfg := c.getConfig() shard := cfg.slot2shard(slot) nodes := cfg.nodes <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> addr <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> policy { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> MasterOnly: addr = shard.addr[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">// master is always first node := nodes[addr] if conn = node.getConn(c.opts.ConnHostPolicy, needConnected); conn == nil { conn = node.getConn(c.opts.ConnHostPolicy, mayBeConnected) } case MasterAndSlaves: n := uint32(len(shard.addr)) off := c.opts.RoundRobinSeed.Current() for _, needState := range []int{needConnected, mayBeConnected} { mask := atomic.LoadUint32(&amp;shard.good) // load health information for ; mask != 0; off++ { bit := 1 &lt;&lt; (off % n) if mask&amp;bit == 0 { // replica isn't healthy, or already viewed continue } mask &amp;^= bit addr = shard.addr[k] if conn = nodes[addr].getConn(c.opts.ConnHostPolicy, needState); conn != nil { return conn, nil } } } } if conn == nil { c.ForceReloading() return nil, c.err(ErrNoAliveConnection) } return conn, nil } func (n *node) getConn(policy ConnHostPolicyEnum, liveness int) *redisconn.Connection { for _, conn := range n.conns { switch liveness { case needConnected: if c.ConnectedNow() { return conn } case mayBeConnected: if c.MayBeConnected() { return conn } } } return nil }</span></span></code> </pre> <br><p>  Hay un críptico <code>RoundRobinSeed.Current()</code> .  Esto, por un lado, es una fuente de aleatoriedad, y por otro, aleatoriedad que no cambia con frecuencia.  Si selecciona una nueva conexión para cada solicitud, esto degrada la eficiencia de la canalización.  Es por eso que la implementación predeterminada cambia el valor de Current cada pocas decenas de milisegundos.  Para tener menos superposiciones en el tiempo, cada host selecciona su propio intervalo. </p><br><p>  Como recordará, la conexión utiliza el concepto de Futuro para solicitudes asincrónicas.  El clúster utiliza el mismo concepto: un futuro personalizado se envuelve en uno agrupado, y ese se alimenta a la conexión. </p><br><p>  ¿Por qué envolver el futuro personalizado?  En primer lugar, en el modo de clúster, "rábano" devuelve maravillosos "errores" MOVIDO y PREGUNTA con la información a dónde ir para obtener la clave que necesita, y, después de recibir dicho error, debe repetir la solicitud a otro host.  En segundo lugar, dado que todavía necesitamos implementar la lógica de redireccionamiento, ¿por qué no incrustar la solicitud para volver a intentarlo con un error de E / S (por supuesto, solo si la solicitud de lectura): </p><br><pre> <code class="go hljs"><span class="hljs-keyword"><span class="hljs-keyword">type</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> { c *Cluster req Request cb Future slot <span class="hljs-keyword"><span class="hljs-keyword">uint16</span></span> policy ReplicaPolicyEnum mayRetry <span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(c *Cluster)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SendWithPolicy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy ReplicaPolicyEnum, req Request, cb Future)</span></span></span></span> { slot := redisclusterutil.ReqSlot(req) policy = c.fixPolicy(slot, req, policy) conn, err := c.connForSlot(slot, policy, <span class="hljs-literal"><span class="hljs-literal">nil</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { cb.Resolve(err) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } r := &amp;request{ c: c, req: req, cb: cb, slot: slot, policy: policy, mayRetry: policy != MasterOnly || redis.ReplicaSafe(req.Cmd), } conn.Send(req, r, <span class="hljs-number"><span class="hljs-number">0</span></span>) } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r *request)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Resolve</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(res </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">interface</span></span></span></span><span class="hljs-function"><span class="hljs-params">{}, _ </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">uint64</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { err := redis.AsErrorx(res) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { r.resolve(res) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> err.IsOfType(redis.ErrIO): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> !r.mayRetry { <span class="hljs-comment"><span class="hljs-comment">// It is not safe to retry read-write operation r.resolve(err) return } fallthrough case err.HasTrait(redis.ErrTraitNotSent): // It is request were not sent at all, it is safe to retry both readonly and write requests. conn, err := rcconnForSlot(r.slot, r.policy, r.seen) if err != nil { r.resolve(err) return } conn.Send(r.req, r) return case err.HasTrait(redis.ErrTraitClusterMove): addr := movedTo(err) ask := err.IsOfType(redis.ErrAsk) rcensureConnForAddress(addr, func(conn *redisconn.Connection, cerr error) { if cerr != nil { r.resolve(cerr) } else { r.lastconn = conn conn.SendAsk(r.req, r, ask) } }) return default: // All other errors: just resolve. r.resolve(err) } }</span></span></code> </pre> <br><p>  Este también es un código simplificado.  Se omite la restricción en el número de reintentos, la memorización de conexiones problemáticas, etc. </p><br><h2 id="komfort">  Comodidad </h2><br><p>  Solicitudes asincrónicas, ¡Future es una superkule!  Pero terriblemente incómodo. </p><br><p>  La interfaz es lo más importante.  Puedes vender cualquier cosa si tiene una buena interfaz.  Es por eso que Redis y MongoDB han ganado popularidad. </p><br><p>  Por lo tanto, es necesario convertir nuestras solicitudes asincrónicas en síncronas. </p><br><pre> <code class="go hljs"><span class="hljs-comment"><span class="hljs-comment">// Sync provides convenient synchronous interface over asynchronous Sender. type Sync struct { S Sender } // Do is convenient method to construct and send request. // Returns value that could be either result or error. func (s Sync) Do(cmd string, args ...interface{}) interface{} { return s.Send(Request{cmd, args}) } // Send sends request to redis. // Returns value that could be either result or error. func (s Sync) Send(r Request) interface{} { var res syncRes res.Add(1) sSSend(r, &amp;res) res.Wait() return res.r } type syncRes struct { r interface{} sync.WaitGroup } // Resolve implements Future.Resolve func (s *syncRes) Resolve(res interface{}) { sr = res s.Done() } // Usage func get(s redis.Sender, key string) (interface{}, error) { res := redis.Sync{s}.Do("GET", key) if err := redis.AsError(res); err != nil { return nil, err } return res, nil }</span></span></code> </pre> <br><p>  <code>AsError</code> no se ve como un Go-way nativo para obtener un error.  Pero me gusta porque  desde mi punto de vista, el resultado es <code>Result&lt;T,Error&gt;</code> y <code>AsError</code> es un patrón de coincidencia ersatz. </p><br><h2 id="nedostatki">  Desventajas </h2><br><p>  Pero, desafortunadamente, hay una mosca en la pomada en este bienestar. </p><br><p>  El protocolo Redis no implica reordenar solicitudes.  Y al mismo tiempo, tiene solicitudes de bloqueo como BLPOP, BRPOP. </p><br><p>  Esto es un fracaso </p><br><p>  Como sabe, si dicha solicitud está bloqueada, bloqueará todas las solicitudes que la sigan.  Y no hay nada que hacer al respecto. </p><br><p>  Después de una larga discusión, se decidió prohibir el uso de estas solicitudes en RedisPipe. </p><br><p>  Por supuesto, si realmente lo necesita, puede: poner el parámetro <code>ScriptMode: true</code> , y eso está en su conciencia. </p><br><h2 id="alternativy">  Alternativas </h2><br><p>  De hecho, todavía hay una alternativa que no mencioné, pero que los lectores expertos pensaron, es el rey de la twemproxy de cachés de clúster. </p><br><p>  Él hace por Redis lo que hace nuestro conector: transforma una "solicitud / respuesta" grosera y desalmada en una suave "colocación de tuberías". </p><br><p>  Pero la propia twemproxy sufrirá el hecho de que tendrá que trabajar en un sistema de "solicitud / respuesta".  Esta vez  Y en segundo lugar, usamos "rábano" y "almacenamiento poco confiable" y, a veces, cambiamos el tamaño del clúster.  Y twemproxy no facilita la tarea de reequilibrio de ninguna manera y, además, requiere un reinicio al cambiar la configuración del clúster. </p><br><h2 id="vliyanie">  Influencia </h2><br><p>  No tuve tiempo de escribir un artículo, y las olas de RedisPipe ya se han ido.  Se ha adoptado un parche en Radix.v3 que agrega canalización a su Pool: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Consulte RedisPipe y descubra si se puede incorporar su estrategia de canalización / procesamiento por lotes implícito</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Canalización automática para comandos en Pool</a> </p><br><p>  Son ligeramente inferiores en velocidad (a juzgar por sus puntos de referencia, pero no lo diré con certeza).  Pero su ventaja es que pueden enviar comandos de bloqueo a otras conexiones desde el grupo. </p><br><h2 id="zaklyuchenie">  Conclusión </h2><br><p>  Ya hace un año que RedisPipe contribuye a la efectividad de nuestro servicio. <br>  Y en previsión de cualquier "día caluroso", uno de los recursos, cuya capacidad no causa preocupación, es la CPU en los servidores Redis. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repositorio</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Punto de referencia</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/438026/">https://habr.com/ru/post/438026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../438012/index.html">En Alemania, en 2018 las fuentes de energía renovables lideradas por el viento proporcionaron más energía que el carbón.</a></li>
<li><a href="../438016/index.html">Nasdaq y Citi invierten millones de dólares en startup para introducir blockchain en los mercados financieros</a></li>
<li><a href="../438018/index.html">CNC en un taller de hobby (parte 2)</a></li>
<li><a href="../438020/index.html">Caterpillar presenta una excavadora eléctrica de 26 toneladas con una batería gigante de 300 kWh</a></li>
<li><a href="../438024/index.html">Pagos rápidos: de qué deben preocuparse los bancos</a></li>
<li><a href="../438028/index.html">No necesita blockchain: 8 casos de usuarios populares y por qué no funcionan</a></li>
<li><a href="../438032/index.html">Código abierto popular - segunda parte: 5 herramientas de administración en la nube</a></li>
<li><a href="../438034/index.html">Android, Rx y Kotlin, o cómo hacer que una garra de Lego se encoja. Parte 1</a></li>
<li><a href="../438036/index.html">3blue1brown y MIT en ruso</a></li>
<li><a href="../438038/index.html">Carrera de esteroides. Historias reales</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>