<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚óºÔ∏è üçÜ ‚úäüèª Projekte in verschiedenen Rechenzentren kombinieren üß¢ üï¥üèº üÜò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Artikel werden wir untersuchen, warum der traditionelle Ansatz zur Kombination lokaler Netzwerke auf L2-Ebene bei einer signifikanten Erh√∂hu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Projekte in verschiedenen Rechenzentren kombinieren</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/436648/"><img src="https://habrastorage.org/webt/zy/ry/ey/zyryeyqpveem2dyaws3drxmvori.png"><br><br>  In diesem Artikel werden wir untersuchen, warum der traditionelle Ansatz zur Kombination lokaler Netzwerke auf L2-Ebene bei einer signifikanten Erh√∂hung der Anzahl der Ger√§te unwirksam ist, und Ihnen erl√§utern, welche Probleme wir bei der Kombination von Projekten an verschiedenen Standorten l√∂sen konnten. <br><br><h2>  Normale L2-Schaltung </h2><br>  Da die IT-Infrastruktur im Rechenzentrum w√§chst, m√ºssen Kunden Server, Speicher und Firewalls in einem einzigen Netzwerk kombinieren.  Zu diesem Zweck schl√§gt Selectel zun√§chst die Verwendung eines lokalen Netzwerks vor. <br><br>  Das lokale Netzwerk ist als klassisches "Campus" -Netzwerk innerhalb desselben Rechenzentrums angeordnet. Nur Zugriffsschalter befinden sich direkt in den Racks mit den Servern.  Zugriffsschalter werden weiter zu einem einzigen Aggregationsschichtschalter kombiniert.  Jeder Kunde kann f√ºr jedes Ger√§t, das er mietet oder bei uns im Rechenzentrum platziert <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, eine Verbindung</a> zum lokalen Netzwerk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bestellen</a> . <br><a name="habracut"></a><br>  Zum Organisieren eines lokalen Netzwerks werden dedizierte Zugriffs- und Aggregationsschalter verwendet, sodass Probleme im Internetnetzwerk das lokale Netzwerk nicht beeintr√§chtigen. <br><br><img src="https://habrastorage.org/webt/j5/a5/re/j5a5re8shkxu356b54gjja2-mbg.png" title="Beispielschema f√ºr einen Client"><br>  Es spielt keine Rolle, in welchem ‚Äã‚ÄãRack sich der n√§chste Server befindet - Selectel kombiniert die Server in einem einzigen lokalen Netzwerk, und Sie m√ºssen nicht √ºber Switches oder den Serverstandort nachdenken.  Sie k√∂nnen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Server bestellen,</a> wenn er ben√∂tigt wird, und er wird mit dem lokalen Netzwerk verbunden. <br><br>  L2 funktioniert hervorragend, wenn das Rechenzentrum klein ist und nicht alle Racks voll sind.  Mit zunehmender Anzahl von Racks, Servern in Racks, Switches und Clients wird die Wartung der Schaltung jedoch sehr viel schwieriger. <br><br><img src="https://habrastorage.org/webt/tg/kw/im/tgkwimba1eybjn6vngl8yohqsm0.png" title="Beispielschema f√ºr mehrere Clients"><br>  Server eines Clients k√∂nnen sich in mehreren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rechenzentren befinden</a> , um Katastrophenschutz zu gew√§hrleisten, oder wenn es nicht m√∂glich ist, den Server in einem vorhandenen Rechenzentrum zu platzieren (z. B. sind alle Racks und alle Pl√§tze belegt).  Zwischen mehreren Rechenzentren ist auch eine Konnektivit√§t erforderlich - zwischen Servern in einem lokalen Netzwerk. <br><br>  Mit zunehmender Anzahl von Rechenzentren, Racks und Servern wird die Schaltung komplizierter.  Die Konnektivit√§t zwischen den Servern verschiedener Rechenzentren wurde zun√§chst einfach auf der Ebene der Aggregations-Switches mithilfe der VLAN-Technologie durchgef√ºhrt. <br><br><img src="https://habrastorage.org/webt/dj/jd/rf/djjdrfihfysh2jo3tblzpa4evmk.png" title="Ein Beispielschema f√ºr einen einzelnen Client in mehreren Rechenzentren"><br>  Der VLAN-Identifikationsraum ist jedoch sehr begrenzt (4095 VLAN-IDs).  Daher m√ºssen Sie f√ºr jedes Rechenzentrum einen eigenen Satz von VLANs verwenden, wodurch die Anzahl der m√∂glichen Kennungen verringert wird, die zwischen Rechenzentren verwendet werden k√∂nnen. <br><br><h2>  L2 Probleme </h2><br>  Bei Verwendung eines Schemas auf L2-Ebene mit VLAN kann ein fehlerhafter Betrieb eines der Server im Rechenzentrum zu Unterbrechungen bei der Bereitstellung von Diensten auf anderen Servern f√ºhren.  Die h√§ufigsten Probleme sind: <br><br><ul><li> Probleme mit STP (Spanning-Tree Protocol) </li><li>  Probleme mit Broadcast Storms </li><li>  Probleme mit falscher Multicast-Verarbeitung </li><li>  Human Factor (Link Transfer, VLAN Transfer) </li><li>  Probleme bei der Organisation der Reservierung f√ºr L2 </li><li>  Probleme mit unbekanntem Unicast-Verkehr </li><li>  Probleme mit der Anzahl der MAC-Adressen </li></ul><br>  Probleme mit STP h√§ngen h√§ufig mit den Einstellungen von Client-Servern oder Client-Ger√§ten zusammen.  Im Gegensatz zu g√§ngigen Verkehrsaustauschpunkten k√∂nnen wir STP nicht vollst√§ndig nach Zugriffsports filtern und Ports l√∂schen, wenn eine STP-PDU eintrifft.  Bei STP implementieren eine Reihe von Herstellern von Netzwerkger√§ten grundlegende Funktionen von Rechenzentrums-Switches wie das Erkennen von Schleifen im Netzwerk. <br><br>  Wenn STP auf der Clientseite nicht ordnungsgem√§√ü funktioniert, ist m√∂glicherweise die gesamte STP-Dom√§ne von mindestens einem Zugriffsschalter betroffen.  Die Verwendung von STP-Erweiterungen wie MSTP ist ebenfalls keine L√∂sung, da die Anzahl der Ports, VLANs und Switches h√§ufig die architektonische Skalierbarkeit des STP-Protokolls √ºberschreitet. <br><br><h3>  Sendung </h3><br>  Das Netzwerk im Rechenzentrum kann auf Ger√§ten verschiedener Hersteller aufgebaut werden.  Manchmal reichen sogar die Unterschiede in der Switch-Softwareversion aus, damit die Switches STP unterschiedlich handhaben k√∂nnen.  So gibt es beispielsweise im Rechenzentrum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dubrovka 3</a> 280 Racks, was die maximal m√∂gliche Anzahl von Switches in einer STP-Dom√§ne √ºberschreitet. <br><br>  Mit der weit verbreiteten Verwendung von STP in einem solchen Netzwerk √ºberschreitet die Reaktionszeit auf √Ñnderungen, insbesondere durch einfaches Ein- oder Ausschalten des Ports, alle Warteschwellen.  Sie m√∂chten nicht, dass Ihre Netzwerkverbindung f√ºr einige Minuten verschwindet, wenn einer der Clients den Port einschaltet? <br><br>  Probleme mit dem Broadcast-Verkehr treten h√§ufig sowohl aufgrund falscher Aktionen auf dem Server (z. B. Erstellen einer Br√ºcke zwischen mehreren Server-Ports) als auch aufgrund einer falschen Softwarekonfiguration auf den Servern auf.  Wir versuchen, m√∂gliche Probleme mit der Menge des Broadcast-Verkehrs, der in unser Netzwerk gelangt, auszugleichen.  Wir k√∂nnen dies jedoch an einem Serververbindungsport tun. Wenn 5 Server in einem Switch enthalten sind, von denen jeder die festgelegten Schwellenwerte nicht √ºberschreitet, k√∂nnen sie zusammen gen√ºgend Datenverkehr generieren, um die Steuerung des Aggregations-Switch auszul√∂sen.  Nach unserer eigenen Praxis k√∂nnen Probleme mit einem Broadcast-Sturm von der Serverseite durch einen bestimmten Ausfall der Netzwerkkarte des Servers verursacht werden. <br><br>  Durch den Schutz des gesamten Netzwerks ‚Äûsetzt‚Äú der Aggregationsschalter den Port, an dem die Netzwerkanomalie aufgetreten ist.  Leider f√ºhrt dies zur Inoperabilit√§t der f√ºnf Server, die diesen Vorfall verursacht haben, sowie zur Inoperabilit√§t anderer Server (bis zu mehreren Racks im Rechenzentrum). <br><br><h3>  Multicast </h3><br>  Probleme mit der fehlerhaften Verarbeitung des Multicast-Verkehrs sind sehr spezifische Probleme, die im Komplex aufgrund des fehlerhaften Betriebs der Software auf dem Server und der Software auf dem Switch auftreten.  Beispielsweise wird Corosync im Multicast-Modus zwischen mehreren Servern konfiguriert.  Der Austausch von Hello-Paketen erfolgt regelm√§√üig in kleinen Mengen.  In einigen F√§llen k√∂nnen Server mit installiertem Corosync jedoch eine ganze Reihe von Paketen weiterleiten.  Dieses Volume erfordert entweder eine spezielle Konfiguration der Switches oder die Verwendung der richtigen Verarbeitungsmechanismen (IGMP-Join und andere).  Bei fehlerhafter Bedienung der Mechanismen oder beim Ausl√∂sen von Schwellenwerten kann es zu Betriebsunterbrechungen kommen, die andere Kunden betreffen.  Je weniger Clients sich auf dem Switch befinden, desto unwahrscheinlicher ist es nat√ºrlich, dass Probleme von einem anderen Client auftreten. <br><br>  Der Faktor Mensch ist ein eher unvorhergesehenes Problem, das bei der Arbeit mit Netzwerkger√§ten auftreten kann.  Wenn der Netzwerkadministrator alleine ist und seine Arbeit kompetent aufbaut, die Aktionen dokumentiert und √ºber die Konsequenzen seiner Aktionen nachdenkt, ist die Wahrscheinlichkeit eines Fehlers recht gering.  Wenn jedoch die Anzahl der Ger√§te, die im Rechenzentrum in Betrieb sind, zunimmt, wenn viele Mitarbeiter vorhanden sind, wenn viele Aufgaben vorhanden sind, ist ein v√∂llig anderer Ansatz f√ºr die Arbeitsorganisation erforderlich. <br><br>  Einige Arten typischer Aktionen werden automatisiert, um menschliches Versagen zu vermeiden, aber viele Arten von Aktionen k√∂nnen derzeit nicht automatisiert werden, oder der Preis f√ºr die Automatisierung solcher Aktionen ist unangemessen hoch.  Zum Beispiel das physische Schalten von Patchkabeln auf einem Patchfeld, das Verbinden neuer Links und das Ersetzen vorhandener Links.  Alles im Zusammenhang mit physischem Kontakt mit SCS.  Ja, es gibt Patch-Panels, die einen Fernwechsel erm√∂glichen, aber sie sind sehr teuer, erfordern viel Vorarbeit und sind in ihren Funktionen sehr eingeschr√§nkt. <br><br>  Kein automatisches Patchfeld wird bei Bedarf ein neues Kabel verlegen.  Sie k√∂nnen beim Konfigurieren eines Switches oder Routers einen Fehler machen.  Geben Sie die falsche Portnummer (VLAN-Nummer) an, die bei der Eingabe eines numerischen Werts versiegelt werden soll.  Ber√ºcksichtigen Sie bei der Angabe zus√§tzlicher Einstellungen nicht deren Einfluss auf die vorhandene Konfiguration.  Mit zunehmender Komplexit√§t des Schemas, die das Redundanzschema kompliziert (zum Beispiel weil das aktuelle Schema die Skalierungsgrenze erreicht), steigt die Wahrscheinlichkeit menschlicher Fehler.  Jeder kann einen menschlichen Fehler haben, unabh√§ngig davon, ob sich das Ger√§t in der Konfigurationsphase befindet, ein Server, ein Switch, ein Router oder eine Art Transitger√§t. <br><br>  Die Organisation der Redundanz √ºber L2 scheint auf den ersten Blick eine einfache Aufgabe f√ºr kleine Netzwerke zu sein.  Der Cisco ICND-Kurs behandelt die Grundlagen der Verwendung von STP als Protokoll, das urspr√ºnglich f√ºr die Bereitstellung von L2-Redundanz entwickelt wurde.  STP hat viele Einschr√§nkungen, die in diesem Protokoll als "von Entwurf" bezeichnet werden.  Wir d√ºrfen nicht vergessen, dass jede STP-Dom√§ne eine sehr begrenzte "Breite" hat, dh die Anzahl der Ger√§te in einer STP-Dom√§ne ist im Vergleich zur Anzahl der Racks im Rechenzentrum recht gering.  Das STP-Protokoll in seiner urspr√ºnglichen Version unterteilt die Links in verwendete und Backups, wodurch Uplinks im normalen Betrieb nicht vollst√§ndig genutzt werden. <br><br>  Die Verwendung anderer L2-Reservierungsprotokolle unterliegt Einschr√§nkungen.  Zum Beispiel ERPS (Ethernet Ring Protection Switching) - f√ºr die verwendete physische Topologie, f√ºr die Anzahl der Ringe auf einem Ger√§t und f√ºr die Nutzung aller Verbindungen.  Die Verwendung anderer Protokolle beinhaltet in der Regel propriet√§re √Ñnderungen von verschiedenen Herstellern oder beschr√§nkt den Netzwerkaufbau auf eine ausgew√§hlte Technologie (z. B. TRILL / SPBm-Factory mit Avaya-Ger√§ten). <br><br><h3>  Unbekannt-Unicast </h3><br>  Ich m√∂chte insbesondere den Subtyp von Problemen mit unbekanntem Unicast-Verkehr hervorheben.  Was ist das?  Datenverkehr, der f√ºr eine bestimmte IP-Adresse √ºber L3 bestimmt ist, aber √ºber L2 im Netzwerk gesendet wird, dh an alle zu diesem VLAN geh√∂renden Ports √ºbertragen wird.  Diese Situation kann aus einer Reihe von Gr√ºnden auftreten, beispielsweise beim Empfang von DDoS an eine nicht belegte IP-Adresse.  Oder wenn w√§hrend eines Tippfehlers in der Serverkonfiguration eine Adresse, die im Netzwerk nicht vorhanden ist, als Sicherung angegeben wurde und auf dem Server in der Vergangenheit ein statischer ARP-Datensatz unter dieser Adresse vorhanden ist.  Unknown-Unicast wird angezeigt, wenn alle Eintr√§ge in den ARP-Tabellen vorhanden sind, jedoch keine MAC-Adresse des Empf√§ngers in den Switching-Tabellen der Transit-Switches vorhanden ist. <br><br>  Beispielsweise wechselt der Port, hinter dem sich der Netzwerkhost mit der angegebenen Adresse befindet, sehr oft in den Aus-Zustand.  Diese Art von Verkehr wird durch Transit-Switches begrenzt und h√§ufig auf die gleiche Weise wie Broadcast oder Multicast bedient.  Im Gegensatz zu ihnen kann unbekannter Unicast-Verkehr jedoch ‚Äû√ºber das Internet‚Äú und nicht nur √ºber das Client-Netzwerk initiiert werden.  Das Risiko von unbekanntem Unicast-Verkehr ist besonders hoch, wenn Filterregeln auf Grenzroutern das Spoofing von IP-Adressen von au√üen erm√∂glichen. <br><br>  Sogar die schiere Anzahl von MAC-Adressen kann manchmal ein Problem sein.  Bei einer Rechenzentrumsgr√∂√üe von 200 Racks und 40 Servern pro Rack ist es unwahrscheinlich, dass die Anzahl der MAC-Adressen die Anzahl der Server im Rechenzentrum erheblich √ºberschreitet.  Dies ist jedoch keine wahre Aussage mehr, da eines der Virtualisierungssysteme auf den Servern gestartet werden kann und jede virtuelle Maschine durch ihre MAC-Adresse oder sogar mehrere dargestellt werden kann (beispielsweise beim Emulieren mehrerer Netzwerkkarten in einer virtuellen Maschine).  Insgesamt k√∂nnen wir mehr als mehrere tausend legitime MAC-Adressen von einem Rack in 40 Servern erhalten. <br><br>  Eine solche Anzahl von MAC-Adressen kann bei einigen Switch-Modellen die F√ºlle der Switching-Tabelle beeintr√§chtigen.  Dar√ºber hinaus wird bei bestimmten Switch-Modellen beim Ausf√ºllen der Switching-Tabelle Hashing verwendet, und einige MAC-Adressen k√∂nnen Hash-Kollisionen verursachen, die zum Auftreten von unbekanntem Unicast-Verkehr f√ºhren.  Eine zuf√§llige Suche nach MAC-Adressen auf einem gemieteten Server mit einer Geschwindigkeit von beispielsweise 4.000 Adressen pro Sekunde kann dazu f√ºhren, dass die Switching-Tabelle auf dem Access Switch √ºberl√§uft.  Nat√ºrlich beschr√§nken die Switches die Anzahl der MAC-Adressen, die an den Switch-Ports gelernt werden k√∂nnen. Abh√§ngig von der spezifischen Implementierung dieses Mechanismus k√∂nnen die Daten jedoch unterschiedlich interpretiert werden. <br><br>  Das Senden von Datenverkehr an die durch diesen Mechanismus gefilterte MAC-Adresse f√ºhrt wiederum zum Auftreten von Datenverkehr mit unbekanntem Unicast.  Das Unangenehmste in dieser Situation ist, dass die Schalter vom Hersteller selten auf Selbstheilung nach F√§llen mit √úberlauf des Schalttisches getestet werden.  Ein einzelner √úberlauf der Tabelle, der beispielsweise durch einen Fehler eines Clients in den HPING-Parametern oder durch das Schreiben eines Skripts zur √úberwachung seiner Infrastruktur verursacht wird, kann zu einem Neustart des Switches und einer Unterbrechung der Kommunikation f√ºr alle Server im Rack f√ºhren.  Wenn ein solcher √úberlauf auf dem Switch auf Aggregationsebene auftritt, kann ein Neustart des Switch zu einer 15-min√ºtigen Ausfallzeit des lokalen Netzwerks des gesamten Rechenzentrums f√ºhren. <br><br>  Ich m√∂chte vermitteln, dass die Verwendung von L2 nur f√ºr begrenzte F√§lle gerechtfertigt ist und viele Einschr√§nkungen auferlegt.  Die Gr√∂√üe des Segments, die Anzahl der L2-Segmente - dies sind alle Parameter, die jedes Mal ausgewertet werden m√ºssen, wenn Sie ein neues VLAN mit L2-Konnektivit√§t hinzuf√ºgen.  Und je kleiner die L2-Segmente sind, desto einfacher und damit zuverl√§ssiger ist das Netzwerk in Betrieb. <br><br><h2>  Typische L2-Anwendungsf√§lle </h2><br>  Wie bereits erw√§hnt, wird bei der schrittweisen Entwicklung der Infrastruktur in einem Rechenzentrum ein lokales L2-Netzwerk verwendet.  Leider ist diese Verwendung auch bei der Entwicklung von Projekten in einem anderen Rechenzentrum oder in einer anderen Technologie (z. B. virtuellen Maschinen in der Cloud) impliziert. <br><br><h3>  Verkn√ºpfen Sie Front- und Back-End, Backup </h3><br>  In der Regel beginnt die Verwendung eines lokalen Netzwerks mit der Trennung der Funktionalit√§t der Front- und Back-End-Dienste, wobei das DBMS einem separaten Server zugewiesen wird (um die Leistung zu verbessern und den Betriebssystemtyp auf dem Anwendungsserver und dem DBMS zu trennen).  Die Verwendung von L2 f√ºr diese Zwecke erscheint zun√§chst gerechtfertigt, in dem Segment gibt es nur wenige Server, oft befinden sie sich sogar im selben Rack. <br><br><img src="https://habrastorage.org/webt/je/gf/1p/jegf1phbpite5kxigvt3vxmyj6a.png" title="Einfaches L2-Segment"><br>  Server sind in einem VLAN, in einem oder mehreren Switches enthalten.  Mit zunehmender Anzahl von Ger√§ten werden immer mehr neue Server in die Switches neuer Racks im Rechenzentrum aufgenommen, von denen aus die L2-Dom√§ne an Breite zunimmt. <br><br><img src="https://habrastorage.org/webt/j5/a5/re/j5a5re8shkxu356b54gjja2-mbg.png" title="L2-Segmenterweiterung"><br>  Es werden neue Server angezeigt, einschlie√ülich Sicherungsdatenbankservern, Sicherungsservern und dergleichen.  Solange sich das Projekt in einem Rechenzentrum befindet, treten in der Regel keine Skalierungsprobleme auf.  Anwendungsentwickler gew√∂hnen sich nur daran, dass sich auf dem n√§chsten Server im lokalen Netzwerk die IP-Adresse erst im letzten Oktett √§ndert und Sie keine separaten Routing-Regeln schreiben m√ºssen. <br><br>  Entwickler werden gebeten, ein √§hnliches Schema anzuwenden, wenn das Projekt w√§chst, wenn die folgenden Server bereits in einem anderen Rechenzentrum gemietet sind oder wenn ein Teil des Projekts auf die virtuellen Maschinen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Cloud verschoben wird</a> .  Auf dem Bild sieht alles sehr einfach und sch√∂n aus: <br><br><img src="https://habrastorage.org/webt/bx/2y/ho/bx2yholhikhhkjpdaavttkfgucm.png" title="Ein Beispiel f√ºr die Kombination von zwei Standorten mit L2"><br>  Es scheint, dass Sie nur zwei Aggregations-Switches in DC1 und DC2 mit einem VLAN verbinden m√ºssen.  Aber was steckt hinter dieser einfachen Aktion? <br><br><h3>  Ressourcenreservierung </h3><br>  Zun√§chst erh√∂hen wir die Breite der L2-Dom√§ne, sodass alle potenziellen Probleme des lokalen Netzwerks von DC1 in DC2 auftreten k√∂nnen.  Wem wird es gefallen, dass sich seine Server in DC2 befinden und der Vorfall im Zusammenhang mit der Unzug√§nglichkeit des lokalen Netzwerks aufgrund falscher Aktionen in DC1 auftritt? <br><br>  Zweitens m√ºssen Sie sich um die Sicherung dieses VLAN k√ºmmern.  Der Aggregationsschalter in jedem Rechenzentrum ist der Fehlerpunkt.  Das Kabel zwischen Rechenzentren ist ein weiterer Fehlerpunkt.  Jeder Fehlerpunkt sollte reserviert werden.  Zwei Aggregationsschalter, zwei Kabel von Aggregationsschaltern zu Zugriffsschaltern, zwei Kabel zwischen Rechenzentren ... Jedes Mal, wenn die Anzahl der Komponenten zunimmt, wird die Schaltung komplizierter. <br><br><img src="https://habrastorage.org/webt/nq/ku/ws/nqkuws41ips807benbzzemxfzry.png" title="Beispiel f√ºr ein redundantes L2-Schema"><br>  Die Komplexit√§t des Schemas wird durch die Notwendigkeit verursacht, jedes Element im System zu reservieren.  F√ºr eine vollst√§ndige Sicherung von Ger√§ten und Links m√ºssen Sie fast jedes Element duplizieren.  In einem so gro√üen Netzwerk ist es nicht mehr m√∂glich, STP zum Organisieren von Redundanz zu verwenden.  Es w√§re m√∂glich, alle Netzwerkelemente, insbesondere Zugriffsschalter, als Komponenten der MPLS-Cloud darzustellen, dann w√ºrde aufgrund der Funktionalit√§t des MPLS-Protokolls Redundanz erhalten. <br><br>  MPLS-Ger√§te sind jedoch in der Regel doppelt so teuer wie Nicht-MPLS-Ger√§te.  Und es sollte beachtet werden, dass der MPU-Switch in 1U, der einen guten Grad an Skalierbarkeit aufweist, die Implementierung der vollen MPLS-Funktionalit√§t in der Steuerebene in der Praxis bis vor kurzem nicht existierte.  Infolgedessen m√∂chte ich die Auswirkungen von L2-Problemen auf das vorhandene Netzwerk beseitigen oder minimieren, aber gleichzeitig die M√∂glichkeit behalten, Ressourcen zu reservieren. <br><br><h2>  √úbergang zu L3 </h2><br>  Wenn jede Verbindung im Netzwerk als separates IP-Segment und jedes Ger√§t als separater Router dargestellt wird, ben√∂tigen wir keine Redundanz auf L2-Ebene.  Die Redundanz von Verbindung und Router wird durch dynamische Routing-Protokolle und Routing-Redundanz im Netzwerk sichergestellt. <br><br>  Innerhalb des Rechenzentrums k√∂nnen wir die vorhandenen Server-Interaktionsschemata √ºber L2 miteinander speichern, und der Zugriff auf die Server in einem anderen Rechenzentrum erfolgt √ºber L3. <br><br><img src="https://habrastorage.org/webt/hc/2q/n4/hc2qn4wcnecwsrrkxh-daualss8.png" title="Standortkonnektivit√§t mit L3"><br>  Somit sind Rechenzentren durch L3-Konnektivit√§t miteinander verbunden.  Das hei√üt, es wird emuliert, dass ein Router zwischen den Rechenzentren installiert ist (tats√§chlich mehrere zur Sicherung).  Auf diese Weise k√∂nnen Sie L2-Dom√§nen zwischen Rechenzentren aufteilen, in jedem Rechenzentrum ein eigenes VLAN verwenden und zwischen diesen kommunizieren.  F√ºr jeden Client k√∂nnen Sie sich wiederholende Bereiche von IP-Adressen verwenden, die Netzwerke sind vollst√§ndig voneinander isoliert und Sie k√∂nnen nicht vom Netzwerk eines Clients in das Netzwerk eines anderen Clients gelangen (es sei denn, beide Clients stimmen einer solchen Verbindung zu). <br><br>  Wir empfehlen die Verwendung von IP-Segmenten ab 10.0.0.0/8 f√ºr lokale Netzwerke.  F√ºr das erste Rechenzentrum ist das Netzwerk beispielsweise 10.0.1.0/24, f√ºr das zweite 10.0.2.0/24.  Selectel auf dem Router schreibt die IP-Adresse dieses Subnetzes vor.  In der Regel sind .250-.254-Adressen f√ºr Selectel-Netzwerkger√§te reserviert, und eine .254-Adresse dient als Gateway zu anderen LANs.  Die Route wird allen Ger√§ten in allen Rechenzentren zugewiesen: <br><br> <code>route add 10.0.0.0 mask 255.0.0.0 gw 10.0.x.254</code> <br> <br>  Wobei x die Nummer des Rechenzentrums ist.  Aufgrund dieser Route ‚Äûsehen‚Äú sich die Server in den Rechenzentren durch Routing. <br><br><img src="https://habrastorage.org/webt/hf/x6/h1/hfx6h13labuqr97rfirrzwtw8nk.png" title="Routing-Beispiel zum Kombinieren von zwei Standorten auf L3"><br>  Das Vorhandensein einer solchen Route vereinfacht die Skalierung des Schemas im Fall beispielsweise des Auftretens eines dritten Rechenzentrums.  F√ºr Server im dritten Rechenzentrum werden dann IP-Adressen aus dem n√§chsten Bereich, 10.0.3.0/24, auf dem Router registriert - der Adresse 10.0.3.254. <br><br><img src="https://habrastorage.org/webt/r_/lg/n5/r_lgn5-xrch-wfcytnqvyy7ujiy.png" title="Routing-Beispiel zum Kombinieren von drei Standorten auf L3"><br>  Eine Besonderheit bei der Implementierung eines solchen Schemas besteht darin, dass im Falle eines Ausfalls des Rechenzentrums oder externer Kommunikationskan√§le keine zus√§tzliche Reservierung erforderlich ist.  Wenn beispielsweise das Rechenzentrum 1 ausf√§llt, bleibt die Verbindung zwischen dem Rechenzentrum 2 und dem Rechenzentrum 3 vollst√§ndig erhalten, und wenn das Schema mit dem L2-Feed zwischen den Rechenzentren √ºber eines von ihnen implementiert wird, wie in der Abbildung dargestellt: <br><br><img src="https://habrastorage.org/webt/dj/jd/rf/djjdrfihfysh2jo3tblzpa4evmk.png" title="Routing-Vorgang beim Speichern von L2-Schemata"><br>  Die Verbindung zwischen dem Rechenzentrum 2 und dem Rechenzentrum 3 h√§ngt von der Effizienz des Rechenzentrums 1 ab. Oder die Organisation zus√§tzlicher Verbindungen und die Verwendung komplexer L2-Reservierungsschemata sind erforderlich.  Und w√§hrend das L2-Schema gespeichert wird, reagiert das gesamte Netzwerk immer noch sehr empfindlich auf fehlerhaftes Schalten, die Bildung von Vermittlungsschleifen, verschiedene Verkehrsst√ºrme und andere Probleme. <br><br><h3>  L3-Segmente innerhalb von Projekten </h3><br>  Neben der Verwendung unterschiedlicher L3-Segmente in verschiedenen Rechenzentren ist es sinnvoll, ein separates L3-Netzwerk f√ºr Server in verschiedenen Projekten zuzuweisen, die h√§ufig mit unterschiedlichen Technologien erstellt werden.  Beispielsweise Hardwareserver im Rechenzentrum in einem IP-Subnetz, virtuelle Server im selben Rechenzentrum, aber in der VMware-Cloud in einem anderen IP-Subnetz einige Staging-bezogene Server im dritten IP-Subnetz .  Dann f√ºhren zuf√§llige Fehler in einem der Segmente nicht zu einem vollst√§ndigen Ausfall aller im lokalen Netzwerk enthaltenen Server. <br><br><img src="https://habrastorage.org/webt/v3/zi/gd/v3zigd00-7klicdt31likdem460.png" title="Segmentierung von Netzwerken verschiedener Projekte auf L3-Ebene"><br><h2>  Router-Reservierung </h2><br>  Das ist alles beeindruckend, aber es gibt einen einzigen Fehlerpunkt zwischen Projekten - dies ist der Router.  Was ist in diesem Fall zu tun?  In der Tat ist der Router nicht allein.  F√ºr jedes Rechenzentrum werden zwei Router zugewiesen, und f√ºr jeden Kunden bilden sie mithilfe des VRRP-Protokolls die virtuelle IP .254. <br><br>  Die Verwendung von VRRP zwischen zwei benachbarten Ger√§ten mit einem gemeinsamen L2-Segment ist gerechtfertigt.  F√ºr Rechenzentren, die geografisch verteilt sind, werden verschiedene Router verwendet, zwischen denen MPLS organisiert ist.  Somit ist jeder Client, der auf diese Weise eine Verbindung zum lokalen Netzwerk herstellt, mit einem separaten L3VPN verbunden, der auf diesen MPLS-Routern daf√ºr vorgesehen ist.  Und das Schema sieht in Ann√§herung an die Realit√§t so aus: <br><br><img src="https://habrastorage.org/webt/9y/ae/s8/9yaes8vvilol5-zrhikhychdvs4.png" title="Reservierungsorganisation"><br>  Die Gateway-Adresse f√ºr jedes .254-Segment wird von VRRP zwischen den beiden Routern reserviert. <br><br><h2>  Fazit </h2><br>  Zusammenfassend l√§sst sich sagen, dass wir durch die √Ñnderung des lokalen Netzwerktyps von L2 auf L3 die Skalierbarkeit beibehalten, die Zuverl√§ssigkeit und Fehlertoleranz erh√∂hen und zus√§tzliche Redundanzschemata implementieren konnten.  Dar√ºber hinaus wurden die bestehenden Beschr√§nkungen und ‚ÄûFallstricke‚Äú von L2 umgangen. <br><br>  Mit dem Wachstum von Projekten und Rechenzentren sto√üen vorhandene Standardl√∂sungen an ihre Grenzen der Skalierbarkeit.  Dies bedeutet, dass sie nicht mehr zur effektiven Probleml√∂sung geeignet sind.  Die Anforderungen an Zuverl√§ssigkeit und Stabilit√§t des Gesamtsystems steigen st√§ndig, was sich wiederum auf den Planungsprozess auswirkt.  Es ist wichtig zu ber√ºcksichtigen, dass optimistische Wachstumsprognosen ber√ºcksichtigt werden sollten, damit es in Zukunft kein System gibt, das nicht skaliert werden kann. <br><br>  Sagen Sie uns - verwenden Sie bereits L3VPN?  Wir sehen uns in den Kommentaren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436648/">https://habr.com/ru/post/de436648/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436638/index.html">Verteilen von Fenstern zwischen Monitoren nach dem Aufwachen aus dem Ruhemodus</a></li>
<li><a href="../de436640/index.html">Meine Erfahrung mit Werbung und Entwicklung von Android- und iOS-Anwendungen</a></li>
<li><a href="../de436642/index.html">Ticket to Ride.Europe - bescheidene Schritte in der Arithmetik des Spiels</a></li>
<li><a href="../de436644/index.html">Synthetische Symbole und Module (WinDbg / DbgEng)</a></li>
<li><a href="../de436646/index.html">Durchscheinend auf Android und AdjustResize</a></li>
<li><a href="../de436650/index.html">3 erfolgreiche App-Monetarisierungsstrategien im Jahr 2019</a></li>
<li><a href="../de436652/index.html">MPS 2018.3: Generierungspl√§ne, Verbesserungen in der Assembler- und Verpackungssprache sowie in der Editor-Sprache, aktualisierte Oberfl√§che</a></li>
<li><a href="../de436654/index.html">Entwicklung eines Teams zur Abfrage von Daten aus der Datenbank - Teil 4, abschlie√üend</a></li>
<li><a href="../de436656/index.html">Automatisierung f√ºr Selbstst√§ndige: Integration von Steuern in ein IT-Projekt</a></li>
<li><a href="../de436658/index.html">Die Zukunft des Einzelhandels: Wichtige digitale Trends basierend auf der Big Show 2019 von NRF Retail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>