<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👆🏻 🚵🏿 🐒 Le philosophe de l'intelligence artificielle Eliezer Yudkowsky sur la singularité, le cerveau bayésien et les gobelins dans un cabinet 🧥 👨🏽‍🍳 👋🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eliezer Shlomo Yudkovsky est un spécialiste américain de l'intelligence artificielle, qui étudie les problèmes de singularité technologique et préconi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le philosophe de l'intelligence artificielle Eliezer Yudkowsky sur la singularité, le cerveau bayésien et les gobelins dans un cabinet</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/404137/"><img src="https://habrastorage.org/getpro/geektimes/post_images/152/af6/3ff/152af63ff689233ebebadde98fed2580.png" alt="image"><br><br>  <i>Eliezer Shlomo Yudkovsky est un spécialiste américain de l'intelligence artificielle, qui étudie les problèmes de singularité technologique et préconise la création de l'IA amie.</i>  <i>Dans les milieux non universitaires, il est mieux connu comme l'auteur de la fan fiction Harry Potter et Méthodes de rationalité sous les auspices de Moins mal.</i> <br><br>  J'ai toujours été étonné par des gens intelligents qui croient en des choses qui me paraissent absurdes.  Par exemple, le généticien et directeur des National Institutes of Health, Francis Collins, pense que Jésus est ressuscité des morts.  Le théoricien de l'IA Eliezer Yudkowsky croit que les voitures ... Mais mieux je vais lui donner la parole moi-même.  En 2008, je l'ai interviewé sur Bloggingheads.tv, mais rien de bon n'en est sorti, car j'ai décidé qu'il était un adepte du gourou de la singularité <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ray Kurzweil</a> .  Mais Yudkovsky n'a suivi personne et n'est jamais allé à l'université.  Il est un théoricien obstiné et original de l'intelligence, humaine et artificielle.  Son travail (par exemple, un essai qui m'a aidé à comprendre ou à donner l'illusion de comprendre les théorèmes de Bayes) dégage l'arrogance de l'autodidacte, dont les arêtes vives n'ont pas été polies par l'éducation formelle - mais cela fait partie de son charme.  Même quand cela vous ennuie, Yudkovsky est drôle, frais, provocateur.  Pour plus de détails sur sa biographie, consultez son site Internet personnel ou le site Internet de l'Institute for the Study of Machine Intelligence, auquel il a participé.  Et lisez cette interview avec un bonus sous forme de commentaires de sa femme Briena. <br><a name="habracut"></a><br>  <b>Horgan</b> : Quand on vous demande à une fête ce que vous faites, que répondez-vous? <br><br>  <b>Yudkovsky</b> : <b>Cela</b> dépend de l'événement.  «Je suis un spécialiste de la théorie de la décision», ou «Co-fondateur de l'Institut pour l'étude de l'intelligence artificielle», ou, s'il s'agit d'un parti d'un type différent, je parle de mes œuvres d'art. <br><br>  <b>X:</b> Quel est votre film IA préféré et pourquoi? <br><br>  <b>Yu: L'</b> IA dans les films est terriblement standard.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ex Machina s'est</a> approché d'une exception à cette règle comme on peut s'y attendre. <br><br>  <b>X:</b> L'utilité des collèges est-elle surfaite? <br><br>  <b>Yu:</b> Je serais surpris si son utilité était sous-estimée, étant donné les exigences sociales pour y mettre fin.  Pour autant que je sache, il n'y a aucune raison de ne pas croire les économistes qui pensent que le collège est devenu une sorte de «produit prestigieux» et que les tentatives d'augmentation des prêts étudiants ont simplement augmenté le coût du collège et le fardeau de la dette étudiante. <br><br>  <b>X:</b> Pourquoi écrivez-vous des histoires de fiction? <br><br>  <b>Yu:</b> Si vous reformulez les bandes dessinées de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Wondermark</a> : "Au début, j'ai essayé de ne pas le faire, mais cela n'a pas fonctionné." <br><br>  De plus, la littérature sérieuse transmet la connaissance, tandis que la fiction transmet l'expérience.  Si vous voulez comprendre la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formule des</a> preuves <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de Bayes</a> , je peux utiliser des diagrammes.  Si vous voulez savoir comment utiliser la logique Bayes, j'ai besoin d'écrire une histoire dans laquelle le personnage le fait. <br><br>  <b>X:</b> Êtes-vous religieux dans un sens? <br><br>  <b>Yu:</b> Non.  Lorsque vous faites une erreur, vous devez éviter la tentation de partir en défense, essayer de trouver un point de vue à partir duquel vous avez au moins un peu raison.  Il est beaucoup plus sage de dire: «Oh», d'admettre que vous n'aviez même pas un peu raison, d'avaler une pilule amère entière et de continuer à vivre.  C'est ainsi que l'humanité devrait se rapporter à la religion. <br><br>  <b>X:</b> Si vous deveniez le «Roi du monde», quel serait en haut de votre liste de choses à faire? <br><br>  <b>Yu:</b> J'ai écrit une fois: «Un test pour un libertaire fonctionne comme ceci: imaginez que vous avez gagné en puissance;  que penserez-vous d'abord - des lois que vous acceptez ou des lois que vous abrogez? »  Je ne suis pas 100% libertaire, car toutes mes envies ne s'expriment pas dans l'abolition des lois et l'assouplissement des restrictions.  Mais j'imagine comment j'essaierais de créer un monde dans lequel un chômeur pourrait vous proposer de vous rendre au travail, obtenir 5 $ pour 20 minutes de route, et rien de mal ne lui arriverait à cause de cela.  Il n'aurait pas à perdre son assurance chômage, à enregistrer son autorisation de faire des affaires, à perdre son assurance médicale, à se soumettre à un audit, à demander à un avocat de certifier que son travail est conforme aux règles de la Labour Protection Administration, etc.  Il aurait juste ajouté 5 $. <br><br>  J'essaierais de retourner dans un état où l'embauche d'un employé serait aussi simple qu'en 1900.  Peut-être que maintenant il y a un sens dans certaines mesures de sécurité, mais j'essaierais de créer une telle sécurité qui n'entrave pas une personne et ne produit pas de papiers à la suite d'un simple retour d'une personne à l'économie. <br><br>  J'essaierais de faire tout ce que les économistes intelligents crient depuis longtemps et qu'aucun État ne fait.  Remplacer les impôts sur les investissements et les revenus par des taxes à la consommation et à l'immobilier.  Remplacez le salaire minimum par des charges sociales négatives.  Établir une politique de ciblage du PIB nominal pour les banques centrales et arrêter les structures de soutien "trop ​​grandes pour faire faillite".  Exiger que, lors d'un litige en matière de brevet, la partie perdante paie des frais juridiques [ <i>à la suite de ce que l'on appelle</i>  <i>Règle anglaise - contrairement aux lois américaines, selon lesquelles chacune des parties paie ses propres frais - env.</i>  <i>perev.</i>  ] et ramène la durée du droit d'auteur à 28 ans.  Éliminez les obstacles à la construction de maisons.  Copiez l'assurance maladie de Singapour.  Gouvernement électronique en Estonie.  Remplacer les comités et les processus décisionnels complexes par des personnes spécifiques qui prennent des décisions publiquement documentées et qui en sont responsables.  Mener des expériences contrôlées avec différentes options de gestion des pays et prendre en compte leurs résultats.  Je peux rester sur la liste pendant des heures. <br><br>  Tout cela n'aura peut-être pas d'importance dans deux cent millions d'années.  Mais les actifs nominaux résultant du boom économique peuvent faire du bon travail pendant que j'essaie de comprendre ce que nous ferons de l'intelligence artificielle.  La chose la plus évidente est le projet Manhattan quelque part sur l'île, avec un paiement basé sur la concurrence entre les plus grands hedge funds, dans lequel les gens peuvent explorer le problème de l'intelligence artificielle généralisée sans publier les résultats de leur travail amènent automatiquement la fin du monde.  Et à moins que nous n'acceptions que j'ai des capacités magiques ou un régime fondamentalement irréversible, je ne vois pas comment une loi que j'accepterais retarderait assez fortement l'approche de l'IA sur une planète où les ordinateurs sont omniprésents. <br><br>  Mais tout cela peut encore être considéré comme une expérience de pensée impossible et dans la vie réelle, la probabilité d'une telle expérience est nulle. <br><br>  <b>X:</b> Qu'est-ce qui est si bon avec le théorème de Bayes? <br><br>  <b>Yu:</b> Eh bien, par exemple, elle est très profonde.  Il est donc difficile de répondre brièvement à une telle question. <br><br>  Je pourrais répondre que le théorème de Bayes peut être appelé la deuxième loi de la thermodynamique pour la cognition.  Si vous concluez que la probabilité d'une hypothèse est de 99%, que ce soit la présence de lait dans le supermarché ou la cause anthropique du réchauffement climatique, alors vous disposez d'une combinaison de probabilités a priori suffisamment bonnes et de preuves assez fiables.  Ce n'est pas une exigence réglementaire, c'est une loi.  Tout comme une voiture ne peut pas conduire sans dissiper l'entropie, vous ne pouvez pas obtenir une bonne image du monde sans effectuer un processus dans lequel une structure bayésienne existe quelque part à l'intérieur, même si les probabilités ne sont pas utilisées directement dans le processus. <br><br>  Personnellement, je pense que la principale chose que Bayes peut nous offrir est l'existence de règles, de lois de fer qui déterminent si la façon de penser fonctionne pour marquer la réalité.  On dit aux mormons qu'ils reconnaissent la vérité du Livre de Mormon à travers une sensation de brûlure dans le cœur.  Acceptez de façon conservatrice la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">probabilité a priori du</a> Livre de Mormon de un sur un milliard.  Ensuite, nous évaluons la probabilité que le Livre de Mormon ne soit pas vrai, et quelqu'un a ressenti une sensation de brûlure dans le cœur après lui avoir dit que cela devait être attendu.  Si vous comprenez la formule de Bayes, nous nous rendrons immédiatement compte que la faible probabilité de la preuve est incommensurable avec la faible probabilité de l'hypothèse qu'ils tentent de prouver avec son aide.  Vous n'avez même pas besoin de trouver des chiffres spécifiques pour comprendre qu'ils ne convergent pas - comme l'a découvert Philip Tetlock dans son étude des « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">superprédicteurs</a> », ils connaissaient souvent la formule de Bayes, mais donnaient rarement des chiffres spécifiques.  Dans un sens, il est plus difficile de vous tromper si vous comprenez qu'il existe une sorte de mathématiques avec lesquelles vous pouvez déterminer avec précision la force de la preuve et comprendre si cela suffit pour surmonter la faible probabilité de l'hypothèse.  Vous ne pouvez pas simplement inventer quelque chose et y croire, car cela ne fonctionne pas comme ça. <br><br>  <b>X:</b> L'hypothèse du cerveau bayésien vous impressionne-t-elle? <br><br>  <b>Yu:</b> Je pense que certaines personnes qui discutent de ce sujet parlent de différentes choses.  Demander si le cerveau est un algorithme bayésien, c'est comme demander si la Honda Accord est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">propulsée par un moteur thermique Carnot</a> .  Si une personne dit: "Chaque voiture est un processus thermodynamique qui nécessite du carburant et dissipe la chaleur parasite", et une autre personne entend: "Si vous construisez un diagramme de cycle de Carnot et montrez sa mécanique, il doit convenir qu'il ressemble à l'intérieur d'une Honda Accord ", Alors un débat houleux est inévitable. <br><br>  Certaines personnes seront très heureuses quand elles ouvriront le moteur à combustion interne, y trouveront les cylindres et diront: "Je suis sûr qu'elles convertissent la chaleur en pression et aident à faire avancer la voiture!"  Et ils auront raison, mais d'autres diront: «Vous vous concentrez sur le seul composant d'un ensemble beaucoup plus large de pièces automobiles.  Le convertisseur catalytique est également très important, et il ne figure pas sur vos diagrammes de cycle Carnot.  Et parfois, un climatiseur fonctionne pour nous, fonctionnant exactement à l'opposé du fonctionnement du moteur thermique selon vos paroles. » <br><br>  Je ne pense pas que ce serait surprenant de dire que les gens qui dénoncent vous: «Vous n'êtes manifestement pas familier avec les voitures modernes;  vous avez besoin de tout un ensemble de méthodes différentes pour construire un moteur, comme des bougies et des convertisseurs catalytiques, et pas seulement de vos processus thermodynamiques », ils manquent un niveau clé d'abstraction. <br><br>  Mais si vous voulez savoir si le cerveau peut être considéré littéralement bayésien, et non un appareil qui effectue un travail cognitif, dont nous pouvons comprendre la nature en utilisant des méthodes bayésiennes, alors je peux répondre à votre question: "Non, bien sûr."  Il peut y avoir plusieurs «cylindres» bayésiens dans ce «moteur», mais beaucoup sembleront aussi étranges que les ceintures de sécurité et la climatisation.  Mais ces ajouts ne changeront pas le fait que pour identifier correctement une pomme sur la base de preuves sensorielles, quelque chose doit être fait qui peut être interprété comme un résultat de l'induction qui peut comprendre le concept d'une pomme et est mis à jour sur la base de preuves qui distinguent les pommes des non-pommes. <br><br>  <b>X:</b> Est-il possible d'être trop rationnel? <br><br>  <b>Yu:</b> Vous pouvez entrer dans le soi-disant  "La vallée de la mauvaise rationalité."  Si avant cela vous étiez irrationnel à plusieurs égards, en vous équilibrant, alors si vous devenez rationnel, vous pouvez devenir pire qu'avant.  Plus vous devenez rationnel, pire vous pouvez obtenir si vous choisissez la mauvaise direction pour appliquer vos compétences. <br><br>  Mais je ne recommanderais pas de prendre trop soin d'une telle opportunité.  À mon avis, les gens qui parlent de l'habileté irrationnelle peuvent être des crétins.  Il est difficile de trouver une situation de vie réaliste, pas farfelue, dans laquelle vous pouvez décider d'être irrationnel, et dont l'issue vous est encore inconnue.  Dans la vraie vie, il vaut mieux se dire la vérité et ne pas être intelligent. <br><br>  Il est possible que le représentant idéal de la pensée bayésienne soit incompatible avec une vie intéressante et divertissante.  Mais ce n'est clairement pas un problème aussi important que notre tendance à l'autodestruction. <br><br>  <b>X: En</b> quoi votre point de vue sur la singularité diffère-t-il de celui de Kurzweil? <br><br>  <b>Yu:</b> <br>  • Je ne pense pas que la loi de Moore puisse être appliquée à l'IA.  L'IA est un problème logiciel. <br>  • Je ne pense pas que le premier intellect surhumain surgira de la fusion des machines avec les gens.  Cent ans se sont écoulés depuis l'avènement des voitures, et nous essayons tout juste de faire un exosquelette pour un cheval, et une voiture ordinaire est encore plus rapide. <br>  • Je ne pense pas que la première IA forte sera basée sur des algorithmes de neurobiologie, tout comme les avions n'étaient pas basés sur des oiseaux. <br>  • Je ne pense pas que la fusion des nano, info et biotechnologies soit possible, inévitable, bien définie ou nécessaire. <br>  • Je pense que de 1930 à 1970, il y a eu plus de changements que de 1970 à 2010. <br>  • Je pense que dans les pays développés, la productivité stagne. <br>  • Je pense que l'extrapolation de la loi de Moore au progrès technologique, supposant prédire tout ce qui sera plus intelligent que les humains après l'avènement de l'IA, est une chose très étrange.  Une intelligence artificielle plus intelligente ruine tous vos graphiques. <br>  • Certains analystes, comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Illka ​​Tuomi</a> , pensent que la loi de Moore a violé au début des années 2000.  Je ne suis pas sûr de pouvoir m'opposer. <br>  • Le seul seuil technologique qui m'intéresse est celui où l'IA gagne la capacité de s'améliorer.  Nous n'avons pas de calendrier pour atteindre ce seuil, et on ne sait pas ce qu'il sera (bien qu'il ne devrait pas dépasser largement le niveau d'une personne, car une personne comprend l'informatique), de sorte que son offensive ne peut être prédite. <br>  • Je ne pense pas que le résultat de tels progrès sera bon par défaut.  Je pense que cela peut être réparé, mais il faudra y réfléchir sérieusement, et les chiffres clés ne sont pas intéressés par cela.  Dire aux gens que nous sommes sur une trajectoire naturelle vers de grands et merveilleux moments sera un mensonge. <br>  • Je pense que la «singularité» est devenue un mot valise avec trop de sens et de détails incompatibles à l'intérieur, alors j'ai cessé de l'utiliser. <br><br>  <b>X:</b> Êtes-vous susceptible de devenir un cyborg ultra-intelligent? <br><br>  <b>Yu:</b> La loi de conjonction des probabilités dit que P (A&amp;B) &lt;= P (A).  La probabilité d'occurrence simultanée des événements A et B est inférieure à la probabilité d'occurrence d'un événement A. Dans les expériences où les gens croient que P (A&amp;B)&gt; P (A) pour deux événements A et B, une « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">erreur de conjonction</a> » apparaît - par exemple, en 1982, des experts du Congrès international des prévisions ont attribué une plus grande probabilité à l'événement «La Russie envahit la Pologne et la rupture des relations diplomatiques avec l'URSS» que la probabilité d'un événement distinct «rupture des relations diplomatiques avec l'URSS», désigné par un autre groupe.  De même, un autre groupe a attribué une plus grande probabilité à l'événement "Un tremblement de terre en Californie conduit à une inondation faisant des milliers de victimes" qu'un autre - la probabilité de l'événement "Quelque part en Amérique du Nord, il y a une inondation avec des milliers de victimes."  Bien que l'ajout de détails supplémentaires à l'histoire la rende moins probable, cela la rend plus crédible.  Pour moi, comprendre ce fait, c'est comme un « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pont d'âne</a> » pour un futurisme sérieux - la différence entre le fait que vous pesez soigneusement chaque hypothèse individuelle et découvrez si vous pouvez soutenir cette clarification indépendamment de toutes les autres et que vous composez simplement une merveilleuse et une histoire vibrante. <br><br>  C'est tout ce que je dis dans le contexte de la réponse à la question: «Pourquoi ajoutez-vous un raffinement comme un cyborg à cela?  Je ne veux pas être un cyborg. "  Il est nécessaire de tisser soigneusement des détails supplémentaires aux déclarations. <br><br>  <b>X:</b> Avez-vous une chance d'immortalité? <br><br>  <b>Yu:</b> Littéralement?  L'immortalité littérale est difficile à réaliser.  Pour vivre beaucoup plus longtemps que quelques milliers de milliards d'années, vous devez reconsidérer le sort attendu d'un univers en expansion.  Pour vivre plus longtemps que les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">googolpleks</a> , il est nécessaire que nous nous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">trompions</a> sur les fondements des lois physiques, et pas seulement dans les détails. <br><br>  Même si certains des raisonnements inhabituels s'avèrent vrais et que notre univers peut générer des univers filles, cela ne nous donnera pas l'immortalité.  Afin de vivre beaucoup plus d'années de Googleplex et de ne pas vous répéter, vous aurez besoin d'ordinateurs avec plus d'éléments que Google, et une telle machine ne rentrera pas dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sphère Hubble</a> . <br><br>  Et googolpleks n'est pas l'infini.  Pour paraphraser Martin Gardner, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le nombre de Graham</a> est encore assez petit, car la plupart des chiffres finaux sont beaucoup plus grands que lui.  Si vous voulez être époustouflé, lisez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les hiérarchies à croissance rapide</a> et l'infini sera encore plus long.  Seules les théories anthropiques très étranges et effrayantes vous permettront de vivre assez longtemps pour regarder un arrêt de la machine Turing la plus ancienne avec des centaines d'états. <br><br>  Cependant, je ne pense pas que d'un point de vue émotionnel, j'aimerais vivre assez longtemps pour voir le centième numéro du jeu " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chasser un travailleur acharné</a> ".  Je peux en quelque sorte faire preuve d'empathie envers moi-même, qui a vécu une centaine d'années à partir de maintenant.  Cet avenir, je serai en mesure de comprendre moi-même l'avenir dans cent ans.  Et peut-être que quelque part dans cette séquence, il y aura quelqu'un qui risque de mettre fin à son existence, et il peut être très triste à ce sujet.  Mais je ne suis pas sûr de pouvoir imaginer cette personne.  «Je veux vivre un autre jour.  Demain, je veux aussi vivre un autre jour.  Par conséquent, je veux vivre pour toujours, prouvé par l'induction d'entiers positifs. "  Même mon désir d'une longue vie dans un univers physiquement possible est une abstraction générée par induction.  Je ne peux pas m'imaginer dans un billion d'années. <br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> J'ai décrit la singularité comme une fantaisie évasive et pseudoscientifique qui nous distrait du changement climatique, des guerres, des inégalités et d'autres problèmes graves. Pourquoi ai-je tort? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Parce que vous essayez de prédire des faits empiriques par la psychanalyse. Ça ne marchera jamais. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Supposons que nous vivions pour voir l'avènement de l'IA suffisamment intelligente pour qu'elle fasse le même travail d'amélioration de l'IA que les gens. Il peut s'ajuster, programmer, inventer de nouveaux algorithmes. Pour s'améliorer. Que se passera-t-il ensuite - il deviendra plus intelligent, verra encore plus d'opportunités d'amélioration et atteindra rapidement un niveau très élevé? Ou ne se passera-t-il rien de spécial?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il peut arriver que (A) l'auto-amélioration sur un certain delta rende l'IA assez intelligente pour qu'elle puisse regarder en arrière et trouver une nouvelle amélioration potentielle de la taille de k * delta, où k&gt; 1, et cela sera répété plusieurs fois pour conduire à une auto-amélioration rapide pour niveau de superintelligence. Ce </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">qu'Irving John Goode a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> appelé «l'explosion du renseignement». Ou (B), k est inférieur à l'unité ou toutes ces améliorations sont petites et ne conduisent pas à l'apparition de superintelligence, ou la superintelligence est généralement impossible, et au lieu d'une explosion il y aura un zilch. Qu'est-ce qui est vrai, A ou B? Si vous construisez une IA d'un certain niveau et qu'il essaie de le faire, quelque chose se passera dans le monde réel empirique, et cet événement sera déterminé par des faits concernant le paysage des algorithmes et des améliorations réalisables.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Des informations fiables sur cet événement ne peuvent être obtenues de la psychanalyse des personnes. C'est comme essayer de démarrer une voiture sans carburant - c'est ce que le théorème de Bayes nous dit. Certaines personnes seront toujours des évadés, quelles que soient les valeurs réelles des variables cachées en informatique, donc l'observation de certains évadés ne peut pas être qualifiée de preuve rigoureuse. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C'est une idée fausse sur la nature de la rationalité - qu'il est rationnel de croire que "les gobelins dans les placards n'existent pas" parce que la foi en les gobelins d'un placard est stupide, immature, obsolète, et seuls les idiots y croient. Le vrai principe de rationalité est d'aller vérifier dans le placard. Donc, dans ces univers où les gobelins vivent dans des placards, vous croirez aux gobelins, et dans les univers où les gobelins seront dans des placards, vous n'y croirez pas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C'est difficile, mais en principe, il est possible d'essayer de regarder par la porte entrouverte et de demander: "Qu'est-ce qui serait différent dans l'Univers s'il n'était pas possible d'obtenir un bon revenu des investissements cognitifs, c'est-à-dire qu'une IA essayant de s'améliorer se terminerait non pas par une explosion, mais par un zilch?" Quels autres faits seraient caractéristiques d'un tel univers? » </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il y a des gens qui prétendent que l'IA ne peut être élevée qu'au niveau d'une personne, car nous sommes nous-mêmes des gens, et nous ne pouvons pas l'augmenter plus haut. Il me semble que si notre univers est tel, alors nous devrions observer une diminution des revenus des investissements dans le matériel et les logiciels pour les échecs informatiques qui dépasse le niveau d'une personne - ce qui en fait ne se produit pas. De plus, la sélection naturelle ne serait pas en mesure de créer une personne à ce moment-là, et la mère d'Einstein aurait dû être une physicienne incroyable, etc. etc.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il y a des gens qui soutiennent que plus l'algorithme est complexe, plus il a besoin d'ajustements et que notre intelligence sert de limitation à ce processus. Mais cela ne correspond pas aux archives anthropologiques de l'intelligence humaine; les investissements dans le réglage et les mutations du cerveau offrent des capacités cognitives améliorées. Nous le savons, car la génétique nous dit que les mutations avec une petite réponse statistique ne sont pas fixes au cours de l'évolution. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et les hominidés n'avaient pas besoin d'un cerveau exponentiellement plus grand que les chimpanzés. Et la tête de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John von Neumann n'était</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pas exponentiellement plus grande que la tête de la personne moyenne.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">D'un point de vue purement pratique, les axones humains transmettent des informations à une vitesse d'un million de fois inférieure à la vitesse de la lumière, et même du point de vue de la dissipation thermique, chaque opération synaptique consomme un million de fois plus que la dissipation thermique minimale d'une opération binaire irréversible à 300 Kelvin, et ainsi de suite. Pourquoi devrions-nous supposer que le logiciel du cerveau est plus proche de l'optimum que le fer? Le privilège de l'intelligence humaine est qu'il s'agit du plus petit niveau d'intelligence capable de créer un ordinateur. S'il était possible de créer un ordinateur avec un niveau d'intelligence inférieur, nous en discuterions à un niveau inférieur.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais ce n'est pas un argument simple, et pour une description détaillée, j'envoie des gens à l'un de mes anciens travaux, «La microéconomie de l'explosion de l'intelligence», qui, malheureusement, est toujours la meilleure source d'information. Mais ce sont précisément ces questions qui doivent être posées afin d'utiliser les preuves disponibles pour discuter si nous verrons une explosion de l'IA dans laquelle une certaine amélioration des capacités cognitives investies dans l'auto-optimisation donnera une augmentation supérieure à cette amélioration. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quant aux opportunités et à leurs prix: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vous pouvez imaginer un monde sans explosion d'intelligence et sans superintelligence. Ou un monde où les astuces que les experts en apprentissage automatique utiliseront pour contrôler la super-IA conviennent au contrôle des personnes et du régime surhumain. Ou un monde où </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">l'internalisme moral</font></a><font style="vertical-align: inherit;"> fonctionne</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, donc toutes les IA assez avancées sont bonnes. Dans de tels mondes, personne n'a besoin de tout le travail et de tous les soucis du Machine Learning Research Institute. Et plusieurs moustiquaires ont été gaspillées, et il valait mieux les remettre au fonds de lutte contre le paludisme. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez également imaginer un monde dans lequel vous luttez contre le paludisme, combattez et maintenez les émissions de carbone au niveau approprié, ou utilisez des solutions de géo-ingénierie pour neutraliser les erreurs déjà commises. Et tout cela s'avère inutile, car la civilisation est incapable de résoudre le problème de la moralité de l'IA - et tous les enfants sauvés du paludisme à l'aide de filets ne grandissent que pour que les nanomachines les tuent dans un rêve.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je pense que les gens qui essaient de s'engager dans une organisation caritative raisonnable conviendront que nous n'aimerions pas vivre dans aucun de ces mondes. La seule question est laquelle est la plus probable. Le principe central de la rationalité n'est pas de rejeter la croyance aux gobelins, parce qu'elle est stupide et non prestigieuse, et de ne pas croire aux gobelins, car elle est saine et belle. Le principe central de la rationalité est de savoir quels signes observables et conclusions logiques nous aideront à choisir l'un de ces deux mondes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je pense que le premier monde est peu probable, et le second est probable. Je comprends qu'essayer de convaincre les autres de cela, c'est nager contre le flux de la foi dans la normalité éternelle. Croire que seule notre civilisation à court terme, qui existe depuis plusieurs décennies, et que notre espèce, qui n’existe qu’un instant aux échelles évolutives et géologiques, ont un sens et doivent exister pour toujours. Et même si je crois que le premier monde n'est qu'un rêve optimiste, je ne pense pas que nous devions ignorer le problème, dont nous paniquerons à l'avenir. L'Institut a pour mission de mener aujourd'hui des recherches qui, selon les personnes vivant après 30 ans, auraient dû commencer il y a 30 ans. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Votre femme Brijena croit-elle à la singularité?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brijena: Si quelqu'un me demandait si je crois en une singularité, je lèverais un sourcil et lui demanderais s'il croit aux camions automatiques. C'est une étrange question. Je ne sais pas quelle sera la première flotte de camions automatiques, ni combien de temps il leur faudra pour remplacer le système de transport de marchandises existant. Et je ne crois pas aux camions robotiques, je prédis avec confiance que le transport sans pilote remplacera le transport moderne avec la participation des gens, parce que nous allons dans cette direction si rien de vraiment étrange ne se produit. Pour la même raison, je prédis avec confiance une explosion d'intelligence. Dans d'autres sens du mot «singularité», je n'en suis pas si sûr. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Brijena a donné sa réponse sans voir mes réponses. C'est juste que nous sommes bien adaptés les uns aux autres. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Est-il possible de créer une superintelligence sans comprendre comment fonctionne le cerveau? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dans le même sens que vous pouvez faire des avions sans comprendre comment un oiseau vole. Vous n'avez pas besoin d'être un expert des oiseaux, mais en même temps, vous avez besoin de beaucoup de connaissances pour construire un avion, après avoir obtenu ce qui, en principe, vous pouvez déjà comprendre comment approximativement un oiseau plane ou repousse de l'air. Par conséquent, j'écris sur la rationalité humaine - si vous allez assez loin dans la question de l'intelligence artificielle, vous ne pouvez pas vous empêcher de penser à quelques idées sur la façon dont les gens pensent. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Que pourrait vouloir la superintelligence? Auront-ils quelque chose comme le désir sexuel? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pensez à un vaste espace de possibilités, à une sphère multidimensionnelle géante. C'est un espace de types d'esprit, un ensemble de tous les algorithmes cognitifs possibles. Imaginez que quelque part au bas de la sphère se trouve un tout petit point désignant toutes les personnes qui ont déjà vécu. C'est un petit point, car toutes les personnes ont à peu près le même cerveau, avec cortex, cervelet, thalamus, etc. Certaines personnes ne sont pas comme les autres, il peut donc s'agir d'un point pointu, mais les pointes seront à la même échelle que le point lui-même. Quelle que soit votre neuroatypicalité, vous ne travaillerez pas sur un autre algorithme cortical.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Demander «ce que veut la superintelligence» n'est pas la bonne question. Les superintelligences ne sont pas une étrange tribu de gens qui vivent de l'autre côté de la rivière et ont des coutumes exotiques. L'IA est simplement le nom de tout l'espace des possibilités en dehors d'un petit point humain. Ayant des connaissances suffisantes, vous pouvez grimper dans cet espace d'opportunités et en sortir une IA qui a des désirs qui peuvent être décrits en langage humain par Wishlist, mais pas parce que ce sera la Wishlist naturelle de ces surhumains exotiques, mais parce que vous avez isolé une partie de l'espace des types d'esprit .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En ce qui concerne les désirs sexuels - si vous savez exactement ce que vous faites, vous avez résolu le problème principal de la construction de l'IA, voulant de manière stable certaines choses alors qu'elle s'améliore, si vous avez résolu le problème principal de diriger les fonctions utilitaires de l'IA vers des tâches qui semblent trompeusement simples pour une personne, et un problème encore plus compliqué est la construction de l'IA en utilisant un certain type d'architecture, dans laquelle des choses comme les «désirs sexuels» et le «bonheur du sexe» comptent, alors, peut-être, vous pouvez faire en sorte que l'IA regarde les gens qui modélisent être leurs désirs, en extraire cette partie en ce qui concerne le désir sexuel, et faire en sorte que l'IA en fasse l'expérience. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien sûr, vous pouvez également, avec une bonne connaissance de la biologie organique et de l'aérodynamique, construire des avions pouvant s'accoupler avec des oiseaux.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais je ne pense pas que les frères Wright auraient dû faire de telles choses à l'aube de leurs activités. Cela n'aurait aucun sens. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il semble plus raisonnable de résoudre le problème de pénétrer l'espace des esprits et d'en extraire une IA qui ne veut pas nous démonter en atomes de rechange. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Je veux penser que les créatures extra-intelligentes professeront la non-violence, car elles comprendront que la violence est stupide. Suis-je naïf </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Je pense que oui. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">David hume</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je vous dirais que vous faites une erreur typique en appliquant le prédicat de «stupidité» aux valeurs ou aux opérations officielles d'un individu. Les actions, les choix, les règles peuvent être stupides si vous avez des préférences sur l'état final du monde. Si vous êtes une personne avec des méta-préférences que vous n'avez pas entièrement calculées, vous pouvez avoir une plate-forme sur laquelle vous pouvez compter et appeler certaines spéculations sur les préférences d'objet "stupides". </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maximiseur d'agrafes [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">expérience de pensée démontrant comment l'IA produite sans intention malveillante peut nuire à l'humanité - env.</font></font></i>  <i>perev.</i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] ne fait pas d'erreur de calcul, en choisissant les cas parmi ceux dans lesquels le nombre maximum d'agrafes est obtenu. Ce n'est pas à l'intérieur de votre plateforme de préférences, en choisissant des actions erronées, et ce n'est pas à l'intérieur de votre plateforme de méta-préférences, en choisissant par erreur les préférences. Il calcule la réponse à une autre question, et non à celle que vous vous posez, la question "Que dois-je faire?" Le maximiseur d'agrafes prend simplement l'action menant au plus d'agrafes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un scénario fatal est lorsque l'IA ne vous aime ni ne vous déteste, car vous êtes fait d'atomes qu'elle peut utiliser pour créer autre chose. Théorie des jeux et problèmes de coopération dans </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">le dilemme du prisonnier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ne se manifestent pas dans tous les cas possibles. Par exemple, ils n'apparaissent pas lorsqu'un certain sujet est tellement plus fort que vous qu'il peut vous transformer en atomes lorsque vous voulez cliquer sur les boutons «coopérer» ou «changer». Et lorsque nous franchissons ce seuil, soit vous avez résolu le problème de créer quelque chose qui ne veut pas vous nuire, soit vous avez déjà perdu. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La superintelligence </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">résoudra-t-elle le difficile problème de la conscience</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Oui, et avec le recul, la réponse nous semblera honteusement simple. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X: Les</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> superintelligences auront-elles le libre arbitre? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Oui, mais ils n'auront pas l'illusion du libre arbitre. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X: A</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> quoi ressemble votre utopie? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Je dirigerai vos lecteurs vers les miens "</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Séquences de la théorie du divertissement,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "car je n'ai pas encore réussi à écrire une histoire dont l'action se déroule dans un monde optimal divertissant et théorique.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr404137/">https://habr.com/ru/post/fr404137/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr404125/index.html">Le compte à rebours est terminé: il reste 7 jours avant l'ICO Polybe</a></li>
<li><a href="../fr404127/index.html">Le premier lancement de l'Electron LV a été partiellement réussi</a></li>
<li><a href="../fr404129/index.html">Happy Geeks Day (oui, il est aujourd'hui)</a></li>
<li><a href="../fr404133/index.html">D'ici la fin de l'année, Google prévoit de mettre en service un ordinateur quantique à 49 qubits</a></li>
<li><a href="../fr404135/index.html">Google collecte et analyse les achats hors ligne des utilisateurs d'Android Pay</a></li>
<li><a href="../fr404139/index.html">Bitcoin en Russie: taxe (quelques questions simples)</a></li>
<li><a href="../fr404141/index.html">Concurrence déloyale avec le fournisseur</a></li>
<li><a href="../fr404143/index.html">Tiny Nuggets: un examen des bureaux d'enregistrement et du tube TrendVision russes</a></li>
<li><a href="../fr404147/index.html">Le son, vous n'êtes plus que "l'espace": les écouteurs Campfire Audio Andromeda</a></li>
<li><a href="../fr404149/index.html">Test du lecteur étanche PocketBook 641 Aqua 2 nouvelle génération</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>