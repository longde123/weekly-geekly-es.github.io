<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõÄüèø üôÜüèæ üè∫ Restaura√ß√£o de fotos com base em IA üë©üèΩ‚Äçüé§ üí´ üõÄüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal! Sou engenheiro de pesquisa da equipe de vis√£o computacional do Grupo Mail.ru. Neste artigo, vou contar uma hist√≥ria de como criamos um pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Restaura√ß√£o de fotos com base em IA</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/459696/"><img src="https://habrastorage.org/webt/ya/mt/mm/yamtmmcino7skf3gyqzpsrgqla4.jpeg"><br><br>  Ol√° pessoal!  Sou engenheiro de pesquisa da equipe de vis√£o computacional do Grupo Mail.ru.  Neste artigo, vou contar uma hist√≥ria de como criamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um projeto de restaura√ß√£o de fotos baseado em IA</a> para fotos antigas de militares.  O que √© "restaura√ß√£o de fotos"?  Consiste em tr√™s etapas: <br><br><ul><li>  encontramos todos os defeitos da imagem: fraturas, arranh√µes, buracos; <br></li><li>  pintamos os defeitos descobertos, com base nos valores de pixel ao seu redor; <br></li><li>  n√≥s colorimos a imagem. <br></li></ul><br>  Al√©m disso, descreverei todas as etapas da restaura√ß√£o de fotos e mostrarei como obtivemos nossos dados, quais redes treinamos, o que realizamos e que erros cometemos. <br><a name="habracut"></a><br><h1>  Procurando defeitos </h1><br>  Queremos encontrar todos os pixels relacionados a defeitos em uma foto enviada.  Primeiro, precisamos descobrir que tipo de imagem as pessoas enviar√£o.  Conversamos com os fundadores do projeto "Immortal Regiment", uma organiza√ß√£o n√£o comercial que armazena as fotos herdadas da Segunda Guerra Mundial, que compartilharam seus dados conosco.  Ao analis√°-lo, percebemos que as pessoas enviam principalmente retratos individuais ou de grupo com um n√∫mero moderado a grande de defeitos. <br><br>  Ent√£o tivemos que coletar um conjunto de treinamento.  O conjunto de treinamento para uma tarefa de segmenta√ß√£o √© uma imagem e uma m√°scara em que todos os defeitos s√£o marcados.  A maneira mais f√°cil de fazer isso √© permitir que os avaliadores criem as m√°scaras de segmenta√ß√£o.  Obviamente, as pessoas sabem muito bem como encontrar defeitos, mas isso levaria muito tempo. <br><br><img src="https://habrastorage.org/webt/yg/6y/iu/yg6yiue75v7msnxyffapttyugs8.jpeg"><br><br>  Pode levar uma hora ou o dia inteiro para marcar os pixels com defeito em uma foto.  Portanto, n√£o √© f√°cil coletar um conjunto de treinamento de mais de 100 imagens em poucas semanas.  √â por isso que tentamos aumentar nossos dados e criar nossos pr√≥prios defeitos: tiramos uma boa foto, adicionamos defeitos usando passeios aleat√≥rios na imagem e terminamos com uma m√°scara mostrando as partes da imagem com os defeitos.  Sem aprimoramentos, temos 68 fotos rotuladas manualmente no conjunto de treinamento e 11 fotos no conjunto de valida√ß√£o. <br><br>  A abordagem de segmenta√ß√£o mais popular: use o Unet com codificador pr√©-treinado e minimize a soma de BCE ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entropia cruzada bin√°ria</a> ) e DICE ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">S√∏rensen - coeficiente de dados</a> ). <br><br>  Quais problemas surgem quando usamos essa abordagem de segmenta√ß√£o para nossa tarefa? <br><br><ul><li>  Mesmo que pare√ßa que h√° muitos defeitos na foto, que √© muito antiga e surrada, a √°rea com defeitos ainda √© muito menor que a n√£o danificada.  Para resolver esse problema, podemos aumentar o peso positivo da classe no AEC;  um peso ideal seria a propor√ß√£o de pixels limpos para os defeituosos. <br></li><li>  O segundo problema √© que, se usarmos um Unet pronto para uso com codificador pr√©-treinado (Albunet-18, por exemplo), perderemos muitos dados posicionais.  A primeira camada do Albunet-18 consiste em uma convolu√ß√£o com um n√∫cleo 5 e um passo que √© igual a dois.  Permite que a rede trabalhe rapidamente.  Negociamos o tempo de opera√ß√£o l√≠quido para obter uma localiza√ß√£o melhor dos defeitos: removemos o pool m√°ximo ap√≥s a primeira camada, diminu√≠mos o passo para 1 e diminu√≠mos o kernel de convolu√ß√£o para 3. <br></li><li>  Se trabalharmos com imagens pequenas compactando-as, por exemplo, para 256 x 256 ou 512 x 512 pixels, pequenos defeitos desaparecer√£o devido √† interpola√ß√£o.  Portanto, precisamos trabalhar com imagens maiores.  No momento, estamos segmentando defeitos em fotos com tamanho de 1024 x 1024 em produ√ß√£o.  √â por isso que tivemos que treinar a rede em grandes culturas de imagens.  No entanto, isso causa problemas com um tamanho de lote pequeno em uma √∫nica GPU. <br></li><li>  Durante o treinamento, podemos caber cerca de 20 imagens em uma GPU.  Por isso, acabamos com valores imprecisos de m√©dia e desvio padr√£o nas camadas BatchNorm.  Podemos resolver esse problema usando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">In-place BatchNorm</a> , que, por um lado, economiza espa√ßo na mem√≥ria e, por outro, possui uma vers√£o Synchronized BatchNorm, que sincroniza as estat√≠sticas de todas as GPUs.  Agora calculamos os valores de m√©dia e desvio padr√£o n√£o para 20 imagens em uma √∫nica GPU, mas para 80 imagens de 4 GPUs.  Isso melhora a converg√™ncia l√≠quida. <br></li></ul><br>  Finalmente, ao aumentar o peso do BCE, alterar a arquitetura e usar o BatchNorm no local, melhoramos a segmenta√ß√£o.  No entanto, n√£o custaria muito para fazer algo ainda melhor adicionando o aumento do tempo de teste.  Podemos executar a rede uma vez em uma imagem de entrada, espelh√°-la e executar novamente a rede para encontrar todos os pequenos defeitos. <br><br><img src="https://habrastorage.org/webt/3c/vj/g0/3cvjg04qc_nqsl8lop44jvtjfym.jpeg"><br><br>  A rede converge em 18 horas em quatro GeForce 1080Ti.  A infer√™ncia leva 290 ms.  √â bastante longo, mas esse √© o pre√ßo do nosso desempenho melhor que o padr√£o.  A valida√ß√£o DICE √© igual a 0,35 e ROCAUC - 0,93. <br><br><h1>  Pintura de imagem </h1><br>  O mesmo com a tarefa de segmenta√ß√£o que usamos Unet.  Para fazer a pintura, carregamos uma imagem original e uma m√°scara em que marcamos toda a √°rea limpa com uma e com zeros - todos os pixels que queremos pintar.  √â assim que est√°vamos coletando dados: para qualquer foto de um conjunto de dados de imagens de c√≥digo-fonte aberto, por exemplo, o OpenImagesV4, adicionamos defeitos semelhantes aos que vemos na vida real.  Depois treinamos a rede para restaurar as pe√ßas que faltavam. <br><br>  Como podemos modificar o Unet para esta tarefa? <br><br>  Podemos usar convolu√ß√£o parcial em vez de uma original.  A id√©ia √© que, quando envolvemos uma √°rea com algum kernel, n√£o levamos em considera√ß√£o os valores dos pixels de defeito.  Isso torna a pintura mais precisa.  Mostramos um exemplo do recente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo da NVIDIA</a> .  Eles usaram o Unet com uma convolu√ß√£o bidimensional padr√£o na imagem do meio e uma convolu√ß√£o parcial - na imagem √† direita. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ec1/5ba/bdb/ec15babdbf1cd219be4a5e3ffa4ae50f.jpg"><br><br>  Treinamos a rede por cinco dias.  No √∫ltimo dia, congelamos o BatchNorms para tornar as bordas da parte pintada menos vis√≠veis. <br><br>  S√£o necess√°rios 50 ms l√≠quidos para processar uma imagem de 512 x 512.  Valida√ß√£o PSNR √© igual a 26.4.  No entanto, voc√™ n√£o pode confiar totalmente nas m√©tricas nesta tarefa.  Para escolher o melhor modelo, executamos v√°rios bons modelos em imagens de avalia√ß√£o, anonimizamos os resultados e votamos nos que mais gostamos.  Foi assim que escolhemos nosso modelo final. <br><br>  Mencionei anteriormente que adicionamos artificialmente alguns defeitos √†s imagens limpas.  Voc√™ deve sempre acompanhar o tamanho m√°ximo de defeitos adicionados durante o treinamento;  em um caso em que voc√™ alimenta uma imagem com um defeito muito grande na rede, que nunca √© tratada na fase de treinamento, a rede fica solta e produz um resultado inaplic√°vel.  Portanto, se voc√™ precisar corrigir grandes defeitos, aumente seu conjunto de treinamento com eles. <br><br>  Aqui est√° o exemplo de como nosso algoritmo funciona: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c48/2cd/253/c482cd253865ee12a834475a2e30d619.jpg"><br><br><h1>  Coloriza√ß√£o </h1><br>  N√≥s segmentamos os defeitos e os pintamos;  o terceiro passo - reconstru√ß√£o de cores.  Como eu disse antes, existem muitos retratos individuais e em grupo entre as fotos do Regimento Imortal.  Quer√≠amos que nossa rede funcionasse bem com eles.  Decidimos criar nossa pr√≥pria colora√ß√£o, pois nenhum dos servi√ßos existentes conseguia colorir os retratos com rapidez e efici√™ncia.  Queremos que nossas fotos coloridas sejam mais confi√°veis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cec/b9a/b6c/cecb9ab6c8e1b76b567f49eac1261957.jpg"><br><br>  O GitHub possui um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">reposit√≥rio</a> popular para coloriza√ß√£o de fotos.  Faz um bom trabalho, mas ainda tem alguns problemas.  Por exemplo, tende a pintar as roupas de azul.  Por isso tamb√©m a rejeitamos. <br><br>  Ent√£o, decidimos criar um algoritmo para a colora√ß√£o da imagem.  A id√©ia mais √≥bvia: tire uma imagem em preto e branco e preveja tr√™s canais: vermelho, verde e azul.  No entanto, podemos facilitar nosso trabalho: trabalhe n√£o com a representa√ß√£o de cores RGB, mas com a representa√ß√£o de cores YCbCr.  O componente Y √© o brilho (luma).  Uma imagem em preto e branco carregada √© o canal Y e vamos reutiliz√°-la.  Agora precisamos prever Cb e Cr: Cb √© a diferen√ßa de cor e brilho azul e Cr - a diferen√ßa de cor e brilho vermelho. <br><br><img src="https://habrastorage.org/webt/yo/au/zi/yoauzi06k3bd0uyod2rjnpxgvms.jpeg"><br><br>  Por que escolhemos a representa√ß√£o YCbCr?  Um olho humano √© mais sens√≠vel √†s mudan√ßas de brilho do que √†s mudan√ßas de cor.  √â por isso que reutilizamos o componente Y (brilho) com o qual o olho humano √© mais sens√≠vel e prevemos Cb e Cr com os quais podemos cometer um erro, pois n√£o podemos perceber muito bem a falsidade das cores.  Essa caracter√≠stica espec√≠fica foi amplamente usada no in√≠cio da televis√£o em cores quando a capacidade do canal n√£o era suficiente para transmitir todas as cores.  A imagem foi transmitida em YCbCr, inalterada para o componente Y, e Cb e Cr foram reduzidos pela metade. <br><br><h1>  Como criar uma linha de base </h1><br>  Podemos usar o Unet com um codificador pr√©-treinado e minimizar a perda de L1 entre os valores de CbCr existentes e os previstos.  Queremos pintar retratos e, portanto, al√©m das fotos do OpenImages, precisamos de mais fotos espec√≠ficas da tarefa. <br><br>  Onde podemos obter fotos coloridas de pessoas vestidas com uniforme militar?  Existem pessoas na internet que colorem fotos antigas por hobby ou por um pre√ßo.  Eles fazem isso com muito cuidado, tentando ser muito preciso.  Quando colorem um uniforme, ombreiras e medalhas, eles se referem aos materiais de arquivo, para que os resultados de seu trabalho sejam confi√°veis.  No total, usamos 200 fotos coloridas manualmente com pessoas de uniforme militar. <br><br>  A outra fonte de dados √∫til √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o</a> site do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ex√©rcito Vermelho dos Trabalhadores e Camponeses</a> .  Um de seus fundadores teve sua foto tirada em praticamente todos os uniformes sovi√©ticos da Segunda Guerra Mundial dispon√≠veis. <br><br><img src="https://habrastorage.org/webt/yh/b7/u7/yhb7u74fa3feihqo0k-jpqcyxgk.jpeg"><br><br>  Em algumas fotos, ele imitou as poses de pessoas das famosas fotos de arquivo.  √â bom que suas fotos tenham fundo branco: isso nos permitiu aumentar muito bem os dados adicionando v√°rios objetos naturais ao fundo.  Tamb√©m usamos alguns retratos regulares, complementando-os com ins√≠gnias e outros atributos de tempo de guerra. <br><br>  Treinamos o AlbuNet-50 - √© um Unet que usa o ResNet-50 pr√©-treinado como um codificador.  A rede come√ßou a dar resultados adequados: a pele era rosada, os olhos - cinza-azulados, as ombreiras - amarelados.  No entanto, o problema √© que ele deixa algumas √°reas na foto intocadas.  Isso foi causado pelo fato de que, de acordo com o erro L1, √© t√£o ideal que √© melhor n√£o fazer nada do que tentar prever alguma cor. <br><br><img src="https://habrastorage.org/webt/ov/zh/bn/ovzhbnv-6ch0nnoa4fdbh3nygym.jpeg"><br>  <i>Estamos comparando nosso resultado com uma foto Ground Truth - uma colora√ß√£o manual feita por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Klimbim</a></i> <br><br>  Como podemos resolver esse problema?  Precisamos de um discriminador: uma rede neural que receba uma imagem e nos diga se ela parece realista ou n√£o.  Uma das figuras abaixo √© colorida manualmente e a outra - pelo nosso gerador, AlbuNet-50.  Como o ser humano distingue fotos coloridas manual e automaticamente?  Ao olhar para os detalhes.  Voc√™ pode dizer onde est√° a foto colorida automaticamente por nossa solu√ß√£o de linha de base? <br><br><img src="https://habrastorage.org/webt/fk/er/n_/fkern_az5kgkgr2kwamcoxr_gtg.jpeg"><br><br><div class="spoiler">  <b class="spoiler_title">Resposta</b> <div class="spoiler_text">  a imagem √† esquerda √© colorida manualmente, √† direita - automaticamente. </div></div><br>  Utilizamos o discriminador do artigo da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GAN de auto-aten√ß√£o</a> .  √â uma pequena rede de convolu√ß√£o com a chamada Auto-Aten√ß√£o criada nas camadas superiores.  Isso nos permite "prestar mais aten√ß√£o" aos detalhes da imagem.  Tamb√©m usamos normaliza√ß√£o espectral.  Voc√™ pode encontrar mais informa√ß√µes no documento acima.  Treinamos a rede com uma combina√ß√£o de perda de L1 e perda do discriminador.  Agora, a rede coloriza melhor os detalhes da imagem e o fundo parece mais consistente.  Mais um exemplo: √† esquerda, o trabalho da rede treinado apenas com perda de L1;  √† direita - com uma combina√ß√£o de perdas discriminadoras de L1. <br><br><img src="https://habrastorage.org/webt/nd/3p/91/nd3p91aw1mzzoidhra1egef3zki.jpeg"><br><br>  O processo de treinamento levou dois dias em quatro GeForce 1080Ti.  S√£o necess√°rios 30 ms para processar uma imagem de 512 x 512.  Valida√ß√£o MSE - 34.4.  Assim como na pintura, m√©tricas que voc√™ n√£o deseja confiar nas m√©tricas.  Por isso, escolhemos seis modelos com as melhores m√©tricas de valida√ß√£o e votamos cegamente no melhor modelo. <br><br>  Quando j√° criamos um sistema de produ√ß√£o e lan√ßamos um site, continuamos experimentando e conclu√≠mos que √© melhor minimizar a perda de L1 por pixel, mas a perda de percep√ß√£o.  Para calcul√°-lo, alimentamos as previs√µes l√≠quidas e uma foto de base verdadeira √† rede VGG-16, pegamos os mapas de recursos nas camadas inferiores e comparamos com o MSE.  Essa abordagem pinta mais √°reas e fornece resultados mais coloridos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/676/9c8/b64/6769c8b64fdf00cb66dcd73edcd39e81.jpg"><br><br><h1>  Recapitular </h1><br>  Unet √© um modelo bem legal.  Na primeira tarefa de segmenta√ß√£o, enfrentamos um problema durante o treinamento e trabalhamos com imagens de alta resolu√ß√£o e √© por isso que usamos o In-Place BatchNorm.  Em nossa segunda tarefa (Inpainting), usamos Convolu√ß√£o Parcial em vez de uma padr√£o, e isso nos permitiu obter melhores resultados.  Ao trabalhar na coloriza√ß√£o, adicionamos uma pequena rede discriminadora que penaliza o gerador por imagens irreais.  Tamb√©m usamos uma perda perceptiva. <br><br>  Segunda conclus√£o - os avaliadores s√£o essenciais.  E n√£o apenas durante o est√°gio de cria√ß√£o de m√°scaras de segmenta√ß√£o, mas tamb√©m para a valida√ß√£o do resultado final.  No final, fornecemos ao usu√°rio tr√™s fotos: uma imagem original com defeitos pintados, uma foto colorida com defeitos pintados e uma simplesmente colorida, caso o algoritmo para pesquisa de defeitos e pintura incorreta. <br><br>  Tiramos algumas fotos do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto War Album</a> e as processamos por essas neuronets.  Aqui est√£o os resultados que obtivemos: <br><br><img src="https://habrastorage.org/webt/rm/4z/sb/rm4zsbvc0j_h_r2nobp4xj2p4ei.jpeg"><br><br>  Al√©m disso, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> voc√™ pode ver as imagens originais e todas as etapas do processamento. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt459696/">https://habr.com/ru/post/pt459696/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt459682/index.html">Qualidade dos dados em armazenamento</a></li>
<li><a href="../pt459684/index.html">Mapa do metr√¥ de Moscou e do mundo inteiro para Android</a></li>
<li><a href="../pt459688/index.html">Urbanismo na China: menos descolados, mais ci√™ncia e TI</a></li>
<li><a href="../pt459692/index.html">Como descobrimos modifica√ß√µes de materiais que contradizem os princ√≠pios qu√≠micos estabelecidos</a></li>
<li><a href="../pt459694/index.html">Museum DataArt. Descompacte e inicie o Radio 86RK</a></li>
<li><a href="../pt459698/index.html">Como for√ßar o Oracle BI 12c a criar quantas vari√°veis ‚Äã‚Äãde sess√£o o programador precisa?</a></li>
<li><a href="../pt459704/index.html">LLVM IR e Go</a></li>
<li><a href="../pt459706/index.html">5 raz√µes pelas quais voc√™ deve esquecer o Redux nos aplicativos React</a></li>
<li><a href="../pt459708/index.html">Design de Interface do Jogo. Brent Fox Sobre o que √© o livro?</a></li>
<li><a href="../pt459710/index.html">Sobreviva a uma colis√£o frontal e por que a amn√©sia n√£o √© o que voc√™ pensa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>