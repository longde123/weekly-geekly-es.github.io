<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💃🏻 🤰🏻 🤜🏽 Rede neural LipNet lê lábios com uma precisão de 93,4% 🧜🏻 🤲🏻 🛐</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O comandante Dave Bowman e o co-piloto Frank Poole, não confiando no computador, decidiram desconectá-lo do controle do navio. Para fazer isso, eles c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Rede neural LipNet lê lábios com uma precisão de 93,4%</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/398901/"><img src="https://habrastorage.org/files/f5b/c98/b66/f5bc98b66e6b4c54a8b456037259caab.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O comandante Dave Bowman e o co-piloto Frank Poole, não confiando no computador, decidiram desconectá-lo do controle do navio. Para fazer isso, eles conferem em uma sala à prova de som, mas o HAL 9000 lê a conversa nos lábios. </font></font><font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Filmado a partir do filme “Odisséia no Espaço de 2001” A</font></font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
leitura labial desempenha um papel importante na comunicação. Mais experimentos em 1976 mostraram que as pessoas “ouvem” </font><font style="vertical-align: inherit;">fonemas </font><font style="vertical-align: inherit;">completamente </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diferentes</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> se você aplicar o som errado ao movimento dos lábios (consulte </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Ouvindo lábios e vendo vozes"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , Nature 264, 746-748, 23 de dezembro de 1976, doi: 10.1038 / 264746a0) .</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Do ponto de vista prático, a leitura labial é uma habilidade importante e útil. Você pode entender o interlocutor sem desligar a música nos fones de ouvido, ler as conversas de todas as pessoas no campo de visão (por exemplo, todos os passageiros na sala de espera), ouvir as pessoas através de binóculos ou telescópio. O escopo da habilidade é muito amplo. Um profissional que a domina encontrará facilmente um emprego bem remunerado. Por exemplo, no campo da segurança ou inteligência competitiva.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Os sistemas automáticos de leitura labial também têm um grande potencial prático. São os aparelhos auditivos médicos de nova geração com reconhecimento de fala, sistemas para palestras silenciosas em locais públicos, identificação biométrica, sistemas para transmissão secreta de informações para espionagem, reconhecimento de fala por vídeo de câmeras de vigilância, etc. No final, os computadores do futuro também lerão lábios, como o </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HAL 9000</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Portanto, os cientistas tentam há muitos anos desenvolver sistemas automáticos de leitura labial, mas sem muito sucesso. Mesmo para o inglês relativamente simples, em que o número de fonemas é muito menor que o russo, a precisão do reconhecimento é baixa.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Compreender a fala com base em expressões faciais humanas é uma tarefa assustadora. As pessoas que dominam essa habilidade tentam reconhecer dezenas de fonemas consoantes, muitos dos quais têm aparência muito semelhante. É especialmente difícil para uma pessoa não treinada distinguir entre </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cinco categorias de fonemas visuais</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (isto é, visemes) do idioma inglês. Em outras palavras, distinguir a pronúncia de algumas consoantes pelos lábios é quase impossível. Não é de surpreender que as pessoas se saiam muito mal com a leitura exata dos lábios. Mesmo os melhores das pessoas com deficiência auditiva demonstram precisão de apenas </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">17 ± 12% de 30 monossílabos ou 21 ± 11% de palavras polissilábicas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (daqui em diante os resultados para o idioma inglês).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A leitura automática dos lábios é uma das tarefas da visão de máquina, que se resume ao processamento quadro a quadro de uma sequência de vídeo. A tarefa é muito complicada pela baixa qualidade da maioria dos materiais de vídeo práticos, que não permitem uma leitura precisa do espaço-temporal, isto é, das características espaço-temporais de uma pessoa durante uma conversa. Os rostos se movem e giram em direções diferentes. Desenvolvimentos recentes no campo da visão de máquina estão tentando rastrear o movimento da face no quadro para resolver esse problema. Apesar dos sucessos, até recentemente, eles eram capazes de reconhecer apenas palavras individuais, mas não sentenças. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Um avanço significativo nessa área foi alcançado pelos desenvolvedores da Universidade de Oxford. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O LipNet que eles treinaram</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tornou-se o primeiro do mundo a reconhecer com êxito os lábios no nível de frases inteiras, processando imagens de vídeo. </font><i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;">Mapas de saliência</font></a></i></font><br>
<br>
<img src="https://habrastorage.org/files/c91/f89/dae/c91f89daeefd4e0da9a55c41073a5285.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> quadro a quadro </font><font style="vertical-align: inherit;">para as palavras em inglês "please" (acima) e "lay" (abaixo) quando processados ​​por uma rede neural que lê lábios, destacando os recursos mais atraentes (destacados) do</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
LipNet - uma rede neural recorrente do tipo LSTM (memória de curto prazo). A arquitetura é mostrada na ilustração. A rede neural foi treinada usando o método Connectionist Temporal Classification (CTC), amplamente utilizado em sistemas modernos de reconhecimento de fala, pois elimina a necessidade de treinamento em um conjunto de dados de entrada sincronizados com o resultado correto.</font></font><br>
<br>
<img src="https://habrastorage.org/files/856/4ac/a35/8564aca35bea4280bce7c669cce37fe2.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arquitetura de rede neural LipNet. </font><font style="vertical-align: inherit;">Na entrada, é fornecida uma sequência de quadros T, que são processados ​​por três camadas da rede neural convolucional espaço-temporal (espaço-temporal) (STCNN), cada uma das quais é acompanhada por uma camada de amostragem espacial. </font><font style="vertical-align: inherit;">Para os recursos extraídos, a taxa de amostragem na linha do tempo (upsampling) é aumentada e eles são processados ​​por LTSM duplo. </font><font style="vertical-align: inherit;">Cada vez que a etapa LTSM é processada por uma rede de distribuição direta de duas camadas e a última camada SoftMax.Em</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
um pacote especial de ofertas GRID, a rede neural mostra uma precisão de reconhecimento de 93,4%. </font><font style="vertical-align: inherit;">Isso não apenas excede a precisão do reconhecimento de outros desenvolvimentos de software (que são indicados na tabela abaixo), mas também excede a eficiência da leitura na boca de pessoas especialmente treinadas.</font></font><br>
<br>
<table>
<tbody><tr>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Método</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Conjunto de dados</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tamanho</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Edição</font></font></th>
<th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Precisão</font></font></th>
</tr>
<tr>
<td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fu et al. </font><font style="vertical-align: inherit;">(2008)</font></font></td>
<td>AVICAR</td>
<td>851</td>
<td></td>
<td>37,9%</td>
</tr>
<tr>
<td>Zhao et al. (2009)</td>
<td>AVLetter</td>
<td>78</td>
<td></td>
<td>43,5%</td>
</tr>
<tr>
<td>Papandreou et al. (2009)</td>
<td>CUAVE</td>
<td>1800</td>
<td></td>
<td>83,0%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016a)</td>
<td>OuluVS1</td>
<td>200</td>
<td></td>
<td>91,4%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016b)</td>
<td>OuluVS2</td>
<td>520</td>
<td></td>
<td>94,1%</td>
</tr>
<tr>
<td>Chung &amp; Zisserman (2016a)</td>
<td>BBC TV</td>
<td>&gt;400000</td>
<td></td>
<td>65,4%</td>
</tr>
<tr>
<td>Wand et al. (2016)</td>
<td>GRID</td>
<td>9000</td>
<td></td>
<td>79,6%</td>
</tr>
<tr>
<td>LipNet</td>
<td>GRID</td>
<td>28853</td>
<td><b></b></td>
<td><b>93,4%</b></td>
</tr>
</tbody></table><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O caso GRID especial é composto de acordo com o seguinte modelo: </font></font><br>
<br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comando (4) + cor (4) + preposição (4) + letra (25) + dígito (10) + advérbio (4), em</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
que o número corresponde ao número de variantes de palavras para cada uma das seis categorias verbais . </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Em outras palavras, a precisão de 93,4% ainda é o resultado obtido em condições de laboratório em estufa. </font><font style="vertical-align: inherit;">Obviamente, com o reconhecimento de discurso humano arbitrário, o resultado será muito pior. </font><font style="vertical-align: inherit;">Sem mencionar a análise de dados de vídeo real, em que o rosto de uma pessoa não é fotografado em close com excelente iluminação e alta resolução. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A operação da rede neural LipNet é mostrada no vídeo de demonstração.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fa5QGremQf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
O artigo científico foi preparado para a conferência ICLR 2017 e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> em 4 de novembro de 2016 em domínio público.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt398901/">https://habr.com/ru/post/pt398901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt398889/index.html">Publicado o Relatório de Teste de Sucesso da NASA EmDrive</a></li>
<li><a href="../pt398893/index.html">Estação base GSM oculta em uma impressora de escritório</a></li>
<li><a href="../pt398895/index.html">Como reprogramai meu cérebro para começar a entender matemática</a></li>
<li><a href="../pt398897/index.html">Yandex lançou o serviço de saúde</a></li>
<li><a href="../pt398899/index.html">Masterkeys Pro L: personalize para todos os campos</a></li>
<li><a href="../pt398903/index.html">A beleza duradoura do cosmos</a></li>
<li><a href="../pt398905/index.html">TP-LINK HS110 - um assistente doméstico ou outra tomada com Wi-Fi?</a></li>
<li><a href="../pt398907/index.html">Música sinfônica - problemas de qualidade de reprodução, escolha de formato e equipamento</a></li>
<li><a href="../pt398911/index.html">Quais são os dispositivos de controle de ressonância: exemplo da marca Cold Ray</a></li>
<li><a href="../pt398913/index.html">Como a Valve tenta ensinar educação aos jogadores</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>