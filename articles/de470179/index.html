<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéöÔ∏è üï∫üèæ üëáüèæ PDDM - Neuer modellbasierter Verst√§rkungslernalgorithmus mit Advanced Scheduler ‚úîÔ∏è üôÜ üçí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforcement Learning ist in zwei gro√üe Klassen unterteilt: modellfrei und modellbasiert. Im ersten Fall werden Aktionen direkt durch das Belohnungss...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PDDM - Neuer modellbasierter Verst√§rkungslernalgorithmus mit Advanced Scheduler</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470179/"><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  Reinforcement Learning ist in zwei gro√üe Klassen unterteilt: modellfrei und modellbasiert.  Im ersten Fall werden Aktionen direkt durch das Belohnungssignal optimiert, und im zweiten Fall ist das neuronale Netzwerk nur ein Modell der Realit√§t, und die optimalen Aktionen werden mithilfe eines externen Schedulers ausgew√§hlt.  Jeder Ansatz hat seine eigenen Vor- und Nachteile. </p><br><p>  Entwickler von Berkeley und Google Brain haben den modellbasierten PDDM-Algorithmus mit einem verbesserten Scheduler eingef√ºhrt, mit dem Sie komplexe Bewegungen mit einer gro√üen Anzahl von Freiheitsgraden in einer kleinen Anzahl von Beispielen effektiv lernen k√∂nnen.  Um zu lernen, wie man B√§lle in einem Roboterarm mit realistischen Fingergelenken mit 24 Freiheitsgraden dreht, brauchte man nur 4 Stunden √úbung mit einem echten physischen Roboter. </p><a name="habracut"></a><br><p>  Reinforcement Learning ist das Training von Robotern mit einem Belohnungssignal.  Dies √§hnelt dem Lernen von Lebewesen.  Das Problem wird jedoch durch die Tatsache kompliziert, dass nicht bekannt ist, wie die Gewichte des neuronalen Netzwerks so ge√§ndert werden k√∂nnen, dass die vorgeschlagenen Ma√ünahmen zu einer Erh√∂hung der Belohnungen f√ºhren.  Daher sind beim Reinforcement Learning herk√∂mmliche Trainingsmethoden f√ºr neuronale Netze nicht geeignet.  Schlie√ülich ist nicht bekannt, was genau sie bei ihrem Ausgang herausgeben soll, was bedeutet, dass es unm√∂glich ist, einen Fehler zwischen ihrer Vorhersage und dem tats√§chlichen Zustand der Dinge zu finden.  Um diesen Unterschied durch die Schichten des neuronalen Netzwerks zur√ºckzuspringen und die Gewichte zwischen den Neuronen zu √§ndern, um diesen Fehler zu minimieren.  Dies ist ein klassischer Back-Propagation-Algorithmus, der von neuronalen Netzen gelehrt wird. </p><br><p>  Daher haben Wissenschaftler verschiedene Wege gefunden, um dieses Problem zu l√∂sen. </p><br><h2 id="model-free">  Modellfrei </h2><br><p>  Einer der effektivsten Ans√§tze war das Schauspieler-Kritiker-Modell.  Lassen Sie ein neuronales Netzwerk (Akteur) an seinem Eingang den Zustand der staatlichen Umgebung empfangen und am Ausgang Aktionen ausgeben, die zu einer Erh√∂hung der Belohnungsbelohnungen f√ºhren sollten.  Bisher sind diese Aktionen zuf√§llig und h√§ngen einfach vom Signalfluss innerhalb des Netzwerks ab, da das neuronale Netzwerk noch nicht trainiert wurde.  Und das zweite neuronale Netzwerk (Kritiker) l√§sst die Eingabe auch den Zustand der Zustandsumgebung empfangen, aber auch Aktionen von der Ausgabe des ersten Netzwerks.  Und lassen Sie am Ausgang nur die vorhergesagte Belohnungsbelohnung zu, die erhalten wird, wenn diese Aktionen angewendet werden. </p><br><p>  Beobachten Sie jetzt Ihre H√§nde: Wir wissen nicht, was die besten Aktionen am Ausgang des ersten Netzwerks sein sollten, was zu einer Erh√∂hung der Belohnung f√ºhrt.  Daher k√∂nnen wir den Back-Propagation-Algorithmus nicht trainieren.  Das zweite neuronale Netzwerk kann jedoch sehr gut den genauen Wert der Belohnungsbelohnung (oder besser gesagt normalerweise deren √Ñnderung) vorhersagen, die es erh√§lt, wenn jetzt Aktionen angewendet werden.  Nehmen wir also den Fehler√§nderungsgradienten aus dem zweiten Netzwerk und wenden ihn auf das erste an!  Sie k√∂nnen also das erste neuronale Netzwerk mit der klassischen Methode der R√ºckausbreitung von Fehlern trainieren.  Wir nehmen den Fehler einfach nicht von den Ausg√§ngen des ersten Netzwerks, sondern von den Ausg√§ngen des zweiten. </p><br><p>  Infolgedessen lernt das erste neuronale Netzwerk, optimale Aktionen auszuf√ºhren, was zu einer Erh√∂hung der Belohnungen f√ºhrt.  Denn wenn der Kritiker einen Fehler gemacht und eine geringere Belohnung vorhergesagt hat, als es sich in der Realit√§t herausgestellt hat, bewegt der Gradient dieses Unterschieds die Handlungen des Schauspielers in die Richtung, so dass der Kritiker die Belohnung genauer vorhersagt.  Und das bedeutet optimalere Aktionen (schlie√ülich f√ºhren sie dazu, dass der Kritiker eine h√∂here Auszeichnung genau vorhersagt).  Ein √§hnliches Prinzip funktioniert in die entgegengesetzte Richtung: Wenn der Kritiker die erwartete Belohnung √ºbersch√§tzt, verringert der Unterschied zwischen Erwartung und Realit√§t die Aktionsausgaben des ersten neuronalen Netzwerks, was zu dieser √ºbersch√§tzten Belohnungsanzeige des zweiten Netzwerks f√ºhrte. </p><br><p>  Wie Sie sehen, werden in diesem Fall die Aktionen direkt durch das Belohnungssignal optimiert.  Dies ist die gemeinsame Essenz aller modellfreien Algorithmen beim Reinforcement Learning.  Sie sind derzeit auf dem neuesten Stand der Technik. </p><br><p>  Ihr Vorteil ist, dass durch Gradientenabstieg optimale Aktionen angestrebt werden, so dass am Ende die optimalsten gefunden werden.  Was bedeutet, das beste Ergebnis zu zeigen.  Ein weiterer Vorteil ist die M√∂glichkeit, kleine (und damit schneller lernbare) neuronale Netze zu verwenden.  Wenn aus der ganzen Vielzahl von Umweltfaktoren einige spezifische Faktoren zur L√∂sung des Problems entscheidend sind, kann der Gradientenabstieg sie durchaus identifizieren.  Und verwenden Sie, um das Problem zu l√∂sen.  Diese beiden Vorteile haben den Erfolg mit direkten modellfreien Methoden sichergestellt. </p><br><p>  Sie haben aber auch Nachteile.  Da Aktionen direkt durch das Belohnungssignal vermittelt werden, sind viele Trainingsbeispiele erforderlich.  Dutzende Millionen, auch in sehr einfachen F√§llen.  Sie arbeiten schlecht an Aufgaben mit einer gro√üen Anzahl von Freiheitsgraden.  Wenn es dem Algorithmus nicht sofort gelingt, Schl√ºsselfaktoren in der hochdimensionalen Landschaft zu identifizieren, wird er h√∂chstwahrscheinlich √ºberhaupt nicht lernen.  Auch modellfreie Methoden k√∂nnen Schwachstellen im System ausnutzen, indem sie sich auf nicht optimale Aktionen konzentrieren (wenn der Gradientenabstieg darauf konvergiert) und andere Umgebungsfaktoren ignorieren.  Auch f√ºr geringf√ºgig andere modellfreie Aufgaben m√ºssen die Methoden komplett neu trainiert werden. </p><br><h2 id="model-based">  Modellbasiert </h2><br><p>  Die modellbasierten Methoden im Reinforcement Learning unterscheiden sich grundlegend von dem oben beschriebenen Ansatz.  In Model-Based sagt ein neuronales Netzwerk nur voraus, was als n√§chstes passieren wird.  Keine Aktion anbieten.  Das hei√üt, es ist einfach ein Modell der Realit√§t (daher das "Modell" -basierte im Namen).  Und √ºberhaupt kein Entscheidungssystem. </p><br><p>  Modellbasierte neuronale Netze werden mit dem aktuellen Zustand der Zustandsumgebung und den Aktionen, die wir ausf√ºhren m√∂chten, versorgt.  Und das neuronale Netzwerk sagt voraus, wie sich der Zustand in Zukunft √§ndern wird, nachdem diese Aktionen angewendet wurden.  Sie kann auch vorhersagen, welche Belohnung sich aus diesen Aktionen ergibt.  Dies ist jedoch nicht erforderlich, da die Belohnung normalerweise aus einem bekannten Zustand berechnet werden kann.  Ferner kann dieser Ausgangszustand (zusammen mit neuen vorgeschlagenen Aktionen) auf den Eingang des neuronalen Netzwerks zur√ºckgef√ºhrt werden und so √Ñnderungen in der externen Umgebung viele Schritte vorw√§rts rekursiv vorhersagen. </p><br><p>  Modellbasierte neuronale Netze sind sehr einfach zu erlernen.  Da sie einfach vorhersagen, wie sich die Welt ver√§ndern wird, ohne Vorschl√§ge zu machen, welche optimalen Ma√ünahmen ergriffen werden sollten, damit sich die Belohnung erh√∂ht.  Daher verwendet das modellbasierte neuronale Netzwerk alle vorhandenen Beispiele f√ºr sein Training und nicht nur diejenigen, die zu einer Erh√∂hung oder Verringerung der Belohnungen f√ºhren, wie dies bei modellfrei der Fall ist.  Dies ist der Grund, warum modellbasierte neuronale Netze weitaus weniger Trainingsbeispiele ben√∂tigen. </p><br><p>  Der einzige Nachteil besteht darin, dass das modellbasierte neuronale Netzwerk die tats√§chliche Dynamik des Systems untersuchen und daher √ºber ausreichende Kapazit√§ten verf√ºgen sollte.  Ein modellfreies neuronales Netzwerk kann auf Schl√ºsselfaktoren konvergieren, den Rest ignorieren und daher ein kleines einfaches Netzwerk sein (wenn die Aufgabe im Prinzip durch weniger Ressourcen gel√∂st wird). </p><br><p>  Ein weiterer gro√üer Vorteil ist neben dem Training an einer kleineren Anzahl von Beispielen, dass als universelles Modell der Welt ein einziges modellbasiertes neuronales Netzwerk verwendet werden kann, um eine beliebige Anzahl von Problemen in dieser Welt zu l√∂sen. </p><br><p>  Das Hauptproblem des modellbasierten Ansatzes besteht darin, welche Aktionen Aktionen auf die Eingabe neuronaler Netze angewendet werden sollten.  Schlie√ülich bietet das neuronale Netzwerk selbst keine optimalen Aktionen. </p><br><p>  Der einfachste Weg ist, durch ein solches neuronales Netzwerk Zehntausende von zuf√§lligen Aktionen zu fahren und diejenigen auszuw√§hlen, f√ºr die das neuronale Netzwerk die gr√∂√üte Belohnung vorhersagt.  Dies ist ein klassisches modellbasiertes Reinforcement-Lernen.  Bei gro√üen Dimensionen und langen Zeitketten ist die Anzahl der m√∂glichen Aktionen jedoch zu gro√ü, um sie alle zu sortieren (oder zumindest ein wenig optimal zu erraten). </p><br><p>  Aus diesem Grund sind modellbasierte Methoden in der Regel modellfreien Methoden unterlegen, die durch Gradientenabstieg direkt zu den optimalsten Aktionen konvergieren. </p><br><p>  Eine verbesserte Version f√ºr Bewegungen in der Robotik besteht darin, keine zuf√§lligen Aktionen zu verwenden, sondern die vorherige Bewegung beizubehalten und der Normalverteilung Zuf√§lligkeit hinzuzuf√ºgen.  Da die Bewegungen von Robotern normalerweise reibungslos sind, wird die Anzahl der B√ºsten verringert.  Gleichzeitig kann eine wichtige scharfe Ver√§nderung √ºbersehen werden. </p><br><p>  Die endg√ºltige Entwicklungsoption f√ºr diesen Ansatz kann als CEM-Option betrachtet werden, bei der keine feste Normalverteilung verwendet wird, die Zuf√§lligkeit in den aktuellen Aktionspfad einf√ºhrt, sondern die Parameter der Zufallsverteilung mithilfe der Kreuzentropie ausgew√§hlt werden.  Zu diesem Zweck wird eine Population von Aktionsberechnungen gestartet, von denen die besten verwendet werden, um die Verteilung der Parameter in der n√§chsten Generation zu verfeinern.  So etwas wie ein evolution√§rer Algorithmus. </p><br><h2 id="pddm">  PDDM </h2><br><p>  Eine so lange Einf√ºhrung war erforderlich, um zu erkl√§ren, was in dem neuen vorgeschlagenen modellbasierten PDDM-Verst√§rkungslernalgorithmus geschieht.  Nach dem Lesen eines Artikels im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkeley AI-Blog</a> (oder einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erweiterten Version</a> ) und sogar des Originalartikels von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv.org/abs/1909.11652</a> war dies m√∂glicherweise nicht offensichtlich. </p><br><p>  Die PDDM-Methode wiederholt die Idee von CEM, wenn zuf√§llige Aktionen ausgew√§hlt werden, die √ºber ein modellbasiertes neuronales Netzwerk ausgef√ºhrt werden m√ºssen, um Aktionen mit der h√∂chsten vorhersehbaren Belohnung auszuw√§hlen.  Nur anstatt zuf√§llige Verteilungsparameter auszuw√§hlen, wie dies in CEM der Fall ist, verwendet PDDM eine zeitliche Korrelation zwischen Aktionen und eine weichere Regel zum Aktualisieren der zuf√§lligen Verteilung.  Die Formel ist im Originalartikel angegeben.  Auf diese Weise k√∂nnen Sie eine gr√∂√üere Anzahl geeigneter Aktionen √ºber gro√üe Zeitdistanzen √ºberpr√ºfen, insbesondere wenn Bewegungen eine pr√§zise Koordination erfordern.  Zus√§tzlich filtern die Autoren des Algorithmus Kandidaten f√ºr Aktionen, wodurch eine glattere Bewegungsbahn erhalten wird. </p><br><p>  Einfach ausgedr√ºckt, schlugen die Entwickler einfach eine bessere Formel f√ºr die Auswahl von zuf√§lligen Aktionen vor, die im klassischen modellbasierten Reinforcement-Lernen getestet werden sollen. </p><br><p>  Das Ergebnis war aber sehr gut. </p><br><p>  In nur 4 Stunden Training an einem echten Roboter lernte ein Roboter mit 24 Freiheitsgraden, zwei B√§lle zu halten und sie in den Handfl√§chen zu drehen, ohne sie fallen zu lassen.  Ein unerreichbares Ergebnis f√ºr alle modernen modellfreien Methoden mit einer so geringen Anzahl von Beispielen. </p><br><p>  Interessanterweise verwendeten sie f√ºr das Training einen zweiten Roboterarm mit 7 Freiheitsgraden, der die heruntergefallenen B√§lle aufnahm und zum Hauptroboterarm zur√ºckbrachte: </p><br><p><img src="https://habrastorage.org/webt/7s/yf/wq/7syfwqr2vg2-rlzupbccu-06mic.gif"></p><br><p>  Infolgedessen konnte die Roboruk nach 1-2 Stunden die B√§lle sicher halten und in ihrer Handfl√§che bewegen, und 4 Stunden reichten f√ºr ein vollst√§ndiges Training aus. </p><br><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  Achten Sie auf die zuckenden Bewegungen der Finger.  Dies ist ein Merkmal modellbasierter Ans√§tze.  Da die beabsichtigten Aktionen zuf√§llig ausgew√§hlt werden, stimmen sie nicht immer mit den optimalen √ºberein.  Der modellfreie Algorithmus k√∂nnte m√∂glicherweise zu wirklich optimalen reibungslosen Bewegungen konvergieren. </p><br><p>  Der modellbasierte Ansatz erm√∂glicht es jedoch, mit einem trainierten neuronalen Netzwerk, das die Welt modelliert, verschiedene Probleme zu l√∂sen, ohne sie neu zu trainieren.  Der Artikel enth√§lt mehrere Beispiele. Sie k√∂nnen beispielsweise die Drehrichtung der Kugeln in der Hand leicht √§ndern (in Model-Free m√ºssten Sie das neuronale Netzwerk daf√ºr neu trainieren).  Oder halten Sie den Ball an einer bestimmten Stelle in Ihrer Handfl√§che und folgen Sie dem roten Punkt. </p><br><p><img src="https://habrastorage.org/webt/np/2a/4v/np2a4vibrlvwohr_kimp7w4o_1u.gif"></p><br><p>  Sie k√∂nnen Roboruk auch dazu bringen, beliebige Flugbahnen mit einem Bleistift zu zeichnen. Dies ist f√ºr modellfreie Methoden eine sehr schwierige Aufgabe. </p><br><p><img src="https://habrastorage.org/webt/iz/a_/v5/iza_v5a2jkohqpcl8vb99_yfwri.gif"></p><br><p>  Obwohl der vorgeschlagene Algorithmus kein Allheilmittel und nicht einmal ein KI-Algorithmus im wahrsten Sinne des Wortes ist (in PDDM ersetzt das neuronale Netzwerk einfach das analytische Modell, und Entscheidungen werden durch zuf√§llige Suche mit einer kniffligen Regel getroffen, die die Anzahl der Aufz√§hlungen von Optionen verringert), kann er in der Robotik n√ºtzlich sein.  Da es eine sp√ºrbare Verbesserung der Ergebnisse zeigte und an einer sehr kleinen Anzahl von Beispielen trainiert wird. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470179/">https://habr.com/ru/post/de470179/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470167/index.html">Konferenz f√ºr Wissenschaftsinteressierte, bevor sie zum Mainstream wurde</a></li>
<li><a href="../de470169/index.html">Wie man verhindert, dass die Idee stirbt und ein Team zusammenstellt, das sie nicht umbringt</a></li>
<li><a href="../de470171/index.html">Habr Weekly # 21 / Dobroshrift, Technodom f√ºr eine Katze, das Recht, Haushaltsger√§te zu reparieren, die Europ√§ische Union und ‚Äûtransparente‚Äú Cookies</a></li>
<li><a href="../de470173/index.html">Integrationsplattform als Service</a></li>
<li><a href="../de470175/index.html">F√ºgen Sie Anmelden mit Apple zum Back-End hinzu</a></li>
<li><a href="../de470181/index.html">Wie die Levenberg-Marquardt-Methode funktioniert</a></li>
<li><a href="../de470187/index.html">Die Preisspanne f√ºr das Design und die Gestaltung eines Onlinedienstes liegt zwischen 100.000 und 5 Millionen Rubel. Gr√ºnde</a></li>
<li><a href="../de470189/index.html">Senden von Peer-to-Peer-Nachrichten mit PeerJS</a></li>
<li><a href="../de470191/index.html">Web Probleml√∂sung mit r0ot-mi. Teil 1</a></li>
<li><a href="../de470193/index.html">Universeller Schutz gegen xss-Angriffe und SQL-Injektionen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>