<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèΩ ü§¥üèø üë®üèø‚Äçüé§ Soluci√≥n hiperconvergente AERODISK vAIR. Base: sistema de archivos ARDFS ü§∏üèΩ üëãüèº üí©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola lectores de Habr. Con este art√≠culo, abrimos un ciclo que hablar√° sobre el sistema hiperconvergente AERODISK vAIR que desarrollamos. Inicialmente...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Soluci√≥n hiperconvergente AERODISK vAIR. Base: sistema de archivos ARDFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/469383/"><p><img src="https://habrastorage.org/webt/sk/n3/zh/skn3zhr0fgbcuqfar5ozlg0v2tk.jpeg"></p><br><p>  Hola lectores de Habr.  Con este art√≠culo, abrimos un ciclo que hablar√° sobre el sistema hiperconvergente AERODISK vAIR que desarrollamos.  Inicialmente, quer√≠amos que el primer art√≠culo contara todo sobre todo, pero el sistema es bastante complicado, por lo que comeremos un elefante en partes. </p><br><p>  Comencemos la historia con la historia del sistema, profundicemos en el sistema de archivos ARDFS, que es la base de vAIR, y tambi√©n hablemos un poco sobre el posicionamiento de esta soluci√≥n en el mercado ruso. </p><br><p>  En futuros art√≠culos, hablaremos m√°s sobre los diferentes componentes arquitect√≥nicos (cl√∫ster, hipervisor, equilibrador de carga, sistema de monitoreo, etc.), el proceso de configuraci√≥n, plantearemos problemas de licencia, mostraremos por separado las pruebas de choque y, por supuesto, escribiremos sobre las pruebas de carga y dimensionamiento  Tambi√©n dedicaremos un art√≠culo separado a la versi√≥n comunitaria de vAIR. </p><a name="habracut"></a><br><h2 id="aerodisk---eto-vrode-istoriya-pro-shd-ili-zachem-my-voobsche-nachali-zanimatsya-giperkonvergentom">  ¬øEs un disco de aire una historia sobre almacenamiento?  ¬øO por qu√© empezamos a hiperconvergir? </h2><br><p>  Inicialmente, la idea de crear nuestro propio hiperconvergente surgi√≥ en alg√∫n lugar alrededor del a√±o 2010.  Entonces no hab√≠a Aerodisk, ni soluciones similares (sistemas comerciales hiperconvergentes en caja) en el mercado.  Nuestra tarea era la siguiente: desde un conjunto de servidores con discos locales conectados por una interconexi√≥n a trav√©s de Ethernet, tuvimos que hacer un almacenamiento extendido y ejecutar m√°quinas virtuales y una red de software en el mismo lugar.  Se requer√≠a implementar todo esto sin sistemas de almacenamiento (porque simplemente no hab√≠a dinero para el almacenamiento y su vinculaci√≥n, pero a√∫n no hab√≠amos inventado nuestro propio sistema de almacenamiento). </p><br><p>  Probamos muchas soluciones de c√≥digo abierto y, sin embargo, resolvimos este problema, pero la soluci√≥n era muy complicada y dif√≠cil de repetir.  Adem√°s, esta decisi√≥n fue de la categor√≠a de "¬øObras?  ¬°No lo toques! "  Por lo tanto, despu√©s de resolver ese problema, no desarrollamos m√°s la idea de convertir el resultado de nuestro trabajo en un producto completo. </p><br><p> Despu√©s de ese incidente, nos alejamos de esta idea, pero a√∫n ten√≠amos la sensaci√≥n de que esta tarea era completamente solucionable, y los beneficios de tal soluci√≥n eran m√°s que obvios.  Posteriormente, los productos de HCI de compa√±√≠as extranjeras que se lanzaron solo confirmaron este sentimiento. </p><br><p>  Por lo tanto, a mediados de 2016, volvimos a esta tarea como parte de la creaci√≥n de un producto completo.  Entonces todav√≠a no ten√≠amos ninguna relaci√≥n con los inversores, por lo que tuvimos que comprar un puesto de desarrollo por nuestro poco dinero.  Despu√©s de escribir en los servidores y conmutadores Avito BU-shyh, nos pusimos a trabajar. </p><br><p><img src="https://habrastorage.org/webt/wy/ir/jr/wyirjr50guvzpnolcvdvniyn2mo.jpeg"></p><br><p>  La tarea inicial principal era crear su propio, aunque simple, pero su propio sistema de archivos, que ser√≠a capaz de distribuir datos de forma autom√°tica y uniforme en forma de bloques virtuales en el en√©simo n√∫mero de nodos del cl√∫ster que est√°n interconectados a trav√©s de Ethernet.  En este caso, el FS debe escalarse bien y f√°cilmente e ser independiente de los sistemas adyacentes, es decir.  ser enajenado de vAIR en forma de "solo almacenamiento". </p><br><p><img src="https://habrastorage.org/webt/il/vk/zs/ilvkzsyjkyr6pkcgoqs_ibumyus.jpeg"></p><br><p>  VAIR primer concepto </p><br><p><img src="https://habrastorage.org/webt/h1/e0/pd/h1e0pda3j1ebbxls_cn_gmzm5ug.jpeg"></p><br><p>  Nos negamos intencionalmente a usar soluciones de c√≥digo abierto ya preparadas para organizar el almacenamiento extendido (ceph, gluster, lustre y similares) a favor de nuestro desarrollo, ya que ya ten√≠amos mucha experiencia en proyectos con ellos.  Por supuesto, estas soluciones en s√≠ mismas son maravillosas, y antes de trabajar en Aerodisk, implementamos m√°s de un proyecto de integraci√≥n con ellas.  Pero una cosa es darse cuenta de la tarea espec√≠fica de un cliente, capacitar al personal y, posiblemente, comprar soporte para un gran proveedor, y otra cosa es crear un producto f√°cil de replicar que se utilizar√° para diversas tareas, que nosotros, como proveedor, incluso podemos conocernos a nosotros mismos. No lo haremos.  Para el segundo prop√≥sito, los productos de c√≥digo abierto existentes no nos conven√≠an, as√≠ que decidimos ver el sistema de archivos distribuidos nosotros mismos. <br>  Dos a√±os despu√©s, varios desarrolladores (que combinaron el trabajo en vAIR con el trabajo en el motor de almacenamiento cl√°sico) lograron un cierto resultado. </p><br><p>  Para el a√±o 2018, hab√≠amos escrito el sistema de archivos m√°s simple y lo hab√≠amos complementado con el enlace necesario.  El sistema integr√≥ discos f√≠sicos (locales) de diferentes servidores en un grupo plano a trav√©s de una interconexi√≥n interna y los "cort√≥" en bloques virtuales, luego se crearon dispositivos de bloque con diversos grados de tolerancia a fallas a partir de bloques virtuales, en los que se crearon y ejecutaron hipervisores KVM virtuales. carros </p><br><p>  No nos molestamos con el nombre del sistema de archivos y lo llamamos sucintamente ARDFS (adivina c√≥mo se desencripta) </p><br><p>  Este prototipo se ve√≠a bien (no visualmente, por supuesto, no hab√≠a dise√±o visual en ese momento) y mostr√≥ buenos resultados en rendimiento y escala.  Despu√©s del primer resultado real, establecimos el rumbo para este proyecto, despu√©s de haber organizado un entorno de desarrollo completo y un equipo separado que se dedicaba solo a vAIR. </p><br><p>  Justo en ese momento, la arquitectura general de la soluci√≥n hab√≠a madurado, que hasta ahora no hab√≠a sufrido cambios importantes. </p><br><h2 id="pogruzhaemsya-v-faylovuyu-sistemu-ardfs">  Zambullirse en el sistema de archivos ARDFS </h2><br><p>  ARDFS es la base de vAIR, que proporciona almacenamiento de conmutaci√≥n por error distribuido de todo el cl√∫ster.  Una caracter√≠stica distintiva (pero no la √∫nica) de ARDFS es que no utiliza ning√∫n servidor dedicado adicional para meta y administraci√≥n.  Originalmente, esto ten√≠a la intenci√≥n de simplificar la configuraci√≥n de la soluci√≥n y por su confiabilidad. </p><br><h3 id="struktura-hraneniya">  Estructura de almacenamiento </h3><br><p>  Dentro de todos los nodos del cl√∫ster, ARDFS organiza un grupo l√≥gico de todo el espacio disponible en disco.  Es importante comprender que un grupo a√∫n no son datos ni espacio formateado, sino simplemente marcado, es decir  cualquier nodo con vAIR instalado cuando se agrega al cl√∫ster se agrega autom√°ticamente al grupo ARDFS compartido y los recursos del disco se comparten autom√°ticamente en todo el cl√∫ster (y est√°n disponibles para el almacenamiento de datos en el futuro).  Este enfoque le permite agregar y eliminar nodos sobre la marcha sin ning√∫n impacto grave en un sistema que ya se est√° ejecutando.  Es decir  El sistema es muy f√°cil de escalar con "ladrillos", agregando o eliminando nodos en el cl√∫ster si es necesario. </p><br><p>  Los discos virtuales (objetos de almacenamiento para m√°quinas virtuales) se agregan en la parte superior del grupo ARDFS, que se crean a partir de bloques virtuales de 4 megabytes de tama√±o.  Los discos virtuales almacenan datos directamente.  A nivel de disco virtual, tambi√©n se define un esquema de tolerancia a fallas. </p><br><p>  Como habr√°s adivinado, para la tolerancia a fallas del subsistema de disco, no utilizamos el concepto de RAID (matriz redundante de discos independientes), sino que utilizamos RAIN (matriz redundante de nodos independientes).  Es decir  La tolerancia a fallos se mide, automatiza y gestiona en funci√≥n de nodos, no de discos.  Los discos, por supuesto, tambi√©n son un objeto de almacenamiento, ellos, como todo lo dem√°s, son monitoreados, puede realizar todas las operaciones est√°ndar con ellos, incluida la construcci√≥n de RAID de hardware local, pero el cl√∫ster funciona con nodos. </p><br><p>  En una situaci√≥n en la que realmente desea RAID (por ejemplo, un escenario que admite m√∫ltiples fallas en peque√±os grupos), nada le impide usar controladores RAID locales y hacer un almacenamiento extendido y una arquitectura RAIN en la parte superior.  Este escenario es bastante animado y es compatible con nosotros, por lo que hablaremos de ello en un art√≠culo sobre escenarios t√≠picos para usar vAIR. </p><br><h3 id="shemy-otkazoustoychivosti-hranilischa">  Esquemas de conmutaci√≥n por error de almacenamiento </h3><br><p>  Puede haber dos esquemas de resistencia de disco virtual vAIR: </p><br><p>  1) Factor de replicaci√≥n o simplemente replicaci√≥n: este m√©todo de tolerancia a fallas es simple "como un palo y una cuerda".  Se realiza la replicaci√≥n s√≠ncrona entre nodos con un factor de 2 (2 copias por grupo) o 3 (3 copias, respectivamente).  RF-2 permite que un disco virtual resista una falla de un nodo en un cl√∫ster, pero "come" la mitad del volumen utilizable, y RF-3 resistir√° una falla de 2 nodos en un cl√∫ster, pero reservar√° 2/3 del volumen utilizable para sus necesidades.  Este esquema es muy similar al RAID-1, es decir, un disco virtual configurado en RF-2 es resistente a fallas en cualquier nodo del cl√∫ster.  En este caso, los datos estar√°n bien e incluso la E / S no se detendr√°.  Cuando un nodo ca√≠do vuelve a funcionar, comenzar√° la recuperaci√≥n / sincronizaci√≥n autom√°tica de datos. </p><br><p>  Los siguientes son ejemplos de la distribuci√≥n de datos RF-2 y RF-3 en modo normal y en una situaci√≥n de falla. </p><br><p>  Tenemos una m√°quina virtual con una capacidad de 8 MB de datos √∫nicos (√∫tiles) que se ejecutan en 4 nodos vAIR.  Est√° claro que en realidad es poco probable que haya una cantidad tan peque√±a, pero para un esquema que refleja la l√≥gica de ARDFS, este ejemplo es m√°s comprensible.  AB son bloques virtuales de 4 MB que contienen datos √∫nicos de m√°quinas virtuales.  Con RF-2, se crean dos copias de estos bloques A1 + A2 y B1 + B2, respectivamente.  Estos bloques est√°n "dispuestos" por nodos, evitando la intersecci√≥n de los mismos datos en el mismo nodo, es decir, la copia A1 no estar√° en la misma nota que la copia A2.  Con B1 y B2 es similar. </p><br><p><img src="https://habrastorage.org/webt/ho/xm/oh/hoxmohhemj_whmr38pvgfyfousm.png"></p><br><p>  En caso de falla de uno de los nodos (por ejemplo, el nodo 3, que contiene una copia de B1), esta copia se activa autom√°ticamente en el nodo donde no hay copia de su copia (es decir, copia B2). </p><br><p><img src="https://habrastorage.org/webt/ex/xl/o4/exxlo4frqr3crhwbh_orvlcwl9g.png"></p><br><p>  Por lo tanto, el disco virtual (y las m√°quinas virtuales, respectivamente) sobrevivir√°n f√°cilmente a la falla de un nodo en el esquema RF-2. </p><br><p>  Un circuito con replicaci√≥n, con su simplicidad y confiabilidad, sufre el mismo dolor que RAID1: hay poco espacio utilizable. </p><br><p>  2) La codificaci√≥n de borrado o codificaci√≥n de borrado (tambi√©n conocida como "codificaci√≥n redundante", "codificaci√≥n de borrado" o "c√≥digo de redundancia") simplemente existe para resolver el problema anterior.  EC es un esquema de redundancia que proporciona alta disponibilidad de datos con menos sobrecarga de disco en comparaci√≥n con la replicaci√≥n.  El principio de funcionamiento de este mecanismo es similar al RAID 5, 6, 6P. </p><br><p>  Al codificar, el proceso de EC divide el bloque virtual (4 MB por defecto) en varias "piezas de datos" m√°s peque√±as dependiendo del esquema de EC (por ejemplo, un esquema de 2 + 1 divide cada bloque de 4 MB en 2 piezas de 2 MB).  Adem√°s, este proceso genera "fragmentos de paridad" para "datos" de no m√°s de una de las partes previamente separadas.  Al decodificar, el EC genera las piezas faltantes, leyendo los datos "sobrevivientes" en todo el cl√∫ster. </p><br><p>  Por ejemplo, un disco virtual con un esquema EC 2 + 1, implementado en 4 nodos de un cl√∫ster, puede resistir f√°cilmente una falla de un solo nodo en un cl√∫ster de la misma manera que RF-2.  Al mismo tiempo, los costos generales ser√°n m√°s bajos, en particular, el factor de capacidad con RF-2 es 2, y con EC 2 + 1 ser√° 1.5. </p><br><p>  Si es m√°s sencillo de describir, la conclusi√≥n es que el bloque virtual se divide en 2-8 (por qu√© del 2 al 8, ver m√°s abajo) "piezas", y para estas piezas se calculan las "piezas" de paridad del mismo volumen. </p><br><p>  Como resultado, los datos y la paridad se distribuyen uniformemente en todos los nodos del cl√∫ster.  Al mismo tiempo, al igual que con la replicaci√≥n, ARDFS distribuye autom√°ticamente los datos entre los nodos de manera que se evite el almacenamiento de los mismos datos (copias de datos y su paridad) en un nodo para eliminar la posibilidad de perder datos debido al hecho de que los datos y sus la paridad terminar√° repentinamente en el mismo nodo de almacenamiento, lo que fallar√°. </p><br><p>  A continuaci√≥n se muestra un ejemplo, con la misma m√°quina virtual con 8 MB y 4 nodos, pero ya con el esquema EC 2 + 1. </p><br><p>  Los bloques A y B se dividen en dos partes de 2 MB cada una (dos porque 2 + 1), es decir, A1 + A2 y B1 + B2.  A diferencia de la r√©plica, A1 no es una copia de A2, es un bloque virtual A, dividido en dos partes, tambi√©n con el bloque B. En total, obtenemos dos conjuntos de 4 MB, cada uno de los cuales contiene dos piezas de dos megabytes.  Adem√°s, para cada uno de estos conjuntos, la paridad se calcula con un volumen de no m√°s de una pieza (es decir, 2 MB), obtenemos + 2 piezas de paridad adicionales (AP y BP).  Total tenemos datos 4x2 + paridad 2x2. </p><br><p>  A continuaci√≥n, los nodos "presentan" las piezas para que los datos no se superpongan con su paridad.  Es decir  A1 y A2 no estar√°n en el mismo nodo con AP. </p><br><p><img src="https://habrastorage.org/webt/s9/xi/om/s9xiombqm4ep-bos8lntf8kjq4s.png"></p><br><p>  En el caso de una falla de un nodo (por ejemplo, tambi√©n el tercero), el bloque B1 ca√≠do se restaurar√° autom√°ticamente desde la paridad BP, que est√° almacenada en el nodo No. 2, y se activar√° en el nodo donde no hay paridad B, es decir.  piezas de BP.  En este ejemplo, este es el nodo # 1 </p><br><p><img src="https://habrastorage.org/webt/wu/gk/dr/wugkdrhgfund89fswagb7wc4iba.png"></p><br><p>  Estoy seguro de que el lector tiene una pregunta: </p><br><blockquote>  "Todo lo que describi√≥ ha sido implementado por los competidores y las soluciones de c√≥digo abierto, ¬øcu√°l es la diferencia entre su implementaci√≥n de EC en ARDFS?" </blockquote><p>  Y luego habr√° caracter√≠sticas interesantes del trabajo de ARDFS. </p><br><h3 id="erasure-coding-s-uporom-na-gibkost">  Codificaci√≥n de borrado con √©nfasis en flexibilidad </h3><br><p>  Inicialmente, proporcionamos un esquema EC X + Y bastante flexible, donde X es igual a un n√∫mero del 2 al 8 e Y es igual a un n√∫mero del 1 al 8, pero siempre menor o igual que X. Tal esquema se proporciona por flexibilidad.  Aumentar la cantidad de datos (X) en los que se divide la unidad virtual permite reducir la sobrecarga, es decir, aumentar el espacio utilizable. <br>  Un aumento en el n√∫mero de fragmentos de paridad (Y) aumenta la confiabilidad del disco virtual.  Cuanto mayor sea el valor Y, m√°s nodos en el cl√∫ster pueden fallar.  Por supuesto, aumentar la cantidad de paridad reduce la cantidad de capacidad utilizable, pero esto es un cargo por la confiabilidad. </p><br><p>  La dependencia del rendimiento en los circuitos EC es casi directa: cuanto m√°s "piezas", menor es el rendimiento, aqu√≠, por supuesto, necesita un aspecto equilibrado. </p><br><p>  Este enfoque permite a los administradores la forma m√°s flexible de configurar el almacenamiento extendido.  Dentro del grupo ARDFS, puede usar cualquier esquema de tolerancia a fallas y sus combinaciones, lo que tambi√©n es, en nuestra opini√≥n, muy √∫til. </p><br><p>  La siguiente tabla compara varios (no todos los posibles) circuitos de RF y EC. </p><br><p><img src="https://habrastorage.org/webt/g0/vj/oj/g0vjojekzkzjv2xbxbdhyw1fjy0.png"></p><br><p>  La tabla muestra que incluso la combinaci√≥n "terry" de EC 8 + 7, que permite perder hasta 7 nodos simult√°neamente en un cl√∫ster, "come" menos espacio utilizable (1.875 versus 2) que la replicaci√≥n est√°ndar y protege 7 veces mejor, lo que hace que este mecanismo de protecci√≥n, aunque sea m√°s complejo, pero mucho m√°s atractivo en situaciones en las que necesita garantizar la m√°xima fiabilidad en las condiciones de falta de espacio en disco.  Al mismo tiempo, debe comprender que cada "m√°s" para X o Y ser√° una sobrecarga adicional para la productividad, por lo que debe elegir con mucho cuidado en el tri√°ngulo entre confiabilidad, econom√≠a y rendimiento.  Por esta raz√≥n, dedicaremos un art√≠culo separado a la codificaci√≥n de eliminaci√≥n de tama√±o. </p><br><p><img src="https://habrastorage.org/webt/5v/2o/-j/5v2o-jpchgy8rqib7bugk35vmj4.png"></p><br><h3 id="nadezhnost-i-avtonomnost-faylovoy-sistemy">  Fiabilidad y autonom√≠a del sistema de archivos. </h3><br><p>  ARDFS se ejecuta localmente en todos los nodos del cl√∫ster y los sincroniza por sus propios medios a trav√©s de interfaces Ethernet dedicadas.  Un punto importante es que ARDFS sincroniza independientemente no solo los datos, sino tambi√©n los metadatos relacionados con el almacenamiento.  Mientras trabaj√°bamos en ARDFS, estudiamos simult√°neamente una serie de soluciones existentes y descubrimos que muchas realizan la meta sincronizaci√≥n del sistema de archivos usando un DBMS distribuido externo, que tambi√©n usamos para la sincronizaci√≥n, pero solo configuraciones, no metadatos FS (sobre este y otros subsistemas relacionados) en el proximo articulo). </p><br><p>  La sincronizaci√≥n de metadatos de FS usando un DBMS externo es, por supuesto, una soluci√≥n de trabajo, pero la consistencia de los datos almacenados en ARDFS depender√≠a del DBMS externo y su comportamiento (y ella, francamente, es una mujer caprichosa), lo cual es malo en nuestra opini√≥n.  Por qu√©  Si los metadatos de FS est√°n da√±ados, los datos de FS en s√≠ tambi√©n se pueden decir "adi√≥s", por lo que decidimos seguir un camino m√°s complicado pero confiable. </p><br><p>  Creamos el subsistema de sincronizaci√≥n de metadatos para ARDFS de forma independiente, y vive completamente independiente de los subsistemas adyacentes.  Es decir  Ning√∫n otro subsistema puede corromper los datos ARDFS.  En nuestra opini√≥n, esta es la forma m√°s confiable y correcta, y es realmente as√≠: el tiempo lo dir√°.  Adem√°s, con este enfoque, aparece una ventaja adicional.  ARDFS puede usarse independientemente de vAIR, al igual que el almacenamiento extendido, que sin duda utilizaremos en productos futuros. </p><br><p>  Como resultado, despu√©s de desarrollar ARDFS, obtuvimos un sistema de archivos flexible y confiable que le permite elegir d√≥nde puede ahorrar en capacidad o ceder todo en rendimiento, o hacer que el almacenamiento sea altamente confiable por una tarifa moderada, pero reduciendo los requisitos de rendimiento. </p><br><p>  Junto con una pol√≠tica de licencias simple y un modelo de entrega flexible (mirando hacia el futuro, est√° autorizado por vAIR por nodos y se entrega por software o como PAC), esto le permite adaptar con precisi√≥n la soluci√≥n a los requisitos m√°s diferentes de los clientes y en el futuro es f√°cil mantener este equilibrio. </p><br><h2 id="komu-eto-chudo-nuzhno">  ¬øQui√©n necesita este milagro? </h2><br><p>  Por un lado, podemos decir que ya hay jugadores en el mercado que tienen decisiones serias en el campo de la hiperconvergencia y hacia d√≥nde vamos realmente.  Esta afirmaci√≥n parece ser cierta, PERO ... </p><br><p>  Por otro lado, al salir al campo y comunicarnos con los clientes, nosotros y nuestros socios vemos que este no es el caso.  Hay muchas tareas para el hiperconvergente, en alg√∫n lugar la gente simplemente no sab√≠a que exist√≠an tales soluciones, en alg√∫n lugar parec√≠a costoso, en alg√∫n lugar hab√≠a pruebas fallidas de soluciones alternativas, pero en alg√∫n lugar generalmente prohib√≠an comprar, debido a las sanciones.  En general, el campo no estaba arado, as√≠ que fuimos a criar las tierras v√≠rgenes))). </p><br><h3 id="kogda-shd-luchshe-chem-gks">  ¬øCu√°ndo es mejor el almacenamiento que GCS? </h3><br><p>  En el curso de trabajar con el mercado, a menudo se nos pregunta cu√°ndo es mejor usar el esquema cl√°sico con sistemas de almacenamiento y cu√°ndo es hiperconvergente.  Muchas empresas, fabricantes de GCS (especialmente aquellas que no tienen almacenamiento en su cartera) dicen: "¬°El almacenamiento ha sobrevivido, solo hiperconvergente!"  Esta es una declaraci√≥n audaz, pero no refleja la realidad. </p><br><p>  En verdad, el mercado de almacenamiento, de hecho, nada hacia soluciones hiperconvergentes y similares, pero siempre hay un "pero". </p><br><p>  En primer lugar, los centros de datos y las infraestructuras de TI construidas de acuerdo con el esquema cl√°sico con sistemas de almacenamiento no se pueden reconstruir f√°cilmente de esta manera, por lo que la modernizaci√≥n y finalizaci√≥n de tales infraestructuras sigue siendo un legado de 5-7 a√±os. </p><br><p>  En segundo lugar, las infraestructuras que se est√°n construyendo ahora en su mayor parte (es decir, la Federaci√≥n de Rusia) se est√°n construyendo de acuerdo con el esquema cl√°sico utilizando sistemas de almacenamiento y no porque las personas no conozcan el hiperconvergente, sino porque el mercado hiperconvergente es nuevo, a√∫n no se han establecido soluciones y est√°ndares , Los empleados de TI a√∫n no han recibido capacitaci√≥n, hay poca experiencia y necesitamos construir centros de datos aqu√≠ y ahora.  Y esta tendencia es por otros 3-5 a√±os (y luego otro legado, ver p√°rrafo 1). </p><br><p>  En tercer lugar, una limitaci√≥n puramente t√©cnica en peque√±os retrasos adicionales de 2 milisegundos por escritura (excluyendo el cach√© local, por supuesto), que son tarifas por almacenamiento distribuido. </p><br><p>  Bueno, no nos olvidemos de usar servidores f√≠sicos grandes que aman la escala vertical del subsistema de disco. </p><br><p>  Hay muchas tareas necesarias y populares en las que el sistema de almacenamiento se comporta mejor que el GCS.  Aqu√≠, por supuesto, aquellos fabricantes que no tienen sistemas de almacenamiento en su cartera de productos estar√°n en desacuerdo con nosotros, pero estamos listos para discutir razonablemente.  Por supuesto, nosotros, como desarrolladores de ambos productos en una de las futuras publicaciones, definitivamente haremos una comparaci√≥n de los sistemas de almacenamiento y GCS, donde demostraremos claramente qu√© es mejor en qu√© condiciones. </p><br><h3 id="a-gde-giperkonvergentnye-resheniya-budut-rabotat-luchshe-shd">  ¬øY d√≥nde funcionar√°n mejor las soluciones hiperconvergentes que los sistemas de almacenamiento? </h3><br><p>  Basado en las tesis anteriores, hay tres conclusiones obvias: </p><br><ol><li>  Cuando otros 2 milisegundos de demoras de grabaci√≥n que se producen de manera estable en cualquier producto (ahora no estamos hablando de sint√©ticos, puede mostrar nanosegundos en sint√©ticos) no son cr√≠ticos, el hiperconvergente funcionar√°. </li><li>  Donde la carga de grandes servidores f√≠sicos puede convertirse en muchos servidores virtuales peque√±os y distribuirse por nodos, el hiperconvergente tambi√©n funcionar√° bien all√≠. </li><li>  Donde la escala horizontal es m√°s importante que la escala vertical, GCS tambi√©n funcionar√° bien all√≠. </li></ol><br><h3 id="kakie-eto-resheniya">  ¬øCu√°les son estas soluciones? </h3><br><ol><li>  Todos los servicios de infraestructura est√°ndar (servicio de directorio, correo, EDS, servidores de archivos, sistemas ERP y BI peque√±os o medianos, etc.).  Llamamos a esto "computaci√≥n general". </li><li>  La infraestructura de los proveedores de la nube, donde es necesario expandir y estandarizar r√°pidamente horizontalmente y "cortar" f√°cilmente una gran cantidad de m√°quinas virtuales para los clientes. </li><li>  Infraestructura de escritorios virtuales (VDI), donde muchos usuarios peque√±os virtuala se lanzan y "flotan" silenciosamente dentro de un cl√∫ster uniforme. </li><li>  ,       , ,       15-20  . </li><li>    (big data-, ). ,     ¬´¬ª,  ¬´¬ª. </li><li>  ,     ,    ,   . </li></ol><br><p>          AERODISK vAIR       ( ). ,   , ..     . </p><br><h3 id="itak"> ‚Ä¶ </h3><br><p>        ,           . </p><br><p>   ,    . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/469383/">https://habr.com/ru/post/469383/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../469371/index.html">Descripci√≥n del enfoque para organizar y probar c√≥digo usando Redux Thunk</a></li>
<li><a href="../469373/index.html">Los resultados del proyecto para crear una interfaz neuronal para pacientes completamente paralizados han cuestionado</a></li>
<li><a href="../469375/index.html">¬øPor qu√© Mozilla, Coil y Creative Commons asignan $ 100 millones para proyectos de c√≥digo abierto?</a></li>
<li><a href="../469379/index.html">Aplicaci√≥n de m√©todos formales de validaci√≥n de modelos para IU</a></li>
<li><a href="../469381/index.html">Agones, crea un servidor de juegos multiusuario. Arquitectura e instalaci√≥n</a></li>
<li><a href="../469387/index.html">La historia de un "desarrollador" o c√≥mo un reci√©n llegado a escribir una aplicaci√≥n para iOS</a></li>
<li><a href="../469389/index.html">Parametrizaci√≥n por una red neuronal de un modelo f√≠sico para resolver un problema de optimizaci√≥n topol√≥gica</a></li>
<li><a href="../469391/index.html">Interfaces de audio: el sonido como fuente de informaci√≥n en la carretera, en la oficina y en el cielo</a></li>
<li><a href="../469393/index.html">Resumen de Flare-On 2019</a></li>
<li><a href="../469395/index.html">D√≥nde y c√≥mo usar multicolumnas (columnas CSS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>