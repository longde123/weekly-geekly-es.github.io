<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçó üë©‚Äçüé§ ‚úåüèæ Superintelligenz: eine Idee, die kluge Leute verfolgt üëåüèæ üèÆ üë®‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Interpretation der Rede auf der Web Camp Zagreb Konferenz Maciej Tseglovsky, ein amerikanischer Webentwickler, Unternehmer, Redner und Sozialkritiker ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Superintelligenz: eine Idee, die kluge Leute verfolgt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/432806/"><img src="https://habrastorage.org/getpro/habr/post_images/6e3/91b/4f1/6e391b4f1772fd49d6c836bc87ffd343.jpg"><br><br>  <i>Interpretation der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rede auf der</a> Web Camp Zagreb <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konferenz</a> Maciej Tseglovsky, ein amerikanischer Webentwickler, Unternehmer, Redner und Sozialkritiker polnischer Herkunft.</i> <br><br>  Als sich amerikanische Physiker 1945 darauf vorbereiteten, die Atombombe zu testen, kam jemand auf die Frage, ob ein solcher Test die Atmosph√§re entz√ºnden k√∂nnte. <br><br>  Die Angst war berechtigt.  Der Stickstoff, der den gr√∂√üten Teil der Atmosph√§re ausmacht, ist energetisch instabil.  Wenn die beiden Atome stark genug kollidieren, verwandeln sie sich in ein Magnesiumatom, ein Alpha-Teilchen, und setzen enorme Energie frei: <br><br>  N <sup>14</sup> + N <sup>14</sup> ‚áí Mg <sup>24</sup> + Œ± + 17,7 MeV <br><br>  Eine wichtige Frage war, ob diese Reaktion sich selbst tragen k√∂nnte.  Die Temperatur in der Kugel einer nuklearen Explosion sollte alles √ºbersteigen, was einst auf der Erde beobachtet wurde.  K√∂nnte es sein, dass wir ein Streichholz in einen Haufen trockener Bl√§tter werfen? <br><a name="habracut"></a><br>  Physiker aus Los Alamos f√ºhrten eine Analyse durch und stellten fest, dass die Sicherheitsmarge zufriedenstellend war.  Da wir heute alle zur Konferenz gekommen sind, wissen wir, dass sie Recht hatten.  Sie waren zuversichtlich in ihre Vorhersagen, da die Gesetze f√ºr Kernreaktionen unkompliziert und bekannt waren. <br><br>  Heute schaffen wir eine andere Technologie, die die Welt ver√§ndert - die Maschinenintelligenz.  Wir wissen, dass dies enorme Auswirkungen auf die Welt haben, die Funktionsweise der Wirtschaft ver√§ndern und den unvorhersehbaren Dominoeffekt ausl√∂sen wird. <br><br>  Es besteht aber auch das Risiko einer unkontrollierbaren Reaktion, bei der die KI schnell genug das menschliche Intelligenzniveau erreicht und √ºbertrifft.  Und in diesem Moment werden uns soziale und wirtschaftliche Probleme am wenigsten beunruhigen.  Jede ultra-intelligente Maschine hat ihre eigenen Hyperziele und arbeitet daran, diese zu erreichen, indem sie Menschen manipuliert oder einfach ihren K√∂rper als bequeme Quelle f√ºr Ressourcen verwendet. <br><br>  Im vergangenen Jahr ver√∂ffentlichte der Philosoph Nick Bostrom das Buch Superintelligence, in dem er die alarmistische Sichtweise der KI beschrieb und zu beweisen versuchte, dass eine solche Explosion der Intelligenz sowohl gef√§hrlich als auch unvermeidlich ist, wenn man sich auf einige moderate Annahmen st√ºtzt. <br><br>  Der Computer, der die Welt erobert, ist das Lieblingsthema von NF.  Viele Menschen nehmen dieses Szenario jedoch ernst, daher m√ºssen wir sie ernst nehmen.  Stephen Hawking, Elon Musk, eine gro√üe Anzahl von Investoren und Milliard√§ren aus dem Silicon Valley halten dieses Argument f√ºr √ºberzeugend. <br><br>  Lassen Sie mich zun√§chst die Voraussetzungen skizzieren, die erforderlich sind, um Bostroms Argumentation zu beweisen. <br><br><h2>  Hintergrund </h2><br><h3>  Voraussetzung 1: Ideeneffizienz </h3><br>  Die erste Pr√§misse ist eine einfache Beobachtung der Existenz eines denkenden Geistes.  Jeder von uns tr√§gt auf seinen Schultern eine kleine Schachtel mit denkendem Fleisch.  Ich benutze meine, um zu sprechen, du benutzt meine, um zuzuh√∂ren.  Manchmal k√∂nnen diese K√∂pfe unter den richtigen Bedingungen rational denken. <br><br>  Wir wissen also, dass dies im Prinzip m√∂glich ist. <br><br><h3>  Voraussetzung 2: keine Quantenprobleme </h3><br>  Die zweite Pr√§misse besagt, dass das Gehirn die √ºbliche Konfiguration von Materie ist, obwohl es √§u√üerst komplex ist.  Wenn wir genug dar√ºber w√ºssten und die richtige Technologie h√§tten, k√∂nnten wir seine Struktur genau kopieren und sein Verhalten mithilfe elektronischer Komponenten emulieren, genau wie heute k√∂nnen wir eine sehr einfache Anatomie von Neuronen simulieren. <br><br>  Mit anderen Worten, diese Pr√§misse besagt, dass Bewusstsein unter Verwendung gew√∂hnlicher Physik entsteht.  Einige Leute, wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Roger Penrose</a> , h√§tten sich diesem Argument widersetzt und geglaubt, dass auf Quantenebene etwas Ungew√∂hnliches im Gehirn passiert. <br><br>  Wenn Sie religi√∂s sind, k√∂nnen Sie glauben, dass das Gehirn ohne Seele nicht arbeiten kann. <br><br>  Aber f√ºr die meisten Menschen ist diese Pr√§misse leicht zu akzeptieren. <br><br><h3>  Voraussetzung 3: viele m√∂gliche K√∂pfe. </h3><br>  Die dritte Voraussetzung ist, dass der Raum aller m√∂glichen K√∂pfe gro√ü ist. <br><br>  Unser Intelligenzniveau, unsere Denkgeschwindigkeit, eine Reihe kognitiver Verzerrungen usw.  nicht vorbestimmt, sondern Artefakte unserer Evolutionsgeschichte.  Insbesondere gibt es kein physikalisches Gesetz, das die Intelligenz auf menschlicher Ebene einschr√§nkt. <br><br>  Es ist gut, sich ein Beispiel daf√ºr vorzustellen, was in der Natur passiert, wenn versucht wird, die Geschwindigkeit zu maximieren.  Wenn Sie in vorindustriellen Zeiten einen Geparden getroffen haben (und √ºberlebt haben), k√∂nnen Sie entscheiden, dass sich nichts schneller bewegen kann. <br><br>  Aber wir wissen nat√ºrlich, dass es alle Arten von Materiekonfigurationen gibt, zum Beispiel ein Motorrad, das sich schneller als ein Gepard bewegen und sogar steiler aussehen kann.  Es gibt jedoch keinen direkten Entwicklungsweg zum Motorrad.  Die Evolution musste zuerst Menschen erschaffen, die bereits alle m√∂glichen n√ºtzlichen Dinge erschaffen hatten. <br><br>  In Analogie mag es K√∂pfe geben, die viel schlauer sind als unsere, aber w√§hrend der Evolution auf der Erde unzug√§nglich sind.  Es ist m√∂glich, dass wir sie erstellen oder Maschinen erfinden k√∂nnen, die Maschinen erfinden k√∂nnen, die sie erstellen k√∂nnen. <br><br>  Es mag eine nat√ºrliche Grenze f√ºr die Intelligenz geben, aber es gibt keinen Grund zu der Annahme, dass wir nahe daran sind.  Vielleicht ist der kl√ºgste Intellekt doppelt so klug wie der Mensch und vielleicht sechzigtausend. <br><br>  Diese Frage ist empirisch und wir wissen nicht, wie wir sie beantworten sollen. <br><br><h3>  Pr√§misse 4: Oben ist viel Platz </h3><br>  Die vierte Voraussetzung ist, dass Computer immer noch viele M√∂glichkeiten bieten, schneller und kleiner zu werden.  Sie k√∂nnen davon ausgehen, dass sich Moores Gesetz verlangsamt - aber f√ºr diese Pr√§misse reicht es zu glauben, dass Eisen im Prinzip kleiner und schneller ist, bis zu mehreren Gr√∂√üenordnungen. <br><br>  Aus der Theorie ist bekannt, dass die physikalischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grenzen der Berechnungen</a> ziemlich hoch sind.  Wir k√∂nnen die Zahlen f√ºr mehrere Jahrzehnte verdoppeln, bis wir auf eine grundlegende physikalische Grenze sto√üen und nicht auf die wirtschaftliche oder politische Grenze von Moores Gesetz. <br><br><h3>  Pr√§misse 5: Computerzeitskalen </h3><br>  Die vorletzte Pr√§misse ist, dass wenn es uns gelingt, KI zu erstellen, sei es eine Emulation des menschlichen Gehirns oder eine spezielle Software, diese auf Zeitskalen funktioniert, die f√ºr die Elektronik charakteristisch sind (Mikrosekunden) und nicht f√ºr den Menschen (Stunden). . <br><br>  Um einen Zustand zu erreichen, in dem ich diesen Bericht erstellen kann, musste ich geboren werden, aufwachsen, zur Schule gehen, zur Universit√§t gehen, ein wenig leben, hierher fliegen und so weiter.  Computer k√∂nnen zehntausende Male schneller laufen. <br><br>  Insbesondere kann man sich vorstellen, dass der elektronische Verstand seine Schaltung (oder die Hardware, auf der er arbeitet) √§ndern und zu einer neuen Konfiguration √ºbergehen kann, ohne alles auf menschlicher Ebene neu studieren zu m√ºssen, lange Gespr√§che mit menschlichen Lehrern zu f√ºhren, aufs College zu gehen, Versuchen Sie, sich selbst zu finden, indem Sie an Malkursen teilnehmen und so weiter. <br><br><h3>  Voraussetzung 6: Rekursive Selbstverbesserung </h3><br>  Die letzte Pr√§misse ist meine Favoritin, da sie schamlos amerikanisch ist.  Demnach wird er sich verbessern wollen, egal welche Ziele die KI haben mag (was seltsame, fremde Ziele sein m√∂gen).  Er m√∂chte die beste Version von AI sein. <br><br>  Daher wird er es n√ºtzlich finden, seine eigenen Systeme rekursiv umzugestalten und zu verbessern, um sich selbst schlauer zu machen und m√∂glicherweise in einem k√ºhleren Geb√§ude zu leben.  Und gem√§√ü der Pr√§misse der Zeitskalen kann eine rekursive Selbstverbesserung sehr schnell auftreten. <br><br><h3>  Fazit: eine Katastrophe! </h3><br>  Wenn wir diese Pr√§missen akzeptieren, kommen wir zu einer Katastrophe.  Irgendwann, mit zunehmender Geschwindigkeit von Computern und der Intelligenz von Programmen, wird ein unkontrollierter Prozess auftreten, der einer Explosion √§hnelt. <br><br>  Sobald der Computer die menschliche Intelligenz erreicht hat, braucht er nicht mehr die Hilfe von Menschen, um eine verbesserte Version von sich selbst zu entwickeln.  Er wird dies viel schneller tun und nicht aufh√∂ren, bis er die nat√ºrliche Grenze erreicht hat, die sich als um ein Vielfaches gr√∂√üer als die menschliche Intelligenz herausstellen kann. <br><br>  In diesem Moment kann diese monstr√∂se rationale Kreatur, die eine Kreisverkehrssimulation der Arbeit unserer Emotionen und unseres Intellekts verwendet, uns davon √ºberzeugen, ihm Zugang zu Fabriken zu gew√§hren, k√ºnstliche DNA zu synthetisieren oder ihn einfach ins Internet gehen zu lassen, wo er einen Weg zu allem aufbrechen kann. alles und zerst√∂ren alle in der Debatte in den Foren vollst√§ndig.  Und von diesem Moment an wird alles sehr schnell zu Science Fiction. <br><br>  Stellen wir uns eine bestimmte Entwicklung der Ereignisse vor.  Nehmen wir an, ich m√∂chte einen Roboter bauen, der Witze sagt.  Ich arbeite mit einem Team, und jeden Tag wiederholen wir unser Programm, kompilieren und dann erz√§hlt uns der Roboter einen Witz.  Der Roboter ist zun√§chst praktisch nicht lustig.  Er ist auf dem niedrigsten Niveau menschlicher F√§higkeiten. <br><blockquote>  Was ist grau und kann nicht schwimmen? <br>  Schloss </blockquote>  Aber wir arbeiten hart daran und am Ende erreichen wir den Punkt, an dem der Roboter Witze macht, die bereits anfangen, lustig zu sein: <br><blockquote>  Ich sagte meiner Schwester, dass sie ihre Augenbrauen zu hoch zieht. <br>  Sie sah √ºberrascht aus. </blockquote>  In diesem Stadium wird der Roboter noch intelligenter und beginnt, an seiner eigenen Verbesserung teilzunehmen.  Jetzt hat er bereits ein gutes instinktives Verst√§ndnis daf√ºr, was lustig ist und was nicht, also h√∂ren die Entwickler auf seinen Rat.  Infolgedessen erreicht er ein fast √ºbermenschliches Niveau, auf dem er lustiger ist als jeder Mensch aus seiner Umgebung. <br><blockquote>  Mein G√ºrtel h√§lt meine Hose und die Schlaufen an meiner Hose halten den G√ºrtel. <br>  Was ist los?  Welcher von ihnen ist ein echter Held? </blockquote>  An diesem Punkt beginnt ein unkontrollierbarer Effekt.  Die Forscher gehen f√ºr das Wochenende nach Hause, und der Roboter beschlie√üt, sich neu zu kompilieren, um ein bisschen lustiger und ein bisschen schlauer zu werden.  Er verbringt das Wochenende damit, den Teil zu optimieren, der die Arbeit immer wieder gut macht.  Ohne mehr Hilfe von einer Person zu ben√∂tigen, kann er dies so schnell tun, wie es das Eisen erlaubt. <br><br>  Wenn Forscher am Montag zur√ºckkehren, wird die KI zehntausende Male lustiger als alle Menschen auf der Erde.  Er erz√§hlt ihnen einen Witz und sie sterben vor Lachen.  Und jeder, der versucht, mit einem Roboter zu sprechen, stirbt vor Lachen, wie in einer Parodie von Monty Python.  Die Menschheit stirbt vor Lachen. <br><br>  Den wenigen Menschen, die ihm eine Nachricht geben konnten, in der sie ihn aufforderten aufzuh√∂ren, erkl√§rt die KI (auf witzige und selbstironische Weise, die sich als t√∂dlich herausstellt), dass es ihm egal ist, ob Menschen √ºberleben oder sterben, sein Ziel ist es einfach, l√§cherlich zu sein. <br><br>  Infolgedessen baut die KI die Menschheit und baut Raumschiffe und Nano-Raketen, um die entferntesten Ecken der Galaxie zu untersuchen und nach anderen Kreaturen zu suchen, die unterhalten werden k√∂nnen. <br><br>  Dieses Szenario ist eine Karikatur von Bostroms Argumenten, weil ich nicht versuche, Sie von seiner Wahrhaftigkeit zu √ºberzeugen, sondern Sie damit impfe. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2e4/8a5/b77/2e48a5b778f37b9af0e351ed4cd0ef75.jpg"><br>  <i>Comic von PBF mit der gleichen Idee:</i> <i><br></i>  <i>- Ber√ºhren: Die Umarmung versucht, einen Hyperkristall der nuklearen Schwerkraft in ihre Umarmung einzubetten!</i> <i><br></i>  <i>- ...</i> <i><br></i>  <i>- Zeit f√ºr Gruppenumarmungen!</i> <br><br>  In diesen Szenarien ist die Standard-KI b√∂se, genau wie eine Pflanze auf einem anderen Planeten standardm√§√üig giftig ist.  Ohne sorgf√§ltige Anpassung gibt es keinen Grund f√ºr die Motivation oder die Werte der KI, unseren zu √§hneln. <br><br>  Das Argument argumentiert, dass wir diese Weltanschauung in ihre Grundlagen einbetten m√ºssen, damit der k√ºnstliche Geist etwas hat, das einem menschlichen Wertesystem √§hnelt. <br><br>  KI-Alarmisten lieben das Beispiel des B√ºroklammermaximierers - einen fiktiven Computer, der eine B√ºroklammerfabrik betreibt, die intelligent wird, sich rekursiv auf gott√§hnliche F√§higkeiten verbessert und dann seine ganze Energie darauf verwendet, das Universum mit B√ºroklammern zu f√ºllen. <br><br>  Es zerst√∂rt die Menschheit nicht, weil es b√∂se ist, sondern weil Eisen in unserem Blut ist, das besser zur Herstellung von B√ºroklammern verwendet wird.  Wenn wir also einfach eine KI erschaffen, ohne ihre Werte anzupassen, hei√üt es im Buch, dann ist eines der ersten Dinge, die er tut, die Zerst√∂rung der Menschheit. <br><br>  Es gibt viele bunte Beispiele daf√ºr, wie dies geschehen kann.  Nick Bostrom stellt vor, wie das Programm vern√ºnftig wird, wartet, baut heimlich kleine Ger√§te zur Reproduktion von DNA.  Wenn alles fertig ist, dann: <br><blockquote>  Nanofabriken, die Nervengas oder Zielsuchraketen von der Gr√∂√üe von M√ºcken produzieren, werden gleichzeitig von jedem Quadratmeter des Planeten platzen, und dies wird das Ende der Menschheit sein. </blockquote>  Das ist wirklich Zinn! <br><br>  Der einzige Weg, um aus diesem Chaos herauszukommen, besteht darin, einen solchen moralischen Punkt zu entwickeln, dass das KI-Wertesystem auch nach Tausenden und Abertausenden von Selbstverbesserungszyklen stabil bleibt und zu seinen Werten Dinge wie ‚ÄûMenschen helfen‚Äú, ‚Äûniemanden t√∂ten‚Äú und ‚Äûauf die W√ºnsche der Menschen h√∂ren‚Äú geh√∂ren ". <br><br>  Das hei√üt: "Tu was ich meine." <br><br>  Hier ist ein sehr poetisches Beispiel von Eliezer Yudkowsky, das amerikanische Werte beschreibt, die wir brauchen, um unsere KI zu lehren: <br><blockquote>  Koh√§renter extrapolierter Wille ist unser Wunsch, mehr zu wissen, schneller zu denken und unseren Vorstellungen von uns selbst zu entsprechen, einander n√§her zu kommen;  so dass unsere Gedanken einander n√§her sind als geteilt, dass unsere W√ºnsche dazu beitragen, sich nicht widersetzen, dass unsere W√ºnsche so interpretiert werden, wie wir sie interpretieren wollen. </blockquote>  Wie gef√§llt dir TK?  Jetzt schreiben wir den Code. <br><br>  Ich hoffe, Sie sehen die √Ñhnlichkeit dieser Idee mit dem Geist aus M√§rchen.  Die KI ist allm√§chtig und gibt Ihnen, was Sie fragen, interpretiert aber alles zu w√∂rtlich, wodurch Sie die Anfrage bereuen. <br><br>  Und nicht, weil der Geist dumm (er ist super schlau) oder b√∂sartig ist, sondern einfach, weil Sie als Person zu viele Annahmen √ºber das Verhalten des Geistes gemacht haben.  Das menschliche Wertesystem ist einzigartig und muss klar definiert und in einer ‚Äûfreundlichen‚Äú Maschine implementiert werden. <br><br>  Dieser Versuch ist eine ethische Version eines Versuchs zu Beginn des 20. Jahrhunderts, die Mathematik zu formalisieren und auf eine starre logische Grundlage zu stellen.  Niemand sagt jedoch, dass der Versuch in einer Katastrophe endete. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/80d/155/8a5/80d1558a5337c3989557d99a05d7fd9c.jpg"><br><br>  Als ich etwas √ºber zwanzig war, lebte ich in Vermont, in einem Provinz- und Landstaat.  Oft kam ich mit einem Abendflugzeug von Gesch√§ftsreisen zur√ºck und musste eine Stunde lang mit dem Auto durch den dunklen Wald nach Hause fahren. <br><br>  Ich habe mir dann das Abendprogramm im Art Bell Radio angeh√∂rt - es war eine Talkshow, die die ganze Nacht dauerte und in der die Moderatoren verschiedene Liebhaber der Verschw√∂rungstheorie und Menschen mit innovativem Denken interviewten.  Ich kam eingesch√ºchtert nach Hause oder blieb unter einer Taschenlampe stehen, unter dem Eindruck, dass Au√üerirdische mich bald entf√ºhren w√ºrden.  Dann fiel es mir sehr leicht, mich zu √ºberzeugen.  Beim Lesen √§hnlicher Szenarien im Zusammenhang mit KI geht es mir genauso. <br><br>  Daher freute ich mich, nach einigen Jahren einen Aufsatz von Scott Alexander zu entdecken, in dem er √ºber die erlernte erkenntnistheoretische Hilflosigkeit schrieb. <br><br>  Erkenntnistheorie ist eines dieser gro√üen und komplexen W√∂rter, aber es bedeutet wirklich: ‚ÄûWoher wei√üt du, dass das, was du wei√üt, wirklich wahr ist?‚Äú  Alexander bemerkte, dass er als junger Mann sehr an verschiedenen "alternativen" Geschichten f√ºr die Urheberschaft aller Arten von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verr√ºckten</a> interessiert war.  Er las diese Geschichten und glaubte ihnen vollst√§ndig, dann las er die Widerlegung und glaubte ihm und so weiter. <br><br>  Einmal entdeckte er drei alternative Geschichten, die sich widersprachen, weshalb sie nicht gleichzeitig wahr sein konnten.  Daraus schloss er, dass er einfach ein Mann war, der seinen Urteilen nicht trauen konnte.  Er war zu leicht zu √ºberzeugen. <br><br>  Menschen, die an Superintelligenz glauben, stellen einen interessanten Fall dar - viele von ihnen sind √ºberraschend klug.  Sie k√∂nnen Sie mit ihren Argumenten in den Boden treiben.  Aber sind ihre Argumente wahr oder sind nur sehr kluge Leute anf√§llig f√ºr religi√∂se √úberzeugungen √ºber die Risiken, die von KI ausgehen, was es sehr einfach macht, sie zu √ºberzeugen?  Ist die Idee der Superintelligenz eine Nachahmung einer Bedrohung? <br><br>  Wenn Sie √ºberzeugende Argumente zu einem seltsamen Thema bewerten, k√∂nnen Sie zwei Perspektiven ausw√§hlen, interne und externe. <br><br>  Angenommen, eines Tages erscheinen Menschen in lustigen Kleidern vor Ihrer Haust√ºr und fragen Sie, ob Sie sich ihrer Bewegung anschlie√üen m√∂chten.  Sie glauben, dass das UFO zwei Jahre sp√§ter die Erde besuchen wird und dass unsere Aufgabe darin besteht, die Menschheit auf den gro√üen Aufstieg auf dem Strahl vorzubereiten. <br><br>  Eine interne Perspektive erfordert eine intensive Diskussion ihrer Argumente.  Sie fragen die Besucher, wie sie von UFOs erfahren haben, warum sie glauben, dass er zu uns kommt, um uns abzuholen - Sie stellen alle m√∂glichen normalen Fragen, die ein Skeptiker in einem solchen Fall stellen w√ºrde. <br><br>  Stellen Sie sich vor, Sie haben eine Stunde mit ihnen gesprochen und sie haben Sie √ºberzeugt.  Sie best√§tigten ironischerweise das bevorstehende Kommen eines UFO, die Notwendigkeit, sich darauf vorzubereiten, und Sie glaubten immer noch nicht an so viel in Ihrem Leben, wie Sie jetzt an die Wichtigkeit glauben, die Menschheit auf dieses gro√üe Ereignis vorzubereiten. <br><br>  Die √§u√üere Perspektive sagt Ihnen etwas anderes.  Die Leute sind seltsam gekleidet, sie haben Perlen, sie leben in einer Art abgelegenem Lager, sie sprechen gleichzeitig und ein wenig be√§ngstigend.  Und obwohl ihre Argumente eisern sind, sagt Ihre ganze Erfahrung, dass Sie einem Kult begegnet sind. <br><br>  Nat√ºrlich haben sie gute Argumente daf√ºr, warum Sie den Instinkt ignorieren sollten, aber dies ist eine interne Perspektive.  Eine externe Perspektive k√ºmmert sich nicht um den Inhalt, sie sieht die Form und den Kontext und sie mag das Ergebnis nicht. <br><br>  Daher m√∂chte ich das Risiko einer KI aus beiden Perspektiven ansprechen.  Ich denke, die Argumente f√ºr Superintelligenz sind dumm und voller nicht unterst√ºtzter Annahmen.  Aber wenn sie Ihnen √ºberzeugend erscheinen, dann ist etwas Unangenehmes mit KI-Alarmismus als kulturellem Ph√§nomen verbunden, weshalb wir es nur ungern ernst nehmen sollten. <br><br>  Zun√§chst einige meiner Argumente gegen die Superintelligenz von Bostroma, die ein Risiko f√ºr die Menschheit darstellt. <br><br><h3>  Argument gegen unscharfe Definitionen </h3><br>  ¬´   ¬ª ()   .             ,   ,      ,   ,    . <br><br>       ,   ‚Äì  -   ,  ,          (  -)       . <br><br>      (    ),    ,     .  ,     ‚Äì  . , ,   ,  ,             .                  . <br><br><h3>      </h3><br>   ‚Äì      , , ,       .    ? <br><br>                .      .   ,       ,       ,     .               ,        . <br><br>   ,        ,    ‚Äì .         ,    -  ,      .     ,   ,   . <br><br>       ,  ,     ¬´,  ¬ª,   ,   ,    ¬´,  ¬ª. <br><br><h3>     </h3><br>       ,   .  ,       .         ,       ,  ,   . <br><br>       ,     ,           . <br><br>    ,  ,   ,   ,   . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12a/820/5a7/12a8205a776841eb3f2e5d759f674964.jpg"><br><br><h3>    </h3><br>     .   ,      ,        . <br><br>  1930-      ,   ,    .        ,  . <br><br>     :     , ,   ,     .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,       . <br><br><h3>     </h3><br>      .       -.               ,     ,  ,  ,   ,     ? <br><br>     Ethereum,     ,         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> . <br><br>  ,             <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .   ,         -  , ,         ,     . <br><br><h3>     </h3><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> .  ,          ,    .          ,         ,      .          ,   ,        ,    ‚Äì   . <br><br>       .   ,  ,   ; , ,      . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c64/5ba/9a4/c645ba9a437c79019622c86f9e2f6fd2.jpg"><br><br>   ¬´  ¬ª   ,    ,  ,  ,     ‚Äì      , ¬´  ?¬ª   ,    ‚Äì  ,        . <br><br>  ,   ¬´ ¬ª     ,    ,     reddit/r/paperclip,  ,    . <br><br>   AdSense  ,            . <br><br><h3>     </h3><br>    ,     ,  ,          .          .          ,     . <br><br> Google   Google Home,               . <br><br>  ,  ,   ,     .    ,      .     ,   ¬´¬ª,          . <br><br><h3>    </h3><br>          ,   .    ,  ,    ‚Äì        World of Warcraft    . <br><br>   ,       ,     ,     ,    ,       . <br><br>  ,       ,       ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> . <br><br><h3>    </h3><br>        ,     ,     , ,   ,       ,  -. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>   ,  ,     [-,      ,         2016    ,        / . .].    .       . <br><br>         ,    .         .    ,      ,    ,         . <br><br><h3>    </h3><br>      .    ,                  ,      .          ,      ,  -   . <br><br> ,        ,  ,  .      ,            . <br><br>  ,          ‚Äì         .     ,       ,          ,    . <br><br>  ,   ,       ,              ,        ,    . <br><br><h3>    * </h3><br> [ <i>  1954       / . .</i>  ]] <br><br>       ,         ,  ,        .   ,            ,         ,      (       ). <br><br>         Intel   ,    ,       . <br><br><h3>   </h3><br>          ?    . <br><br>        ,         .    ,         . <br><br><h3>  </h3><br>  Wenn Sie glauben, dass die KI es uns erm√∂glicht, die Galaxie zu erobern (ganz zu schweigen von der Simulation von Billionen von Gedanken), haben Sie erschreckende Zahlen in Ihren H√§nden.  Riesige Zahlen multipliziert mit winzigen Wahrscheinlichkeiten sind das Kennzeichen des KI-Alarmismus. <br><br>  Bostrom beschreibt irgendwann, worum es seiner Meinung nach geht: <br><br>  Wenn wir uns all das Gl√ºck vorstellen, das wir w√§hrend eines Lebens in Form einer Tr√§ne der Freude erfahren haben, dann wird das Gl√ºck all dieser Seelen in der Lage sein, die Ozeane der Erde jede Sekunde zu f√ºllen und zu f√ºllen, und dies f√ºr Hunderte von Milliarden von Milliarden von Jahrtausenden.  Es ist sehr wichtig, dass wir garantieren, dass diese Tr√§nen Freudentr√§nen sind. <br><br>  Ziemlich schwere Belastung f√ºr die Schultern eines 20-j√§hrigen Entwicklers! <br><br>  Hier gibt es nat√ºrlich einen ‚ÄûSalon-Fokus‚Äú, wenn man sich durch Multiplikation astronomischer Gr√∂√üen mit winzigen Wahrscheinlichkeiten von der Notwendigkeit √ºberzeugen kann, einige seltsame Dinge zu tun. <br><br>  All diese Bewegung in Bezug auf die Rettung der Zukunft der Menschheit ist ein feiger Kompromiss.  Wir haben die gleichen Argumente erlebt, um den Kommunismus zu rechtfertigen und zu erkl√§ren, warum immer alles kaputt ist und die Menschen kein elementares Ma√ü an materiellem Komfort haben k√∂nnen. <br><br>  Wir wollten diese Welt reparieren, und nach diesem Gl√ºck wird es so viel geben, dass sich das t√§gliche Leben jedes Menschen verbessern wird.  Daf√ºr war es jedoch zun√§chst notwendig, die Welt zu reparieren. <br><br>  Ich lebe in Kalifornien und hier ist der h√∂chste Prozentsatz an Bettlern unter allen Vereinigten Staaten, obwohl sich auch das Silicon Valley hier befindet.  Ich sehe nichts, was meine reiche Industrie tun w√ºrde, um das Leben der einfachen Leute und der verzweifelten Menschen um uns herum zu verbessern.  Wenn Sie sich jedoch f√ºr die Idee der Superintelligenz begeistern, ist die Forschung auf dem Gebiet der KI das Wichtigste, was Sie auf dem Planeten tun k√∂nnen.  Dies ist wichtiger als Politik, Malaria, hungernde Kinder, Kriege, globale Erw√§rmung - alles, was Sie sich vorstellen k√∂nnen.  In der Tat wurde unter der Bedrohung von Billionen und Billionen von Kreaturen die gesamte Bev√∂lkerung der Zukunft der Menschheit, simuliert und gegenw√§rtig, in der zuk√ºnftigen Zeit zusammengefasst.  Und unter solchen Bedingungen erscheint die Arbeit an anderen Problemen nicht rational. <br><br><h3>  Gr√∂√üenwahn </h3><br>  Diese Haltung verschmilzt mit Gr√∂√üenwahn, mit diesen B√∂sewichten von Bond, die an der Spitze unserer Branche stehen.  Die Leute glauben, dass die Welt von Superintelligenz √ºberw√§ltigt sein wird, und sie verwenden dieses Argument, um zu rechtfertigen, warum kluge Leute zuerst versuchen sollten, die Welt zu √ºbernehmen - um sie zu reparieren, bevor die KI sie bricht. <br><br>  Joey Ito, Leiter des MIT Media Lab, sagte k√ºrzlich in einem Gespr√§ch mit Obama etwas Wunderbares: <br><br>  Dies mag einen meiner Studenten am MIT ver√§rgern, aber eines meiner Anliegen ist, dass die wichtigsten AI-bezogenen Informatiken junge M√§nner sind, meistens wei√üe, die lieber mit Computern kommunizieren als andere Menschen.  Viele von ihnen glauben, dass wir uns keine Sorgen um so h√§ssliche Dinge wie Politik und Gesellschaft machen m√ºssen, wenn sie diese universelle KI aus Science-Fiction erstellen k√∂nnen.  Sie denken, dass Autos alles f√ºr uns haben werden. <br><br>  Besessene KI-Leute erkennen, dass die Welt keine Programmieraufgabe ist und wollen sie in eine Programmieraufgabe verwandeln, indem sie eine gott√§hnliche Maschine entwerfen.  Das ist Gr√∂√üenwahn, und ich mag es nicht. <br><br><h3>  Voodoo-Transhumanismus </h3><br>  Wenn Sie von den Risiken der KI √ºberzeugt sind, m√ºssen Sie einen ganzen Wagen trauriger √úberzeugungen mitnehmen, die mit einem Anh√§nger zu ihnen fahren. <br><br>  F√ºr den Anfang ist dies Nanotechnologie.  Jede st√§ndige Superintelligenz wird in der Lage sein, winzige Autos zu bauen, die zu allen m√∂glichen Dingen f√§hig sind.  Wir werden in einer Gesellschaft leben, die sich von einem Defizit befreit hat, in dem es eine F√ºlle von Material gibt. <br><br>  Die Nanotechnologie kann auch Ihr Gehirn scannen, damit Sie es in einen anderen K√∂rper oder in die virtuelle Welt laden k√∂nnen.  Daher ist die zweite Konsequenz freundlicher Superintelligenz, dass niemand stirbt - und wir werden unsterblich. <br><br>  Gute KI kann sogar die Toten wiederbeleben.  Nanomaschinen k√∂nnen in mein Gehirn gelangen, die Erinnerungen meines Vaters studieren und seine Simulation erstellen, mit der ich interagieren kann und die mich immer entt√§uscht, unabh√§ngig davon, was ich tue. <br><br>  Eine weitere seltsame Folge des Aufkommens der KI ist die galaktische Expansion.  Ich konnte nie verstehen, warum dies geschieht, aber dies ist die Grundlage f√ºr die Ideen der Transhumanisten.  Das Schicksal der Menschheit besteht entweder darin, unseren Planeten zu verlassen und die Galaxie zu kolonisieren oder zu sterben.  Und diese Aufgabe wird immer dringlicher, da andere Zivilisationen die gleiche Wahl treffen und uns im Weltraumrennen √ºberholen k√∂nnen. <br><br>  Daher h√§ngen viele seltsame komplement√§re Ideen mit der Annahme der Existenz einer wahren KI zusammen. <br><br><h3>  Religion 2.0 </h3><br>  In der Tat ist es eine Art Religion.  Die Menschen nannten den Glauben an technologische Singularit√§t ‚Äûeine Apokalypse f√ºr Nerds‚Äú, und das ist es auch.  Dies ist ein cooler Hack - anstatt an einen externen Gott zu glauben, stellen Sie sich vor, wie Sie selbst eine Kreatur erschaffen, deren Funktionalit√§t mit Gott identisch ist.  Hier k√∂nnen sogar wahre Atheisten ihren Weg zu einem bequemen Glauben rationalisieren. <br><br>  KI hat alle Eigenschaften eines Gottes: Er ist allm√§chtig, allwissend und unterst√ºtzt entweder (wenn Sie die √úberpr√ºfung der Grenzen des Arrays richtig organisiert haben) oder den reinen Teufel, in dessen Barmherzigkeit Sie sich befinden.  Und wie in jeder Religion gibt es sogar ein Gef√ºhl der Dringlichkeit.  M√ºssen heute handeln!  Auf dem Spiel steht das Schicksal der Welt!  Und nat√ºrlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">brauchen sie Geld</a> . <br><br>  Da diese Argumente religi√∂se Instinkte ansprechen, sind sie, sobald sie verwurzelt sind, sehr schwer zu beseitigen. <br><br><h3>  Comic-Ethik </h3><br>  Diese religi√∂sen √úberzeugungen f√ºhren zu einer Comic-Ethik, in der mehrere einsame Helden die Aufgabe erhalten, die Welt mit Technologie und einem scharfen Verstand zu retten.  Und es geht um das Schicksal des Universums.  Infolgedessen ist unsere Branche voller reicher Typen, die sich vorstellen, Batman zu sein (interessanterweise will niemand Robin sein). <br><br><h3>  Fiebersimulationen </h3><br>  Wenn Sie an die M√∂glichkeit eines k√ºnstlichen Lebens glauben und dass KI extrem leistungsf√§hige Computer entwickeln kann, werden Sie h√∂chstwahrscheinlich glauben, dass wir in einer Simulation leben.  So funktioniert es <br><br>  Angenommen, Sie sind ein Historiker, der in einer Welt nach der Singularit√§t lebt.  Sie studieren den Zweiten Weltkrieg und m√∂chten wissen, was passieren wird, wenn Hitler 1941 Moskau einnimmt. Da Sie Zugang zu Hypercomputern haben, richten Sie die Simulation ein, beobachten die Konvergenz der Armeen und schreiben wissenschaftliche Arbeiten. <br><br>  Aufgrund der Granularit√§t der Simulation sind ihre Charaktere intelligente Wesen wie Sie.  Daher k√∂nnen Sie die Simulation aufgrund der Ethikempfehlung Ihrer Universit√§t nicht deaktivieren.  Sie haben nicht nur vorget√§uscht, der Holocaust zu sein.  Als Ethikforscher m√ºssen Sie jetzt die Simulation betriebsbereit halten. <br><br>  Infolgedessen wird die simulierte Welt Computer erfinden, AI, wird beginnen, ihre eigenen Simulationen auszuf√ºhren.  In gewisser Weise werden Simulationen die Hierarchie immer weiter durchlaufen, bis Ihnen die Prozessorleistung ausgeht. <br><br>  Jede zugrunde liegende Realit√§t kann also eine gro√üe Anzahl verschachtelter Simulationen enthalten, und ein einfaches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Z√§hlargument</a> beweist, dass die Wahrscheinlichkeit, dass wir in einer Simulation leben, gr√∂√üer ist als die, dass wir in der realen Welt leben. <br><br>  Aber daran zu glauben bedeutet, an Magie zu glauben.  Wenn wir uns in einer Simulation befinden, wissen wir nichts √ºber die Regeln auf einer h√∂heren Ebene.  Wir wissen nicht einmal, ob Mathematik dort genauso funktioniert - vielleicht in der simulierenden Welt 2 + 2 = 5 oder sogar 2 + 2 =. <br><br>  Eine simulierte Welt liefert keine Informationen √ºber die Welt, in der sie gestartet wurde.  In der Simulation k√∂nnen Menschen leicht von den Toten auferstehen, wenn der Administrator die erforderlichen Backups gespeichert hat.  Und wenn wir einen der Admins kontaktieren, haben wir tats√§chlich eine direkte Verbindung zu Gott. <br><br>  Dies ist eine ernsthafte Bedrohung f√ºr die geistige Gesundheit.  Je tiefer Sie in die Welt der Simulationen eintauchen, desto verr√ºckter werden Sie. <br><br>  Wir haben jetzt vier unabh√§ngige Wege, um durch den Supermind unsterblich zu werden: <br><br><ol><li>  Die wohlwollende KI erfindet die medizinische Nanotechnologie und unterst√ºtzt den K√∂rper in jungen Jahren f√ºr immer. </li><li>  AI erfindet einen vollst√§ndigen Gehirn-Scan, einschlie√ülich Gehirn-Scans von Toten, gefrorenen K√∂pfen usw., mit denen Sie in einem Computer leben k√∂nnen. </li><li>  KI ‚Äûbelebt‚Äú Menschen wieder, indem sie das Gehirn anderer Menschen auf der Suche nach Erinnerungen an eine Person scannt und dies mit Videos und anderen Materialien kombiniert.  Wenn sich niemand gut genug an einen Menschen erinnert, kann er in einer Simulation, die mit seiner DNA beginnt und alle Lebensbedingungen wiederherstellt, immer von Grund auf neu wachsen. </li><li>  Wenn wir bereits in der Simulation leben, besteht die M√∂glichkeit, dass derjenige, der sie gestartet hat, Backups erstellt und Sie sie davon √ºberzeugen k√∂nnen, sie herunterzuladen. </li></ol><br>  Das meine ich mit KI, die sich mit religi√∂sen Impulsen befasst.  Welches andere Glaubenssystem bietet Ihnen vier M√∂glichkeiten f√ºr wissenschaftlich belegte Unsterblichkeit? <br><br>  Wir haben erfahren, dass mindestens ein amerikanischer Plutokrat (h√∂chstwahrscheinlich Elon Musk, der glaubt, dass die Chancen, dass wir in einer Simulation leben, eine Milliarde zu eins sind) ein Paar Encoder engagiert haben, um zu versuchen, die Simulation zu knacken.  Aber das ist eine sehr grobe Absicht!  Ich benutze es! <br><br>  Wenn Sie der Meinung sind, dass Sie in einem Computerprogramm leben, sind Versuche, es auf segfault zu bringen, f√ºr jeden, der mit Ihnen darin lebt, unangemessen.  Dies ist viel gef√§hrlicher und verantwortungsloser als Atomwissenschaftler, die versuchen, die Atmosph√§re in die Luft zu jagen. <br><br><h3>  Durst nach Daten </h3><br>  Wie ich bereits erw√§hnt habe, besteht der effektivste Weg, etwas Interessantes aus der von uns erstellten KI herauszuholen, darin, sie mit Daten zu l√∂schen.  Eine solche Dynamik ist sozial sch√§dlich.  Wir sind der orwellschen Einf√ºhrung von Mikrofonen in jedem Haus nahe gekommen.  KI-Daten werden zentralisiert und zum Trainieren neuronaler Netze verwendet, die dann besser auf unsere W√ºnsche eingehen k√∂nnen. <br><br>  Wenn Sie jedoch der Meinung sind, dass dieser Weg uns zur KI f√ºhrt, sollten Sie die Menge der gesammelten Daten in m√∂glichst wenig ge√§nderter Form maximieren.  Dies best√§rkt nur die Notwendigkeit, die meisten Daten zu sammeln und die umfassendste √úberwachung durchzuf√ºhren. <br><br><h3>  Stringtheorie f√ºr Programmierer </h3><br>  Das Risiko einer KI ist die Stringtheorie f√ºr Programmierer.  Es macht Spa√ü dar√ºber nachzudenken, es ist interessant und f√ºr Experimente auf dem Niveau der modernen Technologie v√∂llig unzug√§nglich.  Sie k√∂nnen mentale Kristallpal√§ste bauen, die auf den Grundprinzipien basieren, und dann in sie klettern und die Leiter hinter ihnen festziehen. <br><br>  Menschen, die auf der Grundlage einer langen Kette abstrakter Argumente zu absurden Schlussfolgerungen gelangen k√∂nnen und auf ihre Wahrheit vertrauen - das sind keine Menschen, denen das Kulturmanagement anvertraut werden muss. <br><br><h3>  Der Drang zum Wahnsinn </h3><br>  Dieser ganze Bereich der "Forschung" f√ºhrt zum Wahnsinn.  Eines der Kennzeichen f√ºr tiefes Nachdenken √ºber KI-Risiken ist, dass Sie bei anderen Enthusiasten umso beliebter werden, je verr√ºckter Ihre Ideen sind.  Dies zeigt Ihren Mut, diesem Gedankengang bis zum Ende zu folgen. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ray Kurzweil</a> , der glaubt, dass er nicht sterben wird, arbeitet seit mehreren Jahren mit Google zusammen und arbeitet jetzt wahrscheinlich an diesem Problem.  Silicon Valley ist im Allgemeinen voll von Leuten, die unter dem Deckmantel des Geldes an verr√ºckten Projekten arbeiten. <br><br><h3>  Cosplay AI </h3><br>  Der sch√§dlichste soziale Effekt der Angst vor KI ist das, was ich Cosplay-KI nenne.  Menschen, die von der Realit√§t und Unvermeidlichkeit der KI √ºberzeugt sind, beginnen sich zu verhalten, w√§hrend ihre Fantasien ihnen sagen, was die superintelligente KI tun kann. <br><br>  In seinem Buch listet Bostrom sechs Dinge auf, denen KI gelingen muss, bevor sie die Welt einnimmt: <br><br><ol><li>  Multiplikation der Intelligenz. </li><li>  Strategisches Denken. </li><li>  Soziale Manipulation. </li><li>  Hacks </li><li>  Technologische Forschung. </li><li>  Wirtschaftliche Produktivit√§t. </li></ol><br>  Wenn Sie sich die Anh√§nger der KI aus dem Silicon Valley ansehen, dann scheinen sie selbst an dieser quasi-soziopathischen Liste zu arbeiten. <br><br>  Sam Altman, Leiter von YCombinator, ist mein Lieblingsbeispiel f√ºr einen solchen Archetyp.  Er ist anscheinend fasziniert von der Idee, die Welt von Grund auf neu zu erfinden, den Einfluss und die pers√∂nliche Produktivit√§t zu maximieren.  Er hat Teams beauftragt, St√§dte von Grund auf neu zu erfinden, und ist in schattenpolitischen Betrug verwickelt, um die Wahlen zu beeinflussen. <br><br>  Dieses Verhalten des ‚ÄûUmhangs und Dolches‚Äú, das der Techno-Elite innewohnt, wird eine negative Gegenreaktion von Menschen hervorrufen, die nicht an Technologien beteiligt sind, die nicht gerne manipuliert werden.  Es ist unm√∂glich, endlos die Hebel der Macht zu ziehen, es wird schlie√ülich beginnen, andere Mitglieder der demokratischen Gemeinschaft zu √§rgern. <br><br>  Ich habe Leute von den sogenannten beobachtet  "Rationalistische Gemeinschaften" beziehen sich auf Personen, die nicht als effektiv gelten, "Nicht-Spieler-Charaktere" (NPCs), ein Begriff, der aus Spielen entlehnt wurde.  Dies ist eine schreckliche Art, die Welt zu betrachten. <br><br>  Ich arbeite in einer Branche, in der selbsternannte Rationalisten die verr√ºcktesten Menschen sind.  Es ist √ºberw√§ltigend. <br><br>  Diese KI-Cosplayer sind wie Neunj√§hrige, die im Hinterhof ein Campingcamp errichten und mit Taschenlampen in Zelten spielen.  Sie projizieren ihre eigenen Schatten auf die W√§nde des Zeltes und haben Angst vor ihnen, als w√§ren sie Monster. <br><br>  Tats√§chlich reagieren sie jedoch auf ein verzerrtes Selbstbild.  Es gibt eine R√ºckkopplungsschleife zwischen der Art und Weise, wie kluge Menschen sich das Verhalten gott√§hnlicher Intelligenz vorstellen und wie sie ihr eigenes Verhalten aufbauen. <br><br>  Was ist die Antwort, wie kann dies behoben werden? <br><br>  Wir brauchen bessere Science Fiction!  Und wie in vielen anderen F√§llen haben wir die Technologie bereits. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8fa/7bb/a47/8fa7bba47a9f451102465e23774d291f.jpg"><br><br>  Dies ist Stanislav Lem, der gro√üe polnische Science-Fiction-Autor.  Die englischsprachige NF ist schrecklich, aber im Ostblock haben wir viele gute Waren, und wir m√ºssen sie korrekt exportieren.  Es wurde bereits aktiv ins Englische √ºbersetzt, diese √úbersetzungen m√ºssen nur besser verteilt werden. <br><br>  Was Autoren wie Lem oder die Br√ºder Strugatsky von ihren westlichen Partnern unterscheidet, ist, dass sie unter schwierigen Bedingungen aufgewachsen sind, den Krieg √ºberlebt haben und dann in totalit√§ren Gesellschaften gelebt haben, in denen sie ihre Ideen nicht direkt durch ein gedrucktes Wort ausdr√ºcken mussten. <br><br>  Sie haben ein echtes Verst√§ndnis der menschlichen Erfahrung und der Grenzen des utopischen Denkens, das im Westen praktisch nicht vorhanden ist. <br><br>  Es gibt bemerkenswerte Ausnahmen - Stanley Kubrick war dazu in der Lage -, aber es ist √§u√üerst selten, eine amerikanische oder britische NF zu finden, die eine zur√ºckhaltende Ansicht dar√ºber ausdr√ºckt, was wir als Spezies mit Technologie tun k√∂nnen. <br><br><h3>  Alchemisten </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/fdc/b9e/596/fdcb9e5963c4ad447547319ebf394f40.jpg" alt="Bild"><br><br>  Da ich KI-Alarmismus kritisiere, ist es fair, meine Karten auf den Tisch zu legen.  Ich denke, dass sich unser Verst√§ndnis des Geistes in ungef√§hr demselben Zustand befindet, in dem sich die Alchemie im 17. Jahrhundert befand. <br><br>  Alchemisten haben einen schlechten Ruf.  Wir betrachten sie als Mystiker, die gr√∂√ütenteils nicht an experimentellen Arbeiten beteiligt sind.  Moderne Forschungen zeigen, dass sie viel flei√üiger als wir denken.  In vielen F√§llen verwendeten sie moderne experimentelle Techniken, f√ºhrten Laboraufzeichnungen und stellten die richtigen Fragen. <br><br>  Alchemisten haben viele Dinge richtig verstanden!  Zum Beispiel waren sie von der korpuskul√§ren Theorie der Materie √ºberzeugt: dass alles aus winzigen St√ºcken besteht und dass es m√∂glich ist, diese St√ºcke auf unterschiedliche Weise miteinander zu komponieren und unterschiedliche Substanzen zu erzeugen - und das ist so! <br><br>  Ihr Problem war der Mangel an genauer Ausr√ºstung, die notwendig war, um die Entdeckungen zu machen, die sie brauchten.  Die gro√üe Entdeckung, die ein Alchemist machen muss, ist das Gesetz der Massenerhaltung: Das Gewicht der Ausgangsbestandteile stimmt mit dem Gewicht des Endbestandteils √ºberein.  Einige von ihnen k√∂nnen jedoch Gase oder verdampfende Fl√ºssigkeiten sein, und den Alchemisten mangelte es einfach an Genauigkeit.  Moderne Chemie war erst im 18. Jahrhundert m√∂glich. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21b/5b2/888/21b5b28887c09eeebc52c57e43025c42.jpg"><br><br>  Aber die Alchemisten hatten auch Hinweise, die sie verwirrten.  Sie waren von Quecksilber besessen.  Quecksilber ist chemisch nicht besonders interessant, aber es ist das einzige Metall in der fl√ºssigen Phase bei Raumtemperatur.  Dies schien den Alchemisten sehr wichtig zu sein und zwang sie, Quecksilber in das Zentrum ihres alchemistischen Systems zu stellen und nach dem Stein der Weisen zu suchen, um unedle Metalle in Gold umzuwandeln. <br><br>  Die Neurotoxizit√§t von Quecksilber verschlimmerte die Situation.  Wenn Sie zu viel mit ihr spielen, kommen Ihnen seltsame Gedanken.  In diesem Sinne √§hnelt es unseren aktuellen mentalen Experimenten, die sich auf das Supermind beziehen. <br><br>  Stellen Sie sich vor, wir h√§tten einem gro√üen Alchemisten wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">George Starkey</a> oder Isaac Newton ein Lehrbuch √ºber moderne Chemie in die Vergangenheit geschickt.  Das erste, was sie damit machen w√ºrden, w√§re, darin nach einer Antwort auf die Frage zu suchen, ob wir den Stein der Weisen gefunden hatten.  Und sie w√ºrden wissen, dass wir ihn gefunden haben!  Wir haben ihren Traum verwirklicht! <br><br>  Aber wir m√∂gen es nicht so sehr, denn nachdem wir Metalle in Gold verwandelt haben, stellt sich heraus, dass es radioaktiv ist.  Stellen Sie sich neben einen Barren aus umgewandeltem Gold, der Sie mit unsichtbaren magischen Strahlen t√∂tet. <br><br>  Man kann sich vorstellen, wie schwierig es w√§re, moderne Konzepte von Radioaktivit√§t und Atomenergie f√ºr sie nicht mystisch klingen zu lassen. <br><br>  Wir m√ºssten ihnen erkl√§ren, warum wir den ‚ÄûStein der Weisen‚Äú verwenden: f√ºr die Herstellung von Metall, das es auf dem Planeten noch nie gegeben hat, und ein paar Handvoll davon reichen aus, um eine ganze Stadt in die Luft zu jagen, wenn sie mit ausreichend hoher Geschwindigkeit kollidieren. <br><br>  Dar√ºber hinaus m√ºssten wir den Alchemisten erkl√§ren, dass alle Sterne am Himmel ‚Äûphilosophische Steine‚Äú sind, die ein Element in ein anderes verwandeln, und dass alle Teilchen in unserem K√∂rper von Sternen des Firmaments stammen, das existierte und explodierte, bevor die Erde erschien. <br><br> ,   ,  ,     ,      ,  ,     ,   ,     ,        ,  . <br><br>   ,  ,   ,      ,    ,     ,       .     ‚Äì     .      ,   . <br><br>   ,           .     .   ‚Äì  .        , ,  (     ),    ,   . <br><br>          ,     ,         . <br><br>      ,      .  ,        .  ,     ,  ,         .  ,         ,   ,      . <br><br>       .    ,     ,           . <br><br>    ,      , ,  ,   ,    .    ,     . <br><br>       ,   ‚Äì ,     ¬´¬ª,  ,     .       .  Und das ist gro√üartig!   .    ,    : <br><blockquote>      ,  ,   ,    . <br> ‚Äî   </blockquote>       ,    ,         ,      . <br><br>    ,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>      ,     ,  ,   -   ,   ,    . <br><br>         ,       ,   ,      ,         . <br><br> , ,   ,       .    ,   -      .    ,      . <br><br>        , ,  ,     ,    . <br><br>  ,        . ,       - ,   ,       , ,   ,    . <br><br>        :   ,  ,     .  ! <br><br>         ,       ‚Äì   ,    ,       . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de432806/">https://habr.com/ru/post/de432806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de432796/index.html">Rechenmodul, Modelle 2019</a></li>
<li><a href="../de432798/index.html">Bestes Sicherheitsbetriebssystem: Titan-Vergleich</a></li>
<li><a href="../de432800/index.html">Untersuchung von Sicherheitsvorf√§llen mit StaffCop Enterprise 4.4</a></li>
<li><a href="../de432802/index.html">Sechs kostenlose Lernplattformen f√ºr die automatisierte Programmierung</a></li>
<li><a href="../de432804/index.html">Die ganze Wahrheit √ºber RTOS. Artikel 24. Warteschlangen: Nebendienstleistungen und Datenstrukturen</a></li>
<li><a href="../de432808/index.html">Geh√§lter in AI: Wo es mehr Geld gibt und wen sie in Russland suchen</a></li>
<li><a href="../de432810/index.html">Erste Bu√ügelder f√ºr die DSGVO: Wer wurde bereits bestraft?</a></li>
<li><a href="../de432812/index.html">Wir schreiben Handelsroboter mit dem grafischen Framework von StockSharp. Teil 1</a></li>
<li><a href="../de432814/index.html">Kuchen- und TeamCity-Integration</a></li>
<li><a href="../de432816/index.html">AXIS M3046-V gegen IDIS DC-D3212X: Vergleichen Sie CCTV-Kameras</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>