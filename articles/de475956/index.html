<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö£üèæ üëã ü¶â Wie wir die Technologie der optischen Texterkennung geschaffen haben. OCR auf Yandex üî´ üéØ üéØ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo! Heute werde ich den Lesern von Habr erz√§hlen, wie wir die Texterkennungstechnologie entwickelt haben, die in 45 Sprachen funktioniert und f√ºr Y...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir die Technologie der optischen Texterkennung geschaffen haben. OCR auf Yandex</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/475956/">  Hallo!  Heute werde ich den Lesern von Habr erz√§hlen, wie wir die Texterkennungstechnologie entwickelt haben, die in 45 Sprachen funktioniert und f√ºr Yandex.Cloud-Benutzer zug√§nglich ist, welche Aufgaben wir gestellt und wie wir sie gel√∂st haben.  Es ist n√ºtzlich, wenn Sie an √§hnlichen Projekten arbeiten oder herausfinden m√∂chten, wie es dazu kam, dass Sie heute nur ein Schild eines t√ºrkischen Gesch√§fts fotografieren m√ºssen, damit Alice es ins Russische √ºbersetzt. <br><br><img src="https://habrastorage.org/webt/fm/c4/rx/fmc4rxrj9iczvwvfjku7cm-nwim.png"><br><a name="habracut"></a><br>  Die OCR-Technologie (Optical Character Recognition) entwickelt sich seit Jahrzehnten weltweit.  Wir bei Yandex haben begonnen, unsere eigene OCR-Technologie zu entwickeln, um unsere Dienste zu verbessern und den Benutzern mehr Optionen zu bieten.  Bilder sind ein gro√üer Teil des Internets und ohne die F√§higkeit, sie zu verstehen, ist die Suche im Internet unvollst√§ndig. <br><br>  Bildanalysel√∂sungen werden immer beliebter.  Dies ist auf die Verbreitung k√ºnstlicher neuronaler Netze und Ger√§te mit hochwertigen Sensoren zur√ºckzuf√ºhren.  Es ist klar, dass es sich in erster Linie um Smartphones handelt, aber nicht nur um solche. <br><br>  Die Komplexit√§t der Aufgaben im Bereich der Texterkennung nimmt stetig zu - alles begann mit der Erkennung gescannter Dokumente.  Dann wurde die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennung von</a> Born-Digital-Bildern mit Texten aus dem Internet hinzugef√ºgt.  Dann, mit der wachsenden Popularit√§t von mobilen Kameras, die Erkennung von guten Kameraaufnahmen ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Focused Scene Text</a> ).  Und je weiter, desto komplizierter die Parameter: Der Text kann unscharf ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Incidental Scene Text</a> ) sein, in beliebiger Biegung oder Spirale in verschiedenen Kategorien geschrieben - von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fotos von</a> Belegen bis zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regalen</a> und Schildern. <br><br><h3>  Welchen Weg sind wir gegangen? </h3><br>  Die Texterkennung ist eine separate Klasse von Computer Vision-Aufgaben.  Wie viele Computer-Vision-Algorithmen beruhte sie vor der Popularit√§t neuronaler Netze haupts√§chlich auf manuellen Funktionen und Heuristiken.  In j√ºngster Zeit hat sich jedoch mit dem √úbergang zu neuronalen Netzen die Qualit√§t der Technologie erheblich verbessert.  Schauen Sie sich das Beispiel auf dem Foto an.  Wie das passiert ist, werde ich weiter erz√§hlen. <br><br>  Vergleichen Sie die heutigen Erkennungsergebnisse mit den Ergebnissen zu Beginn des Jahres 2018: <br><div class="scrollable-table"><table><tbody><tr><td><img src="https://habrastorage.org/webt/uf/ux/-g/ufux-gtblu3a_fc96rdowuul7co.png"></td><td><img src="https://habrastorage.org/webt/so/pa/yg/sopaygej-n6c9gurt75zfwa-f3y.png"></td></tr><tr><td> <b><i>2018</i></b> <b><i><br></i></b>  <b><i>Feuchtigkeitsspendend</i></b> <b><i><br></i></b>  <b><i>n HO - Mizellen</i></b> <b><i><br></i></b>  <b><i>luxuri√∂ses glattes Wasser.</i></b> <b><i><br></i></b>  <b><i>sanft multifunktional</i></b> <b><i><br></i></b>  <b><i>die Formel</i></b> <b><i><br></i></b>  <b><i>Verwenden Sie als Mittel</i></b> <b><i><br></i></b>  <b><i>Sl F√úR</i></b> <b><i><br></i></b>  <b><i>anstelle einer Reinigungslotion oder</i></b> <b><i><br></i></b>  <b><i>Tonikum</i></b> <b><i><br></i></b>  <b><i>Kein Alkohol, Farbstoffe, Parabene</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td><td>  <b><i>2019</i></b> <b><i><br></i></b>  <b><i>FEUCHTIGKEIT</i></b> <b><i><br></i></b>  <b><i>THERMISCH-MIKELLARES WASSER</i></b> <b><i><br></i></b>  <b><i>LUXUS-GLATTHEIT</i></b> <b><i><br></i></b>  <b><i>AUBY Weich und sanft</i></b> <b><i><br></i></b>  <b><i>multifunktionale Formel</i></b> <b><i><br></i></b>  <b><i>f√ºr den t√§glichen Gebrauch in</i></b> <b><i><br></i></b>  <b><i>als Mittel zu</i></b> <b><i><br></i></b>  <b><i>Make-up-Entferner statt Reinigung</i></b> <b><i><br></i></b>  <b><i>Lotion oder Tonikum.</i></b> <b><i><br></i></b>  <b><i>Kein Alkohol, Farbstoffe, Parabene</i></b> <b><i><br></i></b>  <b><i>...</i></b> </td></tr></tbody></table></div><h3>  Auf welche Schwierigkeiten stie√üen wir zuerst? </h3><br>  Zu Beginn unserer Reise haben wir Erkennungstechnologien f√ºr Russisch und Englisch entwickelt. Die Hauptanwendungsf√§lle waren fotografierte Seiten mit Texten und Bildern aus dem Internet.  Im Laufe der Arbeit haben wir jedoch festgestellt, dass dies nicht ausreicht: Der Text auf den Bildern wurde in jeder Sprache und auf jeder Oberfl√§che gefunden, und die Bilder erwiesen sich manchmal als von sehr unterschiedlicher Qualit√§t.  Dies bedeutet, dass die Erkennung in jeder Situation und bei allen Arten von eingehenden Daten funktionieren sollte. <br><br>  Und hier stehen wir vor einer Reihe von Schwierigkeiten.  Hier sind nur einige: <br><br><ul><li>  <b>Einzelheiten</b>  F√ºr eine Person, die es gewohnt ist, Informationen aus Text abzurufen, besteht der Text im Bild aus Abs√§tzen, Linien, W√∂rtern und Buchstaben, f√ºr ein neuronales Netzwerk sieht alles anders aus.  Aufgrund der Komplexit√§t des Textes ist das Netzwerk gezwungen, sowohl das Bild als Ganzes zu sehen (zum Beispiel, wenn Menschen sich an den H√§nden f√ºgten und eine Inschrift bauten) als auch die kleinsten Details (in der vietnamesischen Sprache √§ndern √§hnliche Symbole  und ·ª´ die Bedeutung von W√∂rtern).  Separate Herausforderungen bestehen darin, beliebigen Text und nicht standardm√§√üige Schriftarten zu erkennen. </li><li>  <b>Mehrsprachigkeit</b> .  Je mehr Sprachen wir hinzuf√ºgten, desto mehr stie√üen wir auf ihre Besonderheiten: In kyrillischen und lateinischen W√∂rtern bestehen sie aus getrennten Buchstaben, in Arabisch werden sie zusammen geschrieben, in Japanisch werden keine getrennten W√∂rter unterschieden.  Einige Sprachen verwenden die Schreibweise von links nach rechts, andere von rechts nach links.  Einige W√∂rter sind horizontal, andere vertikal geschrieben.  Ein universelles Tool sollte all diese Funktionen ber√ºcksichtigen. </li><li>  <b>Die Struktur des Textes</b> .  Um bestimmte Bilder wie Schecks oder komplexe Dokumente zu erkennen, ist eine Struktur, die das Layout von Abs√§tzen, Tabellen und anderen Elementen ber√ºcksichtigt, von entscheidender Bedeutung. </li><li>  <b>Leistung</b> .  Die Technologie wird auf einer Vielzahl von Ger√§ten eingesetzt, auch offline. Daher mussten wir die strengen Leistungsanforderungen ber√ºcksichtigen. </li></ul><br><h3>  Auswahl des Erkennungsmodells </h3><br>  Der erste Schritt zum Erkennen von Text besteht in der Bestimmung seiner Position (Erkennung). <br>  Die Texterkennung kann als Objekterkennungsaufgabe betrachtet werden, bei der einzelne <b>Zeichen</b> , <b>W√∂rter</b> oder <b>Linien</b> als Objekt fungieren k√∂nnen. <br><br>  Es war uns wichtig, dass das Modell anschlie√üend auf andere Sprachen skaliert wurde (jetzt unterst√ºtzen wir 45 Sprachen). <br><br>  Viele Forschungsartikel zur Texterkennung verwenden Modelle, die die Position einzelner <b>W√∂rter</b> vorhersagen.  Bei einem <b>universellen Modell weist</b> dieser Ansatz jedoch mehrere Einschr√§nkungen auf. So unterscheidet sich beispielsweise das Konzept eines Wortes f√ºr die chinesische Sprache grundlegend vom Konzept eines Wortes, beispielsweise auf Englisch.  Einzelne chinesische W√∂rter werden nicht durch ein Leerzeichen getrennt.  In Thai werden nur einzelne S√§tze mit einem Leerzeichen verworfen. <br><br>  Hier sind Beispiele f√ºr denselben Text in Russisch, Chinesisch und Thail√§ndisch: <br><br> <code>  .    . <br>‰ªäÂ§©Â§©Ê∞îÂæàÂ•Ω ËøôÊòØ‰∏Ä‰∏™Áæé‰∏ΩÁöÑ‰∏ÄÂ§©Êï£Ê≠•„ÄÇ <br> ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏•‡πà‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß</code> <br> <br>  <b>Linien</b> sind wiederum in Bezug auf das Seitenverh√§ltnis sehr variabel.  Aus diesem Grund sind die M√∂glichkeiten solcher g√§ngiger Erfassungsmodelle (beispielsweise SSD- oder RCNN-basiert) zur Linienvorhersage begrenzt, da diese Modelle auf Kandidatenregionen / Ankerboxen mit vielen vordefinierten Seitenverh√§ltnissen basieren.  Dar√ºber hinaus k√∂nnen die Linien eine beliebige Form haben, beispielsweise gekr√ºmmt, weshalb es f√ºr eine qualitative Beschreibung der Linien nicht ausreicht, auch bei einem Drehwinkel ausschlie√ülich ein Quad zu beschreiben. <br><br>  Trotz der Tatsache, dass die Positionen der einzelnen <b>Zeichen</b> lokal und beschrieben sind, besteht der Nachteil darin, dass ein separater Nachbearbeitungsschritt erforderlich ist - Sie m√ºssen Heuristiken ausw√§hlen, um Zeichen in W√∂rter und Zeilen zu kleben. <br><br>  Daher haben wir <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das SegLink-Modell</a></b> als Grundlage f√ºr die Erkennung <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herangezogen</a></b> , dessen Hauptidee darin besteht, Zeilen / W√∂rter in zwei weitere lokale Einheiten zu zerlegen: Segmente und Beziehungen zwischen ihnen. <br><br><h3>  Detektorarchitektur </h3><br>  Die Architektur des Modells basiert auf einer SSD, die die Position von Objekten auf mehreren Merkmalsskalen vorhersagt.  Nur zus√§tzlich zur Vorhersage der Koordinaten einzelner "Segmente" werden auch "Verbindungen" zwischen benachbarten Segmenten vorhergesagt, dh ob zwei Segmente zu derselben Linie geh√∂ren.  "Verbindungen" werden sowohl f√ºr benachbarte Segmente mit demselben Merkmalsma√üstab als auch f√ºr Segmente in benachbarten Bereichen mit benachbarten Ma√üst√§ben vorhergesagt (Segmente aus unterschiedlichen Ma√üst√§ben von Merkmalen k√∂nnen geringf√ºgig unterschiedlich gro√ü sein und zur selben Linie geh√∂ren). <br><br>  F√ºr jede Skala ist jede Merkmalszelle einem entsprechenden ‚ÄûSegment‚Äú zugeordnet.  F√ºr jedes Segment s <sup>(x, y, l)</sup> am Punkt (x, y) auf einer Skala l wird Folgendes trainiert: <br>  - p <sub>s</sub> ob das gegebene Segment Text ist; <br>  - x <sub>s</sub> , y <sub>s</sub> , w <sub>s</sub> , h <sub>s</sub> , Œ∏ <sub>s</sub> - der Versatz der Basiskoordinaten und der Neigungswinkel des Segments; <br>  - 8 Punkte f√ºr das Vorhandensein von ‚ÄûVerbindungen‚Äú mit Segmenten neben der l-ten Skala (L <sup>ws</sup> <sub>, s '</sub> , s' von {s <sup>(x ', y', l)</sup> } / s <sup>(x, y, l)</sup> , wobei x ‚Äì1 ‚â§ x '‚â§ x + 1, y - 1 ‚â§ y' ‚â§ y + 1); <br>  - 4 Punkte f√ºr das Vorhandensein von ‚ÄûVerbindungen‚Äú mit Segmenten neben der 1-Skala (L <sup>c</sup> <sub>s, s '</sub> , s' von {s <sup>(x ', y', 1-1)</sup> }, wobei 2x ‚â§ x '‚â§ 2x + 1 , 2y ‚â§ y '‚â§ 2y + 1) (was auf die Tatsache zur√ºckzuf√ºhren ist, dass sich die Dimension von Merkmalen auf benachbarten Ma√üst√§ben genau zweimal unterscheidet). <br><br><img src="https://habrastorage.org/webt/fn/ox/en/fnoxen3f1izpei9ecdbdl_eq2xk.png"><br><h5>  <sup><sub>Funktionsweise von SegLink Detector durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkennen von orientiertem Text in nat√ºrlichen Bildern durch Verkn√ºpfen von Segmenten</a></sub></sup> </h5><br>  Wenn wir nach solchen Vorhersagen als Eckpunkte alle Segmente nehmen, f√ºr die die Wahrscheinlichkeit, dass es sich um Text handelt, gr√∂√üer als die Schwelle Œ± ist, und als Kanten alle Bindungen, deren Wahrscheinlichkeit gr√∂√üer als die Schwelle Œ≤ ist, dann bilden die Segmente verbundene Komponenten, von denen jede eine Textzeile beschreibt . <br><br>  Das resultierende Modell weist eine <b>hohe Verallgemeinerungsf√§higkeit auf</b> : Selbst wenn es in den ersten Ans√§tzen mit russischen und englischen Daten geschult wurde, fand es qualitativ chinesischen und arabischen Text. <br><br><h3>  Zehn Skripte </h3><br>  Wenn wir zur Erkennung ein Modell erstellen konnten, das sofort f√ºr alle Sprachen funktioniert, dann ist es f√ºr die Erkennung der gefundenen Linien viel schwieriger, ein solches Modell zu erhalten.  Aus diesem Grund haben wir beschlossen, <b>f√ºr jedes Skript</b> ein <b>eigenes Modell zu verwenden</b> (kyrillisch, lateinisch, arabisch, hebr√§isch, griechisch, armenisch, georgisch, koreanisch, thail√§ndisch).  Aufgrund der gro√üen Schnittmenge in Hieroglyphen wird f√ºr Chinesisch und Japanisch ein separates allgemeines Modell verwendet. <br><br>  Das Modell, das dem gesamten Skript gemeinsam ist, unterscheidet sich vom separaten Modell f√ºr jede Sprache um weniger als 1 S.p.  Qualit√§t.  Gleichzeitig ist die Erstellung und Implementierung eines Modells einfacher als beispielsweise 25 Modelle (die Anzahl der von unserem Modell unterst√ºtzten lateinischen Sprachen).  Aufgrund der h√§ufigen Pr√§senz von Englisch in allen Sprachen k√∂nnen alle unsere Modelle zus√§tzlich zur Hauptsprache auch lateinische Zeichen vorhersagen. <br><br>  Um zu verstehen, welches Modell f√ºr die Erkennung verwendet werden soll, bestimmen wir zun√§chst, ob die empfangenen Zeilen zu einem der 10 zur Erkennung verf√ºgbaren Skripte geh√∂ren. <br><br>  Es sollte separat angemerkt werden, dass es nicht immer m√∂glich ist, sein Skript eindeutig entlang der Linie zu bestimmen.  Beispielsweise sind in vielen Skripten Zahlen oder einzelne lateinische Zeichen enthalten, sodass eine der Ausgabeklassen des Modells ein "undefiniertes" Skript ist. <br><br><h3>  Skriptdefinition </h3><br>  Um das Skript zu definieren, haben wir einen separaten Klassifikator erstellt.  Das Definieren eines Skripts ist viel einfacher als das Erkennen, und das neuronale Netzwerk kann auf einfache Weise auf synthetische Daten umgeschult werden.  Daher wurde in unseren Experimenten eine signifikante Verbesserung der Qualit√§t des Modells durch <b>Vortraining zum Problem der Zeichenkettenerkennung erzielt</b> .  Dazu haben wir das Netzwerk zun√§chst auf das Erkennungsproblem f√ºr alle verf√ºgbaren Sprachen geschult.  Danach wurde das resultierende Backbone verwendet, um das Modell f√ºr die Skriptklassifizierungsaufgabe zu initialisieren. <br><br>  W√§hrend ein Skript in einer einzelnen Zeile oft ziemlich laut ist, enth√§lt das gesamte Bild meistens Text in einer Sprache, entweder zus√§tzlich zu dem mit Englisch durchsetzten Haupttext (oder im Fall unserer russischen Benutzer).  Um <b>die</b> Stabilit√§t zu <b>erh√∂hen</b> , aggregieren wir daher die Vorhersagen der Linien aus dem Bild, um eine stabilere Vorhersage des Bildskripts zu erhalten.  Zeilen mit einer vorhergesagten Klasse von "unbestimmt" werden bei der Aggregation nicht ber√ºcksichtigt. <br><br><h3>  Linienerkennung </h3><br>  Wenn wir im n√§chsten Schritt die Position jeder Zeile und ihres Skripts bereits bestimmt haben, m√ºssen wir <b>die Zeichenfolge anhand des angegebenen Skripts erkennen</b> , dh anhand der Pixelfolge, um die Zeichenfolge vorherzusagen.  Nach vielen Experimenten sind wir zu folgendem aufmerksamkeitsbasiertem Modell von sequence2sequence gekommen: <br><br><img src="https://habrastorage.org/webt/_0/6k/sf/_06ksfdetbjobwudopmi4xq0j4c.png"><br><br>  Wenn Sie CNN + BiLSTM im Encoder verwenden, k√∂nnen Sie Zeichen abrufen, die sowohl lokale als auch globale Kontexte erfassen.  F√ºr Text ist dies wichtig - oft wird er in einer Schriftart geschrieben (das Unterscheiden √§hnlicher Buchstaben mit Schriftinformationen ist viel einfacher).  Um zwei Buchstaben, die mit einem Leerzeichen geschrieben sind, von aufeinanderfolgenden Buchstaben zu unterscheiden, werden auch globale Statistiken f√ºr die Zeile ben√∂tigt. <br><br>  <b>Eine interessante Beobachtung</b> : Im resultierenden Modell k√∂nnen die Ausgaben der Aufmerksamkeitsmaske f√ºr ein bestimmtes Symbol verwendet werden, um dessen Position im Bild vorherzusagen. <br><br>  Dies hat uns dazu inspiriert, <b>die Aufmerksamkeit des Modells klar zu ‚Äûfokussieren‚Äú</b> .  Solche Ideen wurden auch in Artikeln gefunden - zum Beispiel im Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Focusing Attention: Towards Accurate Text Recognition in Natural Images‚Äú</a> . <br><br>  Da der Aufmerksamkeitsmechanismus eine Wahrscheinlichkeitsverteilung √ºber den Merkmalsraum ergibt, erhalten wir den Teil der ‚ÄûAufmerksamkeit‚Äú, der sich direkt darauf konzentriert, wenn wir als zus√§tzlichen Verlust die Summe der Aufmerksamkeitsausgaben innerhalb der Maske nehmen, die dem in diesem Schritt vorhergesagten Buchstaben entsprechen. <br><br>  Durch die Einf√ºhrung von loss -log (‚àë <sub>i, j‚ààM <sub>t</sub></sub> Œ± <sub>i, j</sub> ), wobei M <sub>t</sub> die Maske des zehnten Buchstabens und Œ± die Ausgabe der Aufmerksamkeit ist, f√∂rdern wir die Aufmerksamkeit f√ºr die Fokussierung auf das gegebene Symbol und helfen damit Neuronale Netze lernen besser. <br><br>  Bei Trainingsbeispielen, bei denen die Position einzelner Zeichen unbekannt oder ungenau ist (nicht alle Trainingsdaten haben Markierungen auf der Ebene einzelner Zeichen, nicht W√∂rter), wurde dieser Begriff im endg√ºltigen Verlust nicht ber√ºcksichtigt. <br><br>  Ein weiteres nettes Feature: Mit dieser Architektur k√∂nnen Sie die <b>Erkennung von Linien von rechts nach links</b> ohne zus√§tzliche √Ñnderungen vorhersagen (was zum Beispiel f√ºr Sprachen wie Arabisch und Hebr√§isch wichtig ist).  Das Modell selbst beginnt von rechts nach links zu erkennen. <br><br><h3>  Schnelle und langsame Modelle </h3><br>  Dabei sind wir auf ein Problem gesto√üen: <b>Bei "hohen" Schriften</b> , dh vertikal verl√§ngerten Schriften, funktionierte das Modell schlecht.  Dies wurde durch die Tatsache verursacht, dass die Abmessung von Zeichen auf der Aufmerksamkeitsebene 8-mal kleiner ist als die Abmessung des Originalbilds, da die Architektur des Faltungsteils des Netzwerks schrittweise ver√§ndert wurde.  Die Positionen mehrerer benachbarter Zeichen im Quellbild k√∂nnen der Position desselben Merkmalsvektors entsprechen, was in solchen Beispielen zu Fehlern f√ºhren kann.  Der Einsatz von Architektur mit einer geringeren Einengung der Merkmalsdimension f√ºhrte zu einer Qualit√§tssteigerung, aber auch zu einer Verl√§ngerung der Bearbeitungszeit. <br><br>  Um dieses Problem zu l√∂sen und <b>die Verarbeitungszeit nicht zu verl√§ngern</b> , haben wir das Modell folgenderma√üen verfeinert: <br><br><img src="https://habrastorage.org/webt/po/pp/ob/poppobis-rbsdqtyrbgyaq8jzik.png"><br><br>  Wir haben sowohl ein schnelles Modell mit vielen Schritten als auch ein langsames mit weniger Schritten trainiert.  Auf der Ebene, auf der sich die Modellparameter zu unterscheiden begannen, wurde eine separate Netzwerkausgabe hinzugef√ºgt, die vorhersagte, welches Modell weniger Erkennungsfehler aufweisen w√ºrde.  Der Totalverlust des Modells setzte sich aus L <sub>klein</sub> + L <sub>gro√ü</sub> + L <sub>Qualit√§t zusammen</sub> .  Auf der Zwischenebene hat das Modell also gelernt, die ‚ÄûKomplexit√§t‚Äú dieses Beispiels zu bestimmen.  In der Anwendungsphase wurden au√üerdem der allgemeine Teil und die Vorhersage der ‚ÄûKomplexit√§t‚Äú des Beispiels f√ºr alle Linien ber√ºcksichtigt, und je nach Ausgabe wurde in Zukunft je nach Schwellenwert entweder ein schnelles oder ein langsames Modell verwendet.  Dies erm√∂glichte es uns, eine Qualit√§t zu erzielen, die sich kaum von der eines langen Modells unterscheidet, w√§hrend die Geschwindigkeit nur um 5% anstatt der gesch√§tzten 30% zunahm. <br><br><h3>  Trainingsdaten </h3><br>  Ein wichtiger Schritt bei der Erstellung eines qualitativ hochwertigen Modells ist die Vorbereitung eines gro√üen und abwechslungsreichen Trainingsmusters.  Die "synthetische" Natur des Textes erm√∂glicht es, gro√üe Mengen von Beispielen zu generieren und mit realen Daten gute Ergebnisse zu erzielen. <br><br>  Nach dem ersten Ansatz zur Generierung synthetischer Daten haben wir die Ergebnisse des erhaltenen Modells sorgf√§ltig gepr√ºft und festgestellt, dass das Modell einzelne Buchstaben "I" aufgrund der Verzerrung in den Texten, die zur Erstellung des Trainingssatzes verwendet wurden, nicht gut erkennt.  Aus diesem Grund haben wir eine <b>Reihe von ‚Äûproblematischen‚Äú Beispielen</b> generiert, und als wir sie zu den urspr√ºnglichen Daten des Modells hinzuf√ºgten, erh√∂hte sich die Qualit√§t erheblich.  Wir haben diesen Vorgang viele Male wiederholt und dabei immer komplexere Schichten hinzugef√ºgt, bei denen wir die Erkennungsqualit√§t verbessern wollten. <br><br>  Wichtig ist, dass die generierten <b>Daten vielf√§ltig und realistisch sind</b> .  Wenn Sie m√∂chten, dass das Modell Fotos von Text auf Papierb√∂gen bearbeitet und der gesamte synthetische Datensatz Text enth√§lt, der √ºber Landschaften geschrieben wurde, funktioniert dies m√∂glicherweise nicht. <br><br>  Ein weiterer wichtiger Schritt besteht darin, die Beispiele zu trainieren, bei denen die aktuelle Erkennung falsch ist.  Wenn es eine gro√üe Anzahl von Bildern gibt, f√ºr die es kein Markup gibt, k√∂nnen Sie die Ausgaben des aktuellen Erkennungssystems, bei denen sie sich nicht sicher ist, nur markieren, wodurch die Kosten f√ºr das Markup reduziert werden. <br><br>  F√ºr komplexe Beispiele haben wir die Nutzer des Yandex.Tolok-Dienstes gebeten, gegen eine Geb√ºhr <b>Bilder einer bestimmten ‚Äûkomplexen‚Äú Gruppe</b> zu fotografieren und uns zu senden - zum Beispiel Fotos von Warenpaketen: <br><br><img src="https://habrastorage.org/webt/tm/zx/0k/tmzx0kmyswtdxz6u_ri_yfxdrzy.png" width="50%"><img src="https://habrastorage.org/webt/n9/pb/ru/n9pbrufm0gcwp8lggc9bk9kxaxe.png" width="50%"><br><br><h3>  Qualit√§t der Arbeit an "komplexen" Daten </h3><br>  Wir m√∂chten unseren Nutzern die M√∂glichkeit geben, mit Fotos jeglicher Komplexit√§t zu arbeiten, da es erforderlich sein kann, Texte nicht nur auf der Seite eines Buches oder eines gescannten Dokuments, sondern auch auf einem Stra√üenschild, einer Werbung oder einer Produktverpackung zu erkennen oder zu √ºbersetzen.  W√§hrend wir die hohe Qualit√§t der Arbeit am Fluss von B√ºchern und Dokumenten aufrechterhalten (wir werden diesem Thema eine eigene Geschichte widmen), widmen wir daher ‚Äûkomplexen Bilds√§tzen‚Äú besondere Aufmerksamkeit. <br><br>  Auf die oben beschriebene Weise haben wir eine Reihe von Bildern zusammengestellt, die Text in freier Wildbahn enthalten und f√ºr unsere Benutzer n√ºtzlich sein k√∂nnen: Fotografien von Schildern, Ank√ºndigungen, Tafeln, Buchumschl√§gen, Texten auf Haushaltsger√§ten, Kleidung und Gegenst√§nden.  An diesem Datensatz (auf den unten verwiesen wird) haben wir die Qualit√§t unseres Algorithmus bewertet. <br><br>  Als Vergleichsma√ü haben wir das Standardma√ü f√ºr die Genauigkeit und Vollst√§ndigkeit der Worterkennung im Datensatz sowie das F-Ma√ü verwendet.  Ein erkanntes Wort gilt als richtig gefunden, wenn seine Koordinaten mit den Koordinaten des markierten Wortes (IoU&gt; 0.3) √ºbereinstimmen und die Erkennung mit der genau zum Fall markierten √ºbereinstimmt.  Zahlen zum resultierenden Datensatz: <br><div class="scrollable-table"><table><tbody><tr><td>  Erkennungssystem </td><td>  Vollst√§ndigkeit </td><td>  Genauigkeit </td><td>  F-Ma√ü </td></tr><tr><td>  Yandex Vision </td><td>  73,99 </td><td>  86,57 </td><td>  79,79 </td></tr></tbody></table></div><br>  Datensatz, Metriken und Skripte zur Reproduktion der Ergebnisse finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  Upd.  Freunde, die unsere Technologie mit einer √§hnlichen L√∂sung von Abbyy verglichen haben, sorgten f√ºr gro√üe Kontroversen.  Wir respektieren die Meinungen der Community und der Branchenkollegen.  Gleichzeitig sind wir zuversichtlich in unsere Ergebnisse und haben uns daher f√ºr diesen Weg entschieden: Wir werden die Ergebnisse anderer Produkte aus dem Vergleich entfernen, die Testmethode erneut mit ihnen besprechen und zu den Ergebnissen zur√ºckkehren, in denen wir eine allgemeine Einigung erzielen. <br><br><h3>  N√§chste Schritte </h3><br>  An der Schnittstelle zwischen einzelnen Schritten wie Erkennung und Erkennung treten immer wieder Probleme auf: Kleinste √Ñnderungen des Erkennungsmodells machen eine √Ñnderung des Erkennungsmodells erforderlich, sodass wir aktiv daran experimentieren, eine End-to-End-L√∂sung zu erstellen. <br><br>  Zus√§tzlich zu den bereits beschriebenen M√∂glichkeiten zur Verbesserung der Technologie werden wir eine Anleitung zur Analyse der Struktur des Dokuments entwickeln, die f√ºr das Extrahieren von Informationen von grundlegender Bedeutung ist und von den Benutzern nachgefragt wird. <br><br><h3>  Fazit </h3><br>  Benutzer sind bereits an praktische Technologien gew√∂hnt und schalten ohne zu z√∂gern die Kamera ein, zeigen auf das Ladenschild, das Men√º im Restaurant oder die Seite im Buch in einer Fremdsprache und erhalten schnell eine √úbersetzung.  Wir erkennen Texte in 45 Sprachen mit bew√§hrter Genauigkeit, und die M√∂glichkeiten werden sich nur erweitern.  Eine Reihe von Tools in Yandex.Cloud erm√∂glicht es jedem, die bew√§hrten Methoden zu nutzen, die Yandex seit langer Zeit f√ºr sich verwendet. <br><br>  Heute k√∂nnen Sie die fertige Technologie einfach in Ihre eigene Anwendung integrieren und verwenden, um neue Produkte zu erstellen und Ihre eigenen Prozesse zu automatisieren.  Die Dokumentation zu unserer OCR finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  Was zu lesen: <br><br><ol><li><a name="karatzas1"></a>  D. Karatzas, SR Mestre, J. Mas, F. Nourbakhsh und PP Roy, ‚ÄûICDAR 2011 - Robuster Lesewettbewerb - Herausforderung 1: Lesen von Text in geborenen digitalen Bildern (Web und E-Mail)‚Äú in Document Analysis and Recognition (ICDAR) ), 2011 Internationale Konferenz √ºber.  IEEE, 2011, pp.  1485-1490. </li><li><a name="karatzas2"></a>  Karatzas D. et al.  ICDAR 2015-Wettbewerb f√ºr robustes Lesen // 2015 13. Internationale Konferenz f√ºr Dokumentenanalyse und -erkennung (ICDAR).  - IEEE, 2015 - S. 1156-1160. </li><li><a name="chng"></a>  Chee-Kheng Chng et.  al.  ICDAR2019 Robust Reading Challenge f√ºr beliebig geformten Text (RRC-ArT) [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv: 1909.07145v1</a> ] </li><li><a name="icdar2019"></a>  ICDAR 2019 Robust Reading Challenge f√ºr gescannte Belege OCR und Informationsextraktion <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">rrc.cvc.uab.es/?ch=13</a> </li><li><a name="shopsign"></a>  ShopSign: Ein Textdatensatz mit verschiedenen Szenen chinesischer Ladenschilder in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stra√üenansichten</a> [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv: 1903.10412</a> ] </li><li><a name="seglink"></a>  Baoguang Shi, Xiang Bai und Serge Belongie erkennen orientierten Text in nat√ºrlichen Bildern durch Verkn√ºpfung von Segmenten [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arxiv: 1703.06520</a> ]. </li><li><a name="focusing"></a>  Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu und Shuigeng Zhou. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475956/">https://habr.com/ru/post/de475956/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475940/index.html">Vue Storefront: Zweiter Shell-Ansatz</a></li>
<li><a href="../de475942/index.html">Illustriertes OAuth- und OpenID Connect-Handbuch</a></li>
<li><a href="../de475944/index.html">Laufen ist ein idealer Sport f√ºr Fernarbeiter. Teil 2: Physik und Material</a></li>
<li><a href="../de475948/index.html">JH Rainwater "Wie man Katzen weidet" (Teil 2): ‚Äã‚ÄãAlles, was technisch zu meistern bleibt</a></li>
<li><a href="../de475950/index.html">Warum sollte sich ein Roboter darauf beschr√§nken, Golfb√§lle zu sammeln? Es gibt auch Tennis</a></li>
<li><a href="../de475958/index.html">Die Geschichte, wie sich das M√§dchen in der IT versammelt hat</a></li>
<li><a href="../de475960/index.html">AHURATUS Smart Home Sprachassistent</a></li>
<li><a href="../de475968/index.html">Interessante Neuigkeiten Vue 3</a></li>
<li><a href="../de475974/index.html">Wie wir im Zug einen Hackathon machten und was daraus wurde</a></li>
<li><a href="../de475978/index.html">Wof√ºr ist das Hauptquartier des Zuges?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>