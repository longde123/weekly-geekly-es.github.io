<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÜüèø ‚è´ üå©Ô∏è Aufbau einer fehlertoleranten IT-Infrastruktur. Teil 1 - Vorbereiten der Bereitstellung des oVirt 4.3-Clusters üö° ü¶Ñ üö§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Leser werden gebeten, sich mit den Prinzipien des Aufbaus einer fehlertoleranten Infrastruktur eines kleinen Unternehmens in einem Rechenzentrum v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aufbau einer fehlertoleranten IT-Infrastruktur. Teil 1 - Vorbereiten der Bereitstellung des oVirt 4.3-Clusters</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/483980/"><p>  Die Leser werden gebeten, sich mit den Prinzipien des Aufbaus einer fehlertoleranten Infrastruktur eines kleinen Unternehmens in einem Rechenzentrum vertraut zu machen, die in einer kurzen Artikelserie ausf√ºhrlich behandelt werden. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  Einleitung </h2><br><p>  Unter dem <strong>DPC</strong> (Data Processing Center) versteht man: </p><br><ul><li>  eigenes Rack in seinem ‚ÄûServerraum‚Äú auf dem Gebiet des Unternehmens, das die Mindestanforderungen f√ºr die Bereitstellung von Strom- und K√ºhlger√§ten sowie den Internetzugang √ºber zwei unabh√§ngige Anbieter erf√ºllt; </li><li>  gemieteten Rack mit eigener Ausr√ºstung in diesem Rechenzentrum befindet - die sogenannte  Kollokation, die dem Tier III- oder IV-Standard entspricht, eine zuverl√§ssige Stromversorgung und K√ºhlung gew√§hrleistet und einen fehlertoleranten Internetzugang bietet; </li><li>  vollst√§ndig geleaste Ger√§te in einem Tier III- oder IV-Rechenzentrum. </li></ul><br><p>  Welche Unterkunftsoption Sie w√§hlen sollten - in jedem Fall ist alles individuell und h√§ngt in der Regel von mehreren Hauptfaktoren ab: </p><br><ul><li>  Warum verf√ºgt das Unternehmen √ºber eine eigene IT-Infrastruktur? </li><li>  Was genau w√ºnscht sich das Unternehmen von der IT-Infrastruktur (Zuverl√§ssigkeit, Skalierbarkeit, Verwaltbarkeit usw.)? </li><li>  die H√∂he der Erstinvestition in die IT-Infrastruktur sowie die Art der Kosten - Kapital (dh Kauf Ihrer Ausr√ºstung) oder Betrieb (Ausr√ºstung wird normalerweise gemietet); </li><li>  Planungshorizont des Unternehmens </li></ul><br><p>  Es kann viel √ºber die Faktoren geschrieben werden, die die Entscheidung des Unternehmens beeinflussen, seine IT-Infrastruktur zu erstellen und zu nutzen. Unser Ziel ist es jedoch, in der Praxis zu zeigen, wie genau diese Infrastruktur so aufgebaut werden kann, dass sie sowohl fehlertolerant als auch kosteng√ºnstig ist - die Anschaffungskosten f√ºr kommerzielle Software senken oder sogar vermeiden. </p><br><p>  Wie die Praxis zeigt, lohnt es sich nicht, Hardware einzusparen, da sich das Geizhals doppelt und noch viel mehr auszahlt.  Aber andererseits - gute Hardware, dies ist nur eine Empfehlung, und letztendlich h√§ngt es von den F√§higkeiten des Unternehmens und der "Gier" seines Managements ab, was genau zu kaufen ist und wie viel es kostet.  Dar√ºber hinaus ist das Wort "Gier" im wahrsten Sinne des Wortes zu verstehen, da es besser ist, in der Anfangsphase in Eisen zu investieren, damit es sp√§ter keine ernsthaften Probleme bei der weiteren Unterst√ºtzung und Skalierung gibt, da anf√§nglich falsche Planung und √ºberm√§√üige Einsparungen zur Zukunft f√ºhren k√∂nnen teurer als zu Beginn eines Projekts. </p><br><p>  Also, die anf√§nglichen Daten f√ºr das Projekt: </p><br><ul><li>  Es gibt ein Unternehmen, das beschlossen hat, ein eigenes Webportal einzurichten und seine Aktivit√§ten im Internet zu platzieren. </li><li>  Das Unternehmen hat beschlossen, ein Rack zu mieten, um seine Ger√§te in einem guten Rechenzentrum unterzubringen, das nach dem Tier III-Standard zertifiziert ist. </li><li>  Das Unternehmen entschied sich, nicht viel an Hardware zu sparen und kaufte daher die folgenden Ger√§te mit erweiterten Garantien und Support: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Ausr√ºstungsliste</b> <div class="spoiler_text"><blockquote><ul><li>  zwei physische Dell PowerEdge R640-Server wie folgt: </li><li>  <em>zwei Intel Xeon Gold 5120 Prozessoren</em> </li><li>  <em>512 GB RAM</em> </li><li>  <em>zwei SAS-Festplatten in RAID1 f√ºr die Installation des Betriebssystems</em> </li><li>  <em>Eingebaute 1G-Netzwerkkarte mit 4 Ports</em> </li><li>  <em>zwei 10G-Netzwerkkarten mit 2 Ports</em> </li><li>  <em>ein 2-Port FC HBA 16G.</em> </li><li>  2-Controller-Speichersystem Dell MD3820f, das √ºber FC 16G direkt mit Dell-Hosts verbunden ist; </li><li>  zwei Switches der zweiten Ebene - Cisco WS-C2960RX-48FPS-L gestapelt; </li><li> zwei Layer 3-Switches - Cisco WS-C3850-24T-E, gestapelt; </li><li>  Rack-, USV-, PDU- und Konsolenserver - werden vom Rechenzentrum bereitgestellt. </li></ul><br></blockquote></div></div><br><p>  Wie wir sehen k√∂nnen, haben die vorhandenen Ger√§te gute Aussichten f√ºr eine horizontale und vertikale Skalierung, wenn das Unternehmen im Internet mit anderen Unternehmen mit √§hnlichem Profil konkurrieren kann und anf√§ngt, einen Gewinn zu erzielen, der in die Erweiterung der Ressourcen f√ºr weiteren Wettbewerb und Gewinnwachstum investiert werden kann. </p><br><p>  Welche Ausr√ºstung k√∂nnen wir hinzuf√ºgen, wenn das Unternehmen beschlie√üt, die Leistung unseres Computerclusters zu steigern: </p><br><ul><li>  Wir verf√ºgen √ºber eine gro√üe Reserve f√ºr die Anzahl der Ports von 2960X-Switches, sodass Sie weitere Hardware-Server hinzuf√ºgen k√∂nnen. </li><li>  Kaufen Sie zwei FC-Switches, um Speichersysteme und zus√§tzliche Server mit ihnen zu verbinden. </li><li>  Bereits vorhandene Server k√∂nnen aktualisiert werden. F√ºgen Sie Speicher hinzu, ersetzen Sie Prozessoren durch effizientere, und schlie√üen Sie vorhandene Netzwerkadapter an ein 10G-Netzwerk an. </li><li>  Sie k√∂nnen dem Speicher zus√§tzliche Festplattenregale mit dem erforderlichen Festplattentyp hinzuf√ºgen - SAS, SATA oder SSD, je nach geplanter Belastung. </li><li>  Nachdem Sie FC-Switches hinzugef√ºgt haben, k√∂nnen Sie ein anderes Speichersystem erwerben, um die Festplattenkapazit√§t zu erh√∂hen. Wenn Sie die spezielle Remote Replication-Option dazu erwerben, k√∂nnen Sie die Datenreplikation zwischen Speichersystemen innerhalb desselben Rechenzentrums und zwischen Rechenzentren konfigurieren (dies geht jedoch bereits dar√ºber hinaus) den Umfang des Artikels); </li><li>  Es gibt auch Switches der dritten Ebene - den Cisco 3850, der als fehlertoleranter Kern des Netzwerks f√ºr das Hochgeschwindigkeitsrouting zwischen internen Netzwerken verwendet werden kann.  Dies wird in Zukunft sehr hilfreich sein, da die interne Infrastruktur w√§chst.  Der 3850 verf√ºgt auch √ºber 10G-Ports, die sp√§ter beim Upgrade von Netzwerkger√§ten auf 10G aktiviert werden k√∂nnen. </li></ul><br><p>  Da es jetzt nirgendwo ohne Virtualisierung gibt, sind wir nat√ºrlich im Trend, zumal dies eine gro√üartige M√∂glichkeit ist, die Anschaffungskosten f√ºr teure Server f√ºr bestimmte Infrastrukturelemente (Webserver, Datenbanken usw.) zu senken, die nicht immer optimal sind Wird bei geringer Auslastung verwendet und ist genau das, was zu Beginn des Projektstarts sein wird. </p><br><p>  Dar√ºber hinaus bietet die Virtualisierung viele weitere Vorteile, die f√ºr uns von gro√üem Nutzen sein k√∂nnen: VM-Fehlertoleranz bei Hardwareserverausf√§llen, Live-Migration zwischen Hardwareknoten des Clusters f√ºr deren Wartung, manueller oder automatischer Lastausgleich zwischen Clusterknoten usw. </p><br><p>  F√ºr die vom Unternehmen erworbene Hardware bietet sich die Bereitstellung des hoch zug√§nglichen VMware vSphere-Clusters an. Da jedoch jede Software von VMware f√ºr ihre <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>Pferdepreise bekannt</strong></a> ist, verwenden wir eine absolut kostenlose Virtualisierungsverwaltungssoftware - <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>oVirt</strong></a> , auf deren Grundlage die bekannten aber bereits kommerzielles Produkt - <a href="https://ru.bmstu.wiki/RHEV_(Red_Hat_Enterprise_Virtualization)"><strong>RHEV</strong></a> . </p><br><p>  Die <strong>oVirt-</strong> Software ist erforderlich, um alle Infrastrukturelemente zusammenzuf√ºhren, damit Sie bequem mit hoch zug√§nglichen virtuellen Maschinen arbeiten k√∂nnen - dies sind Datenbanken, Webanwendungen, Proxies, Balancer, Server zum Sammeln von Protokollen und Analysen usw. n., das ist, woraus das Webportal unseres Unternehmens besteht. </p><br><p>  Zusammenfassend erwarten uns die folgenden Artikel, die in der Praxis zeigen, wie die gesamte Hardware- und Software-Infrastruktur des Unternehmens bereitgestellt wird: </p><br><div class="spoiler">  <b class="spoiler_title">Liste der Artikel</b> <div class="spoiler_text"><ul><li>  <strong>Teil 1.</strong> Vorbereitung f√ºr die Bereitstellung des oVirt-Clusters 4.3. </li><li>  <strong>Teil 2.</strong> Installieren und Konfigurieren des Ovirt-Clusters 4.3. </li><li>  <strong>Teil 3.</strong> Organisation des fehlertoleranten Routings auf virtuellen VyOS-Routern. </li><li>  <strong>Teil 4.</strong> Konfigurieren des Cisco 3850-Stacks und Organisieren des Intranet-Routings. </li></ul></div></div><br><h2 id="chast-1-podgotovka-k-razvyortyvaniyu-klastera-ovirt-43">  Teil 1. Bereitstellen des oVirt 4.3-Clusters vorbereiten </h2><br><h3 id="bazovaya-nastroyka-hostov">  Grundlegende Host-Konfiguration </h3><br><p>  Das Installieren und Konfigurieren des Betriebssystems ist der einfachste Schritt.  Es gibt viele Artikel √ºber die ordnungsgem√§√üe Installation und Konfiguration des Betriebssystems. Es macht also keinen Sinn, etwas Exklusives dar√ºber zu ver√∂ffentlichen. </p><br><p>  Wir haben also zwei Dell PowerEdge R640-Hosts, auf denen Sie das Betriebssystem installieren und Vorkonfigurationen vornehmen m√ºssen, um sie als Hypervisoren f√ºr die Ausf√ºhrung virtueller Maschinen im oVirt 4.3-Cluster zu verwenden. </p><br><p>  Da wir die kostenlose, nichtkommerzielle Software oVirt verwenden m√∂chten, wurde <strong>CentOS 7.7</strong> f√ºr die Bereitstellung von Hosts ausgew√§hlt, obwohl f√ºr oVirt auch andere Betriebssysteme auf Hosts installiert werden k√∂nnen: </p><br><ul><li>  Sonderbau basierend auf RHEL, dem sogenannten  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html"><strong>oVirt Node</strong></a> ; </li><li>  OS Oracle Linux hat im Sommer 2019 die Unterst√ºtzung f√ºr den Betrieb von oVirt auf diesem Betriebssystem <a href="https://blogs.oracle.com/virtualization/announcing-oracle-linux-virtualization-manager">angek√ºndigt</a> . </li></ul><br><p>  Vor der Installation des Betriebssystems wird Folgendes empfohlen: </p><br><ul><li>  Konfigurieren Sie die iDRAC-Netzwerkschnittstelle auf beiden Hosts </li><li>  Aktualisieren Sie die Firmware f√ºr BIOS und iDRAC auf die neuesten Versionen. </li><li>  Es ist w√ºnschenswert, den Systemprofilserver im Leistungsmodus zu konfigurieren. </li><li>  Konfigurieren Sie RAID von lokalen Festplatten (RAID1 wird empfohlen), um das Betriebssystem auf dem Server zu installieren. </li></ul><br><p>  Anschlie√üend installieren wir das Betriebssystem auf der zuvor √ºber iDRAC erstellten Festplatte. Der Installationsvorgang ist normal, es sind keine besonderen Momente enthalten.  Der Zugriff auf die Serverkonsole zum Starten der Installation des Betriebssystems kann auch √ºber iDRAC erfolgen, obwohl nichts Sie daran hindert, den Monitor, die Tastatur und die Maus direkt mit dem Server zu verbinden und das Betriebssystem von einem Flash-Laufwerk aus zu installieren. </p><br><p>  Nehmen Sie nach der Installation des Betriebssystems die anf√§nglichen Einstellungen vor: </p><br><pre><code class="plaintext hljs">systemctl enable network.service systemctl start network.service systemctl status network.service</code> </pre> <br><pre> <code class="plaintext hljs">systemctl stop NetworkManager systemctl disable NetworkManager systemctl status NetworkManager</code> </pre> <br><pre> <code class="plaintext hljs">yum install -y ntp systemctl enable ntpd.service systemctl start ntpd.service</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysconfig/selinux SELINUX=disabled SELINUXTYPE=targeted</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysctl.conf vm.max_map_count = 262144 vm.swappiness = 1</code> </pre> <br><p>  <u>Installieren Sie die Basis-Software</u> </p><br><p>  F√ºr die Erstkonfiguration des Betriebssystems m√ºssen Sie eine Netzwerkschnittstelle auf dem Server konfigurieren, damit Sie auf das Internet zugreifen, das Betriebssystem aktualisieren und die erforderlichen Softwarepakete installieren k√∂nnen.  Dies kann sowohl w√§hrend der Installation des Betriebssystems als auch danach erfolgen. </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum update yum -y install bind-utils yum-utils net-tools git htop iotop nmon pciutils sysfsutils sysstat mc nc rsync wget traceroute gzip unzip telnet</code> </pre> <br><p>  Alle oben genannten Einstellungen und eine Reihe von Software sind eine Frage der pers√∂nlichen Pr√§ferenz, und diese Reihe ist nur eine Empfehlung. </p><br><p>  Da unser Host die Rolle eines Hypervisors spielt, aktivieren wir das gew√ºnschte Leistungsprofil: </p><br><pre> <code class="plaintext hljs">systemctl enable tuned systemctl start tuned systemctl status tuned</code> </pre> <br><pre> <code class="plaintext hljs">tuned-adm profile tuned-adm profile virtual-host</code> </pre> <br><p>  Weitere Informationen zum Leistungsprofil finden Sie hier: " <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-virtualization_tuning_optimization_guide-tuned">Kapitel 4. tuned und tuned-adm</a> ". </p><br><p>  Nach der Installation des Betriebssystems fahren wir mit dem n√§chsten Teil fort - der Konfiguration der Netzwerkschnittstellen auf den Hosts und dem Cisco 2960X-Switch-Stack. </p><br><h3 id="nastroyka-steka-kommutatorov-cisco-2960x">  Konfigurieren des Cisco 2960X-Switch-Stacks </h3><br><p>  In unserem Projekt wird die folgende Anzahl von VLANs oder Broadcast-Dom√§nen verwendet, die voneinander isoliert sind, um verschiedene Arten von Datenverkehr zu trennen: </p><br><p>  <strong>VLAN 10</strong> - Internet <br>  <strong>VLAN 17</strong> - Verwaltung (iDRAC, Speicher, Switch-Verwaltung) <br>  <strong>VLAN 32</strong> - VM-Produktionsnetzwerk <br>  <strong>VLAN 33</strong> - Verbindungsnetz (zu externen Auftragnehmern) <br>  <strong>VLAN 34</strong> - VM-Testnetzwerk <br>  <strong>VLAN 35</strong> - VM Developer Network <br>  <strong>VLAN 40</strong> - √úberwachungsnetzwerk </p><br><p>  Bevor wir mit der Arbeit beginnen, pr√§sentieren wir ein Diagramm auf L2-Ebene, zu dem wir schlie√ülich kommen sollten: </p><br><p><img src="https://habrastorage.org/webt/in/ak/8a/inak8aoyty8yujo2zagwdehdytg.jpeg"></p><br><p>  F√ºr die Netzwerkinteraktion zwischen den oVirt-Hosts und den virtuellen Maschinen sowie f√ºr die Verwaltung unseres Speichers m√ºssen Sie den Cisco 2960X-Switch-Stack konfigurieren. </p><br><p>  Dell-Hosts verf√ºgen √ºber integrierte 4-Port-Netzwerkkarten. Daher ist es ratsam, die Verbindung zum Cisco 2960X √ºber eine fehlertolerante Netzwerkverbindung zu organisieren, indem physische Netzwerkports in einer logischen Schnittstelle und das LACP-Protokoll (802.3ad) gruppiert werden: </p><br><ul><li>  Die ersten beiden Ports auf dem Host werden im Bonding-Modus konfiguriert und mit dem 2960X-Switch verbunden. Auf dieser logischen Schnittstelle wird eine <strong><em>Br√ºcke</em></strong> mit einer Adresse f√ºr die Verwaltung des Hosts, die √úberwachung und die Kommunikation mit anderen Hosts im Ovirt-Cluster konfiguriert und auch f√ºr die Livemigration virtueller Maschinen verwendet. </li><li>  Die zweiten beiden Ports auf dem Host werden ebenfalls im Bonding-Modus konfiguriert und mit dem 2960X verbunden. Auf dieser logischen Schnittstelle werden √ºber oVirt sp√§tere Bridges (in den entsprechenden VLANs) erstellt, mit denen virtuelle Maschinen verbunden werden. </li><li>  beide Netzwerkports innerhalb derselben logischen Schnittstelle sind aktiv, d. h.  Der Verkehr auf ihnen kann gleichzeitig im Ausgleichsmodus √ºbertragen werden. </li><li>  Die Netzwerkeinstellungen auf den Clusterknoten m√ºssen mit Ausnahme der IP-Adressen absolut GLEICH sein. </li></ul><br><p>  <u>Grundkonfiguration des <strong>2960X-</strong> Switch- <strong>Stacks</strong> und seiner Ports</u> </p><br><p>  Zuvor sollten unsere Schalter sein: </p><br><ul><li>  in einem Gestell montiert; </li><li>  verbunden durch zwei Spezialkabel der gew√ºnschten L√§nge, z. B. CAB-STK-E-1M; </li><li>  an die Stromversorgung angeschlossen; </li><li>  Zur Erstkonfiguration √ºber den Konsolenport mit der Workstation des Administrators verbunden. </li></ul><br><p>  Die dazu notwendigen Anleitungen finden Sie auf <a href="https://www.cisco.com/c/en/us/support/switches/catalyst-2960-x-series-switches/products-installation-guides-list.html">der offiziellen Seite des</a> Herstellers. </p><br><p>  Konfigurieren Sie die Switches, nachdem Sie die obigen Schritte ausgef√ºhrt haben. <br>  Was jedes Team bedeutet, soll im Rahmen dieses Artikels nicht entschl√ºsselt werden, bei Bedarf k√∂nnen alle Informationen unabh√§ngig gefunden werden. <br>  Unser Ziel ist es, den Switch-Stack so schnell wie m√∂glich zu konfigurieren und die Hosts und Speicherverwaltungsschnittstellen damit zu verbinden. </p><br><p>  1) Stellen Sie eine Verbindung zum Hauptschalter her, wechseln Sie in den privilegierten Modus, wechseln Sie in den Konfigurationsmodus und nehmen Sie grundlegende Einstellungen vor. </p><br><div class="spoiler">  <b class="spoiler_title">Grundlegende Switch-Konfiguration:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> enable configure terminal hostname 2960X no service pad service timestamps debug datetime msec service timestamps log datetime localtime show-timezone msec no service password-encryption service sequence-numbers switch 1 priority 15 switch 2 priority 14 stack-mac persistent timer 0 clock timezone MSK 3 vtp mode transparent ip subnet-zero vlan 17 name Management vlan 32 name PROD vlan 33 name Interconnect vlan 34 name Test vlan 35 name Dev vlan 40 name Monitoring spanning-tree mode rapid-pvst spanning-tree etherchannel guard misconfig spanning-tree portfast bpduguard default spanning-tree extend system-id spanning-tree vlan 1-40 root primary spanning-tree loopguard default vlan internal allocation policy ascending port-channel load-balance src-dst-ip errdisable recovery cause loopback errdisable recovery cause bpduguard errdisable recovery interval 60 line con 0 session-timeout 60 exec-timeout 60 0 logging synchronous line vty 5 15 session-timeout 60 exec-timeout 60 0 logging synchronous ip http server ip http secure-server no vstack interface Vlan1 no ip address shutdown exit</code> </pre></div></div><br><p>  Wir speichern die Konfiguration mit dem Befehl <strong>wr mem</strong> und laden den Switch-Stack mit dem Befehl <strong>reload</strong> auf dem Master-Switch 1 neu. </p><br><p>  2) Konfigurieren Sie die Netzwerkports des Switches im Zugriffsmodus in VLAN 17, um die Verwaltungsschnittstellen von Speicher- und iDRAC-Servern zu verbinden. </p><br><div class="spoiler">  <b class="spoiler_title">Verwaltungsport-Einstellungen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface GigabitEthernet1/0/5 description iDRAC - host1 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet1/0/6 description Storage1 - Cntr0/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/5 description iDRAC - host2 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/6 description Storage1 ‚Äì Cntr1/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge exit</code> </pre> </div></div><br><p>  3) √úberpr√ºfen Sie nach dem Neustart des Stapels, ob dieser ordnungsgem√§√ü funktioniert: </p><br><div class="spoiler">  <b class="spoiler_title">√úberpr√ºfen der Funktion des Stacks:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">2960X#show switch stack-ring speed Stack Ring Speed : 20G Stack Ring Configuration: Full Stack Ring Protocol : FlexStack 2960X#show switch stack-ports Switch # Port 1 Port 2 -------- ------ ------ 1 Ok Ok 2 Ok Ok 2960X#show switch neighbors Switch # Port 1 Port 2 -------- ------ ------ 1 2 2 2 1 1 2960X#show switch detail Switch/Stack Mac Address : 0cd0.f8e4. Mac persistency wait time: Indefinite H/W Current Switch# Role Mac Address Priority Version State ---------------------------------------------------------- *1 Master 0cd0.f8e4. 15 4 Ready 2 Member 0029.c251. 14 4 Ready Stack Port Status Neighbors Switch# Port 1 Port 2 Port 1 Port 2 -------------------------------------------------------- 1 Ok Ok 2 2 2 Ok Ok 1 1</code> </pre> </div></div><br><p>  4) Konfigurieren des SSH-Zugriffs auf den 2960X-Stack </p><br><p>  F√ºr das Remote-Stack-Management √ºber SSH verwenden wir IP 172.20.1.10, das auf SVI (Switch Virtual Interface) <strong>VLAN17 konfiguriert ist</strong> . </p><br><p>  Obwohl es f√ºr Verwaltungszwecke ratsam ist, einen speziellen dedizierten Port am Switch zu verwenden, ist dies eine Frage der pers√∂nlichen Pr√§ferenz und Gelegenheit. </p><br><div class="spoiler">  <b class="spoiler_title">Konfigurieren des SSH-Zugriffs auf den Switch-Stack:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ip default-gateway 172.20.1.2 interface vlan 17 ip address 172.20.1.10 255.255.255.0 hostname 2960X ip domain-name hw.home-lab.ru no ip domain-lookup clock set 12:47:04 06 Dec 2019 crypto key generate rsa ip ssh version 2 ip ssh time-out 90 line vty 0 4 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh line vty 5 15 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh aaa new-model aaa authentication login default local username cisco privilege 15 secret my_ssh_password</code> </pre> </div></div><br><p>  Konfigurieren Sie ein Kennwort, um in den privilegierten Modus zu wechseln: </p><br><pre> <code class="plaintext hljs">enable secret *myenablepassword* service password-encryption</code> </pre> <br><p>  NTP konfigurieren: </p><br><pre> <code class="plaintext hljs">ntp server 85.21.78.8 prefer ntp server 89.221.207.113 ntp server 185.22.60.71 ntp server 192.36.143.130 ntp server 185.209.85.222 show ntp status show ntp associations show clock detail</code> </pre> <br><p>  5) Konfigurieren Sie logische Etherchannel-Schnittstellen und physische Ports, die mit Hosts verbunden sind.  Zur Vereinfachung der Konfiguration sind alle verf√ºgbaren VLANs auf allen logischen Schnittstellen zul√§ssig. In der Regel wird jedoch empfohlen, nur das zu konfigurieren, was Sie ben√∂tigen: </p><br><div class="spoiler">  <b class="spoiler_title">Konfigurieren Sie Etherchannel-Schnittstellen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface Port-channel1 description EtherChannel with Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel2 description EtherChannel with Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel3 description EtherChannel with Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel4 description EtherChannel with Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface GigabitEthernet1/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet1/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet1/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet1/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active interface GigabitEthernet2/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet2/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet2/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet2/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active</code> </pre> </div></div><br><p>  <u>Erstkonfiguration von Netzwerkschnittstellen f√ºr virtuelle Maschinen auf <strong>Host1</strong> und <strong>Host2</strong></u> </p><br><p>  Wir pr√ºfen die Verf√ºgbarkeit der notwendigen Module f√ºr das Bonden im System, installieren das Modul zur Verwaltung von Bridges: </p><br><pre> <code class="plaintext hljs">modinfo bonding modinfo 8021q yum install bridge-utils</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Konfigurieren der logischen BOND1-Schnittstelle f√ºr virtuelle Maschinen auf Hosts und deren physischen Schnittstellen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1 #DESCRIPTION - management DEVICE=bond1 NAME=bond1 TYPE=Bond IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em2 #DESCRIPTION - management DEVICE=em2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em3 #DESCRIPTION - management DEVICE=em3 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Nach Abschluss der Einstellungen auf dem <strong>2960X-</strong> Stack und den Hosts starten wir das Netzwerk auf den Hosts neu und √ºberpr√ºfen die logische Schnittstelle. </p><br><ul><li>  auf Host: </li></ul><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: IEEE 802.3ad Dynamic link aggregation Transmit Hash Policy: layer2+3 (2) MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 ... 802.3ad info LACP rate: fast Min links: 0 Aggregator selection policy (ad_select): stable System priority: 65535 ... Slave Interface: em2 MII Status: up Speed: 1000 Mbps Duplex: full ... Slave Interface: em3 MII Status: up Speed: 1000 Mbps Duplex: full</code> </pre> <br><ul><li>  auf dem <strong>2960X-</strong> Switch- <strong>Stack</strong> : </li></ul><br><pre> <code class="plaintext hljs">2960X#show lacp internal Flags: S - Device is requesting Slow LACPDUs F - Device is requesting Fast LACPDUs A - Device is in Active mode P - Device is in Passive mode Channel group 1 LACP port Admin Oper Port Port Port Flags State Priority Key Key Number State Gi1/0/1 SA bndl 32768 0x1 0x1 0x102 0x3D Gi2/0/1 SA bndl 32768 0x1 0x1 0x202 0x3D 2960X#sh etherchannel summary Flags: D - down P - bundled in port-channel I - stand-alone s - suspended H - Hot-standby (LACP only) R - Layer3 S - Layer2 U - in use N - not in use, no aggregation f - failed to allocate aggregator M - not in use, minimum links not met m - not in use, port not aggregated due to minimum links not met u - unsuitable for bundling w - waiting to be aggregated d - default port A - formed by Auto LAG Number of channel-groups in use: 11 Number of aggregators: 11 Group Port-channel Protocol Ports ------+-------------+-----------+----------------------------------------------- 1 Po1(SU) LACP Gi1/0/1(P) Gi2/0/1(P)</code> </pre> <br><p>  Erstkonfiguration von Netzwerkschnittstellen zum Verwalten von <strong>Clusterressourcen</strong> auf <strong>Host1</strong> und <strong>Host2</strong> </p><br><div class="spoiler">  <b class="spoiler_title">Konfigurieren der logischen BOND1-Schnittstelle f√ºr die Verwaltung und ihrer physischen Schnittstellen auf den Hosts:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond0 #DESCRIPTION - management DEVICE=bond0 NAME=bond0 TYPE=Bond BONDING_MASTER=yes IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em0 #DESCRIPTION - management DEVICE=em0 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em1 #DESCRIPTION - management DEVICE=em1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Nach Abschluss der Einstellungen auf dem <strong>2960X-</strong> Stack und den Hosts starten wir das Netzwerk auf den Hosts neu und √ºberpr√ºfen die logische Schnittstelle. </p><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 2960X#show lacp internal 2960X#sh etherchannel summary</code> </pre> <br><p>  Wir konfigurieren die Steuerungsnetzwerkschnittstelle auf jedem Host in <strong>VLAN 17</strong> und binden sie an die logische Schnittstelle BOND1: </p><br><div class="spoiler">  <b class="spoiler_title">Konfigurieren von VLAN17 auf Host1:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.163 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Konfigurieren von VLAN17 auf Host2:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.164 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><p>  Wir starten das Netzwerk auf den Hosts neu und √ºberpr√ºfen ihre Sichtbarkeit untereinander. </p><br><p>  Damit ist die Einrichtung des Cisco 2960X-Switch-Stacks abgeschlossen. Wenn alles korrekt durchgef√ºhrt wurde, besteht nun eine Netzwerkverbindung aller Infrastrukturelemente auf L2-Ebene. </p><br><h3 id="nastroyka-shd-dell-md3820f">  Konfigurieren Sie den Dell MD3820f-Speicher </h3><br><p>  Bevor Sie mit dem Einrichten des Speichers beginnen, sollten Sie ihn bereits √ºber FC mit dem Cisco <strong>2960X-</strong> Switch-Stack <strong>mit</strong> Steuerungsschnittstellen sowie mit <strong>Host1-</strong> und <strong>Host2-Hosts verbinden</strong> . </p><br><p>  Das allgemeine Schema, wie Speicher mit dem Switch-Stack verbunden werden soll, wurde im vorherigen Kapitel beschrieben. </p><br><p>  Das Schema zum Verbinden des Speichers auf dem FC mit den Hosts sollte folgenderma√üen aussehen: </p><br><p><img src="https://habrastorage.org/webt/el/k8/5a/elk85aqc6ilmxeiowmh6acrkwri.jpeg"></p><br><p>  W√§hrend der Verbindung m√ºssen WWPN-Adressen f√ºr FC-HBA-Hosts aufgezeichnet werden, die mit FC-Ports auf dem Speichersystem verbunden sind. Dies ist f√ºr die anschlie√üende Konfiguration der Bindung von Hosts an LUNs auf dem Speichersystem erforderlich. </p><br><p>  Laden Sie auf der Arbeitsstation des Administrators das Speicherverwaltungsdienstprogramm Dell MD3820f - <strong>PowerVault Modular Disk Storage Manager</strong> ( <strong>MDSM</strong> ) herunter und installieren <strong>Sie es</strong> . <br>  Wir stellen eine Verbindung √ºber seine Standard-IP-Adressen her und konfigurieren dann unsere Adressen von <strong>VLAN17 aus</strong> , um die Controller √ºber TCP / IP zu steuern: </p><br><p>  <strong>Storage1</strong> : </p><br><pre> <code class="plaintext hljs">ControllerA IP - 172.20.1.13, MASK - 255.255.255.0, Gateway - 172.20.1.2 ControllerB IP - 172.20.1.14, MASK - 255.255.255.0, Gateway - 172.20.1.2</code> </pre> <br><p>  Rufen Sie nach dem Festlegen der Adressen die Speicherverwaltungsoberfl√§che auf und legen Sie ein Kennwort fest, stellen Sie die Uhrzeit ein, aktualisieren Sie die Firmware f√ºr Controller und Festplatten, falls erforderlich, usw. <br>  Wie dies geschieht, wird im <a href="https://www.dell.com/support/manuals/ru/ru/rubsdc/powervault-md3820f/mdseriesagpub/introduction%3Fguid%3Dguid-51208376-0df9-48a8-be66-63c1b06512ae%26lang%3Den-us">Handbuch zur</a> Speicherverwaltung beschrieben. </p><br><p>  Nachdem Sie die obigen Einstellungen vorgenommen haben, m√ºssen Sie nur noch ein paar Schritte ausf√ºhren: </p><br><ol><li>  Konfigurieren Sie die <strong>Host-</strong> FC- <strong>Kennungen</strong> . </li><li>  Erstellen Sie eine Hostgruppe - <strong>Hostgruppe</strong> und f√ºgen Sie unsere beiden Dell-Hosts hinzu. </li><li>  Erstellen Sie eine Datentr√§gergruppe und virtuelle Datentr√§ger (oder LUNs), die den Hosts angezeigt werden. </li><li>  Konfigurieren Sie die Pr√§sentation von virtuellen Laufwerken (oder LUNs) f√ºr Hosts. </li></ol><br><p>  Das Hinzuf√ºgen neuer Hosts und das Binden von IDs von Host-FC-Ports erfolgt √ºber das Men√º - <strong>Host-Zuordnungen</strong> -&gt; <strong>Definieren</strong> -&gt; <strong>Hosts ...</strong> <br>  WWPN-Adressen von FC-HBA-Hosts finden Sie beispielsweise in einem iDRAC-Server. </p><br><p>  Als Ergebnis sollten wir so etwas bekommen: </p><br><p><img src="https://habrastorage.org/webt/p_/uk/u4/p_uku4o-ewgqpyi0xudxqjesfjc.png"></p><br><p>  Das Hinzuf√ºgen einer neuen Hostgruppe und das Binden von Hosts erfolgt √ºber das Men√º - <strong>Hostzuordnungen</strong> -&gt; <strong>Definieren</strong> -&gt; <strong>Hostgruppe ...</strong> <br>  W√§hlen Sie f√ºr Hosts den Betriebssystemtyp - <strong><em>Linux (DM-MP)</em></strong> . </p><br><p>  Erstellen Sie nach dem Erstellen der Hostgruppe √ºber die Registerkarte <strong>Storage &amp; Copy Services</strong> eine Datentr√§gergruppe - <strong>Datentr√§gergruppe</strong> , deren Typ von den Fehlertoleranzanforderungen abh√§ngt, z. B. RAID10, und darin virtuelle Datentr√§ger mit der richtigen Gr√∂√üe: </p><br><p><img src="https://habrastorage.org/webt/ue/g2/kn/ueg2kn0m3usv1kb4tygvx2vajz0.png"></p><br><p>  Und schlie√ülich ist die letzte Phase die Pr√§sentation von virtuellen Laufwerken (oder LUNs) f√ºr Hosts. <br>  Dazu nehmen wir √ºber das Men√º - <strong>Host-</strong> <strong>Zuordnungen</strong> -&gt; <strong>Lun-Zuordnung</strong> -&gt; <strong>Hinzuf√ºgen ...</strong> die Bindung der virtuellen Festplatten an die Hosts vor und weisen ihnen Nummern zu. </p><br><p>  Alles sollte so aussehen wie in diesem Screenshot: </p><br><p><img src="https://habrastorage.org/webt/db/a-/xq/dba-xqbacgrjxrutupvhvmua7ro.png"></p><br><p>  Wir sind mit dem Einrichten des Speichersystems fertig. Wenn alles korrekt durchgef√ºhrt wurde, sollten die Hosts die LUNs sehen, die ihnen √ºber ihre FC-HBAs angezeigt werden. <br>  Veranlassen Sie das System, die Informationen zu den zugeordneten Laufwerken zu aktualisieren: </p><br><pre> <code class="plaintext hljs">ls -la /sys/class/scsi_host/ echo "- - -" &gt; /sys/class/scsi_host/host[0-9]/scan</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Mal sehen, welche Ger√§te auf unseren Servern sichtbar sind:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /proc/scsi/scsi Attached devices: Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: DELL Model: PERC H330 Mini Rev: 4.29 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 lsscsi [0:2:0:0] disk DELL PERC H330 Mini 4.29 /dev/sda [15:0:0:0] disk DELL MD38xxf 0825 - [15:0:0:1] disk DELL MD38xxf 0825 /dev/sdb [15:0:0:4] disk DELL MD38xxf 0825 /dev/sdc [15:0:0:11] disk DELL MD38xxf 0825 /dev/sdd [15:0:0:31] disk DELL Universal Xport 0825 - [18:0:0:0] disk DELL MD38xxf 0825 - [18:0:0:1] disk DELL MD38xxf 0825 /dev/sdi [18:0:0:4] disk DELL MD38xxf 0825 /dev/sdj [18:0:0:11] disk DELL MD38xxf 0825 /dev/sdk [18:0:0:31] disk DELL Universal Xport 0825 -</code> </pre> </div></div><br><p>  Sie k√∂nnen <strong>Multipath auch</strong> auf den Hosts konfigurieren. Bei der Installation von oVirt kann er dies jedoch selbst tun. Es ist jedoch besser, den MP im Voraus selbst zu √ºberpr√ºfen. </p><br><p>  <u>Installieren und konfigurieren Sie DM Multipath</u> </p><br><pre> <code class="plaintext hljs">yum install device-mapper-multipath mpathconf --enable --user_friendly_names y cat /etc/multipath.conf | egrep -v "^\s*(#|$)" defaults { user_friendly_names yes find_multipaths yes } blacklist { wwid 26353900f02796769 devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[az]" }</code> </pre> <br><p>  Installieren Sie den MP-Dienst im Autostart und f√ºhren Sie ihn aus: </p><br><pre> <code class="plaintext hljs">systemctl enable multipathd &amp;&amp; systemctl restart multipathd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Informationen √ºber geladene Module auf MP-Betrieb pr√ºfen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">lsmod | grep dm_multipath dm_multipath 27792 6 dm_service_time dm_mod 124407 139 dm_multipath,dm_log,dm_mirror modinfo dm_multipath filename: /lib/modules/3.10.0-957.12.2.el7.x86_64/kernel/drivers/md/dm-multipath.ko.xz license: GPL author: Sistina Software &lt;dm-devel@redhat.com&gt; description: device-mapper multipath target retpoline: Y rhelversion: 7.6 srcversion: 985A03DCAF053D4910E53EE depends: dm-mod intree: Y vermagic: 3.10.0-957.12.2.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: A3:2D:39:46:F2:D3:58:EA:52:30:1F:63:37:8A:37:A5:54:03:00:45 sig_hashalgo: sha256</code> </pre> </div></div><br><p>  Sehen Sie sich die Zusammenfassung der vorhandenen Multipath-Konfiguration an: </p><br><pre> <code class="plaintext hljs">mpathconf multipath is enabled find_multipaths is disabled user_friendly_names is disabled dm_multipath module is loaded multipathd is running</code> </pre> <br><p>  Nachdem Sie dem Speicher eine neue LUN hinzugef√ºgt und dem Host pr√§sentiert haben, m√ºssen Sie diese scannen, die mit dem HBA-Host verbunden ist. </p><br><pre> <code class="plaintext hljs">systemctl reload multipathd multipath -v2</code> </pre> <br><p>  Zuletzt pr√ºfen wir, ob alle LUNs auf dem Speicher f√ºr Hosts vorhanden sind und ob alle zwei Pfade haben. </p><br><div class="spoiler">  <b class="spoiler_title">MP-Betrieb pr√ºfen:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">multipath -ll 3600a098000e4b4b3000003175cec1840 dm-2 DELL ,MD38xxf size=2.0T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:1 sdb 8:16 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:1 sdi 8:128 active ready running 3600a098000e4b48f000002ab5cec1921 dm-6 DELL ,MD38xxf size=10T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 18:0:0:11 sdk 8:160 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 15:0:0:11 sdd 8:48 active ready running 3600a098000e4b4b3000003c95d171065 dm-3 DELL ,MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> </div></div><br><p>  Wie Sie sehen, sind alle drei virtuellen Laufwerke im Speichersystem auf zwei Arten sichtbar.  Damit sind alle Vorarbeiten abgeschlossen, sodass wir mit dem Hauptteil fortfahren k√∂nnen - dem Aufbau des OVIRT-Clusters, der im n√§chsten Artikel behandelt wird. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de483980/">https://habr.com/ru/post/de483980/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de483964/index.html">Haben Sie keine Angst vor JSON oder Ihrer ersten API-Anwendung</a></li>
<li><a href="../de483972/index.html">So f√∂rdern Sie mit Quora Ihr Gesch√§ft</a></li>
<li><a href="../de483974/index.html">Ceph via iSCSI - oder Skifahren in der H√§ngematte</a></li>
<li><a href="../de483976/index.html">2020 Cybersicherheit und Bedrohungen: Was uns nach den Ferien erwartet</a></li>
<li><a href="../de483978/index.html">Verst√§ndnis des Konzepts der modernen Web-App-Entwicklung im Jahr 2020</a></li>
<li><a href="../de483986/index.html">Roboter Gedanken mit Emotiv Insight steuern</a></li>
<li><a href="../de483988/index.html">MicroSPA oder wie man ein Vierkantrad erfindet</a></li>
<li><a href="../de483992/index.html">Warum fressen einige Planeten ihren Himmel?</a></li>
<li><a href="../de483994/index.html">IT Umzug auf eine Yacht. Von Schweden nach Spanien</a></li>
<li><a href="../de484004/index.html">@ Pythonetc Dezember 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>