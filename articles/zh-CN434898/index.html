<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⚙️ 💆 🧐 Grasp2Vec：通过自学捕获来学习表示对象 📧 👨‍🚀 😳</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="尽管没有特别的教导，但是年龄惊人的人们已经能够识别并收藏他们喜欢的物品。 根据对认知能力发展的研究，与我们周围世界的物体进行交互的可能性在诸如感知和操纵物体（例如目标捕获）之类的能力的发展中起着至关重要的作用。 与外界互动时，人们可以通过纠正自己的错误来学习：我们知道我们做了什么，并从结果中学到东西...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grasp2Vec：通过自学捕获来学习表示对象</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434898/"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br> 尽管没有特别的教导，但是年龄惊人的人们已经能够识别并收藏他们喜欢的物品。 根据对认知能力发展的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">研究，</a>与我们周围世界的物体进行交互的可能性在诸如感知和操纵物体（例如目标捕获）之类的能力的发展中起着至关重要的作用。 与外界互动时，人们可以通过纠正自己的错误来学习：我们知道我们做了什么，并从结果中学到东西。 在机器人技术中，人们积极地研究了这种具有错误自我纠正功能的训练方法，因为它可以使机器人系统无需大量的训练数据或手动调整即可学习。 <br><br>  Google受到<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">对象持久性概念的</a>启发，提供了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Grasp2Vec</a>系统-一种构造对象表示的简单而有效的算法。  Grasp2Vec基于一种直观的理解，即尝试举起任何物体都会为我们提供一些信息-如果机器人抓住并拾取了该物体，则该物体需要在该位置之前被捕获。 另外，机器人知道如果捕获的对象在其捕获中，则意味着该对象不再位于其所在的位置。 使用这种形式的自学习，机器人可以学习识别由于捕获对象后场景中的视觉变化而引起的对象。 <br><a name="habracut"></a><br> 基于我们<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">与X Robotics</a>的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">合作</a> ，在该培训中，仅使用一台摄像机作为输入数据源，就同时培训了多个机器人来捕获家用物体，我们使用机器人捕获来“无意间”捕获物体，这种经验使我们对物体有了丰富的认识。 当机器人手臂可以按需举起物体时，这种想法已经可以用来获得“有意捕获”的能力。 <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2> 创建感知奖励功能 </h2><br> 在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">强化学习</a>平台上<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">，</a>任务<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">的</a>成功是通过奖励函数来衡量的。 通过最大化奖励，机器人可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">从头开始</a>学习各种捕捉技能。 当可以通过简单的传感器读数来衡量成功时，创建奖励功能很容易。 一个简单的示例是一个按钮，通过单击该按钮可以将奖励<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">直接</a>转移<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">到机器人的输入</a> 。 <br><br> 但是，当成功的标准取决于对任务的感知理解时，创建奖励功能就复杂得多。 在一个示例中考虑捕获问题，在该示例中，为机器人提供了捕获中保留的所需对象的图像。 机器人尝试捕获对象后，将检查捕获的内容。 此任务的奖励功能取决于对模式识别问题的答案：对象是否重合？ <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>在左侧，手柄握住画笔，在背景中可见多个对象（黄色杯子，蓝色塑料块）。</i>  <i>在右侧，把手握住杯子，刷子在背景中。</i>  <i>如果左图表示期望的结果，则良好的奖励功能是“了解”这两张照片对应于两个不同的对象。</i> <br><br> 为了解决识别问题，我们需要一个感知系统，该感知系统从非结构化图像（不是由人签名）中提取有意义的对象概念，并在没有老师的情况下学会可视化对象。 本质上，无教师学习算法通过创建有关数据的结构假设来工作。 通常认为图像可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">压缩到较小的空间</a> ，并且视频帧可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">从先前的</a>帧中<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">预测出来</a> 。 但是，如果没有关于数据内容的其他假设，通常这不足以从对象的不相关表示中学习。 <br><br> 如果我们在数据收集过程中使用机器人将对象物理分离怎么办？ 机器人技术为学习如何表示对象提供了绝佳的机会，因为机器人可以操纵它们，从而提供必要的变化因子。 我们的方法基于以下想法：捕获对象会将其从场景中删除。 结果是1）捕获之前的场景图像； 2）捕获之后的场景图像； 3）捕获对象的单独视图。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>左-要捕获的对象。</i>  <i>在中心-捕获后。</i>  <i>右边是捕获的对象。</i> <br><br> 如果我们考虑从图像中提取“一组对象”的内置函数，则应保留以下减法关系： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>捕获之前的对象-捕获之后的对象=捕获的对象</i> <br><br> 我们通过卷积架构和简单的度量学习算法实现了这种平等。 在训练期间，下面显示的架构将捕获之前和之后的图像嵌入到密集<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">的空间特性图中</a> 。 这些图通过平均并集变成矢量，“捕获前”和“捕获后”向量之间的差代表一组对象。 该向量和该感知对象的向量的对应表示通过N对函数等效。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br> 经过训练后，我们的模型自然具有两个有用的属性。 <br><br><h2>  1.对象的相似性 </h2><br> 向量嵌入之间<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">的</a>距离<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">的余弦系数</a>使我们可以比较对象并确定它们是否相同。 这可用于实施奖励功能以进行强化学习，并允许机器人学习如何在没有人工标记的情况下捕获示例。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2.寻找目标 </h2><br> 我们可以结合场景的空间图和对象的嵌入来在图像空间中定位“所需对象”。 进行空间特征图的逐元素乘法和所需对象的矢量对应，我们可以找到空间图上与目标对象相对应的所有像素。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>使用Grasp2Vec镶嵌来定位场景中的对象。</i>  <i>左上方是篮子中的物品。</i>  <i>左下方-要捕获的所需对象。</i>  <i>目标物体的矢量和图像空间特征的标量积为我们提供了图像给定部分与目标相似度的每个像素的“激活图”（右上图）。</i>  <i>该地图可用于更接近目标。</i> <br><br> 当多个对象对应于目标时，甚至当目标包含多个对象（两个向量的平均值）时，我们的方法也适用。 例如，在这种情况下，机器人会识别场景中的多个橙色块。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>生成的“热图”可用于计划机器人到目标对象的接近。</i>  <i>我们将Grasp2Vec的本地化和实例识别与我们的“捕获任何东西”策略相结合，在数据收集过程中80％的案例中取得成功，在59％的情况下使用机器人以前从未遇到过的新对象取得成功。</i> <br><br><h2> 结论 </h2><br> 在我们的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">工作中，</a>我们展示了机器人的抓手技能如何创建用于教授对象表示的数据。 然后，我们可以使用演示训练来快速掌握更复杂的技能，例如根据示例进行捕获，同时在我们的自主捕获系统中保留没有老师的所有教学属性。 <br><br> 除了我们的工作之外，其他几本近期著作还研究了如何在没有老师的情况下进行交互，通过<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">捕获</a> ， <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">推动</a>和与环境中的对象进行其他类型的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">交互</a>来获取对象的表示。 我们很高兴地期待着不仅机器学习可以为机器人提供更好的感知和控制方面的知识，而且还期待机器人可以在新的自学范式方面为机器学习提供什么。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN434898/">https://habr.com/ru/post/zh-CN434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN434888/index.html">二进制区的新年礼物</a></li>
<li><a href="../zh-CN434890/index.html">Qiwi银行（JSC）为用户分配资金</a></li>
<li><a href="../zh-CN434892/index.html">Swift中的绘图代码，PaintCode</a></li>
<li><a href="../zh-CN434894/index.html">萨满教或Olinuxino定制固件的艺术。 第一部分</a></li>
<li><a href="../zh-CN434896/index.html">消费电子产品名人堂：最近50年最佳产品的故事，第1部分</a></li>
<li><a href="../zh-CN434902/index.html">在Spring Data Neo4j中创建自定义查询生成器（第1部分）</a></li>
<li><a href="../zh-CN434906/index.html">在没有宏和动态内存的C ++中进行测试</a></li>
<li><a href="../zh-CN434908/index.html">程序员教育-什么？ 在哪 什么时候</a></li>
<li><a href="../zh-CN434912/index.html">保时捷Taycan的年度库存已经预留，主要由特斯拉车主</a></li>
<li><a href="../zh-CN434924/index.html">关于工作场所的组织，协同工作和远程工作空间的设计要阅读的内容</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>