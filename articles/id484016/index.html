<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ¾â€ğŸ¤â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€ğŸ­ ğŸ˜’ Dasar-dasar pembelajaran mendalam pada contoh debug otomatisencoder, bagian nomor 1 ğŸ“« ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Jika Anda membaca pelatihan tentang auto-encoders di situs keras.io, maka salah satu pesan pertama ada sesuatu seperti ini: dalam praktiknya, auto-enc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Dasar-dasar pembelajaran mendalam pada contoh debug otomatisencoder, bagian nomor 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/484016/"><p>  Jika Anda membaca pelatihan tentang auto-encoders di situs keras.io, maka salah satu pesan pertama ada sesuatu seperti ini: dalam praktiknya, auto-encoders hampir tidak pernah digunakan, tetapi mereka sering dibicarakan dalam pelatihan dan orang-orang datang, jadi kami memutuskan untuk menulis tutorial kami sendiri tentang mereka: </p><br><p>  <em>Klaim utama mereka untuk ketenaran berasal dari yang ditampilkan dalam banyak kelas pembelajaran mesin pengantar tersedia online.</em>  <em>Akibatnya, banyak pendatang baru di bidang ini benar-benar menyukai autoencoder dan tidak bisa mendapatkan cukup dari mereka.</em>  <em>Inilah alasan mengapa tutorial ini ada!</em> </p><br><p>  Namun demikian, salah satu tugas praktis yang dapat diterapkan untuk diri sendiri adalah mencari anomali, dan saya secara pribadi membutuhkannya dalam kerangka proyek malam. </p><br><p>  Di Internet, ada banyak tutorial tentang auto-encoders, untuk apa menulis satu lagi?  Sejujurnya, ada beberapa alasan untuk ini: </p><br><ul><li>  Ada perasaan bahwa sebenarnya tutorial itu sekitar 3 atau 4, sisanya ditulis ulang dengan kata-kata mereka sendiri; </li><li>  Hampir semuanya - di MNIST'e yang sudah lama menderita dengan gambar 28x28; </li><li>  Menurut pendapat saya yang sederhana - mereka tidak mengembangkan intuisi tentang bagaimana semua ini harus bekerja, tetapi hanya menawarkan untuk mengulang; </li><li>  Dan faktor yang paling penting - secara pribadi, ketika saya mengganti MNIST dengan <strong>dataset saya sendiri - semuanya dengan bodohnya berhenti bekerja</strong> . </li></ul><br><p>  Berikut ini menjelaskan jalur saya tempat memasukkan kerucut.  Jika Anda mengambil salah satu dari model flat (non-convolutional) yang diusulkan dari banyak tutorial dan menyalinnya dengan bodoh, maka tidak ada, yang mengejutkan, tidak berfungsi.  Tujuan artikel ini adalah untuk memahami mengapa dan, menurut saya, mendapatkan semacam pemahaman intuitif tentang bagaimana semua ini bekerja. </p><br><p>  Saya bukan spesialis pembelajaran mesin dan menggunakan pendekatan yang saya gunakan dalam pekerjaan sehari-hari.  Bagi para ilmuwan data yang berpengalaman, mungkin seluruh artikel ini akan liar, tetapi bagi pemula, bagi saya, sesuatu yang baru mungkin muncul. </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">proyek seperti apa</b> <div class="spoiler_text"><p>  Singkatnya tentang proyek, meskipun artikel itu bukan tentang dia.  Ada penerima ADS-B, ia menangkap data dari pesawat terbang dengan dan menuliskannya, pesawat terbang, koordinat ke pangkalan.  Kadang-kadang, pesawat terbang berperilaku dengan cara yang tidak biasa - mereka berputar untuk membakar bahan bakar sebelum mendarat, atau hanya penerbangan pribadi terbang melewati rute standar (koridor).  Sangat menarik untuk mengisolasi dari sekitar seribu pesawat per hari yang tidak berperilaku seperti yang lain.  Saya sepenuhnya mengakui bahwa penyimpangan dasar dapat dihitung lebih mudah, tetapi saya tertarik untuk mencobanya <del>  keajaiban </del>  jaringan saraf. </p></div></div><br><p>  Mari kita mulai.  Saya memiliki dataset 4000 gambar hitam dan putih 64x64 piksel, tampilannya seperti ini: </p><br><p><img src="https://habrastorage.org/webt/vw/5r/mh/vw5rmhetruoniuc7p4ksjulshde.png"></p><br><p>  Hanya beberapa garis pada latar belakang hitam, dan pada gambar 64x64 sekitar 2% poin terisi.  Jika Anda melihat banyak gambar, maka, tentu saja, ternyata sebagian besar garis sangat mirip. </p><br><p>  Saya tidak akan masuk ke rincian tentang bagaimana dataset dimuat, diproses, karena tujuan artikel, sekali lagi, bukan ini.  Cukup tunjukkan potongan kode yang menakutkan. </p><br><div class="spoiler">  <b class="spoiler_title">Kode</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># only for google colab %tensorflow_version 2.x import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os import zipfile import datetime import tensorflow_addons as tfa BATCH_SIZE = 128 AUTOTUNE=tf.data.experimental.AUTOTUNE def load_image(fpath): img_raw = tf.io.read_file(fpath) img = tf.io.decode_png(img_raw, channels=1, dtype=tf.uint8) return tf.image.convert_image_dtype(img, dtype=tf.float32) ## for splitting test/train def is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x,y) ## for image augmentation def random_flip_flop(img): return tf.image.random_flip_left_right(img) def transform_aug(shift_val): def random_transform(img): return tfa.image.translate(img,tf.random.uniform([2], -1*shift_val, shift_val)) return random_transform def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, transform=0, flip=False): if cache: if isinstance(cache, str): ds = ds.cache(cache) else: ds = ds.cache() ds = ds.shuffle(buffer_size=shuffle_buffer_size) if transform != 0: ds = ds.map(transform_aug(transform)) if flip: ds = ds.map(random_flip_flop) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds def prepare_input_output(x): return (x, x) list_ds = tf.data.Dataset.list_files("/content/planes64/*") imgs_df = list_ds.map(load_image) train = imgs_df.enumerate().filter(is_train).map(lambda x,y: y) train_ds = prepare_for_training(train, transform=10, flip=True) train_ds = train_ds.map(prepare_input_output) val = imgs_df.enumerate().filter(is_test).map(lambda x, y: y) val_ds = val.map(prepare_input_output).batch(BATCH_SIZE, drop_remainder=True)</span></span></code> </pre> </div></div><br><p>  Di sini, misalnya, adalah model pertama yang diusulkan dengan keras.io, yang mereka kerjakan dan latih di mnist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># this is the size of our encoded representations encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats # this is our input placeholder input_img = Input(shape=(784,)) # "encoded" is the encoded representation of the input encoded = Dense(encoding_dim, activation='relu')(input_img) # "decoded" is the lossy reconstruction of the input decoded = Dense(784, activation='sigmoid')(encoded)</span></span></code> </pre> <br><p>  Dalam kasus saya, model didefinisikan seperti ini: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ada sedikit perbedaan yang saya ratakan dan bentuk ulang langsung dalam model, dan bahwa saya "kompres" tidak 25 kali, tetapi hanya 10. Ini seharusnya tidak mempengaruhi apa pun. </p><br><p>  Sebagai fungsi-rugi - galat kuadrat rata-rata, pengoptimal tidak mendasar, biarkan adam.  Selanjutnya, kami melatih 20 era, 100 langkah per era. </p><br><p>  Jika Anda melihat metrik - semuanya terbakar.  Akurasi == 0,993.  Jika Anda melihat jadwal pelatihan - semuanya sedikit lebih sedih, kami mencapai dataran tinggi di wilayah era ketiga. </p><br><p><img src="https://habrastorage.org/webt/fo/rq/r7/forqr7krelrj1xq1qis_jg2jeoq.png"></p><br><p>  Nah, jika Anda melihat langsung pada hasil encoder, Anda mendapatkan gambar yang umumnya sedih (aslinya ada di atas, dan hasil encoding-decoding di bawah): </p><br><p><img src="https://habrastorage.org/webt/xo/mn/oq/xomnoqmjj80uaal0echobmal9xm.png"></p><br><p>  Secara umum, ketika Anda mencoba mencari tahu mengapa sesuatu tidak berfungsi, itu adalah pendekatan yang cukup baik untuk memecah semua fungsi menjadi blok besar dan memeriksa masing-masing secara terpisah.  Jadi mari kita lakukan. </p><br><p>  Dalam asli tutorial - data datar dipasok ke input model dan diambil pada output.  Mengapa tidak memeriksa tindakan saya pada perataan dan pembentukan kembali.  Berikut adalah model no-op: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Hasil: <br><img src="https://habrastorage.org/webt/vx/z0/aj/vxz0ajadu2qiktdq5ndzj5xje8w.png"></p><br><p>  Tidak ada yang diajarkan di sini.  Nah, pada saat yang sama, itu membuktikan bahwa fungsi visualisasi saya juga berfungsi. </p><br><p>  Selanjutnya, cobalah membuat model ini bukan tanpa-op, tetapi sebodoh mungkin - cukup potong lapisan kompresi, sisakan satu lapisan ukuran input.  Seperti yang mereka katakan di semua tutorial, kata mereka, sangat penting bahwa model Anda mempelajari fitur, dan bukan hanya fungsi identitas.  Ya, itulah tepatnya yang akan kami coba dapatkan, mari kita sampaikan gambar yang dihasilkan ke output. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Dia mempelajari sesuatu, akurasi == 0,995 dan lagi-lagi dia tersandung ke dataran tinggi. <br><img src="https://habrastorage.org/webt/ro/t6/jk/rot6jkf2ertweb7weeh8d_layui.png"></p><br><p>  Tetapi, secara umum, jelas bahwa itu tidak bekerja dengan baik.  Pokoknya - apa yang harus dipelajari di sana, melewati pintu masuk ke pintu keluar dan hanya itu. </p><br><p>  Jika Anda membaca dokumentasi keras tentang lapisan padat, itu menjelaskan apa yang mereka lakukan: <code>output = activation(dot(input, kernel) + bias)</code> <br>  Agar keluaran bertepatan dengan input, dua hal sederhana sudah cukup - bias = 0 dan kernel - matriks identitas (penting untuk tidak membiarkan matriks diisi dengan unit di sini - ini adalah hal yang sangat berbeda).  Untungnya, ini dan itu bisa dilakukan dengan mudah dari dokumentasi untuk <code>Dense</code> sama. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation = <span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer = tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Karena  kami segera mengatur beratnya, maka Anda tidak dapat mempelajari apa pun - segera itu bagus: <br><img src="https://habrastorage.org/webt/dm/tk/5a/dmtk5ardo9xksg8c5v5febzikdo.png"></p><br><p>  Tetapi jika Anda memulai pelatihan, maka itu dimulai, pada pandangan pertama, secara mengejutkan - model dimulai dengan akurasi == 1.0, tetapi dengan cepat jatuh. <br>  Evaluasi hasil sebelum pelatihan: <code>8/Unknown - 1s 140ms/step - loss: 0.2488 - accuracy: 1.0000[0.24875330179929733, 1.0]</code> .  Pelatihan: </p><br><pre> <code class="plaintext hljs">Epoch 1/20 100/100 [==============================] - 6s 56ms/step - loss: 0.1589 - accuracy: 0.9990 - val_loss: 0.0944 - val_accuracy: 0.9967 Epoch 2/20 100/100 [==============================] - 5s 51ms/step - loss: 0.0836 - accuracy: 0.9964 - val_loss: 0.0624 - val_accuracy: 0.9958 Epoch 3/20 100/100 [==============================] - 5s 50ms/step - loss: 0.0633 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9958 Epoch 4/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0520 - accuracy: 0.9961 - val_loss: 0.0423 - val_accuracy: 0.9961 Epoch 5/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0457 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9962</code> </pre> <br><p>  Ya, dan itu tidak terlalu jelas, kami sudah memiliki model yang ideal - gambar keluar 1 dalam 1, dan kerugian (berarti kuadrat kesalahan) menunjukkan hampir 0,25. </p><br><p>  Ngomong-ngomong, ini adalah pertanyaan yang sering muncul di forum - kerugian menurun, tetapi akurasi tidak bertambah, bagaimana mungkin? <br>  Di sini perlu diingat kembali definisi dari layer Dense: <code>output = activation(dot(input, kernel) + bias)</code> dan kata aktivasi yang disebutkan di dalamnya, yang berhasil saya abaikan di atas.  Dengan bobot dari matriks identitas dan tanpa bias, kita mendapatkan <code>output = activation(input)</code> . </p><br><p>  Sebenarnya, fungsi aktivasi dalam kode sumber kami sudah ditunjukkan, sigmoid, saya cukup bodoh menyalinnya dan hanya itu.  Dan dalam tutorial disarankan untuk menggunakannya di mana-mana.  Tetapi Anda harus mencari tahu. </p><br><p>  Sebagai permulaan, Anda dapat membaca dalam dokumentasi apa yang mereka tulis tentang itu: <code>The sigmoid activation: (1.0 / (1.0 + exp(-x)))</code> .  Itu secara pribadi tidak memberi tahu saya apa-apa, karena saya tidak phantomo sekali untuk membuat grafik seperti itu di kepala saya. <br>  Tetapi Anda dapat membangun dengan pena: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.sigmoid(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/tp/bs/1b/tpbs1bjym9vqahdihjhm1aubrne.png"></p><br><p>  Dan di sini menjadi jelas bahwa pada nol sigmoid mengambil nilai 0,5, dan dalam unit - sekitar 0,73.  Dan poin yang kami miliki adalah hitam (0,0) atau putih (1,0).  Jadi ternyata kesalahan kuadrat dari fungsi identitas tetap tidak nol. </p><br><p>  Anda bahkan dapat melihat pena, di sini ada satu baris dari gambar yang dihasilkan: </p><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> ], dtype=float32)</code> </pre> <br><p>  Dan itu semua, pada kenyataannya, sangat keren, karena beberapa pertanyaan muncul sekaligus: </p><br><ul><li>  mengapa ini tidak terlihat dalam visualisasi di atas? </li><li>  lalu mengapa akurasi == 1.0, karena gambar aslinya 0 dan 1. </li></ul><br><p>  Dengan visualisasi, semuanya sangat sederhana.  Untuk menampilkan gambar, saya menggunakan matplotlib: <code>plt.imshow(res_imgs[i][:, :, 0])</code> .  Dan, seperti biasa, jika Anda pergi ke dokumentasi, semuanya akan ditulis di sana: <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code> <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code>  Yaitu  perpustakaan menormalkan 0,5 dan 0,73 dalam kisaran dari 0 hingga 1. Ubah kode: </p><br><pre> <code class="python hljs">plt.imshow(res_imgs[i][:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>], norm=matplotlib.colors.Normalize(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/rn/qn/ta/rnqntag1dohhikail8y6myq7siy.png"></p><br><p>  Dan inilah pertanyaan dengan akurasi.  Untuk mulai dengan - karena kebiasaan, kita pergi ke dokumentasi, baca untuk <code>tf.keras.metrics.Accuracy</code> dan di sana tampaknya mereka menulis dimengerti: </p><br><pre> <code class="plaintext hljs">For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75.</code> </pre> <br><p>  Tetapi dalam kasus ini, keakuratan kami seharusnya 0. Saya, sebagai akibatnya, mengubur diri saya di sumber dan itu cukup jelas bagi diri saya sendiri: </p><br><pre> <code class="plaintext hljs"> When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.</code> </pre> <br><p>  Selain itu, dalam dokumentasi di situs untuk beberapa alasan paragraf ini tidak ada dalam deskripsi <code>.compile</code> . </p><br><p>  Berikut adalah sepotong kode dari <a href="https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py">https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py</a> </p><br><pre> <code class="python hljs">y_t_rank = len(y_t.shape.as_list()) y_p_rank = len(y_p.shape.as_list()) y_t_last_dim = y_t.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] y_p_last_dim = y_p.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] is_binary = y_p_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> is_sparse_categorical = ( y_t_rank &lt; y_p_rank <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> y_t_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> y_p_last_dim &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, <span class="hljs-string"><span class="hljs-string">'acc'</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_binary: metric_obj = metrics_mod.binary_accuracy <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> is_sparse_categorical: metric_obj = metrics_mod.sparse_categorical_accuracy <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: metric_obj = metrics_mod.categorical_accuracy</code> </pre> <br><p>  <code>y_t</code> adalah y_true, atau output yang diharapkan, <code>y_p</code> adalah y_predicted, atau hasil yang diprediksi. <br>  Kami memiliki format data: <code>shape=(64,64,1)</code> , sehingga ternyata akurasi dianggap sebagai binary_accuracy.  Minat demi bagaimana hal itu dipertimbangkan: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">binary_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred, threshold=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> threshold = math_ops.cast(threshold, y_pred.dtype) y_pred = math_ops.cast(y_pred &gt; threshold, y_pred.dtype) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(math_ops.equal(y_true, y_pred), axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  Sangat lucu bahwa di sini kita hanya beruntung - secara default, semuanya dianggap sebagai unit yang lebih dari 0,5, dan 0,5 dan kurang - nol.  Jadi akurasi keluar seratus persen untuk model identitas kita, walaupun sebenarnya jumlahnya tidak sama.  Ya, jelas bahwa jika kita benar-benar ingin, maka kita dapat memperbaiki ambang batas dan mengurangi akurasi menjadi nol, misalnya, hanya saja itu tidak benar-benar diperlukan.  Ini adalah metrik, itu tidak memengaruhi pelatihan, Anda hanya perlu memahami bahwa Anda dapat menghitungnya dalam ribuan cara berbeda dan mendapatkan indikator yang sama sekali berbeda.  Seperti contoh, Anda dapat menarik berbagai metrik dengan pena dan mentransfer data kami kepada mereka: </p><br><pre> <code class="python hljs">m = tf.keras.metrics.BinaryAccuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Akan memberi kita <code>1.0</code> . </p><br><p>  Dan disini </p><br><pre> <code class="python hljs">m = tf.keras.metrics.Accuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Akan memberi kami <code>0.0</code> pada data yang sama. </p><br><p>  Ngomong-ngomong, potongan kode yang sama dapat digunakan untuk bermain dengan fungsi yang hilang dan memahami cara kerjanya.  Jika Anda membaca tutorial tentang auto-encoders, maka pada dasarnya mereka menyarankan untuk menggunakan salah satu dari dua fungsi yang hilang: baik error kuadrat rata-rata atau 'binary_crossentropy'.  Anda juga dapat melihatnya secara bersamaan. </p><br><p>  Saya mengingatkan Anda bahwa untuk <code>mse</code> saya sudah memberikan model <code>evaluate</code> : </p><br><pre> <code class="plaintext hljs">8/Unknown - 2s 221ms/step - loss: 0.2488 - accuracy: 1.0000[0.24876083992421627, 1.0]</code> </pre> <br><p>  Yaitu  loss == 0,2488.  Mari kita lihat mengapa ini terjadi.  Menurut saya pribadi itu adalah yang paling sederhana dan paling dapat dipahami: perbedaan antara y_true dan y_predict dikurangi piksel demi piksel, setiap hasil dikuadratkan, kemudian hasilnya rata-rata dicari. </p><br><pre> <code class="python hljs">tf.keras.backend.mean(tf.math.squared_difference(x_batch[<span class="hljs-number"><span class="hljs-number">0</span></span>], res_imgs[<span class="hljs-number"><span class="hljs-number">0</span></span>]))</code> </pre> <br><p>  Dan pada output: </p><br><pre> <code class="plaintext hljs">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.24826494&gt;</code> </pre> <br><p>  Di sini intuisi sangat sederhana - mayoritas piksel kosong, model menghasilkan 0,5, mereka mendapatkan 0,25 - kuadrat perbedaannya. </p><br><p>  Dengan binary crossenttrtopy, segalanya menjadi sedikit lebih rumit, dan ada artikel lengkap tentang cara kerjanya, tetapi secara pribadi selalu lebih mudah bagi saya untuk membaca sumbernya, dan terlihat seperti ini: </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> from_logits: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> isinstance(output, (ops.EagerTensor, variables_module.Variable)): output = _backtrack_identity(output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output.op.type == <span class="hljs-string"><span class="hljs-string">'Sigmoid'</span></span>: <span class="hljs-comment"><span class="hljs-comment"># When sigmoid activation function is used for output operation, we # use logits from the sigmoid function directly to compute loss in order # to prevent collapsing zero when training. assert len(output.op.inputs) == 1 output = output.op.inputs[0] return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) # Compute cross entropy from probabilities. bce = target * math_ops.log(output + epsilon()) bce += (1 - target) * math_ops.log(1 - output + epsilon()) return -bce</span></span></code> </pre> <br><p>  Sejujurnya, saya memutar otak atas beberapa baris kode ini untuk waktu yang sangat lama.  Pertama, segera jelas bahwa dua implementasi dapat bekerja: baik <code>sigmoid_cross_entropy_with_logits</code> akan dipanggil, atau sepasang baris terakhir akan berfungsi.  Perbedaannya adalah bahwa <code>sigmoid_cross_entropy_with_logits</code> berfungsi dengan logit (sesuai namanya, doh), dan kode utama berfungsi dengan probabilitas. </p><br><p>  Siapa yang log?  Jika Anda membaca sejuta artikel berbeda tentang topik ini, maka mereka akan menyebutkan definisi, rumus, dan hal lain dalam matematika.  Dalam praktiknya, semuanya tampak sangat sederhana (koreksi saya jika saya salah).  Output mentah dari prediksi adalah log.  Nah, atau peluang-log, peluang logaritmik yang diukur dalam <strong>log</strong> istic un <strong>-nya</strong> - beo logistik. </p><br><div class="spoiler">  <b class="spoiler_title">Ada penyimpangan kecil - mengapa ada logaritma</b> <div class="spoiler_text"><p>  Peluang adalah rasio jumlah peristiwa yang kita butuhkan dengan jumlah peristiwa yang tidak kita butuhkan (berbeda dengan probabilitas, yang merupakan rasio peristiwa yang kita butuhkan dengan jumlah semua peristiwa secara umum).  Misalnya - jumlah kemenangan tim kami dengan jumlah kekalahannya.  Dan ada satu masalah.  Melanjutkan contoh dengan kemenangan tim, tim kami dapat menjadi pecundang menengah dan memiliki peluang untuk memenangkan 1/2 (satu hingga dua), dan mungkin sangat pecundang - dan memiliki peluang untuk menang 1/100.  Dan di arah yang berlawanan - menengah-curam dan 2/1, lebih curam dari gunung tertinggi - dan kemudian 100/1.  Dan ternyata seluruh jajaran tim yang kalah digambarkan dengan angka dari 0 hingga 1, dan tim keren - dari 1 hingga tak terbatas.  Akibatnya, sulit untuk membandingkan, tidak ada simetri, untuk bekerja dengan ini secara umum tidak nyaman untuk semua orang, matematika keluar jelek.  Dan jika Anda mengambil logaritma peluang, maka semuanya menjadi simetris: </p><br><pre> <code class="plaintext hljs">ln(1/2) == -0.69 ln(2/1) == 0.69 ln(1/100) == -4.6 ln(100/1) == 4.6</code> </pre> </div></div><br><p>  Dalam kasus tensorflow, ini agak arbitrer, karena, secara tegas, output dari layer secara matematis bukan log-odds, tetapi sudah diterima.  Jika nilai mentahnya dari -âˆ hingga + âˆ - lalu log.  Kemudian mereka dapat dikonversi menjadi probabilitas.  Ada dua opsi untuk ini: softmax dan kasing khusus, sigmoid.  Softmax - Mengambil vektor log dan mengubahnya menjadi vektor probabilitas, dan meskipun demikian jumlah probabilitas semua peristiwa di dalamnya ternyata 1. Sigmoid (dalam kasus tf) juga mengambil vektor log, tetapi mengonversi masing-masing menjadi probabilitas secara terpisah, secara terpisah. dari yang lain. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1+ln(0.5) == 0.30685281944 tf.math.softmax(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25, 0.5 , 0.25], dtype=float32)&gt; tf.math.sigmoid(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.57611686, 0.7310586 , 0.57611686], dtype=float32)&gt;</span></span></code> </pre> <br><p>  Anda bisa melihatnya dengan cara ini.  Ada tugas klasifikasi multi-label, ada tugas klasifikasi multi-kelas.  Multiclass - ini adalah jika Anda perlu menentukan apel dalam gambar atau jeruk, dan mungkin bahkan nanas.  Dan multilabel adalah ketika ada vas buah di gambar dan Anda perlu mengatakan bahwa ada apel dan jeruk di atasnya, tetapi tidak ada nanas.  Jika kita ingin multiclass - kita perlu softmax, jika kita ingin multilabel - kita perlu sigmoid. <br>  Di sini kita memiliki kasus multilabel - perlu untuk setiap pixel individu (kelas) untuk mengatakan apakah itu diinstal. </p><br><p>  Kembali ke tensorflow dan mengapa dalam crossentropy biner (paling tidak dalam fungsi crossentropy lainnya hampir sama) ada dua cabang global.  Crossentropy selalu bekerja dengan probabilitas, kami akan membicarakannya nanti.  Kemudian hanya ada dua cara: probabilitas sudah masuk input, atau log datang ke input - dan kemudian sigmoid diterapkan pertama kali untuk mendapatkan probabilitas.  Kebetulan menerapkan sigmoid dan menghitung crossentropy ternyata lebih baik daripada hanya menghitung crossentropy dari probabilitas (output matematika dari alasannya adalah dalam sumber fungsi <code>sigmoid_cross_entropy_with_logits</code> , plus bagi yang penasaran Anda dapat google 'stabilitas numerik lintas entropi'), sehingga bahkan para pengembang lambat dapat merekomendasikan untuk tidak melewatkan probabilitas untuk melewati probabilitas. input fungsi crossentropy, dan memberikan kembali log mentah.  Nah, tepat di kode, fungsi kerugian diperiksa jika lapisan terakhir adalah sigmoid, maka mereka akan memotongnya dan mengambil input aktivasi, bukan outputnya, untuk menghitung, mengirim semuanya untuk dipertimbangkan dalam <code>sigmoid_cross_entropy_with_logits</code> . </p><br><p>  Oke, bereskan, sekarang binary_crossentropy.  Ada dua penjelasan "intuitif" populer yang mengukur cross-entropy. </p><br><p>  Lebih formal: bayangkan bahwa ada model tertentu yang untuk n kelas tahu probabilitas kemunculannya (y <sub>0</sub> , y <sub>1</sub> , ..., y <sub>n</sub> ).  Dan sekarang dalam kehidupan, masing-masing kelas ini telah muncul k <sub>n</sub> kali (k <sub>1</sub> , k <sub>1</sub> , ..., k <sub>n</sub> ).  Probabilitas peristiwa semacam itu adalah produk dari probabilitas untuk setiap kelas individu - (y <sub>1</sub> ^ k <sub>1</sub> ) (y <sub>2</sub> ^ k <sub>2</sub> ) ... (y <sub>n</sub> ^ k <sub>n</sub> ).  Pada prinsipnya - ini sudah merupakan definisi normal dari cross-entropy - probabilitas satu dataset dinyatakan dalam probabilitas dari dataset lain.  Masalah dengan definisi ini adalah bahwa itu akan berubah dari 0 ke 1 dan akan sering sangat kecil, tidak nyaman untuk membandingkan nilai-nilai tersebut. <br>  Jika kita mengambil logaritma dari ini, maka k <sub>1</sub> log (y <sub>1</sub> ) + k <sub>2</sub> log (y <sub>2</sub> ) akan keluar dan seterusnya.  Kisaran nilai menjadi dari -âˆ hingga 0. Lipat gandakan semua ini dengan -1 / n - dan rentang dari 0 hingga + âˆ keluar, apalagi, karena  itu dinyatakan sebagai jumlah nilai untuk setiap kelas, perubahan di setiap kelas tercermin dalam nilai keseluruhan dengan cara yang sangat dapat diprediksi. </p><br><p>  Lebih sederhana: cross-entropy menunjukkan berapa banyak bit tambahan yang diperlukan untuk mengekspresikan sampel dalam hal model asli.  Jika kita ada di sana untuk membuat logaritma dengan basis 2, maka kita akan langsung bit.  Kami menggunakan logaritma alami di mana-mana, sehingga menunjukkan jumlah nat ( <a href="https://en.wikipedia.org/wiki/Nat_(unit">https://en.wikipedia.org/wiki/Nat_(unit</a> )), bukan bit. </p><br><p>  Binary cross-entropy, pada gilirannya, adalah kasus khusus dari cross-entropy biasa, ketika jumlah kelas adalah dua.  Kemudian kita memiliki pengetahuan yang cukup tentang probabilitas kemunculan satu kelas - y <sub>1</sub> , dan probabilitas yang kedua adalah (1-y <sub>1</sub> ). </p><br><p>  Tapi, bagi saya, agak membuat saya tergelincir.  Biarkan saya mengingatkan Anda, terakhir kali kami mencoba membangun sebuah auto-encoder identitas, dia menunjukkan kepada kita sebuah gambar yang indah, dan bahkan akurasi 1,0, tetapi ternyata angkanya ternyata mengerikan.  Demi percobaan, Anda dapat melakukan beberapa tes lagi: <br>  1) aktivasi dapat dihapus sama sekali, akan ada identitas bersih <br>  2) Anda dapat mencoba fungsi aktivasi lainnya, misalnya relu yang sama </p><br><p>  Tanpa aktivasi: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Kami mendapatkan model identitas yang sempurna: </p><br><pre> <code class="python hljs">model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  Ngomong-ngomong, pelatihan tidak akan mengarah pada apa pun, karena kerugian == 0,0. </p><br><p>  Sekarang dengan relu.  Grafiknya terlihat seperti ini: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.relu(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">1</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/wq/ph/iw/wqphiwwmhtmxwfogld4nstyfp9w.png"></p><br><p>  Di bawah nol - nol, di atas - y = x, yaitu  dalam teori, kita harus mendapatkan efek yang sama dengan tidak adanya aktivasi - model yang ideal. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, loss=<span class="hljs-string"><span class="hljs-string">"binary_crossentropy"</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>]) model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 158ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  Oke, kami menemukan model identitas, bahkan dengan beberapa bagian dari teori itu menjadi lebih jelas.  Sekarang mari kita coba latih model yang sama sehingga menjadi identitas. </p><br><p>  Untuk bersenang-senang, saya akan melakukan percobaan ini pada tiga fungsi aktivasi.  Untuk mulai dengan - relu, karena ia menunjukkan dirinya jauh lebih awal (semuanya seperti sebelumnya, tetapi kernel_initializer dihapus, jadi secara default akan <code>glorot_uniform</code> ): </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ia belajar dengan luar biasa: </p><br><p><img src="https://habrastorage.org/webt/hv/wz/oo/hvwzoodkp6dxopq7pjkopeeuorw.png"></p><br><p>  Hasilnya cukup bagus, akurasi: 0,9999, loss (mse): 2e-04 setelah 20 era dan Anda bisa berlatih lebih lanjut. </p><br><p><img src="https://habrastorage.org/webt/4y/io/jt/4yiojttsafqnf796ha6all0qmsy.png"></p><br><p>  Selanjutnya, coba dengan sigmoid: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Saya sudah mengajarkan sesuatu yang serupa sebelumnya, dengan satu-satunya perbedaan adalah bahwa bias dinonaktifkan di sini.  Dia belajar dengan tenang, pergi di dataran tinggi di wilayah era ke-50, akurasi: 0,9970, kerugian: 0,01 setelah 60 era. </p><br><p>  Hasilnya lagi tidak mengesankan: </p><br><p><img src="https://habrastorage.org/webt/_7/fl/o_/_7flo_xh5gkgh8oennqthe2ymhk.png"></p><br><p>  Nah, periksa juga tanh: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Hasilnya sebanding dengan relu - akurasi: 0,9999, loss: 6e-04 setelah 20 era, dan Anda dapat melatih lebih lanjut: </p><br><p><img src="https://habrastorage.org/webt/m4/ib/xc/m4ibxctlge5cxs7eqozjt5uwfsc.png"></p><br><p><img src="https://habrastorage.org/webt/sk/r3/p8/skr3p8etvlatcf-mnc6q9sabtfk.png"></p><br><p>  Bahkan, saya tersiksa oleh pertanyaan apakah sesuatu dapat dilakukan untuk membuat sigmoid menunjukkan hasil yang sebanding.  Khusus karena minat olahraga. </p><br><p>  Misalnya, Anda dapat mencoba menambahkan BatchNormalisasi: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.BatchNormalization()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Dan kemudian semacam sihir terjadi.  Di era ke-13, akurasi: 1.0.  Dan hasil yang berapi-api: </p><br><p><img src="https://habrastorage.org/webt/6x/md/di/6xmddijppdnstc8ire1mxlcwkca.png"></p><br><p>  III ... di gantungan tebing ini saya akan menyelesaikan bagian pertama, karena teks terlalu dofig, dan tidak jelas apakah seseorang membutuhkannya atau tidak.  Pada bagian kedua, saya akan memahami apa yang terjadi dengan sihir, bereksperimen dengan pengoptimal yang berbeda, mencoba membangun penyandi-dekoder yang jujur, membenturkan kepala ke atas meja.  Saya harap seseorang tertarik dan membantu. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id484016/">https://habr.com/ru/post/id484016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id484004/index.html">@Pythonetc Desember 2019</a></li>
<li><a href="../id484006/index.html">Kiat dan trik dari saluran Telegram saya @pythonetc, Desember 2019</a></li>
<li><a href="../id484008/index.html">Apa yang menjadi pemimpin tim</a></li>
<li><a href="../id484012/index.html">Merampingkan proses penulisan dalam bloknot notebook</a></li>
<li><a href="../id484014/index.html">10 Mitos SEO untuk Ditinggalkan di Tahun 2020</a></li>
<li><a href="../id484018/index.html">Sisi teknis TI berperahu pesiar</a></li>
<li><a href="../id484020/index.html">Siapa yang coba Anda buat terkesan dengan tenggat waktu Anda?</a></li>
<li><a href="../id484026/index.html">Bagian 6: Porting MemTest86 + ke RISC-V</a></li>
<li><a href="../id484028/index.html">Horseshoe Bend - tablet konversi dengan layar lipat</a></li>
<li><a href="../id484034/index.html">Implementasi skema kerja penyimpanan target barang berdasarkan unit akuntansi gudang 1C Otomasi Terpadu 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>