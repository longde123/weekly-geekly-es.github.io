<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤜🏽 🎃 🙍 Inmersión en redes neuronales convolucionales: transferencia de aprendizaje 👦🏾 🔠 🙌🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El curso completo en ruso se puede encontrar en este enlace . 
 El curso de inglés original está disponible en este enlace . 



 Contenido 


1. Entr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Inmersión en redes neuronales convolucionales: transferencia de aprendizaje</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467967/"><p>  El curso completo en ruso se puede encontrar en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . <br>  El curso de inglés original está disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p><img src="https://habrastorage.org/webt/wu/ie/7c/wuie7cgpktklm4bytoweu7ki0oq.jpeg"></p><a name="habracut"></a><br><h1>  Contenido </h1><br><ol><li>  Entrevista con Sebastian Trun </li><li>  Introduccion </li><li>  Modelo de aprendizaje de transferencia </li><li>  MobileNet </li><li>  CoLab: Gatos Vs Perros con entrenamiento de transferencia </li><li>  Zambullirse en redes neuronales convolucionales </li><li>  Parte práctica: determinación de colores con la transferencia de formación. </li><li>  Resumen </li></ol><br><h1>  Entrevista con Sebastian Trun </h1><br><p>  - Esta es la lección 6 y está completamente dedicada a la transferencia de aprendizaje.  La transferencia de aprendizaje es el proceso de usar un modelo existente con poco refinamiento para nuevas tareas.  La transferencia de capacitación ayuda a reducir el tiempo de capacitación del modelo al aumentar la eficiencia al aprender desde el principio.  Sebastian, ¿qué opinas sobre la transferencia de entrenamiento?  ¿Alguna vez ha podido utilizar la metodología de transferencia de enseñanza en su trabajo e investigación? <br>  - Mi disertación se dedicó solo al tema de la transferencia de capacitación y se denominó " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Explicación sobre la base de la transferencia de capacitación</a> ".  Cuando estábamos trabajando en una disertación, la idea era que es posible enseñar a distinguir todos los demás objetos de este tipo en un objeto (conjunto de datos, entidad) en diversas variaciones y formatos.  En el trabajo, utilizamos el algoritmo desarrollado, que distinguía las características principales (atributos) del objeto y podía compararlas con otro objeto.  Las bibliotecas como Tensorflow ya vienen con modelos pre-entrenados. <br>  - Sí, en Tensorflow tenemos un conjunto completo de modelos pre-entrenados que puede usar para resolver problemas prácticos.  Hablaremos sobre sets ya hechos un poco más tarde. <br>  - Si, si!  Si lo piensa, las personas se dedican a la transferencia de capacitación todo el tiempo a lo largo de sus vidas. <br>  - ¿Podemos decir que gracias al método de transferencia de capacitación, nuestros nuevos estudiantes en algún momento no tendrán que saber algo sobre el aprendizaje automático porque será suficiente para conectar un modelo ya preparado y usarlo? <br>  - La programación es escribir línea por línea, le damos comandos a la computadora.  Nuestro objetivo es asegurarnos de que todos en el planeta puedan y puedan programar proporcionando a la computadora solo ejemplos de datos de entrada.  De acuerdo, si quieres enseñarle a una computadora a distinguir gatos de perros, entonces encontrar 100k imágenes diferentes de gatos y 100k imágenes diferentes de perros es bastante difícil, y gracias a la transferencia de entrenamiento puedes resolver este problema en varias líneas. <br>  - Sí, realmente lo es!  Gracias por las respuestas y finalmente pasemos al aprendizaje. </p><br><h1>  Introduccion </h1><br><p>  - Hola y bienvenido de nuevo! <br>  - La última vez entrenamos una red neuronal convolucional para clasificar gatos y perros en la imagen.  Nuestra primera red neuronal se volvió a entrenar, por lo que su resultado no fue tan alto: aproximadamente 70% de precisión.  Después de eso, implementamos la extensión y el abandono de datos (desconexión arbitraria de las neuronas), lo que nos permitió aumentar la precisión de las predicciones hasta en un 80%. <br>  - A pesar de que el 80% puede parecer un excelente indicador, el error del 20% sigue siendo demasiado grande.  Derecho?  ¿Qué podemos hacer para aumentar la precisión de la clasificación?  En esta lección, utilizaremos la técnica de transferencia de conocimiento (transferencia del modelo de conocimiento), que nos permitirá usar el modelo desarrollado por expertos y capacitados en grandes conjuntos de datos.  Como veremos en la práctica, al transferir el modelo de conocimiento podemos lograr una precisión de clasificación del 95%.  ¡Empecemos! </p><br><h1>  Transferencia de modelo de aprendizaje </h1><br><p>  En 2012, la red neuronal AlexNet revolucionó el mundo del aprendizaje automático y popularizó el uso de redes neuronales convolucionales para la clasificación al ganar el desafío de reconocimiento visual ImageNet a gran escala. </p><br><p><img src="https://habrastorage.org/webt/fs/xx/di/fsxxdicwkitfkxibie8kkjcyexu.png"></p><br><p>  Después de eso, la lucha comenzó a desarrollar redes neuronales más precisas y eficientes que podrían superar a AlexNet en las tareas de clasificar imágenes del conjunto de datos ImageNet. </p><br><p><img src="https://habrastorage.org/webt/oe/t0/ov/oet0ovye40p1cziih64hpmbhuus.png"></p><br><p>  Durante varios años, se han desarrollado redes neuronales que hacen frente a la tarea de clasificación mejor que AlexNet - Inception y ResNet. <br>  ¿Está de acuerdo en que sería genial poder aprovechar estas redes neuronales ya capacitadas en grandes conjuntos de datos de ImageNet y usarlas en su clasificador de perros y gatos? </p><br><p>  ¡Resulta que podemos hacerlo!  La técnica se llama transferencia de aprendizaje.  La idea principal del método de transferencia del modelo de entrenamiento se basa en el hecho de que después de haber entrenado una red neuronal en un gran conjunto de datos, podemos aplicar el modelo obtenido a un conjunto de datos que este modelo aún no ha encontrado.  Es por eso que la técnica se llama aprendizaje de transferencia: transferir el proceso de aprendizaje de un conjunto de datos a otro. </p><br><p>  Para poder aplicar la metodología de transferencia del modelo de entrenamiento, necesitamos cambiar la última capa de nuestra red neuronal convolucional: </p><br><p><img src="https://habrastorage.org/webt/3j/-g/g3/3j-gg3yxm9kvrtphiswnho2jgtc.png"></p><br><p>  Realizamos esta operación porque cada conjunto de datos consta de un número diferente de clases de salida.  Por ejemplo, los conjuntos de datos en ImageNet contienen 1000 clases de salida diferentes.  FashionMNIST contiene 10 clases.  Nuestro conjunto de datos de clasificación consta de solo 2 clases: gatos y perros. </p><br><p><img src="https://habrastorage.org/webt/5e/pm/ej/5epmejbamklkdfgzzw9v8rzb1ts.png"></p><br><p>  Es por eso que es necesario cambiar la última capa de nuestra red neuronal convolucional para que contenga el número de salidas que correspondería al número de clases en el nuevo conjunto. </p><br><p><img src="https://habrastorage.org/webt/cg/mk/fz/cgmkfzxqmyqbdzsmvfdppjhsl9e.png"></p><br><p>  También debemos asegurarnos de no cambiar el modelo pre-entrenado durante el proceso de capacitación.  La solución es desactivar las variables del modelo pre-entrenado: simplemente prohibimos que el algoritmo actualice los valores durante la propagación hacia adelante y hacia atrás para cambiarlos. <br>  Este proceso se llama "congelar el modelo". </p><br><p><img src="https://habrastorage.org/webt/8z/oz/0n/8zoz0nenaad4-lgezhhia25k_18.png"></p><br><p>  Al "congelar" los parámetros del modelo pre-entrenado, nos permitimos aprender solo la última capa de la red de clasificación, los valores de las variables del modelo pre-entrenado permanecen sin cambios. </p><br><p>  Otra ventaja indiscutible de los modelos pre-entrenados es que reducimos el tiempo de entrenamiento al entrenar solo la última capa con un número significativamente menor de variables, y no todo el modelo. </p><br><p>  Si no "congelamos" las variables del modelo pre-entrenado, entonces durante el proceso de entrenamiento, los valores de las variables cambiarán en el nuevo conjunto de datos.  Esto se debe a que los valores de las variables en la última capa de la clasificación se rellenarán con valores aleatorios.  Debido a los valores aleatorios en la última capa, nuestro modelo cometerá grandes errores en la clasificación, lo que, a su vez, implicará fuertes cambios en los pesos iniciales en el modelo pre-entrenado, lo cual es extremadamente indeseable para nosotros. </p><br><p><img src="https://habrastorage.org/webt/wp/uz/km/wpuzkmnan5rahfg3irexdsvrstm.png"></p><br><p>  Es por esta razón que siempre debemos recordar que cuando se usan modelos existentes, los valores de las variables deben "congelarse" y la necesidad de entrenar un modelo pre-entrenado debe desactivarse. </p><br><p>  Ahora que sabemos cómo funciona la transferencia del modelo de entrenamiento, ¡solo tenemos que elegir una red neuronal pre-entrenada para usar en nuestro propio clasificador!  Esto lo haremos en la siguiente parte. </p><br><h1>  MobileNet </h1><br><p>  Como mencionamos anteriormente, se desarrollaron redes neuronales extremadamente eficientes que mostraron altos resultados en los conjuntos de datos de ImageNet: AlexNet, Inception, Resonant.  Estas redes neuronales son redes muy profundas y contienen miles e incluso millones de parámetros.  Una gran cantidad de parámetros permite a la red aprender patrones más complejos y, por lo tanto, lograr una mayor precisión de clasificación.  Una gran cantidad de parámetros de entrenamiento de la red neuronal afecta la velocidad de aprendizaje, la cantidad de memoria requerida para almacenar la red y la complejidad de los cálculos. </p><br><p>  En esta lección usaremos la moderna red neuronal convolucional MobileNet.  MobileNet es una arquitectura de red neuronal convolucional eficiente que reduce la cantidad de memoria utilizada para la computación al tiempo que mantiene una alta precisión de las predicciones.  Es por eso que MobileNet es ideal para usar en dispositivos móviles con una cantidad limitada de memoria y recursos informáticos. </p><br><p>  MobileNet fue desarrollado por Google y capacitado en el conjunto de datos ImageNet. </p><br><p>  Dado que MobileNet recibió capacitación en 1,000 clases del conjunto de datos de ImageNet, MobileNet tiene 1,000 clases de salida, en lugar de las dos que necesitamos: un gato y un perro. </p><br><p><img src="https://habrastorage.org/webt/he/2f/ox/he2foxizd_xmt7rijxumteg-94k.png"></p><br><p>  Para completar la transferencia de capacitación, precargamos el vector de características sin una capa de clasificación: </p><br><p><img src="https://habrastorage.org/webt/1b/o2/no/1bo2nop9cpz3ebag_jcyfrgje7w.png"></p><br><p>  En Tensorflow, un vector de características cargado puede usarse como una capa Keras normal con datos de entrada de cierto tamaño. </p><br><p>  Como MobileNet recibió capacitación en el conjunto de datos de ImageNet, tendremos que llevar el tamaño de los datos de entrada a los que se utilizaron en el proceso de capacitación.  En nuestro caso, MobileNet recibió capacitación en imágenes RGB de 224x224px de tamaño fijo. </p><br><p>  TensorFlow contiene un repositorio previamente entrenado llamado TensorFlow Hub. </p><br><p><img src="https://habrastorage.org/webt/o5/we/9d/o5we9dvkdtaomahwj4nzomgquca.png"></p><br><p>  TensorFlow Hub contiene algunos modelos previamente entrenados en los que la última capa de clasificación se excluyó de la arquitectura de la red neuronal para su posterior reutilización. </p><br><p>  Puede usar el TensorFlow Hub en el código en varias líneas: </p><br><p><img src="https://habrastorage.org/webt/1h/ai/hg/1haihgg1llinsfv_wyhde4z0egy.png"></p><br><p>  Es suficiente especificar la URL del vector de características del modelo de entrenamiento deseado y luego incrustar el modelo en nuestro clasificador con la última capa con el número deseado de clases de salida.  Es la última capa que se someterá a entrenamiento y a cambio de valores de parámetros.  La compilación y capacitación de nuestro nuevo modelo se lleva a cabo de la misma manera que lo hicimos antes: </p><br><p><img src="https://habrastorage.org/webt/uh/cr/5e/uhcr5esktfnxtxwxfgutvttxbxk.png"></p><br><p>  Veamos cómo funcionará esto realmente y escriba el código apropiado. </p><br><h1>  CoLab: Gatos Vs Perros con entrenamiento de transferencia </h1><br><p>  Enlace a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CoLab en ruso</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CoLab en inglés</a> . </p><br><p>  TensorFlow Hub es un repositorio con modelos pre-entrenados que podemos usar. </p><br><p>  La transferencia de aprendizaje es un proceso en el que tomamos un modelo previamente entrenado y lo expandimos para realizar una tarea específica.  Al mismo tiempo, dejamos intacta la parte del modelo pre-entrenado que integramos en la red neuronal, pero solo entrenamos las últimas capas de salida para obtener el resultado deseado. </p><br><p>  En esta parte práctica, probaremos ambas opciones. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Este enlace le</a> permite explorar la lista completa de modelos disponibles. </p><br><p>  <strong>En esta parte de Colab</strong> </p><br><ol><li>  Usaremos el modelo TensorFlow Hub para las predicciones; </li><li>  Usaremos el modelo TensorFlow Hub para el conjunto de datos de gatos y perros; </li><li>  Transfieramos el entrenamiento usando el modelo del TensorFlow Hub. </li></ol><br><p> Antes de continuar con la implementación de la parte práctica actual, recomendamos restablecer el <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p>  <strong>Importaciones de la biblioteca</strong> </p><br><p>  En esta parte práctica, utilizaremos una serie de características de la biblioteca TensorFlow que aún no están en el lanzamiento oficial.  Es por eso que primero instalaremos la versión TensorFlow y TensorFlow Hub para desarrolladores. </p><br><p>  La instalación de la versión de desarrollo de TensorFlow activa automáticamente la última versión instalada.  Después de que terminemos de tratar con esta parte práctica, recomendamos restaurar la configuración de TensorFlow y volver a la versión estable a través del elemento de menú <code>Runtime -&gt; Reset all runtimes...</code>  La ejecución de este comando restablecerá todas las configuraciones del entorno a las originales. </p><br><pre> <code class="python hljs">!pip install tf-nightly-gpu !pip install <span class="hljs-string"><span class="hljs-string">"tensorflow_hub==0.4.0"</span></span> !pip install -U tensorflow_datasets</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1) Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7) Collecting tf-estimator-nightly (from tf-nightly-gpu) Downloading https://files.pythonhosted.org/packages/ea/72/f092fc631ef2602fd0c296dcc4ef6ef638a6a773cb9fdc6757fecbfffd33/tf_estimator_nightly-1.14.0.dev2019092201-py2.py3-none-any.whl (450kB) |████████████████████████████████| 450kB 45.9MB/s Requirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.5) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2) Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.0.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.6) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tf-nightly-gpu) (2.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (41.2.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (0.15.6) Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu Successfully installed tb-nightly-1.15.0a20190911 tf-estimator-nightly-1.14.0.dev2019092201 tf-nightly-gpu-1.15.0.dev20190821 Collecting tensorflow_hub==0.4.0 Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB) |████████████████████████████████| 81kB 5.0MB/s Requirement already satisfied: protobuf&gt;=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (3.7.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.16.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.12.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.4.0-&gt;tensorflow_hub==0.4.0) (41.2.0) Installing collected packages: tensorflow-hub Found existing installation: tensorflow-hub 0.6.0 Uninstalling tensorflow-hub-0.6.0: Successfully uninstalled tensorflow-hub-0.6.0 Successfully installed tensorflow-hub-0.4.0 Collecting tensorflow_datasets Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB) |████████████████████████████████| 2.3MB 4.9MB/s Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0) Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.11.2) Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.16.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.21.0) Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.1) Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (5.4.8) Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1) Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.8.0) Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.14.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.0) Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0) Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.1.0) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.8) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2019.6.16) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (1.24.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow_datasets) (41.2.0) Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.6.0) Installing collected packages: tensorflow-datasets Successfully installed tensorflow-datasets-1.2.0</code> </pre> <br><p>  Ya hemos visto y usado algunas importaciones antes.  Desde el nuevo - import <code>tensorflow_hub</code> , que instalamos y que utilizaremos en esta parte práctica. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0. Please upgrade your code to TensorFlow 2.0: * https://www.tensorflow.org/beta/guide/migration_guide Or install the latest stable TensorFlow 1.X release: * `pip install -U "tensorflow==1.*"` Otherwise your code may be broken by the change.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p>  <strong>Parte 1: use TensorFlow Hub MobileNet para predicciones</strong> </p><br><p>  En esta parte de CoLab, tomaremos un modelo previamente entrenado, lo cargaremos a Keras y lo probaremos. </p><br><p>  El modelo que utilizamos es MobileNet v2 (en lugar de MobileNet, se puede utilizar cualquier otro modelo de clasificador de imágenes compatible con tf2 con tfhub.dev). </p><br><p>  <strong>Descargar clasificador</strong> </p><br><p>  Descargue el modelo MobileNet y cree un modelo Keras a partir de él.  MobileNet en la entrada espera recibir una imagen de 224x224 píxeles de tamaño con 3 canales de color (RGB). </p><br><pre> <code class="python hljs">CLASSIFIER_URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span></span> IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> model = tf.keras.Sequential([ hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>)) ])</code> </pre> <br><p>  <strong>Ejecute el clasificador en una sola imagen</strong> </p><br><p>  MobileNet ha sido entrenado en el conjunto de datos ImageNet.  ImageNet contiene 1000 clases de salida y una de estas clases es un uniforme militar.  Busquemos la imagen en la que se ubicará el uniforme militar y que no formará parte del kit de entrenamiento de ImageNet para verificar la precisión de la clasificación. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PIL.Image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Image grace_hopper = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'image.jpg'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span></span>) grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES)) grace_hopper</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg 65536/61306 [================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/8g/gn/0u/8ggn0ur3pmnr_rxvezfdo4vrwcc.png"></p><br><pre> <code class="python hljs">grace_hopper = np.array(grace_hopper)/<span class="hljs-number"><span class="hljs-number">255.0</span></span> grace_hopper.shape</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">(224, 224, 3)</code> </pre> <br><p>  Tenga en cuenta que los modelos siempre reciben un conjunto (bloque) de imágenes para procesar en la entrada.  En el siguiente código, agregamos una nueva dimensión: el tamaño del bloque. </p><br><pre> <code class="python hljs">result = model.predict(grace_hopper[np.newaxis, ...]) result.shape</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">(1, 1001)</code> </pre> <br><p>  El resultado de la predicción fue un vector con un tamaño de 1.001 elementos, donde cada valor representa la probabilidad de que el objeto en la imagen pertenezca a una determinada clase. </p><br><p>  La posición del valor de probabilidad máxima se puede encontrar usando la función <code>argmax</code> .  Sin embargo, hay una pregunta que aún no hemos respondido: ¿cómo podemos determinar a qué clase pertenece un elemento con la máxima probabilidad? </p><br><pre> <code class="python hljs">predicted_class = np.argmax(result[<span class="hljs-number"><span class="hljs-number">0</span></span>], axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">653</code> </pre> <br><p>  <strong>Descifrando predicciones</strong> </p><br><p>  Para que podamos determinar la clase a la que se refieren las predicciones, cargamos la lista de etiquetas ImageNet y, mediante el índice, con la máxima fidelidad, determinamos la clase a la que se refiere la predicción. </p><br><pre> <code class="python hljs">labels_path = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'ImageNetLabels.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span></span>) imagenet_labels = np.array(open(labels_path).read().splitlines()) plt.imshow(grace_hopper) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) predicted_class_name = imagenet_labels[predicted_class] _ = plt.title(<span class="hljs-string"><span class="hljs-string">"Prediction: "</span></span> + predicted_class_name.title())</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt 16384/10484 [==============================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/ai/1k/rt/ai1krt6mkpcafhb5jx8ozf6mq8s.png"></p><br><p>  Bingo!  Nuestro modelo identificó correctamente el uniforme militar. </p><br><p>  <strong>Parte 2: use el modelo TensorFlow Hub para un conjunto de datos de perros y gatos</strong> </p><br><p>  Ahora usaremos la versión completa del modelo MobileNet y veremos cómo manejará el conjunto de datos de gatos y perros. </p><br><p>  <strong>Conjunto de datos</strong> </p><br><p>  Podemos usar los conjuntos de datos TensorFlow para descargar un conjunto de datos de perros y gatos. </p><br><pre> <code class="python hljs">splits = tfds.Split.ALL.subsplit(weighted=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) splits, info = tfds.load(<span class="hljs-string"><span class="hljs-string">'cats_vs_dogs'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split = splits) (train_examples, validation_examples) = splits num_examples = info.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_classes = info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/2.0.1... /usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) WARNING:absl:1738 images were corrupted and were skipped Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/2.0.1. Subsequent calls will reuse this data.</code> </pre><br><p>  No todas las imágenes en un conjunto de datos de perros y gatos son del mismo tamaño. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example_image <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_examples.take(<span class="hljs-number"><span class="hljs-number">3</span></span>)): print(<span class="hljs-string"><span class="hljs-string">"Image {} shape: {}"</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example_image[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape))</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Image 1 shape: (500, 343, 3) Image 2 shape: (375, 500, 3) Image 3 shape: (375, 500, 3)</code> </pre> <br><p>  Por lo tanto, las imágenes del conjunto de datos obtenido requieren una reducción a un tamaño único, que el modelo MobileNet espera en la entrada: 224 x 224. </p><br><p>  La función <code>.repeat()</code> y <code>steps_per_epoch</code> no son necesarios aquí, pero le permiten ahorrar unos 15 segundos por iteración de entrenamiento, porque  el búfer temporal se debe inicializar solo una vez al comienzo del proceso de aprendizaje. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / <span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = train_examples.shuffle(num_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  <strong>Ejecute el clasificador en conjuntos de imágenes</strong> </p><br><p>  Permítame recordarle que en esta etapa, todavía hay una versión completa de la red MobileNet pre-entrenada, que contiene 1,000 clases de salida posibles.  ImageNet contiene una gran cantidad de imágenes de perros y gatos, así que intentemos ingresar una de las imágenes de prueba de nuestro conjunto de datos y ver qué predicción nos dará el modelo. </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches.take(<span class="hljs-number"><span class="hljs-number">1</span></span>))) image_batch = image_batch.numpy() label_batch = label_batch.numpy() result_batch = model.predict(image_batch) predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)] predicted_class_names</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">array(['Persian cat', 'mink', 'Siamese cat', 'tabby', 'Bouvier des Flandres', 'dishwasher', 'Yorkshire terrier', 'tiger cat', 'tabby', 'Egyptian cat', 'Egyptian cat', 'tabby', 'dalmatian', 'Persian cat', 'Border collie', 'Newfoundland', 'tiger cat', 'Siamese cat', 'Persian cat', 'Egyptian cat', 'tabby', 'tiger cat', 'Labrador retriever', 'German shepherd', 'Eskimo dog', 'kelpie', 'mink', 'Norwegian elkhound', 'Labrador retriever', 'Egyptian cat', 'computer keyboard', 'boxer'], dtype='&lt;U30')</code> </pre> <br><p>  Las etiquetas son similares a los nombres de razas de gatos y perros.  Ahora muestremos algunas imágenes de nuestro conjunto de datos de perros y gatos y coloquemos una etiqueta prevista en cada uno de ellos. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) plt.title(predicted_class_names[n]) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"ImageNet predictions"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/fc/af/w-/fcafw-kdtzotffq7k-nihlntuda.png"></p><br><p>  <strong>Parte 3: Implementar la transferencia de aprendizaje con el TensorFlow Hub</strong> </p><br><p>  Ahora usemos el TensorFlow Hub para transferir el aprendizaje de un modelo a otro. </p><br><p>  En el proceso de transferir el entrenamiento, reutilizamos un modelo pre-entrenado cambiando su última capa, o varias capas, y luego comenzamos el proceso de entrenamiento nuevamente en un nuevo conjunto de datos. </p><br><p>  En TensorFlow Hub, puede encontrar no solo modelos completos previamente entrenados (con la última capa), sino también modelos sin la última capa de clasificación.  Este último puede usarse fácilmente para transferir el entrenamiento.  Continuaremos usando MobileNet v2 por la sencilla razón de que en las partes posteriores de nuestro curso transferiremos este modelo y lo lanzaremos en un dispositivo móvil usando TensorFlow Lite. </p><br><p>  También seguiremos utilizando el conjunto de datos de gatos y perros, por lo que tendremos la oportunidad de comparar el rendimiento de este modelo con los que implementamos desde cero. </p><br><p>  Tenga en cuenta que llamamos al modelo parcial con el TensorFlow Hub (sin la última capa de clasificación) <code>feature_extractor</code> .  Este nombre se explica por el hecho de que el modelo acepta datos como entrada y los transforma en un conjunto finito de propiedades (características) seleccionadas.  Por lo tanto, nuestro modelo hizo el trabajo de identificar el contenido de la imagen, pero no produjo la distribución de probabilidad final sobre las clases de salida.  El modelo extrajo un conjunto de propiedades de la imagen. </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2'</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p>  Ejecutemos un conjunto de imágenes a través de <code>feature_extractor</code> y observemos el formulario resultante (formato de salida).  32 - el número de imágenes, 1280 - el número de neuronas en la última capa del modelo pre-entrenado con el TensorFlow Hub. </p><br><pre> <code class="python hljs">feature_batch = feature_extractor(image_batch) print(feature_batch.shape)</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">(32, 1280)</code> </pre> <br><p>  "Congelamos" las variables en la capa de extracción de propiedades para que solo los valores de las variables de la capa de clasificación cambien durante el proceso de capacitación. </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p>  <strong>Agregar una capa de clasificación</strong> </p><br><p>  Ahora envuelva la capa del TensorFlow Hub en el modelo <code>tf.keras.Sequential</code> y agregue una capa de clasificación. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 2) 2562 ================================================================= Total params: 2,260,546 Trainable params: 2,562 Non-trainable params: 2,257,984 _________________________________________________________________</code> </pre> <br><p>  <strong>Modelo de tren</strong> </p><br><p>  Ahora entrenamos el modelo resultante de la forma en que lo hicimos antes de llamar a <code>compile</code> seguido de <code>fit</code> para entrenamiento. </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] ) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">Epoch 1/6 582/582 [==============================] - 77s 133ms/step - loss: 0.2381 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 582/582 [==============================] - 70s 120ms/step - loss: 0.1827 - acc: 0.9618 - val_loss: 0.1629 - val_acc: 0.9670 Epoch 3/6 582/582 [==============================] - 69s 119ms/step - loss: 0.1733 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9666 Epoch 4/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1677 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9677 Epoch 5/6 582/582 [==============================] - 68s 118ms/step - loss: 0.1636 - acc: 0.9689 - val_loss: 0.1634 - val_acc: 0.9675 Epoch 6/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1604 - acc: 0.9701 - val_loss: 0.1643 - val_acc: 0.9668</code> </pre> <br><p>  Como probablemente notó, pudimos lograr ~ 97% de precisión de las predicciones en el conjunto de datos de validación.  Impresionante!  El enfoque actual ha aumentado significativamente la precisión de la clasificación en comparación con el primer modelo en el que nos capacitamos y obtuvimos una precisión de clasificación de ~ 87%.  La razón es que MobileNet fue diseñado por expertos y desarrollado cuidadosamente durante un largo período de tiempo, y luego capacitado en un conjunto de datos ImageNet increíblemente grande. </p><br><p>  Puede ver cómo crear su propia MobileNet en Keras en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p>  Creemos gráficos de cambios en la precisión y los valores de pérdida en los conjuntos de datos de capacitación y validación. </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/o7/5c/7a/o75c7aieqvmudmyglkroiddrm90.png"></p><br><p>  Lo interesante aquí es que los resultados en el conjunto de datos de validación son mejores que los resultados en el conjunto de datos de capacitación desde el principio hasta el final del proceso de aprendizaje. </p><br><p>  Una razón para este comportamiento es que la precisión en el conjunto de datos de validación se mide al final de la iteración de entrenamiento, y la precisión en el conjunto de datos de entrenamiento se considera como el valor promedio entre todas las iteraciones de entrenamiento. </p><br><p>  La razón principal de este comportamiento es el uso de la subred MobileNet pre-entrenada, que fue entrenada previamente en un gran conjunto de datos de gatos y perros.  En el proceso de aprendizaje, nuestra red continúa expandiendo el conjunto de datos de entrenamiento de entrada (el mismo aumento), pero no el conjunto de validación.  Esto significa que las imágenes generadas en el conjunto de datos de entrenamiento son más difíciles de clasificar que las imágenes normales del conjunto de datos validados. </p><br><p>  <strong>Verificar los resultados de la predicción</strong> </p><br><p>  Para repetir el gráfico de la sección anterior, primero debe obtener una lista ordenada de nombres de clase: </p><br><pre> <code class="python hljs">class_names = np.array(info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) class_names</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">array(['cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Pase el bloque con imágenes a través del modelo y convierta los índices resultantes en nombres de clase: </p><br><pre> <code class="python hljs">predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] predicted_class_names</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">array(['cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Echemos un vistazo a las etiquetas verdaderas y pronosticadas: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, predicted_ids)</code> </pre> <br><p>  Conclusión </p><br><pre> <code class="plaintext hljs">: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1] : [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1]</code> </pre> <br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"  (: , : )"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/dc/ox/4w/dcox4wk3fek_1e2j9ltjmvai1ia.png"></p><br><h1>  Zambullirse en redes neuronales convolucionales </h1><br><p>  Utilizando redes neuronales convolucionales, logramos asegurarnos de que se las arreglan bien con la tarea de clasificar imágenes.  Sin embargo, por el momento, apenas podemos imaginar cómo funcionan realmente.  Si pudiéramos entender cómo ocurre el proceso de aprendizaje, entonces, en principio, podríamos mejorar aún más el trabajo de clasificación.  Una forma de entender cómo funcionan las redes neuronales convolucionales es visualizar las capas y los resultados de su trabajo.  Le recomendamos que estudie los materiales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí</a> para comprender mejor cómo visualizar los resultados de las capas convolucionales. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/50f/d7a/3eb/50fd7a3eb31650740807d84eb8ff1da2.gif" alt="imagen"></p><br><p>  El campo de visión por computadora vio la luz al final del túnel y ha progresado significativamente desde el advenimiento de las redes neuronales convolucionales.  La increíble velocidad con la que se lleva a cabo la investigación en esta área y las enormes series de imágenes publicadas en Internet han dado resultados increíbles en los últimos años.  El auge de las redes neuronales convolucionales comenzó con AlexNet en 2012, que fue creado por Alex Krizhevsky, Ilya Sutskever y Jeffrey Hinton y ganó el famoso desafío de reconocimiento visual a gran escala ImageNet.  Desde entonces, no había duda en el futuro brillante utilizando redes neuronales convolucionales, y el campo de la visión por computadora y los resultados del trabajo en él solo confirmaron este hecho.  Comenzando por reconocer su cara en un teléfono móvil y terminando con el reconocimiento de objetos en automóviles autónomos, las redes neuronales convolucionales ya han logrado mostrar y demostrar su fuerza y ​​resolver muchos problemas del mundo real. </p><br><p>  A pesar de la gran cantidad de grandes conjuntos de datos y modelos pre-entrenados de redes neuronales convolucionales, a veces es extremadamente difícil entender cómo funciona la red y para qué está capacitada exactamente esta red, especialmente para las personas que no tienen el conocimiento suficiente en el campo del aprendizaje automático.                 ,            , ,   Inception,   .                     .            ,    ,         ,         ,         . </p><br><p>       "   Python"  <br> François Chollet.   ,        .    Keras,     ,   " " TensorFlow, MXNET  Theano.   ,        ,            .           ,       . </p><br><p> <strong>  </strong> </p><br><p>            ,    ,           . </p><br><p>             (training accuracy)     .         ,          ,        , ,   Inception,                . </p><br><p>           ,       ,      .   Inception v3 (     ImageNet)     ,    Kaggle.         Inception,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> ,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Inception v3</a>      . </p><br><p>     10  ()     32 ,    2292293.           0.3195,     — 0.6377.     <code>ImageDataGenerator</code>     ,      .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub </a> . </p><br><p> <strong>  </strong> </p><br><p>             ,    ""   ,      .               . </p><br><p> ,              Inception v3 ,        . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d4c/ac1/6f5/d4cac16f506f3d14ab4cca070f4d876b.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/222/102/eda/222102eda480f7108db0c9869ab46057.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca8/176/7c0/ca81767c0f5cae4748f1db6cee625e01.jpg" alt="imagen"></p><br><p>    —     .             . </p><br><p>         ,              ()   .        (),       , ,  ,      .          ,      ,        ,     ,       . </p><br><p>   ReLU-    .    ,     <code>ReLU(z) = max(0, z)</code>     . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9e3/b87/e17/9e3b87e175577fe97da51fd1a2b50eac.png" alt="imagen"></p><br><p>          ,   ,   ,        ,      ,           ,   , ,   ..            ,            .     "" ()     ,            ,     ,             . </p><br><p> <strong>   </strong> </p><br><p>         ""        .               . </p><br><p>   ,     Inveption V3      : </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/00e/a03/d78/00ea03d78e161d7f6fff59ba1a133309.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/247/875/0da/2478750da1a4eb167f8dc1c9c55252d6.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4a9/635/bd8/4a9635bd81a0d2e46435a05c39d3457a.jpg" alt="imagen"></p><br><p>              ,         . ,                   ,      ,           ,   ..          ,       ,                .            ,       ,  ,           "" ( ,      ). </p><br><p> <strong>    </strong> </p><br><p>       ,         , ,      .      ,                  . </p><br><p>     Class Activation Map (  ).      CAM       .       2D              ,                 . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f8/95e/f8b/2f895ef8b9086c9ea56745ce0f441ef9.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/6ff/392/e60/6ff392e60f007791bee52e439099759f.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fe9/896/210/fe9896210055693195c21a96b74f3188.jpg" alt="imagen"></p><br><p>       ,     .    ,    ,        Mixed-  Inception V3-,        .        () ,           . </p><br><p>     ,          ,        .          <strong></strong> ,            ,        .       ,          .   ,                 ,        ,    ,        . </p><br><p>           ,      ""  -          .               .             . </p><br><p>    ,           ,                   . </p><br><h1>  :      </h1><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colab  </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colab  </a> . </p><br><p> <strong>TensorFlow Hub</strong> </p><br><p> TensorFlow Hub      ,      . </p><br><p>                  .     ,      ,   ,          . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>      . </p><br><p>              <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p> <strong></strong> </p><br><p>  ,    : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p> <strong>      TensorFlow Datasets</strong> </p><br><p>            TensorFlow Datasets.   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ,       — <code>tf_flowers</code> .        ,       .                <code>tfds.splits</code>   (70%)   (30%).        <code>tfds.load</code> .    <code>tfds.load</code> ,            ,      . </p><br><pre> <code class="python hljs">splits = tfds.Split.TRAIN.subsplit([<span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>]) (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset tf_flowers (218.21 MiB) to /root/tensorflow_datasets/tf_flowers/1.0.0... Dl Completed... 1/|/100% 1/1 [00:07&lt;00:00, 3.67s/ url] Dl Size... 218/|/100% 218/218 [00:07&lt;00:00, 30.69 MiB/s] Extraction completed... 1/|/100% 1/1 [00:07&lt;00:00, 7.05s/ file] Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/1.0.0. Subsequent calls will reuse this data.</code> </pre> <br><p> <strong>     </strong> </p><br><p> ,      ,    ()         ,      ,          —   . </p><br><pre> <code class="python hljs">num_classes = dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes num_training_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> num_validation_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> training_set: num_training_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> validation_set: num_validation_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> print(<span class="hljs-string"><span class="hljs-string">'Total Number of Classes: {}'</span></span>.format(num_classes)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Training Images: {}'</span></span>.format(num_training_examples)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Validation Images: {} \n'</span></span>.format(num_validation_examples))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Total Number of Classes: 5 Total Number of Training Images: 2590 Total Number of Validation Images: 1080</code> </pre> <br><p>        —  . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(training_set.take(<span class="hljs-number"><span class="hljs-number">5</span></span>)): print(<span class="hljs-string"><span class="hljs-string">'Image {} shape: {} label: {}'</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape, example[<span class="hljs-number"><span class="hljs-number">1</span></span>]))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Image 1 shape: (226, 240, 3) label: 0 Image 2 shape: (240, 145, 3) label: 2 Image 3 shape: (331, 500, 3) label: 2 Image 4 shape: (240, 320, 3) label: 0 Image 5 shape: (333, 500, 3) label: 1</code> </pre> <br><p> <strong>     </strong> </p><br><p>          — ,   MobilNet v2     — 224224     (grayscale).     <code>image</code> ()  <code>label</code> ()       . </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/<span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> <strong>    TensorFlow Hub</strong> </p><br><p>    TensorFlow Hub   . ,                      ,         . </p><br><p> <strong>   </strong> </p><br><p>      <code>feature_extractor</code>  MobileNet v2. ,      TensorFlow Hub (   )   .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .   <code>tf2-preview/mobilenet_v2/feature_vector</code> ,     URL       MobileNet v2 .   <code>feature_extractor</code>   <code>hub.KerasLayer</code>      <code>input_shape</code> . </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p> <strong>   </strong> </p><br><p>                   ,   : </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p> <strong>  </strong> </p><br><p>               ,   .            .          . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 5) 6405 ================================================================= Total params: 2,264,389 Trainable params: 6,405 Non-trainable params: 2,257,984</code> </pre> <br><p> <strong> </strong> </p><br><p>            ,           . </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 17s 216ms/step - loss: 0.7765 - acc: 0.7170 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 12s 147ms/step - loss: 0.3806 - acc: 0.8757 - val_loss: 0.3485 - val_acc: 0.8833 Epoch 3/6 81/81 [==============================] - 12s 146ms/step - loss: 0.3011 - acc: 0.9031 - val_loss: 0.3190 - val_acc: 0.8907 Epoch 4/6 81/81 [==============================] - 12s 147ms/step - loss: 0.2527 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8917 Epoch 5/6 81/81 [==============================] - 12s 148ms/step - loss: 0.2177 - acc: 0.9371 - val_loss: 0.2933 - val_acc: 0.8972 Epoch 6/6 81/81 [==============================] - 12s 146ms/step - loss: 0.1905 - acc: 0.9456 - val_loss: 0.2870 - val_acc: 0.9000</code> </pre> <br><p>         ~90%  6  ,     !   ,    ,           ~76%  80  .        ,  MobilNet v2                . </p><br><p> <strong>         </strong> </p><br><p>               . </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'Training Accuracy'</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'Validation Accuracy'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Accuracy'</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'Training Loss'</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'Validation Loss'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Loss'</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/ox/nb/hy/oxnbhyqanark0qrt1xmeqg3qv4a.png"></p><br><p>   ,   ,                    . </p><br><p>        ,           ,              . </p><br><p>        - MobileNet,           .              (  augmentation),    .                  . </p><br><p> <strong> </strong> </p><br><p>              NumPy.     ,      . </p><br><pre> <code class="python hljs">class_names = np.array(dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) print(class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['dandelion' 'daisy' 'tulips' 'sunflowers' 'roses']</code> </pre> <br><p> <strong>        </strong> </p><br><p>   <code>next()</code>   <code>image_batch</code> ( )   <code>label_batch</code> ( ).   <code>image_batch</code>  <code>label_batch</code>  NumPy     <code>.numpy()</code> .    <code>.predict()</code>      .        <code>np.argmax()</code>   .            . </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches)) image_batch = image_batch.numpy() label_batch = label_batch.numpy() predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] print(predicted_class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['sunflowers' 'roses' 'tulips' 'tulips' 'daisy' 'dandelion' 'tulips' 'sunflowers' 'daisy' 'daisy' 'tulips' 'daisy' 'daisy' 'tulips' 'tulips' 'tulips' 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'roses' 'daisy' 'daisy' 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'roses' 'dandelion']</code> </pre> <br><p> <strong>     </strong> </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Labels: "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">"Predicted labels: "</span></span>, predicted_ids)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0] Predicted labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0]</code> </pre> <br><p> <strong>  </strong> </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"Model predictions (blue: correct, red: incorrect)"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/e1/q-/rg/e1q-rgvnr6qns8-vvrcr8yzbexi.png"></p><br><p> <strong>     Inception-</strong> </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> TensorFlow Hub</a>     <code>tf2-preview/inception_v3/feature_vector</code> .        Inception V3 .      ,      Inception V3     .  ,  Inception V3       299299 .   Inception V3    MobileNet V2. </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">299</span></span> (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits) train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) model_inception = tf.keras.Sequential([ feature_extractor, tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model_inception.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 2048) 21802784 _________________________________________________________________ dense_1 (Dense) (None, 5) 10245 ================================================================= Total params: 21,813,029 Trainable params: 10,245 Non-trainable params: 21,802,784</code> </pre> <br><pre> <code class="python hljs">model_inception.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model_inception.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 44s 541ms/step - loss: 0.7594 - acc: 0.7309 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3927 - acc: 0.8772 - val_loss: 0.3945 - val_acc: 0.8657 Epoch 3/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3074 - acc: 0.9120 - val_loss: 0.3586 - val_acc: 0.8769 Epoch 4/6 81/81 [==============================] - 35s 434ms/step - loss: 0.2588 - acc: 0.9282 - val_loss: 0.3385 - val_acc: 0.8796 Epoch 5/6 81/81 [==============================] - 35s 436ms/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.3256 - val_acc: 0.8824 Epoch 6/6 81/81 [==============================] - 35s 435ms/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3164 - val_acc: 0.8861</code> </pre> <br><h1>  Resumen </h1><br><p>                   .          : </p><br><ul><li> <strong> :</strong> ,                .              . </li><li> <strong> :</strong>       . ""     ,       ,      . </li><li> <strong>MobileNet:</strong>       Google,                  . MobileNet              . </li></ul><br><p>              MobileNet      .                     .                   MobileNet    . </p><br><p> …   call-to-action — ,     share :) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegrama</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VKontakte</a> <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ojok</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467967/">https://habr.com/ru/post/467967/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467953/index.html">Probar servidores virtuales baratos</a></li>
<li><a href="../467957/index.html">Lo que hay detrás de la constante de Feigenbaum</a></li>
<li><a href="../467959/index.html">Cosmología y fluctuaciones cuánticas en el navegador.</a></li>
<li><a href="../467961/index.html">Problemas y matices al desarrollar para SmartTV usando React.js</a></li>
<li><a href="../467965/index.html">Vivaldi 2.8 - Menú, por favor</a></li>
<li><a href="../467969/index.html">Presentaciones modales en pantalla modal en iOS 13</a></li>
<li><a href="../467973/index.html">Nacimiento de la plataforma</a></li>
<li><a href="../467975/index.html">Huawei Dorado V6: calor de Sichuan</a></li>
<li><a href="../467977/index.html">Crear una aplicación usando componentes con estilo en Vue.js</a></li>
<li><a href="../467979/index.html">Integraciones publicitarias: ¿cómo funciona?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>