<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>   Inmersi贸n en redes neuronales convolucionales: transferencia de aprendizaje   </title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El curso completo en ruso se puede encontrar en este enlace . 
 El curso de ingl茅s original est谩 disponible en este enlace . 



 Contenido 


1. Entr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Inmersi贸n en redes neuronales convolucionales: transferencia de aprendizaje</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467967/"><p>  El curso completo en ruso se puede encontrar en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . <br>  El curso de ingl茅s original est谩 disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p><img src="https://habrastorage.org/webt/wu/ie/7c/wuie7cgpktklm4bytoweu7ki0oq.jpeg"></p><a name="habracut"></a><br><h1>  Contenido </h1><br><ol><li>  Entrevista con Sebastian Trun </li><li>  Introduccion </li><li>  Modelo de aprendizaje de transferencia </li><li>  MobileNet </li><li>  CoLab: Gatos Vs Perros con entrenamiento de transferencia </li><li>  Zambullirse en redes neuronales convolucionales </li><li>  Parte pr谩ctica: determinaci贸n de colores con la transferencia de formaci贸n. </li><li>  Resumen </li></ol><br><h1>  Entrevista con Sebastian Trun </h1><br><p>  - Esta es la lecci贸n 6 y est谩 completamente dedicada a la transferencia de aprendizaje.  La transferencia de aprendizaje es el proceso de usar un modelo existente con poco refinamiento para nuevas tareas.  La transferencia de capacitaci贸n ayuda a reducir el tiempo de capacitaci贸n del modelo al aumentar la eficiencia al aprender desde el principio.  Sebastian, 驴qu茅 opinas sobre la transferencia de entrenamiento?  驴Alguna vez ha podido utilizar la metodolog铆a de transferencia de ense帽anza en su trabajo e investigaci贸n? <br>  - Mi disertaci贸n se dedic贸 solo al tema de la transferencia de capacitaci贸n y se denomin贸 " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Explicaci贸n sobre la base de la transferencia de capacitaci贸n</a> ".  Cuando est谩bamos trabajando en una disertaci贸n, la idea era que es posible ense帽ar a distinguir todos los dem谩s objetos de este tipo en un objeto (conjunto de datos, entidad) en diversas variaciones y formatos.  En el trabajo, utilizamos el algoritmo desarrollado, que distingu铆a las caracter铆sticas principales (atributos) del objeto y pod铆a compararlas con otro objeto.  Las bibliotecas como Tensorflow ya vienen con modelos pre-entrenados. <br>  - S铆, en Tensorflow tenemos un conjunto completo de modelos pre-entrenados que puede usar para resolver problemas pr谩cticos.  Hablaremos sobre sets ya hechos un poco m谩s tarde. <br>  - Si, si!  Si lo piensa, las personas se dedican a la transferencia de capacitaci贸n todo el tiempo a lo largo de sus vidas. <br>  - 驴Podemos decir que gracias al m茅todo de transferencia de capacitaci贸n, nuestros nuevos estudiantes en alg煤n momento no tendr谩n que saber algo sobre el aprendizaje autom谩tico porque ser谩 suficiente para conectar un modelo ya preparado y usarlo? <br>  - La programaci贸n es escribir l铆nea por l铆nea, le damos comandos a la computadora.  Nuestro objetivo es asegurarnos de que todos en el planeta puedan y puedan programar proporcionando a la computadora solo ejemplos de datos de entrada.  De acuerdo, si quieres ense帽arle a una computadora a distinguir gatos de perros, entonces encontrar 100k im谩genes diferentes de gatos y 100k im谩genes diferentes de perros es bastante dif铆cil, y gracias a la transferencia de entrenamiento puedes resolver este problema en varias l铆neas. <br>  - S铆, realmente lo es!  Gracias por las respuestas y finalmente pasemos al aprendizaje. </p><br><h1>  Introduccion </h1><br><p>  - Hola y bienvenido de nuevo! <br>  - La 煤ltima vez entrenamos una red neuronal convolucional para clasificar gatos y perros en la imagen.  Nuestra primera red neuronal se volvi贸 a entrenar, por lo que su resultado no fue tan alto: aproximadamente 70% de precisi贸n.  Despu茅s de eso, implementamos la extensi贸n y el abandono de datos (desconexi贸n arbitraria de las neuronas), lo que nos permiti贸 aumentar la precisi贸n de las predicciones hasta en un 80%. <br>  - A pesar de que el 80% puede parecer un excelente indicador, el error del 20% sigue siendo demasiado grande.  Derecho?  驴Qu茅 podemos hacer para aumentar la precisi贸n de la clasificaci贸n?  En esta lecci贸n, utilizaremos la t茅cnica de transferencia de conocimiento (transferencia del modelo de conocimiento), que nos permitir谩 usar el modelo desarrollado por expertos y capacitados en grandes conjuntos de datos.  Como veremos en la pr谩ctica, al transferir el modelo de conocimiento podemos lograr una precisi贸n de clasificaci贸n del 95%.  隆Empecemos! </p><br><h1>  Transferencia de modelo de aprendizaje </h1><br><p>  En 2012, la red neuronal AlexNet revolucion贸 el mundo del aprendizaje autom谩tico y populariz贸 el uso de redes neuronales convolucionales para la clasificaci贸n al ganar el desaf铆o de reconocimiento visual ImageNet a gran escala. </p><br><p><img src="https://habrastorage.org/webt/fs/xx/di/fsxxdicwkitfkxibie8kkjcyexu.png"></p><br><p>  Despu茅s de eso, la lucha comenz贸 a desarrollar redes neuronales m谩s precisas y eficientes que podr铆an superar a AlexNet en las tareas de clasificar im谩genes del conjunto de datos ImageNet. </p><br><p><img src="https://habrastorage.org/webt/oe/t0/ov/oet0ovye40p1cziih64hpmbhuus.png"></p><br><p>  Durante varios a帽os, se han desarrollado redes neuronales que hacen frente a la tarea de clasificaci贸n mejor que AlexNet - Inception y ResNet. <br>  驴Est谩 de acuerdo en que ser铆a genial poder aprovechar estas redes neuronales ya capacitadas en grandes conjuntos de datos de ImageNet y usarlas en su clasificador de perros y gatos? </p><br><p>  隆Resulta que podemos hacerlo!  La t茅cnica se llama transferencia de aprendizaje.  La idea principal del m茅todo de transferencia del modelo de entrenamiento se basa en el hecho de que despu茅s de haber entrenado una red neuronal en un gran conjunto de datos, podemos aplicar el modelo obtenido a un conjunto de datos que este modelo a煤n no ha encontrado.  Es por eso que la t茅cnica se llama aprendizaje de transferencia: transferir el proceso de aprendizaje de un conjunto de datos a otro. </p><br><p>  Para poder aplicar la metodolog铆a de transferencia del modelo de entrenamiento, necesitamos cambiar la 煤ltima capa de nuestra red neuronal convolucional: </p><br><p><img src="https://habrastorage.org/webt/3j/-g/g3/3j-gg3yxm9kvrtphiswnho2jgtc.png"></p><br><p>  Realizamos esta operaci贸n porque cada conjunto de datos consta de un n煤mero diferente de clases de salida.  Por ejemplo, los conjuntos de datos en ImageNet contienen 1000 clases de salida diferentes.  FashionMNIST contiene 10 clases.  Nuestro conjunto de datos de clasificaci贸n consta de solo 2 clases: gatos y perros. </p><br><p><img src="https://habrastorage.org/webt/5e/pm/ej/5epmejbamklkdfgzzw9v8rzb1ts.png"></p><br><p>  Es por eso que es necesario cambiar la 煤ltima capa de nuestra red neuronal convolucional para que contenga el n煤mero de salidas que corresponder铆a al n煤mero de clases en el nuevo conjunto. </p><br><p><img src="https://habrastorage.org/webt/cg/mk/fz/cgmkfzxqmyqbdzsmvfdppjhsl9e.png"></p><br><p>  Tambi茅n debemos asegurarnos de no cambiar el modelo pre-entrenado durante el proceso de capacitaci贸n.  La soluci贸n es desactivar las variables del modelo pre-entrenado: simplemente prohibimos que el algoritmo actualice los valores durante la propagaci贸n hacia adelante y hacia atr谩s para cambiarlos. <br>  Este proceso se llama "congelar el modelo". </p><br><p><img src="https://habrastorage.org/webt/8z/oz/0n/8zoz0nenaad4-lgezhhia25k_18.png"></p><br><p>  Al "congelar" los par谩metros del modelo pre-entrenado, nos permitimos aprender solo la 煤ltima capa de la red de clasificaci贸n, los valores de las variables del modelo pre-entrenado permanecen sin cambios. </p><br><p>  Otra ventaja indiscutible de los modelos pre-entrenados es que reducimos el tiempo de entrenamiento al entrenar solo la 煤ltima capa con un n煤mero significativamente menor de variables, y no todo el modelo. </p><br><p>  Si no "congelamos" las variables del modelo pre-entrenado, entonces durante el proceso de entrenamiento, los valores de las variables cambiar谩n en el nuevo conjunto de datos.  Esto se debe a que los valores de las variables en la 煤ltima capa de la clasificaci贸n se rellenar谩n con valores aleatorios.  Debido a los valores aleatorios en la 煤ltima capa, nuestro modelo cometer谩 grandes errores en la clasificaci贸n, lo que, a su vez, implicar谩 fuertes cambios en los pesos iniciales en el modelo pre-entrenado, lo cual es extremadamente indeseable para nosotros. </p><br><p><img src="https://habrastorage.org/webt/wp/uz/km/wpuzkmnan5rahfg3irexdsvrstm.png"></p><br><p>  Es por esta raz贸n que siempre debemos recordar que cuando se usan modelos existentes, los valores de las variables deben "congelarse" y la necesidad de entrenar un modelo pre-entrenado debe desactivarse. </p><br><p>  Ahora que sabemos c贸mo funciona la transferencia del modelo de entrenamiento, 隆solo tenemos que elegir una red neuronal pre-entrenada para usar en nuestro propio clasificador!  Esto lo haremos en la siguiente parte. </p><br><h1>  MobileNet </h1><br><p>  Como mencionamos anteriormente, se desarrollaron redes neuronales extremadamente eficientes que mostraron altos resultados en los conjuntos de datos de ImageNet: AlexNet, Inception, Resonant.  Estas redes neuronales son redes muy profundas y contienen miles e incluso millones de par谩metros.  Una gran cantidad de par谩metros permite a la red aprender patrones m谩s complejos y, por lo tanto, lograr una mayor precisi贸n de clasificaci贸n.  Una gran cantidad de par谩metros de entrenamiento de la red neuronal afecta la velocidad de aprendizaje, la cantidad de memoria requerida para almacenar la red y la complejidad de los c谩lculos. </p><br><p>  En esta lecci贸n usaremos la moderna red neuronal convolucional MobileNet.  MobileNet es una arquitectura de red neuronal convolucional eficiente que reduce la cantidad de memoria utilizada para la computaci贸n al tiempo que mantiene una alta precisi贸n de las predicciones.  Es por eso que MobileNet es ideal para usar en dispositivos m贸viles con una cantidad limitada de memoria y recursos inform谩ticos. </p><br><p>  MobileNet fue desarrollado por Google y capacitado en el conjunto de datos ImageNet. </p><br><p>  Dado que MobileNet recibi贸 capacitaci贸n en 1,000 clases del conjunto de datos de ImageNet, MobileNet tiene 1,000 clases de salida, en lugar de las dos que necesitamos: un gato y un perro. </p><br><p><img src="https://habrastorage.org/webt/he/2f/ox/he2foxizd_xmt7rijxumteg-94k.png"></p><br><p>  Para completar la transferencia de capacitaci贸n, precargamos el vector de caracter铆sticas sin una capa de clasificaci贸n: </p><br><p><img src="https://habrastorage.org/webt/1b/o2/no/1bo2nop9cpz3ebag_jcyfrgje7w.png"></p><br><p>  En Tensorflow, un vector de caracter铆sticas cargado puede usarse como una capa Keras normal con datos de entrada de cierto tama帽o. </p><br><p>  Como MobileNet recibi贸 capacitaci贸n en el conjunto de datos de ImageNet, tendremos que llevar el tama帽o de los datos de entrada a los que se utilizaron en el proceso de capacitaci贸n.  En nuestro caso, MobileNet recibi贸 capacitaci贸n en im谩genes RGB de 224x224px de tama帽o fijo. </p><br><p>  TensorFlow contiene un repositorio previamente entrenado llamado TensorFlow Hub. </p><br><p><img src="https://habrastorage.org/webt/o5/we/9d/o5we9dvkdtaomahwj4nzomgquca.png"></p><br><p>  TensorFlow Hub contiene algunos modelos previamente entrenados en los que la 煤ltima capa de clasificaci贸n se excluy贸 de la arquitectura de la red neuronal para su posterior reutilizaci贸n. </p><br><p>  Puede usar el TensorFlow Hub en el c贸digo en varias l铆neas: </p><br><p><img src="https://habrastorage.org/webt/1h/ai/hg/1haihgg1llinsfv_wyhde4z0egy.png"></p><br><p>  Es suficiente especificar la URL del vector de caracter铆sticas del modelo de entrenamiento deseado y luego incrustar el modelo en nuestro clasificador con la 煤ltima capa con el n煤mero deseado de clases de salida.  Es la 煤ltima capa que se someter谩 a entrenamiento y a cambio de valores de par谩metros.  La compilaci贸n y capacitaci贸n de nuestro nuevo modelo se lleva a cabo de la misma manera que lo hicimos antes: </p><br><p><img src="https://habrastorage.org/webt/uh/cr/5e/uhcr5esktfnxtxwxfgutvttxbxk.png"></p><br><p>  Veamos c贸mo funcionar谩 esto realmente y escriba el c贸digo apropiado. </p><br><h1>  CoLab: Gatos Vs Perros con entrenamiento de transferencia </h1><br><p>  Enlace a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CoLab en ruso</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CoLab en ingl茅s</a> . </p><br><p>  TensorFlow Hub es un repositorio con modelos pre-entrenados que podemos usar. </p><br><p>  La transferencia de aprendizaje es un proceso en el que tomamos un modelo previamente entrenado y lo expandimos para realizar una tarea espec铆fica.  Al mismo tiempo, dejamos intacta la parte del modelo pre-entrenado que integramos en la red neuronal, pero solo entrenamos las 煤ltimas capas de salida para obtener el resultado deseado. </p><br><p>  En esta parte pr谩ctica, probaremos ambas opciones. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Este enlace le</a> permite explorar la lista completa de modelos disponibles. </p><br><p>  <strong>En esta parte de Colab</strong> </p><br><ol><li>  Usaremos el modelo TensorFlow Hub para las predicciones; </li><li>  Usaremos el modelo TensorFlow Hub para el conjunto de datos de gatos y perros; </li><li>  Transfieramos el entrenamiento usando el modelo del TensorFlow Hub. </li></ol><br><p> Antes de continuar con la implementaci贸n de la parte pr谩ctica actual, recomendamos restablecer el <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p>  <strong>Importaciones de la biblioteca</strong> </p><br><p>  En esta parte pr谩ctica, utilizaremos una serie de caracter铆sticas de la biblioteca TensorFlow que a煤n no est谩n en el lanzamiento oficial.  Es por eso que primero instalaremos la versi贸n TensorFlow y TensorFlow Hub para desarrolladores. </p><br><p>  La instalaci贸n de la versi贸n de desarrollo de TensorFlow activa autom谩ticamente la 煤ltima versi贸n instalada.  Despu茅s de que terminemos de tratar con esta parte pr谩ctica, recomendamos restaurar la configuraci贸n de TensorFlow y volver a la versi贸n estable a trav茅s del elemento de men煤 <code>Runtime -&gt; Reset all runtimes...</code>  La ejecuci贸n de este comando restablecer谩 todas las configuraciones del entorno a las originales. </p><br><pre> <code class="python hljs">!pip install tf-nightly-gpu !pip install <span class="hljs-string"><span class="hljs-string">"tensorflow_hub==0.4.0"</span></span> !pip install -U tensorflow_datasets</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1) Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7) Collecting tf-estimator-nightly (from tf-nightly-gpu) Downloading https://files.pythonhosted.org/packages/ea/72/f092fc631ef2602fd0c296dcc4ef6ef638a6a773cb9fdc6757fecbfffd33/tf_estimator_nightly-1.14.0.dev2019092201-py2.py3-none-any.whl (450kB) || 450kB 45.9MB/s Requirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.5) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2) Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.0.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.6) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tf-nightly-gpu) (2.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (41.2.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (0.15.6) Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu Successfully installed tb-nightly-1.15.0a20190911 tf-estimator-nightly-1.14.0.dev2019092201 tf-nightly-gpu-1.15.0.dev20190821 Collecting tensorflow_hub==0.4.0 Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB) || 81kB 5.0MB/s Requirement already satisfied: protobuf&gt;=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (3.7.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.16.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.12.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.4.0-&gt;tensorflow_hub==0.4.0) (41.2.0) Installing collected packages: tensorflow-hub Found existing installation: tensorflow-hub 0.6.0 Uninstalling tensorflow-hub-0.6.0: Successfully uninstalled tensorflow-hub-0.6.0 Successfully installed tensorflow-hub-0.4.0 Collecting tensorflow_datasets Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB) || 2.3MB 4.9MB/s Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0) Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.11.2) Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.16.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.21.0) Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.1) Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (5.4.8) Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1) Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.8.0) Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.14.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.0) Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0) Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.1.0) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.8) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2019.6.16) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (1.24.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow_datasets) (41.2.0) Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.6.0) Installing collected packages: tensorflow-datasets Successfully installed tensorflow-datasets-1.2.0</code> </pre> <br><p>  Ya hemos visto y usado algunas importaciones antes.  Desde el nuevo - import <code>tensorflow_hub</code> , que instalamos y que utilizaremos en esta parte pr谩ctica. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0. Please upgrade your code to TensorFlow 2.0: * https://www.tensorflow.org/beta/guide/migration_guide Or install the latest stable TensorFlow 1.X release: * `pip install -U "tensorflow==1.*"` Otherwise your code may be broken by the change.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p>  <strong>Parte 1: use TensorFlow Hub MobileNet para predicciones</strong> </p><br><p>  En esta parte de CoLab, tomaremos un modelo previamente entrenado, lo cargaremos a Keras y lo probaremos. </p><br><p>  El modelo que utilizamos es MobileNet v2 (en lugar de MobileNet, se puede utilizar cualquier otro modelo de clasificador de im谩genes compatible con tf2 con tfhub.dev). </p><br><p>  <strong>Descargar clasificador</strong> </p><br><p>  Descargue el modelo MobileNet y cree un modelo Keras a partir de 茅l.  MobileNet en la entrada espera recibir una imagen de 224x224 p铆xeles de tama帽o con 3 canales de color (RGB). </p><br><pre> <code class="python hljs">CLASSIFIER_URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span></span> IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> model = tf.keras.Sequential([ hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>)) ])</code> </pre> <br><p>  <strong>Ejecute el clasificador en una sola imagen</strong> </p><br><p>  MobileNet ha sido entrenado en el conjunto de datos ImageNet.  ImageNet contiene 1000 clases de salida y una de estas clases es un uniforme militar.  Busquemos la imagen en la que se ubicar谩 el uniforme militar y que no formar谩 parte del kit de entrenamiento de ImageNet para verificar la precisi贸n de la clasificaci贸n. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PIL.Image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Image grace_hopper = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'image.jpg'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span></span>) grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES)) grace_hopper</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg 65536/61306 [================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/8g/gn/0u/8ggn0ur3pmnr_rxvezfdo4vrwcc.png"></p><br><pre> <code class="python hljs">grace_hopper = np.array(grace_hopper)/<span class="hljs-number"><span class="hljs-number">255.0</span></span> grace_hopper.shape</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">(224, 224, 3)</code> </pre> <br><p>  Tenga en cuenta que los modelos siempre reciben un conjunto (bloque) de im谩genes para procesar en la entrada.  En el siguiente c贸digo, agregamos una nueva dimensi贸n: el tama帽o del bloque. </p><br><pre> <code class="python hljs">result = model.predict(grace_hopper[np.newaxis, ...]) result.shape</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">(1, 1001)</code> </pre> <br><p>  El resultado de la predicci贸n fue un vector con un tama帽o de 1.001 elementos, donde cada valor representa la probabilidad de que el objeto en la imagen pertenezca a una determinada clase. </p><br><p>  La posici贸n del valor de probabilidad m谩xima se puede encontrar usando la funci贸n <code>argmax</code> .  Sin embargo, hay una pregunta que a煤n no hemos respondido: 驴c贸mo podemos determinar a qu茅 clase pertenece un elemento con la m谩xima probabilidad? </p><br><pre> <code class="python hljs">predicted_class = np.argmax(result[<span class="hljs-number"><span class="hljs-number">0</span></span>], axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">653</code> </pre> <br><p>  <strong>Descifrando predicciones</strong> </p><br><p>  Para que podamos determinar la clase a la que se refieren las predicciones, cargamos la lista de etiquetas ImageNet y, mediante el 铆ndice, con la m谩xima fidelidad, determinamos la clase a la que se refiere la predicci贸n. </p><br><pre> <code class="python hljs">labels_path = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'ImageNetLabels.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span></span>) imagenet_labels = np.array(open(labels_path).read().splitlines()) plt.imshow(grace_hopper) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) predicted_class_name = imagenet_labels[predicted_class] _ = plt.title(<span class="hljs-string"><span class="hljs-string">"Prediction: "</span></span> + predicted_class_name.title())</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt 16384/10484 [==============================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/ai/1k/rt/ai1krt6mkpcafhb5jx8ozf6mq8s.png"></p><br><p>  Bingo!  Nuestro modelo identific贸 correctamente el uniforme militar. </p><br><p>  <strong>Parte 2: use el modelo TensorFlow Hub para un conjunto de datos de perros y gatos</strong> </p><br><p>  Ahora usaremos la versi贸n completa del modelo MobileNet y veremos c贸mo manejar谩 el conjunto de datos de gatos y perros. </p><br><p>  <strong>Conjunto de datos</strong> </p><br><p>  Podemos usar los conjuntos de datos TensorFlow para descargar un conjunto de datos de perros y gatos. </p><br><pre> <code class="python hljs">splits = tfds.Split.ALL.subsplit(weighted=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) splits, info = tfds.load(<span class="hljs-string"><span class="hljs-string">'cats_vs_dogs'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split = splits) (train_examples, validation_examples) = splits num_examples = info.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_classes = info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/2.0.1... /usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) WARNING:absl:1738 images were corrupted and were skipped Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/2.0.1. Subsequent calls will reuse this data.</code> </pre><br><p>  No todas las im谩genes en un conjunto de datos de perros y gatos son del mismo tama帽o. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example_image <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_examples.take(<span class="hljs-number"><span class="hljs-number">3</span></span>)): print(<span class="hljs-string"><span class="hljs-string">"Image {} shape: {}"</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example_image[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape))</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Image 1 shape: (500, 343, 3) Image 2 shape: (375, 500, 3) Image 3 shape: (375, 500, 3)</code> </pre> <br><p>  Por lo tanto, las im谩genes del conjunto de datos obtenido requieren una reducci贸n a un tama帽o 煤nico, que el modelo MobileNet espera en la entrada: 224 x 224. </p><br><p>  La funci贸n <code>.repeat()</code> y <code>steps_per_epoch</code> no son necesarios aqu铆, pero le permiten ahorrar unos 15 segundos por iteraci贸n de entrenamiento, porque  el b煤fer temporal se debe inicializar solo una vez al comienzo del proceso de aprendizaje. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / <span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = train_examples.shuffle(num_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  <strong>Ejecute el clasificador en conjuntos de im谩genes</strong> </p><br><p>  Perm铆tame recordarle que en esta etapa, todav铆a hay una versi贸n completa de la red MobileNet pre-entrenada, que contiene 1,000 clases de salida posibles.  ImageNet contiene una gran cantidad de im谩genes de perros y gatos, as铆 que intentemos ingresar una de las im谩genes de prueba de nuestro conjunto de datos y ver qu茅 predicci贸n nos dar谩 el modelo. </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches.take(<span class="hljs-number"><span class="hljs-number">1</span></span>))) image_batch = image_batch.numpy() label_batch = label_batch.numpy() result_batch = model.predict(image_batch) predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)] predicted_class_names</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">array(['Persian cat', 'mink', 'Siamese cat', 'tabby', 'Bouvier des Flandres', 'dishwasher', 'Yorkshire terrier', 'tiger cat', 'tabby', 'Egyptian cat', 'Egyptian cat', 'tabby', 'dalmatian', 'Persian cat', 'Border collie', 'Newfoundland', 'tiger cat', 'Siamese cat', 'Persian cat', 'Egyptian cat', 'tabby', 'tiger cat', 'Labrador retriever', 'German shepherd', 'Eskimo dog', 'kelpie', 'mink', 'Norwegian elkhound', 'Labrador retriever', 'Egyptian cat', 'computer keyboard', 'boxer'], dtype='&lt;U30')</code> </pre> <br><p>  Las etiquetas son similares a los nombres de razas de gatos y perros.  Ahora muestremos algunas im谩genes de nuestro conjunto de datos de perros y gatos y coloquemos una etiqueta prevista en cada uno de ellos. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) plt.title(predicted_class_names[n]) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"ImageNet predictions"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/fc/af/w-/fcafw-kdtzotffq7k-nihlntuda.png"></p><br><p>  <strong>Parte 3: Implementar la transferencia de aprendizaje con el TensorFlow Hub</strong> </p><br><p>  Ahora usemos el TensorFlow Hub para transferir el aprendizaje de un modelo a otro. </p><br><p>  En el proceso de transferir el entrenamiento, reutilizamos un modelo pre-entrenado cambiando su 煤ltima capa, o varias capas, y luego comenzamos el proceso de entrenamiento nuevamente en un nuevo conjunto de datos. </p><br><p>  En TensorFlow Hub, puede encontrar no solo modelos completos previamente entrenados (con la 煤ltima capa), sino tambi茅n modelos sin la 煤ltima capa de clasificaci贸n.  Este 煤ltimo puede usarse f谩cilmente para transferir el entrenamiento.  Continuaremos usando MobileNet v2 por la sencilla raz贸n de que en las partes posteriores de nuestro curso transferiremos este modelo y lo lanzaremos en un dispositivo m贸vil usando TensorFlow Lite. </p><br><p>  Tambi茅n seguiremos utilizando el conjunto de datos de gatos y perros, por lo que tendremos la oportunidad de comparar el rendimiento de este modelo con los que implementamos desde cero. </p><br><p>  Tenga en cuenta que llamamos al modelo parcial con el TensorFlow Hub (sin la 煤ltima capa de clasificaci贸n) <code>feature_extractor</code> .  Este nombre se explica por el hecho de que el modelo acepta datos como entrada y los transforma en un conjunto finito de propiedades (caracter铆sticas) seleccionadas.  Por lo tanto, nuestro modelo hizo el trabajo de identificar el contenido de la imagen, pero no produjo la distribuci贸n de probabilidad final sobre las clases de salida.  El modelo extrajo un conjunto de propiedades de la imagen. </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2'</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p>  Ejecutemos un conjunto de im谩genes a trav茅s de <code>feature_extractor</code> y observemos el formulario resultante (formato de salida).  32 - el n煤mero de im谩genes, 1280 - el n煤mero de neuronas en la 煤ltima capa del modelo pre-entrenado con el TensorFlow Hub. </p><br><pre> <code class="python hljs">feature_batch = feature_extractor(image_batch) print(feature_batch.shape)</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">(32, 1280)</code> </pre> <br><p>  "Congelamos" las variables en la capa de extracci贸n de propiedades para que solo los valores de las variables de la capa de clasificaci贸n cambien durante el proceso de capacitaci贸n. </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p>  <strong>Agregar una capa de clasificaci贸n</strong> </p><br><p>  Ahora envuelva la capa del TensorFlow Hub en el modelo <code>tf.keras.Sequential</code> y agregue una capa de clasificaci贸n. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 2) 2562 ================================================================= Total params: 2,260,546 Trainable params: 2,562 Non-trainable params: 2,257,984 _________________________________________________________________</code> </pre> <br><p>  <strong>Modelo de tren</strong> </p><br><p>  Ahora entrenamos el modelo resultante de la forma en que lo hicimos antes de llamar a <code>compile</code> seguido de <code>fit</code> para entrenamiento. </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] ) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">Epoch 1/6 582/582 [==============================] - 77s 133ms/step - loss: 0.2381 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 582/582 [==============================] - 70s 120ms/step - loss: 0.1827 - acc: 0.9618 - val_loss: 0.1629 - val_acc: 0.9670 Epoch 3/6 582/582 [==============================] - 69s 119ms/step - loss: 0.1733 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9666 Epoch 4/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1677 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9677 Epoch 5/6 582/582 [==============================] - 68s 118ms/step - loss: 0.1636 - acc: 0.9689 - val_loss: 0.1634 - val_acc: 0.9675 Epoch 6/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1604 - acc: 0.9701 - val_loss: 0.1643 - val_acc: 0.9668</code> </pre> <br><p>  Como probablemente not贸, pudimos lograr ~ 97% de precisi贸n de las predicciones en el conjunto de datos de validaci贸n.  Impresionante!  El enfoque actual ha aumentado significativamente la precisi贸n de la clasificaci贸n en comparaci贸n con el primer modelo en el que nos capacitamos y obtuvimos una precisi贸n de clasificaci贸n de ~ 87%.  La raz贸n es que MobileNet fue dise帽ado por expertos y desarrollado cuidadosamente durante un largo per铆odo de tiempo, y luego capacitado en un conjunto de datos ImageNet incre铆blemente grande. </p><br><p>  Puede ver c贸mo crear su propia MobileNet en Keras en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este enlace</a> . </p><br><p>  Creemos gr谩ficos de cambios en la precisi贸n y los valores de p茅rdida en los conjuntos de datos de capacitaci贸n y validaci贸n. </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/o7/5c/7a/o75c7aieqvmudmyglkroiddrm90.png"></p><br><p>  Lo interesante aqu铆 es que los resultados en el conjunto de datos de validaci贸n son mejores que los resultados en el conjunto de datos de capacitaci贸n desde el principio hasta el final del proceso de aprendizaje. </p><br><p>  Una raz贸n para este comportamiento es que la precisi贸n en el conjunto de datos de validaci贸n se mide al final de la iteraci贸n de entrenamiento, y la precisi贸n en el conjunto de datos de entrenamiento se considera como el valor promedio entre todas las iteraciones de entrenamiento. </p><br><p>  La raz贸n principal de este comportamiento es el uso de la subred MobileNet pre-entrenada, que fue entrenada previamente en un gran conjunto de datos de gatos y perros.  En el proceso de aprendizaje, nuestra red contin煤a expandiendo el conjunto de datos de entrenamiento de entrada (el mismo aumento), pero no el conjunto de validaci贸n.  Esto significa que las im谩genes generadas en el conjunto de datos de entrenamiento son m谩s dif铆ciles de clasificar que las im谩genes normales del conjunto de datos validados. </p><br><p>  <strong>Verificar los resultados de la predicci贸n</strong> </p><br><p>  Para repetir el gr谩fico de la secci贸n anterior, primero debe obtener una lista ordenada de nombres de clase: </p><br><pre> <code class="python hljs">class_names = np.array(info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) class_names</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">array(['cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Pase el bloque con im谩genes a trav茅s del modelo y convierta los 铆ndices resultantes en nombres de clase: </p><br><pre> <code class="python hljs">predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] predicted_class_names</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">array(['cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Echemos un vistazo a las etiquetas verdaderas y pronosticadas: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, predicted_ids)</code> </pre> <br><p>  Conclusi贸n </p><br><pre> <code class="plaintext hljs">: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1] : [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1]</code> </pre> <br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"  (: , : )"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/dc/ox/4w/dcox4wk3fek_1e2j9ltjmvai1ia.png"></p><br><h1>  Zambullirse en redes neuronales convolucionales </h1><br><p>  Utilizando redes neuronales convolucionales, logramos asegurarnos de que se las arreglan bien con la tarea de clasificar im谩genes.  Sin embargo, por el momento, apenas podemos imaginar c贸mo funcionan realmente.  Si pudi茅ramos entender c贸mo ocurre el proceso de aprendizaje, entonces, en principio, podr铆amos mejorar a煤n m谩s el trabajo de clasificaci贸n.  Una forma de entender c贸mo funcionan las redes neuronales convolucionales es visualizar las capas y los resultados de su trabajo.  Le recomendamos que estudie los materiales <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu铆</a> para comprender mejor c贸mo visualizar los resultados de las capas convolucionales. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/50f/d7a/3eb/50fd7a3eb31650740807d84eb8ff1da2.gif" alt="imagen"></p><br><p>  El campo de visi贸n por computadora vio la luz al final del t煤nel y ha progresado significativamente desde el advenimiento de las redes neuronales convolucionales.  La incre铆ble velocidad con la que se lleva a cabo la investigaci贸n en esta 谩rea y las enormes series de im谩genes publicadas en Internet han dado resultados incre铆bles en los 煤ltimos a帽os.  El auge de las redes neuronales convolucionales comenz贸 con AlexNet en 2012, que fue creado por Alex Krizhevsky, Ilya Sutskever y Jeffrey Hinton y gan贸 el famoso desaf铆o de reconocimiento visual a gran escala ImageNet.  Desde entonces, no hab铆a duda en el futuro brillante utilizando redes neuronales convolucionales, y el campo de la visi贸n por computadora y los resultados del trabajo en 茅l solo confirmaron este hecho.  Comenzando por reconocer su cara en un tel茅fono m贸vil y terminando con el reconocimiento de objetos en autom贸viles aut贸nomos, las redes neuronales convolucionales ya han logrado mostrar y demostrar su fuerza y resolver muchos problemas del mundo real. </p><br><p>  A pesar de la gran cantidad de grandes conjuntos de datos y modelos pre-entrenados de redes neuronales convolucionales, a veces es extremadamente dif铆cil entender c贸mo funciona la red y para qu茅 est谩 capacitada exactamente esta red, especialmente para las personas que no tienen el conocimiento suficiente en el campo del aprendizaje autom谩tico.                 ,            , ,   Inception,   .                     .            ,    ,         ,         ,         . </p><br><p>       "   Python"  <br> Fran莽ois Chollet.   ,        .    Keras,     ,   " " TensorFlow, MXNET  Theano.   ,        ,            .           ,       . </p><br><p> <strong>  </strong> </p><br><p>            ,    ,           . </p><br><p>             (training accuracy)     .         ,          ,        , ,   Inception,                . </p><br><p>           ,       ,      .   Inception v3 (     ImageNet)     ,    Kaggle.         Inception,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> ,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Inception v3</a>      . </p><br><p>     10  ()     32 ,    2292293.           0.3195,      0.6377.     <code>ImageDataGenerator</code>     ,      .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub </a> . </p><br><p> <strong>  </strong> </p><br><p>             ,    ""   ,      .               . </p><br><p> ,              Inception v3 ,        . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d4c/ac1/6f5/d4cac16f506f3d14ab4cca070f4d876b.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/222/102/eda/222102eda480f7108db0c9869ab46057.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca8/176/7c0/ca81767c0f5cae4748f1db6cee625e01.jpg" alt="imagen"></p><br><p>         .             . </p><br><p>         ,              ()   .        (),       , ,  ,      .          ,      ,        ,     ,       . </p><br><p>   ReLU-    .    ,     <code>ReLU(z) = max(0, z)</code>     . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9e3/b87/e17/9e3b87e175577fe97da51fd1a2b50eac.png" alt="imagen"></p><br><p>          ,   ,   ,        ,      ,           ,   , ,   ..            ,            .     "" ()     ,            ,     ,             . </p><br><p> <strong>   </strong> </p><br><p>         ""        .               . </p><br><p>   ,     Inveption V3      : </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/00e/a03/d78/00ea03d78e161d7f6fff59ba1a133309.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/247/875/0da/2478750da1a4eb167f8dc1c9c55252d6.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4a9/635/bd8/4a9635bd81a0d2e46435a05c39d3457a.jpg" alt="imagen"></p><br><p>              ,         . ,                   ,      ,           ,   ..          ,       ,                .            ,       ,  ,           "" ( ,      ). </p><br><p> <strong>    </strong> </p><br><p>       ,         , ,      .      ,                  . </p><br><p>     Class Activation Map (  ).      CAM       .       2D              ,                 . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f8/95e/f8b/2f895ef8b9086c9ea56745ce0f441ef9.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/6ff/392/e60/6ff392e60f007791bee52e439099759f.jpg" alt="imagen"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fe9/896/210/fe9896210055693195c21a96b74f3188.jpg" alt="imagen"></p><br><p>       ,     .    ,    ,        Mixed-  Inception V3-,        .        () ,           . </p><br><p>     ,          ,        .          <strong></strong> ,            ,        .       ,          .   ,                 ,        ,    ,        . </p><br><p>           ,      ""  -          .               .             . </p><br><p>    ,           ,                   . </p><br><h1>  :      </h1><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colab  </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colab  </a> . </p><br><p> <strong>TensorFlow Hub</strong> </p><br><p> TensorFlow Hub      ,      . </p><br><p>                  .     ,      ,   ,          . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>      . </p><br><p>              <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p> <strong></strong> </p><br><p>  ,    : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p> <strong>      TensorFlow Datasets</strong> </p><br><p>            TensorFlow Datasets.   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ,        <code>tf_flowers</code> .        ,       .                <code>tfds.splits</code>   (70%)   (30%).        <code>tfds.load</code> .    <code>tfds.load</code> ,            ,      . </p><br><pre> <code class="python hljs">splits = tfds.Split.TRAIN.subsplit([<span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>]) (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset tf_flowers (218.21 MiB) to /root/tensorflow_datasets/tf_flowers/1.0.0... Dl Completed... 1/|/100% 1/1 [00:07&lt;00:00, 3.67s/ url] Dl Size... 218/|/100% 218/218 [00:07&lt;00:00, 30.69 MiB/s] Extraction completed... 1/|/100% 1/1 [00:07&lt;00:00, 7.05s/ file] Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/1.0.0. Subsequent calls will reuse this data.</code> </pre> <br><p> <strong>     </strong> </p><br><p> ,      ,    ()         ,      ,             . </p><br><pre> <code class="python hljs">num_classes = dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes num_training_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> num_validation_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> training_set: num_training_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> validation_set: num_validation_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> print(<span class="hljs-string"><span class="hljs-string">'Total Number of Classes: {}'</span></span>.format(num_classes)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Training Images: {}'</span></span>.format(num_training_examples)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Validation Images: {} \n'</span></span>.format(num_validation_examples))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Total Number of Classes: 5 Total Number of Training Images: 2590 Total Number of Validation Images: 1080</code> </pre> <br><p>          . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(training_set.take(<span class="hljs-number"><span class="hljs-number">5</span></span>)): print(<span class="hljs-string"><span class="hljs-string">'Image {} shape: {} label: {}'</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape, example[<span class="hljs-number"><span class="hljs-number">1</span></span>]))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Image 1 shape: (226, 240, 3) label: 0 Image 2 shape: (240, 145, 3) label: 2 Image 3 shape: (331, 500, 3) label: 2 Image 4 shape: (240, 320, 3) label: 0 Image 5 shape: (333, 500, 3) label: 1</code> </pre> <br><p> <strong>     </strong> </p><br><p>           ,   MobilNet v2      224224     (grayscale).     <code>image</code> ()  <code>label</code> ()       . </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/<span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> <strong>    TensorFlow Hub</strong> </p><br><p>    TensorFlow Hub   . ,                      ,         . </p><br><p> <strong>   </strong> </p><br><p>      <code>feature_extractor</code>  MobileNet v2. ,      TensorFlow Hub (   )   .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> .   <code>tf2-preview/mobilenet_v2/feature_vector</code> ,     URL       MobileNet v2 .   <code>feature_extractor</code>   <code>hub.KerasLayer</code>      <code>input_shape</code> . </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p> <strong>   </strong> </p><br><p>                   ,   : </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p> <strong>  </strong> </p><br><p>               ,   .            .          . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 5) 6405 ================================================================= Total params: 2,264,389 Trainable params: 6,405 Non-trainable params: 2,257,984</code> </pre> <br><p> <strong> </strong> </p><br><p>            ,           . </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 17s 216ms/step - loss: 0.7765 - acc: 0.7170 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 12s 147ms/step - loss: 0.3806 - acc: 0.8757 - val_loss: 0.3485 - val_acc: 0.8833 Epoch 3/6 81/81 [==============================] - 12s 146ms/step - loss: 0.3011 - acc: 0.9031 - val_loss: 0.3190 - val_acc: 0.8907 Epoch 4/6 81/81 [==============================] - 12s 147ms/step - loss: 0.2527 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8917 Epoch 5/6 81/81 [==============================] - 12s 148ms/step - loss: 0.2177 - acc: 0.9371 - val_loss: 0.2933 - val_acc: 0.8972 Epoch 6/6 81/81 [==============================] - 12s 146ms/step - loss: 0.1905 - acc: 0.9456 - val_loss: 0.2870 - val_acc: 0.9000</code> </pre> <br><p>         ~90%  6  ,     !   ,    ,           ~76%  80  .        ,  MobilNet v2                . </p><br><p> <strong>         </strong> </p><br><p>               . </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'Training Accuracy'</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'Validation Accuracy'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Accuracy'</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'Training Loss'</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'Validation Loss'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Loss'</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/ox/nb/hy/oxnbhyqanark0qrt1xmeqg3qv4a.png"></p><br><p>   ,   ,                    . </p><br><p>        ,           ,              . </p><br><p>        - MobileNet,           .              (  augmentation),    .                  . </p><br><p> <strong> </strong> </p><br><p>              NumPy.     ,      . </p><br><pre> <code class="python hljs">class_names = np.array(dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) print(class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['dandelion' 'daisy' 'tulips' 'sunflowers' 'roses']</code> </pre> <br><p> <strong>        </strong> </p><br><p>   <code>next()</code>   <code>image_batch</code> ( )   <code>label_batch</code> ( ).   <code>image_batch</code>  <code>label_batch</code>  NumPy     <code>.numpy()</code> .    <code>.predict()</code>      .        <code>np.argmax()</code>   .            . </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches)) image_batch = image_batch.numpy() label_batch = label_batch.numpy() predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] print(predicted_class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['sunflowers' 'roses' 'tulips' 'tulips' 'daisy' 'dandelion' 'tulips' 'sunflowers' 'daisy' 'daisy' 'tulips' 'daisy' 'daisy' 'tulips' 'tulips' 'tulips' 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'roses' 'daisy' 'daisy' 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'roses' 'dandelion']</code> </pre> <br><p> <strong>     </strong> </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Labels: "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">"Predicted labels: "</span></span>, predicted_ids)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0] Predicted labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0]</code> </pre> <br><p> <strong>  </strong> </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"Model predictions (blue: correct, red: incorrect)"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/e1/q-/rg/e1q-rgvnr6qns8-vvrcr8yzbexi.png"></p><br><p> <strong>     Inception-</strong> </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> TensorFlow Hub</a>     <code>tf2-preview/inception_v3/feature_vector</code> .        Inception V3 .      ,      Inception V3     .  ,  Inception V3       299299 .   Inception V3    MobileNet V2. </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">299</span></span> (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits) train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) model_inception = tf.keras.Sequential([ feature_extractor, tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model_inception.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 2048) 21802784 _________________________________________________________________ dense_1 (Dense) (None, 5) 10245 ================================================================= Total params: 21,813,029 Trainable params: 10,245 Non-trainable params: 21,802,784</code> </pre> <br><pre> <code class="python hljs">model_inception.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model_inception.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 44s 541ms/step - loss: 0.7594 - acc: 0.7309 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3927 - acc: 0.8772 - val_loss: 0.3945 - val_acc: 0.8657 Epoch 3/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3074 - acc: 0.9120 - val_loss: 0.3586 - val_acc: 0.8769 Epoch 4/6 81/81 [==============================] - 35s 434ms/step - loss: 0.2588 - acc: 0.9282 - val_loss: 0.3385 - val_acc: 0.8796 Epoch 5/6 81/81 [==============================] - 35s 436ms/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.3256 - val_acc: 0.8824 Epoch 6/6 81/81 [==============================] - 35s 435ms/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3164 - val_acc: 0.8861</code> </pre> <br><h1>  Resumen </h1><br><p>                   .          : </p><br><ul><li> <strong> :</strong> ,                .              . </li><li> <strong> :</strong>       . ""     ,       ,      . </li><li> <strong>MobileNet:</strong>       Google,                  . MobileNet              . </li></ul><br><p>              MobileNet      .                     .                   MobileNet    . </p><br><p>    call-to-action  ,     share :) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegrama</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VKontakte</a> <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ojok</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467967/">https://habr.com/ru/post/467967/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467953/index.html">Probar servidores virtuales baratos</a></li>
<li><a href="../467957/index.html">Lo que hay detr谩s de la constante de Feigenbaum</a></li>
<li><a href="../467959/index.html">Cosmolog铆a y fluctuaciones cu谩nticas en el navegador.</a></li>
<li><a href="../467961/index.html">Problemas y matices al desarrollar para SmartTV usando React.js</a></li>
<li><a href="../467965/index.html">Vivaldi 2.8 - Men煤, por favor</a></li>
<li><a href="../467969/index.html">Presentaciones modales en pantalla modal en iOS 13</a></li>
<li><a href="../467973/index.html">Nacimiento de la plataforma</a></li>
<li><a href="../467975/index.html">Huawei Dorado V6: calor de Sichuan</a></li>
<li><a href="../467977/index.html">Crear una aplicaci贸n usando componentes con estilo en Vue.js</a></li>
<li><a href="../467979/index.html">Integraciones publicitarias: 驴c贸mo funciona?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>