<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👙 📺 🈹 Perendaman dalam jaringan saraf convolutional: transfer learning 💜 👍 👎🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kursus lengkap dalam bahasa Rusia dapat ditemukan di tautan ini . 
 Kursus bahasa Inggris asli tersedia di tautan ini . 



 Isi 


1. Wawancara denga...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Perendaman dalam jaringan saraf convolutional: transfer learning</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467967/"><p>  Kursus lengkap dalam bahasa Rusia dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tautan ini</a> . <br>  Kursus bahasa Inggris asli tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tautan ini</a> . </p><br><p><img src="https://habrastorage.org/webt/wu/ie/7c/wuie7cgpktklm4bytoweu7ki0oq.jpeg"></p><a name="habracut"></a><br><h1>  Isi </h1><br><ol><li>  Wawancara dengan Sebastian Trun </li><li>  Pendahuluan </li><li>  Model Pembelajaran Transfer </li><li>  MobileNet </li><li>  CoLab: Kucing Vs Anjing dengan Pelatihan Transfer </li><li>  Menyelam ke jaringan saraf convolutional </li><li>  Bagian praktis: penentuan warna dengan transfer pelatihan </li><li>  Ringkasan </li></ol><br><h1>  Wawancara dengan Sebastian Trun </h1><br><p>  - Ini adalah pelajaran 6 dan sepenuhnya didedikasikan untuk mentransfer pembelajaran.  Transfer pembelajaran adalah proses menggunakan model yang sudah ada dengan sedikit perbaikan untuk tugas-tugas baru.  Pemindahan pelatihan membantu mengurangi waktu pelatihan model dengan memberikan peningkatan efisiensi ketika belajar di awal.  Sebastian, apa pendapat Anda tentang transfer pelatihan?  Apakah Anda pernah dapat menggunakan metodologi transfer pengajaran dalam pekerjaan dan penelitian Anda? <br>  - Disertasi saya dikhususkan hanya untuk topik transfer pelatihan dan disebut " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Penjelasan atas dasar transfer pelatihan</a> ."  Ketika kami mengerjakan disertasi, idenya adalah memungkinkan untuk mengajar membedakan semua objek lain dari jenis ini pada satu objek (kumpulan data, entitas) dalam berbagai variasi dan format.  Dalam karya ini, kami menggunakan algoritma yang dikembangkan, yang membedakan karakteristik utama (atribut) dari objek dan dapat membandingkannya dengan objek lain.  Perpustakaan seperti Tensorflow sudah dilengkapi dengan model pra-terlatih. <br>  - Ya, di Tensorflow kami memiliki satu set lengkap model pra-terlatih yang dapat Anda gunakan untuk memecahkan masalah praktis.  Kita akan membicarakan set yang sudah jadi sedikit nanti. <br>  - Ya, ya!  Jika Anda memikirkannya, orang akan terlibat dalam transfer pelatihan sepanjang waktu sepanjang hidup mereka. <br>  - Bisakah kita mengatakan bahwa berkat metode transfer pelatihan, siswa baru kita di beberapa titik tidak akan perlu tahu sesuatu tentang pembelajaran mesin karena itu akan cukup untuk menghubungkan model yang sudah disiapkan dan menggunakannya? <br>  - Pemrograman adalah menulis baris demi baris, kami memberikan perintah ke komputer.  Tujuan kami adalah untuk memastikan bahwa semua orang di planet ini mampu dan dapat memprogram dengan menyediakan komputer dengan hanya contoh data input.  Setuju, jika Anda ingin mengajarkan komputer untuk membedakan kucing dari anjing, maka menemukan 100 ribu gambar kucing yang berbeda dan 100 ribu gambar anjing yang berbeda cukup sulit, dan berkat transfer pelatihan Anda dapat menyelesaikan masalah ini dalam beberapa baris. <br>  - Ya itu benar!  Terima kasih atas jawabannya dan mari kita lanjut belajar. </p><br><h1>  Pendahuluan </h1><br><p>  - Halo dan selamat datang kembali! <br>  - Terakhir kali kami melatih jaringan saraf convolutional untuk mengklasifikasikan kucing dan anjing dalam gambar.  Jaringan saraf pertama kami dilatih ulang, sehingga hasilnya tidak terlalu tinggi - sekitar 70% akurasi.  Setelah itu, kami menerapkan ekstensi dan putus data (pemutusan neuron yang sewenang-wenang), yang memungkinkan kami meningkatkan akurasi prediksi hingga 80%. <br>  - Terlepas dari kenyataan bahwa 80% mungkin tampak seperti indikator yang sangat baik, kesalahan 20% masih terlalu besar.  Benar?  Apa yang bisa kita lakukan untuk meningkatkan akurasi klasifikasi?  Dalam pelajaran ini, kita akan menggunakan teknik transfer pengetahuan (transfer of the knowledge model), yang akan memungkinkan kita untuk menggunakan model yang dikembangkan oleh para ahli dan dilatih tentang kumpulan data yang sangat besar.  Seperti yang akan kita lihat dalam praktik, dengan mentransfer model pengetahuan kita dapat mencapai akurasi klasifikasi 95%.  Ayo mulai! </p><br><h1>  Transfer Model Pembelajaran </h1><br><p>  Pada 2012, jaringan saraf AlexNet merevolusi dunia pembelajaran mesin dan mempopulerkan penggunaan jaringan saraf konvolusional untuk klasifikasi dengan memenangkan tantangan pengakuan Visual Skala Besar ImageNet. </p><br><p><img src="https://habrastorage.org/webt/fs/xx/di/fsxxdicwkitfkxibie8kkjcyexu.png"></p><br><p>  Setelah itu, perjuangan mulai mengembangkan jaringan saraf yang lebih akurat dan efisien yang dapat melampaui AlexNet dalam tugas mengklasifikasikan gambar dari dataset ImageNet. </p><br><p><img src="https://habrastorage.org/webt/oe/t0/ov/oet0ovye40p1cziih64hpmbhuus.png"></p><br><p>  Selama beberapa tahun, jaringan saraf telah dikembangkan yang mengatasi tugas klasifikasi lebih baik daripada AlexNet - Inception dan ResNet. <br>  Setuju bahwa akan lebih baik untuk bisa memanfaatkan jaringan saraf yang sudah dilatih tentang kumpulan data besar dari ImageNet dan menggunakannya dalam klasifikasi kucing dan anjing Anda? </p><br><p>  Ternyata kita bisa melakukannya!  Teknik ini disebut transfer learning.  Gagasan utama metode mentransfer model pelatihan didasarkan pada fakta bahwa setelah melatih jaringan saraf pada set data yang besar, kita dapat menerapkan model yang diperoleh ke set data yang model ini belum temui.  Itulah sebabnya teknik ini disebut transfer learning - mentransfer proses belajar dari satu set data ke yang lain. </p><br><p>  Agar kita dapat menerapkan metodologi mentransfer model pelatihan, kita perlu mengubah lapisan terakhir dari jaringan saraf convolutional kami: </p><br><p><img src="https://habrastorage.org/webt/3j/-g/g3/3j-gg3yxm9kvrtphiswnho2jgtc.png"></p><br><p>  Kami melakukan operasi ini karena setiap set data terdiri dari sejumlah kelas output yang berbeda.  Misalnya, kumpulan data di ImageNet berisi 1000 kelas output yang berbeda.  FashionMNIST berisi 10 kelas.  Kumpulan data klasifikasi kami hanya terdiri dari 2 kelas - kucing dan anjing. </p><br><p><img src="https://habrastorage.org/webt/5e/pm/ej/5epmejbamklkdfgzzw9v8rzb1ts.png"></p><br><p>  Itu sebabnya perlu untuk mengubah lapisan terakhir dari jaringan saraf convolutional kami sehingga mengandung jumlah output yang akan sesuai dengan jumlah kelas di set baru. </p><br><p><img src="https://habrastorage.org/webt/cg/mk/fz/cgmkfzxqmyqbdzsmvfdppjhsl9e.png"></p><br><p>  Kita juga perlu memastikan bahwa kita tidak mengubah model pra-terlatih selama proses pelatihan.  Solusinya adalah mematikan variabel model pra-terlatih - kami hanya melarang algoritma memperbarui nilai selama propagasi maju dan mundur untuk mengubahnya. <br>  Proses ini disebut "pembekuan model". </p><br><p><img src="https://habrastorage.org/webt/8z/oz/0n/8zoz0nenaad4-lgezhhia25k_18.png"></p><br><p>  Dengan "membekukan" parameter model pra-terlatih, kami memungkinkan kami untuk hanya mempelajari lapisan terakhir dari jaringan klasifikasi, nilai-nilai variabel dari model pra-terlatih tetap tidak berubah. </p><br><p>  Keuntungan lain yang tak terbantahkan dari model pra-terlatih adalah kami mengurangi waktu pelatihan dengan hanya melatih lapisan terakhir dengan jumlah variabel yang jauh lebih kecil, dan bukan keseluruhan model. </p><br><p>  Jika kita tidak "membekukan" variabel model pra-terlatih, maka selama proses pelatihan, nilai-nilai variabel akan berubah pada set data baru.  Ini karena nilai-nilai variabel pada lapisan terakhir dari klasifikasi akan diisi dengan nilai acak.  Karena nilai acak pada lapisan terakhir, model kami akan membuat kesalahan besar dalam klasifikasi, yang, pada gilirannya, akan memerlukan perubahan kuat pada bobot awal dalam model pra-terlatih, yang sangat tidak diinginkan bagi kami. </p><br><p><img src="https://habrastorage.org/webt/wp/uz/km/wpuzkmnan5rahfg3irexdsvrstm.png"></p><br><p>  Karena alasan inilah kita harus selalu ingat bahwa ketika menggunakan model yang ada, nilai-nilai variabel harus "beku" dan kebutuhan untuk melatih model pra-terlatih harus dimatikan. </p><br><p>  Sekarang kita tahu bagaimana transfer model pelatihan bekerja, kita hanya harus memilih jaringan saraf pra-terlatih untuk digunakan dalam pengklasifikasi kita sendiri!  Ini akan kita lakukan di bagian selanjutnya. </p><br><h1>  MobileNet </h1><br><p>  Seperti yang kami sebutkan sebelumnya, jaringan saraf yang sangat efisien dikembangkan yang menunjukkan hasil tinggi pada dataset ImageNet - AlexNet, Inception, Resonant.  Jaringan saraf ini adalah jaringan yang sangat dalam dan berisi ribuan bahkan jutaan parameter.  Sejumlah besar parameter memungkinkan jaringan untuk mempelajari pola yang lebih kompleks dan dengan demikian mencapai peningkatan akurasi klasifikasi.  Sejumlah besar parameter pelatihan jaringan saraf mempengaruhi kecepatan belajar, jumlah memori yang diperlukan untuk menyimpan jaringan dan kompleksitas perhitungan. </p><br><p>  Dalam pelajaran ini kita akan menggunakan jaringan neural convolutional modern MobileNet.  MobileNet adalah arsitektur jaringan saraf convolutional yang efisien yang mengurangi jumlah memori yang digunakan untuk komputasi sambil mempertahankan akurasi prediksi yang tinggi.  Itulah sebabnya MobileNet ideal untuk digunakan pada perangkat seluler dengan jumlah terbatas memori dan sumber daya komputasi. </p><br><p>  MobileNet dikembangkan oleh Google dan dilatih tentang dataset ImageNet. </p><br><p>  Karena MobileNet dilatih dalam 1.000 kelas dari set data ImageNet, MobileNet memiliki 1.000 kelas output, bukan dua yang kita butuhkan - kucing dan anjing. </p><br><p><img src="https://habrastorage.org/webt/he/2f/ox/he2foxizd_xmt7rijxumteg-94k.png"></p><br><p>  Untuk menyelesaikan transfer pelatihan, kami memuat vektor fitur tanpa lapisan klasifikasi: </p><br><p><img src="https://habrastorage.org/webt/1b/o2/no/1bo2nop9cpz3ebag_jcyfrgje7w.png"></p><br><p>  Dalam Tensorflow, vektor fitur yang dimuat dapat digunakan sebagai layer Keras biasa dengan data input ukuran tertentu. </p><br><p>  Karena MobileNet dilatih pada dataset ImageNet, kita perlu membawa ukuran data input ke yang digunakan dalam proses pelatihan.  Dalam kasus kami, MobileNet dilatih tentang gambar RGB ukuran tetap 224x224px. </p><br><p>  TensorFlow berisi repositori pra-terlatih yang disebut TensorFlow Hub. </p><br><p><img src="https://habrastorage.org/webt/o5/we/9d/o5we9dvkdtaomahwj4nzomgquca.png"></p><br><p>  TensorFlow Hub berisi beberapa model pra-terlatih di mana lapisan klasifikasi terakhir dikeluarkan dari arsitektur jaringan saraf untuk digunakan kembali berikutnya. </p><br><p>  Anda dapat menggunakan TensorFlow Hub dalam kode dalam beberapa baris: </p><br><p><img src="https://habrastorage.org/webt/1h/ai/hg/1haihgg1llinsfv_wyhde4z0egy.png"></p><br><p>  Cukup untuk menentukan URL vektor fitur dari model pelatihan yang diinginkan dan kemudian menanamkan model di classifier kami dengan lapisan terakhir dengan jumlah kelas output yang diinginkan.  Ini adalah lapisan terakhir yang akan dikenakan pelatihan dan mengubah nilai parameter.  Kompilasi dan pelatihan model baru kami dilakukan dengan cara yang sama seperti yang kami lakukan sebelumnya: </p><br><p><img src="https://habrastorage.org/webt/uh/cr/5e/uhcr5esktfnxtxwxfgutvttxbxk.png"></p><br><p>  Mari kita lihat bagaimana ini akan bekerja dan menulis kode yang sesuai. </p><br><h1>  CoLab: Kucing Vs Anjing dengan Pelatihan Transfer </h1><br><p>  Tautan ke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CoLab dalam bahasa Rusia</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CoLab dalam bahasa Inggris</a> . </p><br><p>  TensorFlow Hub adalah repositori dengan model pra-terlatih yang dapat kita gunakan. </p><br><p>  Transfer pembelajaran adalah proses di mana kami mengambil model pra-terlatih dan mengembangkannya untuk melakukan tugas tertentu.  Pada saat yang sama, kami membiarkan bagian dari model pra-pelatihan yang kami integrasikan ke dalam jaringan saraf tidak tersentuh, tetapi hanya melatih lapisan keluaran terakhir untuk mendapatkan hasil yang diinginkan. </p><br><p>  Pada bagian praktis ini, kami akan menguji kedua opsi. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan ini</a> memungkinkan Anda menjelajahi seluruh daftar model yang tersedia. </p><br><p>  <strong>Di bagian Colab ini</strong> </p><br><ol><li>  Kami akan menggunakan model TensorFlow Hub untuk prediksi; </li><li>  Kami akan menggunakan model-TensorFlow Hub untuk kumpulan data kucing dan anjing; </li><li>  Mari mentransfer pelatihan menggunakan model dari TensorFlow Hub. </li></ol><br><p> Sebelum melanjutkan dengan implementasi bagian praktis saat ini, kami menyarankan pengaturan ulang <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p>  <strong>Impor Perpustakaan</strong> </p><br><p>  Pada bagian praktis ini, kami akan menggunakan sejumlah fitur perpustakaan TensorFlow yang belum dalam rilis resmi.  Itulah sebabnya kami akan menginstal versi TensorFlow dan TensorFlow Hub untuk pengembang. </p><br><p>  Menginstal versi dev TensorFlow secara otomatis mengaktifkan versi terinstal terbaru.  Setelah kami selesai berurusan dengan bagian praktis ini, kami sarankan mengembalikan pengaturan TensorFlow dan kembali ke versi stabil melalui item menu <code>Runtime -&gt; Reset all runtimes...</code>  Menjalankan perintah ini akan mengatur ulang semua pengaturan lingkungan ke yang asli. </p><br><pre> <code class="python hljs">!pip install tf-nightly-gpu !pip install <span class="hljs-string"><span class="hljs-string">"tensorflow_hub==0.4.0"</span></span> !pip install -U tensorflow_datasets</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1) Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7) Collecting tf-estimator-nightly (from tf-nightly-gpu) Downloading https://files.pythonhosted.org/packages/ea/72/f092fc631ef2602fd0c296dcc4ef6ef638a6a773cb9fdc6757fecbfffd33/tf_estimator_nightly-1.14.0.dev2019092201-py2.py3-none-any.whl (450kB) |████████████████████████████████| 450kB 45.9MB/s Requirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.5) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2) Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.0.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.6) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tf-nightly-gpu) (2.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (41.2.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (0.15.6) Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu Successfully installed tb-nightly-1.15.0a20190911 tf-estimator-nightly-1.14.0.dev2019092201 tf-nightly-gpu-1.15.0.dev20190821 Collecting tensorflow_hub==0.4.0 Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB) |████████████████████████████████| 81kB 5.0MB/s Requirement already satisfied: protobuf&gt;=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (3.7.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.16.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.12.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.4.0-&gt;tensorflow_hub==0.4.0) (41.2.0) Installing collected packages: tensorflow-hub Found existing installation: tensorflow-hub 0.6.0 Uninstalling tensorflow-hub-0.6.0: Successfully uninstalled tensorflow-hub-0.6.0 Successfully installed tensorflow-hub-0.4.0 Collecting tensorflow_datasets Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB) |████████████████████████████████| 2.3MB 4.9MB/s Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0) Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.11.2) Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.16.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.21.0) Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.1) Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (5.4.8) Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1) Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.8.0) Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.14.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.0) Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0) Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.1.0) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.8) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2019.6.16) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (1.24.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow_datasets) (41.2.0) Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.6.0) Installing collected packages: tensorflow-datasets Successfully installed tensorflow-datasets-1.2.0</code> </pre> <br><p>  Kami telah melihat dan menggunakan beberapa impor sebelumnya.  Dari yang baru - impor <code>tensorflow_hub</code> , yang kami pasang dan yang akan kami gunakan di bagian praktis ini. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0. Please upgrade your code to TensorFlow 2.0: * https://www.tensorflow.org/beta/guide/migration_guide Or install the latest stable TensorFlow 1.X release: * `pip install -U "tensorflow==1.*"` Otherwise your code may be broken by the change.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p>  <strong>Bagian 1: gunakan TensorFlow Hub MobileNet untuk prediksi</strong> </p><br><p>  Di bagian CoLab ini, kami akan mengambil model yang sudah dilatih sebelumnya, mengunggahnya ke Keras dan mengujinya. </p><br><p>  Model yang kami gunakan adalah MobileNet v2 (bukan MobileNet, model classifier gambar tf2 lain yang kompatibel dengan tfhub.dev dapat digunakan). </p><br><p>  <strong>Unduh classifier</strong> </p><br><p>  Unduh model MobileNet dan buat model Keras darinya.  MobileNet pada input mengharapkan untuk menerima gambar dengan ukuran 224x224 piksel dengan 3 saluran warna (RGB). </p><br><pre> <code class="python hljs">CLASSIFIER_URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span></span> IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> model = tf.keras.Sequential([ hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>)) ])</code> </pre> <br><p>  <strong>Jalankan classifier pada satu gambar</strong> </p><br><p>  MobileNet telah dilatih tentang dataset ImageNet.  ImageNet berisi 1000 kelas keluaran dan salah satu dari kelas ini adalah seragam militer.  Mari kita cari gambar di mana seragam militer akan ditempatkan dan yang tidak akan menjadi bagian dari paket pelatihan ImageNet untuk memverifikasi akurasi klasifikasi. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PIL.Image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Image grace_hopper = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'image.jpg'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span></span>) grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES)) grace_hopper</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg 65536/61306 [================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/8g/gn/0u/8ggn0ur3pmnr_rxvezfdo4vrwcc.png"></p><br><pre> <code class="python hljs">grace_hopper = np.array(grace_hopper)/<span class="hljs-number"><span class="hljs-number">255.0</span></span> grace_hopper.shape</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">(224, 224, 3)</code> </pre> <br><p>  Perlu diingat bahwa model selalu menerima set (blok) gambar untuk diproses pada input.  Dalam kode di bawah ini, kami menambahkan dimensi baru - ukuran blok. </p><br><pre> <code class="python hljs">result = model.predict(grace_hopper[np.newaxis, ...]) result.shape</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">(1, 1001)</code> </pre> <br><p>  Hasil prediksi adalah vektor dengan ukuran 1,001 elemen, di mana setiap nilai mewakili probabilitas bahwa objek dalam gambar milik kelas tertentu. </p><br><p>  Posisi nilai probabilitas maksimum dapat ditemukan menggunakan fungsi <code>argmax</code> .  Namun, ada pertanyaan yang masih belum kami jawab - bagaimana kami bisa menentukan kelas mana yang termasuk dalam elemen dengan probabilitas maksimum? </p><br><pre> <code class="python hljs">predicted_class = np.argmax(result[<span class="hljs-number"><span class="hljs-number">0</span></span>], axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">653</code> </pre> <br><p>  <strong>Mengartikan Prediksi</strong> </p><br><p>  Agar kami dapat menentukan kelas yang terkait dengan prediksi, kami mengunggah daftar tag ImageNet dan dengan indeks dengan kesetiaan maksimum, kami menentukan kelas yang terkait dengan prediksi. </p><br><pre> <code class="python hljs">labels_path = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'ImageNetLabels.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span></span>) imagenet_labels = np.array(open(labels_path).read().splitlines()) plt.imshow(grace_hopper) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) predicted_class_name = imagenet_labels[predicted_class] _ = plt.title(<span class="hljs-string"><span class="hljs-string">"Prediction: "</span></span> + predicted_class_name.title())</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt 16384/10484 [==============================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/ai/1k/rt/ai1krt6mkpcafhb5jx8ozf6mq8s.png"></p><br><p>  Bingo!  Model kami mengidentifikasi dengan benar seragam militer. </p><br><p>  <strong>Bagian 2: gunakan model TensorFlow Hub untuk dataset kucing dan anjing</strong> </p><br><p>  Sekarang kita akan menggunakan versi lengkap model MobileNet dan melihat bagaimana itu akan mengatasi kumpulan data kucing dan anjing. </p><br><p>  <strong>Dataset</strong> </p><br><p>  Kita dapat menggunakan Kumpulan Data TensorFlow untuk mengunduh dataset kucing dan anjing. </p><br><pre> <code class="python hljs">splits = tfds.Split.ALL.subsplit(weighted=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) splits, info = tfds.load(<span class="hljs-string"><span class="hljs-string">'cats_vs_dogs'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split = splits) (train_examples, validation_examples) = splits num_examples = info.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_classes = info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/2.0.1... /usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) WARNING:absl:1738 images were corrupted and were skipped Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/2.0.1. Subsequent calls will reuse this data.</code> </pre><br><p>  Tidak semua gambar dalam dataset kucing dan anjing memiliki ukuran yang sama. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example_image <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_examples.take(<span class="hljs-number"><span class="hljs-number">3</span></span>)): print(<span class="hljs-string"><span class="hljs-string">"Image {} shape: {}"</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example_image[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape))</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Image 1 shape: (500, 343, 3) Image 2 shape: (375, 500, 3) Image 3 shape: (375, 500, 3)</code> </pre> <br><p>  Oleh karena itu, gambar dari kumpulan data yang diperoleh memerlukan reduksi ke ukuran tunggal, yang diharapkan oleh model MobileNet pada input - 224 x 224. </p><br><p>  Fungsi <code>.repeat()</code> dan <code>steps_per_epoch</code> tidak diperlukan di sini, tetapi mereka memungkinkan Anda untuk menghemat sekitar 15 detik per iterasi pelatihan, karena  buffer sementara harus diinisialisasi hanya sekali pada awal proses pembelajaran. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / <span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = train_examples.shuffle(num_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  <strong>Jalankan classifier pada set gambar</strong> </p><br><p>  Biarkan saya mengingatkan Anda bahwa pada tahap ini, masih ada versi lengkap dari jaringan MobileNet yang sudah dilatih sebelumnya, yang berisi 1.000 kemungkinan kelas keluaran.  ImageNet berisi sejumlah besar gambar anjing dan kucing, jadi mari kita coba memasukkan salah satu gambar uji dari kumpulan data kami dan melihat prediksi apa yang akan diberikan oleh model. </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches.take(<span class="hljs-number"><span class="hljs-number">1</span></span>))) image_batch = image_batch.numpy() label_batch = label_batch.numpy() result_batch = model.predict(image_batch) predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)] predicted_class_names</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">array(['Persian cat', 'mink', 'Siamese cat', 'tabby', 'Bouvier des Flandres', 'dishwasher', 'Yorkshire terrier', 'tiger cat', 'tabby', 'Egyptian cat', 'Egyptian cat', 'tabby', 'dalmatian', 'Persian cat', 'Border collie', 'Newfoundland', 'tiger cat', 'Siamese cat', 'Persian cat', 'Egyptian cat', 'tabby', 'tiger cat', 'Labrador retriever', 'German shepherd', 'Eskimo dog', 'kelpie', 'mink', 'Norwegian elkhound', 'Labrador retriever', 'Egyptian cat', 'computer keyboard', 'boxer'], dtype='&lt;U30')</code> </pre> <br><p>  Labelnya mirip dengan nama ras kucing dan anjing.  Sekarang mari kita menampilkan beberapa gambar dari dataset kucing dan anjing kita dan menempatkan label yang diprediksi pada masing-masing gambar. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) plt.title(predicted_class_names[n]) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"ImageNet predictions"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/fc/af/w-/fcafw-kdtzotffq7k-nihlntuda.png"></p><br><p>  <strong>Bagian 3: Menerapkan Transfer Pembelajaran dengan TensorFlow Hub</strong> </p><br><p>  Sekarang mari kita gunakan TensorFlow Hub untuk mentransfer pembelajaran dari satu model ke model lainnya. </p><br><p>  Dalam proses mentransfer pelatihan, kami menggunakan kembali satu model pra-terlatih dengan mengubah lapisan terakhirnya, atau beberapa lapisan, dan kemudian memulai proses pelatihan lagi pada kumpulan data baru. </p><br><p>  Di TensorFlow Hub, Anda dapat menemukan tidak hanya model pra-latih lengkap (dengan lapisan terakhir), tetapi juga model tanpa lapisan klasifikasi terakhir.  Yang terakhir dapat dengan mudah digunakan untuk mentransfer pelatihan.  Kami akan terus menggunakan MobileNet v2 untuk alasan sederhana bahwa pada bagian selanjutnya dari kursus kami, kami akan mentransfer model ini dan meluncurkannya pada perangkat seluler menggunakan TensorFlow Lite. </p><br><p>  Kami juga akan terus menggunakan kumpulan data kucing dan anjing, jadi kami akan memiliki kesempatan untuk membandingkan kinerja model ini dengan yang kami terapkan dari awal. </p><br><p>  Perhatikan bahwa kami menyebut model parsial dengan TensorFlow Hub (tanpa lapisan klasifikasi terakhir) <code>feature_extractor</code> .  Nama ini dijelaskan oleh fakta bahwa model menerima data sebagai input dan mengubahnya menjadi sekumpulan properti (karakteristik) terbatas.  Dengan demikian, model kami melakukan pekerjaan mengidentifikasi isi gambar, tetapi tidak menghasilkan distribusi probabilitas akhir atas kelas output.  Model mengekstraksi sekumpulan properti dari gambar. </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2'</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p>  Mari kita jalankan serangkaian gambar melalui <code>feature_extractor</code> dan lihat bentuk yang dihasilkan (format output).  32 - jumlah gambar, 1280 - jumlah neuron pada lapisan terakhir model pra-terlatih dengan TensorFlow Hub. </p><br><pre> <code class="python hljs">feature_batch = feature_extractor(image_batch) print(feature_batch.shape)</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">(32, 1280)</code> </pre> <br><p>  Kami "membekukan" variabel dalam lapisan ekstraksi properti sehingga hanya nilai-nilai variabel dari lapisan klasifikasi yang berubah selama proses pelatihan. </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p>  <strong>Tambahkan lapisan klasifikasi</strong> </p><br><p>  Sekarang bungkus layer dari TensorFlow Hub di model <code>tf.keras.Sequential</code> dan tambahkan layer klasifikasi. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 2) 2562 ================================================================= Total params: 2,260,546 Trainable params: 2,562 Non-trainable params: 2,257,984 _________________________________________________________________</code> </pre> <br><p>  <strong>Model kereta</strong> </p><br><p>  Sekarang kita melatih model yang dihasilkan seperti yang kita lakukan sebelum memanggil <code>compile</code> diikuti oleh <code>fit</code> untuk pelatihan. </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] ) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">Epoch 1/6 582/582 [==============================] - 77s 133ms/step - loss: 0.2381 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 582/582 [==============================] - 70s 120ms/step - loss: 0.1827 - acc: 0.9618 - val_loss: 0.1629 - val_acc: 0.9670 Epoch 3/6 582/582 [==============================] - 69s 119ms/step - loss: 0.1733 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9666 Epoch 4/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1677 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9677 Epoch 5/6 582/582 [==============================] - 68s 118ms/step - loss: 0.1636 - acc: 0.9689 - val_loss: 0.1634 - val_acc: 0.9675 Epoch 6/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1604 - acc: 0.9701 - val_loss: 0.1643 - val_acc: 0.9668</code> </pre> <br><p>  Seperti yang mungkin Anda perhatikan, kami dapat mencapai ~ 97% akurasi prediksi pada set data validasi.  Luar biasa!  Pendekatan saat ini telah secara signifikan meningkatkan akurasi klasifikasi dibandingkan dengan model pertama yang kami latih sendiri dan memperoleh akurasi klasifikasi ~ 87%.  Alasannya adalah bahwa MobileNet dirancang oleh para ahli dan dikembangkan dengan hati-hati dalam jangka waktu yang lama, dan kemudian dilatih pada dataset ImageNet yang sangat besar. </p><br><p>  Anda dapat melihat cara membuat MobileNet Anda sendiri di Keras di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tautan ini</a> . </p><br><p>  Mari kita buat grafik perubahan dalam nilai akurasi dan kehilangan pada set data pelatihan dan validasi. </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/o7/5c/7a/o75c7aieqvmudmyglkroiddrm90.png"></p><br><p>  Yang menarik di sini adalah bahwa hasil pada set data validasi lebih baik daripada hasil pada set data pelatihan dari awal hingga akhir proses pembelajaran. </p><br><p>  Salah satu alasan perilaku ini adalah bahwa akurasi pada dataset validasi diukur pada akhir iterasi pelatihan, dan akurasi pada dataset pelatihan dianggap sebagai rata-rata semua iterasi pelatihan. </p><br><p>  Alasan terbesar untuk perilaku ini adalah penggunaan sub-jaringan MobileNet yang telah dilatih sebelumnya, yang sebelumnya dilatih pada kumpulan data besar kucing dan anjing.  Dalam proses pembelajaran, jaringan kami terus memperluas set data pelatihan input (augmentasi yang sama), tetapi bukan set validasi.  Ini berarti bahwa gambar yang dihasilkan pada set data pelatihan lebih sulit untuk diklasifikasikan daripada gambar normal dari set data yang divalidasi. </p><br><p>  <strong>Periksa Hasil Prediksi</strong> </p><br><p>  Untuk mengulang grafik dari bagian sebelumnya, pertama Anda harus mendapatkan daftar nama kelas yang diurutkan: </p><br><pre> <code class="python hljs">class_names = np.array(info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) class_names</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">array(['cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Lewati blok dengan gambar melalui model dan konversi indeks yang dihasilkan menjadi nama kelas: </p><br><pre> <code class="python hljs">predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] predicted_class_names</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">array(['cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Mari kita lihat label yang sebenarnya dan prediksi: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, predicted_ids)</code> </pre> <br><p>  Kesimpulan: </p><br><pre> <code class="plaintext hljs">: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1] : [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1]</code> </pre> <br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"  (: , : )"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/dc/ox/4w/dcox4wk3fek_1e2j9ltjmvai1ia.png"></p><br><h1>  Menyelam ke jaringan saraf convolutional </h1><br><p>  Menggunakan jaringan saraf convolutional, kami berhasil memastikan bahwa mereka mengatasi dengan baik tugas mengklasifikasikan gambar.  Namun, saat ini, kita tidak dapat membayangkan bagaimana mereka benar-benar bekerja.  Jika kita dapat memahami bagaimana proses pembelajaran terjadi, maka, pada prinsipnya, kita dapat meningkatkan pekerjaan klasifikasi lebih jauh.  Salah satu cara untuk memahami cara kerja jaringan saraf convolutional adalah memvisualisasikan lapisan dan hasil pekerjaan mereka.  Kami sangat menyarankan Anda mempelajari bahan-bahan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> untuk lebih memahami cara memvisualisasikan hasil dari lapisan konvolusional. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/50f/d7a/3eb/50fd7a3eb31650740807d84eb8ff1da2.gif" alt="gambar"></p><br><p>  Bidang visi komputer melihat cahaya di ujung terowongan dan telah membuat kemajuan yang signifikan sejak munculnya jaringan saraf convolutional.  Kecepatan luar biasa yang digunakan dalam penelitian di bidang ini dan sejumlah besar gambar yang dipublikasikan di Internet telah memberikan hasil yang luar biasa selama beberapa tahun terakhir.  Munculnya jaringan saraf convolutional dimulai dengan AlexNet pada 2012, yang diciptakan oleh Alex Krizhevsky, Ilya Sutskever dan Jeffrey Hinton dan memenangkan Tantangan Pengenalan Visual Skala Besar ImageNet yang terkenal.  Sejak itu, tidak ada keraguan di masa depan yang cerah menggunakan jaringan saraf convolutional, dan bidang visi komputer dan hasil kerja di dalamnya hanya mengkonfirmasi fakta ini.  Mulai dari mengenali wajah Anda di ponsel dan berakhir dengan pengenalan objek di mobil otonom, jaringan saraf convolutional telah berhasil menunjukkan dan membuktikan kekuatan mereka dan menyelesaikan banyak masalah dari dunia nyata. </p><br><p>  Terlepas dari sejumlah besar kumpulan data besar dan model pra-pelatihan jaringan saraf convolutional, kadang-kadang sangat sulit untuk memahami bagaimana jaringan bekerja dan untuk apa sebenarnya jaringan ini dilatih, terutama bagi orang-orang yang tidak memiliki pengetahuan yang cukup di bidang pembelajaran mesin.                 ,            , ,   Inception,   .                     .            ,    ,         ,         ,         . </p><br><p>       "   Python"  <br> François Chollet.   ,        .    Keras,     ,   " " TensorFlow, MXNET  Theano.   ,        ,            .           ,       . </p><br><p> <strong>  </strong> </p><br><p>            ,    ,           . </p><br><p>             (training accuracy)     .         ,          ,        , ,   Inception,                . </p><br><p>           ,       ,      .   Inception v3 (     ImageNet)     ,    Kaggle.         Inception,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">    </a> ,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Inception v3</a>      . </p><br><p>     10  ()     32 ,    2292293.           0.3195,     — 0.6377.     <code>ImageDataGenerator</code>     ,      .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">GitHub </a> . </p><br><p> <strong>  </strong> </p><br><p>             ,    ""   ,      .               . </p><br><p> ,              Inception v3 ,        . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d4c/ac1/6f5/d4cac16f506f3d14ab4cca070f4d876b.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/222/102/eda/222102eda480f7108db0c9869ab46057.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca8/176/7c0/ca81767c0f5cae4748f1db6cee625e01.jpg" alt="gambar"></p><br><p>    —     .             . </p><br><p>         ,              ()   .        (),       , ,  ,      .          ,      ,        ,     ,       . </p><br><p>   ReLU-    .    ,     <code>ReLU(z) = max(0, z)</code>     . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9e3/b87/e17/9e3b87e175577fe97da51fd1a2b50eac.png" alt="gambar"></p><br><p>          ,   ,   ,        ,      ,           ,   , ,   ..            ,            .     "" ()     ,            ,     ,             . </p><br><p> <strong>   </strong> </p><br><p>         ""        .               . </p><br><p>   ,     Inveption V3      : </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/00e/a03/d78/00ea03d78e161d7f6fff59ba1a133309.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/247/875/0da/2478750da1a4eb167f8dc1c9c55252d6.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4a9/635/bd8/4a9635bd81a0d2e46435a05c39d3457a.jpg" alt="gambar"></p><br><p>              ,         . ,                   ,      ,           ,   ..          ,       ,                .            ,       ,  ,           "" ( ,      ). </p><br><p> <strong>    </strong> </p><br><p>       ,         , ,      .      ,                  . </p><br><p>     Class Activation Map (  ).      CAM       .       2D              ,                 . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f8/95e/f8b/2f895ef8b9086c9ea56745ce0f441ef9.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/6ff/392/e60/6ff392e60f007791bee52e439099759f.jpg" alt="gambar"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fe9/896/210/fe9896210055693195c21a96b74f3188.jpg" alt="gambar"></p><br><p>       ,     .    ,    ,        Mixed-  Inception V3-,        .        () ,           . </p><br><p>     ,          ,        .          <strong></strong> ,            ,        .       ,          .   ,                 ,        ,    ,        . </p><br><p>           ,      ""  -          .               .             . </p><br><p>    ,           ,                   . </p><br><h1>  :      </h1><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Colab  </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Colab  </a> . </p><br><p> <strong>TensorFlow Hub</strong> </p><br><p> TensorFlow Hub      ,      . </p><br><p>                  .     ,      ,   ,          . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a>      . </p><br><p>              <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p> <strong></strong> </p><br><p>  ,    : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p> <strong>      TensorFlow Datasets</strong> </p><br><p>            TensorFlow Datasets.   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> ,       — <code>tf_flowers</code> .        ,       .                <code>tfds.splits</code>   (70%)   (30%).        <code>tfds.load</code> .    <code>tfds.load</code> ,            ,      . </p><br><pre> <code class="python hljs">splits = tfds.Split.TRAIN.subsplit([<span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>]) (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset tf_flowers (218.21 MiB) to /root/tensorflow_datasets/tf_flowers/1.0.0... Dl Completed... 1/|/100% 1/1 [00:07&lt;00:00, 3.67s/ url] Dl Size... 218/|/100% 218/218 [00:07&lt;00:00, 30.69 MiB/s] Extraction completed... 1/|/100% 1/1 [00:07&lt;00:00, 7.05s/ file] Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/1.0.0. Subsequent calls will reuse this data.</code> </pre> <br><p> <strong>     </strong> </p><br><p> ,      ,    ()         ,      ,          —   . </p><br><pre> <code class="python hljs">num_classes = dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes num_training_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> num_validation_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> training_set: num_training_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> validation_set: num_validation_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> print(<span class="hljs-string"><span class="hljs-string">'Total Number of Classes: {}'</span></span>.format(num_classes)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Training Images: {}'</span></span>.format(num_training_examples)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Validation Images: {} \n'</span></span>.format(num_validation_examples))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Total Number of Classes: 5 Total Number of Training Images: 2590 Total Number of Validation Images: 1080</code> </pre> <br><p>        —  . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(training_set.take(<span class="hljs-number"><span class="hljs-number">5</span></span>)): print(<span class="hljs-string"><span class="hljs-string">'Image {} shape: {} label: {}'</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape, example[<span class="hljs-number"><span class="hljs-number">1</span></span>]))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Image 1 shape: (226, 240, 3) label: 0 Image 2 shape: (240, 145, 3) label: 2 Image 3 shape: (331, 500, 3) label: 2 Image 4 shape: (240, 320, 3) label: 0 Image 5 shape: (333, 500, 3) label: 1</code> </pre> <br><p> <strong>     </strong> </p><br><p>          — ,   MobilNet v2     — 224224     (grayscale).     <code>image</code> ()  <code>label</code> ()       . </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/<span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> <strong>    TensorFlow Hub</strong> </p><br><p>    TensorFlow Hub   . ,                      ,         . </p><br><p> <strong>   </strong> </p><br><p>      <code>feature_extractor</code>  MobileNet v2. ,      TensorFlow Hub (   )   .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> .   <code>tf2-preview/mobilenet_v2/feature_vector</code> ,     URL       MobileNet v2 .   <code>feature_extractor</code>   <code>hub.KerasLayer</code>      <code>input_shape</code> . </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p> <strong>   </strong> </p><br><p>                   ,   : </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p> <strong>  </strong> </p><br><p>               ,   .            .          . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 5) 6405 ================================================================= Total params: 2,264,389 Trainable params: 6,405 Non-trainable params: 2,257,984</code> </pre> <br><p> <strong> </strong> </p><br><p>            ,           . </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 17s 216ms/step - loss: 0.7765 - acc: 0.7170 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 12s 147ms/step - loss: 0.3806 - acc: 0.8757 - val_loss: 0.3485 - val_acc: 0.8833 Epoch 3/6 81/81 [==============================] - 12s 146ms/step - loss: 0.3011 - acc: 0.9031 - val_loss: 0.3190 - val_acc: 0.8907 Epoch 4/6 81/81 [==============================] - 12s 147ms/step - loss: 0.2527 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8917 Epoch 5/6 81/81 [==============================] - 12s 148ms/step - loss: 0.2177 - acc: 0.9371 - val_loss: 0.2933 - val_acc: 0.8972 Epoch 6/6 81/81 [==============================] - 12s 146ms/step - loss: 0.1905 - acc: 0.9456 - val_loss: 0.2870 - val_acc: 0.9000</code> </pre> <br><p>         ~90%  6  ,     !   ,    ,           ~76%  80  .        ,  MobilNet v2                . </p><br><p> <strong>         </strong> </p><br><p>               . </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'Training Accuracy'</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'Validation Accuracy'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Accuracy'</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'Training Loss'</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'Validation Loss'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Loss'</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/ox/nb/hy/oxnbhyqanark0qrt1xmeqg3qv4a.png"></p><br><p>   ,   ,                    . </p><br><p>        ,           ,              . </p><br><p>        - MobileNet,           .              (  augmentation),    .                  . </p><br><p> <strong> </strong> </p><br><p>              NumPy.     ,      . </p><br><pre> <code class="python hljs">class_names = np.array(dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) print(class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['dandelion' 'daisy' 'tulips' 'sunflowers' 'roses']</code> </pre> <br><p> <strong>        </strong> </p><br><p>   <code>next()</code>   <code>image_batch</code> ( )   <code>label_batch</code> ( ).   <code>image_batch</code>  <code>label_batch</code>  NumPy     <code>.numpy()</code> .    <code>.predict()</code>      .        <code>np.argmax()</code>   .            . </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches)) image_batch = image_batch.numpy() label_batch = label_batch.numpy() predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] print(predicted_class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['sunflowers' 'roses' 'tulips' 'tulips' 'daisy' 'dandelion' 'tulips' 'sunflowers' 'daisy' 'daisy' 'tulips' 'daisy' 'daisy' 'tulips' 'tulips' 'tulips' 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'roses' 'daisy' 'daisy' 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'roses' 'dandelion']</code> </pre> <br><p> <strong>     </strong> </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Labels: "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">"Predicted labels: "</span></span>, predicted_ids)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0] Predicted labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0]</code> </pre> <br><p> <strong>  </strong> </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"Model predictions (blue: correct, red: incorrect)"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/e1/q-/rg/e1q-rgvnr6qns8-vvrcr8yzbexi.png"></p><br><p> <strong>     Inception-</strong> </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> TensorFlow Hub</a>     <code>tf2-preview/inception_v3/feature_vector</code> .        Inception V3 .      ,      Inception V3     .  ,  Inception V3       299299 .   Inception V3    MobileNet V2. </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">299</span></span> (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits) train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) model_inception = tf.keras.Sequential([ feature_extractor, tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model_inception.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 2048) 21802784 _________________________________________________________________ dense_1 (Dense) (None, 5) 10245 ================================================================= Total params: 21,813,029 Trainable params: 10,245 Non-trainable params: 21,802,784</code> </pre> <br><pre> <code class="python hljs">model_inception.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model_inception.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 44s 541ms/step - loss: 0.7594 - acc: 0.7309 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3927 - acc: 0.8772 - val_loss: 0.3945 - val_acc: 0.8657 Epoch 3/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3074 - acc: 0.9120 - val_loss: 0.3586 - val_acc: 0.8769 Epoch 4/6 81/81 [==============================] - 35s 434ms/step - loss: 0.2588 - acc: 0.9282 - val_loss: 0.3385 - val_acc: 0.8796 Epoch 5/6 81/81 [==============================] - 35s 436ms/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.3256 - val_acc: 0.8824 Epoch 6/6 81/81 [==============================] - 35s 435ms/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3164 - val_acc: 0.8861</code> </pre> <br><h1>  Ringkasan </h1><br><p>                   .          : </p><br><ul><li> <strong> :</strong> ,                .              . </li><li> <strong> :</strong>       . ""     ,       ,      . </li><li> <strong>MobileNet:</strong>       Google,                  . MobileNet              . </li></ul><br><p>              MobileNet      .                     .                   MobileNet    . </p><br><p> …   call-to-action — ,     share :) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Telegram</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">VKontakte</a> <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ojok</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id467967/">https://habr.com/ru/post/id467967/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id467953/index.html">Menguji server virtual murah</a></li>
<li><a href="../id467957/index.html">Apa yang ada di balik konstanta Feigenbaum</a></li>
<li><a href="../id467959/index.html">Fluktuasi kosmologi dan kuantum di browser</a></li>
<li><a href="../id467961/index.html">Masalah dan nuansa saat mengembangkan untuk SmartTV menggunakan React.js</a></li>
<li><a href="../id467965/index.html">Vivaldi 2.8 - Menu, silakan</a></li>
<li><a href="../id467969/index.html">Presentasi layar modal modal di iOS 13</a></li>
<li><a href="../id467973/index.html">Kelahiran platform</a></li>
<li><a href="../id467975/index.html">Huawei Dorado V6: Panas Sichuan</a></li>
<li><a href="../id467977/index.html">Membuat Aplikasi Menggunakan Komponen Bergaya di Vue.js</a></li>
<li><a href="../id467979/index.html">Integrasi iklan: bagaimana cara kerjanya?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>