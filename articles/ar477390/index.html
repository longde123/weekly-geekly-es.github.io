<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👼 💾 👸🏽 تأخير شبكة التصحيح في Kubernetes ♠️ 🍠 🎅🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="قبل عامين ، نوقشت Kubernetes بالفعل على مدونة جيثب الرسمية. منذ ذلك الحين ، أصبحت التكنولوجيا القياسية لنشر الخدمات. تدير Kubernetes الآن جزءًا كبيرًا...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>تأخير شبكة التصحيح في Kubernetes</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/477390/" style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/getpro/habr/post_images/c82/5b1/413/c825b1413d9c59cf78c51e6e2c8f8049.png"><br><br>  قبل عامين ، نوقشت Kubernetes <a href="https://github.blog/2017-08-16-kubernetes-at-github/">بالفعل</a> على مدونة جيثب الرسمية.  منذ ذلك الحين ، أصبحت التكنولوجيا القياسية لنشر الخدمات.  تدير Kubernetes الآن جزءًا كبيرًا من الخدمات الداخلية والعامة.  مع نمو مجموعاتنا وأصبحت متطلبات الأداء أكثر صرامة ، بدأنا نلاحظ أن بعض الخدمات على Kubernetes تعرض بشكل متقطع التأخيرات التي لا يمكن تفسيرها من خلال تحميل التطبيق نفسه. <br><br>  في الواقع ، في التطبيقات ، يحدث تأخير في شبكة عشوائية تصل إلى 100 مللي ثانية أو أكثر ، مما يؤدي إلى مهلات أو إعادة المحاولة.  كان من المتوقع أن تكون الخدمات قادرة على الاستجابة للطلبات أسرع بكثير من 100 مللي ثانية.  ولكن هذا غير ممكن إذا كان الاتصال نفسه يستغرق الكثير من الوقت.  بشكل منفصل ، لاحظنا استعلامات MySQL سريعة جدًا ، والتي كان من المفترض أن تأخذ مللي ثانية ، وتمكنت MySQL من إدارة ميلي ثانية ، ولكن من وجهة نظر التطبيق الطالب ، استغرقت الاستجابة 100 مللي ثانية أو أكثر. <br><a name="habracut"></a><br>  أصبح من الواضح على الفور أن المشكلة تحدث فقط عند الاتصال بمضيف Kubernetes ، حتى لو كانت المكالمة من خارج Kubernetes.  تتمثل أسهل طريقة لإعادة إنتاج المشكلة في اختبار <a href="https://github.com/tsenart/vegeta">Vegeta</a> ، الذي يتم تشغيله من أي مضيف داخلي ، ويقوم باختبار خدمة Kubernetes على منفذ معين ، ويسجل بشكل متقطع تأخيرًا كبيرًا.  في هذه المقالة ، سننظر في كيفية تمكننا من تعقب سبب هذه المشكلة. <br><br><h1 style=";text-align:right;direction:rtl">  تخلص من التعقيدات غير الضرورية في سلسلة الفشل </h1><br>  بعد إعادة إنتاج نفس المثال ، أردنا تضييق نطاق المشكلة وإزالة طبقات التعقيد الإضافية.  في البداية ، كان هناك الكثير من العناصر في التيار بين Vegeta والقرون على Kubernetes.  لتحديد مشكلة شبكة أعمق ، تحتاج إلى استبعاد بعضها. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/488/8c1/d29/4888c1d29a8fc1b4a1194c4c3a14c9ff.png"><br><br>  ينشئ العميل (Vegeta) اتصال TCP مع أي عقدة في الكتلة.  تعمل Kubernetes كشبكة تراكب (عبر شبكة مركز البيانات الحالية) ، والتي تستخدم <a href="https://en.wikipedia.org/wiki/IP_in_IP">IPIP</a> ، أي ، تقوم بتغليف حزم IP لشبكة التراكب داخل حزم IP الخاصة بمركز البيانات.  عند الاتصال بالعقدة الأولى ، يتم تنفيذ ترجمة عنوان شبكة ترجمة عنوان الشبكة (NAT) مع مراقبة الحالة لتحويل عنوان IP والمنفذ الخاص بمضيف Kubernetes إلى عنوان IP والمنفذ على شبكة التراكب (خاصة ، الحافظة مع التطبيق).  للحزم المستلمة ، يتم تنفيذ التسلسل العكسي.  هذا هو نظام معقد مع العديد من الدول والعديد من العناصر التي يتم تحديثها باستمرار وتغييرها مع نشر الخدمات ونقلها. <br><br>  الأداة المساعدة <code>tcpdump</code> في اختبار Vegeta يعطي تأخير أثناء مصافحة TCP (بين SYN و SYN-ACK).  لإزالة هذا التعقيد غير الضروري ، يمكنك استخدام <code>hping3</code> " <code>hping3</code> " البسيطة مع حزم SYN.  تحقق مما إذا كان هناك تأخير في حزمة الاستجابة ، ثم قم بإعادة تعيين الاتصال.  يمكننا تصفية البيانات من خلال تضمين الحزم التي تزيد عن 100 مللي ثانية فقط ، والحصول على خيار أبسط لإعادة إنتاج المشكلة من اختبار مستوى الشبكة الكامل 7 في Vegeta.  فيما يلي "الأصوات" لمضيف Kubernetes باستخدام TCP SYN / SYN-ACK على مضيف "المنفذ" للخدمة (30927) مع فاصل زمني قدره 10 مللي ثانية ، تتم تصفيته بواسطة أبطأ الاستجابات: <br><br> <code>theojulienne@shell ~ $ sudo hping3 172.16.47.27 -S -p 30927 -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1485 win=29200 rtt=127.1 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1486 win=29200 rtt=117.0 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1487 win=29200 rtt=106.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=1488 win=29200 rtt=104.1 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=5024 win=29200 rtt=109.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=59 DF id=0 sport=30927 flags=SA seq=5231 win=29200 rtt=109.2 ms</code> <br> <br>  يمكن أن تجعل على الفور أول ملاحظة.  توضح الأرقام التسلسلية والتوقيتات أن هذه ليست احتقان لمرة واحدة.  التأخير يتراكم في الغالب ، ويتم معالجته في النهاية. <br><br>  بعد ذلك ، نريد أن نعرف المكونات التي قد تشارك في ظهور الازدحام.  ربما هذه هي بعض المئات من قواعد iptables في NAT؟  أو بعض المشاكل مع نفق IPIP على الشبكة؟  إحدى طرق التحقق من ذلك هي التحقق من كل خطوة من خطوات النظام عن طريق استبعادها.  ماذا يحدث إذا قمت بإزالة منطق NAT وجدار الحماية ، تاركة جزءًا فقط من IPIP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b3/e2a/cff/5b3e2acff2ef9f1f8c7c527356741d92.png"><br><br>  لحسن الحظ ، يسهل Linux الوصول مباشرة إلى طبقة تراكب IP إذا كان الجهاز على نفس الشبكة: <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 10.125.20.64 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7346 win=0 rtt=127.3 ms <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7347 win=0 rtt=117.3 ms <br> <br> len=40 ip=10.125.20.64 ttl=64 DF id=0 sport=0 flags=RA seq=7348 win=0 rtt=107.2 ms</code> <br> <br>  اذا حكمنا من خلال النتائج ، لا تزال المشكلة قائمة!  هذا يستبعد iptables و NAT.  وبالتالي فإن المشكلة هي في TCP؟  دعونا نرى كيف يذهب بينغ ICMP العادية: <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 10.125.20.64 --icmp -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=28 ip=10.125.20.64 ttl=64 id=42594 icmp_seq=104 rtt=110.0 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49448 icmp_seq=4022 rtt=141.3 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49449 icmp_seq=4023 rtt=131.3 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49450 icmp_seq=4024 rtt=121.2 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49451 icmp_seq=4025 rtt=111.2 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=49452 icmp_seq=4026 rtt=101.1 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50023 icmp_seq=4343 rtt=126.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50024 icmp_seq=4344 rtt=116.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=50025 icmp_seq=4345 rtt=106.8 ms <br> <br> len=28 ip=10.125.20.64 ttl=64 id=59727 icmp_seq=9836 rtt=106.1 ms</code> <br> <br>  أظهرت النتائج أن المشكلة لم تختف.  ربما هذا هو نفق IPIP؟  دعونا تبسيط الاختبار: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/267/ff6/137/267ff613754b99f8cc1bb1d89119206e.png"><br><br>  هل يتم إرسال جميع الحزم بين هذين المضيفين؟ <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 172.16.47.27 --icmp -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41127 icmp_seq=12564 rtt=140.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41128 icmp_seq=12565 rtt=130.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41129 icmp_seq=12566 rtt=120.8 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41130 icmp_seq=12567 rtt=110.8 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=41131 icmp_seq=12568 rtt=100.7 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9062 icmp_seq=31443 rtt=134.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9063 icmp_seq=31444 rtt=124.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9064 icmp_seq=31445 rtt=114.2 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 id=9065 icmp_seq=31446 rtt=104.2 ms</code> <br> <br>  لقد قمنا بتبسيط الموقف على مضيفين من Kubernetes بإرسال أي حزمة إلى بعضهما البعض ، حتى بين اختبار ICMP.  ما زالوا يرون تأخيرًا إذا كان المضيف الهدف "سيئًا" (البعض أسوأ من الآخرين). <br><br>  الآن السؤال الأخير: لماذا يحدث التأخير فقط على خوادم عقدة kube؟  وهل يحدث عندما تكون kube-node هي المرسل أو المستقبل؟  لحسن الحظ ، من السهل للغاية معرفة ذلك بإرسال حزمة من مضيف خارج Kubernetes ، ولكن مع نفس المستلم "السيئ المعروف".  كما ترون ، لم تختف المشكلة: <br><br> <code>theojulienne@shell ~ $ sudo hping3 172.16.47.27 -p 9876 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=312 win=0 rtt=108.5 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=5903 win=0 rtt=119.4 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=6227 win=0 rtt=139.9 ms <br> <br> len=46 ip=172.16.47.27 ttl=61 DF id=0 sport=9876 flags=RA seq=7929 win=0 rtt=131.2 ms</code> <br> <br>  بعد ذلك ، نقوم بتنفيذ الطلبات نفسها من عقدة المصدر السابقة إلى المضيف الخارجي (الذي يستبعد المضيف الأصلي ، نظرًا لأن ping يشتمل على مكونات RX و TX): <br><br> <code>theojulienne@kube-node-client ~ $ sudo hping3 172.16.33.44 -p 9876 -S -i u10000 | egrep --line-buffered 'rtt=[0-9]{3}\.' <br> ^C <br> --- 172.16.33.44 hping statistic --- <br> 22352 packets transmitted, 22350 packets received, 1% packet loss <br> round-trip min/avg/max = 0.2/7.6/1010.6 ms</code> <br> <br>  بعد فحص عمليات التقاط الحزمة المؤجلة ، حصلنا على بعض المعلومات الإضافية.  على وجه الخصوص ، أن المرسل (أدناه) يرى هذه المهلة ، لكن المتلقي (أعلاه) لا يراها - راجع عمود دلتا (بالثواني): <br><br> <a href=""><img src="https://habrastorage.org/webt/4m/-t/dj/4m-tdjzws9lrhnva3xcxijel7eg.png"></a> <br><br>  بالإضافة إلى ذلك ، إذا نظرت إلى الفرق في ترتيب حزم TCP و ICMP (حسب أرقام التسلسل) على جانب المستلم ، فإن حزم ICMP تصل دائمًا بنفس التسلسل الذي تم إرسالها به ، لكن مع توقيت مختلف.  في الوقت نفسه ، تتبدل حزم TCP أحيانًا ، ويتعطل بعضها.  على وجه الخصوص ، إذا فحصنا منافذ حزم SYN ، فعندئذٍ من جانب المرسل يذهبون بالترتيب ، لكن من جانب المستلم لا يفعلون ذلك. <br><br>  هناك اختلاف بسيط في كيفية معالجة <a href="https://en.wikipedia.org/wiki/Network_address_translation">بطاقات الشبكة</a> للخوادم الحديثة (كما في مركز البيانات الخاص بنا) الحزم التي تحتوي على TCP أو ICMP.  عندما تصل الحزمة ، يقوم محول الشبكة "بتجزئةها عبر الاتصال" ، أي أنه يحاول قطع الاتصالات بالتناوب وإرسال كل قائمة انتظار إلى معالج معالج منفصل.  بالنسبة إلى TCP ، تتضمن هذه التجزئة كل من عنوان IP للمصدر والوجهة والمنفذ.  بمعنى آخر ، يتم تجزئة كل اتصال (يحتمل) بشكل مختلف.  بالنسبة لـ ICMP ، يتم تجزئة عناوين IP فقط ، نظرًا لعدم وجود منافذ. <br><br>  ملاحظة جديدة أخرى: خلال هذه الفترة نرى تأخير ICMP على جميع الاتصالات بين المضيفين ، ولكن TCP لا.  هذا يخبرنا أن السبب ربما يرجع إلى تجزئة قوائم الانتظار RX: من شبه المؤكد أن الاحتقان يحدث في معالجة حزم RX ، بدلاً من إرسال الردود. <br><br>  يستثني هذا إرسال الحزم من قائمة الأسباب المحتملة.  نعلم الآن أن مشكلة معالجة الحزم هي في جانب المتلقي على بعض خوادم العقدة kube. <br><br><h1 style=";text-align:right;direction:rtl">  فهم معالجة الحزمة في Linux Kernel </h1><br>  لفهم سبب حدوث المشكلة مع المستلم على بعض خوادم kube-node ، دعونا نرى كيف تعالج kernel Linux الحزم. <br><br>  بالرجوع إلى أبسط التنفيذ التقليدي ، تتلقى بطاقة الشبكة الحزمة وترسل <a href="https://en.wikipedia.org/wiki/Interrupt">مقاطعة</a> إلى kernel Linux ، وهي الحزمة التي تحتاج إلى معالجة.  توقف kernel عملية أخرى ، وتقوم بتبديل السياق إلى معالج المقاطعة ، ومعالجة الحزمة ، ثم العودة إلى المهام الحالية. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a2/3c3/4ee/1a23c34eea2236294913fd09a25aa1e4.png"><br><br>  رمز تبديل السياق هذا بطيء: قد لا يكون زمن الوصول ملحوظًا على بطاقات الشبكة ذات 10 ميجابايت في التسعينات ، ولكن على بطاقات 10G الحديثة بسرعات قصوى تصل إلى 15 مليون حزمة في الثانية ، يمكن مقاطعة كل نواة خادم صغير ذي ثمانية مراكز بملايين المرات في الثانية. <br><br>  من أجل عدم التعامل مع المقاطعة بشكل مستمر ، أضاف Linux منذ عدة سنوات <a href="https://en.wikipedia.org/wiki/New_API">NAPI</a> : واجهة برمجة تطبيقات للشبكة تستخدمها جميع برامج التشغيل الحديثة لزيادة الأداء بسرعات عالية.  عند السرعات المنخفضة ، لا يزال kernel يقبل المقاطعات من بطاقة الشبكة بالطريقة القديمة.  بمجرد وصول عدد كافٍ من الحزم يتجاوز العتبة ، يقوم kernel بتعطيل المقاطعات وبدلاً من ذلك يبدأ استطلاع لمحول الشبكة وأخذ الحزم على دفعات.  تتم المعالجة في softirq ، أي في <a href="https://www.kernel.org/doc/htmldocs/kernel-hacking/basics-softirqs.html">سياق</a> مقاطعات <a href="https://www.kernel.org/doc/htmldocs/kernel-hacking/basics-softirqs.html">البرامج</a> بعد مكالمات النظام ومقاطع الأجهزة عندما تكون النواة (على عكس مساحة المستخدم) قيد التشغيل بالفعل. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/22a/50d/ee1/22a50dee1fffdbc20614db2b1db28fc4.png"><br><br>  هذا أسرع بكثير ، لكنه يسبب مشكلة مختلفة.  إذا كان هناك الكثير من الحزم ، فكل الوقت الذي تستغرقه معالجة الحزم من بطاقة الشبكة ، ولا تملك عمليات مساحة المستخدم الوقت لإفراغ قوائم الانتظار هذه فعليًا (القراءة من اتصالات TCP ، إلخ).  في النهاية ، تملأ الطوابير ونبدأ في إسقاط الحزم.  في محاولة للعثور على رصيد ، يحدد kernel ميزانية لأقصى عدد من الحزم التي تتم معالجتها في سياق softirq.  بمجرد تجاوز هذه الميزانية ، <code>ksoftirqd</code> مؤشر ترابط <code>ksoftirqd</code> منفصل (سترى واحدًا منها في <code>ps</code> لكل نواة) ، والذي يعالج هذه softirqs خارج مسار syscall / interrupt العادي.  يتم تخطيط مؤشر الترابط هذا باستخدام جدولة عملية قياسية تحاول توزيع الموارد بشكل عادل. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d0f/1e6/f0c/d0f1e6f0c54d45c24d62cb2bcf90c674.png"><br><br>  بعد فحص كيفية معالجة النواة للحزم ، يمكنك أن ترى أن هناك احتمالًا معينًا للازدحام.  إذا تم تلقي مكالمات softirq بشكل متكرر أقل ، فستضطر الحزم إلى الانتظار بعض الوقت للمعالجة في قائمة انتظار RX على بطاقة الشبكة.  ربما يكون هذا بسبب بعض المهام التي تمنع نواة المعالج ، أو أي شيء آخر يمنع النواة من بدء تشغيل softirq. <br><br><h1 style=";text-align:right;direction:rtl">  نقوم بتضييق نطاق المعالجة إلى النواة أو الطريقة </h1><br>  التأخير Softirq هي مجرد افتراض.  لكن هذا منطقي ، ونعلم أننا نرى شيئًا مشابهًا للغاية.  لذلك ، فإن الخطوة التالية هي تأكيد هذه النظرية.  وإذا تم تأكيد ذلك ، فابحث عن سبب التأخير. <br><br>  العودة إلى الحزم البطيئة لدينا: <br><br> <code>len=46 ip=172.16.53.32 ttl=61 id=29573 icmp_seq=1953 rtt=99.3 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29574 icmp_seq=1954 rtt=89.3 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29575 icmp_seq=1955 rtt=79.2 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29576 icmp_seq=1956 rtt=69.1 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29577 icmp_seq=1957 rtt=59.1 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29790 icmp_seq=2070 rtt=75.7 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29791 icmp_seq=2071 rtt=65.6 ms <br> <br> len=46 ip=172.16.53.32 ttl=61 id=29792 icmp_seq=2072 rtt=55.5 ms</code> <br> <br>  كما تمت مناقشته سابقًا ، يتم تجزئة حزم ICMP هذه إلى قائمة انتظار NIC RX واحدة ومعالجتها بواسطة وحدة معالجة مركزية واحدة.  إذا كنا نريد أن نفهم كيف يعمل Linux ، فمن المفيد أن نعرف أين (على أي وحدة المعالجة المركزية الأساسية) وكيف (softirq ، ksoftirqd) تتم معالجة هذه الحزم لتتبع العملية. <br><br>  حان الوقت الآن لاستخدام الأدوات التي تسمح بمراقبة نواة Linux في الوقت الفعلي.  هنا استخدمنا <a href="https://github.com/iovisor/bcc">مخفية</a> .  تتيح لك مجموعة الأدوات هذه كتابة برامج C الصغيرة التي تعترض الوظائف التعسفية في أحداث kernel و buffer في برنامج Python الخاص بمساحة المستخدم والذي يمكنه معالجتها وإرجاع النتيجة.  الخطافات للوظائف التعسفية في kernel معقدة ، ولكن الأداة المساعدة مصممة لتوفير أقصى درجات الأمان وتم تصميمها لتتبع مشكلات الإنتاج بدقة والتي ليس من السهل إعادة إنتاجها في بيئة اختبار أو تطوير. <br><br>  الخطة هنا بسيطة: نحن نعلم أن kernel يعالج هذه الأصوات لـ ICMP ، لذلك نضع <a href="">ربطًا على وظيفة icmp_echo</a> kernel ، التي تستقبل حزمة ICMP الواردة "طلب الارتداد" وتبدأ إرسال استجابة ICMP "استجابة الارتداد".  يمكننا تحديد الحزمة عن طريق زيادة رقم icmp_seq ، والذي يظهر <code>hping3</code> أعلاه. <br><br>  يبدو رمز <a href="https://gist.github.com/theojulienne/9d78a0cb68dbe56f19a2ae6316bc6846">البرنامج النصي bcc</a> معقدًا ، لكنه ليس مخيفًا كما يبدو.  تقوم دالة <code>icmp_echo</code> بتمرير <code>struct sk_buff *skb</code> : هذه هي الحزمة مع طلب "طلب الارتداد".  يمكننا تتبع ذلك ، وسحب تسلسل <code>echo.sequence</code> (الذي يعين <code>icmp_seq</code> من hping3 <code></code> ) ، وإرساله إلى مساحة المستخدم.  كما أنه مناسب لالتقاط اسم / معرف العملية الحالي.  فيما يلي النتائج التي نراها مباشرةً أثناء معالجة الحزم بواسطة kernel: <br><br><pre style=";text-align:right;direction:rtl">  TGID PID PROCESS NAME ICMP_SEQ
 0 0 swapper / 11،770
 0 0 swapper / 11،771
 0 0 swapper / 11 772
 0 0 swapper / 11 773
 0 0 swapper / 11،774
 20041 20086 prometheus 775
 0 0 swapper / 11،776
 0 0 swapper / 11،777
 0 0 swapper / 11 778
 4512 4542 speaker-report-s 779 </pre><br>  تجدر الإشارة هنا إلى أنه في سياق <code>softirq</code> العمليات التي أجرت مكالمات النظام كـ "عمليات" ، على الرغم من أن هذا النواة في الحقيقة يعالج الحزم بأمان في سياق النواة. <br><br>  باستخدام هذه الأداة ، يمكننا تأسيس اتصال عمليات محددة مع حزم محددة تظهر تأخيرًا في <code>hping3</code> .  نصنع <code>grep</code> بسيطًا في هذا الالتقاط لقيم <code>icmp_seq</code> محددة.  تم وضع علامة على الحزم المقابلة لقيم icmp_seq أعلاه بعلامة RTT الخاصة بها ، والتي لاحظناها أعلاه (بين قوسين هي قيم RTT المتوقعة للحزم التي قمنا بتصفيتها بسبب قيم RTT التي تقل عن 50 مللي ثانية): <br><br><pre style=";text-align:right;direction:rtl">  TGID PID PROCESS NAME ICMP_SEQ ** RTT
 -
 10137 10436 cadvisor 1951
 10137 10436 cadvisor 1952
 76 76 ksoftirqd / 11 1953 ** 99ms
 76 76 ksoftirqd / 11 1954 ** 89ms
 76 76 ksoftirqd / 11 1955 ** 79ms
 76 76 ksoftirqd / 11 1956 ** 69ms
 76 76 ksoftirqd / 11 1957 ** 59ms
 76 76 ksoftirqd / 11 1958 ** (49ms)
 76 76 ksoftirqd / 11 1959 ** (39ms)
 76 76 ksoftirqd / 11 1960 ** (29ms)
 76 76 ksoftirqd / 11 1961 ** (19ms)
 76 76 ksoftirqd / 11 1962 ** (9ms)
 -
 10137 10436 cadvisor 2068
 10137 10436 cadvisor 2069
 76 76 ksoftirqd / 11 2070 ** 75ms
 76 76 ksoftirqd / 11 2071 ** 65ms
 76 76 ksoftirqd / 11 2072 ** 55ms
 76 76 ksoftirqd / 11 2073 ** (45ms)
 76 76 ksoftirqd / 11 2074 ** (35ms)
 76 76 ksoftirqd / 11 2075 ** (25ms)
 76 76 ksoftirqd / 11 2076 ** (15ms)
 76 76 ksoftirqd / 11 2077 ** (5ms) </pre><br>  النتائج تخبرنا بعض الأشياء.  أولاً ، يعالج سياق <code>ksoftirqd/11</code> كل هذه الحزم.  هذا يعني أنه بالنسبة لهذا الزوج المعين من الآلات ، تم تجزئة حزم ICMP على النواة 11 في الطرف المتلقي.  نرى أيضًا أنه في كل ازدحام مروري ، توجد حزم يتم معالجتها في سياق استدعاء نظام <code>cadvisor</code> .  ثم تأخذ <code>ksoftirqd</code> المهمة وتلبي قائمة الانتظار المتراكمة: بالضبط عدد الحزم التي تراكمت بعد <code>cadvisor</code> . <br><br>  حقيقة أن <code>cadvisor</code> يعمل دائما قبل هذا مباشرة يعني تورطه في المشكلة.  ومن المفارقات أن الغرض من <a href="https://github.com/google/cadvisor">cadvisor</a> هو "تحليل استخدام الموارد وخصائص أداء حاويات التشغيل" ، بدلاً من التسبب في مشكلة الأداء هذه. <br><br>  كما هو الحال مع الجوانب الأخرى لمناولة الحاويات ، فهذه كلها أدوات متطورة للغاية يمكن من خلالها توقع حدوث مشكلات في الأداء في بعض الظروف غير المتوقعة. <br><br><h1 style=";text-align:right;direction:rtl">  ماذا يفعل cadvisor أن يبطئ قائمة انتظار الحزمة؟ </h1><br>  الآن لدينا فهم جيد لكيفية حدوث الفشل ، وأي عملية تتسبب فيه ، وعلى أي وحدة المعالجة المركزية.  إننا نرى أنه نظرًا للحظر الصعب ، لا يوجد لدى Linux kernel الوقت لجدولة <code>ksoftirqd</code> .  ونحن نرى أن الحزم تتم معالجتها في سياق <code>cadvisor</code> .  من المنطقي افتراض أن <code>cadvisor</code> يبدأ <code>cadvisor</code> بطيئًا ، وبعد ذلك تتم معالجة جميع الحزم المتراكمة في هذا الوقت: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6fd/6fb/970/6fd6fb970f2d27943039910db9b41743.png"><br><br>  هذه هي النظرية ، ولكن كيف لاختبارها؟  ما يمكننا القيام به هو تتبع تشغيل وحدة المعالجة المركزية الأساسية خلال هذه العملية ، والعثور على النقطة التي يتم فيها تجاوز الميزانية من خلال عدد الحزم ويسمى ksoftirqd ، ثم تبدو قبل ذلك بقليل - ما الذي تم عمله بالضبط على وحدة المعالجة المركزية الأساسية قبل تلك اللحظة بالضبط.  يشبه الأشعة السينية لوحدة المعالجة المركزية كل بضعة ميلي ثانية.  سيبدو شيء مثل هذا: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/44a/954/6e8/44a9546e8de19e43cb125eb8a03a8f47.png"><br><br>  مريح ، كل هذا يمكن القيام به مع الأدوات الموجودة.  على سبيل المثال ، يتحقق <a href="https://perf.wiki.kernel.org/index.php/Tutorial">سجل الأداء</a> من وحدة المعالجة المركزية المحددة بالتردد المشار إليه ويمكنه إنشاء جدول للمكالمات إلى نظام يعمل ، بما في ذلك مساحة المستخدم ونواة Linux.  يمكنك أخذ هذا السجل ومعالجته باستخدام شوكة صغيرة من برنامج <a href="https://github.com/brendangregg/FlameGraph">FlameGraph</a> من Brendan Gregg ، والذي يحافظ على ترتيب تتبع المكدس.  يمكننا حفظ آثار مكدس سطر واحد كل 1 مللي ثانية ، ثم تحديد وحفظ العينة لمدة 100 مللي ثانية قبل أن تحصل <code>ksoftirqd</code> على التتبع: <br><br> <code># record 999 times a second, or every 1ms with some offset so not to align exactly with timers <br> sudo perf record -C 11 -g -F 999 <br> # take that recording and make a simpler stack trace. <br> sudo perf script 2&gt;/dev/null | ./FlameGraph/stackcollapse-perf-ordered.pl | grep ksoftir -B 100</code> <br> <br>  وهنا النتائج: <br><br> <code>( ,   ) <br> <br> cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_iter cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages cadvisor;[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];[cadvisor];entry_SYSCALL_64_after_swapgs;do_syscall_64;sys_read;vfs_read;seq_read;memcg_stat_show;mem_cgroup_nr_lru_pages;mem_cgroup_node_nr_lru_pages ksoftirqd/11;ret_from_fork;kthread;kthread;smpboot_thread_fn;smpboot_thread_fn;run_ksoftirqd;__do_softirq;net_rx_action;ixgbe_poll;ixgbe_clean_rx_irq;napi_gro_receive;netif_receive_skb_internal;inet_gro_receive;bond_handle_frame;__netif_receive_skb_core;ip_rcv_finish;ip_rcv;ip_forward_finish;ip_forward;ip_finish_output;nf_iterate;ip_output;ip_finish_output2;__dev_queue_xmit;dev_hard_start_xmit;ipip_tunnel_xmit;ip_tunnel_xmit;iptunnel_xmit;ip_local_out;dst_output;__ip_local_out;nf_hook_slow;nf_iterate;nf_conntrack_in;generic_packet;ipt_do_table;set_match_v4;ip_set_test;hash_net4_kadt;ixgbe_xmit_frame_ring;swiotlb_dma_mapping_error;hash_net4_test ksoftirqd/11;ret_from_fork;kthread;kthread;smpboot_thread_fn;smpboot_thread_fn;run_ksoftirqd;__do_softirq;net_rx_action;gro_cell_poll;napi_gro_receive;netif_receive_skb_internal;inet_gro_receive;__netif_receive_skb_core;ip_rcv_finish;ip_rcv;ip_forward_finish;ip_forward;ip_finish_output;nf_iterate;ip_output;ip_finish_output2;__dev_queue_xmit;dev_hard_start_xmit;dev_queue_xmit_nit;packet_rcv;tpacket_rcv;sch_direct_xmit;validate_xmit_skb_list;validate_xmit_skb;netif_skb_features;ixgbe_xmit_frame_ring;swiotlb_dma_mapping_error;__dev_queue_xmit;dev_hard_start_xmit;__bpf_prog_run;__bpf_prog_run</code> <br> <br>  هناك الكثير من الأشياء هنا ، ولكن الشيء الرئيسي هو أننا نجد قالب "cadvisor قبل ksoftirqd" الذي رأيناه سابقًا في تتبع ICMP.  ماذا يعني هذا؟ <br><br>  كل سطر هو تتبع وحدة المعالجة المركزية في وقت معين من الزمن.  يتم فصل كل استدعاء لأسفل المكدس في خط بفاصلة منقوطة.  في منتصف السطور ، نرى syscall يسمى: <code>read(): .... ;do_syscall_64;sys_read; ...</code>  <code>read(): .... ;do_syscall_64;sys_read; ...</code> .  وبالتالي ، يقضي cadvisor الكثير من الوقت على مكالمة نظام <code>read()</code> ، المتعلقة <code>mem_cgroup_*</code> (أعلى مكدس المكالمة / نهاية السطر). <br><br>  في تتبع المكالمات ، من غير <code>strace</code> معرفة ما تتم قراءته بالضبط ، لذا قم بتشغيله <code>strace</code> ما يفعله cadvisor ، وابحث عن مكالمات النظام التي تزيد عن 100 مللي ثانية: <br><br> <code>theojulienne@kube-node-bad ~ $ sudo strace -p 10137 -T -ff 2&gt;&amp;1 | egrep '&lt;0\.[1-9]' <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.156784&gt; <br> [pid 10432] &lt;... futex resumed&gt; ) = 0 &lt;0.258285&gt; <br> [pid 10137] &lt;... futex resumed&gt; ) = 0 &lt;0.678382&gt; <br> [pid 10384] &lt;... futex resumed&gt; ) = 0 &lt;0.762328&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 154234880\nrss 507904\nrss_h"..., 4096) = 658 &lt;0.179438&gt; <br> [pid 10384] &lt;... futex resumed&gt; ) = 0 &lt;0.104614&gt; <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.175936&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 577 &lt;0.228091&gt; <br> [pid 10427] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 577 &lt;0.207334&gt; <br> [pid 10411] &lt;... epoll_ctl resumed&gt; ) = 0 &lt;0.118113&gt; <br> [pid 10382] &lt;... pselect6 resumed&gt; ) = 0 (Timeout) &lt;0.117717&gt; <br> [pid 10436] &lt;... read resumed&gt; "cache 154234880\nrss 507904\nrss_h"..., 4096) = 660 &lt;0.159891&gt; <br> [pid 10417] &lt;... futex resumed&gt; ) = 0 &lt;0.917495&gt; <br> [pid 10436] &lt;... futex resumed&gt; ) = 0 &lt;0.208172&gt; <br> [pid 10417] &lt;... futex resumed&gt; ) = 0 &lt;0.190763&gt; <br> [pid 10417] &lt;... read resumed&gt; "cache 0\nrss 0\nrss_huge 0\nmapped_"..., 4096) = 576 &lt;0.154442&gt;</code> <br> <br>  كما قد تتوقع ، نرى هنا مكالمات <code>read()</code> بطيئة.  من محتويات عمليات القراءة <code>mem_cgroup</code> ، يمكن ملاحظة أن مكالمات <code>read()</code> تشير إلى ملف <code>memory.stat</code> ، الذي يوضح استخدام الذاكرة وقيود cgroup (تقنية عزل مورد Docker).  تقوم أداة cadvisor باستقصاء هذا الملف للحصول على معلومات حول استخدام الموارد للحاويات.  دعونا نتحقق مما إذا كان هذا الجهاز أو cadvisor يقوم بعمل غير متوقع: <br><br> <code>theojulienne@kube-node-bad ~ $ time cat /sys/fs/cgroup/memory/memory.stat &gt;/dev/null <br> <br> real 0m0.153s <br> user 0m0.000s <br> sys 0m0.152s <br> theojulienne@kube-node-bad ~ $</code> <br> <br>  الآن يمكننا إعادة إنتاج الأخطاء وفهم أن نواة Linux تواجه علم الأمراض. <br><br><h1 style=";text-align:right;direction:rtl">  ما الذي يجعل القراءة بطيئة للغاية؟ </h1><br>  في هذه المرحلة ، من الأسهل بكثير العثور على رسائل من مستخدمين آخرين حول مشكلات مماثلة.  كما اتضح ، في تعقب cadvisor تم الإبلاغ عن هذا الخطأ <a href="https://github.com/google/cadvisor/issues/1774">كمشكلة استخدام وحدة المعالجة المركزية المفرطة</a> ، فقط لم يلاحظ أحد أن التأخير انعكس بشكل عشوائي أيضًا في مكدس الشبكة.  في الواقع ، لوحظ أن cadvisor يستهلك وقتًا أطول من المتوقع للمعالج ، لكن هذا لم يعط أهمية كبيرة ، لأن خوادمنا لديها الكثير من موارد المعالج ، لذلك لم ندرس المشكلة بعناية. <br><br>  المشكلة هي أن مجموعات التحكم (cgroups) تأخذ في الاعتبار استخدام الذاكرة داخل مساحة الاسم (الحاوية).  عند انتهاء جميع العمليات في هذه المجموعة ، يحرر Docker مجموعة تحكم من الذاكرة.  ومع ذلك ، فإن "الذاكرة" ليست مجرد ذاكرة عملية.  على الرغم من أن ذاكرة العملية نفسها لم تعد مستخدمة ، فقد تبين أن النواة تقوم أيضًا بتعيين محتوى مؤقت ، مثل الأسنان و inode (الدليل وبيانات الملف الوصفية) ، والتي يتم تخزينها في ذاكرة التخزين المؤقت cgroup.  من وصف المشكلة: <br><br><blockquote style=";text-align:right;direction:rtl">  cgroups zombies: مجموعات التحكم التي لا توجد فيها عمليات ويتم حذفها ، لكن من أجلها لا تزال الذاكرة مخصصة (في حالتي ، من ذاكرة التخزين المؤقت لطب الأسنان ، ولكن يمكن تخصيصها أيضًا من ذاكرة التخزين المؤقت للصفحة أو tmpfs). </blockquote><br>  يمكن أن يكون التحقق من قبل النواة جميع الصفحات الموجودة في ذاكرة التخزين المؤقت عند تحرير cgroup بطيئًا للغاية ، لذلك يتم اختيار العملية البطيئة: انتظر حتى يتم طلب هذه الصفحات مرة أخرى ، وحتى عندما تكون هناك حاجة حقيقية للذاكرة ، قم بإلغاء تحديد cgroup أخيرًا.  حتى الآن ، لا تزال cgroup تؤخذ في الاعتبار عند جمع الإحصاءات. <br><br>  فيما يتعلق بالأداء ، فقد ضحوا بالذاكرة من أجل الأداء: تسريع عملية التنظيف الأولي نظرًا لحقيقة وجود القليل من الذاكرة المخزنة مؤقتًا.  هذا طبيعي.  عندما تستخدم kernel الجزء الأخير من الذاكرة المخبأة ، يتم مسح cgroup في النهاية ، لذلك لا يمكن تسميته "تسرب".  لسوء الحظ ، يؤدي التنفيذ <code>memory.stat</code> لآلية البحث <code>memory.stat</code> في هذا الإصدار kernel (4.9) ، إلى جانب الكم الهائل من الذاكرة على خوادمنا ، إلى أن الأمر يتطلب وقتًا أطول بكثير لاستعادة أحدث البيانات المخزنة مؤقتًا ومسح زومبي cgroup. <br><br>  اتضح أن هناك الكثير من الزومبي cgroup على بعض العقد لدينا أن القراءة والكمون تجاوزت الثانية. <br><br>  يتمثل أحد حل المشكلة في cadvisor في مسح ذاكرة التخزين المؤقت على الفور dentries / inodes في جميع أنحاء النظام ، مما يحل على الفور كمون القراءة وكذلك زمن انتقال الشبكة على المضيف ، لأن حذف ذاكرة التخزين المؤقت يتضمن cgroup zombie الصفحات المخزنة مؤقتًا ، كما يتم تحريرها.  هذا ليس حلاً ، لكنه يؤكد سبب المشكلة. <br><br>  اتضح أن الإصدارات الأحدث من kernel (4.19+) حسّنت أداء مكالمة <code>memory.stat</code> ، لذا فإن التبديل إلى هذا kernel حل المشكلة.  في الوقت نفسه ، كان لدينا أدوات للكشف عن العقد المشكلة في مجموعات Kubernetes ، واستنزافها بأمان وإعادة التشغيل.  تمشيطنا من خلال جميع المجموعات ، وجدنا العقد مع تأخير عالية بما فيه الكفاية وإعادة تشغيلها.  هذا أعطانا الوقت لتحديث نظام التشغيل على بقية الخوادم. <br><br><h1 style=";text-align:right;direction:rtl">  لتلخيص </h1><br>  نظرًا لأن هذا الخطأ توقف عن معالجة قوائم انتظار NIC RX لمئات الميلي ثانية ، فقد تسبب في وقت واحد في تأخير كبير على الاتصالات القصيرة وتأخير في منتصف الاتصال ، على سبيل المثال ، بين استعلامات MySQL وحزم الاستجابة. <br><br>       ,   Kubernetes,            .    Kubernetes    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar477390/">https://habr.com/ru/post/ar477390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar477378/index.html">Mixed Agile - نهج الشلال عند تطبيق تطبيقات الأعمال (الملقب Agile-like)</a></li>
<li><a href="../ar477382/index.html">الرياضات الإلكترونية - تحقيق ربح: مرسيدس ، مكبر الصوت ، المراهنة والعلامات التجارية للرياضات الإلكترونية</a></li>
<li><a href="../ar477384/index.html">مؤتمر "أمن المعلومات. تهديدات الحاضر والمستقبل "</a></li>
<li><a href="../ar477386/index.html">أسبوع الأمان 48: تسرب البيانات الضخمة وضعف Whatsapp</a></li>
<li><a href="../ar477388/index.html">NILFS2 - نظام ملفات مضاد للرصاص لـ / home</a></li>
<li><a href="../ar477392/index.html">فتح الميكروفون: الخلفية. نحن ندعو المتحدثين</a></li>
<li><a href="../ar477396/index.html">كيفية التسجيل في الدورة و ... الذهاب إلى النهاية</a></li>
<li><a href="../ar477400/index.html">حول مهنة مدير المنتج: كيفية تحقيق المثالي؟</a></li>
<li><a href="../ar477402/index.html">نشر نموذج التعلم العميق Keras كتطبيق Python على الويب</a></li>
<li><a href="../ar477404/index.html">مشكلة إنشاء وحذف الكائنات في C ++ بشكل متكرر</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>