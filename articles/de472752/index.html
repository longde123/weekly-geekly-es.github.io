<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ≥Ô∏è üë®üèø‚Äçüé§ üìï CSE: Kubernetes f√ºr alle in vCloud üëø üìô üèáüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! 

 So kam es, dass unser kleines Entwicklungsteam, ganz zu schweigen davon, dass es in letzter Zeit und sicherlich nicht pl√∂tzlich e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CSE: Kubernetes f√ºr alle in vCloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/linxdatacenter/blog/472752/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/0v/u9/pk0vu9vp36lten5zgvswrtyzpl8.png"></div><br>  <b>Hallo allerseits!</b> <br><br>  So kam es, dass unser kleines Entwicklungsteam, ganz zu schweigen davon, dass es in letzter Zeit und sicherlich nicht pl√∂tzlich einige (und in Zukunft alle) Produkte an Kubernetes √ºbertragen hat. <br><br>  Es gab viele Gr√ºnde daf√ºr, aber in unserer Geschichte geht es nicht um Holivar. <br><br>  Von der Infrastruktur her hatten wir wenig Auswahl.  vCloud Director und vCloud Director.  Wir haben uns f√ºr eine neuere Version entschieden und beschlossen zu starten. <br><br>  Als ich noch einmal ‚ÄûThe Hard Way‚Äú durchgesehen habe, bin ich schnell zu dem Schluss gekommen, dass gestern ein Tool zur Automatisierung zumindest grundlegender Prozesse wie Bereitstellung und Dimensionierung ben√∂tigt wurde.  Durch das tiefe Eintauchen in Google wurde ein Produkt wie VMware Container Service Extension (CSE) ans Licht gebracht - ein Open Source-Produkt, mit dem Sie die Erstellung und Dimensionierung von k8s-Clustern f√ºr Benutzer in vCloud automatisieren k√∂nnen. <a name="habracut"></a><br><blockquote>  Haftungsausschluss: CSE hat seine Grenzen, aber f√ºr unsere Zwecke wurde es perfekt umgesetzt.  Die L√∂sung sollte auch vom Cloud-Anbieter unterst√ºtzt werden. Da der Serverteil ebenfalls Open Source ist, fordern Sie ihn beim n√§chsten Manager an :) </blockquote><br><h2>  CSE-Client-Installation </h2><br><ol><li>  F√ºr den Einstieg ben√∂tigen Sie ein Administratorkonto in der vCloud-Organisation und ein im Voraus f√ºr den Cluster erstelltes geroutetes Netzwerk.  <i><b>Wichtig:</b> W√§hrend des Bereitstellungsprozesses ben√∂tigen Sie einen Internetzugang √ºber dieses Netzwerk. Vergessen Sie nicht, Firewall / NAT zu konfigurieren.</i> <br><br>  Adressierung spielt keine Rolle.  Nehmen Sie in diesem Beispiel 10.0.240.0/24: <br><br><img src="https://habrastorage.org/webt/lz/3i/0e/lz3i0euak9ln8kfp1fmbwuobcdi.png"><br><blockquote>  Da nach der Erstellung des Clusters eine Verwaltung erforderlich ist, wird das Vorhandensein eines VPN mit Routing zum erstellten Netzwerk empfohlen.  Wir verwenden Standard-SSL-VPN, das auf dem Edge Gateway unseres Unternehmens konfiguriert ist. </blockquote></li><li>  Als N√§chstes m√ºssen Sie den CSE-Client dort installieren, wo k8s-Cluster verwaltet werden.  In meinem Fall ist dies ein funktionierender Laptop und ein paar <s>gut versteckte</s> Container, die die Automatisierung steuern. <br><br>  Der Client ben√∂tigt installierte Python-Version 3.7.3 und h√∂her sowie ein installiertes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vcd-cli-Modul</a> , daher werden wir beide installieren. <br><br><pre><code class="bash hljs">pip3 install vcd-cli pip3 install container-service-extension</code> </pre> </li><li>  √úberpr√ºfen Sie nach der Installation die Version von CSE und erhalten Sie Folgendes: <br><br><pre> <code class="plaintext hljs"># vcd cse version Error: No such command "cse".</code> </pre><br>  Unerwartet, aber reparabel. </li><li>  Wie sich herausstellte, muss CSE als Modul an vcd-cli angeschraubt werden. <br>  Dazu m√ºssen Sie sich zuerst bei vcd-cli bei unserer Organisation anmelden: <br><br><pre> <code class="plaintext hljs"># vcd login MyCloud.provider.com org-dev admin Password: admin logged in, org: 'org-dev', vdc: 'org-dev_vDC01'</code> </pre></li><li>  Danach erstellt vcd-cli die Konfigurationsdatei <i>~ / .vcd-cli / profile.yaml</i> <br>  Am Ende m√ºssen Sie Folgendes hinzuf√ºgen: <br><br><pre> <code class="plaintext hljs">extensions: - container_service_extension.client.cse</code> </pre></li><li>  Dann √ºberpr√ºfen wir noch einmal: <br><br><pre> <code class="plaintext hljs"># vcd cse version CSE, Container Service Extension for VMware vCloud Director, version 2.5.0</code> </pre></li></ol><br>  Die Client-Installationsphase ist abgeschlossen.  Versuchen wir, den ersten Cluster bereitzustellen. <br><br><h2>  Clusterbereitstellung </h2><br>  CSE verf√ºgt √ºber mehrere S√§tze von Verwendungsparametern, die alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> angezeigt werden k√∂nnen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> <br><br><ol><li>  Erstellen Sie zun√§chst die Schl√ºssel f√ºr den kennwortlosen Zugriff auf den zuk√ºnftigen Cluster.  Dieser Punkt ist wichtig, da die Kennworteingabe auf Knoten standardm√§√üig deaktiviert ist.  Und wenn Sie die Schl√ºssel nicht angeben, k√∂nnen Sie √ºber die Konsole der virtuellen Maschine viel Arbeit erledigen, was nicht bequem ist. <br><br><pre> <code class="plaintext hljs"># ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub.</code> </pre></li><li>  Wir versuchen, einen Cluster zu erstellen: <br><br><pre> <code class="bash hljs">vcd cse cluster create MyCluster --network k8s_cluster_net --ssh-key ~/.ssh/id_rsa.pub --nodes 3 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-nfs</code> </pre> </li><li>  Wenn der <i>Fehler angezeigt wird: Die Sitzung ist abgelaufen oder der Benutzer ist nicht angemeldet.</i>  <i>Bitte melden Sie sich erneut an.</i>  - Melden Sie sich wie oben beschrieben erneut in vcd-cli bei vCloud an und versuchen Sie es erneut. <br><br>  Dieses Mal ist alles in Ordnung und die Aufgabe zum Erstellen eines Clusters hat begonnen. <br><br><pre> <code class="bash hljs">cluster operation: Creating cluster vApp <span class="hljs-string"><span class="hljs-string">'MyCluster'</span></span> (38959587-54f4-4a49-8f2e-61c3a3e879e0) from template <span class="hljs-string"><span class="hljs-string">'photon-v2_k8-1.12_weave-2.3.0'</span></span> (revision 1)</code> </pre></li><li>  Die Ausf√ºhrung der Aufgabe dauert ca. 20 Minuten.  In der Zwischenzeit werden wir die wichtigsten Startparameter analysieren. <br><blockquote><ul><li>  --network - das Netzwerk, das wir zuvor erstellt haben. </li><li>  --ssh-key - von uns erstellte Schl√ºssel, die in die Clusterknoten geschrieben werden. </li><li>  --nodes n - Die Anzahl der Worker-Knoten des Clusters.  Es wird immer einen Master geben, dies ist eine CSE-Einschr√§nkung. </li><li>  --enable-nfs - Erstellt einen zus√§tzlichen Knoten f√ºr NFS-B√§lle unter persistenten Volumes.  Als Pedaloption werden wir etwas sp√§ter darauf zur√ºckkommen, was es tut. </li></ul><br></blockquote></li><li>  In vCloud k√∂nnen Sie die Erstellung eines Clusters visuell beobachten. <br><br><img src="https://habrastorage.org/webt/x0/ni/qk/x0niqksfidmbunupuflf61u7qww.png"></li><li>  Sobald die Aufgabe zum Erstellen eines Clusters abgeschlossen ist, kann es losgehen. </li><li>  √úberpr√ºfen Sie die Bereitstellung mit dem <i>Befehl vcd cse cluster info MyCluster</i> . <br><br><img src="https://habrastorage.org/webt/fx/4m/tj/fx4mtj6uzscxl3pns7shd8lni6a.png"></li><li>  Als n√§chstes m√ºssen wir die Cluster-Konfiguration f√ºr die Verwendung von <i>kubectl erhalten</i> . <br><br><pre> <code class="plaintext hljs"># vcd cse cluster config MyCluster &gt; ./.kube/config</code> </pre></li><li>  Und Sie k√∂nnen den Status des Clusters damit √ºberpr√ºfen: <br><br><img src="https://habrastorage.org/webt/vu/1k/58/vu1k58dcahsedsulmu_vixrstge.png"></li></ol><br><h2>  Nicht so einfach </h2><br>  Zu diesem Zeitpunkt kann der Cluster als bedingt betriebsbereit betrachtet werden, wenn nicht f√ºr die Story mit persistenten Volumes.  Da wir uns in vCloud befinden, schl√§gt die Verwendung von vSphere Provider fehl.  Die Option <i>--enable-nfs wurde</i> entwickelt, um dieses <i>Problem zu beheben</i> , hat aber bis zum Ende nicht funktioniert.  Manuelle Abstimmung ist erforderlich. <br><br><ol><li>  Zu Beginn muss unser Knoten eine separate unabh√§ngige Festplatte in vCloud erstellen.  Dies stellt sicher, dass unsere Daten nicht im Cluster verschwinden, wenn sie gel√∂scht werden.  Schlie√üen Sie das Laufwerk auch an NFS an. <br><br><pre> <code class="plaintext hljs"># vcd disk create nfs-shares-1 100g --description 'Kubernetes NFS shares' # vcd vapp attach mycluster nfsd-9604 nfs-shares-1</code> </pre></li><li>  Danach gehen wir durch ssh (haben Sie die Schl√ºssel wirklich erstellt?) Zu unserem NFS-Knoten und verbinden schlie√ülich die Festplatte: <br><br><pre> <code class="plaintext hljs">root@nfsd-9604:~# parted /dev/sdb (parted) mklabel gpt Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes (parted) unit GB (parted) mkpart primary 0 100 (parted) print Model: VMware Virtual disk (scsi) Disk /dev/sdb: 100GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 0.00GB 100GB 100GB primary (parted) quit root@nfsd-9604:~# mkfs -t ext4 /dev/sdb1 Creating filesystem with 24413696 4k blocks and 6111232 inodes Filesystem UUID: 8622c0f5-4044-4ebf-95a5-0372256b34f0 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done</code> </pre></li><li>  Erstellen Sie ein Verzeichnis f√ºr die Daten und h√§ngen Sie dort einen neuen Abschnitt ein: <br><br><pre> <code class="plaintext hljs">mkdir /export echo '/dev/sdb1 /export ext4 defaults 0 0' &gt;&gt; /etc/fstab mount -a</code> </pre></li><li>  Erstellen wir f√ºnf Testabschnitte und teilen sie f√ºr den Cluster: <br><br><pre> <code class="bash hljs">&gt;<span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span> &gt;mkdir vol1 vol2 vol3 vol4 vol5 &gt;vi /etc/exports <span class="hljs-comment"><span class="hljs-comment">#     /export/vol1 *(rw,sync,no_root_squash,no_subtree_check) /export/vol2 *(rw,sync,no_root_squash,no_subtree_check) /export/vol3 *(rw,sync,no_root_squash,no_subtree_check) /export/vol4 *(rw,sync,no_root_squash,no_subtree_check) /export/vol5 *(rw,sync,no_root_squash,no_subtree_check) #:wq! ;) # -   &gt;exportfs -r</span></span></code> </pre></li><li>  Nach all dieser Magie k√∂nnen Sie PV und PVC in unserem Cluster wie folgt erstellen: <br><br>  <b>PV</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs-vol1 spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: <span class="hljs-comment"><span class="hljs-comment"># Same IP as the NFS host we ssh'ed to earlier. server: 10.150.200.22 path: "/export/vol1" EOF</span></span></code> </pre><br>  <b>PVC</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany storageClassName: <span class="hljs-string"><span class="hljs-string">""</span></span> resources: requests: storage: 10Gi EOF</code> </pre></li></ol><br>  Damit endet die Geschichte der Schaffung eines Clusters und die Geschichte seines Lebenszyklus beginnt.  Als Bonus gibt es zwei weitere n√ºtzliche CSE-Befehle, mit denen Sie manchmal Ressourcen sparen k√∂nnen <s>oder nicht</s> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    8   &gt;cse cluster resize MyCluster --network k8s_cluster_net --nodes 8 #         &gt;vcd cse node delete MyCluster node-1a2v node-6685 --yes</span></span></code> </pre><br>  Vielen Dank f√ºr Ihre Zeit, wenn Sie Fragen haben - fragen Sie in den Kommentaren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472752/">https://habr.com/ru/post/de472752/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472738/index.html">So aktualisieren Sie ein vorhandenes Projekt von ASP.NET MVC auf ASP.NET Core. Praktische Anleitung</a></li>
<li><a href="../de472744/index.html">MRP funktioniert nicht ... Was ist die Alternative?</a></li>
<li><a href="../de472746/index.html">Terminalserver f√ºr Admin; Keine einzige SSH-L√ºcke</a></li>
<li><a href="../de472748/index.html">Semantischer Browser oder Leben ohne Websites</a></li>
<li><a href="../de472750/index.html">OK, brauche ich wirklich Kubernetes?</a></li>
<li><a href="../de472754/index.html">Wie man in einem Monat Englisch spricht. 9 einfache und bew√§hrte Schritte</a></li>
<li><a href="../de472758/index.html">Vorschlag: try - integrierte Fehlerpr√ºfungsfunktion</a></li>
<li><a href="../de472760/index.html">Reduzieren Sie die Rechenzeit von einigen Jahren auf Minuten. Quantenmaschinelles Lernen verstehen</a></li>
<li><a href="../de472762/index.html">Technische Analyse des checkm8-Exploits</a></li>
<li><a href="../de472766/index.html">Parametrierung aus Datei in py.test</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>