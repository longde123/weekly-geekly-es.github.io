<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛳️ 👨🏿‍🎤 📕 CSE: Kubernetes für alle in vCloud 👿 📙 🏇🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! 

 So kam es, dass unser kleines Entwicklungsteam, ganz zu schweigen davon, dass es in letzter Zeit und sicherlich nicht plötzlich e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CSE: Kubernetes für alle in vCloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/linxdatacenter/blog/472752/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/0v/u9/pk0vu9vp36lten5zgvswrtyzpl8.png"></div><br>  <b>Hallo allerseits!</b> <br><br>  So kam es, dass unser kleines Entwicklungsteam, ganz zu schweigen davon, dass es in letzter Zeit und sicherlich nicht plötzlich einige (und in Zukunft alle) Produkte an Kubernetes übertragen hat. <br><br>  Es gab viele Gründe dafür, aber in unserer Geschichte geht es nicht um Holivar. <br><br>  Von der Infrastruktur her hatten wir wenig Auswahl.  vCloud Director und vCloud Director.  Wir haben uns für eine neuere Version entschieden und beschlossen zu starten. <br><br>  Als ich noch einmal „The Hard Way“ durchgesehen habe, bin ich schnell zu dem Schluss gekommen, dass gestern ein Tool zur Automatisierung zumindest grundlegender Prozesse wie Bereitstellung und Dimensionierung benötigt wurde.  Durch das tiefe Eintauchen in Google wurde ein Produkt wie VMware Container Service Extension (CSE) ans Licht gebracht - ein Open Source-Produkt, mit dem Sie die Erstellung und Dimensionierung von k8s-Clustern für Benutzer in vCloud automatisieren können. <a name="habracut"></a><br><blockquote>  Haftungsausschluss: CSE hat seine Grenzen, aber für unsere Zwecke wurde es perfekt umgesetzt.  Die Lösung sollte auch vom Cloud-Anbieter unterstützt werden. Da der Serverteil ebenfalls Open Source ist, fordern Sie ihn beim nächsten Manager an :) </blockquote><br><h2>  CSE-Client-Installation </h2><br><ol><li>  Für den Einstieg benötigen Sie ein Administratorkonto in der vCloud-Organisation und ein im Voraus für den Cluster erstelltes geroutetes Netzwerk.  <i><b>Wichtig:</b> Während des Bereitstellungsprozesses benötigen Sie einen Internetzugang über dieses Netzwerk. Vergessen Sie nicht, Firewall / NAT zu konfigurieren.</i> <br><br>  Adressierung spielt keine Rolle.  Nehmen Sie in diesem Beispiel 10.0.240.0/24: <br><br><img src="https://habrastorage.org/webt/lz/3i/0e/lz3i0euak9ln8kfp1fmbwuobcdi.png"><br><blockquote>  Da nach der Erstellung des Clusters eine Verwaltung erforderlich ist, wird das Vorhandensein eines VPN mit Routing zum erstellten Netzwerk empfohlen.  Wir verwenden Standard-SSL-VPN, das auf dem Edge Gateway unseres Unternehmens konfiguriert ist. </blockquote></li><li>  Als Nächstes müssen Sie den CSE-Client dort installieren, wo k8s-Cluster verwaltet werden.  In meinem Fall ist dies ein funktionierender Laptop und ein paar <s>gut versteckte</s> Container, die die Automatisierung steuern. <br><br>  Der Client benötigt installierte Python-Version 3.7.3 und höher sowie ein installiertes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vcd-cli-Modul</a> , daher werden wir beide installieren. <br><br><pre><code class="bash hljs">pip3 install vcd-cli pip3 install container-service-extension</code> </pre> </li><li>  Überprüfen Sie nach der Installation die Version von CSE und erhalten Sie Folgendes: <br><br><pre> <code class="plaintext hljs"># vcd cse version Error: No such command "cse".</code> </pre><br>  Unerwartet, aber reparabel. </li><li>  Wie sich herausstellte, muss CSE als Modul an vcd-cli angeschraubt werden. <br>  Dazu müssen Sie sich zuerst bei vcd-cli bei unserer Organisation anmelden: <br><br><pre> <code class="plaintext hljs"># vcd login MyCloud.provider.com org-dev admin Password: admin logged in, org: 'org-dev', vdc: 'org-dev_vDC01'</code> </pre></li><li>  Danach erstellt vcd-cli die Konfigurationsdatei <i>~ / .vcd-cli / profile.yaml</i> <br>  Am Ende müssen Sie Folgendes hinzufügen: <br><br><pre> <code class="plaintext hljs">extensions: - container_service_extension.client.cse</code> </pre></li><li>  Dann überprüfen wir noch einmal: <br><br><pre> <code class="plaintext hljs"># vcd cse version CSE, Container Service Extension for VMware vCloud Director, version 2.5.0</code> </pre></li></ol><br>  Die Client-Installationsphase ist abgeschlossen.  Versuchen wir, den ersten Cluster bereitzustellen. <br><br><h2>  Clusterbereitstellung </h2><br>  CSE verfügt über mehrere Sätze von Verwendungsparametern, die alle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> angezeigt werden können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> <br><br><ol><li>  Erstellen Sie zunächst die Schlüssel für den kennwortlosen Zugriff auf den zukünftigen Cluster.  Dieser Punkt ist wichtig, da die Kennworteingabe auf Knoten standardmäßig deaktiviert ist.  Und wenn Sie die Schlüssel nicht angeben, können Sie über die Konsole der virtuellen Maschine viel Arbeit erledigen, was nicht bequem ist. <br><br><pre> <code class="plaintext hljs"># ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub.</code> </pre></li><li>  Wir versuchen, einen Cluster zu erstellen: <br><br><pre> <code class="bash hljs">vcd cse cluster create MyCluster --network k8s_cluster_net --ssh-key ~/.ssh/id_rsa.pub --nodes 3 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-nfs</code> </pre> </li><li>  Wenn der <i>Fehler angezeigt wird: Die Sitzung ist abgelaufen oder der Benutzer ist nicht angemeldet.</i>  <i>Bitte melden Sie sich erneut an.</i>  - Melden Sie sich wie oben beschrieben erneut in vcd-cli bei vCloud an und versuchen Sie es erneut. <br><br>  Dieses Mal ist alles in Ordnung und die Aufgabe zum Erstellen eines Clusters hat begonnen. <br><br><pre> <code class="bash hljs">cluster operation: Creating cluster vApp <span class="hljs-string"><span class="hljs-string">'MyCluster'</span></span> (38959587-54f4-4a49-8f2e-61c3a3e879e0) from template <span class="hljs-string"><span class="hljs-string">'photon-v2_k8-1.12_weave-2.3.0'</span></span> (revision 1)</code> </pre></li><li>  Die Ausführung der Aufgabe dauert ca. 20 Minuten.  In der Zwischenzeit werden wir die wichtigsten Startparameter analysieren. <br><blockquote><ul><li>  --network - das Netzwerk, das wir zuvor erstellt haben. </li><li>  --ssh-key - von uns erstellte Schlüssel, die in die Clusterknoten geschrieben werden. </li><li>  --nodes n - Die Anzahl der Worker-Knoten des Clusters.  Es wird immer einen Master geben, dies ist eine CSE-Einschränkung. </li><li>  --enable-nfs - Erstellt einen zusätzlichen Knoten für NFS-Bälle unter persistenten Volumes.  Als Pedaloption werden wir etwas später darauf zurückkommen, was es tut. </li></ul><br></blockquote></li><li>  In vCloud können Sie die Erstellung eines Clusters visuell beobachten. <br><br><img src="https://habrastorage.org/webt/x0/ni/qk/x0niqksfidmbunupuflf61u7qww.png"></li><li>  Sobald die Aufgabe zum Erstellen eines Clusters abgeschlossen ist, kann es losgehen. </li><li>  Überprüfen Sie die Bereitstellung mit dem <i>Befehl vcd cse cluster info MyCluster</i> . <br><br><img src="https://habrastorage.org/webt/fx/4m/tj/fx4mtj6uzscxl3pns7shd8lni6a.png"></li><li>  Als nächstes müssen wir die Cluster-Konfiguration für die Verwendung von <i>kubectl erhalten</i> . <br><br><pre> <code class="plaintext hljs"># vcd cse cluster config MyCluster &gt; ./.kube/config</code> </pre></li><li>  Und Sie können den Status des Clusters damit überprüfen: <br><br><img src="https://habrastorage.org/webt/vu/1k/58/vu1k58dcahsedsulmu_vixrstge.png"></li></ol><br><h2>  Nicht so einfach </h2><br>  Zu diesem Zeitpunkt kann der Cluster als bedingt betriebsbereit betrachtet werden, wenn nicht für die Story mit persistenten Volumes.  Da wir uns in vCloud befinden, schlägt die Verwendung von vSphere Provider fehl.  Die Option <i>--enable-nfs wurde</i> entwickelt, um dieses <i>Problem zu beheben</i> , hat aber bis zum Ende nicht funktioniert.  Manuelle Abstimmung ist erforderlich. <br><br><ol><li>  Zu Beginn muss unser Knoten eine separate unabhängige Festplatte in vCloud erstellen.  Dies stellt sicher, dass unsere Daten nicht im Cluster verschwinden, wenn sie gelöscht werden.  Schließen Sie das Laufwerk auch an NFS an. <br><br><pre> <code class="plaintext hljs"># vcd disk create nfs-shares-1 100g --description 'Kubernetes NFS shares' # vcd vapp attach mycluster nfsd-9604 nfs-shares-1</code> </pre></li><li>  Danach gehen wir durch ssh (haben Sie die Schlüssel wirklich erstellt?) Zu unserem NFS-Knoten und verbinden schließlich die Festplatte: <br><br><pre> <code class="plaintext hljs">root@nfsd-9604:~# parted /dev/sdb (parted) mklabel gpt Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes (parted) unit GB (parted) mkpart primary 0 100 (parted) print Model: VMware Virtual disk (scsi) Disk /dev/sdb: 100GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 0.00GB 100GB 100GB primary (parted) quit root@nfsd-9604:~# mkfs -t ext4 /dev/sdb1 Creating filesystem with 24413696 4k blocks and 6111232 inodes Filesystem UUID: 8622c0f5-4044-4ebf-95a5-0372256b34f0 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done</code> </pre></li><li>  Erstellen Sie ein Verzeichnis für die Daten und hängen Sie dort einen neuen Abschnitt ein: <br><br><pre> <code class="plaintext hljs">mkdir /export echo '/dev/sdb1 /export ext4 defaults 0 0' &gt;&gt; /etc/fstab mount -a</code> </pre></li><li>  Erstellen wir fünf Testabschnitte und teilen sie für den Cluster: <br><br><pre> <code class="bash hljs">&gt;<span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span> &gt;mkdir vol1 vol2 vol3 vol4 vol5 &gt;vi /etc/exports <span class="hljs-comment"><span class="hljs-comment">#     /export/vol1 *(rw,sync,no_root_squash,no_subtree_check) /export/vol2 *(rw,sync,no_root_squash,no_subtree_check) /export/vol3 *(rw,sync,no_root_squash,no_subtree_check) /export/vol4 *(rw,sync,no_root_squash,no_subtree_check) /export/vol5 *(rw,sync,no_root_squash,no_subtree_check) #:wq! ;) # -   &gt;exportfs -r</span></span></code> </pre></li><li>  Nach all dieser Magie können Sie PV und PVC in unserem Cluster wie folgt erstellen: <br><br>  <b>PV</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs-vol1 spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: <span class="hljs-comment"><span class="hljs-comment"># Same IP as the NFS host we ssh'ed to earlier. server: 10.150.200.22 path: "/export/vol1" EOF</span></span></code> </pre><br>  <b>PVC</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany storageClassName: <span class="hljs-string"><span class="hljs-string">""</span></span> resources: requests: storage: 10Gi EOF</code> </pre></li></ol><br>  Damit endet die Geschichte der Schaffung eines Clusters und die Geschichte seines Lebenszyklus beginnt.  Als Bonus gibt es zwei weitere nützliche CSE-Befehle, mit denen Sie manchmal Ressourcen sparen können <s>oder nicht</s> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    8   &gt;cse cluster resize MyCluster --network k8s_cluster_net --nodes 8 #         &gt;vcd cse node delete MyCluster node-1a2v node-6685 --yes</span></span></code> </pre><br>  Vielen Dank für Ihre Zeit, wenn Sie Fragen haben - fragen Sie in den Kommentaren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472752/">https://habr.com/ru/post/de472752/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472738/index.html">So aktualisieren Sie ein vorhandenes Projekt von ASP.NET MVC auf ASP.NET Core. Praktische Anleitung</a></li>
<li><a href="../de472744/index.html">MRP funktioniert nicht ... Was ist die Alternative?</a></li>
<li><a href="../de472746/index.html">Terminalserver für Admin; Keine einzige SSH-Lücke</a></li>
<li><a href="../de472748/index.html">Semantischer Browser oder Leben ohne Websites</a></li>
<li><a href="../de472750/index.html">OK, brauche ich wirklich Kubernetes?</a></li>
<li><a href="../de472754/index.html">Wie man in einem Monat Englisch spricht. 9 einfache und bewährte Schritte</a></li>
<li><a href="../de472758/index.html">Vorschlag: try - integrierte Fehlerprüfungsfunktion</a></li>
<li><a href="../de472760/index.html">Reduzieren Sie die Rechenzeit von einigen Jahren auf Minuten. Quantenmaschinelles Lernen verstehen</a></li>
<li><a href="../de472762/index.html">Technische Analyse des checkm8-Exploits</a></li>
<li><a href="../de472766/index.html">Parametrierung aus Datei in py.test</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>