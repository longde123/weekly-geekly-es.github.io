<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👓 🤶🏻 🥉 Análisis del color emocional de las reseñas de Kinopoisk 🧒🏾 😦 👈🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entrada 
 El procesamiento del lenguaje natural (PNL) es un área popular e importante del aprendizaje automático. En este centro, describiré mi primer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Análisis del color emocional de las reseñas de Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Entrada </h3><br>  El procesamiento del lenguaje natural (PNL) es un área popular e importante del aprendizaje automático.  En este centro, describiré mi primer proyecto relacionado con el análisis del color emocional de las críticas de películas escritas en Python.  La tarea del análisis sentimental es bastante común entre aquellos que quieren dominar los conceptos básicos de PNL, y puede convertirse en un análogo del 'Hola mundo' en esta área. <br><br>  En este artículo, veremos todas las etapas principales del proceso de Data Science: desde crear su propio conjunto de datos, procesarlo y extraer características usando la biblioteca NLTK, y finalmente aprender y ajustar el modelo usando scikit-learn.  La tarea en sí misma es clasificar las revisiones en tres clases: negativa, neutral y positiva. <br><a name="habracut"></a><br><h3>  Formación de corpus de datos </h3><br>  Para resolver este problema, uno podría usar un cuerpo de datos anotado y listo con reseñas de IMDB, de los cuales hay muchos en GitHub.  Pero se decidió crear la suya propia con reseñas en ruso tomadas de Kinopoisk.  Para no copiarlos manualmente, escribiremos un analizador web.  <i>Usaré la</i> biblioteca de <i>solicitudes</i> para enviar <i>solicitudes</i> http y <i>BeautifulSoup</i> para procesar archivos html.  Primero, definamos una función que tomará un enlace a las reseñas de películas y las recuperará.  Para que Kinopoisk no reconozca el bot en nosotros, debe especificar el argumento de <i>encabezado</i> en la función request.get, que simulará el navegador.  Es necesario pasarle un diccionario con las claves User-Agent, Accept-language y Accept, cuyos valores se pueden encontrar en las herramientas de desarrollo del navegador.  A continuación, se crea un analizador y las revisiones se recuperan de la página, que se almacenan en la clase de marcado _reachbanner_ html. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Nos deshicimos del marcado html, sin embargo, nuestras revisiones siguen siendo objetos <i>BeautifulSoup</i> , pero necesitamos convertirlos en cadenas.  La función de <i>conversión</i> hace exactamente eso.  También escribiremos una función que recupere el nombre de la película, que luego se utilizará para guardar reseñas. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  La última función del analizador tomará un enlace a la página principal de la película, una clase de crítica y una forma de guardar reseñas.  La función también define los <i>retrasos</i> entre las solicitudes que son necesarios para evitar una prohibición.  La función contiene un bucle que recupera y almacena revisiones desde la primera página, hasta que encuentra una página inexistente de la cual la función <i>load_data</i> extraerá una lista vacía y el bucle se romperá. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Luego, utilizando el siguiente ciclo, puede extraer reseñas de películas que están en la lista de <i>urles</i> .  Será necesario crear una lista de películas manualmente.  Sería posible, por ejemplo, obtener una lista de enlaces a películas escribiendo una función que las extrajera de las 250 mejores películas de búsqueda de películas para no hacerlo manualmente, pero 15-20 películas serían suficientes para formar un pequeño conjunto de datos de mil revisiones para cada clase.  Además, si obtiene una prohibición, el programa mostrará en qué película y clase se detuvo el analizador para continuar desde el mismo lugar después de aprobar la prohibición. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Pretratamiento </h3><br>  Después de escribir el analizador, recordando películas aleatorias para él y varias prohibiciones de la búsqueda de películas, mezclé las reseñas en carpetas y seleccioné 900 reseñas de cada clase para capacitación y el resto para el grupo de control.  Ahora es necesario preprocesar la carcasa, es decir, tokenizarla y normalizarla.  Tokenizar significa dividir el texto en componentes, en este caso en palabras, ya que usaremos la representación de una bolsa de palabras.  Y la normalización consiste en convertir palabras en minúsculas, eliminar palabras de detención y ruido excesivo, tartamudear y cualquier otro truco que ayude a reducir el espacio de los signos. <br><br>  Importamos las bibliotecas necesarias. <br><br><div class="spoiler">  <b class="spoiler_title">Texto oculto</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Comenzamos definiendo algunas funciones pequeñas para el preprocesamiento de texto.  El primero, llamado <i>lower_pos_tag,</i> tomará una lista con palabras, las convertirá a minúsculas y guardará cada token en una tupla con su parte del discurso.  La operación de agregar parte del discurso a una palabra se denomina etiquetado Parte del discurso (POS) y a menudo se usa en PNL para extraer entidades.  En nuestro caso, utilizaremos partes del discurso en la siguiente función para filtrar palabras. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Los textos contienen una gran cantidad de palabras que con demasiada frecuencia resultan útiles para el modelo (las llamadas palabras de detención).  Básicamente, estas son preposiciones, conjunciones, pronombres por los cuales es imposible determinar a qué clase se refiere el recuerdo.  La función <i>clean</i> solo deja sustantivos, adjetivos, verbos y adverbios.  Tenga en cuenta que elimina partes del discurso, ya que no son necesarias para el modelo en sí.  También puede notar que esta función utiliza el tartamudeo, cuya esencia es eliminar los sufijos y prefijos de las palabras.  Esto le permite reducir la dimensión de los signos, ya que las palabras con diferentes géneros y casos se reducirán al mismo token.  Existe un análogo más poderoso de la tartamudez: la lematización, le permite restaurar la forma inicial de la palabra.  Sin embargo, funciona más lento que el tartamudeo y, además, NLTK no tiene un lematizador ruso. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  A continuación, escribimos la función final que tomará la etiqueta de clase y recuperará todas las revisiones con esta clase.  Para leer el caso, utilizaremos el método sin <i>procesar</i> del objeto <i>PlaintextCorpusReader</i> , que le permite extraer texto del archivo especificado.  A continuación, se usa la tokenización RegexpTokenizer, que funciona sobre la base de una expresión regular.  Además de palabras individuales, agregué al modelo bigrams, que son combinaciones de todas las palabras vecinas.  Esta función también utiliza el objeto <i>FreqDist</i> , que devuelve la frecuencia de aparición de palabras.  Se usa aquí para eliminar palabras que aparecen en todas las revisiones de una clase en particular solo una vez (también se llaman hapaks).  Por lo tanto, la función devolverá un diccionario que contiene documentos presentados como una bolsa de palabras y una lista de todas las palabras para una clase en particular. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  La etapa de preprocesamiento es la más larga, por lo que tiene sentido paralelizar el procesamiento de nuestro caso.  Esto se puede hacer usando el módulo de <i>multiprocesamiento</i> .  En el siguiente fragmento de código de programa, comienzo tres procesos que procesarán simultáneamente tres carpetas con diferentes clases.  A continuación, los resultados se recopilarán en un diccionario.  Este preprocesamiento se ha completado. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vectorización </h3><br>  Después de preprocesar el caso, tenemos un diccionario donde para cada etiqueta de clase contiene una lista con reseñas que tokenizamos, normalizamos y enriquecimos con bigrams, así como una lista de palabras de todas las revisiones de esta clase.  Dado que el modelo no puede percibir el lenguaje natural como lo hacemos nosotros, la tarea ahora es presentar nuestras revisiones en forma numérica.  Para hacer esto, crearemos un vocabulario común, que consta de tokens únicos, y con él vectorizaremos cada revisión. <br><br>  Para empezar, creamos una lista que contiene revisiones de todas las clases junto con sus etiquetas.  A continuación, creamos un vocabulario común, tomando de cada clase 10,000 de las palabras más comunes usando el método <i>most_common</i> del mismo <i>FreqDist</i> .  Como resultado, obtuve un vocabulario que consta de aproximadamente 17,000 palabras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Hay varias formas de vectorizar texto.  El más popular de ellos: TF-IDF, codificación directa y de frecuencia.  Utilicé la codificación de frecuencia, cuya esencia es presentar cada revisión como un vector, cuyos elementos son el número de apariciones de cada palabra del vocabulario.  <i>NLTK</i> tiene sus propios clasificadores, puede usarlos, pero funcionan más lentamente que sus contrapartes de <i>scikit-learn</i> y tienen menos configuraciones.  A continuación se muestra el código de codificación para <i>NLTK</i> .  Sin embargo, <i>usaré</i> el modelo Naive Bayes de <i>scikit-learn</i> y codificaré las revisiones, almacenando los atributos en una matriz dispersa de <i>SciPy</i> y las etiquetas de clase en una matriz <i>NumPy</i> separada. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Como en el conjunto de datos las revisiones con ciertas etiquetas van una tras otra, es decir, primero todas neutrales, luego todas negativas, etc., debe mezclarlas.  Para hacer esto, puede usar la función <i>aleatoria</i> de <i>scikit-learn</i> .  Es adecuado para situaciones en las que los signos y las etiquetas de clase se encuentran en matrices diferentes, ya que le permite mezclar dos matrices al unísono. <br><br><h3>  Entrenamiento modelo </h3><br>  Ahora queda entrenar el modelo y verificar su precisión en el grupo de control.  Como modelo, utilizaremos el modelo del clasificador Naive Bayes.  <i>Scikit-learn</i> tiene tres modelos Naive Bayes dependiendo de la distribución de datos: binario, discreto y continuo.  Como la distribución de nuestras funciones es discreta, elegimos <i>MultinomialNB</i> . <br><br>  El clasificador bayesiano tiene el hiperparámetro <i>alfa</i> , que es responsable de suavizar el modelo.  Naive Bayes calcula las probabilidades de que cada revisión pertenezca a todas las clases, para esto multiplica las probabilidades condicionales de la aparición de todas las palabras de revisión, siempre que pertenezcan a una clase en particular.  Pero si no se encontró alguna palabra de revisión en el conjunto de datos de entrenamiento, entonces su probabilidad condicional es igual a cero, lo que anula la probabilidad de que la revisión pertenezca a cualquier clase.  Para evitar esto, por defecto, se agrega una unidad a todas las probabilidades de palabras condicionales, es decir, <i>alfa</i> es igual a uno.  Sin embargo, este valor puede no ser óptimo.  Puede intentar seleccionar <i>alfa</i> utilizando la búsqueda de cuadrícula y la validación cruzada. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  En mi caso, el hogar de la cuadrícula da el valor óptimo del hiperparámetro igual a 0 con una precisión de 0.965.  Sin embargo, este valor obviamente no será óptimo para el conjunto de datos de control, ya que habrá una gran cantidad de palabras que no se han encontrado previamente en el conjunto de entrenamiento.  Para un conjunto de datos de referencia, este modelo tiene una precisión de 0.598.  Sin embargo, si aumenta <i>alfa</i> a 0.1, la precisión en los datos de entrenamiento se reducirá a 0.82, y en los datos de control aumentará a 0.62.  Lo más probable es que, en un conjunto de datos más grande, la diferencia sea más significativa. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Conclusión </h3><br>  Se supone que el modelo debe usarse para predecir revisiones cuyas palabras no se usaron para formar un vocabulario.  Por lo tanto, la calidad del modelo puede evaluarse por su precisión en la parte de control de los datos, que es 0.62.  Esto es casi dos veces mejor que solo adivinar, pero la precisión sigue siendo bastante baja. <br><br>  Según el informe de clasificación, está claro que el modelo funciona peor con revisiones que tienen un color neutro (precisión 0.47 versus 0.68 para positivo y 0.76 para negativo).  De hecho, las revisiones neutrales contienen palabras que son características de las críticas positivas y negativas.  Probablemente, la precisión del modelo puede mejorarse aumentando el volumen del conjunto de datos, ya que el conjunto de datos número tres mil es bastante modesto.  Además, sería posible reducir el problema a una clasificación binaria de revisiones en positivas y negativas, lo que también aumentaría la precisión. <br><br>  Gracias por leer <br><br>  PD: si quieres practicar tú mismo, mi conjunto de datos se puede descargar debajo del enlace. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace al conjunto de datos</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467081/">https://habr.com/ru/post/467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467061/index.html">Fila babilónica: 5 problemas de seguridad en el negocio de la construcción</a></li>
<li><a href="../467063/index.html">Monitoreo de combustible para generadores diesel de centros de datos: ¿cómo hacerlo y por qué es tan importante?</a></li>
<li><a href="../467065/index.html">Archivo de problemas olímpicos en física para escolares</a></li>
<li><a href="../467073/index.html">“En Occidente, no hay directores de arte menores de 40 años. Con nosotros puede ser hasta 30. " ¿Cómo es ser diseñador en TI?</a></li>
<li><a href="../467079/index.html">CSS y Javascript Carrusel de hormigas</a></li>
<li><a href="../467083/index.html">Cómo se usa la extraña instrucción popcount en procesadores modernos</a></li>
<li><a href="../467085/index.html">C, C ++ y DotNet decompile son los principios básicos de la inversión. Resolviendo problemas para revertir con r0ot-mi. Parte 1</a></li>
<li><a href="../467087/index.html">Cómo me preparé y pasé la Certificación SQL de Oracle Database (1Z0-071)</a></li>
<li><a href="../467089/index.html">Exim parcheado - parche nuevamente. Nueva ejecución remota de comandos en Exim 4.92 en una solicitud</a></li>
<li><a href="../467091/index.html">Una introducción rápida a Svelte desde una perspectiva de desarrollador angular</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>