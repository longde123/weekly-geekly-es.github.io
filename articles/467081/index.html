<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëì ü§∂üèª ü•â An√°lisis del color emocional de las rese√±as de Kinopoisk üßíüèæ üò¶ üëàüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entrada 
 El procesamiento del lenguaje natural (PNL) es un √°rea popular e importante del aprendizaje autom√°tico. En este centro, describir√© mi primer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lisis del color emocional de las rese√±as de Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Entrada </h3><br>  El procesamiento del lenguaje natural (PNL) es un √°rea popular e importante del aprendizaje autom√°tico.  En este centro, describir√© mi primer proyecto relacionado con el an√°lisis del color emocional de las cr√≠ticas de pel√≠culas escritas en Python.  La tarea del an√°lisis sentimental es bastante com√∫n entre aquellos que quieren dominar los conceptos b√°sicos de PNL, y puede convertirse en un an√°logo del 'Hola mundo' en esta √°rea. <br><br>  En este art√≠culo, veremos todas las etapas principales del proceso de Data Science: desde crear su propio conjunto de datos, procesarlo y extraer caracter√≠sticas usando la biblioteca NLTK, y finalmente aprender y ajustar el modelo usando scikit-learn.  La tarea en s√≠ misma es clasificar las revisiones en tres clases: negativa, neutral y positiva. <br><a name="habracut"></a><br><h3>  Formaci√≥n de corpus de datos </h3><br>  Para resolver este problema, uno podr√≠a usar un cuerpo de datos anotado y listo con rese√±as de IMDB, de los cuales hay muchos en GitHub.  Pero se decidi√≥ crear la suya propia con rese√±as en ruso tomadas de Kinopoisk.  Para no copiarlos manualmente, escribiremos un analizador web.  <i>Usar√© la</i> biblioteca de <i>solicitudes</i> para enviar <i>solicitudes</i> http y <i>BeautifulSoup</i> para procesar archivos html.  Primero, definamos una funci√≥n que tomar√° un enlace a las rese√±as de pel√≠culas y las recuperar√°.  Para que Kinopoisk no reconozca el bot en nosotros, debe especificar el argumento de <i>encabezado</i> en la funci√≥n request.get, que simular√° el navegador.  Es necesario pasarle un diccionario con las claves User-Agent, Accept-language y Accept, cuyos valores se pueden encontrar en las herramientas de desarrollo del navegador.  A continuaci√≥n, se crea un analizador y las revisiones se recuperan de la p√°gina, que se almacenan en la clase de marcado _reachbanner_ html. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Nos deshicimos del marcado html, sin embargo, nuestras revisiones siguen siendo objetos <i>BeautifulSoup</i> , pero necesitamos convertirlos en cadenas.  La funci√≥n de <i>conversi√≥n</i> hace exactamente eso.  Tambi√©n escribiremos una funci√≥n que recupere el nombre de la pel√≠cula, que luego se utilizar√° para guardar rese√±as. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  La √∫ltima funci√≥n del analizador tomar√° un enlace a la p√°gina principal de la pel√≠cula, una clase de cr√≠tica y una forma de guardar rese√±as.  La funci√≥n tambi√©n define los <i>retrasos</i> entre las solicitudes que son necesarios para evitar una prohibici√≥n.  La funci√≥n contiene un bucle que recupera y almacena revisiones desde la primera p√°gina, hasta que encuentra una p√°gina inexistente de la cual la funci√≥n <i>load_data</i> extraer√° una lista vac√≠a y el bucle se romper√°. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Luego, utilizando el siguiente ciclo, puede extraer rese√±as de pel√≠culas que est√°n en la lista de <i>urles</i> .  Ser√° necesario crear una lista de pel√≠culas manualmente.  Ser√≠a posible, por ejemplo, obtener una lista de enlaces a pel√≠culas escribiendo una funci√≥n que las extrajera de las 250 mejores pel√≠culas de b√∫squeda de pel√≠culas para no hacerlo manualmente, pero 15-20 pel√≠culas ser√≠an suficientes para formar un peque√±o conjunto de datos de mil revisiones para cada clase.  Adem√°s, si obtiene una prohibici√≥n, el programa mostrar√° en qu√© pel√≠cula y clase se detuvo el analizador para continuar desde el mismo lugar despu√©s de aprobar la prohibici√≥n. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Pretratamiento </h3><br>  Despu√©s de escribir el analizador, recordando pel√≠culas aleatorias para √©l y varias prohibiciones de la b√∫squeda de pel√≠culas, mezcl√© las rese√±as en carpetas y seleccion√© 900 rese√±as de cada clase para capacitaci√≥n y el resto para el grupo de control.  Ahora es necesario preprocesar la carcasa, es decir, tokenizarla y normalizarla.  Tokenizar significa dividir el texto en componentes, en este caso en palabras, ya que usaremos la representaci√≥n de una bolsa de palabras.  Y la normalizaci√≥n consiste en convertir palabras en min√∫sculas, eliminar palabras de detenci√≥n y ruido excesivo, tartamudear y cualquier otro truco que ayude a reducir el espacio de los signos. <br><br>  Importamos las bibliotecas necesarias. <br><br><div class="spoiler">  <b class="spoiler_title">Texto oculto</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Comenzamos definiendo algunas funciones peque√±as para el preprocesamiento de texto.  El primero, llamado <i>lower_pos_tag,</i> tomar√° una lista con palabras, las convertir√° a min√∫sculas y guardar√° cada token en una tupla con su parte del discurso.  La operaci√≥n de agregar parte del discurso a una palabra se denomina etiquetado Parte del discurso (POS) y a menudo se usa en PNL para extraer entidades.  En nuestro caso, utilizaremos partes del discurso en la siguiente funci√≥n para filtrar palabras. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Los textos contienen una gran cantidad de palabras que con demasiada frecuencia resultan √∫tiles para el modelo (las llamadas palabras de detenci√≥n).  B√°sicamente, estas son preposiciones, conjunciones, pronombres por los cuales es imposible determinar a qu√© clase se refiere el recuerdo.  La funci√≥n <i>clean</i> solo deja sustantivos, adjetivos, verbos y adverbios.  Tenga en cuenta que elimina partes del discurso, ya que no son necesarias para el modelo en s√≠.  Tambi√©n puede notar que esta funci√≥n utiliza el tartamudeo, cuya esencia es eliminar los sufijos y prefijos de las palabras.  Esto le permite reducir la dimensi√≥n de los signos, ya que las palabras con diferentes g√©neros y casos se reducir√°n al mismo token.  Existe un an√°logo m√°s poderoso de la tartamudez: la lematizaci√≥n, le permite restaurar la forma inicial de la palabra.  Sin embargo, funciona m√°s lento que el tartamudeo y, adem√°s, NLTK no tiene un lematizador ruso. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  A continuaci√≥n, escribimos la funci√≥n final que tomar√° la etiqueta de clase y recuperar√° todas las revisiones con esta clase.  Para leer el caso, utilizaremos el m√©todo sin <i>procesar</i> del objeto <i>PlaintextCorpusReader</i> , que le permite extraer texto del archivo especificado.  A continuaci√≥n, se usa la tokenizaci√≥n RegexpTokenizer, que funciona sobre la base de una expresi√≥n regular.  Adem√°s de palabras individuales, agregu√© al modelo bigrams, que son combinaciones de todas las palabras vecinas.  Esta funci√≥n tambi√©n utiliza el objeto <i>FreqDist</i> , que devuelve la frecuencia de aparici√≥n de palabras.  Se usa aqu√≠ para eliminar palabras que aparecen en todas las revisiones de una clase en particular solo una vez (tambi√©n se llaman hapaks).  Por lo tanto, la funci√≥n devolver√° un diccionario que contiene documentos presentados como una bolsa de palabras y una lista de todas las palabras para una clase en particular. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  La etapa de preprocesamiento es la m√°s larga, por lo que tiene sentido paralelizar el procesamiento de nuestro caso.  Esto se puede hacer usando el m√≥dulo de <i>multiprocesamiento</i> .  En el siguiente fragmento de c√≥digo de programa, comienzo tres procesos que procesar√°n simult√°neamente tres carpetas con diferentes clases.  A continuaci√≥n, los resultados se recopilar√°n en un diccionario.  Este preprocesamiento se ha completado. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vectorizaci√≥n </h3><br>  Despu√©s de preprocesar el caso, tenemos un diccionario donde para cada etiqueta de clase contiene una lista con rese√±as que tokenizamos, normalizamos y enriquecimos con bigrams, as√≠ como una lista de palabras de todas las revisiones de esta clase.  Dado que el modelo no puede percibir el lenguaje natural como lo hacemos nosotros, la tarea ahora es presentar nuestras revisiones en forma num√©rica.  Para hacer esto, crearemos un vocabulario com√∫n, que consta de tokens √∫nicos, y con √©l vectorizaremos cada revisi√≥n. <br><br>  Para empezar, creamos una lista que contiene revisiones de todas las clases junto con sus etiquetas.  A continuaci√≥n, creamos un vocabulario com√∫n, tomando de cada clase 10,000 de las palabras m√°s comunes usando el m√©todo <i>most_common</i> del mismo <i>FreqDist</i> .  Como resultado, obtuve un vocabulario que consta de aproximadamente 17,000 palabras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Hay varias formas de vectorizar texto.  El m√°s popular de ellos: TF-IDF, codificaci√≥n directa y de frecuencia.  Utilic√© la codificaci√≥n de frecuencia, cuya esencia es presentar cada revisi√≥n como un vector, cuyos elementos son el n√∫mero de apariciones de cada palabra del vocabulario.  <i>NLTK</i> tiene sus propios clasificadores, puede usarlos, pero funcionan m√°s lentamente que sus contrapartes de <i>scikit-learn</i> y tienen menos configuraciones.  A continuaci√≥n se muestra el c√≥digo de codificaci√≥n para <i>NLTK</i> .  Sin embargo, <i>usar√©</i> el modelo Naive Bayes de <i>scikit-learn</i> y codificar√© las revisiones, almacenando los atributos en una matriz dispersa de <i>SciPy</i> y las etiquetas de clase en una matriz <i>NumPy</i> separada. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Como en el conjunto de datos las revisiones con ciertas etiquetas van una tras otra, es decir, primero todas neutrales, luego todas negativas, etc., debe mezclarlas.  Para hacer esto, puede usar la funci√≥n <i>aleatoria</i> de <i>scikit-learn</i> .  Es adecuado para situaciones en las que los signos y las etiquetas de clase se encuentran en matrices diferentes, ya que le permite mezclar dos matrices al un√≠sono. <br><br><h3>  Entrenamiento modelo </h3><br>  Ahora queda entrenar el modelo y verificar su precisi√≥n en el grupo de control.  Como modelo, utilizaremos el modelo del clasificador Naive Bayes.  <i>Scikit-learn</i> tiene tres modelos Naive Bayes dependiendo de la distribuci√≥n de datos: binario, discreto y continuo.  Como la distribuci√≥n de nuestras funciones es discreta, elegimos <i>MultinomialNB</i> . <br><br>  El clasificador bayesiano tiene el hiperpar√°metro <i>alfa</i> , que es responsable de suavizar el modelo.  Naive Bayes calcula las probabilidades de que cada revisi√≥n pertenezca a todas las clases, para esto multiplica las probabilidades condicionales de la aparici√≥n de todas las palabras de revisi√≥n, siempre que pertenezcan a una clase en particular.  Pero si no se encontr√≥ alguna palabra de revisi√≥n en el conjunto de datos de entrenamiento, entonces su probabilidad condicional es igual a cero, lo que anula la probabilidad de que la revisi√≥n pertenezca a cualquier clase.  Para evitar esto, por defecto, se agrega una unidad a todas las probabilidades de palabras condicionales, es decir, <i>alfa</i> es igual a uno.  Sin embargo, este valor puede no ser √≥ptimo.  Puede intentar seleccionar <i>alfa</i> utilizando la b√∫squeda de cuadr√≠cula y la validaci√≥n cruzada. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  En mi caso, el hogar de la cuadr√≠cula da el valor √≥ptimo del hiperpar√°metro igual a 0 con una precisi√≥n de 0.965.  Sin embargo, este valor obviamente no ser√° √≥ptimo para el conjunto de datos de control, ya que habr√° una gran cantidad de palabras que no se han encontrado previamente en el conjunto de entrenamiento.  Para un conjunto de datos de referencia, este modelo tiene una precisi√≥n de 0.598.  Sin embargo, si aumenta <i>alfa</i> a 0.1, la precisi√≥n en los datos de entrenamiento se reducir√° a 0.82, y en los datos de control aumentar√° a 0.62.  Lo m√°s probable es que, en un conjunto de datos m√°s grande, la diferencia sea m√°s significativa. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Conclusi√≥n </h3><br>  Se supone que el modelo debe usarse para predecir revisiones cuyas palabras no se usaron para formar un vocabulario.  Por lo tanto, la calidad del modelo puede evaluarse por su precisi√≥n en la parte de control de los datos, que es 0.62.  Esto es casi dos veces mejor que solo adivinar, pero la precisi√≥n sigue siendo bastante baja. <br><br>  Seg√∫n el informe de clasificaci√≥n, est√° claro que el modelo funciona peor con revisiones que tienen un color neutro (precisi√≥n 0.47 versus 0.68 para positivo y 0.76 para negativo).  De hecho, las revisiones neutrales contienen palabras que son caracter√≠sticas de las cr√≠ticas positivas y negativas.  Probablemente, la precisi√≥n del modelo puede mejorarse aumentando el volumen del conjunto de datos, ya que el conjunto de datos n√∫mero tres mil es bastante modesto.  Adem√°s, ser√≠a posible reducir el problema a una clasificaci√≥n binaria de revisiones en positivas y negativas, lo que tambi√©n aumentar√≠a la precisi√≥n. <br><br>  Gracias por leer <br><br>  PD: si quieres practicar t√∫ mismo, mi conjunto de datos se puede descargar debajo del enlace. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace al conjunto de datos</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/467081/">https://habr.com/ru/post/467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../467061/index.html">Fila babil√≥nica: 5 problemas de seguridad en el negocio de la construcci√≥n</a></li>
<li><a href="../467063/index.html">Monitoreo de combustible para generadores diesel de centros de datos: ¬øc√≥mo hacerlo y por qu√© es tan importante?</a></li>
<li><a href="../467065/index.html">Archivo de problemas ol√≠mpicos en f√≠sica para escolares</a></li>
<li><a href="../467073/index.html">‚ÄúEn Occidente, no hay directores de arte menores de 40 a√±os. Con nosotros puede ser hasta 30. " ¬øC√≥mo es ser dise√±ador en TI?</a></li>
<li><a href="../467079/index.html">CSS y Javascript Carrusel de hormigas</a></li>
<li><a href="../467083/index.html">C√≥mo se usa la extra√±a instrucci√≥n popcount en procesadores modernos</a></li>
<li><a href="../467085/index.html">C, C ++ y DotNet decompile son los principios b√°sicos de la inversi√≥n. Resolviendo problemas para revertir con r0ot-mi. Parte 1</a></li>
<li><a href="../467087/index.html">C√≥mo me prepar√© y pas√© la Certificaci√≥n SQL de Oracle Database (1Z0-071)</a></li>
<li><a href="../467089/index.html">Exim parcheado - parche nuevamente. Nueva ejecuci√≥n remota de comandos en Exim 4.92 en una solicitud</a></li>
<li><a href="../467091/index.html">Una introducci√≥n r√°pida a Svelte desde una perspectiva de desarrollador angular</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>