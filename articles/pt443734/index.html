<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😯 🤳 🧙🏿 As redes neurais têm uma estratégia de classificação de imagem incrivelmente simples. 🌚 🍙 🕯️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As redes neurais convolucionais fazem um excelente trabalho ao classificar imagens distorcidas, diferentemente dos humanos. 


 Neste artigo, mostrare...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>As redes neurais têm uma estratégia de classificação de imagem incrivelmente simples.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443734/"><h3>  As redes neurais convolucionais fazem um excelente trabalho ao classificar imagens distorcidas, diferentemente dos humanos. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/2e6/a7c/4c8/2e6a7c4c8f0ba811e57fa24193375289.jpg"><br><br>  Neste artigo, mostrarei por que redes neurais profundas avançadas podem reconhecer perfeitamente imagens distorcidas e como isso ajuda a revelar a estratégia surpreendentemente simples usada por redes neurais para classificar fotografias naturais.  Essas descobertas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">publicadas</a> no ICLR 2019, têm muitas consequências: em primeiro lugar, demonstram que é muito mais fácil encontrar uma solução “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ImageNet</a> ” do que se pensava.  Em segundo lugar, eles nos ajudam a criar sistemas de classificação de imagem mais interpretáveis ​​e compreensíveis.  Em terceiro lugar, eles explicam vários fenômenos observados nas redes neurais convolucionais modernas (SNA), por exemplo, sua tendência a procurar texturas (veja nosso outro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho</a> no ICLR 2019 e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entrada de blog</a> correspondente) e ignorando o arranjo espacial de partes do objeto. <br><a name="habracut"></a><br><h2>  Bons velhos modelos "saco de palavras" </h2><br>  Nos bons velhos tempos, antes do advento do aprendizado profundo, o reconhecimento de imagens naturais era bastante simples: definimos um conjunto de recursos visuais principais (“palavras”), determinamos com que frequência cada recurso visual ocorre em uma imagem (“bolsa”) e classificamos a imagem com base nelas. números.  Portanto, esses modelos em visão computacional são chamados de "saco de palavras" (saco de palavras ou BoW).  Por exemplo, suponha que tenhamos duas características visuais, o olho humano e a caneta, e desejemos classificar as imagens em duas classes, "pessoas" e "pássaros".  O modelo mais simples de BoW seria este: para cada olho encontrado na imagem, aumentamos o testemunho em favor da "pessoa" em 1. E vice-versa, para cada caneta aumentamos o testemunho em favor do "pássaro" em 1. Qual classe obtém mais evidências, será essa. <br><br>  Uma propriedade conveniente de um modelo de BoW tão simples é a interpretabilidade e a clareza do processo de tomada de decisão: podemos verificar com precisão quais recursos específicos da imagem falam em favor de uma classe específica, a integração espacial de recursos é muito simples (em comparação com a integração não linear de recursos em redes neurais profundas), portanto apenas entenda como o modelo toma suas decisões. <br><br>  Os modelos tradicionais de BoW eram extremamente populares e funcionavam muito bem antes da invasão do aprendizado profundo, mas rapidamente saíram de moda devido à eficiência relativamente baixa.  Mas temos certeza de que as redes neurais usam uma estratégia de decisão fundamentalmente diferente da BoW? <br><br><h2>  Rede Interpretada em Profundidade com Recursos de Bolsa (BagNet) </h2><br>  Para testar essa suposição, combinamos a interpretabilidade e a clareza dos modelos de BoW com a eficiência das redes neurais.  A estratégia é assim: <br><ul><li>  Divida a imagem em pedaços pequenos qx q. </li><li>  Passamos as peças pela rede neural para obter evidências de associação de classe (logits) para cada peça. </li><li>  Resuma as evidências em todas as peças para obter uma solução no nível de toda a imagem. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/474/6f5/363/4746f53632bd9e3e7477de9ecb76d396.png"><br><br>  Para implementar essa estratégia, da maneira mais simples, adotamos a arquitetura padrão ResNet-50 e substituímos quase todas as convoluções 3x3 por convoluções 1x1.  Como resultado, cada elemento oculto na última camada convolucional "vê" apenas uma pequena parte da imagem (ou seja, seu campo de percepção é muito menor que o tamanho da imagem).  Portanto, evitamos a marcação imposta da imagem e o mais próximo possível do SNA padrão, enquanto aplicamos uma estratégia pré-planejada.  Chamamos a arquitetura resultante BagNet-q, em que q denota o tamanho do campo de percepção da camada superior (testamos o modelo com q = 9, 17 e 33).  O BagNet-q roda cerca de 2,5 a mais que o ResNet-50. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b5e/4b6/1c4/b5e4b61c431e103ac9a6ec3fe4b02894.jpg"><br><br>  O desempenho da BagNet em dados do banco de dados ImageNet é impressionante, mesmo quando se usa pedaços pequenos: fragmentos de 17x17 pixels são suficientes para atingir a eficiência do nível AlexNet, e fragmentos de 33x33 pixels são suficientes para atingir 87% de precisão, entrando no top 5.  Você pode aumentar a eficiência colocando os pacotes 3x3 com mais cuidado e ajustando os hiperparâmetros. <br><br>  Este é o nosso primeiro grande resultado: o ImageNet pode ser resolvido usando apenas um conjunto de pequenos recursos de imagem.  As relações espaciais distantes das partes da composição, como a forma dos objetos ou a interação entre partes do objeto, podem ser completamente ignoradas;  eles não são absolutamente necessários para resolver o problema. <br><br>  Uma característica notável do BagNet'ov é a transparência do seu sistema de tomada de decisão.  Por exemplo, você pode descobrir quais recursos das imagens serão mais característicos para uma determinada classe.  Por exemplo, tenca, um peixe grande, é geralmente reconhecida pela imagem dos dedos em um fundo verde.  Porque  Porque na maioria das fotos desta categoria, há um pescador segurando uma tenca como seu troféu.  E quando o BagNet reconhece incorretamente a imagem como uma linha, isso geralmente acontece porque em algum lugar da foto há dedos em um fundo verde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/175/197/727/175197727cc15da939117f5c2502d82c.jpg"><br>  <i>As partes mais características das imagens.</i>  <i>A linha superior em cada célula corresponde ao reconhecimento correto e a parte inferior aos fragmentos que causam distração que levaram ao reconhecimento incorreto</i> <br><br>  Também obtemos o "mapa de calor" exato, que mostra quais partes da imagem contribuíram para a decisão. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89b/510/f5b/89b510f5b5d816a2a1d8590f4487abf6.jpg"><br>  <i>Os mapas de calor não são uma aproximação; eles mostram com precisão a contribuição de cada parte da imagem.</i> <br><br>  Os BagNet demonstram que você pode obter alta precisão com o ImageNet apenas com base em correlações estatísticas fracas entre os recursos locais das imagens e a categoria de objetos.  Se isso é suficiente, por que as redes neurais padrão ResNet-50 aprendem algo fundamentalmente diferente?  Por que o ResNet-50 deveria estudar relações complexas de larga escala, como a forma de um objeto, se a abundância de recursos locais da imagem é suficiente para resolver o problema? <br><br>  Para testar a hipótese de que os SNAs modernos aderem a uma estratégia semelhante à operação das redes BoW mais simples, testamos redes diferentes - ResNet, DenseNet e VGG nos seguintes "sinais" do BagNet: <br><ul><li>  As soluções são independentes do embaralhamento espacial dos recursos da imagem (isso só pode ser verificado nos modelos VGG). </li><li>  Modificações de diferentes partes da imagem não devem depender umas das outras (no sentido de sua influência na participação na classe). </li><li>  Os erros cometidos pelo SNA padrão e pelo BagNet'ami devem ser semelhantes. </li><li>  O SNS padrão e o BagNet devem ser sensíveis a recursos semelhantes. </li></ul><br><br>  Nas quatro experiências, descobrimos comportamentos surpreendentemente semelhantes do SNS e da BagNet.  Por exemplo, no último experimento, mostramos que o BagNet é mais sensível (se, por exemplo, eles se sobrepõem) aos mesmos lugares nas imagens que o SNA.  De fato, os mapas de calor BagNet (mapas de sensibilidade espacial) preveem melhor a sensibilidade do DenseNet-169 do que os mapas de calor obtidos por métodos de atribuição como o DeepLift (cálculo direto dos mapas de calor para o DenseNet-169).  Obviamente, o SNA não repete exatamente o comportamento do BagNet, mas certos desvios demonstram.  Em particular, quanto mais profundas as redes se tornam, maiores os tamanhos dos recursos e mais ampliadas as dependências.  Portanto, redes neurais profundas são de fato uma melhoria em relação aos modelos BagNet, mas não acho que a base de sua classificação esteja de alguma forma mudando. <br><br><h2>  Indo além da classificação BoW </h2><br>  Observar a tomada de decisão do SNA no estilo das estratégias de BoW pode explicar algumas das características estranhas do SNA.  Em primeiro lugar, isso explica por que o SNA está tão <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ligado às texturas</a> .  Em segundo lugar, por que o SNA não é sensível à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mistura de</a> partes da imagem.  Isso pode até explicar a existência de adesivos contraditórios e distúrbios contraditórios: sinais confusos podem ser colocados em qualquer lugar da imagem, e o SNS certamente captará esse sinal, independentemente de ele se encaixar no restante da imagem. <br><br>  De fato, nosso trabalho mostra que o SNA, ao reconhecer imagens, usa muitas leis estatísticas fracas e não procede à integração de partes da imagem no nível dos objetos, como as pessoas fazem.  O mesmo é provavelmente verdadeiro para outras tarefas e modalidades sensoriais. <br><br>  Precisamos planejar cuidadosamente nossas arquiteturas, tarefas e métodos de treinamento para superar a tendência de usar correlações estatísticas fracas.  Uma abordagem é traduzir a distorção do treinamento de SNA de pequenos recursos locais para outros mais globais.  Outra é remover ou substituir os recursos nos quais a rede neural não deve confiar, o que fizemos em outra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">publicação</a> para o ICLR 2019, usando o pré-processamento de transferência de estilo para eliminar a textura de um objeto natural. <br><br>  Um dos maiores problemas, no entanto, continua sendo a classificação das imagens: se as características locais são suficientes, não há incentivo para estudar a verdadeira "física" do mundo natural.  Precisamos reestruturar a tarefa de maneira a mover modelos para estudar a natureza física dos objetos.  Para fazer isso, muito provavelmente, você terá que ir além do ensino puramente observacional até as correlações dos dados de entrada e saída, para que os modelos possam extrair relacionamentos causais. <br><br>  Juntos, nossos resultados sugerem que o SNA pode seguir uma estratégia de classificação extremamente simples.  O fato de que tal descoberta possa ser feita em 2019 enfatiza o quão pouco ainda entendemos os recursos internos do trabalho de redes neurais profundas.  A falta de entendimento não nos permite desenvolver modelos e arquiteturas fundamentalmente aprimorados que preenchem a lacuna entre a percepção do homem e da máquina.  Aprofundar nossa compreensão nos permitirá descobrir maneiras de diminuir essa lacuna.  Isso pode ser extremamente útil: tentando mudar o SNA para as propriedades físicas dos objetos, de repente alcançamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resistência ao ruído no</a> nível humano.  Espero o aparecimento de um grande número de outros resultados interessantes em nosso caminho para o desenvolvimento do SNA, que realmente compreendem a natureza física e causal do nosso mundo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt443734/">https://habr.com/ru/post/pt443734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt443722/index.html">Design e nomeação de filas</a></li>
<li><a href="../pt443724/index.html">AMD Radeon VII: chip de última geração (parte 1)</a></li>
<li><a href="../pt443726/index.html">Recursos de configuração da Palo Alto Networks: SSL VPN</a></li>
<li><a href="../pt443728/index.html">O Google Plus para de funcionar. E daí?</a></li>
<li><a href="../pt443730/index.html">Ctrl-Alt-Del: obsolescência planejada de programadores</a></li>
<li><a href="../pt443736/index.html">Configuração das ferramentas de gerenciamento de rede (NUT) do zero para gerenciar um no-break conectado localmente</a></li>
<li><a href="../pt443738/index.html">Como conseguir um emprego na Alemanha para profissionais de TI</a></li>
<li><a href="../pt443740/index.html">Primeiro lançamento da ferramenta de teste de pesquisa de produto aberta</a></li>
<li><a href="../pt443744/index.html">Enciclopédia de Iluminação por Naughty Dog</a></li>
<li><a href="../pt443746/index.html">Mercado de jogos, tendências e previsões - ótimas análises da App Annie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>