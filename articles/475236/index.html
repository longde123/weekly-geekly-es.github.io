<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë¶üèø üßí üë®üèø‚Äçüî¨ Nunca ignores el entrenamiento de refuerzo de nuevo. ü¶Ö üñïüèª üôÖüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Les presento la traducci√≥n del art√≠culo "No vuelvas a ignorar el aprendizaje de refuerzo de nuevo" por Michel Kana, Ph.D. 

 Aprender con u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nunca ignores el entrenamiento de refuerzo de nuevo.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Hola Habr!  Les presento la traducci√≥n del art√≠culo "No vuelvas a ignorar el aprendizaje de refuerzo de nuevo" por Michel Kana, Ph.D. <br><br>  Aprender con un maestro y aprender sin un maestro no es todo.  Todo el mundo lo sabe.  Comience con OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="imagen"></div><br>  <i>¬øVas a derrotar al campe√≥n mundial de ajedrez, al backgammon o ir?</i> <br><br>  Hay una manera que te permitir√° hacer esto: entrenamiento de refuerzo. <br><a name="habracut"></a><br><h2>  ¬øQu√© es el aprendizaje por refuerzo? </h2><br>  El aprendizaje reforzado es aprender a tomar decisiones consistentes en un entorno con la m√°xima recompensa recibida por cada acci√≥n. <br><br>  No hay maestro en √©l, solo una se√±al de recompensa del medio ambiente.  El tiempo importa y las acciones afectan los datos posteriores.  Tales condiciones crean dificultades para aprender con o sin un maestro. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="imagen"></div><br>  En el siguiente ejemplo, el mouse intenta encontrar tanta comida como sea posible y evita las descargas el√©ctricas cuando es posible. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="imagen"></div><br>  Un rat√≥n puede ser valiente y puede recibir una descarga para llegar a un lugar con mucho queso.  Ser√° mejor que quedarse quieto y no recibir nada. <br><br>  El mouse no quiere tomar las mejores decisiones en cada situaci√≥n espec√≠fica.  Esto requerir√≠a un gran desembolso mental de ella, y no ser√≠a universal. <br><br>  El entrenamiento con refuerzos proporciona algunos conjuntos m√°gicos de m√©todos que permiten a nuestro mouse aprender a evitar descargas el√©ctricas y obtener la mayor cantidad de comida posible. <br><br>  El mouse es un agente.  Un laberinto con paredes, queso y pistolas paralizantes es <b>el medio ambiente</b> .  El mouse puede moverse hacia la izquierda, derecha, arriba, abajo: estas son <b>acciones</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="imagen"></div><br>  El rat√≥n quiere queso, no una descarga el√©ctrica.  El queso es una <b>recompensa</b> .  El mouse puede inspeccionar el medio ambiente; estas son <b>observaciones</b> . <br><br><h2>  Entrenamiento de refuerzo de hielo </h2><br>  Dejemos el mouse en el laberinto y pasemos al hielo.  ‚ÄúHa llegado el invierno.  T√∫ y tus amigos estaban tirando frisbee en el parque cuando de repente tiraste frisbee en medio del lago.  B√°sicamente, el agua en el lago estaba congelada, pero hab√≠a algunos agujeros donde el hielo se derret√≠a.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fuente</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="imagen"></div><br>  ‚ÄúSi pisas uno de los agujeros, caer√°s en agua helada.  Adem√°s, hay una gran escasez de frisbee en el mundo, por lo que es absolutamente esencial que rodees el lago y encuentres un disco ‚Äù. ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fuente</a> ) <br><br><h2>  ¬øC√≥mo te sientes en una situaci√≥n similar? </h2><br>  Este es un desaf√≠o para el aprendizaje por refuerzo.  El agente controla los movimientos del personaje en el mundo de la cuadr√≠cula.  Algunas fichas de cuadr√≠cula son pasables, mientras que otras hacen que el personaje caiga al agua.  El agente recibe una recompensa por encontrar un camino transitable hacia la meta. <br><br>  Podemos simular dicho entorno utilizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI Gym</a> , un juego de herramientas para desarrollar y comparar algoritmos de aprendizaje con refuerzos.  Proporciona acceso a un conjunto estandarizado de entornos, como en nuestro ejemplo, que se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Frozen Lake</a> .  Este es un medio de texto que se puede crear con un par de l√≠neas de c√≥digo. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Ahora necesitamos una estructura que nos permita abordar sistem√°ticamente los problemas de aprendizaje con refuerzo. <br><br><h2>  Proceso de toma de decisiones de Markov </h2><br>  En nuestro ejemplo, el agente controla el movimiento del personaje en el mundo de la cuadr√≠cula, y este entorno se denomina entorno totalmente observable. <br><br>  Dado que el mosaico futuro no depende de mosaicos pasados ‚Äã‚Äãteniendo en cuenta el mosaico actual <br>  (estamos tratando con una secuencia de estados aleatorios, es decir, con la <b>propiedad de Markov</b> ), por lo tanto, estamos tratando con el llamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">proceso de Markov</a> . <br><br>  El estado actual encapsula todo lo que se necesita para decidir cu√°l ser√° el pr√≥ximo movimiento; no se necesita nada para recordar. <br><br>  En cada celda siguiente (es decir, una situaci√≥n), el agente elige con cierta probabilidad la acci√≥n que conduce a la celda siguiente, es decir, la situaci√≥n y el entorno responde al agente con observaci√≥n y recompensa. <br><br>  Agregamos la funci√≥n de recompensa y el coeficiente de descuento al proceso de Markov, y obtenemos el llamado <i>proceso de recompensa de Markov</i> .  Al agregar un conjunto de acciones, obtenemos <i>el proceso de toma de decisiones de Markov</i> ( <b>MDP</b> ).  Los componentes de MDP se describen con m√°s detalle a continuaci√≥n. <br><br><h2>  Condici√≥n </h2><br>  Un estado es parte del entorno, una representaci√≥n num√©rica de lo que el agente observa en un determinado momento en el entorno, el estado de la red del lago.  S es el punto de partida, G es el objetivo, F es el hielo s√≥lido sobre el cual el agente puede pararse y H es el agujero en el que caer√° el agente si lo pisa.  Tenemos 16 estados en un entorno de cuadr√≠cula de 4 por 4, o 64 estados en un entorno de 8 por 8. A continuaci√≥n, dibujaremos un ejemplo de un entorno de 4 por 4 con OpenAI Gym. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Acciones </h2><br>  El agente tiene 4 acciones posibles, que se representan en el entorno como 0, 1, 2, 3 para izquierda, derecha, abajo, arriba, respectivamente. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="imagen"></div><br><h2>  Modelo de transici√≥n de estado </h2><br>  El modelo de transici√≥n de estado describe c√≥mo cambia el estado del entorno cuando un agente toma medidas en funci√≥n de su estado actual. <br><br>  El modelo generalmente se describe por la probabilidad de transici√≥n, que se expresa como una matriz de transici√≥n cuadrada de tama√±o N x N, donde N es el n√∫mero de estados de nuestro modelo.  La siguiente ilustraci√≥n es un ejemplo de dicha matriz para las condiciones clim√°ticas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="imagen"></div><br>  En el entorno del lago congelado, suponemos que el lago no es resbaladizo.  Si vamos a la derecha, entonces definitivamente vamos a la derecha.  Por lo tanto, todas las probabilidades son iguales. <br><br>  "Izquierda" mueve la celda del agente 1 a la izquierda o la deja en la misma posici√≥n si el agente est√° en el borde izquierdo. <br><br>  "Derecha" lo mueve 1 celda hacia la derecha o lo deja en la misma posici√≥n si el agente est√° en el borde derecho. <br><br>  ‚ÄúArriba‚Äù mueve la celda del agente 1 hacia arriba, o el agente permanece en el mismo lugar si est√° en el l√≠mite superior. <br><br>  ‚ÄúAbajo‚Äù mueve la celda del agente 1 hacia abajo, o permanece en el mismo lugar si est√° en el l√≠mite inferior. <br><br><h2>  Remuneraci√≥n </h2><br>  En cada estado F, el agente recibe 0 recompensas; en el estado H, recibe -1, ya que, al pasar a este estado, el agente muere.  Y cuando el agente alcanza la meta, recibe una recompensa +1. <br><br>  Debido al hecho de que ambos modelos, el modelo de transici√≥n y el modelo de recompensa, son funciones deterministas, esto hace que el entorno sea determinista.  \ <br><br><h2>  Descuento </h2><br>  El descuento es un par√°metro opcional que controla la importancia de futuras recompensas.  Se mide en el rango de 0 a 1. El prop√≥sito de este par√°metro es evitar que la recompensa total llegue al infinito. <br><br>  El descuento tambi√©n modela el comportamiento del agente cuando el agente prefiere la recompensa inmediata a la recompensa que se puede recibir en el futuro. <br><br><h2>  Valor </h2><br>  El valor de la fortuna es el ingreso esperado a largo plazo con un descuento por la fortuna. <br><br><h2>  Pol√≠tica (œÄ) </h2><br>  La estrategia que utiliza el agente para seleccionar la siguiente acci√≥n se denomina pol√≠tica.  Entre todas las pol√≠ticas disponibles, la √≥ptima es la que maximiza la cantidad de remuneraci√≥n recibida o esperada durante el episodio. <br><br><h2>  Episodio </h2><br>  El episodio comienza cuando el agente aparece en la celda inicial y termina cuando el agente cae en el agujero o llega a la celda objetivo. <br><br><h2>  Vamos a visualizarlo todo </h2><br>  Despu√©s de revisar todos los conceptos involucrados en el proceso de toma de decisiones de Markov, ahora podemos modelar varias acciones aleatorias en un entorno de 16x16 usando OpenAI Gym.  Cada vez, el agente selecciona una acci√≥n aleatoria y la realiza.  El sistema calcula la recompensa y muestra el nuevo estado del entorno. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="imagen"></div><br><h2>  Conclusi√≥n </h2><br>  En este art√≠culo, revisamos brevemente los conceptos b√°sicos del aprendizaje por refuerzo.  Nuestro ejemplo proporcion√≥ una introducci√≥n al kit de herramientas OpenAI Gym, que facilita la experimentaci√≥n con entornos predise√±ados. <br><br>  En la siguiente parte, presentaremos c√≥mo dise√±ar e implementar pol√≠ticas que le permitan al agente tomar una serie de acciones para lograr el objetivo y recibir un premio, como derrotar a un campe√≥n mundial. <br><br>  Gracias por su atencion </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/475236/">https://habr.com/ru/post/475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../475212/index.html">Busque datos y objetos en la base de datos de MS SQL Server con la utilidad gratuita dbForge Search</a></li>
<li><a href="../475214/index.html">Cuando una empresa muere: c√≥mo sobrevivir a la bancarrota</a></li>
<li><a href="../475218/index.html">Protocolos criptogr√°ficos: definiciones, registros, propiedades, clasificaci√≥n, ataques.</a></li>
<li><a href="../475226/index.html">Pasant√≠a en la Fundaci√≥n Haxe</a></li>
<li><a href="../475228/index.html">Tenedor de n√≥mina. Eres un programador para mam√°</a></li>
<li><a href="../475238/index.html">Cronolog√≠a de Blade Runner: noviembre de 2019. ¬øEl pron√≥stico se hizo realidad?</a></li>
<li><a href="../475240/index.html">Uso de m√≥dulos estrictos en proyectos de Python a gran escala: experiencia de Instagram. Parte 1</a></li>
<li><a href="../475242/index.html">Uso de m√≥dulos estrictos en proyectos de Python a gran escala: experiencia de Instagram. Parte 2</a></li>
<li><a href="../475244/index.html">Nuevas caracter√≠sticas de JavaScript esperadas que debe conocer</a></li>
<li><a href="../475246/index.html">Programaci√≥n asincr√≥nica de Python: una breve descripci√≥n</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>