<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ˜¼ â¤µï¸ ğŸ§šğŸ» Word2vec en images ğŸ’Œ ğŸš€ ğŸ‘¸ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Â« Tout recÃ¨le un motif qui fait partie de l'univers. Il a la symÃ©trie, l'Ã©lÃ©gance et la beautÃ© - des qualitÃ©s qui sont d'abord saisies par tout vÃ©rita...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec en images</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">Â« <b>Tout recÃ¨le un motif qui fait partie de l'univers.</b></font>  <font color="gray"><b>Il a la symÃ©trie, l'Ã©lÃ©gance et la beautÃ©</b> - des qualitÃ©s qui sont d'abord saisies par tout vÃ©ritable artiste qui capture le monde.</font>  <font color="gray">Ce motif peut Ãªtre pris dans le changement de saison, dans la faÃ§on dont le sable s'Ã©coule le long de la pente, dans les branches emmÃªlÃ©es d'un arbuste crÃ©osote, dans le motif de sa feuille.</font> <font color="gray"><br><br></font>  <font color="gray">Nous essayons de copier ce modÃ¨le dans notre vie et notre sociÃ©tÃ© et donc nous aimons le rythme, le chant, la danse, diverses formes qui nous rendent heureux et nous rÃ©confortent.</font>  <font color="gray">Cependant, on peut Ã©galement discerner le danger qui se cache dans la recherche de la perfection absolue, car il est Ã©vident que le modÃ¨le parfait est inchangÃ©.</font>  <font color="gray">Et Ã  l'approche de la perfection, tout va Ã  la mort Â»- <i>Dune</i> (1965)</font> </blockquote><br>  Je pense que le concept d'intÃ©gration est l'une des idÃ©es les plus remarquables de l'apprentissage automatique.  Si vous avez dÃ©jÃ  utilisÃ© Siri, Google Assistant, Alexa, Google Translate ou mÃªme un clavier de smartphone avec la prÃ©diction du mot suivant, vous avez dÃ©jÃ  travaillÃ© avec le modÃ¨le de traitement du langage naturel basÃ© sur les piÃ¨ces jointes.  Au cours des derniÃ¨res dÃ©cennies, ce concept a subi un dÃ©veloppement important pour les modÃ¨les neuronaux (les dÃ©veloppements rÃ©cents incluent des intÃ©grations de mots contextualisÃ©es dans des modÃ¨les avancÃ©s tels que BERT et GPT2). <br><a name="habracut"></a><br>  Word2vec est une mÃ©thode de crÃ©ation d'investissement efficace dÃ©veloppÃ©e en 2013.  En plus de travailler avec des mots, certains de ses concepts ont Ã©tÃ© efficaces pour dÃ©velopper des mÃ©canismes de recommandation et donner du sens aux donnÃ©es mÃªme dans des tÃ¢ches commerciales non linguistiques.  Cette technologie a Ã©tÃ© utilisÃ©e par des sociÃ©tÃ©s comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Spotify</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Anghami</a> dans leurs moteurs de recommandation. <br><br>  Dans cet article, nous examinerons le concept et la mÃ©canique de gÃ©nÃ©ration de piÃ¨ces jointes Ã  l'aide de word2vec.  CommenÃ§ons par un exemple pour vous familiariser avec la faÃ§on de reprÃ©senter des objets sous forme vectorielle.  Savez-vous combien une liste de cinq nombres (vectoriels) peut dire de votre personnalitÃ©? <br><br><h1>  Personnalisation: qu'Ãªtes-vous? </h1><br><blockquote>  <font color="gray">Â«Je vous donne le camÃ©lÃ©on du dÃ©sert;</font>  <font color="gray">sa capacitÃ© Ã  fusionner avec le sable vous dira tout ce que vous devez savoir sur les racines de l'Ã©cologie et les raisons de prÃ©server votre personnalitÃ©. Â»</font>  <font color="gray">- <i>Enfants de la Dune</i></font> </blockquote><br>  Sur une Ã©chelle de 0 Ã  100, avez-vous un type de personnalitÃ© introverti ou extraverti (oÃ¹ 0 est le type le plus introverti et 100 est le type le plus extraverti)?  Avez-vous dÃ©jÃ  passÃ© un test de personnalitÃ©: par exemple, MBTI, ou mieux encore <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">, les Big Five</a> ?  On vous donne une liste de questions puis vous Ã©valuez sur plusieurs axes, dont l'introversion / extroversion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Exemple des rÃ©sultats du test Big Five.</font></i>  <i><font color="gray">Il en dit long sur la personnalitÃ© et est capable de prÃ©dire la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">rÃ©ussite scolaire</a> , <a href="">personnelle</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">professionnelle</a> .</font></i>  <i><font color="gray">Par exemple, vous pouvez le parcourir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .</font></i> <br><br>  Supposons que j'obtienne un score de 38 sur 100 pour l'Ã©valuation de l'introversion / extraversion.  Cela peut Ãªtre reprÃ©sentÃ© comme suit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Ou sur une Ã©chelle de -1 Ã  +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Dans quelle mesure reconnaissons-nous une personne uniquement Ã  partir de cette Ã©valuation?  Pas vraiment.  Les humains sont des crÃ©atures complexes.  Par consÃ©quent, nous ajoutons une dimension de plus: une caractÃ©ristique de plus du test. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Vous pouvez imaginer ces deux dimensions comme un point sur le graphique, ou, mieux encore, comme un vecteur de l'origine Ã  ce point.</font></i>  <i><font color="gray">Il existe d'excellents outils vectoriels qui seront trÃ¨s utiles trÃ¨s bientÃ´t.</font></i> <br><br>  Je ne montre pas quels traits de personnalitÃ© nous mettons sur le graphique afin que vous ne vous attachiez pas Ã  des traits spÃ©cifiques, mais comprenez immÃ©diatement la reprÃ©sentation vectorielle de la personnalitÃ© d'une personne dans son ensemble. <br><br>  Maintenant, nous pouvons dire que ce vecteur reflÃ¨te partiellement ma personnalitÃ©.  Ceci est une description utile lors de la comparaison de diffÃ©rentes personnes.  Supposons que j'ai Ã©tÃ© frappÃ© par un bus rouge et que vous deviez me remplacer par une personne similaire.  Laquelle des deux personnes du tableau suivant me ressemble le plus? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Lorsque vous travaillez avec des vecteurs, la similitude est gÃ©nÃ©ralement calculÃ©e par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le coefficient Otiai</a> (coefficient gÃ©omÃ©trique): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">La personne n Â° 1</font> <font color="gray">me ressemble davantage.</font></i>  <i><font color="gray">Les vecteurs dans une direction (la longueur est Ã©galement importante) donnent un coefficient Otiai plus grand</font></i> <br><br>  Encore une fois, deux dimensions ne suffisent pas pour Ã©valuer les gens.  Des dÃ©cennies de dÃ©veloppement de la science psychologique ont conduit Ã  la crÃ©ation d'un test pour cinq caractÃ©ristiques de base de la personnalitÃ© (avec de nombreuses autres).  Alors, utilisons les cinq dimensions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  Le problÃ¨me avec les cinq dimensions est qu'il ne sera plus possible de dessiner des flÃ¨ches soignÃ©es en 2D.  Il s'agit d'un problÃ¨me courant dans l'apprentissage automatique, oÃ¹ vous devez souvent travailler dans un espace multidimensionnel.  Il est bon que le coefficient gÃ©omÃ©trique fonctionne avec n'importe quel nombre de mesures: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">Le coefficient gÃ©omÃ©trique fonctionne pour n'importe quel nombre de mesures.</font></i>  <i><font color="gray">En cinq dimensions, le rÃ©sultat est beaucoup plus prÃ©cis.</font></i> <br><br>  Ã€ la fin de ce chapitre, je veux rÃ©pÃ©ter deux idÃ©es principales: <br><br><ol><li>  Les gens (et d'autres objets) peuvent Ãªtre reprÃ©sentÃ©s comme des vecteurs numÃ©riques (ce qui est idÃ©al pour les voitures!). <br></li><li>  Nous pouvons facilement calculer la similitude des vecteurs. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Incorporation de mots </h1><br><blockquote>  <font color="gray">"Le don des mots est le don de la tromperie et de l'illusion."</font>  <font color="gray">- <i>Enfants de la Dune</i></font> </blockquote><br>  Avec cette comprÃ©hension, nous allons passer aux reprÃ©sentations vectorielles des mots obtenus Ã  la suite de la formation (ils sont aussi appelÃ©s attachements) et examiner leurs propriÃ©tÃ©s intÃ©ressantes. <br><br>  Voici la piÃ¨ce jointe pour le mot Â«roiÂ» (vecteur GloVe, formÃ© sur WikipÃ©dia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Nous voyons une liste de 50 numÃ©ros, mais il est difficile de dire quelque chose.  Visualisons-les pour comparer avec d'autres vecteurs.  Mettez les chiffres sur une ligne: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Coloriez les cellules par leurs valeurs (rouge pour prÃ¨s de 2, blanc pour prÃ¨s de 0, bleu pour prÃ¨s de -2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Maintenant, oubliez les chiffres, et seulement par les couleurs, nous contrastons le Â«roiÂ» avec d'autres mots: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Vous voyez que Â«l'hommeÂ» et la Â«femmeÂ» sont beaucoup plus proches l'un de l'autre que du Â«roiÂ»?  Ã‡a dit quelque chose.  Les reprÃ©sentations vectorielles capturent beaucoup d'informations / sens / associations de ces mots. <br><br>  Voici une autre liste d'exemples (comparer des colonnes avec des couleurs similaires): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Il y a plusieurs choses Ã  noter: <br><br><ol><li>  Ã€ travers tous les mots passe une colonne rouge.  Autrement dit, ces mots sont similaires dans cette dimension particuliÃ¨re (et nous ne savons pas ce qui y est codÃ©). <br></li><li>  Vous pouvez voir que Â«femmeÂ» et Â«filleÂ» sont trÃ¨s similaires.  La mÃªme chose avec Â«hommeÂ» et Â«garÃ§onÂ». <br></li><li>  Â«GarÃ§onÂ» et Â«filleÂ» sont Ã©galement similaires dans certaines dimensions, mais diffÃ¨rent de Â«femmeÂ» et Â«hommeÂ».  Serait-ce une vague idÃ©e codÃ©e de la jeunesse?  Probablement. <br></li><li>  Tout sauf le dernier mot, ce sont les idÃ©es des gens.  J'ai ajoutÃ© un objet (eau) pour montrer les diffÃ©rences entre les catÃ©gories.  Par exemple, vous pouvez voir comment la colonne bleue descend et s'arrÃªte devant le vecteur d'eau. <br></li><li>  Il y a des dimensions claires oÃ¹ le Â«roiÂ» et la Â«reineÂ» sont similaires et diffÃ©rents les uns des autres.  Peut-Ãªtre qu'un vague concept de redevance y est codÃ©? </li></ol><br><h1>  Analogies </h1><br><blockquote>  <font color="gray">Â«Les mots supportent toutes les charges que nous souhaitons.</font>  <font color="gray">Tout ce qu'il faut, c'est un accord sur la tradition, selon lequel nous construisons des concepts. Â»</font>  <font color="gray">- <i>Dieu l'empereur de Dune</i></font> </blockquote><br>  Des exemples cÃ©lÃ¨bres qui montrent les propriÃ©tÃ©s incroyables des investissements sont le concept d'analogies.  Nous pouvons ajouter et soustraire des vecteurs de mots, obtenant des rÃ©sultats intÃ©ressants.  L'exemple le plus cÃ©lÃ¨bre est la formule Â«roi - homme + femmeÂ»: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">En utilisant la bibliothÃ¨que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gensim</a> en python, nous pouvons ajouter et soustraire des vecteurs de mots, et la bibliothÃ¨que trouvera les mots les plus proches du vecteur rÃ©sultant.</font></i>  <i><font color="gray">L'image montre une liste des mots les plus similaires, chacun avec un coefficient de similitude gÃ©omÃ©trique</font></i> <br><br>  Nous visualisons cette analogie comme prÃ©cÃ©demment: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">Le vecteur rÃ©sultant du calcul Â«roi - homme + femmeÂ» n'est pas tout Ã  fait Ã©gal Ã  la Â«reineÂ», mais c'est le rÃ©sultat le plus proche de 400 000 piÃ¨ces jointes de mots dans l'ensemble de donnÃ©es</font></i> <br><br>  AprÃ¨s avoir considÃ©rÃ© l'attachement des mots, apprenons comment se dÃ©roule l'apprentissage.  Mais avant de passer Ã  word2vec, vous devez jeter un Å“il Ã  l'ancÃªtre conceptuel de l'intÃ©gration de mots: un modÃ¨le de langage neuronal. <br><br><h1>  ModÃ¨le de langage </h1><br><blockquote>  <font color="gray">Â«Le prophÃ¨te n'est pas soumis aux illusions du passÃ©, du prÃ©sent ou du futur.</font>  <font color="gray"><b>La fixitÃ© des formes linguistiques dÃ©termine de telles diffÃ©rences linÃ©aires.</b></font>  <font color="gray">Les prophÃ¨tes tiennent la clÃ© du verrou de la langue.</font>  <font color="gray">Pour eux, l'image physique ne reste qu'une image physique et rien de plus.</font> <font color="gray"><br><br></font>  <font color="gray">Leur univers n'a pas les propriÃ©tÃ©s d'un univers mÃ©canique.</font>  <font color="gray">L'observateur assume une sÃ©quence linÃ©aire d'Ã©vÃ©nements.</font>  <font color="gray">Cause et effet?</font>  <font color="gray">C'est une question complÃ¨tement diffÃ©rente.</font>  <font color="gray">Le ProphÃ¨te prononce des paroles fatidiques.</font>  <font color="gray">Vous voyez un aperÃ§u d'un Ã©vÃ©nement qui devrait se produire Â«selon la logique des chosesÂ».</font>  <font color="gray">Mais le prophÃ¨te libÃ¨re instantanÃ©ment l'Ã©nergie d'un pouvoir miraculeux infini.</font>  <font color="gray">L'univers subit un changement spirituel. Â»</font>  <font color="gray">- <i>Dieu l'empereur de Dune</i></font> </blockquote><br>  Un exemple de NLP (Natural Language Processing) est la fonction de prÃ©diction du mot suivant sur le clavier d'un smartphone.  Des milliards de personnes l'utilisent des centaines de fois par jour. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  La prÃ©diction du mot suivant est une tÃ¢che appropriÃ©e pour <i>un modÃ¨le de langage</i> .  Elle peut prendre une liste de mots (disons deux mots) et essayer de prÃ©dire ce qui suit. <br><br>  Dans la capture d'Ã©cran ci-dessus, le modÃ¨le a pris ces deux mots verts ( <code>thou shalt</code> ) et a renvoyÃ© une liste d'options (trÃ¨s probablement pour le mot <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  On peut imaginer le modÃ¨le comme une boÃ®te noire: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Mais en pratique, le modÃ¨le produit plus d'un mot.  Il dÃ©rive une estimation de la probabilitÃ© de pratiquement tous les mots connus (le "dictionnaire" du modÃ¨le varie de plusieurs milliers Ã  plus d'un million de mots).  L'application clavier trouve ensuite les mots ayant les scores les plus Ã©levÃ©s et les montre Ã  l'utilisateur. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Un modÃ¨le de langage neuronal donne la probabilitÃ© de tous les mots connus.</font></i>  <i><font color="gray">Nous indiquons la probabilitÃ© en pourcentage, mais dans le vecteur rÃ©sultant, 40% seront reprÃ©sentÃ©s comme 0,4</font></i> <br><br>  AprÃ¨s l'entraÃ®nement, les premiers modÃ¨les neuronaux ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bengio 2003</a> ) ont calculÃ© le pronostic en trois Ã©tapes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  La premiÃ¨re Ã©tape pour nous est la plus pertinente, alors que nous discutons des investissements.  Ã€ la suite de la formation, une matrice est crÃ©Ã©e avec les piÃ¨ces jointes de tous les mots de notre dictionnaire.  Pour obtenir le rÃ©sultat, nous recherchons simplement les plongements des mots d'entrÃ©e et exÃ©cutons la prÃ©diction: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Examinons maintenant le processus d'apprentissage et dÃ©couvrons comment cette matrice d'investissements est crÃ©Ã©e. <br><br><h1>  Formation sur le modÃ¨le linguistique </h1><br><blockquote>  <font color="gray">Â«Le processus ne peut pas Ãªtre compris en y mettant fin.</font>  <font color="gray">La comprÃ©hension doit avancer avec le processus, fusionner avec son flux et couler avec lui Â»- <i>Dune</i></font> </blockquote><br>  Les modÃ¨les de langage ont un Ã©norme avantage sur la plupart des autres modÃ¨les d'apprentissage automatique: ils peuvent Ãªtre formÃ©s sur des textes que nous avons en abondance.  Pensez Ã  tous les livres, articles, documents WikipÃ©dia et autres formes de donnÃ©es textuelles dont nous disposons.  Comparez avec d'autres modÃ¨les d'apprentissage automatique qui nÃ©cessitent un travail manuel et des donnÃ©es spÃ©cialement collectÃ©es. <br><br><blockquote>  <b>Â«Vous devez apprendre le mot par son entrepriseÂ» - J. R. Furs</b> </blockquote><br>  Les piÃ¨ces jointes des mots sont calculÃ©es en fonction des mots environnants, qui apparaissent le plus souvent Ã  proximitÃ©.  La mÃ©canique est la suivante: <br><br><ol><li>  Nous obtenons beaucoup de donnÃ©es textuelles (disons, tous les articles Wikipedia) <br></li><li>  DÃ©finissez une fenÃªtre (par exemple, de trois mots) qui glisse dans le texte. <br></li><li>  Une fenÃªtre coulissante gÃ©nÃ¨re des modÃ¨les pour l'apprentissage de notre modÃ¨le. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Lorsque cette fenÃªtre glisse sur le texte, nous gÃ©nÃ©rons (en fait) un ensemble de donnÃ©es, que nous utilisons ensuite pour former le modÃ¨le.  Pour comprendre, voyons comment une fenÃªtre coulissante gÃ¨re cette phrase: <br><br><blockquote>  <b>Â«Puissiez-vous ne pas construire une machine dotÃ©e de la ressemblance de l'esprit humainÂ» - <i>Dune</i></b> </blockquote><br>  Lorsque nous commenÃ§ons, la fenÃªtre se trouve sur les trois premiers mots de la phrase: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Nous prenons les deux premiers mots pour les signes, et le troisiÃ¨me mot pour l'Ã©tiquette: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Nous avons gÃ©nÃ©rÃ© le premier Ã©chantillon dans un ensemble de donnÃ©es qui peut ensuite Ãªtre utilisÃ© pour enseigner un modÃ¨le de langage</font></i> <br><br>  Ensuite, nous dÃ©plaÃ§ons la fenÃªtre Ã  la position suivante et crÃ©ons un deuxiÃ¨me Ã©chantillon: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Et trÃ¨s bientÃ´t, nous accumulons un plus grand ensemble de donnÃ©es: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  En pratique, les modÃ¨les sont gÃ©nÃ©ralement formÃ©s directement au processus de dÃ©placement d'une fenÃªtre coulissante.  Mais logiquement, la phase de Â«gÃ©nÃ©ration des ensembles de donnÃ©esÂ» est distincte de la phase de formation.  En plus des approches de rÃ©seau de neurones, la mÃ©thode N-gram Ã©tait souvent utilisÃ©e plus tÃ´t pour enseigner les modÃ¨les de langage (voir le troisiÃ¨me chapitre du livre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Speech and Language ProcessingÂ»</a> ).  Pour voir la diffÃ©rence lors du passage de N-grammes Ã  des modÃ¨les neuronaux dans des produits rÃ©els, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">voici un article de 2015 sur le blog Swiftkey</a> , le dÃ©veloppeur de mon clavier Android prÃ©fÃ©rÃ©, qui prÃ©sente son modÃ¨le de langage neuronal et le compare avec le modÃ¨le N-gram prÃ©cÃ©dent.  J'aime cet exemple car il montre comment les propriÃ©tÃ©s algorithmiques des investissements peuvent Ãªtre dÃ©crites dans un langage marketing. <br><br><h1>  Nous regardons dans les deux sens </h1><br><blockquote>  <font color="gray">Â«Un paradoxe est un signe que nous devons essayer de rÃ©flÃ©chir Ã  ce qui se cache derriÃ¨re.</font>  <font color="gray">Si le paradoxe vous inquiÃ¨te, cela signifie que vous vous efforcez de l'absolu.</font>  <font color="gray">Les relativistes considÃ¨rent le paradoxe simplement comme une pensÃ©e intÃ©ressante, peut-Ãªtre drÃ´le, parfois effrayante, mais une pensÃ©e trÃ¨s instructive. Â»</font>  <font color="gray"><i>Dieu empereur de Dune</i></font> </blockquote><br>  Sur la base de ce qui prÃ©cÃ¨de, comblez le vide: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Comme contexte, il y a cinq mots prÃ©cÃ©dents (et une rÃ©fÃ©rence antÃ©rieure Ã  Â«busÂ»).  Je suis sÃ»r que la plupart d'entre vous ont devinÃ© qu'il devrait y avoir un "bus".  Mais si je vous donne un autre mot aprÃ¨s l'espace, cela changera-t-il votre rÃ©ponse? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Cela change complÃ¨tement la situation: maintenant le mot manquant est trÃ¨s probablement Â«rougeÂ».  De toute Ã©vidence, les mots ont une valeur informative Ã  la fois avant et aprÃ¨s un espace.  Il s'avÃ¨re que la comptabilitÃ© dans les deux sens (gauche et droite) permet de calculer de meilleurs investissements.  Voyons comment configurer la formation du modÃ¨le dans une telle situation. <br><br><h1>  Sauter le gramme </h1><br><blockquote>  <font color="gray">"Quand un choix absolument indubitable est inconnu, l'intellect a la chance de travailler avec des donnÃ©es limitÃ©es dans l'arÃ¨ne, oÃ¹ les erreurs sont non seulement possibles mais aussi nÃ©cessaires."</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  En plus de deux mots avant la cible, vous pouvez prendre en compte deux autres mots aprÃ¨s celle-ci. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Ensuite, l'ensemble de donnÃ©es pour la formation du modÃ¨le ressemblera Ã  ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Il s'agit de l'architecture CBOW (Continuous Bag of Words) et est dÃ©crite dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un des documents word2vec</a> [pdf].  Il existe une autre architecture, qui montre Ã©galement d'excellents rÃ©sultats, mais est arrangÃ©e un peu diffÃ©remment: elle essaie de deviner les mots voisins par le mot courant.  Une fenÃªtre coulissante ressemble Ã  ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">Dans la fente verte est le mot d'entrÃ©e, et chaque champ rose reprÃ©sente une sortie possible</font></i> <br><br>  Les rectangles roses ont des nuances diffÃ©rentes car cette fenÃªtre coulissante crÃ©e en fait quatre modÃ¨les distincts dans notre jeu de donnÃ©es d'entraÃ®nement: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Cette mÃ©thode est appelÃ©e architecture <b>skip-gram</b> .  Vous pouvez visualiser une fenÃªtre coulissante comme suit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Les quatre exemples suivants sont ajoutÃ©s Ã  l'ensemble de donnÃ©es de formation: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Ensuite, nous dÃ©plaÃ§ons la fenÃªtre Ã  la position suivante: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Ce qui gÃ©nÃ¨re quatre autres exemples: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  BientÃ´t, nous aurons beaucoup plus d'Ã©chantillons: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revue d'apprentissage </h1><br><blockquote>  <font color="gray">Â«Muad'Dib Ã©tait un apprenant rapide parce qu'il apprenait principalement Ã  apprendre.</font>  <font color="gray">Mais la toute premiÃ¨re leÃ§on a Ã©tÃ© l'assimilation de la croyance qu'il peut apprendre, et c'est la base de tout.</font>  <font color="gray">C'est incroyable combien de personnes ne croient pas qu'elles peuvent apprendre et apprendre, et combien de personnes pensent que l'apprentissage est trÃ¨s difficile. "</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Maintenant que nous avons l'ensemble skip-gram, nous l'utilisons pour former le modÃ¨le neuronal de base du langage qui prÃ©dit un mot voisin. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  CommenÃ§ons par le premier Ã©chantillon de notre ensemble de donnÃ©es.  Nous prenons le signe et l'envoyons au modÃ¨le non formÃ© avec la demande de prÃ©dire le mot suivant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  Le modÃ¨le passe par trois Ã©tapes et affiche un vecteur de prÃ©diction (avec probabilitÃ© pour chaque mot du dictionnaire).  Le modÃ¨le n'Ã©tant pas entraÃ®nÃ©, Ã  ce stade, ses prÃ©visions sont probablement incorrectes.  Mais ce nâ€™est rien.  Nous savons quel mot elle prÃ©dit - c'est la cellule rÃ©sultante dans la ligne que nous utilisons actuellement pour former le modÃ¨le: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Un Â«vecteur cibleÂ» est un vecteur dans lequel le mot cible a une probabilitÃ© de 1, et tous les autres mots ont une probabilitÃ© de 0</font></i> <br><br>  Ã€ quel point le modÃ¨le Ã©tait-il mauvais?  Soustrayez le vecteur de prÃ©vision de la cible et obtenez le vecteur d'erreur: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Ce vecteur d'erreur peut maintenant Ãªtre utilisÃ© pour mettre Ã  jour le modÃ¨le, donc la prochaine fois, il est plus susceptible de donner un rÃ©sultat prÃ©cis sur les mÃªmes donnÃ©es d'entrÃ©e. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Ici se termine la premiÃ¨re Ã©tape de la formation.  Nous continuons Ã  faire de mÃªme avec l'Ã©chantillon suivant dans l'ensemble de donnÃ©es, puis avec le suivant, jusqu'Ã  ce que nous examinions tous les Ã©chantillons.  C'est la fin de la premiÃ¨re Ã¨re d'apprentissage.  Nous rÃ©pÃ©tons tout et maintes et maintes fois pendant plusieurs Ã©poques, et en consÃ©quence, nous obtenons un modÃ¨le formÃ©: vous pouvez en extraire la matrice d'investissement et l'utiliser dans toutes les applications. <br><br>  Bien que nous ayons beaucoup appris, mais pour bien comprendre comment word2vec apprend vraiment, quelques idÃ©es clÃ©s manquent. <br><br><h1>  SÃ©lection nÃ©gative </h1><br><blockquote>  <font color="gray">Â«Essayer de comprendre Muad'Dib sans comprendre ses ennemis mortels - les Harkonnenov - revient Ã  essayer de comprendre la VÃ©ritÃ© sans comprendre ce qu'est le mensonge.</font>  <font color="gray">C'est une tentative de connaÃ®tre la LumiÃ¨re sans connaÃ®tre l'ObscuritÃ©.</font>  <font color="gray">C'est impossible. "</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Rappelez-vous les trois Ã©tapes comment un modÃ¨le neuronal calcule une prÃ©vision: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  La troisiÃ¨me Ã©tape est trÃ¨s coÃ»teuse d'un point de vue informatique, surtout si vous le faites pour chaque Ã©chantillon de l'ensemble de donnÃ©es (des dizaines de millions de fois).  Il est nÃ©cessaire d'augmenter en quelque sorte la productivitÃ©. <br><br>  Une faÃ§on consiste Ã  diviser l'objectif en deux Ã©tapes: <br><br><ol><li>  CrÃ©ez des piÃ¨ces jointes de mots de haute qualitÃ© (sans prÃ©dire le mot suivant). <br></li><li>  Utilisez ces investissements de haute qualitÃ© pour enseigner le modÃ¨le linguistique (pour les prÃ©visions). </li></ol><br>  Cet article se concentrera sur la premiÃ¨re Ã©tape.  Pour augmenter la productivitÃ©, vous pouvez vous Ã©loigner de la prÃ©diction d'un mot voisin ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... et passez Ã  un modÃ¨le qui prend les mots d'entrÃ©e et de sortie et calcule la probabilitÃ© de leur proximitÃ© (de 0 Ã  1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Une transition aussi simple remplace le rÃ©seau neuronal par un modÃ¨le de rÃ©gression logistique - ainsi, les calculs deviennent beaucoup plus simples et plus rapides. <br><br>  En mÃªme temps, nous devons affiner la structure de notre ensemble de donnÃ©es: l'Ã©tiquette est maintenant une nouvelle colonne avec des valeurs 0 ou 1. Dans notre tableau, les unitÃ©s sont partout, car nous y avons ajoutÃ© des voisins. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Un tel modÃ¨le est calculÃ© Ã  une vitesse incroyable: des millions d'Ã©chantillons en quelques minutes.  Mais vous devez combler une lacune.  Si tous nos exemples sont positifs (objectif: 1), alors un modÃ¨le dÃ©licat peut se former qui renvoie toujours 1, dÃ©montrant une prÃ©cision de 100%, mais il n'apprend rien et gÃ©nÃ¨re des investissements indÃ©sirables. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Pour rÃ©soudre ce problÃ¨me, vous devez entrer <i>des modÃ¨les nÃ©gatifs</i> dans l'ensemble de donnÃ©es - des mots qui ne sont certainement pas voisins.  Pour eux, le modÃ¨le doit retourner 0. Maintenant, le modÃ¨le devra travailler dur, mais les calculs vont toujours Ã  grande vitesse. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Pour chaque Ã©chantillon du jeu de donnÃ©es, ajoutez des exemples nÃ©gatifs Ã©tiquetÃ©s 0</font></i> <br><br>  Mais que prÃ©senter comme mots de sortie?  Choisissez les mots arbitrairement: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Cette idÃ©e est nÃ©e sous l'influence de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la</a> mÃ©thode de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">comparaison</a> du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bruit</a> [pdf].  Nous faisons correspondre le signal rÃ©el (exemples positifs de mots voisins) avec du bruit (mots sÃ©lectionnÃ©s au hasard qui ne sont pas voisins).  Cela fournit un excellent compromis entre performances et performances statistiques. <br><br><h1>  Ã‰chantillon nÃ©gatif de saut de gramme (SGNS) </h1><br>  Nous avons examinÃ© deux concepts centraux de word2vec: ensemble, ils sont appelÃ©s Â«skip-gram avec Ã©chantillonnage nÃ©gatifÂ». <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Learning word2vec </h1><br><blockquote>  <font color="gray">Â«Une machine ne peut pas prÃ©voir tous les problÃ¨mes importants pour une personne vivante.</font>  <font color="gray">Il y a une grande diffÃ©rence entre un espace discret et un continuum continu.</font>  <font color="gray">Nous vivons dans un espace et les machines existent dans un autre. Â»</font>  <font color="gray">- <i>Dieu l'empereur de Dune</i></font> </blockquote><br>  AprÃ¨s avoir examinÃ© les idÃ©es de base du saut de gramme et de l'Ã©chantillonnage nÃ©gatif, nous pouvons procÃ©der Ã  un examen plus approfondi du processus d'apprentissage word2vec. <br><br>  Tout d'abord, nous prÃ©-traitons le texte sur lequel nous formons le modÃ¨le.  DÃ©finissez la taille du dictionnaire (nous l'appellerons <code>vocab_size</code> ), disons, dans 10 000 piÃ¨ces jointes et les paramÃ¨tres des mots dans le dictionnaire. <br><br>  Au dÃ©but de la formation, nous crÃ©ons deux matrices: <code>Embedding</code> et <code>Context</code> .  Les piÃ¨ces jointes de chaque mot sont stockÃ©es dans ces matrices dans notre dictionnaire (donc <code>vocab_size</code> est l'un de leurs paramÃ¨tres).  Le deuxiÃ¨me paramÃ¨tre est la dimension de la piÃ¨ce jointe (gÃ©nÃ©ralement <code>embedding_size</code> dÃ©fini sur 300, mais nous avons examinÃ© prÃ©cÃ©demment un exemple avec 50 dimensions). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Tout d'abord, nous initialisons ces matrices avec des valeurs alÃ©atoires.  Ensuite, nous commenÃ§ons le processus d'apprentissage.  Ã€ chaque Ã©tape, nous prenons un exemple positif et les nÃ©gatifs qui lui sont associÃ©s.  Voici notre premier groupe: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Nous avons maintenant quatre mots: le mot d'entrÃ©e <code>not</code> et les mots de sortie / contextuels <code>thou</code> (voisin rÃ©el), <code>aaron</code> et <code>taco</code> (exemples nÃ©gatifs).  Nous commenÃ§ons la recherche de leurs piÃ¨ces jointes dans les matrices <code>Embedding</code> (pour le mot d'entrÃ©e) et <code>Context</code> (pour les mots de contexte), bien que les deux matrices contiennent des piÃ¨ces jointes pour tous les mots de notre dictionnaire. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Ensuite, nous calculons le produit scalaire de la piÃ¨ce jointe d'entrÃ©e avec chacune des piÃ¨ces jointes contextuelles.  Dans chaque cas, un nombre est obtenu qui indique la similitude des donnÃ©es d'entrÃ©e et des piÃ¨ces jointes contextuelles. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Il nous faut maintenant un moyen de transformer ces estimations en une sorte de vraisemblance: toutes doivent Ãªtre des nombres positifs entre 0 et 1. C'est une excellente tÃ¢che pour les Ã©quations logistiques <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sigmoÃ¯des</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  Le rÃ©sultat du calcul sigmoÃ¯de peut Ãªtre considÃ©rÃ© comme la sortie du modÃ¨le pour ces Ã©chantillons.  Comme vous pouvez le voir, <code>taco</code> le score le plus Ã©levÃ©, tandis que <code>aaron</code> toujours le score le plus bas, avant et aprÃ¨s sigmoÃ¯de. <br><br>  Lorsque le modÃ¨le non formÃ© a fait une prÃ©vision et a un vÃ©ritable objectif cible pour la comparaison, calculons le nombre d'erreurs dans la prÃ©vision du modÃ¨le.  Pour ce faire, il suffit de soustraire le score sigmoÃ¯de des Ã©tiquettes cibles. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  C'est lÃ  que commence la phase Â«d'apprentissageÂ» du terme Â«apprentissage automatiqueÂ».  Nous pouvons maintenant utiliser cette estimation d'erreur pour ajuster les investissements <code>not</code> , <code>thou</code> , <code>aaron</code> et <code>taco</code> , afin que la prochaine fois, le rÃ©sultat soit plus proche des estimations cibles. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Ceci termine une Ã©tape de la formation.  Nous avons un peu amÃ©liorÃ© l'attachement de quelques mots ( <code>not</code> , <code>thou</code> , <code>aaron</code> et <code>taco</code> ).  Nous passons maintenant Ã  l'Ã©tape suivante (le prochain Ã©chantillon positif et les Ã©chantillons nÃ©gatifs qui lui sont associÃ©s) et rÃ©pÃ©tons le processus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Les piÃ¨ces jointes continuent de s'amÃ©liorer Ã  mesure que nous parcourons plusieurs fois l'ensemble des donnÃ©es.  Vous pouvez ensuite arrÃªter le processus, mettre de cÃ´tÃ© la matrice de <code>Context</code> et utiliser la matrice d' <code>Embeddings</code> intÃ©grÃ©e pour la tÃ¢che suivante. <br><br><h1>  Taille de la fenÃªtre et nombre d'Ã©chantillons nÃ©gatifs </h1><br>  Dans le processus d'apprentissage de word2vec, deux hyperparamÃ¨tres clÃ©s sont la taille de la fenÃªtre et le nombre d'Ã©chantillons nÃ©gatifs. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  DiffÃ©rentes tailles de fenÃªtres conviennent Ã  diffÃ©rentes tÃ¢ches.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Il a Ã©tÃ© remarquÃ©</a> que des tailles de fenÃªtre plus petites (2â€“15) gÃ©nÃ¨rent <i>des</i> piÃ¨ces jointes <i>interchangeables</i> avec des index similaires (notez que les antonymes sont souvent interchangeables lorsque vous regardez les mots environnants: par exemple, les mots Â«bonÂ» et Â«mauvaisÂ» sont souvent mentionnÃ©s dans des contextes similaires).  Des tailles de fenÃªtre plus grandes (15â€“50 ou mÃªme plus) gÃ©nÃ¨rent <i>des</i> piÃ¨ces jointes <i>associÃ©es</i> avec des indices similaires.  En pratique, vous devez souvent fournir des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">annotations</a> pour les similitudes sÃ©mantiques utiles dans votre tÃ¢che.  Dans Gensim, la taille de fenÃªtre par dÃ©faut est 5 (deux mots gauche et droit, en plus du mot d'entrÃ©e lui-mÃªme). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le nombre d'Ã©chantillons nÃ©gatifs est un autre facteur dans le processus d'apprentissage. </font><font style="vertical-align: inherit;">Le document original recommande 5-20. </font><font style="vertical-align: inherit;">Il indique Ã©galement que 2 Ã  5 Ã©chantillons semblent suffisants lorsque vous disposez d'un ensemble de donnÃ©es suffisamment volumineux. </font><font style="vertical-align: inherit;">Dans Gensim, la valeur par dÃ©faut est 5 motifs nÃ©gatifs.</font></font><br><br><h1>  Conclusion </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Si votre comportement dÃ©passe vos normes, alors vous Ãªtes une personne vivante, pas un automate" - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">God-Emperor of Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J'espÃ¨re que vous comprenez maintenant l'intÃ©gration des mots et l'essence de l'algorithme word2vec. </font><font style="vertical-align: inherit;">J'espÃ¨re Ã©galement que maintenant vous comprendrez mieux les articles qui mentionnent le concept de "saut de gramme avec Ã©chantillonnage nÃ©gatif" (SGNS), comme dans les systÃ¨mes de recommandation susmentionnÃ©s.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> RÃ©fÃ©rences et lectures complÃ©mentaires </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Â«ReprÃ©sentations distribuÃ©es des mots et des phrases et leur compositionÂ»</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«      Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«   Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«   Â»</a>      â€”    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«      Â»</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> â€”      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â« word2vecÂ»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«  Â»</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Â»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr446530/">https://habr.com/ru/post/fr446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr446514/index.html">Gmail a 15 ans</a></li>
<li><a href="../fr446516/index.html">Visualisation du temps de renaissance de Roshan</a></li>
<li><a href="../fr446518/index.html">Pare-feu d'applications Web</a></li>
<li><a href="../fr446520/index.html">Comment tout a commencÃ©: l'histoire des drones volants</a></li>
<li><a href="../fr446522/index.html">Swift 5.1 - quoi de neuf?</a></li>
<li><a href="../fr446532/index.html">Upwork introduit des frais pour le droit d'Ã©crire Ã  un client potentiel</a></li>
<li><a href="../fr446534/index.html">Sortie de Visual Studio 2019</a></li>
<li><a href="../fr446536/index.html">Files d'attente et JMeter: Ã©change avec Publisher et abonnÃ©</a></li>
<li><a href="../fr446538/index.html">PhotoGuru est passÃ© du Â«cÃ´tÃ© obscurÂ» au Â«plus sageÂ»</a></li>
<li><a href="../fr446544/index.html">Microsoft Ã©tend le programme Azure IP Advantage avec de nouveaux avantages IP pour les innovateurs et les startups Azure IoT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>