<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëäüèº üé∑ ü§ö Configurez le cluster Kubernetes HA sur du m√©tal nu avec kubeadm. Partie 1/3 ‚òùüèª ‚òÄÔ∏è üíò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Partie 2/3 ici 
 Partie 3/3 ici 


 Bonjour √† tous! Dans cet article, je souhaite rationaliser les informations et partager l'exp√©rience de la cr√©atio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configurez le cluster Kubernetes HA sur du m√©tal nu avec kubeadm. Partie 1/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/439562/"><p>  <strong>Partie 2/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>ici</strong></a> <br>  <strong>Partie 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>ici</strong></a> </p><br><p>  Bonjour √† tous!  Dans cet article, je souhaite rationaliser les informations et partager l'exp√©rience de la cr√©ation et de l'utilisation du cluster interne Kubernetes. </p><br><p>  Au cours des derni√®res ann√©es, cette technologie d'orchestration de conteneurs a fait un grand pas en avant et est devenue une sorte de norme d'entreprise pour des milliers d'entreprises.  Certains l'utilisent en production, d'autres le testent simplement sur des projets, mais les passions autour, peu importe comment vous le dites, brillent de s√©rieux.  Si vous ne l'avez jamais utilis√© auparavant, il est temps de commencer √† sortir ensemble. </p><br><h3 id="0-vstuplenie">  0. Introduction </h3><br><p>  Kubernetes est une technologie d'orchestration √©volutive qui peut commencer par l'installation sur un seul n≈ìud et atteindre la taille d'√©normes clusters HA bas√©s sur plusieurs centaines de n≈ìuds √† l'int√©rieur.  Les fournisseurs de cloud les plus populaires proposent diff√©rents types d'impl√©mentations Kubernetes - √† prendre et √† utiliser.  Mais les situations sont diff√©rentes et il y a des entreprises qui n'utilisent pas les nuages ‚Äã‚Äãet qui veulent profiter de tous les avantages des technologies d'orchestration modernes.  Et voici l'installation de Kubernetes sur du m√©tal nu. </p><br><p><img src="https://habrastorage.org/webt/el/ci/ua/elciua9kwxmo0fnnm5yoaabqpvm.jpeg"></p><a name="habracut"></a><br><h3 id="1-vvedenie">  1. Introduction </h3><br><p> Dans cet exemple, nous allons cr√©er un cluster Kubernetes HA avec la topologie pour plusieurs ma√Ætres, avec un cluster externe etcd comme couche de base et un √©quilibreur de charge MetalLB √† l'int√©rieur.  Sur tous les n≈ìuds de travail, nous d√©ploierons GlusterFS comme un simple stockage de cluster distribu√© interne.  Nous essaierons √©galement d'y d√©ployer plusieurs projets de test √† l'aide de notre registre Docker personnel. </p><br><p>  En g√©n√©ral, il existe plusieurs fa√ßons de cr√©er un cluster Kubernetes HA: le chemin difficile et d√©taill√© d√©crit dans le document populaire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kubernetes-the-hard-way</a> , ou la mani√®re plus simple √† l'aide de l'utilitaire <strong>kubeadm</strong> . </p><br><p>  Kubeadm est un outil cr√©√© par la communaut√© Kubernetes sp√©cifiquement pour simplifier l'installation de Kubernetes et faciliter le processus.  Auparavant, Kubeadm √©tait recommand√© uniquement pour cr√©er de petits clusters de test avec un n≈ìud ma√Ætre, pour commencer.  Mais au cours de la derni√®re ann√©e, beaucoup de choses ont √©t√© am√©lior√©es, et maintenant nous pouvons l'utiliser pour cr√©er des clusters HA avec plusieurs n≈ìuds ma√Ætres.  Selon les nouvelles de la communaut√© Kubernetes, √† l'avenir, Kubeadm sera recommand√© comme outil pour installer Kubernetes. </p><br><p>  La documentation de Kubeadm propose deux m√©thodes de base pour impl√©menter un cluster, avec une pile et des topologies etcd externes.  Je choisirai le deuxi√®me chemin avec les n≈ìuds etcd externes en raison de la tol√©rance aux pannes du cluster HA. </p><br><p>  Voici un diagramme de la documentation Kubeadm d√©crivant ce chemin: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/4y/nh/gd/4ynhgd4h3ireojrvdplimgnsk2u.jpeg"></a> </p><br><p>  Je vais le changer un peu.  Tout d'abord, j'utiliserai une paire de serveurs HAProxy comme √©quilibreurs de charge avec le package Heartbeat, qui partagera l'adresse IP virtuelle.  Heartbeat et HAProxy utilisent une petite quantit√© de ressources syst√®me, je vais donc les placer sur une paire de n≈ìuds etcd pour r√©duire l√©g√®rement le nombre de serveurs pour notre cluster. </p><br><p>  Pour ce sch√©ma de cluster Kubernetes, huit n≈ìuds sont requis.  Trois serveurs pour un cluster externe, etc. (les services LB en utiliseront √©galement quelques-uns), deux pour les n≈ìuds du plan de contr√¥le (n≈ìuds ma√Ætres) et trois pour les n≈ìuds de travail.  Il peut s'agir d'un serveur nu ou d'un serveur VM.  Dans ce cas, cela n'a pas d'importance.  Vous pouvez facilement modifier le sch√©ma en ajoutant plus de n≈ìuds ma√Ætres et en pla√ßant HAProxy avec Heartbeat sur des n≈ìuds s√©par√©s, s'il existe de nombreux serveurs gratuits.  Bien que mon option pour la premi√®re impl√©mentation du cluster HA soit suffisante pour les yeux. </p><br><p>  Si vous le souhaitez, ajoutez un petit serveur avec l'utilitaire <strong>kubectl install√©</strong> pour g√©rer ce cluster ou utilisez votre propre bureau Linux pour cela. </p><br><p>  Le diagramme de cet exemple ressemblera √† ceci: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/r4/5w/uc/r45wucdscdlhmaqcuw-gtr7mrmm.jpeg"></a> </p><br><h3 id="2-trebovaniya">  2. Exigences </h3><br><p>  Vous aurez besoin de deux n≈ìuds ma√Ætres Kubernetes avec la configuration syst√®me minimale recommand√©e: 2 CPU et 2 Go de RAM conform√©ment √† la documentation de <strong>kubeadm</strong> .  Pour les n≈ìuds de travail, je recommande d'utiliser des serveurs plus puissants, car nous ex√©cuterons tous nos services d'application sur eux.  Et pour Etcd + LB, nous pouvons √©galement prendre des serveurs avec deux processeurs et au moins 2 Go de RAM. </p><br><p>  S√©lectionnez un r√©seau public ou priv√© pour ce cluster;  Les adresses IP n'ont pas d'importance;  Il est important que tous les serveurs soient accessibles les uns pour les autres et, bien s√ªr, pour vous.  Plus tard, √† l'int√©rieur du cluster Kubernetes, nous mettrons en place un r√©seau de superposition. </p><br><p>  Les exigences minimales pour cet exemple sont: </p><br><ul><li>  2 serveurs avec 2 processeurs et 2 Go de RAM pour le n≈ìud ma√Ætre </li><li>  3 serveurs avec 4 processeurs et 4 √† 8 Go de RAM pour les n≈ìuds de travail </li><li>  3 serveurs avec 2 processeurs et 2 Go de RAM pour Etcd et HAProxy </li><li>  192.168.0.0/24 - le sous-r√©seau. </li></ul><br><p>  192.168.0.1 - l'adresse IP virtuelle de HAProxy, 192.168.0.2 - 4 adresses IP principales des n≈ìuds Etcd et HAProxy, 192.168.0.5 - 6 adresses IP principales du n≈ìud ma√Ætre Kubernetes, 192.168.0.7 - 9 adresses IP principales des n≈ìuds de travail Kubernetes . </p><br><p>  La base de donn√©es Debian 9 est install√©e sur tous les serveurs. </p><br><blockquote>  N'oubliez pas non plus que la configuration requise d√©pend de la taille et de la puissance du cluster.  Pour plus d'informations, consultez la documentation de Kubernetes. </blockquote><br><h3 id="3-nastroyka-haproxy-i-heartbeat">  3. Configurez HAProxy et Heartbeat. </h3><br><p>  Nous avons plus d'un n≈ìud ma√Ætre Kubernetes, et vous devez donc configurer un √©quilibreur de charge HAProxy en face d'eux - pour r√©partir le trafic.  Ce sera une paire de serveurs HAProxy avec une adresse IP virtuelle partag√©e.  La tol√©rance aux pannes est fournie avec le package Heartbeat.  Pour le d√©ploiement, nous utiliserons les deux premiers serveurs etcd. </p><br><p>  Installez et configurez HAProxy avec Heartbeat sur les premier et deuxi√®me serveurs etcd (192.168.0.2‚Äì3 dans cet exemple): </p><br><pre><code class="plaintext hljs">etcd1# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy etcd2# apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install -y haproxy</code> </pre> <br><p>  Enregistrez la configuration d'origine et cr√©ez-en une nouvelle: </p><br><pre> <code class="plaintext hljs">etcd1# mv /etc/haproxy/haproxy.cfg{,.back} etcd1# vi /etc/haproxy/haproxy.cfg etcd2# mv /etc/haproxy/haproxy.cfg{,.back} etcd2# vi /etc/haproxy/haproxy.cfg</code> </pre> <br><p>  Ajoutez ces options de configuration pour les deux HAProxy: </p><br><pre> <code class="plaintext hljs">global user haproxy group haproxy defaults mode http log global retries 2 timeout connect 3000ms timeout server 5000ms timeout client 5000ms frontend kubernetes bind 192.168.0.1:6443 option tcplog mode tcp default_backend kubernetes-master-nodes backend kubernetes-master-nodes mode tcp balance roundrobin option tcp-check server k8s-master-0 192.168.0.5:6443 check fall 3 rise 2 server k8s-master-1 192.168.0.6:6443 check fall 3 rise 2</code> </pre> <br><p>  Comme vous pouvez le voir, les deux services HAProxy partagent l'adresse IP - 192.168.0.1.  Cette adresse IP virtuelle se d√©placera entre les serveurs, nous allons donc √™tre un peu rus√©s et <strong>activer le</strong> param√®tre <strong>net.ipv4.ip_nonlocal_bind</strong> pour permettre la liaison des services syst√®me √† une adresse IP non locale. </p><br><p>  Ajoutez cette fonctionnalit√© au fichier <strong>/etc/sysctl.conf</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1 etcd2# vi /etc/sysctl.conf net.ipv4.ip_nonlocal_bind=1</code> </pre> <br><p>  Ex√©cutez sur les deux serveurs: </p><br><pre> <code class="plaintext hljs">sysctl -p</code> </pre> <br><p>  Ex√©cutez √©galement HAProxy sur les deux serveurs: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl start haproxy etcd2# systemctl start haproxy</code> </pre> <br><p>  Assurez-vous que HAProxy est en cours d'ex√©cution et √©coute sur l'adresse IP virtuelle sur les deux serveurs: </p><br><pre> <code class="plaintext hljs">etcd1# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy etcd2# netstat -ntlp tcp 0 0 192.168.0.1:6443 0.0.0.0:* LISTEN 2833/haproxy</code> </pre> <br><p>  Hood!  Installez maintenant Heartbeat et configurez cette IP virtuelle. </p><br><pre> <code class="plaintext hljs">etcd1# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat etcd2# apt-get -y install heartbeat &amp;&amp; systemctl enable heartbeat</code> </pre> <br><p>  Il est temps de cr√©er plusieurs fichiers de configuration pour cela: pour les premier et deuxi√®me serveurs, ils seront fondamentalement les m√™mes. </p><br><p>  Cr√©ez d'abord le fichier <strong>/etc/ha.d/authkeys</strong> , dans ce fichier Heartbeat stocke les donn√©es pour l'authentification mutuelle.  Le fichier doit √™tre le m√™me sur les deux serveurs: </p><br><pre> <code class="plaintext hljs"># echo -n securepass | md5sum bb77d0d3b3f239fa5db73bdf27b8d29a etcd1# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a etcd2# vi /etc/ha.d/authkeys auth 1 1 md5 bb77d0d3b3f239fa5db73bdf27b8d29a</code> </pre> <br><p>  Ce fichier ne doit √™tre accessible qu'√† root: </p><br><pre> <code class="plaintext hljs">etcd1# chmod 600 /etc/ha.d/authkeys etcd2# chmod 600 /etc/ha.d/authkeys</code> </pre> <br><p>  Cr√©ez maintenant le fichier de configuration principal de Heartbeat sur les deux serveurs: pour chaque serveur, il sera l√©g√®rement diff√©rent. </p><br><p>  Cr√©ez <strong>/etc/ha.d/ha.cf</strong> : </p><br><p>  <strong>etcd1</strong> </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.3 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to log/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  <strong>etcd2</strong> </p><br><pre> <code class="plaintext hljs">etcd2# vi /etc/ha.d/ha.cf # keepalive: how many seconds between heartbeats # keepalive 2 # # deadtime: seconds-to-declare-host-dead # deadtime 10 # # What UDP port to use for udp or ppp-udp communication? # udpport 694 bcast ens18 mcast ens18 225.0.0.1 694 1 0 ucast ens18 192.168.0.2 # What interfaces to heartbeat over? udp ens18 # # Facility to use for syslog()/logger (alternative to vlog/debugfile) # logfacility local0 # # Tell what machines are in the cluster # node nodename ... -- must match uname -n node etcd1_hostname node etcd2_hostname</code> </pre> <br><p>  Obtenez les param√®tres ¬´n≈ìud¬ª pour cette configuration en ex√©cutant uname -n sur les deux serveurs Etcd.  Utilisez √©galement le nom de votre carte r√©seau au lieu de ens18. </p><br><p>  Enfin, vous devez cr√©er le fichier <strong>/etc/ha.d/haresources</strong> sur ces serveurs.  Pour les deux serveurs, le fichier doit √™tre le m√™me.  Dans ce fichier, nous d√©finissons notre adresse IP commune et d√©terminons quel n≈ìud est le ma√Ætre par d√©faut: </p><br><pre> <code class="plaintext hljs">etcd1# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1 etcd2# vi /etc/ha.d/haresources etcd1_hostname 192.168.0.1</code> </pre> <br><p>  Lorsque tout est pr√™t, d√©marrez les services Heartbeat sur les deux serveurs et v√©rifiez que nous avons re√ßu cette IP virtuelle d√©clar√©e sur le <strong>n≈ìud</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# systemctl restart heartbeat etcd2# systemctl restart heartbeat etcd1# ip a ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff inet 192.168.0.2/24 brd 192.168.0.255 scope global ens18 valid_lft forever preferred_lft forever inet 192.168.0.1/24 brd 192.168.0.255 scope global secondary</code> </pre> <br><p>  Vous pouvez v√©rifier que HAProxy fonctionne correctement en ex√©cutant <strong>nc</strong> √† 192.168.0.1 6443. Vous devez avoir expir√© car l'API Kubernetes n'√©coute pas encore c√¥t√© serveur.  Mais cela signifie que HAProxy et Heartbeat sont correctement configur√©s. </p><br><pre> <code class="plaintext hljs"># nc -v 192.168.0.1 6443 Connection to 93.158.95.90 6443 port [tcp/*] succeeded!</code> </pre> <br><h3 id="4-podgotovka-nod-dlya-kubernetes">  4. Pr√©paration des n≈ìuds pour Kubernetes </h3><br><p>  L'√©tape suivante consiste √† pr√©parer tous les n≈ìuds Kubernetes.  Vous devez installer Docker avec des packages suppl√©mentaires, ajouter le r√©f√©rentiel Kubernetes et installer les <strong>packages kubelet</strong> , <strong>kubeadm</strong> , <strong>kubectl</strong> √† partir de celui-ci.  Ce param√®tre est le m√™me pour tous les n≈ìuds Kubernetes (ma√Ætre, travailleurs, etc.) </p><br><blockquote>  Le principal avantage de <strong>Kubeadm</strong> est qu'il n'y a pas vraiment besoin de logiciels suppl√©mentaires.  Installez <strong>kubeadm</strong> sur tous les h√¥tes - et utilisez-le;  g√©n√©rer au moins des certificats CA. </blockquote><p>  Installez Docker sur tous les n≈ìuds: </p><br><pre> <code class="plaintext hljs">Update the apt package index # apt-get update Install packages to allow apt to use a repository over HTTPS # apt-get -y install \ apt-transport-https \ ca-certificates \ curl \ gnupg2 \ software-properties-common Add Docker's official GPG key # curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add docker apt repository # apt-add-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable" Install docker-ce. # apt-get update &amp;&amp; apt-get -y install docker-ce Check docker version # docker -v Docker version 18.09.0, build 4d60db4</code> </pre> <br><p>  Apr√®s cela, installez les packages Kubernetes sur tous les n≈ìuds: </p><br><ul><li>  <strong><code>kubeadm</code></strong> : commande pour charger le cluster. </li><li>  <strong><code>kubelet</code></strong> : un composant qui s'ex√©cute sur tous les ordinateurs du cluster et effectue des actions telles que le lancement de foyers et de conteneurs. </li><li>  <strong><code>kubectl</code></strong> : <strong><code>kubectl</code></strong> ligne de commande pour communiquer avec le cluster. </li><li>  <strong>kubectl</strong> - √† volont√©;  Je l'installe souvent sur tous les n≈ìuds pour ex√©cuter certaines commandes Kubernetes pour le d√©bogage. </li></ul><br><pre> <code class="plaintext hljs"># curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository # cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install packages # apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl Hold back packages # apt-mark hold kubelet kubeadm kubectl Check kubeadm version # kubeadm version kubeadm version: &amp;version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.1", GitCommit:"eec55b9dsfdfgdfgfgdfgdfgdf365bdd920", GitTreeState:"clean", BuildDate:"2018-12-13T10:36:44Z", GoVersion:"go1.11.2", Compiler:"gc", Platform:"linux/amd64"}</code> </pre> <br><p>  Apr√®s avoir <strong>install√© kubeadm</strong> et d'autres packages, n'oubliez pas de d√©sactiver le swap. </p><br><pre> <code class="plaintext hljs"># swapoff -a # sed -i '/ swap / s/^/#/' /etc/fstab</code> </pre> <br><p>  R√©p√©tez l'installation sur les n≈ìuds restants.  Les packages logiciels sont les m√™mes pour tous les n≈ìuds du cluster, et seule la configuration suivante d√©terminera les r√¥les qu'ils recevront ult√©rieurement. </p><br><h3 id="5-nastroyka-klastera-ha-etcd">  5. Configurer le cluster HA Etcd </h3><br><p>  Ainsi, apr√®s avoir termin√© les pr√©paratifs, nous allons configurer le cluster Kubernetes.  La premi√®re brique sera le cluster HA Etcd, qui est √©galement configur√© √† l'aide de l'outil kubeadm. </p><br><blockquote>  Avant de commencer, assurez-vous que tous les n≈ìuds etcd communiquent via les ports 2379 et 2380. De plus, vous devez configurer l'acc√®s ssh entre eux pour utiliser <strong>scp</strong> . </blockquote><p>  Commen√ßons par le premier n≈ìud etcd, puis copions simplement tous les certificats et fichiers de configuration n√©cessaires sur les autres serveurs. </p><br><p>  Sur tous les n≈ìuds <strong>etcd</strong> , vous devez ajouter un nouveau fichier de configuration <strong>systemd</strong> pour l'unit√© <strong>kubelet</strong> avec une priorit√© plus √©lev√©e: </p><br><pre> <code class="plaintext hljs">etcd-nodes# cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true Restart=always EOF etcd-nodes# systemctl daemon-reload etcd-nodes# systemctl restart kubelet</code> </pre> <br><p>  Ensuite, nous passerons par ssh au premier n≈ìud <strong>etcd</strong> - nous l'utiliserons pour g√©n√©rer toutes les configurations <strong>kubeadm</strong> n√©cessaires pour chaque n≈ìud <strong>etcd</strong> , puis les copier. </p><br><pre> <code class="plaintext hljs"># Export all our etcd nodes IP's as variables etcd1# export HOST0=192.168.0.2 etcd1# export HOST1=192.168.0.3 etcd1# export HOST2=192.168.0.4 # Create temp directories to store files for all nodes etcd1# mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ etcd1# ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) etcd1# NAMES=("infra0" "infra1" "infra2") etcd1# for i in "${!ETCDHOSTS[@]}"; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: "kubeadm.k8s.io/v1beta1" kind: ClusterConfiguration etcd: local: serverCertSANs: - "${HOST}" peerCertSANs: - "${HOST}" extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done</code> </pre> <br><p>  Cr√©ez maintenant l'autorit√© de certification principale √† l'aide de <strong>kubeadm</strong> </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase certs etcd-ca</code> </pre> <br><p>  Cette commande cr√©era deux <strong>fichiers ca.crt &amp; ca.key</strong> dans le <strong>r√©pertoire</strong> <strong>/ etc / kubernetes / pki / etcd /</strong> . </p><br><pre> <code class="plaintext hljs">etcd1# ls /etc/kubernetes/pki/etcd/ ca.crt ca.key</code> </pre> <br><p>  Nous allons maintenant g√©n√©rer des certificats pour tous les n≈ìuds <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">### Create certificates for the etcd3 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST2}/ ### cleanup non-reusable certificates etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the etcd2 node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml etcd1# cp -R /etc/kubernetes/pki /tmp/${HOST1}/ ### cleanup non-reusable certificates again etcd1# find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete ### Create certificates for the this local node etcd1# kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1 #kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml etcd1# kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for this node # clean up certs that should not be copied off this host etcd1# find /tmp/${HOST2} -name ca.key -type f -delete etcd1# find /tmp/${HOST1} -name ca.key -type f -delete</code> </pre> <br><p>  Copiez ensuite les certificats et les configurations de kubeadm sur les n≈ìuds <strong>etcd2</strong> et <strong>etcd3</strong> . </p><br><blockquote>  <strong>G√©n√©rez d'</strong> abord une paire de cl√©s ssh sur <strong>etcd1</strong> et ajoutez la partie publique aux <strong>n≈ìuds etcd2</strong> et <strong>3</strong> .  Dans cet exemple, toutes les commandes sont ex√©cut√©es au nom d'un utilisateur qui poss√®de tous les droits sur le syst√®me. </blockquote><br><pre> <code class="plaintext hljs">etcd1# scp -r /tmp/${HOST1}/* ${HOST1}: etcd1# scp -r /tmp/${HOST2}/* ${HOST2}: ### login to the etcd2 or run this command remotely by ssh etcd2# cd /root etcd2# mv pki /etc/kubernetes/ ### login to the etcd3 or run this command remotely by ssh etcd3# cd /root etcd3# mv pki /etc/kubernetes/</code> </pre> <br><p>  Avant de d√©marrer le cluster etcd, assurez-vous que les fichiers existent sur tous les n≈ìuds: </p><br><p>  Liste des fichiers requis sur <strong>etcd1</strong> : </p><br><pre> <code class="plaintext hljs">/tmp/192.168.0.2 ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ ca.key ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Pour le n≈ìud <strong>etcd2,</strong> c'est: </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Et le dernier n≈ìud est <strong>etcd3</strong> : </p><br><pre> <code class="plaintext hljs">/root ‚îî‚îÄ‚îÄ kubeadmcfg.yaml --- /etc/kubernetes/pki ‚îú‚îÄ‚îÄ apiserver-etcd-client.crt ‚îú‚îÄ‚îÄ apiserver-etcd-client.key ‚îî‚îÄ‚îÄ etcd ‚îú‚îÄ‚îÄ ca.crt ‚îú‚îÄ‚îÄ healthcheck-client.crt ‚îú‚îÄ‚îÄ healthcheck-client.key ‚îú‚îÄ‚îÄ peer.crt ‚îú‚îÄ‚îÄ peer.key ‚îú‚îÄ‚îÄ server.crt ‚îî‚îÄ‚îÄ server.key</code> </pre> <br><p>  Lorsque tous les certificats et configurations sont en place, nous cr√©ons des manifestes.  Sur chaque n≈ìud, ex√©cutez la commande <strong>kubeadm</strong> - pour g√©n√©rer un manifeste statique pour le cluster <strong>etcd</strong> : </p><br><pre> <code class="plaintext hljs">etcd1# kubeadm init phase etcd local --config=/tmp/192.168.0.2/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml etcd1# kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml</code> </pre> <br><p>  Maintenant, le cluster <strong>etcd</strong> - en th√©orie - est configur√© et sain.  V√©rifiez en ex√©cutant la commande suivante sur le <strong>n≈ìud</strong> etcd1: </p><br><pre> <code class="plaintext hljs">etcd1# docker run --rm -it \ --net host \ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.24 etcdctl \ --cert-file /etc/kubernetes/pki/etcd/peer.crt \ --key-file /etc/kubernetes/pki/etcd/peer.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints https://192.168.0.2:2379 cluster-health ### status output member 37245675bd09ddf3 is healthy: got healthy result from https://192.168.0.3:2379 member 532d748291f0be51 is healthy: got healthy result from https://192.168.0.4:2379 member 59c53f494c20e8eb is healthy: got healthy result from https://192.168.0.2:2379 cluster is healthy</code> </pre> <br><p>  Le cluster <strong>etcd</strong> a augment√©, alors continuez. </p><br><h3 id="6-nastroyka-master--i-rabochih-nod">  6. Configuration des n≈ìuds ma√Ætre et de travail </h3><br><p>  Configurez les n≈ìuds ma√Ætres de notre cluster - copiez ces fichiers du premier n≈ìud <strong>etcd</strong> vers le premier n≈ìud ma√Ætre: </p><br><pre> <code class="plaintext hljs">etcd1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt 192.168.0.5: etcd1# scp /etc/kubernetes/pki/apiserver-etcd-client.key 192.168.0.5:</code> </pre> <br><p>  Ensuite, allez ssh sur le n≈ìud ma√Ætre <strong>master1</strong> et cr√©ez le <strong>fichier kubeadm-config.yaml</strong> avec le contenu suivant: </p><br><pre> <code class="plaintext hljs">master1# cd /root &amp;&amp; vi kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - "192.168.0.1" controlPlaneEndpoint: "192.168.0.1:6443" etcd: external: endpoints: - https://192.168.0.2:2379 - https://192.168.0.3:2379 - https://192.168.0.4:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key</code> </pre> <br><p>  D√©placez les certificats et la cl√© copi√©s pr√©c√©demment vers le r√©pertoire appropri√© sur le <strong>n≈ìud</strong> master1, comme dans la description du param√®tre. </p><br><pre> <code class="plaintext hljs">master1# mkdir -p /etc/kubernetes/pki/etcd/ master1# cp /root/ca.crt /etc/kubernetes/pki/etcd/ master1# cp /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ master1# cp /root/apiserver-etcd-client.key /etc/kubernetes/pki/</code> </pre> <br><p>  Pour cr√©er le premier n≈ìud ma√Ætre, proc√©dez comme suit: </p><br><pre> <code class="plaintext hljs">master1# kubeadm init --config kubeadm-config.yaml</code> </pre> <br><p>  Si toutes les √©tapes pr√©c√©dentes sont termin√©es correctement, vous verrez ce qui suit: </p><br><pre> <code class="plaintext hljs">You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  Copiez cette <strong>sortie d'</strong> initialisation <strong>kubeadm</strong> dans n'importe quel fichier texte, nous utiliserons ce jeton √† l'avenir lorsque nous attacherons le deuxi√®me ma√Ætre et les n≈ìuds de travail √† notre cluster. </p><br><p>  J'ai d√©j√† dit que le cluster Kubernetes utilisera une sorte de r√©seau de superposition pour les foyers et autres services, donc √† ce stade, vous devez installer une sorte de plugin CNI.  Je recommande le plugin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Weave CNI</a> .  L'exp√©rience a montr√© qu'il est plus utile et moins probl√©matique, mais vous pouvez en choisir un autre, par exemple Calico. </p><br><p>  Installation du plugin r√©seau Weave sur le premier n≈ìud ma√Ætre: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" The connection to the server localhost:8080 was refused - did you specify the right host or port? serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.extensions/weave-net created</code> </pre> <br><p>  Attendez un moment, puis entrez la commande suivante pour v√©rifier que les foyers de composants d√©marrent: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 6m25s coredns-86c58d9df4-xj98p 1/1 Running 0 6m25s kube-apiserver-master1 1/1 Running 0 5m22s kube-controller-manager-master1 1/1 Running 0 5m41s kube-proxy-8ncqw 1/1 Running 0 6m25s kube-scheduler-master1 1/1 Running 0 5m25s weave-net-lvwrp 2/2 Running 0 78s</code> </pre> <br><ul><li>  Il est recommand√© d'attacher de nouveaux n≈ìuds du plan de contr√¥le uniquement apr√®s l'initialisation du premier n≈ìud. </li></ul><br><p>  Pour v√©rifier l'√©tat du cluster, proc√©dez comme suit: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 11m v1.13.1</code> </pre> <br><p>  Super!  Le premier n≈ìud ma√Ætre s'est lev√©.  Maintenant, il est pr√™t et nous terminerons la cr√©ation du cluster Kubernetes - nous ajouterons un deuxi√®me n≈ìud ma√Ætre et des n≈ìuds de travail. <br>  Pour ajouter un deuxi√®me n≈ìud ma√Ætre, cr√©ez une cl√© ssh sur <strong>master1</strong> et ajoutez la partie publique √† <strong>master2</strong> .  Effectuez une connexion de test, puis copiez certains fichiers du premier n≈ìud ma√Ætre vers le second: </p><br><pre> <code class="plaintext hljs">master1# scp /etc/kubernetes/pki/ca.crt 192.168.0.6: master1# scp /etc/kubernetes/pki/ca.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.key 192.168.0.6: master1# scp /etc/kubernetes/pki/sa.pub 192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/front-proxy-ca.key @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.crt @192.168.0.6: master1# scp /etc/kubernetes/pki/apiserver-etcd-client.key @192.168.0.6: master1# scp /etc/kubernetes/pki/etcd/ca.crt 192.168.0.6:etcd-ca.crt master1# scp /etc/kubernetes/admin.conf 192.168.0.6: ### Check that files was copied well master2# ls /root admin.conf ca.crt ca.key etcd-ca.crt front-proxy-ca.crt front-proxy-ca.key sa.key sa.pub</code> </pre> <br><p>  Sur le deuxi√®me n≈ìud ma√Ætre, d√©placez les certificats et cl√©s pr√©c√©demment copi√©s vers les r√©pertoires appropri√©s: </p><br><pre> <code class="plaintext hljs">master2# mkdir -p /etc/kubernetes/pki/etcd mv /root/ca.crt /etc/kubernetes/pki/ mv /root/ca.key /etc/kubernetes/pki/ mv /root/sa.pub /etc/kubernetes/pki/ mv /root/sa.key /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.crt /etc/kubernetes/pki/ mv /root/apiserver-etcd-client.key /etc/kubernetes/pki/ mv /root/front-proxy-ca.crt /etc/kubernetes/pki/ mv /root/front-proxy-ca.key /etc/kubernetes/pki/ mv /root/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /root/admin.conf /etc/kubernetes/admin.conf</code> </pre> <br><p>  Connectez le deuxi√®me n≈ìud ma√Ætre au cluster.  Pour ce faire, vous avez besoin de la sortie de la commande de connexion, qui nous a √©t√© pr√©c√©demment transmise par <strong><code>kubeadm init</code></strong> sur le premier n≈ìud. </p><br><p>  Ex√©cutez le n≈ìud ma√Ætre <strong>master2</strong> : </p><br><pre> <code class="plaintext hljs">master2# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6 --experimental-control-plane</code> </pre> <br><ul><li>  Vous devez ajouter le <strong><code>--experimental-control-plane</code></strong> .  Il automatise la connexion des donn√©es de base √† un cluster.  Sans cet indicateur, le n≈ìud de travail habituel sera simplement ajout√©. </li></ul><br><p>  Attendez un peu que le n≈ìud rejoigne le cluster et v√©rifiez le nouvel √©tat du cluster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 32m v1.13.1 master2 Ready master 46s v1.13.1</code> </pre> <br><p>  Assurez-vous √©galement que tous les pods de tous les n≈ìuds ma√Ætres sont d√©marr√©s normalement: </p><br><pre> <code class="plaintext hljs">master1# kubectl ‚Äî kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system -w NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-d7qfw 1/1 Running 0 46m coredns-86c58d9df4-xj98p 1/1 Running 0 46m kube-apiserver-master1 1/1 Running 0 45m kube-apiserver-master2 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 45m kube-controller-manager-master2 1/1 Running 0 15m kube-proxy-8ncqw 1/1 Running 0 46m kube-proxy-px5dt 1/1 Running 0 15m kube-scheduler-master1 1/1 Running 0 45m kube-scheduler-master2 1/1 Running 0 15m weave-net-ksvxz 2/2 Running 1 15m weave-net-lvwrp 2/2 Running 0 41m</code> </pre> <br><p>  Super!  Nous avons presque termin√© avec la configuration du cluster Kubernetes.  Et la derni√®re chose √† faire est d'ajouter les trois n≈ìuds de travail que nous avons pr√©par√©s plus t√¥t. </p><br><p>  Entrez les n≈ìuds de travail et ex√©cutez la commande kubeadm join sans l' <strong><code>--experimental-control-plane</code></strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# kubeadm join 192.168.0.1:6443 --token aasuvd.kw8m18m5fy2ot387 --discovery-token-ca-cert-hash sha256:dcbaeed8d1478291add0294553b6b90b453780e546d06162c71d515b494177a6</code> </pre> <br><p>  V√©rifiez √† nouveau l'√©tat du cluster: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h30m v1.13.1 master2 Ready master 1h59m v1.13.1 worker1 Ready &lt;none&gt; 1h8m v1.13.1 worker2 Ready &lt;none&gt; 1h8m v1.13.1 worker3 Ready &lt;none&gt; 1h7m v1.13.1</code> </pre> <br><p>  Comme vous pouvez le voir, nous avons un cluster Kubernetes HA enti√®rement configur√© avec deux n≈ìuds ma√Ætre et trois n≈ìuds de travail.  Il est construit sur la base du cluster HA etcd avec un √©quilibreur de charge √† s√©curit√© int√©gr√©e devant les n≈ìuds ma√Ætres.  Cela me semble assez bon. </p><br><h3 id="7-nastroyka-udalennogo-upravleniya-klasterom">  7. Configuration de la gestion de cluster √† distance </h3><br><p>  Une autre action qui reste √† consid√©rer dans cette premi√®re partie de l'article est la configuration de l'utilitaire <strong>kubectl</strong> distant pour la gestion du cluster.  Auparavant, nous <strong>ex√©cutions</strong> toutes les commandes du n≈ìud ma√Ætre <strong>master1</strong> , mais cela ne convient que pour la premi√®re fois - lors de la configuration du cluster.  Ce serait bien de configurer un n≈ìud de contr√¥le externe.  Vous pouvez utiliser un ordinateur portable ou un autre serveur pour cela. </p><br><p>  Connectez-vous √† ce serveur et ex√©cutez: </p><br><pre> <code class="plaintext hljs">Add the Google repository key control# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - Add the Google repository control# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Update and install kubectl control# apt-get update &amp;&amp; apt-get install -y kubectl In your user home dir create control# mkdir ~/.kube Take the Kubernetes admin.conf from the master1 node control# scp 192.168.0.5:/etc/kubernetes/admin.conf ~/.kube/config Check that we can send commands to our cluster control# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 6h58m v1.13.1 master2 Ready master 6h27m v1.13.1 worker1 Ready &lt;none&gt; 5h36m v1.13.1 worker2 Ready &lt;none&gt; 5h36m v1.13.1 worker3 Ready &lt;none&gt; 5h36m v1.13.1</code> </pre> <br><p>  Ok, ex√©cutons maintenant un test sous dans notre cluster et v√©rifions comment cela fonctionne. </p><br><pre> <code class="plaintext hljs">control# kubectl create deployment nginx --image=nginx deployment.apps/nginx created control# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-5c7588df-6pvgr 1/1 Running 0 52s</code> </pre> <br><p>  F√©licitations!  Vous venez de d√©ployer Kubernetes.  Et cela signifie que votre nouveau cluster HA est pr√™t.  En fait, le processus de configuration d'un cluster <strong>Kubernetes √† l'</strong> aide de <strong>kubeadm est</strong> assez simple et rapide. </p><br><p>  Dans la partie suivante de l'article, nous ajouterons du stockage interne en configurant GlusterFS sur tous les n≈ìuds de travail, en configurant un √©quilibreur de charge interne pour notre cluster Kubernetes, et en ex√©cutant √©galement certains tests de stress, en d√©connectant certains n≈ìuds et en v√©rifiant la stabilit√© du cluster. </p><br><h3 id="posleslovie">  Postface </h3><br><p>  Oui, en travaillant sur cet exemple, vous rencontrerez un certain nombre de probl√®mes.  Ne vous inqui√©tez pas: pour annuler les modifications et ramener les n≈ìuds √† leur √©tat d'origine, ex√©cutez simplement <strong>kubeadm reset</strong> - les modifications que <strong>kubeadm a</strong> apport√©es pr√©c√©demment seront r√©initialis√©es et vous pourrez configurer √† nouveau.  N'oubliez pas non plus de v√©rifier l'√©tat des conteneurs Docker sur les n≈ìuds du cluster - assurez-vous qu'ils d√©marrent et fonctionnent tous sans erreur.  Pour plus d'informations sur les conteneurs endommag√©s, utilisez la <strong>commande docker logs containerid</strong> . </p><br><p>  C'est tout pour aujourd'hui.  Bonne chance </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr439562/">https://habr.com/ru/post/fr439562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr439550/index.html">Hackquest 2018. R√©sultats et √©critures. Jour 1-3</a></li>
<li><a href="../fr439552/index.html">Extensions Chrome malveillantes</a></li>
<li><a href="../fr439556/index.html">TDMS Fairway. M√©thodologies PMBOK et organisations de conception russes</a></li>
<li><a href="../fr439558/index.html">Nouveau vieux t√©l√©phone. R√©inventez le t√©l√©phone PSTN</a></li>
<li><a href="../fr439560/index.html">Adaptateur Ethereum blockchain pour plate-forme de donn√©es InterSystems IRIS</a></li>
<li><a href="../fr439564/index.html">Application pratique de la transformation d'arbre AST en utilisant le putout comme exemple</a></li>
<li><a href="../fr439566/index.html">Pourquoi la documentation SRE est importante. 3e partie</a></li>
<li><a href="../fr439568/index.html">SSD bas√©s sur QLC - un tueur de disque dur? Pas vraiment</a></li>
<li><a href="../fr439570/index.html">Magie IPython pour modifier les balises de cellules Jupyter</a></li>
<li><a href="../fr439572/index.html">Conception assist√©e par ordinateur d'√©quipements √©lectroniques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>