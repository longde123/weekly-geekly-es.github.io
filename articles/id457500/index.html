<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¥¨ ğŸ›ŒğŸ¼ ğŸ¬ Bagaimana kami melakukan autopilot untuk stasiun layanan ğŸ‘¨ğŸ½â€ğŸš€ ğŸŒ˜ ğŸ’­</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya bekerja di sebuah startup kecil di Berlin yang mengembangkan autopilots untuk mobil. Kami sedang menyelesaikan proyek untuk stasiun l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana kami melakukan autopilot untuk stasiun layanan</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457500/"> Halo, Habr!  Saya bekerja di sebuah startup kecil di Berlin yang mengembangkan autopilots untuk mobil.  Kami sedang menyelesaikan proyek untuk stasiun layanan dari produsen mobil besar Jerman dan saya ingin membicarakannya: bagaimana kami melakukannya, kesulitan apa yang kami temui dan hal-hal baru apa yang kami temukan.  Pada bagian ini saya akan berbicara tentang modul persepsi dan sedikit tentang arsitektur solusi secara keseluruhan.  Tentang sisa modul, mungkin, kita akan memberi tahu di bagian berikut.  Saya akan sangat senang menerima umpan balik dan pandangan dari luar tentang pendekatan kami. <br><a name="habracut"></a><br>  Siaran pers proyek dari pelanggan dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br>  Untuk memulainya, saya akan memberi tahu Anda mengapa pembuat mobil menoleh kepada kami, dan tidak melakukan proyek sendiri.  Sulit bagi keprihatinan besar Jerman untuk mengubah proses, dan format pengembangan mobil jarang cocok untuk perangkat lunak - iterasi panjang dan membutuhkan perencanaan yang baik.  Menurut saya, para pembuat mobil Jerman memahami hal ini, dan karena itu Anda dapat bertemu dengan startup yang didirikan oleh mereka, tetapi bekerja sebagai perusahaan independen (misalnya, AID dari Audi dan Zenuity dari Volvo).  Pembuat mobil lainnya menyelenggarakan acara seperti Startup Autobahn, di mana mereka mencari kontraktor potensial untuk tugas dan ide baru.  Mereka dapat memesan produk atau prototipe dan setelah waktu yang singkat mendapatkan hasil yang selesai.  Ini mungkin ternyata lebih cepat daripada mencoba melakukan hal yang sama sendiri, dan biayanya tidak lebih dari pengembangan sendiri dalam hal biaya.  Kompleksitas perubahan proses ditunjukkan dengan baik oleh jumlah izin yang diperlukan untuk mulai menguji mobil dengan autopilot di pelanggan: persetujuan untuk merekam video orang (bahkan jika kami tidak menyimpan data, dan kami menggunakan streaming video hanya dalam bentuk anonim tanpa mengidentifikasi orang tertentu), persetujuan untuk merekam video wilayah, persetujuan serikat pekerja dan konsul pekerja untuk menguji teknologi ini, persetujuan dari layanan keamanan, persetujuan dari layanan TI - ini bukan daftar keseluruhan. <br><br><h2>  Tantangan </h2><br>  Dalam proyek saat ini, pelanggan ingin memahami apakah mungkin untuk mengendarai mobil di pusat layanan menggunakan "AI".  Skrip pengguna adalah: <br><br><ol><li>  Teknisi ingin mulai bekerja dengan mesin yang ada di suatu tempat di tempat parkir di luar area pengujian. </li><li>  Dia memilih mobil di tablet, memilih kotak layanan dan mengklik "Drive in". </li><li>  Mobil itu melaju ke dalam dan berhenti di titik akhir (lift, ramp, atau yang lainnya). </li><li>  Ketika teknisi selesai bekerja pada mobil, ia menekan tombol pada tablet, mobil keluar dan memarkir di ruang kosong di luar. </li></ol><br>  Fitur: tidak semua mobil memiliki kamera.  Pada mesin-mesin di mana mereka berada, kami tidak memiliki akses ke mereka.  Satu-satunya data pada mesin yang kami akses adalah sonars dan odometry <br><br><div class="spoiler">  <b class="spoiler_title">Sonar dan odometri</b> <div class="spoiler_text">  Sonars adalah sensor jarak yang dipasang dalam lingkaran di mobil dan sering terlihat seperti titik bulat, mereka memungkinkan Anda untuk memperkirakan jarak ke objek, tetapi hanya menutup dan dengan akurasi rendah.  Odometry - data tentang kecepatan aktual dan arah mobil.  Mengetahui data ini dan posisi awal, Anda dapat menentukan posisi mesin saat ini dengan cukup akurat. <br></div></div><br>  Dengan demikian, mobil harus dikontrol oleh sensor eksternal yang dipasang di area servis. <br><br><h2>  Solusi </h2><br>  Arsitektur produk akhir adalah sebagai berikut: <br><br><ul><li>  Di area layanan kami memasang kamera eksternal, kapten dan hal-hal lain (halo Tesla). </li><li>  Data dari kamera pergi ke Jetson TX2 (masing-masing tiga kamera), yang terlibat dalam tugas menemukan mesin dan pra-pemrosesan gambar dari kamera. </li><li>  Selanjutnya, data kamera datang ke server pusat, yang dengan bangga disebut Menara Kontrol dan di mana mereka jatuh ke dalam modul persepsi, pelacakan dan perencanaan jalur.  Sebagai hasil dari analisis, keputusan dibuat tentang arah pergerakan lebih lanjut dari mobil dan dikirim ke mobil. </li><li>  Pada tahap proyek ini, Jetson TX2 lain dimasukkan ke dalam mobil, yang, menggunakan driver kami, terhubung ke Vector, yang mendekripsi data mobil dan mengirimkan perintah.  TX2 menerima perintah kontrol dari server pusat dan menyiarkannya ke mobil. </li></ul><br>  Untuk tingkat infrastruktur, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ROS</a> digunakan. <br><br>  Inilah yang terjadi setelah seorang teknisi memilih mobil dan mengklik "drive in": <br><br><ol><li>  Sistem mencari mobil: kami mengirim perintah ke mobil untuk mengedipkan alarm, setelah itu kami dapat menentukan mobil mana di tempat parkir yang dipilih oleh teknisi.  Pada tahap awal pengembangan, kami juga mempertimbangkan opsi untuk menentukan mesin berdasarkan plat nomor, tetapi di beberapa area mobil yang diparkir, jumlahnya mungkin tidak terlihat.  Selain itu, jika kami membuat penentuan mobil dengan nomor registrasi, maka resolusi foto harus sangat ditingkatkan, yang akan berdampak negatif pada produktivitas, dan kami menggunakan gambar yang sama untuk mencari dan mengemudi.  Tahap ini terjadi sekali dan diulang hanya jika karena alasan tertentu kami kehilangan mobil dalam pelacakan. </li><li>  Segera setelah mobil ditemukan, kami menjatuhkan gambar dari kamera yang terkena mobil ke dalam modul persepsi, yang membagi ruang dan memberikan koordinat semua objek, orientasi dan ukurannya.  Proses ini sedang berlangsung, berjalan pada sekitar 30 frame per detik.  Proses selanjutnya juga konstan dan berjalan sampai mesin tiba di titik akhir. </li><li>  Modul pelacakan menerima input dari persepsi, sonar, dan odometri, menyimpan semua objek yang ditemukan dalam memori, menggabungkannya, menyaring lokasi, memprediksi posisi dan kecepatan objek. </li><li>  Selanjutnya, perencana jalur, yang dibagi menjadi dua bagian: perencana jalur global untuk rute global dan perencana jalur lokal untuk lokal (bertanggung jawab untuk menghindari rintangan), membangun jalur dan memutuskan ke mana harus pergi ke mobil kami, mengirimkan perintah. </li><li>  Jetson mengambil perintah dengan mobil dan menyiarkannya ke mobil. </li></ol><br>  Keberangkatan berlangsung dengan cara yang sama dengan kedatangan. <br><br><h2>  Persepsi </h2><br>  Salah satu yang utama dan, menurut saya, modul yang paling menarik adalah persepsi.  Modul ini menjelaskan data dari sensor sedemikian rupa sehingga Anda dapat secara akurat membuat keputusan tentang gerakan.  Dalam proyek kami, ini memberikan koordinat, orientasi, dan ukuran semua objek yang jatuh pada kamera.  Ketika merancang modul ini, kami memutuskan untuk memulai dengan algoritma yang memungkinkan kami untuk menganalisis gambar dalam satu pass.  Kami mencoba: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">VAE yang terurai</a> .  Modifikasi kecil yang dibuat untuk Î²-VAE memungkinkan kami untuk melatih jaringan sehingga vektor laten menyimpan informasi gambar dalam tampilan top-down skematis. </li><li>  GAN Bersyarat (implementasi paling terkenal adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pix2pix</a> ).  Jaringan ini dapat digunakan untuk membangun peta.  Kami juga menggunakannya untuk membangun tampilan skematis dari atas, memasukkan data dari satu atau semua kamera ke dalamnya secara bersamaan dan menunggu tampilan skematis dari atas pada output. </li></ol><br>  <i>Salah satu iterasi GAN Bersyarat untuk satu kamera, dari kiri ke kanan: input gambar, prediksi jaringan, hasil yang diharapkan</i> <br><br><img src="https://habrastorage.org/webt/w1/m9/ax/w1m9ax7idlos-x3jawcob3kmxzm.png"><br><br>  Bahkan, ide dari pendekatan ini adalah untuk memastikan bahwa jaringan akhir dapat memahami lokasi dan orientasi semua mobil dan objek bergerak lainnya yang telah jatuh pada kamera dengan melihat foto input sekali.  Data pada objek dalam hal ini akan disimpan dalam vektor laten.  Pelatihan jaringan berlangsung pada data dari simulator, yang merupakan salinan tepat dari titik di mana demonstrasi akan berlangsung.  Dan kami berhasil mencapai hasil tertentu, tetapi kami memutuskan untuk tidak menggunakan metode ini karena beberapa alasan: <br><br><ul><li>  Dalam waktu yang ditentukan, kami tidak dapat mempelajari cara menggunakan data dari vektor laten untuk menggambarkan gambar.  Hasil dari jaringan selalu berupa gambar - tampilan teratas dengan tata letak obyek yang skematis.  Ini kurang akurat dan kami khawatir akurasi seperti itu tidak akan cukup untuk mengendarai mobil. </li><li>  Solusinya tidak dapat diskalakan: untuk semua instalasi berikutnya dan untuk kasus ketika Anda perlu mengubah arah beberapa kamera, diperlukan konfigurasi ulang simulator dan pelatihan penuh berulang. </li></ul><br>  Namun demikian, kami tertarik untuk memahami kemungkinan pendekatan ini, dan kami akan mengingatnya untuk tugas di masa depan. <br><br>  Setelah itu, kami mendekati tugas di sisi lain, melalui pencarian reguler untuk objek + jaringan untuk menentukan posisi spasial dari objek yang ditemukan (misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">itu</a> ).  Menurut kami, opsi ini yang paling akurat.  Satu-satunya minus adalah bahwa itu lebih lambat dari pendekatan yang diusulkan sebelumnya, tetapi cocok dengan kerangka kerja penundaan kami, karena kecepatan mobil di area layanan tidak lebih dari 5 km / jam.  Karya paling menarik di bidang prediksi posisi 3D objek bagi kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">adalah yang ini</a> , yang menunjukkan hasil yang cukup bagus pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">KITTI</a> .  Kami membangun jaringan yang sama dengan beberapa perubahan dan menulis algoritma kami sendiri untuk menentukan kotak di sekitarnya, dan lebih tepatnya, algoritma untuk memperkirakan koordinat pusat proyeksi objek ke tanah - untuk membuat keputusan tentang arah gerakan, kami tidak memerlukan data tentang ketinggian objek.  Gambar objek dan jenisnya (mobil, pejalan kaki, ..) dimasukkan ke input jaringan, dan dimensi serta orientasi spasialnya adalah keluaran.  Selanjutnya, modul mengevaluasi pusat proyeksi dan memberikan data untuk semua objek: koordinat tengah, orientasi dan dimensi (lebar dan panjang). <br><br>  Pada produk akhir, setiap gambar dijalankan pertama kali melalui jaringan untuk mencari objek, kemudian semua objek dikirim ke jaringan 3D untuk memprediksi orientasi dan ukuran, setelah itu kami mengevaluasi pusat proyeksi masing-masing dan mengirimkannya serta orientasi dan ukuran data lebih lanjut.  Fitur dari metode ini adalah sangat terikat dengan keakuratan batas kotak batas dari jaringan pencarian objek.  Karena alasan ini, jaringan seperti YOLO tidak cocok untuk kami.  Kami menemukan keseimbangan optimal antara kinerja dan akurasi kotak batas pada jaringan RetinaNet. <br><br>  Perlu dicatat satu hal yang kita beruntungi dalam proyek ini: tanahnya rata.  Ya, itu tidak datar seperti komunitas terkenal, tetapi tidak ada tikungan di wilayah kami.  Ini memungkinkan penggunaan kamera bermata tetap untuk memproyeksikan objek ke koordinat bidang bumi tanpa informasi tentang jarak ke objek.  Rencana masa depan meliputi pengenalan prediksi kedalaman monokuler.  Ada banyak karya tentang topik ini, misalnya, salah satu yang terakhir dan sangat menarik yang kami coba untuk proyek masa depan.  Prediksi kedalaman akan memungkinkan Anda untuk bekerja tidak hanya di tanah datar, tetapi juga berpotensi meningkatkan akurasi penentuan hambatan, menyederhanakan proses mengonfigurasi kamera baru dan menghilangkan kebutuhan untuk memberi label pada setiap objek - kami tidak peduli objek apa itu jika itu merupakan semacam hambatan. <br><br>  Itu saja, terima kasih sudah membaca, dan saya akan dengan senang hati menjawab pertanyaan.  Sebagai bonus, saya ingin berbicara tentang efek negatif yang tidak terduga: autopilot tidak peduli dengan orientasi mobil, baginya tidak masalah bagaimana harus maju atau mundur.  Hal utama adalah mengemudi secara optimal dan tidak menabrak siapa pun.  Oleh karena itu, ada kemungkinan besar bahwa mobil akan melakukan perjalanan secara terbalik, terutama di daerah kecil di mana kemampuan manuver yang tinggi diperlukan.  Namun, orang terbiasa dengan kenyataan bahwa mobil sebagian besar bergerak maju, dan sering mengharapkan perilaku yang sama dari autopilot.  Jika seorang pebisnis melihat mobil yang, alih-alih mengendarai di depan, mengendarai mundur, maka ia dapat mempertimbangkan bahwa produk tersebut tidak siap dan mengandung kesalahan. <br><br>  <b>PS</b> Saya minta maaf bahwa tidak ada gambar dan video dengan pengujian nyata, tetapi saya tidak dapat mempublikasikannya karena alasan hukum. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id457500/">https://habr.com/ru/post/id457500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id457476/index.html">Mengurangi ukuran gambar buruh pelabuhan dengan aplikasi booting pegas</a></li>
<li><a href="../id457480/index.html">Membuat aplikasi mendengarkan untuk melihat lalu lintas MMORPG seluler</a></li>
<li><a href="../id457490/index.html">Aisioshechka dari Zuckerberg - secara singkat dan dalam kasus Libra</a></li>
<li><a href="../id457494/index.html">"Dan jika aku tidak tahu matematika, apakah aku putus asa?" - spesialis menjawab pertanyaan yang sering diajukan tentang profesi dalam Ilmu Data</a></li>
<li><a href="../id457496/index.html">"Temukan Lima Perbedaan." Perbedaan Scalable dan Generasi - Kumpulan Tes Baru</a></li>
<li><a href="../id457502/index.html">Bagaimana Model Penilaian RICE Meningkatkan Prioritas Fitur Produk</a></li>
<li><a href="../id457504/index.html">Mengapa menulis Kotak Data Bereaksi Anda pada tahun 2019</a></li>
<li><a href="../id457508/index.html">Parenting vs Machine Learning: membandingkan seorang ibu muda</a></li>
<li><a href="../id457510/index.html">Gunakan mcrouter untuk mengukur memcached secara horizontal</a></li>
<li><a href="../id457512/index.html">Replikasi logis antara versi PostgreSQL</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>