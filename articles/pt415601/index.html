<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ô•Ô∏è üóØÔ∏è üïµüèø Cluster Kubernetes HA com container. Ou existe vida sem estivador? üòæ üö¥üèª üèüÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Implantar a HA do kubernetes com o container 



 Boa tarde, queridos leitores de Habr! Em 24 de maio de 2018, um artigo intitulado Kubernetes Contain...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster Kubernetes HA com container. Ou existe vida sem estivador?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd">  Implantar a HA do kubernetes com o container </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br>  Boa tarde, queridos leitores de Habr!  Em 24 de maio de 2018, um artigo intitulado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes Containerd Integration Goes GA</a> foi publicado no blog oficial da Kubernetes, que declara que a integra√ß√£o da containererd com o Kubernetes est√° pronta para produ√ß√£o.  Al√©m disso, os caras da empresa Flant postaram uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tradu√ß√£o do artigo para o russo em</a> seu blog, acrescentando alguns esclarecimentos.  Depois de ler a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">projeto no github</a> , decidi tentar conter "minha pr√≥pria pele". </p><br><p>  Nossa empresa tem v√°rios projetos na fase de "ainda muito longe da produ√ß√£o".  Ent√£o eles se tornar√£o nosso experimental;  Para eles, decidimos tentar implantar um cluster de failover do Kubernetes usando o containerd e ver se h√° vida √∫til sem o docker. </p><br><p>  Se voc√™ estiver interessado em ver como fizemos e o que aconteceu, seja bem-vindo ao gato. </p><a name="habracut"></a><br><p>  <b>Descri√ß√£o esquem√°tica e de implanta√ß√£o</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p>  Ao implantar um cluster, como de costume, (escrevi sobre isso em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo anterior</a> </p><div class="spoiler">  <b class="spoiler_title">keepalived - implementa√ß√µes do VRRP (Protocolo de redund√¢ncia de roteador virtual) para Linux</b> <div class="spoiler_text">  Keepalived cria um IP virtual (VIRTIP) que "aponta" (cria uma subinterface) para o IP de um dos tr√™s mestres.  O daemon keepalived monitora a integridade das m√°quinas e, no caso de uma falha, exclui o servidor com falha da lista de servidores ativos alternando VIRTIP para o IP de outro servidor, de acordo com o "peso" especificado ao configurar o keepalived em cada servidor. </div></div><br><p>  Os daemons mantidos vivos se comunicam via VRRP, enviando mensagens para o endere√ßo 224.0.0.18. </p><br><p>  Se o vizinho n√£o enviou sua mensagem, depois do per√≠odo, ele √© considerado morto.  Assim que o servidor travado come√ßa a enviar suas mensagens para a rede, tudo volta ao seu lugar </p><br><p>  Configuramos o trabalho com o servidor API nos n√≥s do kubernetes da seguinte maneira. </p><br><p>  Ap√≥s instalar o cluster, configure o kube-proxy, altere a porta de 6443 para 16443 (detalhes abaixo).  Em cada um dos mestres, o nginx √© implementado, que funciona como um balanceador de carga, escuta na porta 16443 e faz um upstream de todos os tr√™s mestres na porta 6443 (detalhes abaixo). </p><br><p>  Esse esquema alcan√ßou maior toler√¢ncia a falhas usando keepalived e nginx, foi alcan√ßado o equil√≠brio entre os servidores API nos assistentes. <br></p><br><p>  Em um artigo anterior, descrevi a implanta√ß√£o do nginx e etcd na janela de encaixe.  Mas, neste caso, n√£o temos docker, portanto, nginx e etcd funcionar√£o localmente em masternodes. </p><br><p>  Teoricamente, seria poss√≠vel implantar o nginx e etcd usando o containerd, mas em caso de problemas, essa abordagem complicaria o diagn√≥stico, por isso decidimos n√£o experimentar e execut√°-lo localmente. </p><br><p>  <b>Descri√ß√£o dos servidores para implanta√ß√£o:</b> </p><br><table><thead><tr><th>  Nome </th><th>  IP </th><th>  Servi√ßos </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ </td></tr><tr><td>  kube-master01 </td><td>  172.26.133.161 </td><td>  kubeadm, kubelet, kubectl, etcd, containserd, nginx, keepalived </td></tr><tr><td>  kube-master02 </td><td>  172.26.133.162 </td><td>  kubeadm, kubelet, kubectl, etcd, containserd, nginx, keepalived </td></tr><tr><td>  kube-master03 </td><td>  172.26.133.163 </td><td>  kubeadm, kubelet, kubectl, etcd, containserd, nginx, keepalived </td></tr><tr><td>  kube-node01 </td><td>  172.26.133.164 </td><td>  kubeadm, kubelet, kubectl, container </td></tr><tr><td>  kube-node02 </td><td>  172.26.133.165 </td><td>  kubeadm, kubelet, kubectl, container </td></tr><tr><td>  kube-node03 </td><td>  172.26.133.166 </td><td>  kubeadm, kubelet, kubectl, container </td></tr></tbody></table><br><p>  <b>Instale kubeadm, kubelet, kubectl e pacotes relacionados</b> </p><br><p>  Todos os comandos s√£o executados a partir da raiz </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>Instalar conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>Configurando Configura√ß√µes de Cont√™iner</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p>  Adicionar ao arquivo: </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Come√ßamos a contarinerd, verificamos que est√° tudo bem </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ‚óè containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service ‚îî‚îÄ10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>Instale e execute o etcd</b> </p><br><p>  Nota importante, eu instalei o cluster do kubernetes vers√£o 1.10.  Apenas alguns dias depois, no momento da reda√ß√£o do artigo, foi lan√ßada a vers√£o 1.11. Se voc√™ instalar a vers√£o 1.11, defina a vari√°vel ETCD_VERSION = "v3.2.17", se 1.10, ent√£o ETCD_VERSION = "v3.1.12". </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p>  Copie configura√ß√µes do gitahab. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p>  Configure as vari√°veis ‚Äã‚Äãno arquivo de configura√ß√£o. </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Descri√ß√£o das vari√°veis ‚Äã‚Äãdo arquivo create-config.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p>  configura√ß√µes na m√°quina local de cada n√≥ (cada n√≥ tem seu pr√≥prio) <br>  <b>K8SHA_IPLOCAL</b> - Endere√ßo IP do n√≥ no qual o script est√° configurado <br>  <b>K8SHA_ETCDNAME</b> - nome da m√°quina local no cluster ETCD <br>  <b>K8SHA_KA_STATE</b> - papel em keepalived.  Um n√≥ MASTER, todos os outros BACKUP. <br>  <b>K8SHA_KA_PRIO</b> - prioridade <b>mantida</b> , o mestre possui 102 para os 101, 100 restantes. Quando o mestre com o n√∫mero 102 cai, o n√≥ com o n√∫mero 101 assume seu lugar e assim por diante. <br>  <b>K8SHA_KA_INTF</b> - interface de rede <b>mantida viva</b> .  O nome da interface que o keepalived escutar√°. </p><br><p>  As configura√ß√µes gerais para todos os masternodes s√£o as mesmas: </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160 - IP virtual do cluster. <br>  <b>K8SHA_IP1 ... K8SHA_IP3 -</b> endere√ßos <b>IP</b> de mestres <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3</b> - nomes de host para masternodes.  Um ponto importante, com esses nomes, o kubeadm ir√° gerar certificados. <br>  <b>K8SHA_KA_AUTH</b> - senha para keepalived.  Voc√™ pode especificar qualquer <br>  <b>K8SHA_TOKEN</b> - token de cluster.  Pode ser gerado com o comando <b>kubeadm token generate</b> <br>  <b>K8SHA_CIDR</b> - endere√ßo de sub-rede para lares.  Eu uso flanela para CIDR 0.244.0.0/16.  Certifique-se de rastrear - na configura√ß√£o deve ser K8SHA_CIDR = 10.244.0.0 \ / 16 </p></div></div><br><p>  Execute o script que ir√° configurar nginx, keepalived, etcd e kubeadmin </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p>  Come√ßamos etcd. </p><br>  etcd Eu levantei sem tls.  Se voc√™ precisar de tls, na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial do kubernetes</a> est√° escrito em detalhes como gerar certificados para o etcd. <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p>  Verifica√ß√£o de status </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Se tudo estiver bem, prossiga para a pr√≥xima etapa. </p><br><p>  <b>Configurar o kubeadmin</b> <br>  Se voc√™ estiver usando o kubeadm vers√£o 1.11, poder√° pular esta etapa <br>  Para que o kybernetes comece a trabalhar n√£o com o docker, mas com o containererderd, configure a configura√ß√£o do kubeadmin </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  Ap√≥s [Servi√ßo], adicione uma linha ao bloco </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Toda a configura√ß√£o deve ficar assim:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p>  Se voc√™ instalar a vers√£o 1.11 e quiser experimentar o CoreDNS em vez do kube-dns e testar a configura√ß√£o din√¢mica, remova o coment√°rio do seguinte bloco no arquivo de configura√ß√£o kubeadm-init.yaml: </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Reinicie o kubelet </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>Inicializa√ß√£o do primeiro assistente</b> </p><br><p>  Antes de iniciar o kubeadm, voc√™ precisa reiniciar o keepalived e verificar seu status </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ‚óè keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ‚îú‚îÄ4590 /usr/sbin/keepalived ‚îú‚îÄ4591 /usr/sbin/keepalived ‚îú‚îÄ4593 /usr/sbin/keepalived ‚îú‚îÄ5222 /usr/sbin/keepalived ‚îú‚îÄ5223 sh -c /etc/keepalived/check_apiserver.sh ‚îú‚îÄ5224 /bin/bash /etc/keepalived/check_apiserver.sh ‚îî‚îÄ5231 sleep 5</code> </pre> <br><p>  verifique se VIRTIP pings </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p>  Depois disso, execute o kubeadmin.  Certifique-se de incluir a linha --skip-preflight-checks.  Por padr√£o, o Kubeadmin procura por janela de encaixe e sem ignorar as verifica√ß√µes falhar√° com um erro. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  Ap√≥s o kubeadm funcionar, salve a linha gerada.  Ser√° necess√°rio inserir n√≥s de trabalho no cluster. </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p>  Em seguida, indique onde o arquivo admin.conf est√° armazenado <br>  Se trabalharmos como root, ent√£o: </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p>  Para um usu√°rio simples, siga as instru√ß√µes na tela. </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p>  Adicione mais 2 assistentes ao cluster.  Para fazer isso, copie os certificados de kube-master01 para kube-master02 e kube-master03 no diret√≥rio / etc / kubernetes /.  Para fazer isso, configurei o acesso ssh para root e, depois de copiar os arquivos, retornei as configura√ß√µes. </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p>  Ap√≥s copiar para o kube-master02 e o kube-master03, execute. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>Instalar flanela CIDR</b> </p><br><p>  no kube-master01 execute </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p>  A vers√£o atual do flanel pode ser encontrada na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes</a> . </p><br><p>  Aguardamos at√© que todos os cont√™ineres sejam criados. </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>Fazemos a replica√ß√£o do kube-dns para todos os tr√™s mestres</b> </p><br><p>  No kube-master01, execute </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>Instale e configure o nginx</b> </p><br><p>  Em cada n√≥ principal, instale o nginx como um balanceador para a API do Kubernetes <br>  Eu tenho todas as m√°quinas de cluster no debian.  Dos pacotes nginx, o m√≥dulo stream n√£o suporta, portanto, adicione os reposit√≥rios nginx e instale-o a partir dos reposit√≥rios nginx`a.  Se voc√™ tiver um sistema operacional diferente, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nginx</a> . </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p>  Crie a configura√ß√£o nginx (se ainda n√£o estiver criada) </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p>  usu√°rio nginx; <br>  worker_processes auto; </p><br><p>  error_log /var/log/nginx/error.log warn; <br>  pid /var/run/nginx.pid; </p><br><p>  eventos { <br>  worker_connections 1024; <br>  } </p><br><p>  http { <br>  inclua /etc/nginx/mime.types; <br>  default_type application / octet-stream; </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  } </p><br><p>  stream { <br>  apiserver a montante { <br>  servidor 172.26.133.161:6443 peso = 5 max_fails = 3 fail_timeout = 30s; <br>  servidor 172.26.133.162:6443 peso = 5 max_fails = 3 fail_timeout = 30s; <br>  servidor 172.26.133.163:6443 peso = 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  } </p></div></div><br><p>  Verificamos que est√° tudo bem e aplicamos a configura√ß√£o </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ‚óè nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ‚îú‚îÄ22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf ‚îî‚îÄ22134 nginx: worker process</code> </pre> <br><p>  Teste o balanceador </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>Configure o kube-proxy para funcionar com o balanceador</b> </p><br><p>  Ap√≥s a configura√ß√£o do balanceador, edite a porta nas configura√ß√µes do kubernetes. </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p>  Altere as configura√ß√µes do servidor para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://172.26.133.160:16443</a> <br>  Em seguida, voc√™ precisa configurar o kube-proxy para trabalhar com a nova porta </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p>  Exclu√≠mos todos os pods; ap√≥s a remo√ß√£o, eles s√£o recriados automaticamente com as novas configura√ß√µes </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>Incluindo N√≥s de Trabalho no Cluster</b> </p><br><p>  Em cada nota raiz, execute o comando gerado pelo kubeadm </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p>  Se a linha estiver "perdida", voc√™ precisar√° gerar uma nova </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p>  Nos n√≥s de trabalho nos arquivos /etc/kubernetes/bootstrap-kubelet.conf e /etc/kubernetes/kubelet.conf <br>  valor da vari√°vel do servidor para o nosso virtip </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p>  E reinicie o container e os kubernetes </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>Instala√ß√£o do painel</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p>  Crie um usu√°rio com privil√©gios de administrador: </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p>  Obtemos o token para entrada: </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p>  Configurando o acesso ao painel via NodePort no VIRTIP </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p>  Substitu√≠mos o valor de type: ClusterIP pelo tipo: NodePort e, na se√ß√£o port: adicionamos o valor de nodePort: 30000 (ou a porta no intervalo de 30000 a 32000 na qual voc√™ deseja que o painel esteja acess√≠vel): </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p>  O painel est√° agora dispon√≠vel em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https: // VIRTIP: 30000</a> </p><br><p>  <b>Heapster</b> </p><br><p>  Em seguida, instale o Heapster, uma ferramenta para obter m√©tricas dos componentes do cluster. </p><br><p>  Instala√ß√£o: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>Conclus√µes</b> </p><br><p>  N√£o notei nenhum problema especial ao trabalhar com o container.  Uma vez houve uma falha incompreens√≠vel com uma lareira ap√≥s a remo√ß√£o da implanta√ß√£o.  Kubernetes acreditava que sub foi exclu√≠do, mas sub tornou-se um "zumbi t√£o peculiar. Ele ainda existia no n√≥, mas no status estendido. </p><br><p>  Eu acredito que o Containerd √© mais orientado como um tempo de execu√ß√£o do cont√™iner para o kubernetes.  Muito provavelmente, no futuro, como um ambiente para o lan√ßamento de microsservi√ßos no Kubernetes, ser√° poss√≠vel e necess√°rio usar diferentes ambientes que ser√£o orientados para diferentes tarefas, projetos, etc. </p><br><p>  O projeto est√° se desenvolvendo muito r√°pido.  O Alibaba Cloud come√ßou a usar ativamente o conatinerd e enfatiza que √© o ambiente ideal para a movimenta√ß√£o de cont√™ineres. </p><br><p>  Segundo os desenvolvedores, a integra√ß√£o do container na plataforma de nuvem do Google Kubernetes agora √© equivalente √† integra√ß√£o do Docker. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Um bom exemplo do utilit√°rio crictl console</a> .  Tamb√©m darei alguns exemplos do cluster criado: </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  A CLI do Docker n√£o possui os conceitos b√°sicos do Kubernetes, por exemplo, pod e espa√ßo para nome, enquanto o crictl suporta esses conceitos. </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p>  E, se necess√°rio, podemos observar os cont√™ineres no formato usual, como janela de encaixe </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p>  Podemos ver as imagens que est√£o no n√≥ </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p>  Como se viu, a vida sem docker` √© :) </p><br><p>  √â muito cedo para falar sobre bugs e falhas, o cluster trabalha conosco h√° cerca de uma semana.  Em um futuro pr√≥ximo, o teste ser√° transferido para ele e, em caso de sucesso, provavelmente o suporte de um dos projetos.  Existe uma id√©ia sobre isso para escrever uma s√©rie de artigos que abordam os processos do DevOps, como: criar um cluster, configurar um controlador de entrada e mov√™-lo para separar n√≥s do cluster, automatizar a montagem de imagens, verificar imagens quanto a vulnerabilidades, implanta√ß√£o etc.  Enquanto isso, analisaremos a estabilidade do trabalho, procuraremos bugs e desenvolveremos novos produtos. </p><br><p>  Al√©m disso, este manual √© adequado para implantar um cluster de failover com docker, voc√™ s√≥ precisa instalar o docker de acordo com as instru√ß√µes da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial do Kubernetes</a> e pular as etapas para instalar o container e configurar o kubeadm. </p><br><p>  Ou voc√™ pode colocar o container e o docker simultaneamente no mesmo host e, como os desenvolvedores garantem, eles funcionar√£o perfeitamente juntos.  Containerd √© o ambiente de inicializa√ß√£o do cont√™iner do konbernetes e o docker √© como o docker))) </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br>  O reposit√≥rio containerd possui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um manual de instru√ß√µes</a> para configurar um cluster de assistente √∫nico.  Mas foi mais interessante para mim "elevar" o sistema com minhas m√£os, a fim de entender com mais detalhes a configura√ß√£o de cada componente e entender como ele funciona na pr√°tica. <br><p>  Talvez um dia minhas m√£os cheguem e eu escreverei meu manual para implantar um cluster com HA, pois nos √∫ltimos seis meses eu implantei mais de uma d√∫zia deles e provavelmente seria hora de automatizar o processo. </p><br><p>  Al√©m disso, ao escrever este artigo, a vers√£o kubernetes 1.11 foi lan√ßada.  Voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ler</a> sobre as principais altera√ß√µes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no blog Flant</a> ou no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">blog oficial do kubernetes</a> .  Atualizamos os clusters de teste para a vers√£o 1.11 e substitu√≠mos o kube-dns pelo CoreDNS.  Al√©m disso, eles inclu√≠ram a fun√ß√£o DynamicKubeletConfig para testar os recursos da atualiza√ß√£o din√¢mica de configura√ß√µes. </p><br><p>  Materiais utilizados: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Integra√ß√£o Kubernetes Containerd Goes GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Integra√ß√£o de cont√™ineres Kubernetes substituindo o Docker pronto para produ√ß√£o</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">containserd github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o do Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o NGINX</a> </li></ul><br><p>  Obrigado por ler at√© o fim. </p><br><p>  Como as informa√ß√µes sobre kubernetes, especialmente sobre clusters que operam em condi√ß√µes reais, s√£o muito escassas no RuNet, indica√ß√µes de imprecis√µes s√£o bem-vindas, assim como coment√°rios no esquema geral de implanta√ß√£o de cluster.  Vou tentar lev√°-los em considera√ß√£o e fazer as corre√ß√µes apropriadas.  E estou sempre pronto para responder perguntas nos coment√°rios, no githab e em qualquer rede social indicada no meu perfil. </p><br><p>  Atenciosamente, Eugene. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt415601/">https://habr.com/ru/post/pt415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt415591/index.html">Os engenheiros da Apple interceptam o teclado do MacBook Pro</a></li>
<li><a href="../pt415593/index.html">6 anos depois, uma nova vers√£o do lend√°rio CD de distribui√ß√£o de falhas Hiren</a></li>
<li><a href="../pt415595/index.html">M√©todos para aumentar a reten√ß√£o de jogadores usando jogos de slot como exemplo: Parte 1</a></li>
<li><a href="../pt415597/index.html">Postfix - amavisd-new sem host local ou servidor de correio de uma nova maneira</a></li>
<li><a href="../pt415599/index.html">A √çndia tamb√©m quer obter h√©lio-3</a></li>
<li><a href="../pt415605/index.html">Como salvamos o processamento de cart√µes com o Exadata</a></li>
<li><a href="../pt415611/index.html">Bibliotecas PKI: GCrypt e KSBA como uma alternativa ao OpenSSL com suporte para criptografia russa. Continua√ß√£o</a></li>
<li><a href="../pt415613/index.html">Por que voc√™ n√£o deve comprar lustres de LED</a></li>
<li><a href="../pt415615/index.html">Intera√ß√£o com o servidor por meio da API no iOS no Swift 3. Parte 2</a></li>
<li><a href="../pt415617/index.html">Usando a biblioteca FPC do InternetPCools no Delphi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>