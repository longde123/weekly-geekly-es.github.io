<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚öñÔ∏è ‚õπüèΩ üï∫üèª Que padr√µes as redes neurais encontram? üåé üò¶ üë©üèº‚Äçü§ù‚Äçüë®üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neste post, quero falar sobre os padr√µes que as redes neurais podem encontrar. Muitos guias para iniciantes enfocam a t√©cnica de escrever c√≥digo para ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Que padr√µes as redes neurais encontram?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467895/">  Neste post, quero falar sobre os padr√µes que as redes neurais podem encontrar.  Muitos guias para iniciantes enfocam a t√©cnica de escrever c√≥digo para redes neurais, enquanto quest√µes de "l√≥gica" (o que podem redes neurais? Quais arquiteturas s√£o mais adequadas para quais tarefas e por qu√™?) Muitas vezes permanecem √† margem.  Espero que meu post ajude os iniciantes a entender melhor os recursos das redes neurais.  Para fazer isso, tentaremos ver como eles lidam com algumas tarefas do modelo.  O c√≥digo de exemplo ser√° fornecido em python usando a biblioteca keras. <br><br>  <b>Tarefa 1.</b> Vamos come√ßar com uma simples.  Constru√≠mos uma rede neural aproximando-se do seno. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, np.pi, n) y = np.sin(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n = <span class="hljs-number"><span class="hljs-number">40</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">6</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">4</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">1000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">4</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=np.pi, num=<span class="hljs-number"><span class="hljs-number">500</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Sinus approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre> <br>  Temos o seguinte gr√°fico: <br><br><img src="https://habrastorage.org/webt/t3/xv/_o/t3xv_ocq-o9m8yupmxxdrfxqgai.png" width="500" height="500"><br><br>  Como voc√™ pode ver, a rede neural lidou com sucesso com a tarefa de aproximar uma fun√ß√£o simples. <br><a name="habracut"></a><br>  <b>Tarefa 2.</b> Vamos ver como a rede neural lidar√° com uma tarefa mais complexa.  Introduziremos valores x distribu√≠dos uniformemente no intervalo [0, 1], e y ser√° definido aleatoriamente: para x &lt;0,6, y ser√° uma vari√°vel aleat√≥ria assumindo o valor 0 com uma probabilidade de 0,75 e 1 com uma probabilidade de 0,25 (ou seja, um valor aleat√≥rio binomial com p = 0,25).  Para x&gt; 0,6, y ser√° uma vari√°vel aleat√≥ria assumindo o valor 0 com probabilidade 0,3 e o valor 1 com probabilidade 0,7.  Como uma fun√ß√£o otimizada, tomamos o erro padr√£o. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, n) y0 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.25</span></span>) y1 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.7</span></span>) y = np.where(X &lt; <span class="hljs-number"><span class="hljs-number">0.6</span></span>, y0, y1) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n_inputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n_hidden1 = <span class="hljs-number"><span class="hljs-number">100</span></span> n_hidden2 = <span class="hljs-number"><span class="hljs-number">50</span></span> n_outputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n = <span class="hljs-number"><span class="hljs-number">2000</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(n_hidden1, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(n_hidden2, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">200</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">100</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=<span class="hljs-number"><span class="hljs-number">1</span></span>, num=<span class="hljs-number"><span class="hljs-number">100</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Binomial distribution approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre><br>  Obtemos o seguinte gr√°fico de uma rede neural de fun√ß√£o aproximada: <br><br><img src="https://habrastorage.org/webt/y_/rp/lo/y_rplovhyaxioena0tb8iueed9m.png" width="500" height="500"><br><br>  Como voc√™ pode ver, a rede neural se aproximava da expectativa matem√°tica de nossa vari√°vel aleat√≥ria y.  Portanto, redes neurais podem (em princ√≠pio) aproximar os valores m√©dios de vari√°veis ‚Äã‚Äãaleat√≥rias que dependem dos par√¢metros.  Por exemplo, podemos esperar que eles resolvam o seguinte problema: pessoas com renda de at√© US $ 1.000 s√£o, em m√©dia, infelizes e pessoas com renda acima de US $ 1.000 s√£o, em m√©dia, felizes;  √© preciso aprender a prever o "n√≠vel de felicidade", dependendo da renda.  A rede neural ser√° capaz de encontrar a depend√™ncia do n√≠vel m√©dio de felicidade em rela√ß√£o √† renda, apesar do fato de que, entre as pessoas com qualquer n√≠vel de renda, s√£o felizes e infelizes. <br><br>  <b>Problema 3.</b> Agora nos voltamos para a previs√£o de sequ√™ncias.  Consideraremos seq√º√™ncias de 0 e 1 dadas pela seguinte regra: 10 membros - equiprobably 0 ou 1, e o d√©cimo primeiro igual a 1 se o termo anterior for 0 e igualmente prov√°vel 0 ou 1 se o termo anterior 1. Geraremos essas sequ√™ncias de comprimento 11 (entrada 10) membros da sequ√™ncia e um, o √∫ltimo, prevemos) e trein√°-los em nossa rede neural recorrente.  E ap√≥s o treinamento, vamos verificar como ela lida com a previs√£o de novas sequ√™ncias (tamb√©m o comprimento 11). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(m, n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.binomial(size=(m,n), n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y0 = np.ones(m) y1 = np.random.binomial(size=m, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y = np.where(X[:, n<span class="hljs-number"><span class="hljs-number">-1</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>, y0, y1) X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">50</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">20</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) m_test = <span class="hljs-number"><span class="hljs-number">12</span></span> n_test = <span class="hljs-number"><span class="hljs-number">10</span></span> X_test, y_test = get_X_y(m_test, n_test) y_predicted = model.predict(X_test) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(m_test): print(<span class="hljs-string"><span class="hljs-string">"x_last="</span></span>, X_test[i, n_test<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-string"><span class="hljs-string">"y_predicted="</span></span>, y_predicted[i, <span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Vamos ver quais previs√µes nossa rede neural fornece sobre as seq√º√™ncias testadas (seus resultados ser√£o diferentes, pois aqui a aleatoriedade est√° presente tanto na escolha das sequ√™ncias quanto no treinamento da rede neural). <br><br><div class="scrollable-table"><table><tbody><tr><th>  N√∫mero de sequ√™ncia </th><th>  Pen√∫ltimo membro da sequ√™ncia </th><th>  Valor previsto </th></tr><tr><td>  1 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  2 </td><td>  0 0 </td><td>  0,95 </td></tr><tr><td>  3 </td><td>  0 0 </td><td>  0,97 </td></tr><tr><td>  4 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  5 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  6 </td><td>  1 </td><td>  0,45 </td></tr><tr><td>  7 </td><td>  0 0 </td><td>  0,94 </td></tr><tr><td>  8 </td><td>  1 </td><td>  0,50 </td></tr><tr><td>  9 </td><td>  0 0 </td><td>  0,96 </td></tr><tr><td>  10 </td><td>  1 </td><td>  0,42 </td></tr><tr><td>  11 </td><td>  1 </td><td>  0,44 </td></tr><tr><td>  12 </td><td>  0 0 </td><td>  0,92 </td></tr></tbody></table></div><br><br>  Como voc√™ pode ver, se o pen√∫ltimo membro da sequ√™ncia for 0, a rede neural prediz um valor pr√≥ximo a 1 e, se for 1, ent√£o um valor pr√≥ximo a 0,5.  Isso est√° pr√≥ximo da previs√£o ideal.  Um exemplo semelhante de "vida" poderia ser assim: "se eu for ao cinema hoje, amanh√£ amanh√£ vou almo√ßar em um restaurante;  se vou ao teatro hoje, amanh√£ almo√ßarei em qualquer lugar. "  Como vimos, uma rede neural pode capturar padr√µes desse tipo e prever uma ida a um restaurante indo ao cinema (e indo ao cinema para prever "algo intermedi√°rio"). <br><br>  <b>Tarefa 4.</b> N√≥s complicamos a tarefa da rede neural.  Seja tudo como no exemplo anterior, apenas o d√©cimo primeiro membro da sequ√™ncia ser√° determinado n√£o pelo anterior, mas pelo segundo membro da sequ√™ncia (pela mesma regra).  N√£o forneceremos o c√≥digo aqui, pois ele praticamente n√£o difere do anterior.  Meu experimento mostrou que a rede neural ainda encontra um padr√£o, mas por mais tempo (eu tive que usar 100 √©pocas em vez de 20 para treinamento). <br>  Assim, as redes neurais podem (novamente, em princ√≠pio, esclarecer) capturar depend√™ncias razoavelmente de longo prazo (em nosso "exemplo de vida", elas podem capturar padr√µes como "hoje vou a um restaurante se estivesse em um filme h√° uma semana"). <br><br>  <b>Tarefa 5.</b> Vamos ver como a rede neural usa as informa√ß√µes dispon√≠veis para previs√£o. <br>  Para fazer isso, realizaremos treinamento em seq√º√™ncias de comprimento 4. No total, teremos 3 sequ√™ncias igualmente prov√°veis ‚Äã‚Äãdiferentes: <br><br> <code>0, 0, 1, 1 <br> 0, 1, 0, 1 <br> 0, 1, 1, 0</code> <br> <br>  Assim, ap√≥s a combina√ß√£o inicial de 0, 0, sempre encontramos duas unidades; ap√≥s a combina√ß√£o de 0, 1, temos a mesma probabilidade de encontrar 0 ou 1, mas saberemos com certeza o √∫ltimo n√∫mero.  Agora pediremos √† nossa rede neural que retorne sequ√™ncias definindo return_sequences = True.  Como as seq√º√™ncias previstas, tomamos as mesmas seq√º√™ncias deslocadas em uma etapa e suplementadas por zero √† direita.  Agora j√° podemos assumir o que acontecer√°: na primeira etapa, a rede neural produzir√° um n√∫mero pr√≥ximo a 2/3 (j√° que com uma probabilidade de 2/3, o segundo termo √© 1) e, em seguida, para uma combina√ß√£o de 0, produzir√° dois n√∫meros pr√≥ximos a unidade, e para 0, 1 primeiro, ele fornecer√° um n√∫mero pr√≥ximo a 0,5 e, em seguida, fornecer√° um n√∫mero pr√≥ximo a 0 ou 1, dependendo de termos a sequ√™ncia 0, 1, 0 ou 0, 1, 1. No final da rede neural sempre produzir√° um n√∫mero pr√≥ximo a 0. A verifica√ß√£o com o c√≥digo a seguir mostra que nossas suposi√ß√µes est√£o corretas. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) z = np.array([random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n)]) X[z == <span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] y = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) y[:, :<span class="hljs-number"><span class="hljs-number">3</span></span>] = X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>:] X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) y = np.reshape(y, (y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], y.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">20</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">100</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) X_test = np.zeros((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>)) X_test[<span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] X_test = np.reshape(X_test, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) y_predicted = model.predict(X_test) print(y_predicted)</code> </pre><br><br>  A partir deste exemplo, vemos que a rede neural pode alterar dinamicamente a previs√£o, dependendo das informa√ß√µes recebidas.  Far√≠amos o mesmo, tentando prever uma certa sequ√™ncia: quando as informa√ß√µes dispon√≠veis nos permitem estimar as probabilidades de resultados na pr√≥xima etapa, previmos com base nessas informa√ß√µes;  mas, quando descobrirmos informa√ß√µes adicionais na pr√≥xima etapa, alteramos a previs√£o dependendo dela. <br>  Ent√£o, se vemos que algu√©m est√° vindo para n√≥s do escuro, dizemos "essa √© uma pessoa, n√£o sabemos com mais detalhes";  quando come√ßamos a distinguir cabelos longos no escuro, dizemos "provavelmente √© uma mulher".  Mas se depois considerarmos que uma pessoa tem bigode, dizemos que esse provavelmente √© um homem (embora com cabelos compridos).  Como vimos, uma rede neural age de maneira semelhante, usando a totalidade das informa√ß√µes atualmente dispon√≠veis para previs√£o. <br><br>  Ent√£o, analisamos exemplos simples de como as redes neurais funcionam e quais padr√µes eles podem encontrar.  Em geral, vimos que as redes neurais geralmente se comportam "razoavelmente", fazendo previs√µes pr√≥ximas daquelas que uma pessoa faria.  Embora, note-se, para capturar padr√µes simples, eles precisam de muito mais dados do que pessoas. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt467895/">https://habr.com/ru/post/pt467895/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt467881/index.html">Tutu.ru backend meetup</a></li>
<li><a href="../pt467883/index.html">Kubernetes 1.16 - como atualizar e n√£o quebrar nada</a></li>
<li><a href="../pt467885/index.html">Minas de produtos e segmentos</a></li>
<li><a href="../pt467891/index.html">Perguntas frequentes sobre assinatura [eletr√¥nica] na nuvem</a></li>
<li><a href="../pt467893/index.html">Apenas mais um inv√≥lucro Qt para gRPC e protobuf</a></li>
<li><a href="../pt467897/index.html">Ferramentas de teste autom√°tico, integra√ß√£o com o Yandex Mapkit 3, design interessante e abordagem da interface do usu√°rio orientada a servidor - Android mitap</a></li>
<li><a href="../pt467901/index.html">Refute quatro estere√≥tipos sobre a linguagem de programa√ß√£o Rust</a></li>
<li><a href="../pt467905/index.html">Como fizemos o reconhecimento de refer√™ncia no Cloud Mail.ru e por que</a></li>
<li><a href="../pt467907/index.html">Pr√≥s e contras da terceiriza√ß√£o</a></li>
<li><a href="../pt467909/index.html">Bate-papo no iOS: usando soquetes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>