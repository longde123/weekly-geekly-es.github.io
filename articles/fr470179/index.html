<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë¶‚Äçüë¶ ü•Ñ üëë PDDM - Nouvel algorithme d'apprentissage par renforcement bas√© sur un mod√®le avec planificateur avanc√© ü¶ë üßöüèø ü•´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'apprentissage par renforcement est divis√© en deux grandes classes: sans mod√®le et √† base de mod√®le. Dans le premier cas, les actions sont optimis√©es...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PDDM - Nouvel algorithme d'apprentissage par renforcement bas√© sur un mod√®le avec planificateur avanc√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470179/"><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  L'apprentissage par renforcement est divis√© en deux grandes classes: sans mod√®le et √† base de mod√®le.  Dans le premier cas, les actions sont optimis√©es directement par le signal de r√©compense, et dans le second, le r√©seau neuronal n'est qu'un mod√®le de r√©alit√©, et les actions optimales sont s√©lectionn√©es √† l'aide d'un ordonnanceur externe.  Chaque approche a ses avantages et ses inconv√©nients. </p><br><p>  Les d√©veloppeurs de Berkeley et de Google Brain ont pr√©sent√© l'algorithme PDDM bas√© sur un mod√®le avec un ordonnanceur am√©lior√©, qui vous permet d'apprendre efficacement des mouvements complexes avec un grand nombre de degr√©s de libert√© dans un petit nombre d'exemples.  Pour apprendre √† faire tourner des balles dans un bras robotis√© avec des articulations de doigts r√©alistes avec 24 degr√©s de libert√©, il n'a fallu que 4 heures de pratique sur un vrai robot physique. </p><a name="habracut"></a><br><p>  L'apprentissage par renforcement est la formation de robots avec un signal de r√©compense.  C'est semblable √† la fa√ßon dont les √™tres vivants apprennent.  Mais le probl√®me est compliqu√© par le fait qu'on ne sait pas comment changer les poids du r√©seau neuronal de sorte que ses actions propos√©es conduisent √† une augmentation des r√©compenses.  Par cons√©quent, dans l'apprentissage par renforcement, les m√©thodes de formation de r√©seau de neurones conventionnelles ne conviennent pas.  Apr√®s tout, on ne sait pas exactement ce qu'elle devrait donner √† sa sortie, ce qui signifie qu'il est impossible de trouver une erreur entre sa pr√©diction et l'√©tat r√©el des choses.  Pour ignorer cette diff√©rence √† travers les couches du r√©seau neuronal et modifier les poids entre les neurones pour minimiser cette erreur.  Il s'agit d'un algorithme classique de r√©tropropagation enseign√© par les r√©seaux de neurones. </p><br><p>  Par cons√©quent, les scientifiques ont invent√© plusieurs fa√ßons de r√©soudre ce probl√®me. </p><br><h2 id="model-free">  Sans mod√®le </h2><br><p>  L'une des approches les plus efficaces √©tait le mod√®le acteur-critique.  Laisser un r√©seau neuronal (acteur) √† son entr√©e recevoir l'√©tat de l'environnement de l'√©tat, et √† la sortie, √©mettre des actions qui devraient conduire √† une augmentation des r√©compenses.  Jusqu'√† pr√©sent, ces actions sont al√©atoires et d√©pendent simplement du flux de signaux au sein du r√©seau, car le r√©seau neuronal n'a pas encore √©t√© form√©.  Et le deuxi√®me r√©seau neuronal (critique), laisse l'entr√©e √©galement recevoir l'√©tat de l'environnement d'√©tat, mais aussi les actions de la sortie du premier r√©seau.  Et √† la sortie, ne laissez que la r√©compense pr√©vue, qui sera re√ßue si ces actions sont appliqu√©es. </p><br><p>  Maintenant, surveillez vos mains: nous ne savons pas quelles devraient √™tre les meilleures actions √† la sortie du premier r√©seau, ce qui entra√Ænerait une augmentation de la r√©compense.  Par cons√©quent, en utilisant l'algorithme de propagation inverse, nous ne pouvons pas le former.  Mais le deuxi√®me r√©seau neuronal peut tr√®s bien pr√©dire la valeur exacte de la r√©compense (ou plut√¥t, g√©n√©ralement son changement), qu'il recevra si des actions sont maintenant appliqu√©es.  Prenons donc le gradient de changement d'erreur du deuxi√®me r√©seau et appliquons-le au premier!  Ainsi, vous pouvez former le premier r√©seau neuronal par la m√©thode classique de propagation arri√®re d'erreur.  Nous prenons simplement l'erreur non pas des sorties du premier r√©seau, mais des sorties du second. </p><br><p>  En cons√©quence, le premier r√©seau neuronal apprend √† √©mettre des actions optimales conduisant √† une augmentation des r√©compenses.  Parce que si le critique critique a fait une erreur et a pr√©dit une r√©compense inf√©rieure √† ce qu'elle s'est av√©r√©e √™tre en r√©alit√©, alors le gradient de cette diff√©rence va d√©placer les actions de l'acteur dans la direction afin que le critique pr√©dit plus pr√©cis√©ment la r√©compense.  Et cela signifie vers des actions plus optimales (apr√®s tout, cela conduira au fait que le critique pr√©dit avec pr√©cision une r√©compense plus √©lev√©e).  Un principe similaire fonctionne dans la direction oppos√©e: si le critique surestime la r√©compense attendue, la diff√©rence entre l'attente et la r√©alit√© r√©duira les sorties d'actions du premier r√©seau neuronal, ce qui a conduit √† cette indication de r√©compense surestim√©e du deuxi√®me r√©seau. </p><br><p>  Comme vous pouvez le voir, dans ce cas, les actions sont optimis√©es directement par le signal de r√©compense.  C'est l'essence commune de tous les algorithmes sans mod√®le dans l'apprentissage par renforcement.  Ils sont √† la pointe de la technologie en ce moment. </p><br><p>  Leur avantage est que des actions optimales sont recherch√©es par descente de gradient, donc, finalement, les plus optimales sont trouv√©es.  Ce qui signifie montrer le meilleur r√©sultat.  Un autre avantage est la possibilit√© d'utiliser de petits r√©seaux de neurones (et donc plus faciles √† apprendre).  Si, parmi toute la vari√©t√© des facteurs environnementaux, certains sont sp√©cifiques pour r√©soudre le probl√®me, la descente de gradient est tout √† fait capable de les identifier.  Et utilisez pour r√©soudre le probl√®me.  Ces deux avantages ont assur√© le succ√®s des m√©thodes directes sans mod√®le. </p><br><p>  Mais ils ont aussi des inconv√©nients.  √âtant donn√© que les actions sont enseign√©es directement par le signal de r√©compense, de nombreux exemples de formation sont n√©cessaires.  Des dizaines de millions, m√™me pour des cas tr√®s simples.  Ils travaillent mal sur des t√¢ches avec un grand nombre de degr√©s de libert√©.  Si l'algorithme ne parvient pas imm√©diatement √† identifier les facteurs cl√©s dans le paysage de grande dimension, il n'apprendra probablement pas du tout.  Les m√©thodes sans mod√®le peuvent √©galement exploiter les vuln√©rabilit√©s du syst√®me, en se concentrant sur une action non optimale (si la descente du gradient y converge), en ignorant d'autres facteurs environnementaux.  Pour des t√¢ches sans mod√®le, m√™me l√©g√®rement diff√©rentes, les m√©thodes doivent √™tre enti√®rement r√©entra√Æn√©es. </p><br><h2 id="model-based">  Bas√© sur un mod√®le </h2><br><p>  Les m√©thodes bas√©es sur un mod√®le dans l'apprentissage par renforcement sont fondamentalement diff√©rentes de l'approche d√©crite ci-dessus.  Dans Model-Based, un r√©seau de neurones ne pr√©dit que ce qui va se passer ensuite.  Ne proposant aucune action.  C'est-√†-dire qu'il s'agit simplement d'un mod√®le de r√©alit√© (d'o√π le ¬´mod√®le¬ª - bas√© sur le nom).  Et pas du tout un syst√®me de prise de d√©cision. </p><br><p>  Les r√©seaux de neurones bas√©s sur des mod√®les sont aliment√©s par l'√©tat actuel de l'environnement d'√©tat et les actions que nous voulons effectuer.  Et le r√©seau neuronal pr√©dit la fa√ßon dont l'√©tat changera √† l'avenir apr√®s l'application de ces actions.  Elle peut √©galement pr√©dire quelle r√©compense sera le r√©sultat de ces actions.  Mais cela n'est pas n√©cessaire, car la r√©compense peut g√©n√©ralement √™tre calcul√©e √† partir d'un √©tat bien connu.  De plus, cet √©tat de sortie peut √™tre r√©inject√© √† l'entr√©e du r√©seau neuronal (avec les nouvelles actions propos√©es), et ainsi pr√©dire de mani√®re r√©cursive les changements dans l'environnement externe de nombreux pas en avant. </p><br><p>  Les r√©seaux de neurones bas√©s sur des mod√®les sont tr√®s faciles √† apprendre.  Puisqu'ils pr√©disent simplement comment le monde va changer, sans faire de suggestions quelles actions optimales devraient √™tre pour que la r√©compense augmente.  Par cons√©quent, le r√©seau neuronal bas√© sur un mod√®le utilise tous les exemples existants pour sa formation, et pas seulement ceux qui entra√Ænent une augmentation ou une diminution des r√©compenses, comme c'est le cas dans Model-Free.  C'est la raison pour laquelle les r√©seaux de neurones bas√©s sur des mod√®les ont besoin de beaucoup moins d'exemples de formation. </p><br><p>  Le seul inconv√©nient est que le r√©seau neuronal bas√© sur un mod√®le devrait √©tudier la dynamique r√©elle du syst√®me et devrait donc avoir une capacit√© suffisante pour cela.  Un r√©seau neuronal sans mod√®le peut converger sur des facteurs cl√©s, ignorant le reste, et donc √™tre un petit r√©seau simple (si la t√¢che est en principe r√©solue par moins de ressources). </p><br><p>  Un autre grand avantage, en plus de la formation sur un plus petit nombre d'exemples, est qu'en tant que mod√®le universel du monde, un seul r√©seau neuronal bas√© sur un mod√®le peut √™tre utilis√© pour r√©soudre un certain nombre de probl√®mes dans ce monde. </p><br><p>  Le probl√®me principal de l'approche bas√©e sur un mod√®le est de savoir quelles actions les actions doivent √™tre appliqu√©es √† l'entr√©e des r√©seaux de neurones?  Apr√®s tout, le r√©seau neuronal lui-m√™me n'offre aucune action optimale. </p><br><p>  Le moyen le plus simple consiste √† traverser un tel r√©seau neuronal des dizaines de milliers d'actions al√©atoires et √† choisir celles pour lesquelles le r√©seau neuronal pr√©dira la plus grande r√©compense.  Il s'agit d'un apprentissage par renforcement classique bas√© sur un mod√®le.  Cependant, avec de grandes dimensions et de longues cha√Ænes de temps, le nombre d'actions possibles s'av√®re trop important pour les trier toutes (ou m√™me deviner au moins un peu optimal). </p><br><p>  Pour cette raison, les m√©thodes bas√©es sur un mod√®le sont g√©n√©ralement inf√©rieures √† celles sans mod√®le, qui par descente de gradient convergent directement vers les actions les plus optimales. </p><br><p>  Une version am√©lior√©e applicable aux mouvements en robotique n'est pas d'utiliser des actions al√©atoires, mais de conserver le mouvement pr√©c√©dent, ajoutant de l'al√©atoire √† la distribution normale.  Comme les mouvements des robots sont g√©n√©ralement fluides, cela r√©duit le nombre de bustes.  Mais en m√™me temps, un changement important et important peut √™tre manqu√©. </p><br><p>  L'option de d√©veloppement finale pour cette approche peut √™tre consid√©r√©e comme l'option CEM, qui n'utilise pas une distribution normale fixe qui introduit le caract√®re al√©atoire dans le chemin actuel des actions, mais s√©lectionne les param√®tres de la distribution al√©atoire en utilisant l'entropie crois√©e.  Pour ce faire, une population de calculs d'actions est lanc√©e et les meilleurs d'entre eux sont utilis√©s pour affiner la diffusion des param√®tres dans la prochaine g√©n√©ration.  Quelque chose comme un algorithme √©volutif. </p><br><h2 id="pddm">  PDDM </h2><br><p>  Une si longue introduction √©tait n√©cessaire pour expliquer ce qui se passe dans le nouvel algorithme d'apprentissage par renforcement propos√© bas√© sur un mod√®le PDDM.  Apr√®s avoir lu un article sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">blog Berkeley AI</a> (ou une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">version √©tendue</a> ), et m√™me l'article original <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1909.11652</a> , cela n'√©tait peut-√™tre pas √©vident. </p><br><p>  La m√©thode PDDM r√©p√®te l'id√©e de CEM lors du choix des actions al√©atoires qui doivent √™tre ex√©cut√©es via un r√©seau neuronal bas√© sur un mod√®le afin de s√©lectionner les actions avec la r√©compense pr√©visible la plus √©lev√©e.  Ce n'est qu'au lieu de s√©lectionner des param√®tres de distribution al√©atoire, comme cela se fait dans CEM, que PDDM utilise une corr√©lation temporelle entre les actions et une r√®gle plus souple pour mettre √† jour la distribution al√©atoire.  La formule est donn√©e dans l'article d'origine.  Cela vous permet de v√©rifier un plus grand nombre d'actions appropri√©es sur de longues distances, surtout si les mouvements n√©cessitent une coordination pr√©cise.  De plus, les auteurs de l'algorithme filtrent les candidats pour les actions, obtenant ainsi une trajectoire plus fluide des mouvements. </p><br><p>  En termes simples, les d√©veloppeurs ont simplement propos√© une meilleure formule pour choisir des actions al√©atoires √† tester dans l'apprentissage par renforcement classique bas√© sur un mod√®le. </p><br><p>  Mais le r√©sultat √©tait tr√®s bon. </p><br><p>  En seulement 4 heures d'entra√Ænement sur un vrai robot, un robot √† 24 degr√©s de libert√© a appris √† tenir deux balles et √† les faire tourner dans les paumes sans les faire tomber.  Un r√©sultat inaccessible pour toutes les m√©thodes modernes sans mod√®le avec un si petit nombre d'exemples. </p><br><p>  Fait int√©ressant, pour la formation, ils ont utilis√© un deuxi√®me bras de robot √† 7 degr√©s de libert√©, qui a ramass√© les balles tomb√©es et les a renvoy√©es au bras du robot principal: </p><br><p><img src="https://habrastorage.org/webt/7s/yf/wq/7syfwqr2vg2-rlzupbccu-06mic.gif"></p><br><p>  En cons√©quence, apr√®s 1-2 heures, le roboruk pouvait tenir les balles en toute confiance et les d√©placer dans la paume de sa main, et 4 heures √©taient suffisantes pour un entra√Ænement complet. </p><br><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  Faites attention aux mouvements de contraction des doigts.  Il s'agit d'une caract√©ristique des approches bas√©es sur un mod√®le.  Les actions envisag√©es √©tant choisies au hasard, elles ne co√Øncident pas toujours avec les optimales.  L'algorithme sans mod√®le pourrait potentiellement converger vers des mouvements fluides vraiment optimaux. </p><br><p>  Cependant, l'approche bas√©e sur un mod√®le permet avec un r√©seau de neurones form√© de mod√©liser le monde pour r√©soudre diff√©rents probl√®mes sans recyclage.  Il y a plusieurs exemples dans l'article, par exemple, vous pouvez facilement changer le sens de rotation des boules dans la main (dans Model-Free, vous devrez r√©entra√Æner le r√©seau neuronal pour cela).  Ou tenez le ballon √† un endroit pr√©cis dans la paume de votre main, en suivant le point rouge. </p><br><p><img src="https://habrastorage.org/webt/np/2a/4v/np2a4vibrlvwohr_kimp7w4o_1u.gif"></p><br><p>  Vous pouvez √©galement faire en sorte que Roboruk dessine des trajectoires arbitraires avec un crayon, ce qui est une t√¢che tr√®s difficile pour les m√©thodes sans mod√®le. </p><br><p><img src="https://habrastorage.org/webt/iz/a_/v5/iza_v5a2jkohqpcl8vb99_yfwri.gif"></p><br><p>  Bien que l'algorithme propos√© ne soit pas une panac√©e, et ne soit m√™me pas un algorithme d'IA au sens plein du mot (dans PDDM, le r√©seau neuronal remplace simplement le mod√®le analytique, et les d√©cisions sont prises par recherche al√©atoire avec une r√®gle d√©licate qui r√©duit le nombre d'√©num√©ration des options), il peut √™tre utile en robotique.  Puisqu'il a montr√© une am√©lioration notable des r√©sultats et est form√© sur un tr√®s petit nombre d'exemples. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr470179/">https://habr.com/ru/post/fr470179/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr470167/index.html">Conf√©rence pour ceux qui s'int√©ressent √† la science avant qu'elle ne devienne un courant dominant</a></li>
<li><a href="../fr470169/index.html">Comment emp√™cher l'id√©e de mourir et de rassembler une √©quipe qui ne la tuera pas</a></li>
<li><a href="../fr470171/index.html">Habr Weekly # 21 / Dobroshrift, technodom pour un chat, le droit de r√©parer des appareils √©lectrom√©nagers, l'Union europ√©enne et les cookies ¬´transparents¬ª</a></li>
<li><a href="../fr470173/index.html">Plateforme d'int√©gration en tant que service</a></li>
<li><a href="../fr470175/index.html">Ajouter Se connecter avec Apple au back-end</a></li>
<li><a href="../fr470181/index.html">Fonctionnement de la m√©thode Levenberg-Marquardt</a></li>
<li><a href="../fr470187/index.html">La fourchette de prix pour la conception et la conception d'un service en ligne est de 100 000 √† 5 millions de roubles. Raisons</a></li>
<li><a href="../fr470189/index.html">Envoi de messages poste √† poste avec PeerJS</a></li>
<li><a href="../fr470191/index.html">Web R√©solution de probl√®mes avec r0ot-mi. Partie 1</a></li>
<li><a href="../fr470193/index.html">Protection universelle contre les attaques xss et les injections sql</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>