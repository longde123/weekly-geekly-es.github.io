<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏧 🚐 🤞🏻 如果您没有Python，但是有Keras模型和Java 🌗 📠 🍼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="大家好！ 今天，在构建ML模型中，Python处于领导地位，并在数据科学专家社区中广受欢迎[ 1 ]。 

 像大多数开发人员一样，Python的简洁和简洁的语法吸引了我们。 我们使用它来使用人工神经网络解决机器学习问题。 但是，实际上，产品开发语言并不总是使用Python，这需要我们解决其他集成问...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>如果您没有Python，但是有Keras模型和Java</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/naumen/blog/475338/"> 大家好！ 今天，在构建ML模型中，Python处于领导地位，并在数据科学专家社区中广受欢迎[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">1</a> ]。 <br><br> 像大多数开发人员一样，Python的简洁和简洁的语法吸引了我们。 我们使用它来使用人工神经网络解决机器学习问题。 但是，实际上，产品开发语言并不总是使用Python，这需要我们解决其他集成问题。 <br><br> 在本文中，我将讨论当需要将Python的Keras模型与Java关联时所使用的解决方案。 <br><br> 我们要注意的是： <br><br><ul><li> 功能捆绑Keras模型和Java； </li><li> 准备与DeepLearning4j框架（简称DL4J）一起使用； </li><li> 将Keras模型导入DL4J（请注意，本节包含了许多见解）-如何注册图层，导入模块的局限性，如何检查工作结果。 </li></ul><br> 为什么要阅读？ <br><br><ul><li> 为了节省启动时间，如果您将面临类似集成的任务； </li><li> 找出我们的解决方案是否适合您，以及您是否可以重用我们的经验。 </li></ul><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2m/2k/xj/2m2kxjbwgahv1gx0wejstn2qpjw.png" alt="图片替代"></div><br> 深度学习框架重要性的整体特征[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">2</a> ]。 <br><br> 最受欢迎的深度学习框架的摘要可在此处[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">3</a> ]和此处[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">4</a> ]中找到。 <br><br> 如您所见，这些框架中的大多数都基于Python和C ++：它们使用C ++作为内核来加速基本和高负荷操作，并使用Python作为交互接口来加速开发。 <br><br> 实际上，许多开发语言都更加广泛。  Java是大型企业和组织产品开发的领导者。 一些流行的神经网络框架具有JNI / JNA绑定程序形式的Java端口，但是在这种情况下，需要为每种体系结构构建一个项目，并且需要Java在跨平台模糊问题上的优势。 这种细微差别在复制解决方案中可能非常重要。 <br><br> 另一种替代方法是使用Jython编译为Java字节码。 但这里有一个缺点-仅支持Python的第二版本，以及使用第三方Python库的能力有限。 <br><br> 为了简化Java中神经网络解决方案的开发，正在开发DeepLearning4j框架（简称DL4J）。  DL4除了Java API外，还提供了一组预训练的模型[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">5</a> ]。 通常，此开发工具很难与TensorFlow竞争。  TensorFlow在更详细的文档和大量示例，技术能力，社区规模和快速开发方面胜过DL4J。 尽管如此，Skymind坚持的趋势还是很有希望的。  Java中尚未发现该工具的重要竞争对手。 <br><br>  DL4J库是少数几个（如果不是唯一的）库，可以导入Keras模型；它在功能上扩展了Keras熟悉的层[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">6</a> ]。  DL4J库包含一个目录，其中包含神经网络ML模型的实现示例（dl4j-example）。 在我们的案例中，用Java实现这些模型的精妙之处并不那么有趣。 将更加详细地关注使用DL4J方法将经过训练的Keras / TF模型导入Java。 <br><br><h1> 开始使用 </h1><br> 在开始之前，您需要安装必要的程序： <br><br><ol><li>  Java版本1.7（64位版本）及更高版本。 </li><li>  Apache Maven项目构建系统。 </li><li>  IDE可供选择：Intellij IDEA，Eclipse，Netbeans。 开发人员建议使用第一个选项，此外，还将讨论可用的培训示例。 </li><li>  Git（用于将项目克隆到您的PC）。 </li></ol><br> 在此处[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">7</a> ]或在视频[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">8</a> ]中可以找到带有启动示例的详细说明。 <br><br> 要导入模型，DL4J开发人员建议使用<em>KerasModelImport</em>导入<em>模块</em> （于2016年10月出现）。 模块的功能支持Keras的两种模型架构-它是顺序的（Java中的模拟-类MultiLayerNetwork）和功能性（Java中的模拟-类ComputationGraph）。 该模型将以HDF5格式整体导入或导入2个单独的文件-具有h5扩展名的模型权重和包含神经网络体系结构的json文件。 <br><br> 为了快速起步，DL4J开发人员针对类型为Sequential [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">9</a> ]的模型在Fisher Fisher数据集上准备了一个简单示例的分步分析。 从以两种方式导入模型的角度考虑了另一个培训示例（1：以完整的HDF5格式； 2：在单独的文件中-模型权重（h5扩展）和体系结构（json扩展）），然后比较Python和Java模型的结果[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">10</a> ]。 到此结束了对导入模块实用功能的讨论。 <br><br>  Java中也有TF，但是它处于实验状态，开发人员没有对其稳定操作的任何保证[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">11</a> ]。 版本存在问题，并且Java中的TF具有不完整的API-这就是为什么此处不考虑此选项的原因。 <br><br><h1> 原始Keras / TF模型的功能： </h1><br> 导入神经网络非常简单。 在代码中，我们将更详细地分析将神经网络与更复杂的体系结构集成的示例。 <br><br> 您不应该讨论此模型的实际方面，它是从考虑层（尤其是Lambda层的注册），导入模块的某些细微之处和局限性以及整个DL4J的角度进行指示的。 实际上，提到的细微差别可能需要调整网络体系结构，或者完全放弃通过DL4J启动模型的方法。 <br><br> 型号特点： <br><br>  <b>1.</b>模型类型-功能（具有分支的网络）； <br><br>  <b>2.</b>选择小的训练参数（批的大小，时代数）：批的大小-100，时代数-10，每个时代的步长-10； <br><br>  <b>3.</b> 13层，各层的摘要如图所示： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/85/of/hn/85ofhnioy_usez2msaeqpt2whja.png" alt="图片替代"></div><br><div class="spoiler">  <b class="spoiler_title">简短说明</b> <div class="spoiler_text"><ol><li>  input_1-输入层，接受二维张量（由矩阵表示）； </li><li>  lambda_1-在我们的示例中，用户层使张量在TF中的填充具有相同的数值； </li><li>  embedding_1-为文本数据的输入序列构建嵌入（矢量表示）（将2-D张量转换为3-D）； </li><li>  conv1d_1-一维卷积层； </li><li>  lstm_2-LSTM层（在embed_1（第3号）层之后进行）； </li><li>  lstm_1-LSTM层（在conv1d（第4号）层之后）； </li><li>  lambda_2是在lstm_2（第5号）层之后截断张量的用户层（与lambda_1（第2号）层的填充相反的操作）； </li><li>  lambda_3是在lstm_1（第6号）和conv1d_1（第4号）层之后张量被截断的用户层（与lambda_1（第2号）层中的填充相反的操作）； </li><li>  concatenate_1-截断的（No. 7）和（No. 8）层的粘合； </li><li>  density_1-8个神经元的完全连接层和指数线性激活函数“ elu”； </li><li>  batch_normalization_1-标准化层； </li><li>  density_2-1个神经元和乙状结肠激活功能“乙状结肠”的完全连接层； </li><li>  lambda_4-用户层，在该用户层执行前一层的压缩（TF中的压缩）。 </li></ol></div></div><br>  <b>4.</b>损失函数-binary_crossentropy <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msubsup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF08;</mo></mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF08;</mo></mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF09;</mo></mrow><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF08;</mo></mrow><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF09;</mo></mrow><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF08;</mo></mrow><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF09;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#xFF09;</mo></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="83.652ex" height="2.901ex" viewBox="0 -883.9 36016.8 1249" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6F" x="298" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-73" x="784" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-73" x="1253" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-3D" x="2000" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-2212" x="3057" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-66" x="4085" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="4636" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-61" x="5087" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-63" x="5617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-31" x="6050" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-4E" x="6551" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-73" x="7689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-75" x="8159" y="0"></use><g transform="translate(8731,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-4E" x="1242" y="488"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-31" x="1242" y="-435"></use></g><g transform="translate(10338,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">（</text></g><g transform="translate(11168,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-79" x="0" y="0"></use><g transform="translate(490,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-75" x="812" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-65" x="1385" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-63" x="13318" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-64" x="13751" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6F" x="14275" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-74" x="14760" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6C" x="15122" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6F" x="15420" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-67" x="15906" y="0"></use><g transform="translate(16386,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">（</text></g><g transform="translate(17216,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-79" x="0" y="0"></use><g transform="translate(490,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="503" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-65" x="955" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-64" x="1421" y="0"></use></g></g><g transform="translate(19182,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">）</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-2B" x="20234" y="0"></use><g transform="translate(21235,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">（</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-31" x="22065" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-2212" x="22788" y="0"></use><g transform="translate(23789,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-79" x="0" y="0"></use><g transform="translate(490,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-75" x="812" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-65" x="1385" y="0"></use></g></g><g transform="translate(25689,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">）</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-63" x="26769" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-64" x="27202" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6F" x="27726" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-74" x="28211" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6C" x="28573" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-6F" x="28871" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-67" x="29357" y="0"></use><g transform="translate(29837,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">（</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-31" x="30667" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-2212" x="31390" y="0"></use><g transform="translate(32391,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-79" x="0" y="0"></use><g transform="translate(490,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="503" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-65" x="955" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-64" x="1421" y="0"></use></g></g><g transform="translate(34356,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">）</text></g><g transform="translate(35186,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">）</text></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>N</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>N</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mo>（</mo></mrow><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mo>（</mo></mrow><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo>）</mo></mrow><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo>（</mo></mrow><mn>1</mn><mo>−</mo><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo>）</mo></mrow><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>l</mi><mi>o</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mo>（</mo></mrow><mn>1</mn><mo>−</mo><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo>）</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>）</mo></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> loss =-\ frac {1} {N} \ sum_ {1} ^ {N}（y_ {true} \ cdot log（y_ {pred}）+（1-y_ {true}）\ cdot log（1- y_ {pred}））</script></p><br><br>  <b>5.</b>模型质量度量-谐波平均值（F度量） <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>F</mi><mo>=</mo><mn>2</mn><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x7CBE;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x786E;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x5EA6;</mo></mrow><mtext mathcolor=&quot;red&quot;>\&amp;#x6B21;</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x8C03;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x7528;</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x7CBE;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x786E;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x5EA6;</mo></mrow><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x8C03;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x7528;</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="36.354ex" height="2.66ex" viewBox="0 -832 15652.3 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-46" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-3D" x="1027" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-32" x="2083" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-66" x="2834" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-72" x="3384" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-61" x="3836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMATHI-63" x="4365" y="0"></use><g transform="translate(4799,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">精</text><g transform="translate(829,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">确</text></g><g transform="translate(1659,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">度</text></g><g fill="red" stroke="red" transform="translate(2489,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-5C"></use><g transform="translate(500,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">次</text></g></g><g transform="translate(3820,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">调</text></g><g transform="translate(4650,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">用</text></g></g><g transform="translate(10279,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">精</text><g transform="translate(829,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">确</text></g><g transform="translate(1659,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">度</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://habr.com/ru/company/naumen/blog/475338/&amp;usg=ALkJrhif5AwqKqXe4zCIqGg3K-A5jG09vg#MJMAIN-2B" x="2712" y="0"></use><g transform="translate(3712,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">调</text></g><g transform="translate(4542,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">用</text></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>F</mi><mo>=</mo><mn>2</mn><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo>精</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>确</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>度</mo></mrow><mtext mathcolor="red">\次</mtext><mrow class="MJX-TeXAtom-ORD"><mo>调</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>用</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo>精</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>确</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>度</mo></mrow><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo>调</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>用</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> F = 2 \ frac {精确度\次调用} {精确度+调用} </script></p><br> 在我们的案例中，质量指标的问题不如进口的正确性重要。 导入的正确性取决于在推理模式下工作的Python和Java NN模型中结果的重合性。 <br><br><h1> 在DL4J中导入Keras模型： </h1><br> 使用的版本：Tensorflow 1.5.0和Keras 2.2.5。 在我们的案例中，HDF5文件将Python模型整体上载。 <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># saving model model1.save('model1_functional.h5')</span></span></code> </pre> <br> 将模型导入DL4J时，导入模块不提供用于传递其他参数的API方法：张量流模块的名称（构建模型时从中导入函数）。 <br><br> 一般来说，DL4J仅与Keras函数一起使用，在Keras导入部分[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">6</a> ]中给出了详尽的列表，因此，如果使用TF的方法在Keras上创建了模型（如本例所示），则导入模块将无法识别它们。 <br><br><h3> 导入模型的一般准则 </h3><br> 显然，使用Keras模型意味着需要对其进行反复训练。 为此，为节省时间，设置了训练参数（1个时期），每个时期设置了1步（steps_per_epoch）。 <br><br> 首次导入模型时，尤其是具有唯一的自定义层和稀有层组合的模型时，成功的可能性不大。 因此，建议以迭代方式执行导入过程：减少Keras模型的层数，直到您可以在Java中导入并运行该模型而没有错误为止。 接下来，一次向Keras模型添加一层，然后将结果模型导入Java，以解决发生的错误。 <br><br><h3> 使用TF损失功能 </h3><br> 为了证明，当导入Java时，训练模型的损失函数必须来自Keras，我们使用了Tensorflow中的log_loss（与custom_loss函数最相似）。 我们在控制台中收到以下错误： <br><br><pre> <code class="java hljs">Exception in thread <span class="hljs-string"><span class="hljs-string">"main"</span></span> org.deeplearning4j.nn.modelimport.keras.exceptions.UnsupportedKerasConfigurationException: Unknown Keras loss function log_loss.</code> </pre> <br><h3> 用Keras替换TF方法 </h3><br> 在我们的案例中，TF模块中的函数使用了2次，在所有情况下，它们仅在lambda层中找到。 <br><br>  Lambda图层是用于添加任意功能的自定义图层。 <br><br> 我们的模型只有4个lambda层。 事实是，在Java中，必须通过KerasLayer.registerLambdaLayer手动注册这些lambda层（否则，我们会收到错误[ <a href="">12</a> ]）。 在这种情况下，在lambda层内部定义的函数应该是来自相应Java库的函数。 在Java中，没有注册这些层的示例，也没有为此提供全面的文档。 一个例子在这里[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">13</a> ]。 从示例[ <a href="">14，15</a> ]借来了一般考虑。 <br><br> 依次考虑在Java中注册模型的所有lambda层： <br><br>  1）Lambda层，用于沿给定方向（在本例中为左右）将有限量的常数添加到张量（矩阵）： <br><br> 该层的输入连接到模型的输入。 <br><br>  1.1）Python层： <br><br><pre> <code class="python hljs">padding = keras.layers.Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: tf.pad(x, paddings=[[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]], constant_values=<span class="hljs-number"><span class="hljs-number">1</span></span>))(embedding)</code> </pre> <br> 为了清楚起见，该层的功能正常工作，我们在python层中显式替换了数值。 <br><br><div class="spoiler">  <b class="spoiler_title">带有张量2x2的示例的表</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><td> 是2x2 </td><td> 它变成了2x22 </td></tr><tr><td>  [[ <strong>1，2</strong> ]， <br>  [ <strong>3，4</strong> ]] </td><td>  [[37，37，37，37，37，37，37，37，37，37，1，2，37，37，37，37，37，37，37，37，37，37 <br>  [37、37、37、37、37、37、37、37、37、37、3、4、37、37、37、37、37、37、37、37、37、37] </td></tr></tbody></table></div><br></div></div><br>  1.2）Java层： <br><br><pre> <code class="java hljs">KerasLayer.registerLambdaLayer(<span class="hljs-string"><span class="hljs-string">"lambda_1"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SameDiffLambdaLayer() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> SDVariable </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">defineLayer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(SameDiff sameDiff, SDVariable sdVariable)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sameDiff.nn().pad(sdVariable, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[][]{ { <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> }, { <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> }}, <span class="hljs-number"><span class="hljs-number">1</span></span>); } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> InputType </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getOutputType</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> layerIndex, InputType inputType)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> InputType.feedForward(<span class="hljs-number"><span class="hljs-number">20</span></span>); } });</code> </pre> <br> 在Java中所有已注册的lambda层中，重新定义了2个函数： <br> 第一个函数“ definelayer”负责所使用的方法（一点也不明显：该方法只能在nn（）后端以下使用）；  getOutputType负责注册层的输出，该参数是一个数字参数（此处为20，但通常允许使用任何整数值）。 它看起来不一致，但它的工作原理是这样的。 <br><br>  2）Lambda层，用于沿给定方向（在我们的示例中为左右）修剪张量（矩阵）： <br><br> 在这种情况下，LSTM层进入lambda层的输入。 <br><br>  2.1）Python层： <br><br><pre> <code class="python hljs">slicing_lstm = keras.layers.Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:, <span class="hljs-number"><span class="hljs-number">10</span></span>:<span class="hljs-number"><span class="hljs-number">-10</span></span>])(lstm)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">带有任意张量2x22x5的示例的表</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><td> 是2x22x5 </td><td> 它变成了2x2x5 </td></tr><tr><td>  [[[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]， [1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1 ，2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2 ，3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3 ，4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4 ，5]，[1,2,3,4,5]]， <br><br>  [[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[ 1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1， 2,3,4,5]，[1,2,3,4,5]，[1、2、3、4、5]，[1、2、3、4、5]，[1,2， 3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3， 4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4,5]，[1,2,3,4， 5]，[1,2,3,4,5]]] </td><td>  [[[ <strong>1，2，3，4，5</strong> ]，[ <strong>1，2，3，4，5</strong> ]]， <br>  [[ <strong>1，2，3，4，5</strong> ]，[ <strong>1，2，3，4，5</strong> ]]] </td></tr></tbody></table></div><br></div></div><br>  2.2）Java层： <br><br><pre> <code class="java hljs">KerasLayer.registerLambdaLayer(<span class="hljs-string"><span class="hljs-string">"lambda_2"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SameDiffLambdaLayer() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> SDVariable </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">defineLayer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(SameDiff sameDiff, SDVariable sdVariable)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sameDiff.stridedSlice(sdVariable, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> }, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">0</span></span>], (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">1</span></span>], (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">2</span></span>]-<span class="hljs-number"><span class="hljs-number">10</span></span>}, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> }); } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> InputType </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getOutputType</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> layerIndex, InputType inputType)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> InputType.recurrent(<span class="hljs-number"><span class="hljs-number">60</span></span>); } });</code> </pre> <br> 对于此层，InputType参数从前馈（20）更改为递归（60）。 在递归参数中，数字可以是任何整数（非零），但其与下一个lambda层的递归参数的总和应为160（即在下一层中，参数必须为100）。 数字160是由于必须在图层的输入concatenate_1处接收到具有张量（无，无，160）的张量。 <br><br> 前两个参数是变量，具体取决于输入字符串的大小。 <br><br>  3）Lambda层，用于沿给定方向（在我们的示例中为左右）修剪张量（矩阵）： <br><br> 该层的输入是LSTM层，在该层之前是conv1_d层 <br><br>  3.1）Python层： <br><br><pre> <code class="python hljs">slicing_convolution = keras.layers.Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:,<span class="hljs-number"><span class="hljs-number">10</span></span>:<span class="hljs-number"><span class="hljs-number">-10</span></span>])(lstm_conv)</code> </pre> <br> 此操作与第2.1节中的操作完全相同。 <br><br>  3.2）Java层： <br><br><pre> <code class="java hljs">KerasLayer.registerLambdaLayer(<span class="hljs-string"><span class="hljs-string">"lambda_3"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SameDiffLambdaLayer() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> SDVariable </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">defineLayer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(SameDiff sameDiff, SDVariable sdVariable)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sameDiff.stridedSlice(sdVariable, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> }, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">0</span></span>], (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">1</span></span>], (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>)sdVariable.getShape()[<span class="hljs-number"><span class="hljs-number">2</span></span>]-<span class="hljs-number"><span class="hljs-number">10</span></span>}, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[]{ <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> }); } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> InputType </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getOutputType</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> layerIndex, InputType inputType)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> InputType.recurrent(<span class="hljs-number"><span class="hljs-number">100</span></span>); } });</code> </pre> <br> 除了recurrent（100）参数外，该lambda层重复了先前的lambda层。 在上一层的说明中注明了为什么采用“ 100”。 <br><br> 在第2点和第3点，λ层位于LSTM层之后，因此使用循环类型。 但是，如果在lambda层之前没有LSTM，而是conv1d_1，则仍然有必要设置递归值（它看起来不一致，但是可以这样工作）。 <br><br>  4）Lambda层压缩前一层： <br><br> 该层的输入是完全连接的层。 <br><br>  4.1）Python层： <br><br><pre> <code class="python hljs"> squeeze = keras.layers.Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: tf.squeeze( x, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>))(dense)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">带有任意张量2x4x1的示例的表</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><td> 是2x4x1 </td><td> 变成了2x4 </td></tr><tr><td>  [[[ <strong>[1]，[2]，[3]，[4]]</strong> <br><br>  [ <strong>[1]，[2]，[3]，[4</strong> ]] </td><td>  [[ <strong>1，2，3，4</strong> ]， <br>  [ <strong>1，2，3，4</strong> ]] </td></tr></tbody></table></div><br></div></div><br>  4.2）Java层： <br><br><pre> <code class="java hljs">KerasLayer.registerLambdaLayer(<span class="hljs-string"><span class="hljs-string">"lambda_4"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SameDiffLambdaLayer() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> SDVariable </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">defineLayer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(SameDiff sameDiff, SDVariable sdVariable)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sameDiff.squeeze(sdVariable, -<span class="hljs-number"><span class="hljs-number">1</span></span>); } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> InputType </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getOutputType</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> layerIndex, InputType inputType)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> InputType.feedForward(<span class="hljs-number"><span class="hljs-number">15</span></span>); } });</code> </pre> <br> 该层的输入接收一个完全连接的层，该层的InputType feedForward（15），参数15不影响模型（允许使用任何整数值）。 <br><br><h3> 下载导入的模型 </h3><br> 该模型是通过ComputationGraph模块加载的： <br><br><pre> <code class="java hljs">ComputationGraph model = org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasModelAndWeights(<span class="hljs-string"><span class="hljs-string">"/home/user/Models/model1_functional.h5"</span></span>);</code> </pre> <br><h3> 将数据输出到Java控制台 </h3><br> 在Java中，尤其是在DL4J中，张量是作为高性能Nd4j库的数组编写的，可以将其视为Python中Numpy库的类似物。 <br><br> 假设我们的输入字符串包含4个字符。 例如，根据一些编号，将符号表示为整数（作为索引）。 为它们创建一个相应维数（4）的数组。 <br><br> 例如，我们有4个索引编码的字符：1、3、4、8。 <br><br>  Java代码： <br><br><pre> <code class="java hljs">INDArray myArray = Nd4j.zeros(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>); <span class="hljs-comment"><span class="hljs-comment">// one row 4 column array myArray.putScalar(0,0,1); myArray.putScalar(0,1,3); myArray.putScalar(0,2,4); myArray.putScalar(0,3,8); INDArray output = model.outputSingle(myArray); System.out.println(output);</span></span></code> </pre> <br> 控制台将显示每个输入元素的概率。 <br><br><h3> 进口型号 </h3><br> 原始神经网络的架构和权重均无错误地导入。 推理模式下的Keras和Java神经网络模型都对结果达成共识。 <br><br>  Python模型： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/om/sk/oe/omskoecug43s_gop5osadysm21m.png" alt="图片替代"></div><br>  Java模型： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/qc/px/esqcpxwnhjdgrhqapedjukmvfec.png" alt="图片替代"></div><br> 实际上，导入模型并不是那么简单。 下面我们将简要强调一些在某些情况下可能至关重要的观点。 <br><br>  1）补丁归一化层在递归层之后不起作用。 问题已经在GitHub上公开了将近一年[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">16</a> ]。 例如，如果将此层添加到模型中（在接触层之后），则会出现以下错误： <br><br><pre> <code class="java hljs">Exception in thread <span class="hljs-string"><span class="hljs-string">"main"</span></span> java.lang.IllegalStateException: Invalid input type: Batch norm layer expected input of type CNN, CNN Flat or FF, <span class="hljs-function"><span class="hljs-function">got </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">InputTypeRecurrent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">160</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">for</span></span></span><span class="hljs-function"> layer index -1, layer name </span></span>= batch_normalization_1</code> </pre> <br> 在实践中，该模型拒绝运行，因为在conv1d之后添加归一化层时引用了类似的错误。 在完全连接的层之后，附加功能可以完美地工作。 <br><br>  2）在完全连接层之后，设置“展平”层会导致错误。 在Stackoverflow [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://stackoverflow.com/questions/55753493/problem-opening-a-keras-model-in-java-with-deeplearning4j-">17</a> ]上提到了类似的错误。 六个月以来，没有任何反馈。 <br><br> 当然，这不是使用DL4J时可能遇到的所有限制。 <br> 该模型的最终运行时间在此处[ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">18</a> ]。 <br><br><h1> 结论 </h1><br> 总之，可以注意到，将训练有素的Keras模型无痛地导入DL4J仅适用于简单的情况（当然，如果您没有这样的经验，并且确实有Java的良好命令）。 <br><br> 用户层越少，导入的模型就越轻松，但是如果网络体系结构复杂，则必须花费大量时间将其传输到DL4J。 <br><br> 已开发的导入模块的文档支持以及相关示例的数量似乎很潮湿。 在每个阶段，都会出现新的问题-如何注册Lambda层，参数的含义等。 <br><br> 考虑到神经网络架构的复杂性速度以及各层之间的交互速度，层的复杂性，DL4J尚未积极开发，以达到与人工神经网络一起使用的高端框架的水平。 <br><br> 无论如何，这些家伙都值得尊重他们的工作，并希望看到这个方向继续发展。 <br><br>  <strong>参考文献</strong> <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">人工智能领域的5种最佳编程语言</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">深度学习框架力量得分2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">深度学习软件的比较</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">人工智能世界的9大框架</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DeepLearning4j。</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">可用型号</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DeepLearning4j。</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Keras模型导入。</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">支持的功能。</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Deeplearning4j。</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">快速入门</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">讲座0：DeepLearning4j入门</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Deeplearing4j：Keras模型导入</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">讲座7 |</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Keras模型导入</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">安装TensorFlow for Java</a> </li><li>  <a href="">使用Keras图层</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DeepLearning4j：KerasLayer类</a> </li><li>  <a href="">DeepLearning4j：SameDiffLambdaLayer.java</a> </li><li>  <a href="">DeepLearning4j：KerasLambdaTest.java</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DeepLearning4j：具有RecurrentInputType的BatchNorm</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=https://stackoverflow.com/questions/55753493/problem-opening-a-keras-model-in-java-with-deeplearning4j-">StackOverFlow：使用deeplearning4j（https://deeplearning4j.org/）在Java中打开keras模型时出现问题</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">GitHub：有关模型的完整代码</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Skymind：AI框架比较</a> </li></ol><br></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN475338/">https://habr.com/ru/post/zh-CN475338/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN475324/index.html">从EcmaScript角度进行功能编程。 组成，咖喱，部分涂抹</a></li>
<li><a href="../zh-CN475326/index.html">骗子如何做到这一点。 作弊工具</a></li>
<li><a href="../zh-CN475328/index.html">操作TA505，第四部分。 双胞胎</a></li>
<li><a href="../zh-CN475332/index.html">3.极限交换机上的企业网络设计</a></li>
<li><a href="../zh-CN475336/index.html">如何正确退出（说明）</a></li>
<li><a href="../zh-CN475340/index.html">AMD推出了Threadripper处理器-最快的台式机CPU</a></li>
<li><a href="../zh-CN475342/index.html">Andrey Sebrant（Yandex）：人工智能时代的企业</a></li>
<li><a href="../zh-CN475346/index.html">当美军试图读懂思想时</a></li>
<li><a href="../zh-CN475354/index.html">快乐安全专家日</a></li>
<li><a href="../zh-CN475358/index.html">亚美尼亚IT部门的薪资分析以及TOP10 IT公司的空缺职位</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>