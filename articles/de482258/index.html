<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüéì üßõüèæ ‚õπüèº Wie neuronale Netze funktionieren und warum sie anfingen, viel Geld zu bringen üëåüèª üíú üöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale Netze haben sich von einer akademischen Neugierde zu einer massiven Industrie entwickelt 


 In den letzten zehn Jahren haben Computer ihre ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie neuronale Netze funktionieren und warum sie anfingen, viel Geld zu bringen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  Neuronale Netze haben sich von einer akademischen Neugierde zu einer massiven Industrie entwickelt </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  In den letzten zehn Jahren haben Computer ihre F√§higkeit, die Welt um sie herum zu verstehen, erheblich verbessert.  Software f√ºr Fotoausr√ºstung erkennt automatisch die Gesichter von Menschen.  Smartphones wandeln Sprache in Text um.  Robomobile erkennen Objekte auf der Stra√üe und vermeiden Kollisionen mit ihnen. <br><br>  Im Zentrum all dieser Durchbr√ºche steht die Technologie der k√ºnstlichen Intelligenz (KI), die als Deep Learning (GO) bezeichnet wird.  GO basiert auf neuronalen Netzwerken (NS), Datenstrukturen, die von Netzwerken aus biologischen Neuronen inspiriert sind.  NS sind in Schichten organisiert, und die Eing√§nge einer Schicht sind mit den Ausg√§ngen der benachbarten verbunden. <br><br>  Informatiker experimentieren seit den 1950er Jahren mit NS.  Der Grundstein f√ºr die heutige gro√üe GO-Industrie wurde jedoch durch zwei wichtige Durchbr√ºche gelegt - einen 1986, den zweiten 2012. Der Durchbruch von 2012 - die Revolution von GO - war mit der Entdeckung verbunden, dass die Verwendung von NS mit einer gro√üen Anzahl von Schichten es uns erm√∂glicht, deren Effizienz erheblich zu verbessern.  Die Entdeckung wurde durch das wachsende Datenvolumen und die wachsende Rechenleistung erleichtert. <br><a name="habracut"></a><br>  In diesem Artikel stellen wir Ihnen die Welt der Nationalversammlung vor.  Wir werden erkl√§ren, was NS ist, wie sie arbeiten und woher sie kommen.  Und wir werden untersuchen, warum NS - trotz jahrzehntelanger Forschung - erst 2012 zu etwas wirklich N√ºtzlichem wurden. <br><br><h2>  Neuronale Netze tauchten bereits in den 1950er Jahren auf </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt arbeitet an seinem Perzeptron - einem fr√ºhen NS-Modell</i> <br><br>  Die Idee der Nationalversammlung ist ziemlich alt - zumindest im Vergleich zu den Standards der Informatik.  Bereits 1957 ver√∂ffentlichte <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt</a> von der Cornell University einen <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">Bericht</a> √ºber ein fr√ºhes NS-Konzept namens Perceptron.  1958 schuf er mit Unterst√ºtzung der US Navy ein primitives System, mit dem 20 x 20 Pixel analysiert und einfache geometrische Formen erkannt werden k√∂nnen. <br><br>  Rosenblatts Hauptziel war es nicht, ein praktisches Bildklassifizierungssystem zu schaffen.  Er versuchte zu verstehen, wie das menschliche Gehirn funktioniert, und schuf Computersysteme, die in seinem Bild organisiert waren.  Dieses Konzept hat jedoch zu viel Begeisterung bei Dritten ausgel√∂st. <br><br>  "Heute hat die US Navy der Welt den Keim eines elektronischen Computers offenbart, von dem erwartet wird, dass er in der Lage ist, zu gehen, zu sprechen, zu sehen, zu schreiben, sich selbst zu reproduzieren und sich seiner Existenz bewusst zu sein", schrieb die New York Times. <br><br>  Tats√§chlich ist jedes Neuron im NS nur eine mathematische Funktion.  Jedes Neuron berechnet die gewichtete Summe der Eingabedaten. Je gr√∂√üer die Eingabegewichtung ist, desto st√§rker wirken sich diese Eingabedaten auf die Ausgabe des Neurons aus.  Anschlie√üend wird die gewichtete Summe der nichtlinearen Aktivierungsfunktion zugef√ºhrt. In diesem Schritt k√∂nnen NS komplexe nichtlineare Ph√§nomene simulieren. <br><br>  Die F√§higkeiten der fr√ºhen Perzeptrone, mit denen Rosenblatt experimentierte - und der NS im Allgemeinen - beruhen auf ihrer F√§higkeit, anhand von Beispielen "zu lernen".  NS werden trainiert, indem die Eingabegewichte von Neuronen basierend auf den Ergebnissen des Netzwerks mit den zum Beispiel ausgew√§hlten Eingabedaten abgestimmt werden.  Wenn das Netzwerk das Bild korrekt klassifiziert, nehmen die zur richtigen Antwort beitragenden Gewichte zu, w√§hrend andere abnehmen.  Wenn das Netzwerk falsch ist, werden die Gewichte in die andere Richtung angepasst. <br><br>  Ein solches Verfahren erm√∂glichte es fr√ºhen NS-Patienten, auf eine Art und Weise zu ‚Äûlernen‚Äú, die an das Verhalten des menschlichen Nervensystems erinnert.  Der Hype um diesen Ansatz h√∂rte in den 1960er Jahren nicht auf.  Das <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">einflussreiche Buch</a> von 1969 der Autoren der Informatiker Marvin Minsky und Seymour Papert zeigte jedoch, dass diese fr√ºhen NA erhebliche Einschr√§nkungen aufweisen. <br><br>  Die fr√ºhen Rosenblatt-NS hatten nur ein oder zwei trainierte Schichten.  Minsky und Papert zeigten, dass solche NS mathematisch nicht in der Lage sind, komplexe Ph√§nomene der realen Welt zu modellieren. <br><br>  Im Prinzip waren tiefere NS f√§higer.  Solche NS w√ºrden jedoch die elenden Computerressourcen, √ºber die Computer zu dieser Zeit verf√ºgten, √ºberfordern.  Die einfachsten <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">aufsteigenden</a> Suchalgorithmen, die in den ersten NS verwendet wurden, waren f√ºr tiefere NS nicht skalierbar. <br><br>  In der Folge verlor die Nationalversammlung in den 1970er und fr√ºhen 1980er Jahren jegliche Unterst√ºtzung - es war Teil der √Ñra des ‚ÄûWinters der KI‚Äú. <br><br><h2>  Durchbruchsalgorithmus </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Mein eigenes neuronales Netzwerk, das auf ‚ÄûSoft Equipment‚Äú basiert, geht davon aus, dass die Wahrscheinlichkeit, auf diesem Foto einen Hot Dog zu haben, 1 betr√§gt. Wir werden reich!</i> <br><br>  Das Gl√ºck wandte sich wieder der NS zu, dank des ber√ºhmten <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Werks von</a> 1986, das das Konzept der R√ºckenvermehrung einf√ºhrte - eine praktische Methode, um NS zu unterrichten. <br><br>  Angenommen, Sie arbeiten als Programmierer in einer imagin√§ren Softwarefirma und wurden angewiesen, eine Anwendung zu erstellen, die feststellt, ob das Image einen Hot Dog enth√§lt.  Sie beginnen die Arbeit mit einer zuf√§llig initialisierten NS, die ein Eingabebild aufnimmt und einen Wert von 0 bis 1 ausgibt. Dabei bedeutet 1 "Hot Dog" und 0 "Nicht-Hot Dog". <br><br>  Um das Netzwerk zu trainieren, sammeln Sie Tausende von Bildern, unter denen sich jeweils ein Etikett befindet, das angibt, ob sich auf diesem Bild ein Hot Dog befindet.  Sie f√ºttern ihr das erste Bild - und es ist ein Hot Dog drauf - im neuronalen Netz.  Es gibt einen Ausgabewert von 0,07, was "kein Hot Dog" bedeutet.  Das ist die falsche Antwort;  Das Netzwerk sollte eine Antwort nahe 1 zur√ºckgegeben haben. <br><br>  Der Backpropagation-Algorithmus dient dazu, die Eingabegewichte so anzupassen, dass das Netzwerk einen h√∂heren Wert erzeugt, wenn es erneut dieses Bild erh√§lt - und vorzugsweise andere Bilder, bei denen es Hot Dogs gibt.  Hierzu untersucht der Backpropagation-Algorithmus zun√§chst die Eingangsneuronen der Ausgangsschicht.  Jeder Wert hat eine Gewichtsvariable.  Der Backpropagation-Algorithmus passt jedes Gewicht so an, dass der NS einen h√∂heren Wert ergibt.  Je h√∂her der Eingabewert, desto st√§rker steigt sein Gewicht. <br><br>  Bisher beschreibe ich den einfachsten Aufstieg an die Spitze, den Forscher in den 1960er Jahren kannten.  Der Backpropagation-Durchbruch war der n√§chste Schritt: Der Algorithmus verwendet partielle Ableitungen, um den ‚ÄûFehler‚Äú f√ºr die fehlerhafte Ausgabe auf die Eingaben von Neuronen zu verteilen.  Der Algorithmus berechnet, wie sich eine kleine √Ñnderung in jedem Eingabewert auf die endg√ºltige Ausgabe eines Neurons auswirkt und ob diese √Ñnderung das Ergebnis n√§her an die richtige Antwort r√ºckt oder umgekehrt. <br><br>  Das Ergebnis ist eine Reihe von Fehlerwerten f√ºr jedes Neuron in der vorherigen Schicht - in der Tat ein Signal, das auswertet, ob der Wert jedes Neurons zu gro√ü oder zu klein ist.  Dann wiederholt der Algorithmus den Abstimmungsprozess f√ºr neue Neuronen von der zweiten [von der End-] Schicht.  Dadurch werden die Eingabegewichte der einzelnen Neuronen geringf√ºgig ge√§ndert, um das Netzwerk n√§her an die richtige Antwort heranzuf√ºhren. <br><br>  Anschlie√üend berechnet der Algorithmus anhand partieller Ableitungen erneut, wie sich der Wert jeder Eingabe der vorherigen Ebene auf die Ausgabefehler dieser Ebene auswirkt. Diese Fehler werden dann an die vorherige Ebene weitergegeben, wo der Vorgang erneut wiederholt wird. <br><br>  Dies ist nur ein vereinfachtes Backpropagation-Modell.  Wenn Sie detaillierte mathematische Details ben√∂tigen, empfehle ich das Buch von Michael Nielsen zu diesem Thema.  transl.].  F√ºr unsere Zwecke ist es ausreichend, dass die umgekehrte Verteilung den Bereich der trainierten NS radikal ver√§ndert.  Die Menschen waren nicht l√§nger auf einfache Netzwerke mit einer oder zwei Schichten beschr√§nkt.  Sie k√∂nnten Netzwerke mit f√ºnf, zehn oder f√ºnfzig Schichten erstellen, und diese Netzwerke k√∂nnten eine willk√ºrlich komplexe interne Struktur aufweisen. <br><br>  Die Erfindung der Backpropagation l√∂ste den zweiten Boom der Nationalversammlung aus, der praktische Ergebnisse hervorbrachte.  Eine Gruppe von Forschern von AT &amp; T hat 1998 gezeigt, wie mit neuronalen Netzen handschriftliche Zahlen erkannt werden k√∂nnen, wodurch die Scheckverarbeitung automatisiert werden konnte. <br><br>  "Die Hauptbotschaft dieser Arbeit ist, dass wir verbesserte Systeme zum Erkennen von Mustern entwickeln k√∂nnen, die sich mehr auf automatisches Lernen und weniger auf manuell entwickelte Heuristiken st√ºtzen", schrieben die Autoren. <br><br>  Und doch waren NS in dieser Phase nur eine von vielen Technologien, die Forschern des maschinellen Lernens zur Verf√ºgung standen.  Als ich 2008 in einem AI-Kurs am Institut studierte, waren neuronale Netze nur einer von neun MO-Algorithmen, aus denen wir die f√ºr die Aufgabe geeignete Option ausw√§hlen konnten.  GO bereitete sich jedoch bereits darauf vor, den Rest der Technologie in den Schatten zu stellen. <br><br>  Big Data zeigt die Kraft des Deep Learning <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>Entspannung erkannt.</i>  <i>Chance auf Strand 1.0.</i>  <i>Wir beginnen mit der Anwendung von Mai Tai.</i> <br><br>  Backpropagation erleichterte den Prozess der NS-Berechnung, aber tiefere Netzwerke ben√∂tigten immer noch mehr Rechenressourcen als kleine.  Die Ergebnisse der in den 1990er und 2000er Jahren durchgef√ºhrten Studien zeigten oft, dass es m√∂glich war, von zus√§tzlichen Komplikationen der NS immer weniger Nutzen zu ziehen. <br><br>  Dann wurde das Denken der Menschen durch das ber√ºhmte Werk von 2012 ge√§ndert, das die NS unter dem Namen AlexNet beschrieb, benannt nach dem f√ºhrenden Forscher Alex Krizhevsky.  √Ñhnlich wie tiefere Netzwerke k√∂nnen sie zu einer bahnbrechenden Effizienz f√ºhren, jedoch nur in Kombination mit einer F√ºlle an Computerleistung und einer enormen Datenmenge. <br><br>  AlexNet hat ein Trio von Informatikern der Universit√§t von Toronto f√ºr die Teilnahme am ImageNet-Wissenschaftswettbewerb entwickelt.  Die Organisatoren des Wettbewerbs sammelten im Internet eine Million Bilder, die jeweils einer der Tausenden von Objektkategorien zugeordnet waren, beispielsweise ‚ÄûKirsche‚Äú, ‚ÄûContainerschiff‚Äú oder ‚ÄûLeopard‚Äú.  KI-Forscher wurden gebeten, ihre MO-Programme auf Teile dieser Bilder zu trainieren und dann zu versuchen, die korrekten Bezeichnungen f√ºr andere Bilder anzubringen, auf die die Software zuvor noch nicht gesto√üen war.  Die Software musste f√ºr jedes Bild f√ºnf m√∂gliche Bezeichnungen ausw√§hlen, und der Versuch wurde als erfolgreich angesehen, wenn eine davon mit der tats√§chlichen √ºbereinstimmte. <br><br>  Dies war eine schwierige Aufgabe, und bis 2012 waren die Ergebnisse nicht sehr gut.  F√ºr den Gewinner 2011 betrug die Fehlerquote 25%. <br><br>  2012 √ºbertraf das AlexNet-Team alle Mitbewerber, indem es Antworten mit 15% Fehlern gab.  F√ºr den engsten Wettbewerber lag dieser Wert bei 26%. <br><br>  Forscher aus Toronto kombinierten verschiedene Techniken, um bahnbrechende Ergebnisse zu erzielen.  Eine davon war die Verwendung von <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Faltungsneurosen</a> ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Convolutional Neuroses,</a> SNS).  Tats√§chlich trainiert der SNA sozusagen kleine neuronale Netze - deren Eingangsdaten Quadrate mit einer Seite von 7 bis 11 Pixeln sind - und ‚Äû√ºberlagert‚Äú sie dann auf einem gr√∂√üeren Bild. <br><br>  "Es ist, als ob Sie eine kleine Vorlage oder Schablone genommen und versucht h√§tten, sie mit jedem Punkt im Bild zu vergleichen", sagte KI-Forscher Jie Tan im vergangenen Jahr.  - Sie haben eine Schablone eines Hundes und h√§ngen sie an das Bild an und pr√ºfen, ob dort ein Hund ist?  Wenn nicht, verschieben Sie die Schablone.  Und so f√ºr das ganze Bild.  Und egal wo der Hund auf dem Bild erscheint.  Die Schablone wird damit √ºbereinstimmen.  Jeder Netzwerk-Unterabschnitt sollte kein separater Hundeklassifikator werden. ‚Äú <br><br>  Ein weiterer wichtiger Erfolgsfaktor f√ºr AlexNet war der Einsatz von Grafikkarten, um den Lernprozess zu beschleunigen.  Grafikkarten verf√ºgen √ºber eine parallele Verarbeitungsleistung, die sich gut f√ºr das Repetitive Computing eignet, das zum Trainieren eines neuronalen Netzwerks erforderlich ist.  Durch die √úbertragung der Rechenlast auf zwei GPUs - die Nvidia GTX 580 mit jeweils 3 GB Arbeitsspeicher - konnten die Forscher ein extrem gro√ües und komplexes Netzwerk aufbauen und trainieren.  AlexNet hatte acht trainierbare Schichten, 650.000 Neuronen und 60 Millionen Parameter. <br><br>  Der Erfolg von AlexNet wurde schlie√ülich auch durch die Gr√∂√üe der ImageNet-Trainingsbilddatenbank sichergestellt: eine Million Teile.  F√ºr die Feinabstimmung von 60 Millionen Parametern sind viele Bilder erforderlich.  Um einen entscheidenden Sieg zu erzielen, wurde AlexNet von einer Kombination aus einem komplexen Netzwerk und einem gro√üen Datenbestand unterst√ºtzt. <br><br>  Ich frage mich, warum so ein Durchbruch nicht fr√ºher stattgefunden hat: <br><br><ul><li>  Das von AlexNet-Forschern verwendete GPU-Paar f√ºr Endverbraucher war weit davon entfernt, das leistungsst√§rkste Computerger√§t f√ºr 2012 zu sein.  F√ºnf und sogar zehn Jahre zuvor gab es leistungsst√§rkere Computer.  Dar√ºber hinaus ist die Technologie zur Beschleunigung des Lernens von NS mit Grafikkarten seit mindestens 2004 bekannt. </li><li>  Die Basis von einer Million Bildern war 2012 f√ºr das Unterrichten von MO-Algorithmen ungew√∂hnlich gro√ü. Das Sammeln solcher Daten war jedoch keine neue Technologie f√ºr dieses Jahr.  Ein gut finanziertes Forschungsteam k√∂nnte leicht eine Datenbank dieser Gr√∂√üe vor f√ºnf oder zehn Jahren zusammenstellen. </li><li>  Die in AlexNet verwendeten Hauptalgorithmen waren nicht neu.  Der Backpropagation-Algorithmus von 2012 existierte bereits seit etwa einem Vierteljahrhundert.  Schl√ºsselideen im Zusammenhang mit neuronalen Faltungsnetzen wurden in den 1980er und 1990er Jahren entwickelt. </li></ul><br>  So existierte jedes der Erfolgselemente von AlexNet lange vor dem Durchbruch f√ºr sich.  Offensichtlich fiel niemandem ein, sie zu kombinieren - gr√∂√ütenteils, weil niemand wusste, wie m√§chtig diese Kombination sein w√ºrde. <br><br>  Die Erh√∂hung der Tiefe der NS hat die Effizienz ihrer Arbeit praktisch nicht verbessert, wenn sie nicht gen√ºgend Trainingsdatens√§tze verwendet haben.  Das Erweitern des Datensatzes hat die Leistung kleiner Netzwerke nicht verbessert.  Um die Effizienzsteigerung zu sehen, ben√∂tigten wir sowohl tiefere Netzwerke als auch gr√∂√üere Datenmengen - und eine erhebliche Rechenleistung, die es uns erm√∂glichte, den Schulungsprozess in angemessener Zeit durchzuf√ºhren.  Das AlexNet-Team brachte als erstes alle drei Elemente in einem Programm zusammen. <br><br><h2>  Der Boom des tiefen Lernens </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  Die Demonstration der ganzen Kraft von Deep NS, die durch eine ausreichende Menge an Trainingsdaten bereitgestellt wurde, wurde von vielen Menschen bemerkt - sowohl von Wissenschaftlern, Forschern als auch von Vertretern der Industrie. <br><br>  Der erste ImageNet-Wettbewerb, der sich √§ndert.  Bis 2012 verwendeten die meisten Teilnehmer andere Technologien als Deep Learning.  Wie die Sponsoren feststellten, nutzte im Wettbewerb 2013 die Mehrheit der Teilnehmer GO. <br><br>  Die Fehlerquote unter den Gewinnern ging allm√§hlich zur√ºck - von beeindruckenden 16% bei AlexNet im Jahr 2012 auf 2,3% im Jahr 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  Die GO-Revolution breitete sich schnell in der Branche aus.  2013 erwarb Google ein Startup, das von den Autoren von AlexNet gegr√ºndet wurde, und nutzte seine Technologie als Grundlage f√ºr die Bildsuchfunktion in Google Fotos.  Bis 2014 hat Facebook eine eigene Software beworben, die Bilder mit GO erkennt.  Apple verwendet seit mindestens 2016 GO f√ºr die Gesichtserkennung unter iOS. <br><br>  GO liegt auch der j√ºngsten Verbesserung der Spracherkennungstechnologie zugrunde.  Siri von Apple, Alexa von Amazon, Cortana von Microsoft und Googles Assistent verwenden GO - entweder, um die W√∂rter einer Person zu verstehen oder um eine nat√ºrlichere Stimme zu erzeugen, oder beides. <br><br>  In den letzten Jahren hat sich in der Branche ein sich selbst tragender Trend herausgebildet, bei dem sich die Zunahme von Rechenleistung, Datenvolumen und Netzwerktiefe gegenseitig st√ºtzen.  Das AlexNet-Team nutzte die GPU, weil sie paralleles Rechnen zu einem vern√ºnftigen Preis anbot.  In den letzten Jahren haben jedoch immer mehr Unternehmen begonnen, eigene Chips zu entwickeln, die speziell f√ºr den Einsatz im MO-Bereich entwickelt wurden. <br><br>  Google k√ºndigte die Ver√∂ffentlichung des Tensor Processing Unit-Chips an, der speziell f√ºr den NS entwickelt wurde. Im selben Jahr k√ºndigte Nvidia die Ver√∂ffentlichung einer neuen, f√ºr den NS optimierten GPU namens Tesla P100 an.  Intel antwortete 2017 mit seinem AI-Chip auf den Anruf. 2018 k√ºndigte Amazon die Ver√∂ffentlichung seines eigenen AI-Chips an, der als Teil der Cloud-Dienste des Unternehmens verwendet werden kann.  Sogar Microsoft soll an seinem AI-Chip arbeiten. <br><br>  Smartphone-Hersteller arbeiten auch an Chips, mit denen mobile Ger√§te mehr lokal mit NS rechnen k√∂nnen, ohne Daten auf Server hochladen zu m√ºssen.  Solches Computing auf Ger√§ten reduziert die Latenz und verbessert die Privatsph√§re. <br><br>  Sogar Tesla ist mit speziellen Chips in dieses Spiel eingestiegen.  In diesem Jahr zeigte Tesla einen neuen leistungsstarken Computer, der f√ºr die Berechnung von NS optimiert wurde.  Tesla nannte es Full Self-Driving Computer und pr√§sentierte es als Schl√ºsselmoment in der Strategie des Unternehmens, die Tesla-Flotte in Roboterfahrzeuge zu verwandeln. <br><br>  Die Verf√ºgbarkeit von f√ºr KI optimierten Computerkapazit√§ten hat eine Anforderung nach den Daten erzeugt, die zum Trainieren von immer komplexer werdenden NS erforderlich sind.  Diese Dynamik zeigt sich am deutlichsten im Robomobilsektor, in dem Unternehmen Daten √ºber Millionen Kilometer realer Stra√üen sammeln.  Tesla kann diese Daten automatisch von den Autos der Benutzer sammeln, und seine Konkurrenten Waymo und Cruise bezahlten Fahrer, die ihre Autos auf √∂ffentlichen Stra√üen fuhren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  Die Datenanforderung bietet gro√üen Online-Unternehmen einen Vorteil, die bereits auf gro√üe Mengen von Benutzerdaten zugreifen k√∂nnen. <br><br>  Deep Learning hat aufgrund seiner extremen Flexibilit√§t so viele verschiedene Bereiche erobert.  Jahrzehntelange Versuche und Irrt√ºmer haben es den Forschern erm√∂glicht, die Grundbausteine ‚Äã‚Äãf√ºr die h√§ufigsten Aufgaben im MO-Bereich zu entwickeln - beispielsweise Faltungsnetzwerke f√ºr eine effiziente Bilderkennung.  Wenn Sie jedoch ein f√ºr das Schema geeignetes Netzwerk auf hoher Ebene und gen√ºgend Daten haben, ist der Schulungsprozess einfach.  Deep NSs sind in der Lage, eine au√üergew√∂hnlich breite Palette komplexer Muster zu erkennen, ohne die besonderen Anweisungen menschlicher Entwickler zu beachten. <br><br>  Nat√ºrlich gibt es Einschr√§nkungen.  Zum Beispiel g√∂nnten sich einige Leute die Idee, Robomobile nur mit Hilfe von GO zu trainieren - das hei√üt, sie f√ºtterten Bilder, die von einer Kamera oder einem neuronalen Netzwerk empfangen wurden, und erhielten Anweisungen von ihr, das Lenkrad und das Pedal zu drehen.  Ich bin diesem Ansatz skeptisch gegen√ºber.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Nationalversammlung hat die F√§higkeit, komplexe logische √úberlegungen anzustellen, die erforderlich sind, um bestimmte Bedingungen auf der Stra√üe zu verstehen, noch nicht unter Beweis gestellt. </font><font style="vertical-align: inherit;">Dar√ºber hinaus sind NS ‚ÄûBlack Boxes‚Äú, deren Workflow praktisch unsichtbar ist. </font><font style="vertical-align: inherit;">Es w√§re schwierig, die Sicherheit eines solchen Systems zu bewerten und zu best√§tigen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit GO konnten jedoch in einem unerwartet gro√üen Anwendungsbereich sehr gro√üe Spr√ºnge erzielt werden. </font><font style="vertical-align: inherit;">In den kommenden Jahren ist mit weiteren Fortschritten in diesem Bereich zu rechnen.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de482258/">https://habr.com/ru/post/de482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de482248/index.html">Wir teilen unsere Erfahrungen damit, wie SSDs unter RAID angezeigt werden und welche Array-Ebene rentabler ist</a></li>
<li><a href="../de482250/index.html">Eine einfache Zustandsmaschine f√ºr VueJS</a></li>
<li><a href="../de482252/index.html">Automatische Katzentoilette - Fortsetzung</a></li>
<li><a href="../de482254/index.html">VonmoTrade-Experiment. Teil 3: Optionsscheinheft. Verarbeitung und Speicherung von Handelsinformationen</a></li>
<li><a href="../de482256/index.html">KI und die Zukunft der Arbeit: Besch√§ftigungsaussichten in naher Zukunft</a></li>
<li><a href="../de482260/index.html">TelegramBot. Die Grundfunktionalit√§t. Aufkleber und Emoticons. (Teil 3)</a></li>
<li><a href="../de482262/index.html">So melden Sie sich bei Talend Open Studio an</a></li>
<li><a href="../de482264/index.html">Brasilien, dunkle Magie, Mortal Kombat, Mars und 15.000 Menschen. Ontiko Jahresergebnisse</a></li>
<li><a href="../de482268/index.html">Megastrukturen der Zukunft: Die Dyson-Kugel, der Sternmotor und die ‚ÄûSchwarze-Loch-Bombe‚Äú</a></li>
<li><a href="../de482270/index.html">WebRTC-Streaming in und um die virtuelle Realit√§t</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>