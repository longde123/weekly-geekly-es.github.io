<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍🎓 🧛🏾 ⛹🏼 Wie neuronale Netze funktionieren und warum sie anfingen, viel Geld zu bringen 👌🏻 💜 🚀</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale Netze haben sich von einer akademischen Neugierde zu einer massiven Industrie entwickelt 


 In den letzten zehn Jahren haben Computer ihre ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie neuronale Netze funktionieren und warum sie anfingen, viel Geld zu bringen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482258/"><h3>  Neuronale Netze haben sich von einer akademischen Neugierde zu einer massiven Industrie entwickelt </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/21a/fd3/36a21afd35a805d95a2a67b2ec52080a.jpg"><br><br>  In den letzten zehn Jahren haben Computer ihre Fähigkeit, die Welt um sie herum zu verstehen, erheblich verbessert.  Software für Fotoausrüstung erkennt automatisch die Gesichter von Menschen.  Smartphones wandeln Sprache in Text um.  Robomobile erkennen Objekte auf der Straße und vermeiden Kollisionen mit ihnen. <br><br>  Im Zentrum all dieser Durchbrüche steht die Technologie der künstlichen Intelligenz (KI), die als Deep Learning (GO) bezeichnet wird.  GO basiert auf neuronalen Netzwerken (NS), Datenstrukturen, die von Netzwerken aus biologischen Neuronen inspiriert sind.  NS sind in Schichten organisiert, und die Eingänge einer Schicht sind mit den Ausgängen der benachbarten verbunden. <br><br>  Informatiker experimentieren seit den 1950er Jahren mit NS.  Der Grundstein für die heutige große GO-Industrie wurde jedoch durch zwei wichtige Durchbrüche gelegt - einen 1986, den zweiten 2012. Der Durchbruch von 2012 - die Revolution von GO - war mit der Entdeckung verbunden, dass die Verwendung von NS mit einer großen Anzahl von Schichten es uns ermöglicht, deren Effizienz erheblich zu verbessern.  Die Entdeckung wurde durch das wachsende Datenvolumen und die wachsende Rechenleistung erleichtert. <br><a name="habracut"></a><br>  In diesem Artikel stellen wir Ihnen die Welt der Nationalversammlung vor.  Wir werden erklären, was NS ist, wie sie arbeiten und woher sie kommen.  Und wir werden untersuchen, warum NS - trotz jahrzehntelanger Forschung - erst 2012 zu etwas wirklich Nützlichem wurden. <br><br><h2>  Neuronale Netze tauchten bereits in den 1950er Jahren auf </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/770/c2e/327/770c2e3276b0e875a99025f4887dda36.jpg"><br>  <i>Frank Rosenblatt arbeitet an seinem Perzeptron - einem frühen NS-Modell</i> <br><br>  Die Idee der Nationalversammlung ist ziemlich alt - zumindest im Vergleich zu den Standards der Informatik.  Bereits 1957 veröffentlichte <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt</a> von der Cornell University einen <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">Bericht</a> über ein frühes NS-Konzept namens Perceptron.  1958 schuf er mit Unterstützung der US Navy ein primitives System, mit dem 20 x 20 Pixel analysiert und einfache geometrische Formen erkannt werden können. <br><br>  Rosenblatts Hauptziel war es nicht, ein praktisches Bildklassifizierungssystem zu schaffen.  Er versuchte zu verstehen, wie das menschliche Gehirn funktioniert, und schuf Computersysteme, die in seinem Bild organisiert waren.  Dieses Konzept hat jedoch zu viel Begeisterung bei Dritten ausgelöst. <br><br>  "Heute hat die US Navy der Welt den Keim eines elektronischen Computers offenbart, von dem erwartet wird, dass er in der Lage ist, zu gehen, zu sprechen, zu sehen, zu schreiben, sich selbst zu reproduzieren und sich seiner Existenz bewusst zu sein", schrieb die New York Times. <br><br>  Tatsächlich ist jedes Neuron im NS nur eine mathematische Funktion.  Jedes Neuron berechnet die gewichtete Summe der Eingabedaten. Je größer die Eingabegewichtung ist, desto stärker wirken sich diese Eingabedaten auf die Ausgabe des Neurons aus.  Anschließend wird die gewichtete Summe der nichtlinearen Aktivierungsfunktion zugeführt. In diesem Schritt können NS komplexe nichtlineare Phänomene simulieren. <br><br>  Die Fähigkeiten der frühen Perzeptrone, mit denen Rosenblatt experimentierte - und der NS im Allgemeinen - beruhen auf ihrer Fähigkeit, anhand von Beispielen "zu lernen".  NS werden trainiert, indem die Eingabegewichte von Neuronen basierend auf den Ergebnissen des Netzwerks mit den zum Beispiel ausgewählten Eingabedaten abgestimmt werden.  Wenn das Netzwerk das Bild korrekt klassifiziert, nehmen die zur richtigen Antwort beitragenden Gewichte zu, während andere abnehmen.  Wenn das Netzwerk falsch ist, werden die Gewichte in die andere Richtung angepasst. <br><br>  Ein solches Verfahren ermöglichte es frühen NS-Patienten, auf eine Art und Weise zu „lernen“, die an das Verhalten des menschlichen Nervensystems erinnert.  Der Hype um diesen Ansatz hörte in den 1960er Jahren nicht auf.  Das <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_(%25D0%25BA%25D0%25BD%25D0%25B8%25D0%25B3%25D0%25B0)">einflussreiche Buch</a> von 1969 der Autoren der Informatiker Marvin Minsky und Seymour Papert zeigte jedoch, dass diese frühen NA erhebliche Einschränkungen aufweisen. <br><br>  Die frühen Rosenblatt-NS hatten nur ein oder zwei trainierte Schichten.  Minsky und Papert zeigten, dass solche NS mathematisch nicht in der Lage sind, komplexe Phänomene der realen Welt zu modellieren. <br><br>  Im Prinzip waren tiefere NS fähiger.  Solche NS würden jedoch die elenden Computerressourcen, über die Computer zu dieser Zeit verfügten, überfordern.  Die einfachsten <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2%25D0%25BE%25D1%2581%25D1%2585%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC_%25D0%25BA_%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B5">aufsteigenden</a> Suchalgorithmen, die in den ersten NS verwendet wurden, waren für tiefere NS nicht skalierbar. <br><br>  In der Folge verlor die Nationalversammlung in den 1970er und frühen 1980er Jahren jegliche Unterstützung - es war Teil der Ära des „Winters der KI“. <br><br><h2>  Durchbruchsalgorithmus </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/970/c5b/807/970c5b8074a5b4516be251bd4b9a31b0.jpg"><br>  <i>Mein eigenes neuronales Netzwerk, das auf „Soft Equipment“ basiert, geht davon aus, dass die Wahrscheinlichkeit, auf diesem Foto einen Hot Dog zu haben, 1 beträgt. Wir werden reich!</i> <br><br>  Das Glück wandte sich wieder der NS zu, dank des berühmten <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Werks von</a> 1986, das das Konzept der Rückenvermehrung einführte - eine praktische Methode, um NS zu unterrichten. <br><br>  Angenommen, Sie arbeiten als Programmierer in einer imaginären Softwarefirma und wurden angewiesen, eine Anwendung zu erstellen, die feststellt, ob das Image einen Hot Dog enthält.  Sie beginnen die Arbeit mit einer zufällig initialisierten NS, die ein Eingabebild aufnimmt und einen Wert von 0 bis 1 ausgibt. Dabei bedeutet 1 "Hot Dog" und 0 "Nicht-Hot Dog". <br><br>  Um das Netzwerk zu trainieren, sammeln Sie Tausende von Bildern, unter denen sich jeweils ein Etikett befindet, das angibt, ob sich auf diesem Bild ein Hot Dog befindet.  Sie füttern ihr das erste Bild - und es ist ein Hot Dog drauf - im neuronalen Netz.  Es gibt einen Ausgabewert von 0,07, was "kein Hot Dog" bedeutet.  Das ist die falsche Antwort;  Das Netzwerk sollte eine Antwort nahe 1 zurückgegeben haben. <br><br>  Der Backpropagation-Algorithmus dient dazu, die Eingabegewichte so anzupassen, dass das Netzwerk einen höheren Wert erzeugt, wenn es erneut dieses Bild erhält - und vorzugsweise andere Bilder, bei denen es Hot Dogs gibt.  Hierzu untersucht der Backpropagation-Algorithmus zunächst die Eingangsneuronen der Ausgangsschicht.  Jeder Wert hat eine Gewichtsvariable.  Der Backpropagation-Algorithmus passt jedes Gewicht so an, dass der NS einen höheren Wert ergibt.  Je höher der Eingabewert, desto stärker steigt sein Gewicht. <br><br>  Bisher beschreibe ich den einfachsten Aufstieg an die Spitze, den Forscher in den 1960er Jahren kannten.  Der Backpropagation-Durchbruch war der nächste Schritt: Der Algorithmus verwendet partielle Ableitungen, um den „Fehler“ für die fehlerhafte Ausgabe auf die Eingaben von Neuronen zu verteilen.  Der Algorithmus berechnet, wie sich eine kleine Änderung in jedem Eingabewert auf die endgültige Ausgabe eines Neurons auswirkt und ob diese Änderung das Ergebnis näher an die richtige Antwort rückt oder umgekehrt. <br><br>  Das Ergebnis ist eine Reihe von Fehlerwerten für jedes Neuron in der vorherigen Schicht - in der Tat ein Signal, das auswertet, ob der Wert jedes Neurons zu groß oder zu klein ist.  Dann wiederholt der Algorithmus den Abstimmungsprozess für neue Neuronen von der zweiten [von der End-] Schicht.  Dadurch werden die Eingabegewichte der einzelnen Neuronen geringfügig geändert, um das Netzwerk näher an die richtige Antwort heranzuführen. <br><br>  Anschließend berechnet der Algorithmus anhand partieller Ableitungen erneut, wie sich der Wert jeder Eingabe der vorherigen Ebene auf die Ausgabefehler dieser Ebene auswirkt. Diese Fehler werden dann an die vorherige Ebene weitergegeben, wo der Vorgang erneut wiederholt wird. <br><br>  Dies ist nur ein vereinfachtes Backpropagation-Modell.  Wenn Sie detaillierte mathematische Details benötigen, empfehle ich das Buch von Michael Nielsen zu diesem Thema.  transl.].  Für unsere Zwecke ist es ausreichend, dass die umgekehrte Verteilung den Bereich der trainierten NS radikal verändert.  Die Menschen waren nicht länger auf einfache Netzwerke mit einer oder zwei Schichten beschränkt.  Sie könnten Netzwerke mit fünf, zehn oder fünfzig Schichten erstellen, und diese Netzwerke könnten eine willkürlich komplexe interne Struktur aufweisen. <br><br>  Die Erfindung der Backpropagation löste den zweiten Boom der Nationalversammlung aus, der praktische Ergebnisse hervorbrachte.  Eine Gruppe von Forschern von AT &amp; T hat 1998 gezeigt, wie mit neuronalen Netzen handschriftliche Zahlen erkannt werden können, wodurch die Scheckverarbeitung automatisiert werden konnte. <br><br>  "Die Hauptbotschaft dieser Arbeit ist, dass wir verbesserte Systeme zum Erkennen von Mustern entwickeln können, die sich mehr auf automatisches Lernen und weniger auf manuell entwickelte Heuristiken stützen", schrieben die Autoren. <br><br>  Und doch waren NS in dieser Phase nur eine von vielen Technologien, die Forschern des maschinellen Lernens zur Verfügung standen.  Als ich 2008 in einem AI-Kurs am Institut studierte, waren neuronale Netze nur einer von neun MO-Algorithmen, aus denen wir die für die Aufgabe geeignete Option auswählen konnten.  GO bereitete sich jedoch bereits darauf vor, den Rest der Technologie in den Schatten zu stellen. <br><br>  Big Data zeigt die Kraft des Deep Learning <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br>  <i>Entspannung erkannt.</i>  <i>Chance auf Strand 1.0.</i>  <i>Wir beginnen mit der Anwendung von Mai Tai.</i> <br><br>  Backpropagation erleichterte den Prozess der NS-Berechnung, aber tiefere Netzwerke benötigten immer noch mehr Rechenressourcen als kleine.  Die Ergebnisse der in den 1990er und 2000er Jahren durchgeführten Studien zeigten oft, dass es möglich war, von zusätzlichen Komplikationen der NS immer weniger Nutzen zu ziehen. <br><br>  Dann wurde das Denken der Menschen durch das berühmte Werk von 2012 geändert, das die NS unter dem Namen AlexNet beschrieb, benannt nach dem führenden Forscher Alex Krizhevsky.  Ähnlich wie tiefere Netzwerke können sie zu einer bahnbrechenden Effizienz führen, jedoch nur in Kombination mit einer Fülle an Computerleistung und einer enormen Datenmenge. <br><br>  AlexNet hat ein Trio von Informatikern der Universität von Toronto für die Teilnahme am ImageNet-Wissenschaftswettbewerb entwickelt.  Die Organisatoren des Wettbewerbs sammelten im Internet eine Million Bilder, die jeweils einer der Tausenden von Objektkategorien zugeordnet waren, beispielsweise „Kirsche“, „Containerschiff“ oder „Leopard“.  KI-Forscher wurden gebeten, ihre MO-Programme auf Teile dieser Bilder zu trainieren und dann zu versuchen, die korrekten Bezeichnungen für andere Bilder anzubringen, auf die die Software zuvor noch nicht gestoßen war.  Die Software musste für jedes Bild fünf mögliche Bezeichnungen auswählen, und der Versuch wurde als erfolgreich angesehen, wenn eine davon mit der tatsächlichen übereinstimmte. <br><br>  Dies war eine schwierige Aufgabe, und bis 2012 waren die Ergebnisse nicht sehr gut.  Für den Gewinner 2011 betrug die Fehlerquote 25%. <br><br>  2012 übertraf das AlexNet-Team alle Mitbewerber, indem es Antworten mit 15% Fehlern gab.  Für den engsten Wettbewerber lag dieser Wert bei 26%. <br><br>  Forscher aus Toronto kombinierten verschiedene Techniken, um bahnbrechende Ergebnisse zu erzielen.  Eine davon war die Verwendung von <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Faltungsneurosen</a> ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Convolutional Neuroses,</a> SNS).  Tatsächlich trainiert der SNA sozusagen kleine neuronale Netze - deren Eingangsdaten Quadrate mit einer Seite von 7 bis 11 Pixeln sind - und „überlagert“ sie dann auf einem größeren Bild. <br><br>  "Es ist, als ob Sie eine kleine Vorlage oder Schablone genommen und versucht hätten, sie mit jedem Punkt im Bild zu vergleichen", sagte KI-Forscher Jie Tan im vergangenen Jahr.  - Sie haben eine Schablone eines Hundes und hängen sie an das Bild an und prüfen, ob dort ein Hund ist?  Wenn nicht, verschieben Sie die Schablone.  Und so für das ganze Bild.  Und egal wo der Hund auf dem Bild erscheint.  Die Schablone wird damit übereinstimmen.  Jeder Netzwerk-Unterabschnitt sollte kein separater Hundeklassifikator werden. “ <br><br>  Ein weiterer wichtiger Erfolgsfaktor für AlexNet war der Einsatz von Grafikkarten, um den Lernprozess zu beschleunigen.  Grafikkarten verfügen über eine parallele Verarbeitungsleistung, die sich gut für das Repetitive Computing eignet, das zum Trainieren eines neuronalen Netzwerks erforderlich ist.  Durch die Übertragung der Rechenlast auf zwei GPUs - die Nvidia GTX 580 mit jeweils 3 GB Arbeitsspeicher - konnten die Forscher ein extrem großes und komplexes Netzwerk aufbauen und trainieren.  AlexNet hatte acht trainierbare Schichten, 650.000 Neuronen und 60 Millionen Parameter. <br><br>  Der Erfolg von AlexNet wurde schließlich auch durch die Größe der ImageNet-Trainingsbilddatenbank sichergestellt: eine Million Teile.  Für die Feinabstimmung von 60 Millionen Parametern sind viele Bilder erforderlich.  Um einen entscheidenden Sieg zu erzielen, wurde AlexNet von einer Kombination aus einem komplexen Netzwerk und einem großen Datenbestand unterstützt. <br><br>  Ich frage mich, warum so ein Durchbruch nicht früher stattgefunden hat: <br><br><ul><li>  Das von AlexNet-Forschern verwendete GPU-Paar für Endverbraucher war weit davon entfernt, das leistungsstärkste Computergerät für 2012 zu sein.  Fünf und sogar zehn Jahre zuvor gab es leistungsstärkere Computer.  Darüber hinaus ist die Technologie zur Beschleunigung des Lernens von NS mit Grafikkarten seit mindestens 2004 bekannt. </li><li>  Die Basis von einer Million Bildern war 2012 für das Unterrichten von MO-Algorithmen ungewöhnlich groß. Das Sammeln solcher Daten war jedoch keine neue Technologie für dieses Jahr.  Ein gut finanziertes Forschungsteam könnte leicht eine Datenbank dieser Größe vor fünf oder zehn Jahren zusammenstellen. </li><li>  Die in AlexNet verwendeten Hauptalgorithmen waren nicht neu.  Der Backpropagation-Algorithmus von 2012 existierte bereits seit etwa einem Vierteljahrhundert.  Schlüsselideen im Zusammenhang mit neuronalen Faltungsnetzen wurden in den 1980er und 1990er Jahren entwickelt. </li></ul><br>  So existierte jedes der Erfolgselemente von AlexNet lange vor dem Durchbruch für sich.  Offensichtlich fiel niemandem ein, sie zu kombinieren - größtenteils, weil niemand wusste, wie mächtig diese Kombination sein würde. <br><br>  Die Erhöhung der Tiefe der NS hat die Effizienz ihrer Arbeit praktisch nicht verbessert, wenn sie nicht genügend Trainingsdatensätze verwendet haben.  Das Erweitern des Datensatzes hat die Leistung kleiner Netzwerke nicht verbessert.  Um die Effizienzsteigerung zu sehen, benötigten wir sowohl tiefere Netzwerke als auch größere Datenmengen - und eine erhebliche Rechenleistung, die es uns ermöglichte, den Schulungsprozess in angemessener Zeit durchzuführen.  Das AlexNet-Team brachte als erstes alle drei Elemente in einem Programm zusammen. <br><br><h2>  Der Boom des tiefen Lernens </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/ef0/fb4/922/ef0fb4922c3164251c722134b84460e3.jpg"><br><br>  Die Demonstration der ganzen Kraft von Deep NS, die durch eine ausreichende Menge an Trainingsdaten bereitgestellt wurde, wurde von vielen Menschen bemerkt - sowohl von Wissenschaftlern, Forschern als auch von Vertretern der Industrie. <br><br>  Der erste ImageNet-Wettbewerb, der sich ändert.  Bis 2012 verwendeten die meisten Teilnehmer andere Technologien als Deep Learning.  Wie die Sponsoren feststellten, nutzte im Wettbewerb 2013 die Mehrheit der Teilnehmer GO. <br><br>  Die Fehlerquote unter den Gewinnern ging allmählich zurück - von beeindruckenden 16% bei AlexNet im Jahr 2012 auf 2,3% im Jahr 2017: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ba/95e/b4b/9ba95eb4baee6580ea97ac76347a02e4.png"><br><br>  Die GO-Revolution breitete sich schnell in der Branche aus.  2013 erwarb Google ein Startup, das von den Autoren von AlexNet gegründet wurde, und nutzte seine Technologie als Grundlage für die Bildsuchfunktion in Google Fotos.  Bis 2014 hat Facebook eine eigene Software beworben, die Bilder mit GO erkennt.  Apple verwendet seit mindestens 2016 GO für die Gesichtserkennung unter iOS. <br><br>  GO liegt auch der jüngsten Verbesserung der Spracherkennungstechnologie zugrunde.  Siri von Apple, Alexa von Amazon, Cortana von Microsoft und Googles Assistent verwenden GO - entweder, um die Wörter einer Person zu verstehen oder um eine natürlichere Stimme zu erzeugen, oder beides. <br><br>  In den letzten Jahren hat sich in der Branche ein sich selbst tragender Trend herausgebildet, bei dem sich die Zunahme von Rechenleistung, Datenvolumen und Netzwerktiefe gegenseitig stützen.  Das AlexNet-Team nutzte die GPU, weil sie paralleles Rechnen zu einem vernünftigen Preis anbot.  In den letzten Jahren haben jedoch immer mehr Unternehmen begonnen, eigene Chips zu entwickeln, die speziell für den Einsatz im MO-Bereich entwickelt wurden. <br><br>  Google kündigte die Veröffentlichung des Tensor Processing Unit-Chips an, der speziell für den NS entwickelt wurde. Im selben Jahr kündigte Nvidia die Veröffentlichung einer neuen, für den NS optimierten GPU namens Tesla P100 an.  Intel antwortete 2017 mit seinem AI-Chip auf den Anruf. 2018 kündigte Amazon die Veröffentlichung seines eigenen AI-Chips an, der als Teil der Cloud-Dienste des Unternehmens verwendet werden kann.  Sogar Microsoft soll an seinem AI-Chip arbeiten. <br><br>  Smartphone-Hersteller arbeiten auch an Chips, mit denen mobile Geräte mehr lokal mit NS rechnen können, ohne Daten auf Server hochladen zu müssen.  Solches Computing auf Geräten reduziert die Latenz und verbessert die Privatsphäre. <br><br>  Sogar Tesla ist mit speziellen Chips in dieses Spiel eingestiegen.  In diesem Jahr zeigte Tesla einen neuen leistungsstarken Computer, der für die Berechnung von NS optimiert wurde.  Tesla nannte es Full Self-Driving Computer und präsentierte es als Schlüsselmoment in der Strategie des Unternehmens, die Tesla-Flotte in Roboterfahrzeuge zu verwandeln. <br><br>  Die Verfügbarkeit von für KI optimierten Computerkapazitäten hat eine Anforderung nach den Daten erzeugt, die zum Trainieren von immer komplexer werdenden NS erforderlich sind.  Diese Dynamik zeigt sich am deutlichsten im Robomobilsektor, in dem Unternehmen Daten über Millionen Kilometer realer Straßen sammeln.  Tesla kann diese Daten automatisch von den Autos der Benutzer sammeln, und seine Konkurrenten Waymo und Cruise bezahlten Fahrer, die ihre Autos auf öffentlichen Straßen fuhren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/80a/3be/2eb80a3be8fd038777829faff752e8bf.jpg"><br><br>  Die Datenanforderung bietet großen Online-Unternehmen einen Vorteil, die bereits auf große Mengen von Benutzerdaten zugreifen können. <br><br>  Deep Learning hat aufgrund seiner extremen Flexibilität so viele verschiedene Bereiche erobert.  Jahrzehntelange Versuche und Irrtümer haben es den Forschern ermöglicht, die Grundbausteine ​​für die häufigsten Aufgaben im MO-Bereich zu entwickeln - beispielsweise Faltungsnetzwerke für eine effiziente Bilderkennung.  Wenn Sie jedoch ein für das Schema geeignetes Netzwerk auf hoher Ebene und genügend Daten haben, ist der Schulungsprozess einfach.  Deep NSs sind in der Lage, eine außergewöhnlich breite Palette komplexer Muster zu erkennen, ohne die besonderen Anweisungen menschlicher Entwickler zu beachten. <br><br>  Natürlich gibt es Einschränkungen.  Zum Beispiel gönnten sich einige Leute die Idee, Robomobile nur mit Hilfe von GO zu trainieren - das heißt, sie fütterten Bilder, die von einer Kamera oder einem neuronalen Netzwerk empfangen wurden, und erhielten Anweisungen von ihr, das Lenkrad und das Pedal zu drehen.  Ich bin diesem Ansatz skeptisch gegenüber.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Nationalversammlung hat die Fähigkeit, komplexe logische Überlegungen anzustellen, die erforderlich sind, um bestimmte Bedingungen auf der Straße zu verstehen, noch nicht unter Beweis gestellt. </font><font style="vertical-align: inherit;">Darüber hinaus sind NS „Black Boxes“, deren Workflow praktisch unsichtbar ist. </font><font style="vertical-align: inherit;">Es wäre schwierig, die Sicherheit eines solchen Systems zu bewerten und zu bestätigen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit GO konnten jedoch in einem unerwartet großen Anwendungsbereich sehr große Sprünge erzielt werden. </font><font style="vertical-align: inherit;">In den kommenden Jahren ist mit weiteren Fortschritten in diesem Bereich zu rechnen.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de482258/">https://habr.com/ru/post/de482258/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de482248/index.html">Wir teilen unsere Erfahrungen damit, wie SSDs unter RAID angezeigt werden und welche Array-Ebene rentabler ist</a></li>
<li><a href="../de482250/index.html">Eine einfache Zustandsmaschine für VueJS</a></li>
<li><a href="../de482252/index.html">Automatische Katzentoilette - Fortsetzung</a></li>
<li><a href="../de482254/index.html">VonmoTrade-Experiment. Teil 3: Optionsscheinheft. Verarbeitung und Speicherung von Handelsinformationen</a></li>
<li><a href="../de482256/index.html">KI und die Zukunft der Arbeit: Beschäftigungsaussichten in naher Zukunft</a></li>
<li><a href="../de482260/index.html">TelegramBot. Die Grundfunktionalität. Aufkleber und Emoticons. (Teil 3)</a></li>
<li><a href="../de482262/index.html">So melden Sie sich bei Talend Open Studio an</a></li>
<li><a href="../de482264/index.html">Brasilien, dunkle Magie, Mortal Kombat, Mars und 15.000 Menschen. Ontiko Jahresergebnisse</a></li>
<li><a href="../de482268/index.html">Megastrukturen der Zukunft: Die Dyson-Kugel, der Sternmotor und die „Schwarze-Loch-Bombe“</a></li>
<li><a href="../de482270/index.html">WebRTC-Streaming in und um die virtuelle Realität</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>