<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÖøÔ∏è üîª üå∞ Analisis pewarnaan emosional ulasan dari Kinopoisk üêë üßîüèº üå°Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entri 
 Natural Language Processing (NLP) adalah bidang pembelajaran mesin yang populer dan penting. Di hub ini, saya akan menjelaskan proyek pertama ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analisis pewarnaan emosional ulasan dari Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Entri </h3><br>  Natural Language Processing (NLP) adalah bidang pembelajaran mesin yang populer dan penting.  Di hub ini, saya akan menjelaskan proyek pertama saya yang terkait dengan analisis pewarnaan emosional ulasan film yang ditulis dengan Python.  Tugas analisis sentimental cukup umum di antara mereka yang ingin menguasai konsep dasar NLP, dan dapat menjadi analog dari 'Hello world' di bidang ini. <br><br>  Pada artikel ini, kita akan melalui semua tahapan utama proses Ilmu Data: mulai dari membuat dataset Anda sendiri, memprosesnya dan mengekstraksi fitur menggunakan perpustakaan NLTK, dan akhirnya mempelajari dan menyetel model menggunakan scikit-learning.  Tugas itu sendiri adalah untuk mengklasifikasikan ulasan menjadi tiga kelas: negatif, netral dan positif. <br><a name="habracut"></a><br><h3>  Formasi Data Corpus </h3><br>  Untuk mengatasi masalah ini, orang dapat menggunakan beberapa badan data yang sudah jadi dan beranotasi dengan ulasan dari IMDB, yang banyak terdapat di GitHub.  Tetapi diputuskan untuk membuat ulasan Anda sendiri dalam bahasa Rusia yang diambil dari Kinopoisk.  Agar tidak menyalinnya secara manual, kami akan menulis web parser.  Saya akan menggunakan perpustakaan <i>permintaan</i> untuk mengirim <i>permintaan</i> http, dan <i>BeautifulSoup</i> untuk memproses file html.  Pertama, mari kita tentukan fungsi yang akan membawa tautan ke ulasan film dan mengambilnya.  Agar Kinopoisk tidak mengenali bot di kami, Anda perlu menentukan argumen <i>header</i> di fungsi <i>requests.get</i> , yang akan mensimulasikan browser.  Penting untuk memasukkan kamus ke dalamnya dengan tombol User-Agent, Accept-language dan Accept, yang nilainya dapat ditemukan di alat pengembang browser.  Selanjutnya, pengurai dibuat dan ulasan diambil dari halaman, yang disimpan dalam kelas markup html _reachbanner_. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Kami menyingkirkan markah html, namun, ulasan kami masih berupa objek <i>BeautifulSoup</i> , tetapi kami harus mengubahnya menjadi string.  Fungsi <i>konversi</i> tidak hanya itu.  Kami juga akan menulis fungsi yang mengambil nama film, yang nantinya akan digunakan untuk menyimpan ulasan. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  Fungsi parser terakhir akan mengambil tautan ke halaman utama film, kelas ulasan dan cara untuk menyimpan ulasan.  Fungsi ini juga mendefinisikan <i>penundaan</i> antara permintaan yang diperlukan untuk menghindari larangan.  Fungsi ini berisi loop yang mengambil dan menyimpan ulasan mulai dari halaman pertama, sampai bertemu dengan halaman yang tidak ada di mana fungsi <i>load_data</i> akan mengekstrak daftar kosong dan loop akan rusak. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Kemudian, menggunakan siklus berikut, Anda dapat mengekstrak ulasan dari film yang ada di daftar <i>urles</i> .  Daftar film perlu dibuat secara manual.  Mungkin saja, misalnya, untuk mendapatkan daftar tautan ke film dengan menulis fungsi yang akan mengekstraknya dari 250 film teratas dari pencarian film, sehingga tidak melakukannya secara manual, tetapi 15-20 film akan cukup untuk membentuk dataset kecil dari seribu ulasan untuk setiap kelas.  Juga, jika Anda mendapatkan larangan, program akan menampilkan film dan kelas mana parser berhenti untuk melanjutkan dari tempat yang sama setelah melewati larangan. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Pretreatment </h3><br>  Setelah menulis parser, mengingat film acak untuknya dan beberapa larangan dari pencarian film, saya mencampur ulasan dalam folder dan memilih 900 ulasan dari setiap kelas untuk pelatihan dan sisanya untuk kelompok kontrol.  Sekarang perlu untuk pra-proses perumahan, yaitu, tokenize dan menormalkannya.  Tokenizing berarti memecah teks menjadi beberapa komponen, dalam hal ini menjadi kata-kata, karena kita akan menggunakan representasi sekumpulan kata.  Dan normalisasi terdiri dari mengubah kata menjadi huruf kecil, menghilangkan kata-kata henti dan kebisingan berlebih, gagap, dan trik lainnya yang membantu mengurangi ruang tanda. <br><br>  Kami mengimpor perpustakaan yang diperlukan. <br><br><div class="spoiler">  <b class="spoiler_title">Teks tersembunyi</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Kami mulai dengan mendefinisikan beberapa fungsi kecil untuk preprocessing teks.  Yang pertama, disebut <i>lower_pos_tag,</i> akan membawa daftar dengan kata-kata, mengonversikannya menjadi huruf kecil, dan menyimpan setiap token ke dalam tuple dengan bagian bicaranya.  Operasi penambahan part of speech ke sebuah kata disebut penandaan Part of speech (POS) dan sering digunakan dalam NLP untuk mengekstrak entitas.  Dalam kasus kami, kami akan menggunakan bagian-bagian ucapan dalam fungsi berikut untuk menyaring kata-kata. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Teks-teks tersebut mengandung sejumlah besar kata-kata yang terlalu sering ditemukan sehingga tidak berguna untuk model (yang disebut kata-kata berhenti).  Pada dasarnya, ini adalah preposisi, konjungsi, kata ganti yang dengannya tidak mungkin untuk menentukan yang merujuk kelas recall.  Fungsi <i>bersih</i> hanya menyisakan kata benda, kata sifat, kata kerja dan kata keterangan.  Perhatikan bahwa itu menghilangkan bagian-bagian ucapan, karena mereka tidak diperlukan untuk model itu sendiri.  Anda juga dapat memperhatikan bahwa fungsi ini menggunakan stamming, yang intinya adalah untuk menghilangkan sufiks dan awalan dari kata-kata.  Ini memungkinkan Anda mengurangi dimensi tanda, karena kata-kata dengan genera dan huruf yang berbeda akan dikurangi menjadi token yang sama.  Ada analog yang lebih kuat dari pengaplikasian - lemmatization, ini memungkinkan Anda untuk mengembalikan bentuk awal kata tersebut.  Namun, ini bekerja lebih lambat daripada gagap, dan, di samping itu, NLTK tidak memiliki lemmatizer Rusia. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  Selanjutnya, kita menulis fungsi final yang akan mengambil label kelas dan mengambil semua ulasan dengan kelas ini.  Untuk membaca kasus ini, kita akan menggunakan metode <i>mentah</i> objek <i>PlaintextCorpusReader</i> , yang memungkinkan Anda untuk mengekstrak teks dari file yang ditentukan.  Selanjutnya, tokenization digunakan RegexpTokenizer, bekerja berdasarkan ekspresi reguler.  Selain kata-kata individual, saya menambahkan ke model bigrams, yang merupakan kombinasi dari semua kata tetangga.  Fungsi ini juga menggunakan objek <i>FreqDist</i> , yang mengembalikan frekuensi kemunculan kata.  Ini digunakan di sini untuk menghapus kata-kata yang muncul di semua ulasan dari kelas tertentu hanya sekali (mereka juga disebut hapaks).  Dengan demikian, fungsi akan mengembalikan kamus berisi dokumen yang disajikan sebagai sekumpulan kata dan daftar semua kata untuk kelas tertentu. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  Tahap pra-pemrosesan adalah yang terpanjang, jadi masuk akal untuk memparalelkan pemrosesan kasus kami.  Ini dapat dilakukan dengan menggunakan modul <i>multiprosesing</i> .  Pada bagian selanjutnya dari kode program, saya memulai tiga proses yang secara bersamaan akan memproses tiga folder dengan kelas yang berbeda.  Selanjutnya, hasilnya akan dikumpulkan dalam satu kamus.  Preprocessing ini selesai. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vektorisasi </h3><br>  Setelah kami memproses kasus sebelumnya, kami memiliki kamus di mana untuk setiap label kelas berisi daftar dengan ulasan yang kami token, normalkan, dan diperkaya dengan bigrams, serta daftar kata-kata dari semua ulasan kelas ini.  Karena model tidak dapat memahami bahasa alami seperti yang kita lakukan, tugasnya sekarang adalah menyajikan ulasan kami dalam bentuk angka.  Untuk melakukan ini, kami akan membuat kosakata umum, yang terdiri dari token unik, dan dengan itu kami akan membuat vektor setiap ulasan. <br><br>  Untuk mulai dengan, kami membuat daftar yang berisi ulasan dari semua kelas bersama dengan label mereka.  Selanjutnya, kita membuat kosakata umum, mengambil dari setiap kelas 10.000 kata yang paling umum menggunakan metode <i>most_common</i> dari <i>FreqDist yang</i> sama.  Sebagai hasilnya, saya mendapat kosakata yang terdiri dari sekitar 17.000 kata. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Ada beberapa cara untuk membuat vektor teks.  Yang paling populer di antaranya: TF-IDF, pengkodean langsung dan frekuensi.  Saya menggunakan pengkodean frekuensi, intinya adalah untuk menyajikan setiap ulasan sebagai vektor, unsur-unsurnya adalah jumlah kemunculan setiap kata dari kosa kata.  <i>NLTK</i> memiliki pengklasifikasi sendiri, Anda dapat menggunakannya, tetapi mereka bekerja lebih lambat daripada rekan-rekan mereka dari <i>scikit-learn</i> dan memiliki lebih sedikit pengaturan.  Di bawah ini adalah kode untuk pengkodean untuk <i>NLTK</i> .  Namun, saya akan menggunakan model Naive Bayes dari <i>scikit-belajar</i> dan mengkodekan ulasan, menyimpan atribut dalam matriks jarang dari <i>SciPy</i> , dan label kelas dalam array <i>NumPy yang</i> terpisah. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Karena dalam dataset ulasan dengan tag tertentu berjalan satu demi satu, yaitu, pertama semua netral, lalu semua negatif dan seterusnya, Anda perlu mencampurnya.  Untuk melakukan ini, Anda dapat menggunakan fungsi <i>acak</i> dari <i>scikit-learn</i> .  Ini hanya cocok untuk situasi ketika tanda dan label kelas berada dalam array yang berbeda, karena memungkinkan Anda untuk mencampur dua array secara bersamaan. <br><br><h3>  Pelatihan model </h3><br>  Sekarang tinggal melatih model dan memeriksa akurasinya pada kelompok kontrol.  Sebagai model, kita akan menggunakan model classifier Naive Bayes.  <i>Scikit-belajar</i> memiliki tiga model Naive Bayes tergantung pada distribusi data: biner, diskrit, dan kontinu.  Karena distribusi fitur kami terpisah, kami memilih <i>MultinomialNB</i> . <br><br>  Pengklasifikasi Bayesian memiliki <i>parameter alfa</i> hiper, yang bertanggung jawab untuk menghaluskan model.  Naif Bayes menghitung probabilitas setiap ulasan milik semua kelas, untuk ini mengalikan probabilitas kondisional dari penampilan semua kata ulasan, asalkan mereka milik kelas tertentu.  Tetapi jika beberapa kata ulasan tidak ditemukan dalam kumpulan data pelatihan, maka probabilitas kondisionalnya sama dengan nol, yang membatalkan kemungkinan bahwa ulasan tersebut milik kelas apa pun.  Untuk menghindari ini, secara default, sebuah unit ditambahkan ke semua probabilitas kata bersyarat, mis. <i>Alpha</i> sama dengan satu.  Namun, nilai ini mungkin tidak optimal.  Anda dapat mencoba memilih <i>alpha</i> menggunakan pencarian kisi dan validasi silang. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  Dalam kasus saya, kisi perapian memberikan nilai optimal dari hiperparameter sama dengan 0 dengan akurasi 0,965.  Namun, nilai ini jelas tidak akan optimal untuk dataset kontrol, karena akan ada sejumlah besar kata yang belum ditemukan sebelumnya di set pelatihan.  Untuk dataset referensi, model ini memiliki akurasi 0,598.  Namun, jika Anda meningkatkan <i>alfa</i> ke 0,1, keakuratan pada data pelatihan akan turun menjadi 0,82, dan pada data kontrol itu akan meningkat menjadi 0,62.  Kemungkinan besar, pada kumpulan data yang lebih besar, perbedaannya akan lebih signifikan. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Kesimpulan </h3><br>  Diasumsikan bahwa model tersebut harus digunakan untuk memprediksi ulasan yang kata-katanya tidak digunakan untuk membentuk kosa kata.  Oleh karena itu, kualitas model dapat dievaluasi dengan keakuratannya pada bagian kontrol data, yaitu 0,62.  Ini hampir dua kali lebih baik daripada hanya menebak, tetapi akurasinya masih sangat rendah. <br><br>  Menurut laporan klasifikasi, jelas bahwa model melakukan yang terburuk dengan ulasan yang memiliki warna netral (akurasi 0,47 berbanding 0,68 untuk positif dan 0,76 untuk negatif).  Memang, ulasan netral berisi kata-kata yang merupakan karakteristik dari ulasan positif dan negatif.  Mungkin, keakuratan model dapat ditingkatkan dengan meningkatkan volume dataset, karena set data tiga ribu agak sederhana.  Juga, akan mungkin untuk mengurangi masalah menjadi klasifikasi biner dari tinjauan menjadi positif dan negatif, yang juga akan meningkatkan akurasi. <br><br>  Terima kasih sudah membaca. <br><br>  PS Jika Anda ingin berlatih sendiri, dataset saya dapat diunduh di bawah tautan. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tautan ke dataset</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id467081/">https://habr.com/ru/post/id467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id467061/index.html">Baris Babilonia: 5 masalah keamanan dalam bisnis konstruksi</a></li>
<li><a href="../id467063/index.html">Pemantauan bahan bakar untuk generator diesel pusat data - bagaimana melakukannya dan mengapa begitu penting?</a></li>
<li><a href="../id467065/index.html">Arsip masalah olimpiade dalam fisika untuk anak sekolah</a></li>
<li><a href="../id467073/index.html">‚ÄúDi Barat, tidak ada direktur seni di bawah usia 40 tahun. Bersama kami itu bisa hingga 30. " Bagaimana rasanya menjadi seorang desainer di bidang TI</a></li>
<li><a href="../id467079/index.html">CSS dan Korsel Ant Javascript</a></li>
<li><a href="../id467083/index.html">Bagaimana instruksi popcount aneh digunakan dalam prosesor modern</a></li>
<li><a href="../id467085/index.html">Dekompilasi C, C ++ dan DotNet adalah dasar dari kebalikannya. Memecahkan masalah untuk membalikkan dengan r0ot-mi. Bagian 1</a></li>
<li><a href="../id467087/index.html">Bagaimana saya mempersiapkan dan lulus Sertifikasi Oracle Database SQL (1Z0-071)</a></li>
<li><a href="../id467089/index.html">Patched Exim - patch lagi. Eksekusi Perintah Remote Baru di Exim 4.92 dalam satu permintaan</a></li>
<li><a href="../id467091/index.html">Pengantar cepat ke Svelte dari perspektif pengembang Angular</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>