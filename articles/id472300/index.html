<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍌 👩🏽‍🤝‍👨🏿 ✊🏼 Stochastic gradient descent (SGD) untuk fungsi kehilangan logaritmik (LogLoss) dalam masalah klasifikasi biner 🎆 🐫 😍</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bagian sebelumnya (tentang regresi linier, gradient descent, dan cara kerjanya) - habr.com/en/post/471458 

 Pada artikel ini, saya akan menunjukkan s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stochastic gradient descent (SGD) untuk fungsi kehilangan logaritmik (LogLoss) dalam masalah klasifikasi biner</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/472300/">  Bagian sebelumnya (tentang regresi linier, gradient descent, dan cara kerjanya) - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">habr.com/en/post/471458</a> <br><br>  Pada artikel ini, saya akan menunjukkan solusi untuk masalah klasifikasi pertama, seperti yang mereka katakan, "pena", tanpa perpustakaan pihak ketiga untuk SGD, LogLoss dan menghitung gradien, dan kemudian menggunakan perpustakaan PyTorch. <br><a name="habracut"></a><br>  Tujuan: untuk dua fitur kategorikal yang menggambarkan kekuningan dan simetri, tentukan kelas mana (apel atau pir) yang dimiliki objek (ajarkan model untuk mengklasifikasikan objek). <br><br>  Untuk memulai, unggah dataset kami: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd data = pd.read_csv(<span class="hljs-string"><span class="hljs-string">"https://raw.githubusercontent.com/DLSchool/dlschool_old/master/materials/homeworks/hw04/data/apples_pears.csv"</span></span>) data.head(<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/zi/7n/21/zi7n21oxhiurq3oxapk59kagznm.jpeg"><br><br>  Biarkan: x1 - kekuningan, x2 - simetri, y = lebih besar <br><br>  Kami menyusun fungsi y = w1 * x1 + w2 * x2 + w0 <br>  (w0 akan dianggap bias (eng. - bias)) <br><br>  Sekarang tugas kita direduksi menjadi menemukan bobot w1, w2, dan w0, yang paling akurat menggambarkan ketergantungan y pada x1 dan x2. <br><br>  Kami menggunakan fungsi kehilangan logaritmik: <br><br><img src="https://habrastorage.org/webt/s0/_q/w-/s0_qw-t3ccacue2bagzpxmnhaum.jpeg"><br><br>  Parameter kiri dari fungsi adalah prediksi dengan bobot saat ini w1, w2, w0 <br><br>  Parameter yang benar dari fungsi adalah nilai yang benar (kelas adalah 0 atau 1) <br><br>  σ (x) adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">fungsi aktivasi</a> sigmoid dari x <br><br>  log (x) - logaritma natural x <br><br>  Jelas bahwa semakin kecil nilai fungsi kerugian, semakin baik kita memilih bobot w1, w2, w0.  Untuk melakukan ini, pilih <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">keturunan gradien stokastik</a> . <br><br>  Saya perhatikan bahwa rumus untuk LogLoss akan mengambil pandangan yang berbeda mengingat fakta bahwa dalam SGD kita memilih satu elemen dan bukan seluruh pilihan (atau subsampel, seperti dalam kasus penurunan gradien mini-batch): <br><img src="https://habrastorage.org/webt/cy/jp/b6/cyjpb6dc7zctozuf7yekyf1yimw.jpeg"><br><br>  <b>Kemajuan solusi:</b> <br><br>  Bobot awal w1, w2, w0 diberi nilai acak <br><br>  Kami mengambil objek ke-i tertentu dari dataset kami (misalnya, acak), menghitung LogLoss untuknya (dengan w1, w2, dan w0, yang awalnya kami berikan nilai acaknya), kemudian kami menghitung turunan parsial untuk masing-masing bobot w1, w2 dan w0, kemudian perbarui masing-masing bobot. <br><br>  <b>Sedikit persiapan:</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np X = data.iloc[:,:<span class="hljs-number"><span class="hljs-number">2</span></span>].values <span class="hljs-comment"><span class="hljs-comment">#  - y = data['target'].values.reshape((-1, 1)) #  (    ) x1 = X[:, 0] x2 = X[:, 1] def sigmoid(x): return 1 / (1 + np.exp(-x))</span></span></code> </pre> <br><br>  <b>Implementasi:</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random np.random.seed(<span class="hljs-number"><span class="hljs-number">62</span></span>) w1 = np.random.randn(<span class="hljs-number"><span class="hljs-number">1</span></span>) w2 = np.random.randn(<span class="hljs-number"><span class="hljs-number">1</span></span>) w0 = np.random.randn(<span class="hljs-number"><span class="hljs-number">1</span></span>) print(w1, w2, w0) <span class="hljs-comment"><span class="hljs-comment"># form range 0..999 idx = np.arange(1000) # random shuffling np.random.shuffle(idx) x1, x2, y = x1[idx], x2[idx], y[idx] # learning rate lr = 0.001 # number of epochs n_epochs = 10000 for epoch in range(n_epochs): i = random.randint(0, 999) yhat = w1 * x1[i] + w2 * x2[i] + w0 w1_grad = -((y[i] - sigmoid(yhat)) * x1[i]) w2_grad = -((y[i] - sigmoid(yhat)) * x2[i]) w0_grad = -(y[i] - sigmoid(yhat)) w1 -= lr * w1_grad w2 -= lr * w2_grad w0 -= lr * w0_grad print(w1, w2, w0)</span></span></code> </pre><br>  [0.49671415] [-0.1382643] [0.64768854] <br>  [0.87991625] [-1.14098372] [0.22355905] <br><br>  * _grad adalah turunan dari bobot yang sesuai.  Saya akan menulis formula umum: <br><br><img src="https://habrastorage.org/webt/7u/ci/b1/7ucib1aknkxfzksdzr6jmjs41kc.jpeg"><br><br>  Untuk istilah bebas w0 - faktor x dihilangkan (diambil sama dengan satu). <br><br>  Dengan menggunakan rumus akhir derivatif, kita dapat melihat bahwa kita tidak perlu menghitung fungsi kerugian secara eksplisit (kita hanya perlu derivatif parsial). <br><br>  Mari kita periksa berapa banyak objek dari set pelatihan model kita memberikan jawaban yang benar, dan berapa banyak - yang salah. <br><br><pre> <code class="python hljs">i = <span class="hljs-number"><span class="hljs-number">0</span></span> correct = <span class="hljs-number"><span class="hljs-number">0</span></span> incorrect = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> y: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(np.around(x1[i] * w1 + x2[i] * w2 + w0) == item): correct += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: incorrect += <span class="hljs-number"><span class="hljs-number">1</span></span> i = i + <span class="hljs-number"><span class="hljs-number">1</span></span> print(correct, incorrect)</code> </pre> <br>  925 75 <br><br>  np.around (x) - membulatkan nilai x.  Bagi kami: jika x&gt; 0,5, maka nilainya adalah 1. Jika x ≤ 0,5, maka nilainya adalah 0. <br><br>  Dan apa yang akan kita lakukan jika jumlah fitur objek adalah 5?  10?  100?  Dan kita akan memiliki jumlah bobot yang sesuai (ditambah satu untuk bias).  Jelas bahwa secara manual bekerja dengan setiap bobot, menghitung gradien untuk itu tidak nyaman. <br><br>  Kami akan menggunakan pustaka PyTorch yang populer. <br><br>  PyTorch = NumPy + <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CUDA</a> + Autograd (perhitungan gradien otomatis) <br><br>  Implementasi PyTorch: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Linear, Sigmoid <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, loss_fn, optimizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> model.train() yhat = model(x) loss = loss_fn(yhat, y) loss.backward() optimizer.step() optimizer.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_step X = torch.FloatTensor(data.iloc[:,:<span class="hljs-number"><span class="hljs-number">2</span></span>].values) y = torch.FloatTensor(data[<span class="hljs-string"><span class="hljs-string">'target'</span></span>].values.reshape((<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optim, nn neuron = torch.nn.Sequential( Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, out_features=<span class="hljs-number"><span class="hljs-number">1</span></span>), Sigmoid() ) print(neuron.state_dict()) lr = <span class="hljs-number"><span class="hljs-number">0.1</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">10000</span></span> loss_fn = nn.MSELoss(reduction=<span class="hljs-string"><span class="hljs-string">"mean"</span></span>) optimizer = optim.SGD(neuron.parameters(), lr=lr) train_step = make_train_step(neuron, loss_fn, optimizer) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): loss = train_step(X, y) print(neuron.state_dict()) print(loss)</code> </pre> <br>  OrderedDict ([('0,weight', tensor ([[- 0,4148, -0,5838]]))), ('0.bias', tensor ([0,5448])]])) <br>  OrderedDict ([('0.weight', tensor ([[5.4915, -8.2156]]))), ('0.bias', tensor ([- 1.1130])]])) <br>  0,03930133953690529 <br><br>  Kerugian yang cukup baik pada sampel uji. <br><br>  Di sini, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">MSELoss</a> dipilih sebagai fungsi kerugian. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Lebih lanjut tentang Linear</a> <br><br>  Singkatnya: kita memberikan 2 parameter ke input (x1 dan x2 kita seperti dalam contoh sebelumnya) dan kita mendapatkan satu parameter (y) untuk output, yang, pada gilirannya, diumpankan ke input fungsi aktivasi.  Dan kemudian mereka sudah dihitung: nilai fungsi kesalahan, gradien.  Pada akhirnya - bobot diperbarui. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bahan yang digunakan dalam artikel</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id472300/">https://habr.com/ru/post/id472300/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id472290/index.html">Struktur vs. Kelas</a></li>
<li><a href="../id472292/index.html">Pemblokiran Konten: Panggung Dunia</a></li>
<li><a href="../id472294/index.html">Buat game dan video di YouTube. Percobaan interaksi saya dan penghasilan dari ini</a></li>
<li><a href="../id472296/index.html">Sistem perlindungan kebocoran untuk mesin cuci</a></li>
<li><a href="../id472298/index.html">Intisari bahan-bahan segar dari dunia front-end untuk minggu terakhir No. 385 (14-20 Oktober 2019)</a></li>
<li><a href="../id472304/index.html">NASA merekrut insinyur untuk mengembangkan robot humanoid generasi berikutnya</a></li>
<li><a href="../id472306/index.html">PHP Digest No. 166 (7-21 Oktober, 2019)</a></li>
<li><a href="../id472310/index.html">Identifikasi klien di situs tanpa kata sandi dan cookie: aplikasi untuk standar</a></li>
<li><a href="../id472312/index.html">Pengalaman mentransfer proyek Maven ke Jar Multi-Rilis: sudah mungkin, tetapi masih sulit</a></li>
<li><a href="../id472314/index.html">Saya suka orang kardus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>