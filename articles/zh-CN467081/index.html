<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤺 👩‍👧‍👦 🌃 分析《 Kinopoisk》评论的情感色彩 🤪 🐮 🚞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="参赛作品 
 自然语言处理（NLP）是机器学习的流行且重要领域。 在这个中心，我将描述我的第一个项目，该项目与以Python编写的电影评论的情感色彩分析有关。 情感分析的任务在想要掌握NLP基本概念的人们中很常见，并且可以成为该领域“ Hello world”的类似物。 

 在本文中，我们将经历数...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>分析《 Kinopoisk》评论的情感色彩</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3> 参赛作品 </h3><br> 自然语言处理（NLP）是机器学习的流行且重要领域。 在这个中心，我将描述我的第一个项目，该项目与以Python编写的电影评论的情感色彩分析有关。 情感分析的任务在想要掌握NLP基本概念的人们中很常见，并且可以成为该领域“ Hello world”的类似物。 <br><br> 在本文中，我们将经历数据科学过程的所有主要阶段：从创建自己的数据集，对其进行处理并使用NLTK库提取特征，最后使用scikit-learn学习和调整模型。 任务本身就是将评论分为三类：负面，中立和正面。 <br><a name="habracut"></a><br><h3> 数据语料库的形成 </h3><br> 为了解决这个问题，可以使用一些现成的，带有注释的数据主体以及IMDB的评论，其中有很多在GitHub上。 但是决定使用Kinopoisk的俄语评论来创建您自己的评论。 为了不手动复制它们，我们将编写一个Web解析器。 我将使用<i>请求</i>库发送http <i>请求</i> ，并使用<i>BeautifulSoup</i>处理html文件。 首先，让我们定义一个将链接到电影评论并检索它们的函数。 为了使Kinopoisk无法识别我们中的机器人，您需要在request.get函数中指定<i>headers</i>参数，该函数将模拟浏览器。 必须使用User-Agent，Accept-language和Accept键将字典传递到其中，其值可以在浏览器开发人员工具中找到。 接下来，创建一个解析器，并从页面中检索评论，这些评论存储在_reachbanner_ html标记类中。 <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br> 我们摆脱了html标记，但是，我们的评论仍然是<i>BeautifulSoup</i>对象，但是我们需要将它们转换为字符串。  <i>convert</i>函数就是这样做的。 我们还将编写一个函数来检索电影的名称，该名称随后将用于保存评论。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br> 解析器的最后一个功能将带有指向电影主页的链接，评论类以及保存评论的方法。 该功能还定义了避免禁令所必需的请求之间的<i>延迟</i> 。 该函数包含一个循环，该循环从第一页开始检索并存储评论，直到遇到不存在的页面为止， <i>load_data</i>函数将从该页面提取一个空列表，并且循环将中断。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br> 然后，使用以下循环，您可以从<i>urles</i>列表中的电影中提取评论。 电影列表将需要手动创建。 例如，有可能通过编写一个函数来获取电影的链接列表，该函数将从前250部电影搜索电影中提取电影，而不是手动进行，但是15至20部电影就足以形成一个小数据集，每个班级都有一千条评论。 另外，如果您获得了禁令，则该程序将显示解析器在哪部影片和类上停止播放，以便在通过禁令后从同一位置继续。 <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3> 预处理 </h3><br> 编写了一个解析器，为他召回了随机的电影和电影搜索的若干禁令后，我将评论混入了文件夹中，并从每个班级选择了900条评论进行培训，其余的则作为对照组的评论。 现在有必要对外壳进行预处理，即对其进行标记化和标准化。 标记化是指将文本分解为多个部分，在这种情况下为单词，因为我们将使用一袋单词的表示形式。 归一化包括将单词转换为小写字母，消除停用词和过多的噪音，阻塞以及其他有助于减少符号空间的技巧。 <br><br> 我们导入必要的库。 <br><br><div class="spoiler">  <b class="spoiler_title">隐藏文字</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br> 我们首先定义一些用于文本预处理的小函数。 第一个称为<i>lower_pos_tag，</i>它将获取一个包含单词的列表，将其转换为小写，然后将每个标记及其词性保存到一个元组中。 将词性添加到单词的操作称为词性（POS）标记，通常在NLP中用于提取实体。 在我们的例子中，我们将在以下功能中使用词性来过滤单词。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br> 文本中包含大量的单词，这些单词经常被发现对模型没有用（所谓的停用词）。 基本上，这些是介词，连词，代词，通过它们无法确定召回的类别。  <i>clean</i>函数仅保留名词，形容词，动词和副词。 请注意，由于模型本身不需要这些部分，因此它会删除部分词性。 您还可以注意到该函数使用了填充功能，其实质是从单词中删除后缀和前缀。 这使您可以减小符号的尺寸，因为具有不同属和大小写的单词将被简化为相同的标记。 有一个更强大的词干比喻-lemmatization，它使您可以恢复单词的初始形式。 但是，它的工作速度比填塞要慢，此外，NLTK还没有俄语lemmatizer。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br> 接下来，我们编写最终函数，该函数将带有类标签并检索该类的所有评论。 为了阅读案例，我们将使用<i>PlaintextCorpusReader</i>对象的<i>raw</i>方法，该方法允许您从指定的文件中提取文本。 接下来，在正则表达式的基础上使用RegexpTokenizer进行标记化。 除了单个单词之外，我还在模型bigrams中添加了所有相邻单词的组合。 此函数还使用<i>FreqDist</i>对象，该对象返回单词出现的频率。 它在此处用于删除在特定类别的所有评论中仅出现一次的单词（它们也称为hapaks）。 因此，该函数将返回一个字典，其中包含以单词袋形式呈现的文档以及特定类的所有单词列表。 <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br> 预处理阶段是最长的，因此并行处理我们的案件是有意义的。 这可以使用<i>多处理</i>模块来完成。 在下一部分程序代码中，我将启动三个进程，这些进程将同时处理具有不同类的三个文件夹。 接下来，将结果收集在一个字典中。 预处理完成。 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3> 向量化 </h3><br> 在对案例进行预处理之后，我们将拥有一本字典，其中的每个类别标签都包含一个列表，其中包含我们对标记进行了标记化，规范化和丰富化的评论，以及该类所有评论中的单词列表。 由于模型无法像我们一样感知自然语言，因此现在的任务是以数字形式显示我们的评论。 为此，我们将创建一个由唯一标记组成的通用词汇表，并通过它对每个评论进行矢量化处理。 <br><br> 首先，我们创建一个列表，其中包含所有类别的评论及其标签。 接下来，我们创建一个通用词汇表，使用同一<i>FreqDist</i>的<i>most_common</i>方法从每个类中提取10,000个最常用的单词。 结果，我得到了约17,000个单词的词汇。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br> 有几种矢量化文本的方法。 其中最受欢迎的是：TF-IDF，直接和频率编码。 我使用了频率编码，其本质是将每个评论作为矢量呈现，其要素是词汇中每个单词出现的次数。  <i>NLTK</i>拥有自己的分类器，您可以使用它们，但是它们的工作速度比<i>scikit-learn</i>的分类器慢，并且设置较少。 以下是用于<i>NLTK</i>编码的代码。 但是，我将使用<i>scikit-learn</i>的朴素贝叶斯模型并对评论进行编码，将属性存储在<i>SciPy</i>的稀疏矩阵中，并将类标签存储在单独的<i>NumPy</i>数组中。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br> 由于在数据集中带有特定标签的评论会依次出现，也就是说，所有评论都是中性的，然后都是负面的，依此类推，因此您需要将它们混合在一起。 为此，您可以使用<i>scikit-learn中</i>的<i>shuffle</i>函数。 它仅适用于符号和类标签位于不同数组中的情况，因为它允许您将两个数组统一使用。 <br><br><h3> 模型训练 </h3><br> 现在，仍然需要训练模型并在对照组中检查其准确性。 作为模型，我们将使用朴素贝叶斯分类器的模型。  <i>Scikit-learn</i>根据数据分布具有三种朴素贝叶斯模型：二进制，离散和连续。 由于特征的分布是离散的，因此我们选择<i>MultinomialNB</i> 。 <br><br> 贝叶斯分类器具有<i>alpha</i> hyper <i>参数</i> ，该<i>参数</i>负责平滑模型。 朴素贝叶斯（Naive Bayes）计算属于所有类别的每个评论的概率，如果所有评论单词都属于特定类别，则乘以所有评论单词出现的条件概率。 但是，如果在训练数据集中未找到某个复习词，则其条件概率等于零，这会使该复习属于任何类别的概率无效。 为了避免这种情况，默认情况下，将一个单位添加到所有条件词概率中，即<i>alpha</i>等于1。 但是，此值可能不是最佳的。 您可以尝试使用网格搜索和交叉验证来选择<i>Alpha</i> 。 <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br> 在我的情况下，网格炉床给出的最佳超参数值为0，精度为0.965。 但是，该值对于控制数据集显然不是最佳的，因为在训练集中以前没有找到大量单词。 对于参考数据集，此模型的精度为0.598。 但是，如果将<i>alpha</i>增加到0.1，则训练数据的准确性将下降到0.82，而在控制数据上的准确性将增加到0.62。 在更大的数据集上，差异很有可能会更大。 <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3> 结论 </h3><br> 假定该模型应用于预测其单词未用于形成词汇表的评论。 因此，可以通过模型在数据控制部分的准确性（即0.62）来评估模型的质量。 这几乎比仅仅猜测要好两倍，但是准确性仍然很低。 <br><br> 根据分类报告，很明显，该模型在具有中性色的评论中表现最差（准确度为0.47，正面为0.68，负面为0.76）。 确实，中立的评论包含正面和负面评论都具有的特征。 可能通过增加数据集的体积来提高模型的准确性，因为第3,000个数据集相对较小。 同样，可以将问题简化为评论的肯定和否定的二进制分类，这也将提高准确性。 <br><br> 感谢您的阅读。 <br><br>  PS：如果您想练习，可以在链接下方下载我的数据集。 <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">链接到数据集</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN467081/">https://habr.com/ru/post/zh-CN467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN467061/index.html">巴比伦行：建筑业中的5个安全问题</a></li>
<li><a href="../zh-CN467063/index.html">数据中心柴油发电机的燃料监控-如何做到这一点，为什么如此重要？</a></li>
<li><a href="../zh-CN467065/index.html">面向学生的物理奥林匹克竞赛问题档案</a></li>
<li><a href="../zh-CN467073/index.html">“在西方，没有40岁以下的艺术指导。 对于我们来说，最多可以达到30个。” 成为IT设计师感觉如何</a></li>
<li><a href="../zh-CN467079/index.html">CSS和Javascript蚂蚁轮播</a></li>
<li><a href="../zh-CN467083/index.html">现代处理器如何使用奇怪的popcount指令</a></li>
<li><a href="../zh-CN467085/index.html">C，C ++和DotNet反编译是反向的基础。 用r0ot-mi解决换向问题。 第一部分</a></li>
<li><a href="../zh-CN467087/index.html">我如何准备并通过Oracle数据库SQL认证（1Z0-071）</a></li>
<li><a href="../zh-CN467089/index.html">已修补的Exim-再次修补。 在一个请求中，Exim 4.92中的全新远程命令执行</a></li>
<li><a href="../zh-CN467091/index.html">从Angular开发人员的角度快速介绍Svelte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>