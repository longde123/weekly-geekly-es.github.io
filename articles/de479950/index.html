<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ûüèº üà∂ üà∏ Die Geschichte der Videoprozessoren, Teil 4: Das Aufkommen der Allzweck-GPU ‚õé üå∑ üíÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 1: 1976-1995 

 Teil 2: 3Dfx Voodoo 

 Teil 3: Marktkonsolidierung, Beginn der √Ñra des Wettbewerbs zwischen Nvidia und ATI 

 Vor der Einf√ºhrung ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Geschichte der Videoprozessoren, Teil 4: Das Aufkommen der Allzweck-GPU</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479950/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/645/a4d/a1e/645a4da1e71a179c58695af33e0e101b.jpg" alt="Bild"></div><br>  <a href="https://habr.com/ru/post/473130/">Teil 1: 1976-1995</a> <br><br>  <a href="https://habr.com/ru/post/477778/">Teil 2: 3Dfx Voodoo</a> <br><br>  <a href="https://habr.com/ru/post/478904/">Teil 3: Marktkonsolidierung, Beginn der √Ñra des Wettbewerbs zwischen Nvidia und ATI</a> <br><br>  Vor der Einf√ºhrung von DirectX 10 machte es keinen Sinn, den Chips eine optionale Komplexit√§t zu verleihen, indem die Chipfl√§che vergr√∂√üert wurde, was die Funktionalit√§t von Vertex-Shadern sowie die Genauigkeit der Verarbeitung von Gleitkommazahlen f√ºr Pixel-Shader von 24 auf 32 Bit erh√∂hte, um die Anforderungen von Vertex-Operationen zu erf√ºllen.  Nach dem Erscheinen von DX10 behielten Vertex- und Pixel-Shader ein hohes Ma√ü an Gesamtfunktionalit√§t bei, sodass der √úbergang zu generalisierten Shadern eine Menge unn√∂tiger Duplikationen von Verarbeitungseinheiten ersparte.  Die erste GPU, die diese Architektur verwendete, war die legend√§re G80 von Nvidia. <br><br>  Dank vierj√§hriger Entwicklungszeit und 475 Millionen Dollar wurde ein Monster mit 681 Millionen Transistoren und einer Fl√§che von 484 mm¬≤ hergestellt, das am 8. November 2006 erstmals auf dem Flaggschiff <a href="https://www.techspot.com/review/32-msi-geforce-8800gtx/" rel="nofollow">8800 GTX</a> und 8800 GTS 640MB ver√∂ffentlicht wurde.  Die √ºbertaktete GTX mit dem Namen 8800 Ultra war der H√∂hepunkt der Leistung des G80.  Es wurde zwischen zwei weniger wichtigen Produkten ver√∂ffentlicht: dem 320MB GTS im Februar und dem GTS 640MB / 112 mit einer limitierten Auflage am 19. November 2007. <br><br>  GTX, ausgestattet mit dem neuen Coverage Sample Anti-Aliasing (CSAA) -Algorithmus, besiegte dank seiner unvergleichlichen Leistung alle Konkurrenten sowohl mit einem als auch mit zwei Chips.  Trotz dieses Erfolgs verlor das Unternehmen im vierten Quartal drei Prozent des diskreten Grafikmarktes, was auf die St√§rke der OEM-Vertr√§ge zur√ºckzuf√ºhren ist, die an AMD weitergegeben wurden. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eda/0a4/fa0/eda0a4fa0986ba0ec34eefb62f5e503c.jpg"></div><br>  <i>GeForce 8800 GTX bei MSI</i> <br><br>  Die verbleibenden Komponenten der G80-Gesch√§ftsstrategie von Nvidia wurden im Februar und Juni 2007 umgesetzt.  Die CUDA-Plattform, die auf dem C-SDK (Software Development Kit) basiert, wurde als Beta-Version ver√∂ffentlicht, sodass das √ñkosystem den stark parallelisierten Charakter der GPU nutzen kann.  Die CUDA-Plattform verwendet die Nvidia PhysX-Physik-Engine sowie verteilte Computerprojekte, professionelle Visualisierung und OptiX, die Nvidia Ray Tracing-Engine. <br><br>  Sowohl Nvidia als auch ATI (jetzt AMD) integrieren st√§ndig erweiterte Computerfunktionen in die Grafik-Pipeline.  ATI / AMD entschied sich, sich bei der Auswahl des OpenCL-Pfades auf Entwickler und Komitees zu verlassen, w√§hrend Nvidia engere Pl√§ne hatte, CUDA und Hochleistungs-Computing zu verwenden. <br><br>  Zu diesem Zweck hat Nvidia im Juni eine Reihe mathematischer Tesla-Coprozessoren herausgebracht, die urspr√ºnglich auf demselben G80-Kern basieren, der auch in GeForce und Quadro FX 4600/5600 verwendet wurde.  Nach einer langen Entwicklungsphase, in der mindestens zwei (und m√∂glicherweise drei) ernsthafte Debugging-Phasen durchgef√ºhrt wurden, ver√∂ffentlichte AMD im Mai den R600. <br><br>  Der Medienrummel veranlasste AMD, eine 8800 GTX-Antwort zu erwarten, aber der ver√∂ffentlichte <a href="https://www.techspot.com/review/52-asus-radeon-hd-2900xt/" rel="nofollow">HD 2900 XT</a> war eine Entt√§uschung.  Dies war eine Karte des oberen Teils des mittleren Preissegments mit einem Energieverbrauch auf der Ebene eines professionellen Produkts: Es verbrauchte mehr Strom als alle modernen Systeme. <br><br>  Das Ausma√ü des R600-Fehlers hatte gro√üe Auswirkungen auf ATI und zwang das Unternehmen, seine Strategie zu √§ndern, um die Fristen strenger einzuhalten und die Chancen bei der Ver√∂ffentlichung neuer Produkte zu maximieren.  Die Leistung verbesserte sich beim RV770 (Evergreen) sowie bei den Serien Northern und Southern Islands. <br><br>  Neben der Tatsache, dass der R600 zu dieser Zeit die gr√∂√üte ATI / AMD-GPU war (420 mm¬≤), stellte er weitere Rekorde unter den GPUs auf.  Es war der erste AMD-Chip mit DirectX 10-Unterst√ºtzung, die erste und einzige GPU mit einem 512-Bit-Speicherbus, der erste Desktop-Chip mit einem Tessellation-Block (der aufgrund der Gleichg√ºltigkeit der Spieleentwickler und der fehlenden DirectX-Unterst√ºtzung fast nie verwendet wurde), die erste GPU mit integriertem Speicher Unterst√ºtzung f√ºr Audio √ºber HDMI sowie das erste Produkt, das die VLIW-Architektur verwendet.  Zum ersten Mal seit der Ver√∂ffentlichung der Radeon 7500 ist ATI / AMD nicht auf den Markt gekommen, um die leistungsst√§rksten Karten zu erhalten, die in Preis und Leistung mit denen der Wettbewerber vergleichbar sind. <br><br>  AMD verbesserte den R600 auf RV670, reduzierte die 80-nm-Prozesstechnologie des TSMC auf 55 nm und ersetzte den bidirektionalen 512-Bit-Ringspeicherbus durch einen Standard-256-Bit-Bus.  Dies halbierte die Fl√§che des R600-Kristalls, der zur gleichen Zeit ungef√§hr die gleiche Anzahl von Transistoren enthielt (666 Millionen im Vergleich zu 700 Millionen R600).  AMD hat au√üerdem die GPU auf DX10.1 aktualisiert und Unterst√ºtzung f√ºr PCI Express 2.0 hinzugef√ºgt.  All dies reichte aus, um die HD 2000-Serie zu vervollst√§ndigen und sich mit der Mainstream-GeForce 8800 GT sowie mit weniger leistungsstarken Karten zu messen. <br><br>  In Ermangelung einer High-End-GPU hat AMD im Januar 2008 zwei Dual-GPU-Karten sowie Budget-Karten auf Basis des RV620 / 635 herausgebracht.  Der HD 3850 X2 wurde im April in den Handel gebracht und die neueste All-In-Wonder-Karte, der HD 3650, im Juni.  Dual-GPU-Karten, die mit hochwertigen Treibern geb√ºndelt sind, beeindruckten Kritiker und Kunden sofort.  <a href="https://www.techspot.com/review/86-ati-radeon-hd-3870-x2/" rel="nofollow">Die HD 3870 X2</a> war die schnellste Einzelkarte, und die HD 3850 X2 war nicht viel langsamer als sie.  Im Gegensatz zu Nvidias SLI-System hat AMD Crossfiring-Unterst√ºtzung mit Standard-ASICs implementiert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ed6/fb9/775/ed6fb9775efcc56a75b21c94851798be.jpg"></div><br>  <i>Radeon HD 3870 X2 mit zwei GPUs auf einer Karte</i> <br><br>  Aufbauend auf dem Erfolg der G80 brachte Nvidia am 29. Oktober die G92 f√ºr die 8800 GT auf den Markt, die vor allem aufgrund sehr wettbewerbsf√§higer Preise von vielen technischen Standorten gut angenommen wurde.  Diese 512-Megabyte-Karte mit einem Preisbereich von 199 bis 249 US-Dollar lieferte eine Leistung, die den Kauf des 8800 GTS auf der Basis des G80 √ºberfl√ºssig machte.  Grunds√§tzlich √ºberholte es den HD 2900 XT und den HD 3870, der drei Monate nach dem GT auf den Markt kam und rund 80% der GTX-Geschwindigkeit erreichte.  Es ist nicht verwunderlich, dass der Markt nach einigen Wochen einen Mangel an 8800 GT zu verzeichnen begann.  Die starke Nachfrage nach einem neuen Produkt von Nvidia und seinen ‚ÄûBr√ºdern‚Äú 8600 GS / GT erm√∂glichte es dem Unternehmen, bis zum Jahresende 71% des diskreten Kartenmarktes zu erobern. <br><br>  Unmittelbar nach dem GT ver√∂ffentlichte Nvidia am 11. Dezember den <a href="https://www.techspot.com/review/79-geforce-8800-gts-512/" rel="nofollow">8800 GTS 512MB, der auf dem G92 basiert</a> .  Obwohl das Preis-Leistungs-Verh√§ltnis im Vergleich zum GT insgesamt schlechter war, wurde das GTS durch den Einsatz leistungsst√§rkerer GPUs eingespart, was den √ºbertakteten GTX im Wesentlichen mit dem teuren 8800 Ultra gleichsetzte. <br><br>  Die Geschichte der GeForce 8-Serie w√§re unvollst√§ndig ohne ein unangenehmes Nachskript, n√§mlich die Verwendung von L√∂tzinn mit hohem Bleigehalt im BGA einiger G86-, G84-, G84-, G73-, G72- / 72M-Grafikprozessoren sowie der C51- und MCP67-Grafikchips√§tze.  Dies f√ºhrte zusammen mit dem Bef√ºllen bei niedrigen Temperaturen, unzureichender K√ºhlung und intensivem Heizen und K√ºhlen zu einer ungew√∂hnlich hohen Anzahl von Kartenfehlern. <br><br>  Mitte 2008 stellte Nvidia auf das von AMD verwendete eutektische L√∂tmittel mit hohem Zinngehalt von Hitachi um und √ºberarbeitete den 8800 GT, f√ºgte weitere Blades hinzu und verbesserte das Geh√§use, um den Luftstrom zu verbessern.  Der G92 wurde ebenfalls verd√§chtigt, von dem Problem der √úberflutung betroffen zu sein, obwohl Dual-Ger√§te auf der Basis des 8800 GTS 512M und Karten mit Nicht-Referenz-K√ºhlern dies nicht wirklich betrafen. <br><br>  Aufgrund dieses Problems verlor das Unternehmen <a href="https://www.techspot.com/news/39962-nvidia-posts-141-million-loss-blames-weak-market.html" rel="nofollow">insgesamt 475,9 Millionen Euro. Dies</a> f√ºhrte zu einer starken negativen Reaktion der Verbraucher auf die OEM-Partner von Nvidia, die Laptops herstellten. Sie wussten um das Problem, lange bevor es an die √ñffentlichkeit ging.  Nvidias Platz in der Branche wird f√ºr immer mit diesem schlimmsten Moment in seiner Geschichte verbunden sein. <br><br>  Wenn die Serie 8 f√ºr Nvidia ein technologischer Triumph war, l√§utete die Serie 9 eine Periode der Stagnation ein.  Ein Lichtblick in der Aufstellung war das erste Modell, das im Februar 2008 auf den Markt kam.  Der 9600 GT basierte auf dem ‚Äûneuen‚Äú G94, dem letztj√§hrigen G92, der auf der gleichen TSMC 65-nm-Prozesstechnologie basiert. <br><br>  Aggressive AMD-Preissenkungen bei HD 3870 und HD 3850 sowie der sinkende Wert von 8800 GS und GT Nvidia selbst zwangen dazu, den Rest der Serie 9 umzubenennen. <br><br>  Die ersten 9800 GTs wurden mit dem 8800 GT modifiziert und aus dem 8800 GTS (G92) wurde der 9800 GTX.  Die Umstellung auf die 55-Nanometer-TSMC-Prozesstechnologie reduzierte die Kristallfl√§che um 20% und erm√∂glichte eine leichte Erh√∂hung der Taktfrequenz f√ºr den 9800 GTX +, der mit dem OEM GTS 150 identisch ist, sowie f√ºr den GTS 250, der f√ºnfzehn Monate nach der ersten Karte der Serie 8 in den Einzelhandel kam. <br><br>  Aufgrund des sp√§ten Auftretens des Flaggschiffs GT200 und der Tatsache, dass der AMD HD 3870 X2 jetzt f√ºhrend im Ein-Karten-Rennen war, kehrte Nvidia zu seiner langen Tradition zur√ºck und verdoppelte die Anzahl der GPUs mit zwei 9800 GTs, wodurch der 9800 GX2 entstand.  Obwohl das Produkt die Konkurrenz bei den Benchmarks gewann, stellten die meisten Beobachter schnell fest, dass der Verkauf eines dualen 9800 GT zum Preis von drei separaten 9800 GT f√ºr den K√§ufer nicht sehr attraktiv war. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/69f/c8a/b55/69fc8ab55f11f104edfb2dbd7b7f0e1a.jpg"></div><br>  <i>Nvidia G200 GPU auf GTX 260</i> <br><br>  Bis Juni ver√∂ffentlichte Nvidia die GTX 260 und GTX 280 mit der GT200-GPU (576 mm¬≤) - dem damals gr√∂√üten GPU-Chip (Intel Larrabee hatte eine Fl√§che von ca. 600-700 mm¬≤) und dem gr√∂√üten von TSMC produzierten Produktionschip. <br><br>  Der GT200 war ein weiterer Versuch von Nvidia, die Aufmerksamkeit auf die GPGPU zu lenken: Er implementierte spezielle Ger√§te f√ºr Double Precision (FP64) und Computing.  Die architektonischen √Ñnderungen, die auf Spiele abzielten, waren bescheidener, aber das hinderte Nvidia nicht daran, den Preis f√ºr 280 USD auf 649 USD festzusetzen und 3D Vision-Treiber (f√ºr 3D-Spiele und -Videos) zusammen mit einer 3D-Brille und einem IR-Emitter zu ver√∂ffentlichen - ein sehr teures Kit. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/tDmRsvzUufI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Technologie-Demo f√ºr die Nvidia GTX 200-Serie</i> <br><br>  Die Preise gingen nach der Ver√∂ffentlichung von HD 4870 und 4850 deutlich zur√ºck - die GTX 280 fiel um 38% und kostete 400 USD, w√§hrend die GTX 260 um 25% (299 USD) fiel. <br><br>  AMD reagierte auf die GT200 und G92 mit der Ver√∂ffentlichung des RV770.  Die erste Karte (f√ºr das untere Mainstream-Segment von HD 4730) wurde am 8. Juni herausgebracht, und am 25. Juni folgten die HD 4850 und 4870, die f√ºr den Mainstream- und High-Level-Markt entwickelt wurden.  Die Ausgabe von Karten sorgte nicht f√ºr so viel Aufsehen, da die Spezifikationen fr√ºher "durchgesickert" waren und die Gesch√§fte eine Woche vor Ablauf der NDA mit dem Verkauf von HD 4850 begannen - dies geschieht heute h√§ufig, jedoch nicht im Jahr 2008. <br><br>  Die 4870 und 4850 waren die ersten GDDR5-Consumer-Grafikkarten, die Nvidia achtzehn Monate sp√§ter auf der GT 240 basierend auf der GT215 implementierte. <br><br>  Die HD 4870 und 4850 haben sehr positive Bewertungen verdient. Der Grund daf√ºr war eine gro√üe Auswahl an Funktionen - 7.1 LPCM-Audio √ºber HDMI, allgemeine Leistung und Skalierung mit mehreren GPUs sowie nat√ºrlich der Preis.  Der einzige Nachteil der Karte war die Tendenz, im Bereich der Spannungseinstellkomponenten auf den Referenzplatinen hohe lokale Temperaturen zu erzeugen, die zu unverh√§ltnism√§√üig hohen Ausf√§llen und Einfrierungen f√ºhrten, insbesondere bei Anwendungen mit hoher Belastung wie Furmark. <br><br>  AMD behielt die Tradition fr√ºherer Generationen bei und hatte das Bed√ºrfnis, die zweimonatige F√ºhrung der GTX 280 zu beenden. Im August brachte AMD die HD 4870 X2 auf den Markt.  Die Karte landete schnell in den Top-Benchmarks f√ºr verschiedene Kategorien, einschlie√ülich der Leistung.  Gleichzeitig ist es aufgrund der Konstruktion des Gebl√§ses leider f√ºhrend in der Kategorie Ger√§uschentwicklung und W√§rmeableitung. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/645/a4d/a1e/645a4da1e71a179c58695af33e0e101b.jpg"></div><br>  <i>Radeon HD 4870 X2 (oben) und Radeon HD 4870</i> <br><br>  Im Januar 2009 verzeichnete das Nvidia-Sortiment dank der Umstellung des GT 200 auf die 55-Nanometer-TSMC-Prozesstechnologie ein leichtes Wachstum.  55 Nanometer wurden in den Chips der B3-Version verwendet, die erstmals im September des Vorjahres als Version der Core 216 GTX 260-Karten auf den Markt kamen. Das Unternehmen brachte die GTX 295 heraus, bei der zwei GT200-B3-St√ºmpfe zum Einsatz kamen. <br><br>  Eine Variante einer Single-GPU-Karte wurde im April unter dem Namen GTX 275 herausgebracht. Die Antwort von AMD lautete: Die aufger√ºstete HD 4890 auf der Basis des RV790XT und des HD 4770 (RV740), die auch die erste 40-nm-Karte von AMD war. <br><br>  Obwohl der HD 4770 f√ºr sich genommen kein besonders wichtiges Produkt war, konnte AMD wertvolle Erfahrungen mit der problematischen 40-Nanometer-TSMC-Prozesstechnologie sammeln, die eine gro√üe Variabilit√§t des Leckstroms sowie ein hohes Ma√ü an Zur√ºckweisung aufgrund unvollst√§ndiger Verbindungen zwischen Metallschichten im GPU-Kristall verursachte.  Dank dieser Erfahrung konnte AMD den Herstellungsprozess verbessern und die Probleme beseitigen, auf die Nvidia bei der Entwicklung der Fermi-Architektur stie√ü - Probleme, die bei den ersten 40-Nanometer-Nvidia-GPUs nicht auftraten. <br><br>  Nvidia brachte im Juli seine ersten 40-nm-Produkte auf den Markt.  Die Low-End-Modelle GT216 und GT218 wurden in den GeForce 205, 210 und GT 220 verwendet, bei denen es sich bis Oktober um OEM-Produkte handelte, als die letzten beiden Modelle in den Einzelhandel gingen.  Sie sind nur deshalb bemerkenswert, weil sie die ersten Nvidia-Karten mit Unterst√ºtzung f√ºr DX10.1 waren - AMD hat dieses Top in der HD 4870/4850 aufgenommen.  Dar√ºber hinaus verf√ºgten sie √ºber verbesserte Audiofunktionen mit 7.1-Sound, verlustfreiem LPCM, Dolby TrueHD / DTS-HD / HD-MA-Bitstream und Audio √ºber HDMI.  Die Serie war auf den Heimkino-Markt ausgerichtet und wurde im Februar 2010 in die 300er-Serie umbenannt. <br><br>  In den vier Monaten von September 2009 bis Februar 2010 ver√∂ffentlichte AMD die gesamte Reihe der vier GPUs (Cypress, Juniper, Redwood und Cedar), aus denen die Evergreen-Familie bestand.  Die Linie begann mit dem High-End-Segment HD 5870, wonach eine Woche sp√§ter die HD 5850 f√ºr die Spitze des Durchschnittspreisniveaus erschien. <br><br>  Die problematische 40-Nanometer-TSMC-Prozesstechnologie verhinderte, dass AMD die Abwesenheit von Nvidia Fermi ausnutzte, da die gro√üe Nachfrage das Angebot √ºberstieg.  Dies war haupts√§chlich AMDs F√§higkeit zu verdanken, die Ver√∂ffentlichung von Evergreen mit dem Aufkommen von Windows 7 und der Popularisierung von DirectX 11 zeitlich festzulegen. <br><br>  Obwohl der DX11 einige Zeit brauchte, um bei Evergreen signifikante Leistungssteigerungen zu erzielen, wirkte sich eine weitere in der HD 5000 eingef√ºhrte Funktion sofort in Form von Eyefinity aus, die sich ausschlie√ülich auf die Flexibilit√§t von DisplayPort st√ºtzt, bis zu sechs Display-Pipelines pro Board bereitzustellen.  Sie wurden an einen Standard-DAC oder an eine Kombination aus internen TMDS- und DisplayPort-Sendern weitergeleitet. <br><br>  Bisherige Grafikkarten verwendeten in der Regel eine Kombination aus VGA, DVI und manchmal HDMI. F√ºr jeden Ausgang war eine separate Taktquelle erforderlich.  Dies erh√∂hte die Komplexit√§t, Gr√∂√üe und Anzahl der GPU-Pins.  DisplayPort macht unabh√§ngige Taktraten √ºberfl√ºssig und er√∂ffnet AMD die M√∂glichkeit, bis zu sechs Display-Pipelines in Ger√§te zu integrieren, ohne die Leistung der Anwendersoftware zu beeintr√§chtigen.  Gleichzeitig wurden die Rahmen an den R√§ndern ausgeglichen und das Display mit optimaler Aufl√∂sung entlang der Displays gespannt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/4ANlGVrfPSo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Eyefinity: ATI Scalable Multi-Display-Technologie</i> <br><br>  Die Evergreen-Serie ist in ihrer Klasse unter allen Motherboards f√ºhrend (wenn Sie sich nicht an die Probleme mit der Texturfilterung erinnern): Die HD 5850 und HD 5770 zogen einen gro√üen Prozentsatz der Budget-Gamer an, und die HD 5870 und HD 5970 mit zwei GPUs lieferten ein unvergleichliches Leistungsniveau. <br><br>  Ein halbes Jahr sp√§ter, am 12. April, ver√∂ffentlichte Nvidia schlie√ülich die ersten Fermi-Boards mit den Namen GTX 470 und 480. Keines der Kristalle des Unternehmens war voll funktionsf√§hig (dasselbe geschah mit dem nachfolgenden GF104) konservativ genug gemacht, und die Speicherbandbreite war aufgrund der mangelnden Erfahrung von Nvidia mit GDDR5-E / A geringer. <br><br>  Die bei weitem nicht optimalen Ergebnisse der TSMC-40-Nanometer-Prozesstechnologie, die bereits zu Versorgungsproblemen bei AMD f√ºhrten, nahmen aufgrund der Gr√∂√üe des GF100-Fermi-Kristalls (529 mm¬≤) erheblich zu.  Die Gr√∂√üe des Kristalls ist mit der Menge an Ausschuss, dem Strombedarf und der W√§rmeabgabe verbunden, sodass die Nvidia 400-Serie im Vergleich zur AMD-Linie f√ºr die Leistung in Spielen bezahlt wird. <br><br>  Der GF100 in den Varianten Quadro und Tesla litt aufgrund des auf professionellen M√§rkten bereits etablierten √ñkosystems deutlich weniger.  Einer der guten Aspekte der ver√∂ffentlichten Karten war das Aufkommen der Transparenz-Supersampling-Antialiasing-Technologie (TrSSAA), die zusammen mit der bereits vorhandenen Abdeckung mit der Stichprobe AA (CSAA) verwendet werden sollte. <br><br>  Obwohl die GTX 480 eher kalt getroffen wurde, war der zweite Nvidia Fermi-Chip, der Mainstream GF104 auf der <a href="https://www.techspot.com/review/299-palit-inno3d-geforce-gtx-460/" rel="nofollow">GTX 460</a> , ein gro√üer Erfolg.  Es lieferte eine gute Leistung zu einem g√ºnstigen Preis, 192-Bit / 768-MB f√ºr 199 US-Dollar und 256-Bit / 1-GB f√ºr 229 US-Dollar.  Dank der konservativen Frequenzen, mit denen Nvidia den Stromverbrauch senkt, hat das Unternehmen viele nicht referenzierte und √ºbertaktete Karten mit erheblichen √úbertaktungsm√∂glichkeiten herausgebracht. <br><br>  Teilweise warmer Empfang 460 wurde durch niedrige Erwartungen nach der Freigabe des GF100 verursacht.  Sie sagten, dass das GF104 nicht mehr als die H√§lfte des GF100 ausmacht und im Vergleich zur Cypress-GPU von AMD blass ist.  Dem war aber nicht so.  Die "Experten" -Blogger und AMD erwarteten eine zweite √úberraschung: Im November ver√∂ffentlichte Nvidia eine aktualisierte Version des GF100 - GF110. <br><br>  Das aufger√ºstete Produkt schaffte es, das zu erreichen, was sein Vorg√§nger nicht bew√§ltigen konnte - den gesamten Bereich des Chips zu nutzen.  Die resultierenden GTX 570 und 580 waren, was die urspr√ºngliche 400er-Serie h√§tte sein sollen. <br><br>  Im Oktober erschien Barts, die erste GPU der AMD Northern Islands-Serie.  Es war n√§her an der evolution√§ren Entwicklung von Evergreen und wurde entwickelt, um die Herstellungskosten f√ºr Cypress-Chips zu senken.  Die Leistung konnte nicht wesentlich gesteigert werden: Die GPU entsprach in etwa den bisherigen HD 5830 und HD 5850, reduzierte jedoch die Gr√∂√üe erheblich.  AMD reduzierte die Anzahl der Stream-Prozessoren (Shader), verbesserte den Speichercontroller und √§nderte seine physische Gr√∂√üe (entsprechend, wodurch die Speichergeschwindigkeit verringert wurde). Au√üerdem lehnte AMD die Durchf√ºhrung von Berechnungen mit doppelter Genauigkeit ab.  Barts hatte jedoch die Tessellation im Vergleich zu Evergreen verbessert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/833/4a8/ff2/8334a8ff212c624689b6c4af57e7bb05.jpg"></div><br>  Obwohl die Leistungssteigerung nicht so sp√ºrbar war, hat AMD die Display-Technologie verbessert.  DisplayPort wurde auf Version 1.2 aktualisiert (die M√∂glichkeit, mehrere Monitore von einem Port aus zu steuern, ein Update mit einer Frequenz von 120 Hz f√ºr hochaufl√∂sende Displays und Bitstream-Audio), HDMI - auf Version 1.4a (3D-Videowiedergabe in 1080p, 4K-Bildschirmaufl√∂sung). .  Das Unternehmen hat au√üerdem einen aktualisierten Videodecoder mit DivX-Unterst√ºtzung hinzugef√ºgt. <br><br>  Dar√ºber hinaus verbesserte AMD die F√§higkeiten der Treiber durch das Hinzuf√ºgen von morphologischem Anti-Aliasing (MLAA) - einem Unsch√§rfefilter f√ºr die Nachbearbeitung, dessen Funktionalit√§t (insbesondere zum Zeitpunkt der Ver√∂ffentlichung) alles andere als ideal war. <br><br>  HD 6970 und HD 6950 haben dem Catalyst-Treiber einen Gl√§ttungsmodus mit der Bezeichnung EQAA (Enhanced Quality AA) hinzugef√ºgt.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dar√ºber hinaus implementierte AMD die rudiment√§re Unterst√ºtzung f√ºr HD3D, die bestenfalls skurril war, und den dynamischen Stromverbrauch, diesmal mit PowerTune. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Insgesamt waren Cayman-Produkte besser als die Fermi-Chips der ersten Generation. Sie sollten sie schlagen, aber sie lagen einige Prozent hinter der zweiten Generation (GTX 500-Serie), und die darauffolgende Ver√∂ffentlichung von Treibern beider Unternehmen verst√§rkte die Schwankungen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Ver√∂ffentlichung von Cayman im November wurde um einen Monat verschoben, und am 15. Dezember erschienen HD 6970 und 6950, die (vor√ºbergehend) von der VLIW5-Architektur abweichen, die ATI / AMD seit der R300-Serie verwendete. Stattdessen verwendete das Unternehmen VLIW4, bei dem in keinem Thread-Verarbeitungsblock ein f√ºnfter Sonderfunktions- (oder Transendental-) Block vorhanden war.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies war erforderlich, um den √úberfluss an Ressourcen f√ºr Spiele unter DX9 (und √§lter) zu beseitigen und gleichzeitig die Grafik-Pipeline neu zu organisieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die einzigen anderen Produkte, die auf VLIW4 basieren, sind die integrierten Grafikchips der Serien APU Trinity und Richland. Die neueste Grafikarchitektur von AMD basierte auf GCN (Graphics Core Next), und VLIW5 blieb in der HD 8000-Serie, die als Evergreen-GPU der niedrigsten Stufe umbenannt wurde. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Januar 2011 erschien der Nachkomme der GTX 460 - GTX 560 Ti - und wiederholte die Geschichte der GF100 / GF110. Basierend auf dem GF114 enthielt die Karte ein voll funktionsf√§higes Upgrade des GF104 und erwies sich als ebenso zuverl√§ssig und vielseitig wie sein Vorg√§nger. Auf dieser Basis wurden viele Nichtreferenzversionen mit und ohne Werks√ºbertaktung ver√∂ffentlicht.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AMD reagierte sofort darauf, indem es die Kosten seiner HD 6950 und 6870 senkte, wodurch der Preis-Leistungs-Vorteil der GTX 560 Ti verschwand. Dank der von den meisten Motherboard-Partnern angebotenen Rabatte ist der HD 6950 und insbesondere seine Version mit 1 GB Speicher attraktiver geworden.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b7/dcc/fe6/2b7dccfe6969ac3d7df41907f54aaf0f.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Referenzboard Nvidia GeForce GTX 590 Die</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zweite gro√ü angelegte Ver√∂ffentlichung von Nvidia-Produkten im Jahr 2011, n√§mlich der 26. M√§rz, begann mit einer Explosion. In der GTX 590 wurden zwei voll ausgestattete GF110 auf einem Board kombiniert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Karten wurden von einem Treiber gesteuert, der die Leistungsbegrenzung nicht ordnungsgem√§√ü implementierte, und verf√ºgten √ºber ein BIOS, das das Anlegen von Hochspannung erm√∂glichte. Dieser Fehler f√ºhrte zu einer aggressiven √úberspannung, die zu Fehlfunktionen des MOSFET f√ºhrte. Nvidia korrigierte die Situation, indem es ein strengeres BIOS und einen strengeren Treiber erstellte. Die Ver√∂ffentlichung wurde jedoch von abf√§lligen √úberpr√ºfungen begleitet. In puncto Leistung hat die GTX 590 nur die Parit√§t mit der zwei Wochen zuvor erschienenen AMD HD 6990 erreicht.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Am 9. Januar begann die geplante Ver√∂ffentlichung der Nachfahren der Prozessoren von AMD Northern Islands - Southern Islands, von denen die erste das Flaggschiff HD 7970 war. Dies war die erste Karte f√ºr PCI-E 3.0, die zum ersten Mal die GCN-Architektur von AMD verwendete, die auf der 28-Nanometer-TSMC-Prozesstechnologie aufbaute. Nur drei Wochen sp√§ter schloss sich die zweite Karte auf Tahiti, die HD 7950, der 7970 an, gefolgt von den Mainstream-Karten auf Kap Verde am 15. Februar. Pitcairn-GPU-basierte Leistungskarten kamen im M√§rz in den Handel.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d9/8d2/3f5/6d98d23f50b308a0e55114ef363c9479.jpg"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Karten erwiesen sich als gut, zeigten jedoch keine signifikanten Verbesserungen im Vergleich zu den vorherigen 40-Nanometer-Boards. Dies sowie weniger konkurrenzf√§hige Preise, die f√ºr AMD mit der HD 2000-Serie zum Standard geworden sind, das Fehlen von WHQL-Treibern f√ºr zwei Monate und die funktionsunf√§hige Video Codec Engine (VCE) haben die Begeisterung vieler potenzieller Benutzer und Rezensenten gemildert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Bonus der Tahiti-Produkte war die Best√§tigung, dass AMD eine gro√üe Leistungsspanne hinterlassen hat, die aufgrund von √úbertaktung erzielt werden konnte. Es war ein Kompromiss zwischen Stromverbrauch, K√ºhlk√∂rper und Taktrate, aber es f√ºhrte zu konservativen Kern- und Speicherfrequenzen. M√∂glicherweise wurde dies auch durch die Notwendigkeit, die Ehe zu reduzieren, und die Untersch√§tzung der auf Kepler basierenden Nvidia GTX 680/670 beeinflusst.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nvidia erweiterte seine GPU-Funktionen mit der Ver√∂ffentlichung der Kepler-Architektur weiter. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fr√ºheren Generationen hat Nvidia mit dem anspruchsvollsten Chip begonnen, um Kunden in der High-End-Nische zufrieden zu stellen, und anschlie√üend den langwierigen Testprozess f√ºr professionelle Modelle (Tesla / Quadro) fortgesetzt. In den letzten Generationen war dieser Ansatz nicht sehr praktisch, so dass der bescheidenere GK107 und der leistungsorientierte GK104 Vorrang vor dem leistungsstarken GK110 hatten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vermutlich wurde das GK107 von Nvidia ben√∂tigt, da das Unternehmen umfangreiche Vertr√§ge mit mobilen OEMs hatte und das GK104 f√ºr das obere Desktopsegment ben√∂tigte. Beide GPUs wurden als Version A2-Chips ausgeliefert. Das mobile GK107 (GT 640M / 650M, GTX 660M) wurde im Februar an OEM-Partner ausgeliefert und am 22. M√§rz offiziell angek√ºndigt, an demselben Tag, an dem Nvidia seine GTX 680 auf Basis des GK104 auf den Markt brachte. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein weiterer Unterschied zu den neuesten Nvidia-GPU-Architekturen bestand darin, dass die Shader-Einheiten mit derselben Frequenz wie der Kern ausgef√ºhrt wurden. Beginnend mit der GeForce 8-Serie arbeiteten Shader-Einheiten mit einer Frequenz, die mindestens doppelt so hoch war wie die Kernfrequenz - 2,67-mal h√∂her als die Kernfrequenz in Serie 9 und genau doppelt so hoch wie 400 und 500.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Bedeutung dieser √Ñnderung war, dass Nvidia seinen Fokus (auf den Desktop / Mobile-Markt) von der Rohleistung auf das Verh√§ltnis der Leistung pro Watt verlagerte. Mehr Kerne, die mit niedrigeren Geschwindigkeiten laufen, sind effizienter beim parallelen Rechnen als weniger Kerne mit doppelter Frequenz. In der Tat war dies eine Weiterentwicklung des Unterschieds zwischen dem GPU- und dem CPU-Paradigma (viele Kerne, niedrige Frequenzen, hohe Durchl√§ssigkeit und Verz√∂gerung gegen√ºber einer kleinen Anzahl von Kernen, hohe Frequenz, geringere Durchl√§ssigkeit und Verz√∂gerung).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dar√ºber hinaus hat eine Verringerung der H√§ufigkeit von Shadereinheiten zu einer Verringerung des Stromverbrauchs gef√ºhrt. Dar√ºber hinaus sparte Nvidia noch mehr beim Design, reduzierte die Anzahl der Einheiten mit doppelter Pr√§zision auf dem Chip erheblich und reduzierte die Breite des Busses auf mehr Mainstream-256-Bit. Diese √Ñnderungen wurden zusammen mit der relativ bescheidenen Kerngeschwindigkeit durch die dynamische Boost-Funktion (Overclocking on Demand) verst√§rkt, die zu einem ausgewogeneren Produkt f√ºhrte, wenn auch auf Kosten reduzierter Rechenkapazit√§ten. Wenn Nvidia jedoch die Computerfunktionalit√§t und das Fermi-Bandbreitendesign beibehalten w√ºrde, w√§re das Ergebnis ein gro√ües, hei√ües Design mit hohem Stromverbrauch. Die Gesetze der Physik haben das Chipdesign wieder zu einer Kunst des Kompromisses gemacht.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nvidia hat erneut ein Dual-GPU-Board entwickelt. Bei der GTX 690 handelte es sich im Wesentlichen um zwei per SLI verbundene GTX 680. Der einzige Unterschied besteht darin, dass die maximale Kernfrequenz von 690 (beim √úbertakten) um 52 MHz niedriger ist. Obwohl die Leistung immer noch von der Profilerstellung des SLI-Treibers abhing, war die Funktionalit√§t der Karte ausgezeichnet und ihr Erscheinungsbild war die von ihr getragene Limited Edition-Marke wert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der GK 110-Chip war eine Abkehr von Nvidias √ºblicher Praxis, die ersten GPUs der Serie unter dem Banner der GeForce herauszubringen. Die Tesla K20-Karte, auf der dieser Chip erstmals auftauchte, war in der Nische der Supercomputer sehr gefragt: F√ºr die Systeme ORNL Cray XK7 Titan, NCSA Blue Waters, Swiss CSCS Todi und Piz Daint wurden mehr als 22.000 Einheiten ben√∂tigt.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Normalverbraucher mussten weitere sechs Monate warten, bis das GK110 auf der GeForce-Karte erschien. Die Karte erhielt den Namen </font></font><a href="https://www.techspot.com/review/644-nvidia-geforce-titan/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GTX Titan</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - das Fehlen einer numerischen Modellnummer verst√§rkte Nvidias Wunsch, dieses Produkt als ein Modell zu sehen, das von der bestehenden (und zuk√ºnftigen) Kepler-Serie getrennt ist. Mit einem Preis von 999 US-Dollar war Titan auf den Markt f√ºr Grafikbegeisterte ausgerichtet. Nvidia steigerte auch die Attraktivit√§t seiner Produktlinie f√ºr Forscher und Haushaltsfachleute. Zum ersten Mal erm√∂glichte das Unternehmen der GeForce-Karte, die gleichen Computerfunktionen wie die professionellen Tesla- und Quadro-Modelle beizubehalten.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/874/df7/ae7/874df7ae720a68fdbd3c9b5956228a46.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nvidia GeForce GTX Titan Die</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Karte hat schnell den Status eines f√ºhrenden Gaming-Benchmarks erlangt, was sich insbesondere bei Aufl√∂sungen mit mehreren Monitoren und aktiviertem Antialiasing mit Super-Sampling bemerkbar macht. Die Gleichg√ºltigkeit von Nvidia, OpenCL-Treiber zu unterst√ºtzen, und die Entstehung neuer Spiele in Zusammenarbeit mit AMDs Gaming Evolved-Programm sowie der enorme Preis haben den Einfluss von Titan verringert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Juni bereitete AMD seine Antwort mit der Ver√∂ffentlichung der HD 7970 GHz Edition vor, die die Kernfrequenz um 75 MHz mit einer zus√§tzlichen √úbertaktungsf√§higkeit von weiteren 50 MHz erh√∂hte (im Gegensatz zu der von Nvidia vorgeschlagenen dynamischen Frequenz√§nderung). Die GHz-Edition hatte die Frequenz, mit der die Karte voraussichtlich im Januar erscheinen sollte.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ungl√ºcklicherweise f√ºr AMD hat die Zielgruppe dieses Produkts bereits festgestellt, dass das Standardmodell aufgrund von √úbertaktung in der Regel das gleiche (wenn nicht das beste) Leistungsniveau aufweist und gleichzeitig einen deutlich niedrigeren Preis und eine niedrigere Kernspannung aufweist. </font><font style="vertical-align: inherit;">AMD hat den HD 7950 Boost f√ºr die HD 7970 GHz Edition ver√∂ffentlicht.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de479950/">https://habr.com/ru/post/de479950/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de479932/index.html">Wie Sie Ihre Programmierkenntnisse verbessern k√∂nnen</a></li>
<li><a href="../de479938/index.html">Google Stadia - eine Cloud-basierte Gaming-Plattform mit einer seltsamen Monetarisierungsstrategie</a></li>
<li><a href="../de479940/index.html">Wie man von einem anderen Beruf zur Entwicklung √ºbergeht und schnell an einem neuen Ort w√§chst</a></li>
<li><a href="../de479942/index.html">[Aktualisiert um 10:52, 14.12.19] Im Nginx-B√ºro wurde eine Suche durchgef√ºhrt. Kopeiko: "Nginx wurde von Sysoev unabh√§ngig entwickelt"</a></li>
<li><a href="../de479948/index.html">Gestenverwaltung: Edge to Edge. Teil 1</a></li>
<li><a href="../de479952/index.html">Was steckt in shawarma: Microservices, verteilte Systeme und Kafka. Teilen von Materialien aus Backend United # 5</a></li>
<li><a href="../de479954/index.html">Spring Boot Admin Tutorial</a></li>
<li><a href="../de479958/index.html">Firmware- und Prozessor-Schwachstellen</a></li>
<li><a href="../de479960/index.html">Leistungsmodell der Medienwerbung f√ºr Online-Shops</a></li>
<li><a href="../de479966/index.html">Playrix CI / CD: Wie wir unsere Spiele bauen und testen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>