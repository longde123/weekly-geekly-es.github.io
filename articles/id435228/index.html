<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🤝‍👨🏼 🥛 ⬇️ Kubernetes cluster untuk $ 20 per bulan 👨🏽‍🔬 👩‍✈️ 👩🏻‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 Kami meningkatkan cluster untuk melayani aplikasi web tanpa kewarganegaraan dengan masuknya , letsencrypt , tanpa menggunakan alat otomatisa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes cluster untuk $ 20 per bulan</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  Kami meningkatkan cluster untuk melayani aplikasi web <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tanpa kewarganegaraan</a> dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">masuknya</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">letsencrypt</a> , tanpa menggunakan alat otomatisasi seperti kubespray, kubeadm, dan lainnya. <br>  Waktu baca: ~ 45-60 menit, untuk aksi pemutaran ulang: mulai 3 jam. </p><br><h1 id="preambula">  Pembukaan </h1><br><p>  Saya diminta untuk menulis artikel dengan kebutuhan untuk kubernetes saya sendiri untuk eksperimen.  Instalasi otomatis dan solusi konfigurasi open source tidak berfungsi dalam kasus saya, karena saya menggunakan distribusi Linux non-mainstream.  Pekerjaan intensif dengan kubernet di IPONWEB mendorong Anda untuk memiliki platform seperti itu, menyelesaikan tugas Anda dengan cara yang nyaman, termasuk untuk proyek-proyek rumah. </p><br><h1 id="komponenty">  Komponen </h1><br><p>  Komponen berikut akan muncul di artikel: </p><br><p>  - Linux <em>favorit Anda</em> - Saya menggunakan Gentoo (node-1: systemd / node-2: openrc), Ubuntu 18.04.1. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Server Kubernetes</a> - kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Containerd</a> + <a href="">CNI Plugins (0.7.4)</a> - untuk kontainerisasi, kami akan mengambil contenterd + CNI alih-alih buruh pelabuhan (meskipun pada awalnya seluruh konfigurasi diunggah ke buruh pelabuhan, jadi tidak ada yang akan mencegahnya digunakan jika diperlukan). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CoreDNS</a> - untuk mengatur penemuan layanan dari komponen yang bekerja di dalam kubernetes cluster.  Versi yang tidak lebih rendah dari 1.2.5 direkomendasikan, karena dengan versi ini ada dukungan waras untuk bekerja sebagai proses yang berjalan di luar cluster. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Flannel</a> - untuk mengatur tumpukan jaringan, mengkomunikasikan perapian dan wadah di antara mereka sendiri. <br>  - db <em>favorit Anda</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="Untuk semua"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Keterbatasan dan Asumsi </h1><br><ul><li>  Artikel ini tidak membahas biaya solusi vps / vds di pasar, serta kemungkinan penempatan mesin pada layanan ini.  Diasumsikan bahwa Anda telah memiliki sesuatu yang diperluas, atau Anda dapat melakukannya sendiri.  Juga, instalasi / konfigurasi database favorit Anda dan repositori buruh pelabuhan pribadi, jika Anda memerlukannya, tidak tercakup. </li><li>  Kita bisa menggunakan kedua plugin +erdi cni dan docker.  Artikel ini tidak mempertimbangkan menggunakan buruh pelabuhan sebagai alat kontainerisasi.  Jika Anda ingin menggunakan buruh pelabuhan, Anda sendiri akan dapat mengonfigurasi <a href="">flanel yang sesuai</a> , selain itu Anda harus mengonfigurasi kubelet, yaitu, menghapus semua opsi yang terkait dengan containerd.  Seperti yang diperlihatkan percobaan saya, buruh pelabuhan dan penampung di berbagai node sebagai wadah akan bekerja dengan benar. </li><li> Kami tidak dapat menggunakan backend <code>host-gw</code> untuk flanel, baca bagian <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Konfigurasi Flanel</a> untuk lebih jelasnya </li><li>  Kami tidak akan menggunakan apa pun untuk memantau, mencadangkan, menyimpan file pengguna (status), menyimpan file konfigurasi dan kode aplikasi (git / hg / svn / dll) </li></ul><br><h1 id="vvedenie">  Pendahuluan </h1><br><p>  Dalam perjalanan kerja, saya menggunakan sejumlah besar sumber, tetapi saya ingin menyebutkan secara terpisah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes</a> panduan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">jalan yang</a> agak rinci, yang mencakup sekitar 90% dari konfigurasi dasar cluster sendiri.  Jika Anda sudah membaca manual ini, Anda dapat dengan aman melanjutkan langsung ke bagian <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Konfigurasi Flannel</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Penunjukan</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Daftar Istilah / Glosarium </h2><br><ul><li>  api-server - mesin fisik atau virtual tempat kumpulan aplikasi untuk meluncurkan dan memfungsikan kubernetes kube-apiserver berada.  Untuk keperluan artikel ini, ini adalah etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - instalasi workstation atau VPS khusus, sinonim untuk api-server. </li><li>  simpul-X - stasiun kerja khusus atau instalasi VPS, <code>X</code> menunjukkan nomor seri stasiun.  Dalam artikel ini, semua angka unik dan merupakan kunci untuk memahami: <br><ul><li>  simpul-1 - mesin nomor 1 </li><li>  simpul-2 - nomor mesin 2 </li></ul></li><li>  vCPU - CPU virtual, inti prosesor.  Jumlahnya sesuai dengan jumlah inti: 1vCPU - satu inti, 2vCPU - dua, dan seterusnya. </li><li>  pengguna - pengguna atau ruang pengguna.  Saat menggunakan instruksi baris perintah <code>user$</code> in, istilah ini merujuk ke mesin klien mana pun. </li><li>  pekerja - simpul kerja di mana perhitungan langsung akan dilakukan, secara sinonim dengan <code>node-X</code> </li><li>  sumber daya adalah entitas yang dioperasikan oleh kluster Kubernetes.  Sumber daya Kubernetes mencakup sejumlah besar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">entitas terkait</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Solusi arsitektur jaringan </h1><br><p>  Dalam proses meningkatkan cluster, saya tidak menetapkan tugas mengoptimalkan sumber daya besi sedemikian rupa agar sesuai dengan anggaran $ 20 per bulan.  Itu hanya perlu untuk merakit sebuah cluster yang bekerja dengan setidaknya dua node kerja (node).  Oleh karena itu, pada awalnya cluster tampak seperti ini: </p><br><ul><li>  mesin dengan 2 vCPU / 4G RAM: api-server + node-1 [20 $] </li><li>  mesin dengan 2 vCPU / 4G RAM: node-2 [$ 20] </li></ul><br><p>  Setelah versi pertama dari cluster berfungsi, saya memutuskan untuk membangunnya kembali untuk membedakan antara node yang bertanggung jawab untuk menjalankan aplikasi di dalam cluster (work node, mereka juga pekerja) dan API dari server master. </p><br><p>  Akibatnya, saya mendapat jawaban untuk pertanyaan: "Bagaimana cara mendapatkan lebih banyak atau lebih murah, tetapi berfungsi cluster, jika saya ingin menempatkan bukan aplikasi yang paling tebal di sana." </p><br><div class="spoiler">  <b class="spoiler_title">Keputusan $ 20</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Desain"><br>  (Direncanakan seperti itu) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Arsitektur Umum Informasi Kubernet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Desain"><br>  (Dicuri dari Internet jika seseorang tiba-tiba masih tidak tahu atau belum melihat) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Komponen dan kinerjanya </h2><br><p>  Langkah pertama adalah memahami berapa banyak sumber daya yang saya butuhkan untuk menjalankan paket perangkat lunak yang terkait langsung dengan cluster.  Pencarian untuk "persyaratan perangkat keras" tidak memberikan hasil yang spesifik, jadi saya harus mendekati tugas dari sudut pandang praktis.  Sebagai pengukuran MEM dan CPU, saya mengambil statistik dari systemd - kita dapat mengasumsikan bahwa pengukuran dilakukan dengan cara yang sangat amatir, tetapi saya tidak memiliki tugas untuk mendapatkan nilai yang akurat, karena saya masih tidak dapat menemukan opsi yang lebih murah daripada $ 5 per contoh. </p><br><div class="spoiler">  <b class="spoiler_title">Kenapa tepatnya $ 5?</b> <div class="spoiler_text"><p>  Dimungkinkan untuk menemukan VPS / VDS lebih murah ketika hosting server di Rusia atau CIS, tetapi kisah sedih yang terkait dengan ILV dan tindakannya menimbulkan risiko tertentu dan menimbulkan keinginan alami untuk menghindarinya. </p></div></div><br><p>  Jadi: </p><br><ul><li>  Master Server / Konfigurasi Server (Master Nodes): <br><ul><li>  etcd (3.2.17): 80 - 100M, metrik diambil pada waktu yang dipilih secara acak.  Konsumsi memori rata-rata etcd tidak melebihi 300 juta; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237.6M ~ 300M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): sekitar 90M, tidak naik di atas 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): sekitar 20M, konsumsi di atas 30-50M tidak tetap. </li></ul></li><li>  Konfigurasi server pekerja (Worker Nodes): <br><ul><li>  kubelet (1.12.3 - 1.13.1): sekitar 35 Mb, konsumsi di atas 50M tidak tetap; </li><li>  kube-proxy (1.12.3 - 1.13.1): sekitar 7.5 - 10M; </li><li>  flanel (0.10.0): sekitar 15-20M; </li><li>  coredns (1.3.0): sekitar 25M; </li><li>  containerd (1.2.1): konsumsi containerd rendah, tetapi statistik juga menunjukkan proses kontainer diluncurkan oleh daemon. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">Apakah contenterd / docker diperlukan pada node master?</b> <div class="spoiler_text"><p>  <strong>Tidak, tidak dibutuhkan</strong> .  Master node tidak memerlukan buruh pelabuhan atau server konten per se, meskipun ada sejumlah besar manual di Internet yang termasuk menggunakan lingkungan containerization untuk berbagai keperluan.  Dalam konfigurasi yang dimaksud, Conteerd sengaja dimatikan dari daftar dependensi, namun, saya tidak menyoroti keuntungan nyata dari pendekatan ini. </p><br><p>  Konfigurasi yang disediakan di atas minimal dan cukup untuk memulai cluster.  Tidak ada tindakan / komponen tambahan yang diperlukan, kecuali jika Anda ingin menambahkan sesuatu seperti yang Anda inginkan. </p></div></div><br><p>  Untuk membangun gugus uji atau gugus untuk proyek-proyek rumah, RAM 1vCPU / 1G akan cukup untuk master node berfungsi.  Tentu saja, beban pada node master akan bervariasi tergantung pada jumlah pekerja yang terlibat, serta ketersediaan dan volume permintaan pihak ketiga ke server api. </p><br><p>  Saya meledak konfigurasi master dan pekerja sebagai berikut: </p><br><ul><li>  1x Master dengan komponen yang diinstal: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Pekerja dengan komponen yang dipasang: containerd, coredns, flannel, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  Konfigurasi </h1><br><p>  Untuk mengkonfigurasi wizard, diperlukan komponen berikut: </p><br><ul><li><p>  etcd - untuk menyimpan data untuk api-server, serta untuk flannel; </p><br></li><li><p>  kube-apiserver - sebenarnya, api-server; </p><br></li><li><p>  kube-controller-manager - untuk menghasilkan dan memproses acara; </p><br></li><li><p>  kube-scheduler - untuk distribusi sumber daya yang terdaftar melalui api-server - misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perapian</a> . <br>  Untuk konfigurasi workhorses, diperlukan komponen berikut: </p><br></li><li><p>  kubelet - untuk menjalankan perapian, untuk mengkonfigurasi pengaturan jaringan; </p><br></li><li><p>  kube-proxy - untuk mengatur perutean / penyeimbangan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">layanan</a> kubernetes; </p><br></li><li><p>  coredns - untuk penemuan layanan di dalam wadah yang berjalan; </p><br></li><li><p>  flannel - untuk mengatur akses jaringan kontainer yang beroperasi pada node yang berbeda, serta untuk distribusi dinamis jaringan di antara node (kubernetes node) dari sebuah cluster. </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Penyimpangan kecil harus dilakukan di sini: coredn juga dapat diluncurkan pada server master.  Tidak ada batasan yang akan memaksa coredns untuk berjalan di node kerja, kecuali untuk nuansa konfigurasi coredns.service, yang sama sekali tidak dimulai pada server Ubuntu standar / tidak dimodifikasi karena konflik dengan layanan yang diselesaikan oleh systemd.  Saya tidak mencoba menyelesaikan masalah ini, karena 2 ns server yang terletak di node yang bekerja cukup senang dengan saya. </p></div></div><br><p>  Agar tidak membuang waktu sekarang membiasakan diri dengan semua detail dari proses konfigurasi komponen, saya sarankan Anda membiasakan diri dengan mereka dalam panduan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cara yang sulit Kubernetes</a> .  Saya akan fokus pada fitur yang membedakan opsi konfigurasi saya. </p><br><h2 id="fayly">  File </h2><br><p>  Semua file untuk berfungsinya komponen cluster untuk wisaya dan node kerja ditempatkan di <strong>/ var / lib / kubernetes /</strong> untuk kenyamanan.  Jika perlu, Anda dapat menempatkannya dengan cara lain. </p><br><h2 id="sertifikaty">  Sertifikasi </h2><br><p>  Dasar untuk pembuatan sertifikat adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes yang</a> sama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dengan cara yang sulit</a> , praktis tidak ada perbedaan yang signifikan.  Untuk membuat ulang sertifikat bawahan, skrip bash sederhana ditulis di sekitar aplikasi <a href="">cfssl</a> - ini sangat berguna dalam proses debugging. </p><br><p>  Anda dapat menghasilkan sertifikat untuk kebutuhan Anda menggunakan skrip di bawah ini, resep dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes dengan cara yang sulit</a> atau alat lain yang sesuai. </p><br><div class="spoiler">  <b class="spoiler_title">Pembuatan sertifikat menggunakan skrip bash</b> <div class="spoiler_text"><p>  Anda bisa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mendapatkan</a> skrip di sini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubernetes bootstrap</a> .  Sebelum memulai, edit file <a href="">certs / env.sh</a> , tentukan pengaturan Anda.  Contoh: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Jika Anda menggunakan <code>env.sh</code> dan menentukan semua parameter dengan benar, maka tidak perlu menyentuh sertifikat yang dihasilkan.  Jika Anda membuat kesalahan di beberapa titik, maka sertifikat dapat dibuat ulang di beberapa bagian.  Skrip bash di atas sepele, menyortirnya tidak sulit. </p><br><p>  Catatan penting - Anda tidak harus sering membuat ulang <code>ca.pem</code> dan <code>ca-key.pem</code> , karena mereka adalah sertifikat root untuk semua sertifikat berikutnya, dengan kata lain, Anda harus membuat ulang semua sertifikat yang menyertainya dan mengirimkannya ke semua mesin dan semua direktori yang diperlukan. </p></div></div><br><h3 id="master">  Tuan </h3><br><p>  Sertifikat yang diperlukan untuk memulai layanan pada node master harus diletakkan di <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - sertifikat ini digunakan di mana-mana, hanya dapat dihasilkan sekali dan kemudian digunakan tanpa perubahan, jadi berhati-hatilah.  Ketika Anda membuat ulang, Anda harus menyalinnya ke semua node, serta memperbarui file kubeconfig yang menggunakannya (juga pada semua mesin). </li><li>  ca-key.pem sama dengan menyalin node. </li><li>  kube-controller-manager.pem - dibutuhkan hanya untuk kube-controller-manager. </li><li>  kube-controller-manager-key.pem - dibutuhkan hanya untuk kube-controller-manager. </li><li><p>  kubernetes.pem - diperlukan untuk kain flanel, coredn saat terhubung ke etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Retret teoretis</b> <div class="spoiler_text"><p>  Fitur ini didasarkan pada logika konfigurasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hard drive Kubernetes</a> . <br>  Berdasarkan ini, file ini akan dibutuhkan di mana-mana - baik pada wizard dan pada node kerja.  Saya tidak mengubah pendekatan yang diberikan oleh manual asli, karena dengan bantuannya dimungkinkan untuk mengatur operasi klaster lebih cepat dan lebih jelas dan memahami sejumlah besar dependensi. </p><br><p>  Pendapat pribadi saya adalah bahwa untuk etcd Anda memerlukan sertifikat terpisah yang tidak tumpang tindih dengan sertifikat yang digunakan untuk kubernet. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - tetap berada di server master. </li><li>  service-account.pem - hanya diperlukan untuk daemon kube-controller-manager. </li><li>  service-account-key.pem - sama. </li></ul><br><h3 id="rabochie-uzly">  Unit kerja </h3><br><ul><li>  ca.pem - diperlukan untuk semua layanan yang terlibat pada simpul kerja (kubelet, kube-proxy), serta untuk flanel, coredns.  Antara lain, isinya termasuk dalam file kubeconfig ketika mereka dihasilkan menggunakan kubectl. </li><li>  kubernetes-key.pem - hanya diperlukan untuk flanel dan coredn untuk terhubung ke etcd, yang terletak di node master api. </li><li>  kubernetes.pem - mirip dengan yang sebelumnya, hanya dibutuhkan untuk kain flanel dan coredn. </li><li>  kubelet / node-1.pem - kunci untuk otorisasi simpul-1. </li><li>  kubelet / node-1-key.pem - kunci untuk otorisasi simpul-1. </li></ul><br><p>  <strong>Penting!</strong>  Jika Anda memiliki lebih dari satu node, maka setiap node akan menyertakan file <code>node-X-key.pem</code> , <code>node-X.pem</code> dan <code>node-X.kubeconfig</code> di dalam kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Sertifikat Debugging</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Sertifikat Debugging </h4><br><p>  Terkadang Anda mungkin perlu melihat bagaimana sertifikat dikonfigurasikan untuk mencari tahu IP / DNS host mana yang digunakan untuk menghasilkannya.  Perintah <code>cfssl-certinfo -cert &lt;cert&gt;</code> akan membantu kami dalam hal ini.  Sebagai contoh, kita mempelajari informasi ini untuk <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Semua sertifikat lain untuk kubelet dan kube-proxy disematkan langsung ke kubeconfig yang sesuai. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Semua kubeconfig yang diperlukan dapat dilakukan dengan menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes dengan cara yang sulit</a> , namun, di sini beberapa perbedaan dimulai.  Manual ini menggunakan <code>kubedns</code> dan <code>cni bridge</code> , juga mencakup <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">coredns</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">flanel</a> .  Kedua layanan ini, pada gilirannya, menggunakan <code>kubeconfig</code> untuk <code>kubeconfig</code> ke cluster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  Tuan </h3><br><p>  Untuk panduan, file kubeconfig berikut diperlukan (seperti yang disebutkan di atas, setelah generasi dapat diambil dalam <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  File-file ini akan diperlukan untuk menjalankan masing-masing komponen layanan. </p><br><h3 id="rabochie-uzly-1">  Unit kerja </h3><br><p>  Untuk simpul kerja, file kubeconfig berikut ini diperlukan: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¦  L-- coredns.kubeconfig +-- flanneld ¦  L-- flanneld.kubeconfig +-- kubelet ¦  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Peluncuran layanan </h2><br><div class="spoiler">  <b class="spoiler_title">Layanan</b> <div class="spoiler_text"><p>  Terlepas dari kenyataan bahwa node pekerjaan saya menggunakan sistem inisialisasi yang berbeda, contoh dan repositori memberikan opsi menggunakan systemd.  Dengan bantuan mereka, akan lebih mudah untuk memahami proses mana dan dengan parameter mana Anda perlu memulai, di samping itu, mereka seharusnya tidak menyebabkan masalah besar ketika mempelajari layanan dengan flag tujuan. </p></div></div><br><p>  Untuk memulai layanan, Anda perlu menyalin <code>service-name.service</code> ke <code>/lib/systemd/system/</code> atau direktori lain di mana layanan untuk systemd berada, dan kemudian nyalakan dan mulai layanan.  Contoh untuk apubever kubus: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Tentu saja, semua layanan harus <em>berwarna hijau</em> (yaitu berjalan dan berfungsi).  Jika Anda menemukan kesalahan, perintah <code>journalct -xe</code> atau <code>journal -f -t kube-apiserver</code> akan membantu Anda memahami apa yang sebenarnya salah. </p><br><p>  Jangan terburu-buru untuk memulai semua server sekaligus, untuk memulainya akan cukup untuk mengaktifkan etcd dan kube-apiserver.  Jika semuanya berjalan dengan baik, dan Anda segera mendapatkan keempat layanan pada wizard, peluncuran wizard dapat dianggap berhasil. </p><br><h3 id="master-2">  Tuan </h3><br><p>  Anda dapat menggunakan pengaturan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd</a> atau menghasilkan skrip init untuk konfigurasi yang Anda gunakan.  Seperti yang telah disebutkan, untuk master yang Anda butuhkan: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Unit kerja </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / flanel</a> </p><br><h3 id="klient">  Pelanggan </h3><br><p>  Agar klien dapat bekerja, cukup salin <code>certs/kubeconfig/admin.kubeconfig</code> (setelah Anda membuatnya atau menulis sendiri) di <code>${HOME}/.kube/config</code> </p><br><p>  Unduh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubectl</a> dan periksa pengoperasian kube-apiserver.  Izinkan saya mengingatkan Anda sekali lagi bahwa pada tahap ini, agar kube-apiserver berfungsi, hanya etcd yang akan berfungsi.  Komponen yang tersisa akan dibutuhkan untuk operasi penuh dari cluster sesaat kemudian. </p><br><p>  Periksa apakah kube-apiserver dan kubectl berfungsi: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Konfigurasi Flanel </h1><br><p>  Sebagai konfigurasi flanel, saya memilih backend <code>vxlan</code> .  Baca lebih lanjut tentang backend di <a href="">sini</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw dan mengapa itu tidak berhasil</b> <div class="spoiler_text"><p>  Saya harus mengatakan segera bahwa menjalankan kluster kubernet pada VPS cenderung membatasi Anda untuk menggunakan backend <code>host-gw</code> .  Tidak menjadi insinyur jaringan yang berpengalaman, saya menghabiskan sekitar dua hari debugging untuk memahami apa masalahnya dengan penggunaannya pada penyedia VDS / VPS yang populer. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Linode.com</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">digitalocean</a> telah diuji.  Inti masalahnya adalah bahwa penyedia tidak memberikan L2 jujur ​​untuk jaringan pribadi.  Ini, pada gilirannya, membuat tidak mungkin untuk memindahkan lalu lintas jaringan antara node dalam konfigurasi ini: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Lalu lintas"></p><br><p>  Agar lalu lintas jaringan berfungsi antara node, routing normal akan cukup.  Jangan lupa bahwa net.ipv4.ip_forward harus disetel ke 1, dan rantai FORWARD dalam tabel filter tidak boleh berisi aturan larangan untuk node. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  Inilah yang tidak bekerja pada VPS / VDS yang ditunjukkan (dan, kemungkinan besar, umumnya pada semua). </p><br><p>  Oleh karena itu, jika konfigurasi solusi dengan kinerja jaringan tinggi antara node <strong>penting bagi</strong> Anda, Anda masih harus menghabiskan lebih dari $ 20 untuk mengatur cluster. </p></div></div><br><p>  Anda dapat menggunakan <code>set-flannel-config.sh</code> dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">etc / flannel</a> untuk mengatur konfigurasi flanel yang diinginkan.  <strong>Penting untuk diingat</strong> : jika Anda memutuskan untuk mengubah backend, Anda harus menghapus konfigurasi di etcd dan restart semua daemon flanel pada semua node, jadi pilihlah dengan bijak.  Standarnya adalah vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Setelah Anda mendaftarkan konfigurasi yang diinginkan di etcd, Anda perlu mengonfigurasi layanan untuk menjalankannya di masing-masing node yang berfungsi. </p><br><h2 id="flannelservice">  layanan flannel </h2><br><p>  Contoh untuk layanan dapat diambil di sini: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">layanan flannel</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Kustomisasi </h2><br><p>  Seperti dijelaskan sebelumnya, kita memerlukan file ca.pem, kubernetes.pem dan kubernetes-key.pem untuk otorisasi di etcd.  Semua parameter lain tidak memiliki makna suci.  Satu-satunya hal yang benar-benar penting adalah mengonfigurasi alamat ip global yang akan digunakan paket jaringan antar jaringan: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Jaringan flanel"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Overlay Jaringan Multi-Host dengan Flanel</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Setelah flanel mulai berhasil, Anda harus menemukan antarmuka jaringan flanel. N pada sistem Anda: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Memeriksa bahwa antarmuka Anda berfungsi dengan baik pada semua node cukup sederhana.  Dalam kasus saya, masing-masing node-1 dan node-2 memiliki 10.200.8.0/24 dan 10.200.12.0/24, jadi dengan permintaan icmp reguler kami memeriksa ketersediaannya: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  Dalam hal terjadi masalah, disarankan untuk memeriksa apakah ada aturan pemotongan di iptables atas UDP antara host. </p><br><h1 id="konfiguraciya-containerd">  Konfigurasi Containerd </h1><br><p>  Tempatkan <a href="">etc / berisierd / config.toml</a> di <code>/etc/containerd/config.toml</code> atau di tempat yang nyaman bagi Anda, yang utama adalah ingat untuk mengubah jalur ke file konfigurasi dalam layanan (berisi layananerd, layanan yang dijelaskan di bawah). </p><br><p>  Konfigurasi dengan beberapa modifikasi standar.  <strong>Penting untuk tidak mengatur</strong> <code>enable_tls_streaming = true</code> jika Anda tidak mengerti mengapa Anda melakukan ini.    <code>kubectl exec</code>        ,      . </p><br><h2 id="containerdservice"> containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Kustomisasi </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Kustomisasi </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) —    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               —     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Kustomisasi </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Kustomisasi </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Kustomisasi </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> — <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> —  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://kubernetes-example.w40k.net/</a> —   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Referensi </h1><br><p> ,     ,   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes the hard way</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Intro to Podman</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Stateless Applications</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ) <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            —    . </p><br><p>              . , ,   , —      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id435228/">https://habr.com/ru/post/id435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id435214/index.html">Linux 4.20 dirilis - apa yang telah berubah di versi kernel baru</a></li>
<li><a href="../id435216/index.html">Cara membuat 200 dari dua baris kode, dan mengapa Anda perlu melakukan ini</a></li>
<li><a href="../id435220/index.html">Kotlin Native: melacak file</a></li>
<li><a href="../id435224/index.html">Cara berkomunikasi di kantor berbahasa Inggris: 14 idiom yang bermanfaat</a></li>
<li><a href="../id435226/index.html">Kembalikan data dari awal</a></li>
<li><a href="../id435234/index.html">Lebih cerdas, lebih jauh, lebih tepatnya: bagaimana AI mengubah penerbangan ke ruang angkasa</a></li>
<li><a href="../id435236/index.html">Mesin byte untuk benteng (dan tidak hanya) di Native American (bagian 3)</a></li>
<li><a href="../id435240/index.html">Unreal Engine4 - Efek pemindaian PostProcess</a></li>
<li><a href="../id435242/index.html">Mengapa saya takut menjadi "pria yang dipompa"</a></li>
<li><a href="../id435244/index.html">Proyek ITER pada 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>