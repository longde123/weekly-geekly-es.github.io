<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëàüèø ‚öñÔ∏è üö∫ C√≥mo probamos el almacenamiento definido por software, tambi√©n conocido como Virtual SAN üîö üë©üèΩ‚Äçüé§ üï≥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Decidimos probar en la pr√°ctica una nueva tendencia en la comunidad de TI, a saber, el almacenamiento definido por software.
 
 
 
 Pedimos a nuestros...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo probamos el almacenamiento definido por software, tambi√©n conocido como Virtual SAN</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/383157/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decidimos probar en la pr√°ctica una nueva tendencia en la comunidad de TI, a saber, el almacenamiento definido por software.</font></font><br>
<img src="https://habrastorage.org/files/146/dc9/6c2/146dc96c24854245997228a27e86ee41.jpg"><br>
<a name="habracut"></a><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Pedimos a nuestros maravillosos proveedores un campo de pruebas. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Configuraci√≥n del soporte:</font></font><br>
<br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3 servidores SuperMicro X9SCL / X9SCM de la siguiente configuraci√≥n:</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Procesador Intel E3-1220V2: 4 n√∫cleos x 3.1 GHz</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RAM - 16 Gb</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adaptador RAID Adaptec 6405E: 2x300GB SAS 10K, 1x180GB SSD: todas las unidades deben configurarse en modo JBOD</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2x500GB SATA 7.4K conectado a los conectores en el tapete. </font><font style="vertical-align: inherit;">placa de circuito</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NIC 2x1GB</font></font></li>
</ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En realidad, hay muchas soluciones de Virtual SAN ahora, Starwind VSAN se cay√≥ debido a limitaciones significativas de la versi√≥n gratuita, que elimin√≥ todas las caracter√≠sticas interesantes, VMWARE VSAN se cay√≥ debido a los requisitos de hardware y el alto costo de propiedad, que no es √≥ptimo para presupuestos regionales peque√±os, as√≠ como por el hecho de que esos. Los especialistas de VMWARE al margen no recomiendan usar su VSAN para aplicaciones cr√≠ticas para el negocio. Como resultado, elegimos una soluci√≥n de Nutanix y EMC ScaleIO. </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ScaleIO es una SAN virtuosa definida por software de EMC</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, puede funcionar en modo libre casi sin restricciones (¬°escrito en la licencia para uso que no sea de producci√≥n!): el √∫nico inconveniente grave es que este sistema no tiene mecanismos incorporados para el almacenamiento autom√°tico de datos o el almacenamiento en cach√©, yo uso discos SSD (a diferencia de competidores a quienes el SSD les da una gran ventaja en la velocidad de las m√°quinas virtuales). </font><font style="vertical-align: inherit;">Arquitect√≥nicamente, se parece mucho al sistema de almacenamiento empresarial IBM XIV de gama alta, incluso el tama√±o de bloque es el mismo: 1 MB. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Le√≠ requisitos m√≠nimos contradictorios en diferentes fuentes:</font></font><br>
<ul>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estos son los requisitos m√≠nimos del sistema para una implementaci√≥n de ScaleIO 1.31 y 1.32:</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La soluci√≥n ScaleIO 1.31 y 1.32 solo es compatible con ESXi 5.5 GA.</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tres servidores ESXi con 100 GB de capacidad libre por servidor</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Red de 1 gbps</font></font></li>
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuatro discos SATA de 7.200 rpm por nodo</font></font></li>
</ul><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, en la pr√°ctica, para la versi√≥n 1.32, tales restricciones no se cumplieron. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Debido al hecho de que en la producci√≥n usamos el VMWARE racialmente correcto y no nos gusta el Hyper-V no ortodoxo, seleccionamos la versi√≥n de prueba de VMWARE ESXi 6.0 para su implementaci√≥n como hipervisor. En aras de la justicia, debe tenerse en cuenta que ScaleIO funciona bien en los siguientes hipervisores XEN, ESXi, Hyper-V; en este √∫ltimo, con la ayuda de Windows 2012 r2, los artesanos eludieron la falta de almacenamiento en cach√© de SSD utilizando las herramientas integradas en 2012 R2, que por supuesto tuvo un impacto positivo en el rendimiento del sistema. (hay una t√©cnica en Internet)</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Primero debe instalar el hipervisor ESXi en todos los hosts e instalar el dispositivo VCENTER para administrar el cl√∫ster VMWARE. El primer incidente nos esperaba all√≠ mismo: ESXi nunca quiso ver el controlador Adaptec 6405E, aunque est√° presente en VMWARE HCL. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Descargamos controladores (Offline-Bundles y VIB) del sitio web de Adaptec para VMWARE y a trav√©s de VMWARE powercli usando </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">esta herramienta</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , preparamos una imagen personalizada del hipervisor con controladores integrados; despu√©s de eso, todo sali√≥ bien. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
A continuaci√≥n, se implementa la virtualizaci√≥n del dispositivo VCENTER y, nuevamente, a trav√©s de vmware powercli, se registran los complementos de ScaleIO y las plantillas de m√°quina virtual de ScaleIO en formato .ova se copian en el almac√©n de datos (de acuerdo con las instrucciones de instalaci√≥n de ScaleIO).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de reiniciar todos los hosts, el sistema est√° listo para trabajar en la instalaci√≥n de ScaleIO. </font></font><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Recomiendo leer las instrucciones de implementaci√≥n, de lo contrario, muchos puntos ser√°n incomprensibles: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - por ejemplo, debe comprender cosas como el dominio de protecci√≥n y c√≥mo se relaciona con los grupos de almacenamiento, </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - cu√°les son las recomendaciones para crear grupos de almacenamiento (que no deber√≠an interferir con HDD y SSD , aunque esto es posible) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 : para comprender qu√© es FaultSets, aunque no lo usamos. Comprenda qu√© es la pol√≠tica de ZeroPadding, la pol√≠tica de cach√© de RAM y sus opciones, la pol√≠tica de repuesto. ¬°Muchos </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">matices ScaleIO no admite la capacidad de usar unidades SSD como cach√© para una matriz de datos! Esta es una caracter√≠stica desagradable. Se pueden recopilar agrupaciones de SSD, pero ser√° solo una agrupaci√≥n solo flash, ¬°nada m√°s!</font></font><br>
</b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬°Recomendamos usar el adaptador RAID de lectura / escritura de cach√© como elemento de cach√© si est√° equipado con una bater√≠a, y tambi√©n hay una funci√≥n RAM CACHE al crear un dominio de protecci√≥n que usa parte de la RAM del host de virtualizaci√≥n como cach√© de matriz! (en 16 GB de RAM logramos asignar 1.3 GB como cach√© en la configuraci√≥n, lea la documentaci√≥n) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Tambi√©n necesita 2 VLAN virtuales: administraci√≥n y datos, deber√° emitir cada uno de los 4 (en la configuraci√≥n m√≠nima) administraci√≥n de SVM (SVM - M√°quina virtual ScaleIO) IP y DATA IP con una indicaci√≥n de la puerta de enlace y las m√°scaras. (Es muy recomendable preparar un plan de direccionamiento con anticipaci√≥n para no completar todos los campos requeridos 10 veces).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En total, en modo m√≠nimo, la instalaci√≥n crea 4 SVM en nuestros 3 hosts (SVM - ScaleIO Virtual Machine), ocupa 8 cpu en total (2 en el host: 2 m√°quinas virtuales en el primer host, por lo tanto, 4 vcpu), ocupa 12 Gb de memoria en total. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM en el nodo1: mdm primario, sds, sdc </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM en el nodo2: mdm secundario, sds, sdc </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM en el nodo 3: disyuntor, sdc </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SVM en cualquier nodo: puerta de enlace: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
mdm </font><font style="vertical-align: inherit;">(actualizaci√≥n centralizada, monitoreo, mantenimiento) </font><font style="vertical-align: inherit;">mdm: Meta Data Manager: administra tablas de asignaci√≥n de dispositivos </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
sds - servidor de datos ScaleIO - controla la entrega de unidades f√≠sicas a trav√©s de discos RDM en VMWARE (asignaci√≥n de disco sin formato)</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
sdc: cliente de datos ScaleIO: en el caso de VMWARE, se agrega. m√≥dulos de kernel que se integran durante la fase de implementaci√≥n de ScaleIO y le permiten lograr un mayor rendimiento que cuando trabajan en la m√°quina virtual SVM. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Adem√°s, una gran ventaja de tener SDC en el n√∫cleo es que los vol√∫menes de datos creados en ScaleIO se presentan en VMWARE directamente como LUN para crear un almac√©n de datos, sin usar una capa adicional de virtualizaci√≥n y presentaci√≥n de vol√∫menes a trav√©s del mecanismo iSCSI. </font></font><br>
<br>
<img src="https://habrastorage.org/files/4c3/4ec/bc3/4c34ecbc3e614098ba3e1ac3c061e78f.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El proceso de implementaci√≥n no se realiz√≥ sin problemas, ya que no ten√≠amos las VLAN y direcciones IP necesarias para SVM listo. Despu√©s de preparar los datos y reiniciar el asistente de instalaci√≥n en el √∫ltimo paso, estos errores comenzaron a salir de nosotros: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fall√≥: agregue el dispositivo SDS ScaleIO-4c1885e7 a los discos duros de Storage Pool (ScaleIO - Timeout)</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Con lo que estaba conectado no puedo decir. Nos sentamos, pensamos, reintentos presionados. Los errores anteriores desaparecieron, pero aparecieron otros errores en uno de los SVM en el host2: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Fall√≥: Agregue el dispositivo SDS ScaleIO-4c1885eb a los discos duros de la agrupaci√≥n de almacenamiento (ScaleIO - El SDS ya est√° conectado a este MDM) </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Nos sentamos y pensamos ... descubrimos que en uno de los SVM donde se encontraba el mdm secundario, por alguna raz√≥n durante la instalaci√≥n, no se crearon asignaciones de RDM a discos f√≠sicos y, en consecuencia, el servicio SDS no aument√≥. Por lo tanto, uno de los nodos que no comenzamos. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Hicieron clic de nuevo en el bot√≥n "implementar entorno de scaleio" y, he aqu√≠, el host y la m√°quina virtual vac√≠a se encontraron como no utilizados, ingresaron nuevamente los datos sobre VLAN y direcciones IP, y el host se inici√≥ normalmente sin errores.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de eso, un momento interesante estaba esperando en la etapa de creaci√≥n de volumen en el que colocaremos los datos. No pude encontrar este elemento en la GUI: ni en el tablero de la GUI que est√° instalado en Windows, ni en la GUI de VMWARE, ni en la interfaz web de Gateway. muchas instrucciones para crear desde cli - encontradas por casualidad en alg√∫n blog. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Doy una captura de pantalla: ¬ød√≥nde est√° el bot√≥n atesorado para crear volumen en la GUI de VMWARE? </font></font><br>
<br>
<img src="https://habrastorage.org/files/6f3/891/a2a/6f3891a2a60b4135b40a528c2fc7d49e.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬°Hurra, encontrado! Pero los "aplausos" no fueron largos. De acuerdo con la captura de pantalla en el grupo de almacenamiento con el nombre HDD (donde combin√© los 6 discos SAS de los 3 nodos) tenemos un l√≠mite de capacidad total de 1.6 TB (que generalmente corresponde a 6x300GB), pero el volumen con ese volumen no se crea ... y no est√° claro qu√© volumen disponible para el usuario. Le√≠ instrucciones de Internet sobre c√≥mo usar la CLI para ver el volumen disponible. Revisamos SSH a uno de los SVM de MDM. Doy el comando:</font></font><br>
<br>
<pre><code class="cs hljs">ScaleIO<span class="hljs-number">-10</span><span class="hljs-number">-1</span><span class="hljs-number">-4</span><span class="hljs-number">-203</span>:~ <span class="hljs-meta"># scli --query_storage_pool --protection_domain_name pd2 --storage_pool_name hdds</span>
Error: MDM failed command.  Status: Invalid session. Please login and <span class="hljs-keyword">try</span> again.<font></font>
<font></font>
</code></pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øQu√©? </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Intento con el segundo MDM: el error es algo as√≠ como no se puede conectar localhost: 6611 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En la puerta de enlace generalmente se dice que no hay tales comandos. </font><font style="vertical-align: inherit;">¬°Todo es un estupor! </font><font style="vertical-align: inherit;">No est√° claro c√≥mo usar la CLI. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Estudiamos cuidadosamente la gu√≠a del usuario en ScaleIO, especialmente sobre los conceptos b√°sicos de la CLI, y dice en la parte inferior que, adem√°s, debe iniciar sesi√≥n en la CLI y la fecha del enlace, c√≥mo hacerlo, pero debe hacerlo as√≠:</font></font><br>
<pre><code class="bash hljs">ScaleIO-10-1-4-203:~ <span class="hljs-comment"># scli --login --username admin</span><font></font>
Enter password:<font></font>
</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Y despu√©s de eso, todos los equipos de CLI comenzaron a trabajar maravillosamente.</font></font><br>
<br>
<pre><code class="bash hljs">scli --query_storage_pool --protection_domain_name pd2 --storage_pool_name hdds<font></font>
<font></font>
Storage Pool hdds (Id: 574711eb00000000) has 0 volumes and<font></font>
544.0 GB (557056 MB) available <span class="hljs-keyword">for</span> volume allocation<font></font>
Background device scanner: Disabled<font></font>
Zero padding is disabled<font></font>
Spare policy: 34% out of total<font></font>
Uses RAM Read Cache<font></font>
RAM Read Cache write handling mode is <span class="hljs-string">'cached'</span><font></font>
 1.6 TB (1667 GB) total capacity<font></font>
 1.1 TB (1100 GB) unused capacity<font></font>
 567.1 GB (580708 MB) spare capacity<font></font>
</code></pre><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
¬øEsperar lo? ¬øEl volumen disponible en nuestro caso es 544GB? ¬øC√≥mo? ¬øPor qu√©? Aqu√≠ se requiere una explicaci√≥n de la arquitectura de almacenamiento (ver captura de pantalla): </font></font><br>
<br>
<img src="https://habrastorage.org/files/372/7c1/3a7/3727c13a7e3e4c9ebfff7b3d9c8dd099.png"><br>
 <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
es decir volumen total de agrupaci√≥n de almacenamiento sin formato denominado HDD: 1,6 TB menos 567,1 GB de capacidad de reserva = 1,1 TB de capacidad no utilizada. La capacidad de repuesto est√° determinada por el par√°metro Pol√≠tica de repuesto en la configuraci√≥n. De acuerdo con las recomendaciones de la documentaci√≥n, para una protecci√≥n completa contra la falla de un nodo, es necesario que la capacidad de almacenamiento sea al menos 1 \ N del volumen total de todos los discos f√≠sicos de todos los nodos incluidos en el grupo de almacenamiento, donde N es el n√∫mero de nodos que est√°n en el grupo (es decir, e si tenemos 3 nodos con discos en este grupo de almacenamiento, entonces la capacidad de reserva en nuestro caso es 1 \ 3 de 1.6TB (del total), respectivamente 567.1 GB (580708 MB).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por lo tanto, obtenemos 1.1 TB de espacio no utilizado en el grupo de almacenamiento para crear volumen. Sin embargo, para fines de alta disponibilidad y confiabilidad, ScaleIO almacena 2 copias de cada bloque de datos en todas las unidades f√≠sicas, respectivamente, dividimos el volumen de 1.1TB por 2, y como resultado obtenemos el volumen disponible para crear un volumen de 544.0 GB. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Durante la operaci√≥n de prueba del sistema, logramos los siguientes indicadores (la velocidad era de 85-130 MB / s, pero debe comprender que esto es solo la carga de una m√°quina virtual, al expandir la matriz agregando nodos con discos, creo que todo ser√° proporcional al crecimiento con IOPS): </font></font><br>
<br>
<img src="https://habrastorage.org/files/21a/8c0/ce3/21a8c0ce34c844ce8a914a7c7b4386b4.png"><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nutanix Comminity Edition 4.5 - gratis para uso no comercial</font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En general, la documentaci√≥n para Nutanix, por extra√±o que parezca, es bastante peque√±a ... Algunos puntos se aclararon durante el proceso de instalaci√≥n. Nutanix combina el sistema de administraci√≥n de cl√∫ster de hipervisor y virtualizaci√≥n (ala VMWARE Vcenter) en una "persona", as√≠ como un sistema de almacenamiento distribuido entregado a trav√©s de NFS con funciones de alta disponibilidad. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Desafortunadamente, no pudimos obtener la versi√≥n con el hipervisor ESXi. Por lo tanto, comenzaron a probar la versi√≥n de Community Edition, y tiene muchas limitaciones: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - m√°ximo 4 nodos </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - m√°ximo 4 discos por nodo (incluidos los SSD, por lo que la capacidad m√°xima por host es de 18 TB (3 x 6 TB HDD)) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - Procesador Intel con soporte VT-x M√≠nimo de 4 n√∫cleos </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 : controladores SATA de interfaz de controlador de host avanzado (AHCI) incorporados</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - Controladores LSI en modo TI (las pruebas de Nutanix muestran un mejor rendimiento que IR) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - al menos 16 GB de memoria por host (se recomienda encarecidamente 32 GB o m√°s para la deduplicaci√≥n - no se enciende durante 16) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - SSD m√≠nimo por host 200 GB </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - m√≠nimo el volumen SSD por host para admitir la deduplicaci√≥n de 300 </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
GB </font><font style="vertical-align: inherit;">(preferiblemente m√°s de 400,480) </font><font style="vertical-align: inherit;">Nutanix no determin√≥ el controlador SAS ADAPTEC 6405E, por lo que no pudimos usar discos SAS (utilizamos discos SATA y SSD que est√°n conectados a los conectores de la placa base).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
La instalaci√≥n requiere medios en cada host: no colocamos la imagen terminada en los discos locales del servidor; colocamos una unidad flash de 8GB en cada host (los requisitos m√≠nimos son una unidad flash de 8GB, mejor 16, un hipervisor KVM (Acropolis) y una m√°quina virtual de control CVM) - ¬°y as√≠ sucesivamente para cada host!), El n√∫mero total de m√°quinas virtuales CVM por el n√∫mero de hosts. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En nuestro caso, se obtuvo la siguiente configuraci√≥n: </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - 16 gigabytes de memoria por host </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - para cada host 2 discos duros HDD SATA 500GB </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - para cada host 1 disco duro SSD 500GB </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
 - para la deduplicaci√≥n necesita un m√≠nimo de 24GB de memoria por host + un m√≠nimo de 300GB SSD por host, por lo que ella no se volvi√≥ contra nosotros. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En caso de incumplimiento de los requisitos m√≠nimos, el sistema no est√° instalado ...</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No puede cambiar el hipervisor en Community Edition, solo KVM est√° instalado de forma predeterminada, esto es un gran inconveniente, porque estamos acostumbrados a usar VMWARE ESXi. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de la instalaci√≥n, aparece una m√°quina virtual de servicio (CVM - m√°quina virtual de control) en cada host, que ocupa 2 n√∫cleos de procesador y reserva 12 GB de RAM en cada host; tenga esto en cuenta cuando planifique. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de la instalaci√≥n en todos los nodos del cl√∫ster, creamos vol√∫menes para el almacenamiento de datos: al crear StorageContainer, si Replication Factor 2, el espacio libre del almacenamiento en disco es un 50% menor (todos los bloques de datos se duplican en otros discos, por ejemplo, tenemos 3 hosts, en cada 2 discos SATA 500 GB, un total de 6 unidades + unidades SSD en cada uno de los hosts, solo 2,88 TB de volumen total, de los cuales 1,44 TB est√°n disponibles para el usuario para el almacenamiento).</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Despu√©s de la creaci√≥n, se puede acceder a la matriz de discos y entregarla a trav√©s de NFS: por ejemplo, se conect√≥ perfectamente al cl√∫ster VMWARE 5.5 como un almac√©n de datos, respectivamente, el almacenamiento vmotion tambi√©n funciona, aunque la velocidad es mucho m√°s lenta en comparaci√≥n con FC 4Gb \ seg (es comprensible con una conexi√≥n gigabit - fue interesante conducir√≠a en 10G o infiniband). </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Una m√°quina en vivo de 60 GB del cl√∫ster VMWARE se traslad√≥ unos 24 minutos al almacenamiento de Nutanix. Regres√≥ 13 minutos al almacenamiento FC de nivel empresarial. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Escribir el archivo iso desde el servidor de Windows en la m√°quina virtual vmware que se encuentra en el almacenamiento nutanix a trav√©s de nfs (1Gb / s) - 34MB / seg. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No existe una herramienta interactiva para administrar grupos de puertos y conmutadores virtuales: todo se hace desde cli.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Un punto obvio sobre las redes para ejecutar m√°quinas virtuales: le aconsejo que lea detenidamente la documentaci√≥n. Para las m√°quinas virtuales dentro de Nutanix, utilizamos las VLAN que todos nuestros otros sistemas de virtualizaci√≥n usan para acceder a la red. En la etapa de configuraci√≥n de redes virtuales, el n√∫mero de VLAN de la red virtual simplemente se indica: no hay m√°s configuraciones y, en las propiedades de la m√°quina virtual, el adaptador de red virtual est√° configurado para coincidir con el n√∫mero de red, y eso es todo, ¬°no hay otras configuraciones en la GUI! Pero todo funciona. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Para instalar m√°quinas virtuales de invitados de Windows, debe conectar un segundo CDROM virtual con controladores VirtIO (por ejemplo, puede tomarlo desde aqu√≠ (primero coloque el archivo iso en el repositorio): </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fedoraproject.org/wiki/Windows_Virtio_Drivers#Direct_download</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
El tiempo de instalaci√≥n de Guest VM en Windows 2012 R2 fue de solo 7 minutos desde el principio hasta la carga completa del sistema de trabajo, la m√°quina virtual funciona visualmente tambi√©n r√°pido, obviamente, SSD ayuda. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Puede colocar una m√°quina virtual en el tipo de cortador de disco virtual IDE (sin conectar los controladores VirtIO), pero esta no es una configuraci√≥n recomendada: el rendimiento ser√° mucho menor. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Durante una falla de alimentaci√≥n de prueba de uno de los hosts en el cl√∫ster, HA funcion√≥, el sistema no se bloque√≥, los recursos de disco son accesibles tanto en la red como localmente, las m√°quinas virtuales que estaban en el host desconectado se reiniciaron en otros hosts (si hay suficiente memoria),</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sin embargo, despu√©s de activar el poder del 3er host, el hecho desagradable fue que el estado del sistema se ha mantenido en Critial durante mucho tiempo: no s√© lo que esto atrae, pero no hay mucho bien. (de 1 hora a 2). Por ejemplo, el mismo cl√∫ster VMWARE despu√©s de conectar el host determina r√°pidamente su estado y vuelve a conectarse al cl√∫ster. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
En general, debe tenerse en cuenta que la interfaz Prism para administrar un cl√∫ster de Nutanix CE tiene una interfaz bastante pobre (aunque agradable). Solo hay configuraciones b√°sicas disponibles, pero en general son suficientes a primera vista, hay alg√∫n tipo de an√°lisis gr√°fico personalizable, aunque por supuesto es mucho m√°s d√©bil que en VCENTER.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
No est√° claro c√≥mo hacer una copia de seguridad: despu√©s de todo, ni los productos de Symantec ni Veeam admiten la virtualizaci√≥n de KVM ... obviamente, a trav√©s de instant√°neas, pero ¬øqu√© pasa con la recuperaci√≥n de datos granular en este caso (archivo por archivo). </font></font><br>
<br>
<b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualmente, Nutanix fue mucho m√°s r√°pido, que en la etapa de copiar im√°genes a su sistema de almacenamiento, que en la etapa de instalaci√≥n del sistema operativo en una nueva m√°quina virtual desde la imagen, que en la etapa de reinicio de arranque: casi todo instant√°neamente). </font><font style="vertical-align: inherit;">Existe el deseo de probar Nutanix en una configuraci√≥n m√°s potente y aseg√∫rese de usar VMWARE EXSi. </font></font></b><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Por supuesto, no puedo proporcionar ning√∫n dato de rendimiento particular para ambos sistemas, pero puede obtener informaci√≥n sobre el "rastrillo" en la etapa de configuraci√≥n de la configuraci√≥n. </font><font style="vertical-align: inherit;">Si eso me disculpo, la primera nota sobre Habr√©.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es383157/">https://habr.com/ru/post/es383157/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es383145/index.html">Revisi√≥n del transformador port√°til ASUS Transformer Book T300 Chi</a></li>
<li><a href="../es383147/index.html">TV para Algernon: una descripci√≥n general de los decodificadores que hacen que la TV sea m√°s inteligente</a></li>
<li><a href="../es383151/index.html">Dr. Tarifa calculada que operador de telefon√≠a m√≥vil tiene m√°s de 4G de Internet (parte 2)</a></li>
<li><a href="../es383153/index.html">M√≥dulos de apartamentos o la forma de desarrollo de locales residenciales</a></li>
<li><a href="../es383155/index.html">Android deber√≠a tener miedo solo Android</a></li>
<li><a href="../es383159/index.html">Un tel√©fono inteligente con una bater√≠a potente. Versi√≥n DEXP: 10 modelos de 4.490 a 13.990 rublos, de 3.000 a 5.200 mAh</a></li>
<li><a href="../es383163/index.html">Impresoras 3D con vidrio caliente</a></li>
<li><a href="../es383165/index.html">Anuncio de SmartBand 2</a></li>
<li><a href="../es383167/index.html">IDF 2015. Resumen de ma√±ana</a></li>
<li><a href="../es383169/index.html">C√≥mo armar un cine de alta fidelidad en casa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>