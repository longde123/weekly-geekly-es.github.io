<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔬 🤶🏼 👩‍🔬 Verbesserung des aktienbasierten Q-Learning-Agenten durch Hinzufügen von Wiederholungen und Belohnungen 💘 👨🏻‍🏫 💼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Erinnerung 
 Hallo Habr! Ich mache Sie auf eine weitere Übersetzung meines neuen Artikels aus dem Medium aufmerksam . 

 Beim letzten Mal ( erster Art...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verbesserung des aktienbasierten Q-Learning-Agenten durch Hinzufügen von Wiederholungen und Belohnungen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436628/"><h3>  Erinnerung </h3><br>  Hallo Habr!  Ich mache Sie auf eine weitere Übersetzung meines neuen Artikels aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Medium aufmerksam</a> . <br><br>  Beim letzten Mal ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erster Artikel</a> ) ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Habr</a> ) haben wir einen Agenten mit Q-Learning-Technologie erstellt, der Transaktionen mit simulierten und realen Austauschzeitreihen durchführt, und versucht zu prüfen, ob dieser Aufgabenbereich für verstärktes Lernen geeignet ist. <br><br>  Dieses Mal werden wir eine LSTM-Ebene hinzufügen, um Zeitabhängigkeiten innerhalb der Trajektorie zu berücksichtigen und das Belohnungs-Shaping-Engineering basierend auf Präsentationen durchzuführen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3c4/e75/63f/3c4e7563f3cf1e6f158dc0605fe80a78.png" alt="Bild"><br><a name="habracut"></a><br>  Ich möchte Sie daran erinnern, dass wir zur Überprüfung des Konzepts die folgenden synthetischen Daten verwendet haben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb2/f15/3b6/fb2f153b6c4e14ff1dc19d61a524f5e8.png" alt="Bild"><br><br>  Synthetische Daten: Sinus mit weißem Rauschen. <br><br>  Die Sinusfunktion war der erste Ausgangspunkt.  Zwei Kurven simulieren den Kauf- und Verkaufspreis eines Vermögenswerts, wobei der Spread die minimalen Transaktionskosten darstellt. <br><br>  Dieses Mal möchten wir diese einfache Aufgabe jedoch komplizieren, indem wir den Kreditzuweisungspfad erweitern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/561/c36/928/561c369289780642ca5b22c852d06a8b.png" alt="Bild"><br><br>  Synthetische Daten: Sinus mit weißem Rauschen. <br><br>  Die Sinusphase wurde verdoppelt. <br><br>  Dies bedeutet, dass sich die spärlichen Belohnungen, die wir verwenden, über längere Flugbahnen erstrecken müssen.  Darüber hinaus verringern wir die Wahrscheinlichkeit, eine positive Belohnung zu erhalten, erheblich, da der Agent eine Folge korrekter Aktionen zweimal länger ausführen musste, um die Transaktionskosten zu überwinden.  Beide Faktoren erschweren die Aufgabe für RL selbst unter so einfachen Bedingungen wie einer Sinuswelle erheblich. <br><br>  Darüber hinaus erinnern wir uns, dass wir diese neuronale Netzwerkarchitektur verwendet haben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dc/20c/e1e/3dc20ce1eedec47b871ef59ab7911ef3.png" alt="Bild"><br><br><h3>  Was wurde hinzugefügt und warum </h3><br><h4>  Lstm </h4><br>  Zunächst wollten wir dem Agenten ein besseres Verständnis für die Dynamik von Änderungen innerhalb der Flugbahn vermitteln.  Einfach ausgedrückt, der Agent sollte sein eigenes Verhalten besser verstehen: Was er jetzt und seit einiger Zeit in der Vergangenheit getan hat und wie sich die Verteilung der staatlichen Aktionen sowie die erhaltenen Belohnungen entwickelt haben.  Die Verwendung einer Wiederholungsebene kann genau dieses Problem lösen.  Willkommen zu der neuen Architektur, mit der neue Experimente gestartet wurden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/09c/b6e/b0d/09cb6eb0d07dd7808657aaeaec5c5c7c.png" alt="Bild"><br><br>  Bitte beachten Sie, dass ich die Beschreibung leicht verbessert habe.  Der einzige Unterschied zum alten NN besteht in der ersten verborgenen LSTM-Schicht anstelle einer vollständig gebundenen. <br><br>  Bitte beachten Sie, dass wir mit LSTM in Arbeit die Auswahl von Beispielen für die Reproduktion von Erfahrungen für das Training ändern müssen: Jetzt benötigen wir Übergangssequenzen anstelle von separaten Beispielen.  So funktioniert es (dies ist einer der Algorithmen).  Wir haben zuvor Punktstichproben verwendet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/51a/d0b/412/51ad0b4125f0d084e19e81d81a6b10a4.png" alt="Bild"><br><br>  Das fiktive Schema des Wiedergabepuffers. <br><br>  Wir verwenden dieses Schema mit LSTM: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/fb7/537/1eefb75379450cf86c03d5cf30e93dd9.png" alt="Bild"><br><br>  Nun werden Sequenzen ausgewählt (deren Länge wir empirisch angeben). <br><br>  Nach wie vor und jetzt wird die Stichprobe durch einen Prioritätsalgorithmus reguliert, der auf Fehlern des zeitlich-zeitlichen Lernens basiert. <br><br>  Das LSTM-Wiederholungsniveau ermöglicht die direkte Verbreitung von Informationen aus Zeitreihen, um ein zusätzliches Signal abzufangen, das in früheren Verzögerungen verborgen ist.  Unsere Zeitreihe ist ein zweidimensionaler Tensor mit Größe: die Länge der Sequenz auf der Darstellung unserer Zustandsaktion. <br><br><h4>  Präsentationen </h4><br>  Das preisgekrönte Engineering Potential Based Reward Shaping (PBRS), basierend auf Potenzial, ist ein leistungsstarkes Tool, um die Geschwindigkeit und Stabilität zu erhöhen und nicht die Optimalität des Richtliniensuchprozesses zur Lösung unserer Umgebung zu verletzen.  Ich empfehle, mindestens dieses Originaldokument zum Thema zu lesen: <br><br>  <a href="">people.eecs.berkeley.edu/~russell/papers/ml99-shaping.ps</a> <br><br>  Das Potenzial bestimmt, wie gut unser aktueller Status im Verhältnis zum Zielstatus ist, in den wir eintreten möchten.  Eine schematische Ansicht, wie dies funktioniert: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1b1/6f3/85d/1b16f385dedd440d6bd15f74b76f304d.png" alt="Bild"><br><br>  Es gibt Optionen und Schwierigkeiten, die Sie nach Versuch und Irrtum verstehen könnten, und wir lassen diese Details weg, sodass Sie Ihre Hausaufgaben machen können. <br><br>  Es ist noch eine weitere Sache zu erwähnen, nämlich dass PBRS durch Präsentationen gerechtfertigt werden kann, die eine Form von Expertenwissen (oder simuliertem Wissen) über das <i>nahezu</i> optimale Verhalten des Agenten in der Umgebung darstellen.  Es gibt eine Möglichkeit, solche Präsentationen für unsere Aufgabe mithilfe von Optimierungsschemata zu finden.  Wir lassen die Details der Suche weg. <br><br>  Die potenzielle Belohnung hat folgende Form (Gleichung 1): <br><br>  r '= r + gamma * F (s') - F (s) <br><br>  Dabei ist F das Potenzial des Staates und r die anfängliche Belohnung. Gamma ist der Abzinsungsfaktor (0: 1). <br><br>  <b>Mit diesen Gedanken fahren wir mit der Codierung fort.</b> <br><br>  Implementierung in R. <br>  Hier ist der neuronale Netzwerkcode, der auf der Keras-API basiert: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># configure critic NN — — — — — — library('keras') library('R6') state_names_length &lt;- 12 # just for example lstm_seq_length &lt;- 4 learning_rate &lt;- 1e-3 a_CustomLayer &lt;- R6::R6Class( “CustomLayer” , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { x — k_mean(x, axis = 2, keepdims = T) } ) ) a_normalize_layer &lt;- function(object) { create_layer(a_CustomLayer, object, list(name = 'a_normalize_layer')) } v_CustomLayer &lt;- R6::R6Class( “CustomLayer” , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { k_concatenate(list(x, x, x), axis = 2) } , compute_output_shape = function(input_shape) { output_shape = input_shape output_shape[[2]] &lt;- input_shape[[2]] * 3L output_shape } ) ) v_normalize_layer &lt;- function(object) { create_layer(v_CustomLayer, object, list(name = 'v_normalize_layer')) } noise_CustomLayer &lt;- R6::R6Class( “CustomLayer” , inherit = KerasLayer , lock_objects = FALSE , public = list( initialize = function(output_dim) { self$output_dim &lt;- output_dim } , build = function(input_shape) { self$input_dim &lt;- input_shape[[2]] sqr_inputs &lt;- self$input_dim ** (1/2) self$sigma_initializer &lt;- initializer_constant(.5 / sqr_inputs) self$mu_initializer &lt;- initializer_random_uniform(minval = (-1 / sqr_inputs), maxval = (1 / sqr_inputs)) self$mu_weight &lt;- self$add_weight( name = 'mu_weight', shape = list(self$input_dim, self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_weight &lt;- self$add_weight( name = 'sigma_weight', shape = list(self$input_dim, self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) self$mu_bias &lt;- self$add_weight( name = 'mu_bias', shape = list(self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_bias &lt;- self$add_weight( name = 'sigma_bias', shape = list(self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) } , call = function(x, mask = NULL) { #sample from noise distribution e_i = k_random_normal(shape = list(self$input_dim, self$output_dim)) e_j = k_random_normal(shape = list(self$output_dim)) #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al. eW = k_sign(e_i) * (k_sqrt(k_abs(e_i))) * k_sign(e_j) * (k_sqrt(k_abs(e_j))) eB = k_sign(e_j) * (k_abs(e_j) ** (1/2)) #See section 3 of Fortunato et al. noise_injected_weights = k_dot(x, self$mu_weight + (self$sigma_weight * eW)) noise_injected_bias = self$mu_bias + (self$sigma_bias * eB) output = k_bias_add(noise_injected_weights, noise_injected_bias) output } , compute_output_shape = function(input_shape) { output_shape &lt;- input_shape output_shape[[2]] &lt;- self$output_dim output_shape } ) ) noise_add_layer &lt;- function(object, output_dim) { create_layer( noise_CustomLayer , object , list( name = 'noise_add_layer' , output_dim = as.integer(output_dim) , trainable = T ) ) } critic_input &lt;- layer_input( shape = list(NULL, as.integer(state_names_length)) , name = 'critic_input' ) common_lstm_layer &lt;- layer_lstm( units = 20 , activation = “tanh” , recurrent_activation = “hard_sigmoid” , use_bias = T , return_sequences = F , stateful = F , name = 'lstm1' ) critic_layer_dense_v_1 &lt;- layer_dense( units = 10 , activation = “tanh” ) critic_layer_dense_v_2 &lt;- layer_dense( units = 5 , activation = “tanh” ) critic_layer_dense_v_3 &lt;- layer_dense( units = 1 , name = 'critic_layer_dense_v_3' ) critic_layer_dense_a_1 &lt;- layer_dense( units = 10 , activation = “tanh” ) # critic_layer_dense_a_2 &lt;- layer_dense( # units = 5 # , activation = “tanh” # ) critic_layer_dense_a_3 &lt;- layer_dense( units = length(actions) , name = 'critic_layer_dense_a_3' ) critic_model_v &lt;- critic_input %&gt;% common_lstm_layer %&gt;% critic_layer_dense_v_1 %&gt;% critic_layer_dense_v_2 %&gt;% critic_layer_dense_v_3 %&gt;% v_normalize_layer critic_model_a &lt;- critic_input %&gt;% common_lstm_layer %&gt;% critic_layer_dense_a_1 %&gt;% #critic_layer_dense_a_2 %&gt;% noise_add_layer(output_dim = 5) %&gt;% critic_layer_dense_a_3 %&gt;% a_normalize_layer critic_output &lt;- layer_add( list( critic_model_v , critic_model_a ) , name = 'critic_output' ) critic_model_1 &lt;- keras_model( inputs = critic_input , outputs = critic_output ) critic_optimizer = optimizer_adam(lr = learning_rate) keras::compile( critic_model_1 , optimizer = critic_optimizer , loss = 'mse' , metrics = 'mse' ) train.x &lt;- array_reshape(rnorm(10 * lstm_seq_length * state_names_length) , dim = c(10, lstm_seq_length, state_names_length) , order = 'C') predict(critic_model_1, train.x) layer_name &lt;- 'noise_add_layer' intermediate_layer_model &lt;- keras_model(inputs = critic_model_1$input, outputs = get_layer(critic_model_1, layer_name)$output) predict(intermediate_layer_model, train.x)[1,] critic_model_2 &lt;- critic_model_1</span></span></code> </pre> <br></div></div><br>  Debuggen Sie Ihre Entscheidung auf Ihr Gewissen ... <br><br><h3>  Ergebnisse und Vergleich </h3><br>  Lassen Sie uns gleich auf die Endergebnisse eingehen.  <i>Hinweis: Alle Ergebnisse sind Punktschätzungen und können bei mehreren Läufen mit unterschiedlichen zufälligen Startseiten unterschiedlich sein.</i> <br><br>  Der Vergleich beinhaltet: <br><br><ul><li>  vorherige Version ohne LSTM und Präsentationen </li><li>  einfaches 2-Element-LSTM </li><li>  4-Element-LSTM </li><li>  4-Zellen-LSTM mit generierten PBRS-Belohnungen </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/f2f/886/034/f2f886034a399fd4e1ecacf7ad418829.png" alt="Bild"><br><br>  Die durchschnittliche Rendite pro Folge betrug durchschnittlich über 1000 Folgen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26f/38c/8ef/26f38c8ef64089c605a92abaf899ba10.png" alt="Bild"><br><br>  Die gesamte Folge kehrt zurück. <br><br>  <b>Diagramme für den erfolgreichsten Agenten:</b> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/30e/304/c65/30e304c65ce97da566c6070b0338d472.jpg" alt="Bild"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/000/7d1/8a0/0007d18a0336590c79e4c3edcf5b0a9f.png" alt="Bild"><br><br>  Agentenleistung. <br><br>  Nun, es ist ziemlich offensichtlich, dass der Agent in Form von PBRS im Vergleich zu früheren Versuchen so schnell und stabil konvergiert, dass er als signifikantes Ergebnis akzeptiert werden kann.  Die Geschwindigkeit ist etwa 4-5 mal höher als ohne Präsentationen.  Stabilität ist wunderbar. <br><br>  Bei der Verwendung von LSTM zeigten 4 Zellen eine bessere Leistung als 2 Zellen.  Ein 2-Zellen-LSTM schnitt besser ab als eine Nicht-LSTM-Version (dies ist jedoch möglicherweise eine Illusion eines einzelnen Experiments). <br><br><h3>  Letzte Worte </h3><br>  Wir haben gesehen, dass Wiederholungen und Kapazitätsaufbau helfen.  Mir hat besonders gut gefallen, wie gut das PBRS abschneidet. <br><br>  Glauben Sie niemandem, der mich dazu bringt zu sagen, dass es einfach ist, einen gut konvergierenden RL-Agenten zu erstellen, da dies eine Lüge ist.  Jede neue Komponente, die dem System hinzugefügt wird, macht es möglicherweise weniger stabil und erfordert viel Konfiguration und Debugging. <br><br>  Es gibt jedoch eindeutige Hinweise darauf, dass die Lösung des Problems einfach durch Verbesserung der verwendeten Methoden verbessert werden kann (die Daten blieben erhalten).  Es ist eine Tatsache, dass für jede Aufgabe ein bestimmter Bereich von Parametern besser funktioniert als andere.  In diesem Sinne beschreiten Sie einen erfolgreichen Lernpfad. <br><br>  <b>Vielen Dank.</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436628/">https://habr.com/ru/post/de436628/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436618/index.html">Worauf Sie sich 2019 vorbereiten sollten: Programmtrends</a></li>
<li><a href="../de436620/index.html">Ein integrierter Ansatz zur Visualisierung von Sicherheitsereignissen und zur Messung ihrer Wirksamkeit</a></li>
<li><a href="../de436622/index.html">Botovodstvo</a></li>
<li><a href="../de436624/index.html">Studie: Die meisten Nutzer verstehen nicht, wie Facebook mit ihren Daten umgeht.</a></li>
<li><a href="../de436626/index.html">Python wird zur beliebtesten Programmiersprache der Welt.</a></li>
<li><a href="../de436630/index.html">Microservices. Vereinigung und warum es so wichtig ist. Teil 1 - Konfiguration</a></li>
<li><a href="../de436632/index.html">Wie wir ein System zur Verarbeitung, Speicherung und Analyse von Daten in SIBUR aufbauen</a></li>
<li><a href="../de436634/index.html">Fast interne und externe Einstellungen für die Anwendung in Unity3D</a></li>
<li><a href="../de436636/index.html">So erstelle ich einen VKontakte-Community-Empfehlungsdienst</a></li>
<li><a href="../de436638/index.html">Verteilen von Fenstern zwischen Monitoren nach dem Aufwachen aus dem Ruhemodus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>