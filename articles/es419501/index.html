<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë¶ üïµÔ∏è ‚õπüèæ Aprendizaje profundo: reconocimiento de escenas y puntos de referencia en im√°genes üò™ üßü üë®üèø‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬°Es hora de reponer la alcanc√≠a de buenos informes en ruso sobre Machine Learning! ¬°La alcanc√≠a en s√≠ no se repondr√°! 

 Esta vez nos familiarizaremos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje profundo: reconocimiento de escenas y puntos de referencia en im√°genes</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/419501/">  ¬°Es hora de reponer la alcanc√≠a de buenos informes en ruso sobre Machine Learning!  ¬°La alcanc√≠a en s√≠ no se repondr√°! <br><br>  Esta vez nos familiarizaremos con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la</a> fascinante historia de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Andrei Boyarov</a> sobre el reconocimiento de escenas.  Andrey es un investigador de visi√≥n por computadora que se dedica a la visi√≥n artificial en Mail.Ru Group. <br><br>  El reconocimiento de escenas es una de las √°reas ampliamente utilizadas de la visi√≥n artificial.  Esta tarea es m√°s complicada que el reconocimiento estudiado de objetos: la escena es un concepto m√°s complejo y menos formalizado, es m√°s dif√≠cil distinguir las caracter√≠sticas.  La tarea de reconocer vistas se deriva del reconocimiento de escenas: debe resaltar los lugares conocidos en la foto, asegurando un bajo nivel de falsos positivos. <br><br>  Estos son <b>30 minutos de</b> video de la conferencia Smart Data 2017. El video es conveniente para ver en casa y mientras viaja.  Para aquellos que no est√°n listos para sentarse tanto en la pantalla, o que prefieren percibir la informaci√≥n en forma de texto, aplicamos un descifrado de texto completo, dise√±ado en forma de habrosta. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dL1-OrjtMvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Hago visi√≥n artificial en Mail.ru.  Hoy hablar√© sobre c√≥mo usamos el aprendizaje profundo para reconocer im√°genes de escenas y atracciones. <br><br>  La compa√±√≠a surgi√≥ la necesidad de etiquetar y buscar por im√°genes de los usuarios, y para esto decidimos hacer nuestra propia API de Computer Vision, parte de la cual ser√° una herramienta de etiquetado de escenas.  Como resultado de esta herramienta, queremos obtener algo como lo que se muestra en la imagen a continuaci√≥n: el usuario realiza una solicitud, por ejemplo, "catedral", y recibe todas sus fotos con catedrales. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d18/5f8/a3b/d185f8a3bd0c73280cdae408fe84cca1.png"><br><br>  En Computer Vision-community, el tema del reconocimiento de objetos en im√°genes se ha estudiado bastante bien.  Existe un conocido <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">concurso de ImageNet</a> que se lleva a cabo durante varios a√±os y cuya parte principal es el reconocimiento de objetos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/989/f18/b8a989f189970cabb0de00be7ff48afa.png"><br><br>  B√°sicamente necesitamos localizar alg√∫n objeto y clasificarlo.  Con las escenas, la tarea es algo m√°s complicada, porque la escena es un objeto m√°s complejo, consiste en una gran cantidad de otros objetos y el contexto que los une, por lo que las tareas son diferentes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/be7/a6b/ba4be7a6bc9a4e0c2c7929644540799c.png"><br><br>  En Internet hay servicios disponibles de otras compa√±√≠as que implementan dicha funcionalidad.  En particular, esta es la API de Google Vision o la API de Microsoft Computer Vision, que puede encontrar escenas en im√°genes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/758/fd9/095/758fd909536fd930db97c025ca61ef38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/b28/6df/eeeb286df22a31809f507da7730b7d89.png"><br><br>  Resolvimos este problema con la ayuda del aprendizaje autom√°tico, por lo que para esto necesitamos datos.  Hay dos bases principales para el reconocimiento de escenas en acceso abierto ahora.  El primero de ellos apareci√≥ en 2013: esta es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la base de SUN</a> de la Universidad de Princeton.  Esta base consta de cientos de miles de im√°genes y 397 clases. <br><br>  La segunda base en la que entrenamos es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la base Places2</a> del MIT.  Ella apareci√≥ en 2013 en dos versiones.  El primero es Places2-Standart, una base m√°s equilibrada con 1,8 millones de im√°genes y 365 clases.  La segunda opci√≥n, Places2-Challenge, contiene ocho millones de im√°genes y 365 clases, pero el n√∫mero de im√°genes entre clases no est√° equilibrado.  En el concurso ImageNet 2016, la secci√≥n de Reconocimiento de escena incluy√≥ el Places2-Challenge, y el ganador mostr√≥ el mejor resultado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">error de clasificaci√≥n Top-5</a> de aproximadamente el 9%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a10/ea2/881/a10ea2881a5248449dce8202ceaa28ff.png"><br><br>  Entrenamos sobre la base de Places2.  Aqu√≠ hay una imagen de ejemplo desde all√≠: es un ca√±√≥n, pista, cocina, campo de f√∫tbol.  Estos son objetos complejos completamente diferentes en los que debemos aprender a reconocer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cf/9b2/3ee/1cf9b23eedff87ae9bcbb2d8c4090551.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/602/617/1d1/6026171d19b38452ce0e4a071ba4e836.png"><br><br>  Antes de estudiar, adaptamos las bases que tenemos para satisfacer nuestras necesidades.  Hay un truco para el reconocimiento de objetos al experimentar con modelos en peque√±as bases CIFAR-10 y CIFAR-100 en lugar de ImageNet, y solo entonces los mejores entrenan en ImageNet. <br><br>  Decidimos seguir el mismo camino, tomamos la base de datos SUN, la redujimos, obtuvimos 89 clases, 50 mil im√°genes en el tren y 10 mil im√°genes en la validaci√≥n.  Como resultado, antes de entrenar en Places2, configuramos experimentos y probamos nuestros modelos basados ‚Äã‚Äãen SUN.  El entrenamiento en √©l lleva solo 6-10 horas, a diferencia de varios d√≠as en Places2, lo que permiti√≥ realizar muchos m√°s experimentos y hacerlo m√°s efectivo. <br><br>  Tambi√©n miramos la base de datos de Places2 y nos dimos cuenta de que no necesit√°bamos algunas clases.  Ya sea por consideraciones de producci√≥n, o porque hay muy pocos datos sobre ellos, eliminamos clases como, por ejemplo, un acueducto, una casa en el √°rbol, una puerta de granero. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f8/31f/60f/4f831f60fd70ec7f74ea2bb0a29c69f5.png"><br><br>  Como resultado, despu√©s de todas las manipulaciones, obtuvimos la base de datos Places2, que contiene 314 clases y medio mill√≥n de im√°genes (en su versi√≥n est√°ndar), en la versi√≥n Challenge, alrededor de 7,5 millones de im√°genes.  Construimos capacitaci√≥n sobre estas bases. <br><br>  Adem√°s, al ver las clases restantes, descubrimos que hay demasiadas para producci√≥n, son demasiado detalladas.  Y para esto, aplicamos el mecanismo de mapeo de escenas cuando algunas clases se combinan en una com√∫n.  Por ejemplo, conectamos todo lo relacionado con los bosques en un bosque, todo lo relacionado con los hospitales, en un hospital, con hoteles, en un hotel. <br><br>  Utilizamos el mapeo de escenas solo para pruebas y para el usuario final, porque es m√°s conveniente.  En el entrenamiento, utilizamos todas las clases 314 est√°ndar.  Llamamos a la base resultante Lugares Sift. <br><br><h2>  Enfoques, soluciones </h2><br>  Ahora considere los enfoques que usamos para resolver este problema.  En realidad, tales tareas est√°n conectadas con el enfoque cl√°sico: redes neuronales convolucionales profundas. <br><br>  La imagen a continuaci√≥n muestra una de las primeras redes cl√°sicas, pero ya contiene los principales bloques de construcci√≥n que se utilizan en las redes modernas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a4/61f/7f7/0a461f7f7ae8a9264f5fadb69271f29d.png"><br><br>  Estas son capas convolucionales, estas son capas de arrastre, capas completamente conectadas.  Para determinar la arquitectura, verificamos los mejores resultados de los concursos ImageNet y Places2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7dc/658/7d0/7dc6587d046a968988d4f52fa7bcbb3f.png"><br><br>  Podemos decir que las principales arquitecturas principales se pueden dividir en dos familias: Inception y la familia ResNet (red residual).  En el curso de los experimentos, descubrimos que la familia ResNet es m√°s adecuada para nuestra tarea, y realizamos el siguiente experimento en esta familia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/828/85b/1a882885b70533dd0341bd853d2818fd.png"><br><br>  ResNet es una red profunda que consta de una gran cantidad de bloques residuales.  Este es su bloque de construcci√≥n principal, que consta de varias capas con pesos y conexi√≥n de acceso directo.  Como resultado de este dise√±o, esta unidad aprende cu√°nto difiere la se√±al de entrada x de la salida f (x).  Como resultado, podemos construir redes de tales bloques, y durante el entrenamiento, la red en las √∫ltimas capas puede hacer pesos cercanos a cero. <br><br>  Por lo tanto, podemos decir que la red misma decide qu√© tan profunda debe ser para resolver algunas de las tareas.  Gracias a esta arquitectura, es posible construir redes de gran profundidad con una gran cantidad de capas.  El ganador de ImageNet 2014 conten√≠a solo 22 capas, ResNet super√≥ este resultado y ya conten√≠a 152 capas. <br><br>  La investigaci√≥n central de ResNet es mejorar y construir adecuadamente un bloque residual.  La imagen a continuaci√≥n muestra una versi√≥n emp√≠rica y matem√°ticamente s√≥lida que brinda el mejor resultado.  Tal construcci√≥n del bloque le permite lidiar con uno de los problemas fundamentales del aprendizaje profundo: un gradiente que se desvanece. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/7a4/398/0207a439827163bc14895a47a1489ddc.png"><br><br>  Para entrenar nuestras redes, utilizamos el marco Torch escrito en Lua debido a su flexibilidad y velocidad, y para ResNet bifurcamos la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">implementaci√≥n de ResNet desde Facebook</a> .  Para validar la calidad de la red, utilizamos tres pruebas. <br><br>  La primera prueba val de Places es la validaci√≥n de muchos conjuntos de Sift de Places.  La segunda prueba es Tamizar lugares usando el mapeo de escenas, y la tercera es la prueba de Nube m√°s cercana a la situaci√≥n de combate.  Im√°genes de empleados tomadas de la nube y etiquetadas manualmente.  En la imagen de abajo hay dos ejemplos de tales im√°genes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/796/6d0/066/7966d0066c9257617e81f290fbbc6283.png"><br><br>  Comenzamos a medir y entrenar redes, compararlas entre s√≠.  El primero es el punto de referencia ResNet-152, que viene con Places2, el segundo es ResNet-50, que capacitamos en ImageNet y lo capacitamos en nuestra base, el resultado ya fue mejor.  Luego tomaron ResNet-200, tambi√©n capacitado en ImageNet, y al final mostr√≥ el mejor resultado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d9/0de/320/6d90de3208968f8120c1f4be1e79f74e.png"><br><br>  A continuaci√≥n hay ejemplos de trabajo.  Este es un punto de referencia ResNet-152.  Se pronostican las etiquetas originales que entrega la red.  Las etiquetas mapeadas son las etiquetas que vinieron despu√©s del mapeo de escenas.  Se puede ver que el resultado no es muy bueno.  Es decir, ella parece estar dando algo sobre el caso, pero no muy bien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/892/ceb/86e892ceb75b85dcf22bec203c0f4f3d.png"><br><br>  El siguiente ejemplo es la operaci√≥n de ResNet-200.  Ya muy adecuado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/621/d0d/fa6621d0dd29ac9d77da10378eec9454.png"><br><br><h2>  Mejora de ResNet </h2><br>  Decidimos tratar de mejorar nuestra red, y al principio solo intentamos aumentar la profundidad de la red, pero despu√©s de eso se hizo mucho m√°s dif√≠cil entrenar.  Este es un problema conocido, el a√±o pasado se publicaron varios art√≠culos sobre este tema, que dicen que ResNet, de hecho, es un conjunto de una gran cantidad de redes ordinarias de varias profundidades. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df2/0ad/023/df20ad0234f5eba5597939468f8afaf6.png"><br><br>  Los bloques res, que se encuentran al final de la cuadr√≠cula, hacen una peque√±a contribuci√≥n a la formaci√≥n del resultado final.  Parece m√°s prometedor aumentar no la profundidad de la red, sino su ancho, es decir, la cantidad de filtros dentro del bloque Res. <br><br>  Esta idea es implementada por Wide Residual Network, que apareci√≥ en 2016.  Terminamos usando WRN-50-2, que es el ResNet-50 habitual con el doble de filtros en la convoluci√≥n 3x3 del cuello de botella interno. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5be/f29/aed/5bef29aed5cfce3b0121ce69ee072c35.png"><br><br>  La red muestra en ImageNet resultados similares con el ResNet-200, que ya hemos usado, pero, lo que es m√°s importante, es casi el doble de r√°pido.  Aqu√≠ hay dos implementaciones del bloque Residual en Torch; el par√°metro que se duplica se resalta de manera brillante.  Este es el n√∫mero de filtros en la convoluci√≥n interna. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1a/3fe/d24/f1a3fed24bf735ed30cfc407aac6ca79.png"><br><br>  Estas son mediciones en las pruebas de ResNet-200 ImageNet.  Al principio tomamos WRN-22-6, mostr√≥ un resultado peor.  Luego tomaron WRN-50-2-ImageNet, lo entrenaron, tomaron WRN-50-2, entrenaron en ImageNet y lo entrenaron en Places2-challenge, y mostr√≥ el mejor resultado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/736/fd6/afe/736fd6afe39702160b87ad095ef0edcd.png"><br><br>  Aqu√≠ hay un ejemplo del WRN-50-2: un resultado bastante adecuado en nuestras im√°genes que ya ha visto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0a/442/e1f/b0a442e1f87a4f2e2a6c43a96677ce3d.png"><br><br>  Y este es un ejemplo de trabajo en fotograf√≠as de combate, tambi√©n con √©xito. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/715/6bf/9247156bf459bd5b005b4180d36a1da5.png"><br><br>  Hay, por supuesto, trabajos no muy exitosos.  El puente de Alejandro III en Par√≠s no fue reconocido como un puente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dd/c66/9ec/3ddc669ecf4aabe61b103dafbc4aabb2.png"><br><br><h2>  Mejora modelo </h2><br>  Pensamos en c√≥mo mejorar este modelo.  La familia ResNet contin√∫a mejorando, con nuevos art√≠culos que salen.  En particular, en 2016 se public√≥ un art√≠culo interesante PyramidNet, que mostr√≥ resultados prometedores en CIFAR-10/100 e ImageNet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdd/816/e1b/fdd816e1b970be4465ff8200b7f0cb56.png"><br><br>  La idea no es aumentar bruscamente el ancho del bloque residual, sino hacerlo gradualmente.  Entrenamos varias opciones para esta red, pero, desafortunadamente, mostr√≥ resultados ligeramente peores que nuestro modelo de combate. <br><br>  En la primavera de 2018, se lanz√≥ el modelo ResNext, tambi√©n una idea prometedora: dividir el bloque residual en varios bloques paralelos de menor tama√±o y menor ancho.  Esto es similar a la idea de Inception, tambi√©n experimentamos con ella.  Pero, desafortunadamente, ella mostr√≥ peores resultados que nuestro modelo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/3c4/e0f/7123c4e0f7f826f9dab8a1791d4eb3cd.png"><br><br>  Tambi√©n experimentamos con varios enfoques "creativos" para mejorar nuestros modelos.  En particular, intentamos usar el mapeo de activaci√≥n de clase (CAM), es decir, estos son los objetos que la red mira cuando clasifica la imagen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bf/639/ea4/1bf639ea430bd03bafeeacf8acb29a09.png"><br><br>  Nuestra idea era que las instancias de la misma escena deber√≠an tener los mismos o similares objetos que una clase CAM.  Intentamos usar este enfoque.  Al principio tomaron dos redes.  Uno est√° capacitado en ImageNet, el segundo es nuestro modelo, que queremos mejorar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47d/ac5/538/47dac55383f098852c9a69a3d50f2cc1.png"><br><br>  Tomamos la imagen, corremos a trav√©s de la red 2, agregamos el CAM para la capa, luego lo alimentamos a la entrada de la red 1. Corre a trav√©s de la red 1, agregamos los resultados a la funci√≥n de p√©rdida de la red 2, continuamos con las nuevas funciones de p√©rdida. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/c3e/9c3/860c3e9c37f6d6ffd91d3cdd36d67682.png"><br><br>  La segunda opci√≥n es que ejecutamos la imagen a trav√©s de la red 2, tomamos el CAM, lo alimentamos a la entrada de la red 1, y luego con estos datos simplemente entrenamos la red 1 y usamos el conjunto de los resultados de la red 1 y la red 2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/140/95b/5fb14095b117b05582a79a43ee0eadb6.png"><br><br>  Volvimos a entrenar nuestro modelo en WRN-50-2, como la red 1 usamos ResNet-50 ImageNet, pero no fue posible aumentar significativamente la calidad de nuestro modelo. <br><br>  Pero continuamos investigando sobre c√≥mo mejorar nuestros resultados: estamos entrenando nuevas arquitecturas de CNN, en particular la familia ResNet.  Intentamos experimentar con CAM y considerar varios enfoques con un procesamiento m√°s inteligente de los parches de imagen; nos parece que este enfoque es bastante prometedor. <br><br><h2>  Reconocimiento de hito </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/11b/9da/4c3/11b9da4c376bcee091afb70d3dbb0745.png"><br><br>  Tenemos un buen modelo para reconocer escenas, pero ahora queremos descubrir algunos lugares emblem√°ticos, es decir, lugares de inter√©s.  Adem√°s, los usuarios a menudo les toman fotos o las toman en su contexto. <br><br>  Queremos que el resultado no sea solo las catedrales, como en la imagen de la diapositiva, sino el sistema que diga: "Hay Notre Dame de Paris y las catedrales en Praga". <br><br>  Cuando resolvimos este problema, encontramos algunas dificultades. <br><br><ol><li>  Pr√°cticamente no hay estudios sobre este tema y no hay datos preparados en el dominio p√∫blico. <br></li><li>  Una peque√±a cantidad de im√°genes "limpias" en el dominio p√∫blico para cada atracci√≥n. <br></li><li>  No est√° del todo claro qu√© es un hito de los edificios.  Por ejemplo, una casa con torres en Sq.  Leo Tolstoi en Petersburgo, TripAdvisor no considera atracciones, pero Google considera. <br></li></ol><br>  Comenzamos recopilando una base de datos, compilamos una lista de 100 ciudades y luego utilizamos la API de Google Places para descargar datos JSON para los puntos de inter√©s de estas ciudades. <br><br>  Los datos se filtraron y analizaron, y de acuerdo con la lista, descargamos 20 im√°genes de la B√∫squeda de Google para cada atracci√≥n.  El n√∫mero 20 est√° tomado de consideraciones emp√≠ricas.  Como resultado, obtuvimos una base de 2827 atracciones y alrededor de 56 mil im√°genes.  Esta es la base sobre la que formamos nuestro modelo.  Para validar nuestro modelo, utilizamos dos pruebas. <br><br>  Prueba en la nube: estas son im√°genes de nuestros empleados, etiquetadas manualmente.  Contiene 200 im√°genes en 15 ciudades y 10 mil im√°genes sin atracciones.  El segundo es la prueba de b√∫squeda.  Fue construido usando la b√∫squeda Mail.ru, que contiene de 3 a 10 im√°genes para cada atracci√≥n, pero, desafortunadamente, esta prueba est√° sucia. <br><br>  Entrenamos a los primeros modelos, pero mostraron resultados pobres en la prueba de Cloud en fotos de combate. <br><br>  Aqu√≠ hay un ejemplo de la imagen en la que fuimos entrenados, y un ejemplo de fotograf√≠a de combate.  El problema en las personas es que a menudo son fotografiadas en el contexto de las vistas.  En esas im√°genes que obtuvimos de la b√∫squeda, no hab√≠a personas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1d/8a6/63b/b1d8a663bf52f5910cf35415a93c296a.png"><br><br>  Para combatir esto, agregamos aumento "humano" durante el entrenamiento.  Es decir, utilizamos enfoques est√°ndar: rotaciones aleatorias, corte aleatorio de parte de la imagen, etc.  Pero tambi√©n en el proceso de aprendizaje, agregamos personas al azar a algunas im√°genes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12c/6c7/b54/12c6c7b54ae4d64065ee7fefa18acb72.png"><br><br>  Este enfoque nos ayud√≥ a resolver el problema con las personas y a obtener un modelo de calidad aceptable. <br><br><h2>  Modelos de escena de ajuste fino </h2><br>  C√≥mo entrenamos el modelo: hay una base de entrenamiento, pero es bastante peque√±a.  Pero sabemos que una atracci√≥n tur√≠stica es un caso especial de la escena.  Y tenemos un modelo de escena bastante bueno.  Decidimos entrenarla para las vistas.  Para hacer esto, agregamos varias capas BN completamente conectadas en la parte superior de la red, los capacitamos a ellos y a los tres bloques residuales principales.  El resto de la red estaba congelada. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5e/b4a/e80/a5eb4ae80781b95eec46821dd05d5cba.png"><br><br>  Adem√°s, para el entrenamiento, utilizamos la funci√≥n de p√©rdida central no est√°ndar.  Durante el entrenamiento, la p√©rdida del Centro intenta "separar" representantes de diferentes clases en diferentes grupos, como se muestra en la imagen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/2e8/bbc/8cc2e8bbc85f79dfb6301742d7c3d4ac.png"><br><br>  En el entrenamiento, agregamos otra clase "no una atracci√≥n tur√≠stica".  Y la p√©rdida de centro no se aplic√≥ a esta clase.  En tal funci√≥n de p√©rdida mixta, se realiz√≥ el entrenamiento. <br><br>  Despu√©s de entrenar la red, cortamos la √∫ltima capa de clasificaci√≥n, y cuando la imagen pasa a trav√©s de la red, se convierte en un vector num√©rico llamado incrustaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf2/ebc/0fd/cf2ebc0fd7dc3af7770e6dd2c616fc74.png"><br><br>  Para construir a√∫n m√°s un sistema de reconocimiento de puntos de referencia, creamos vectores de referencia para cada clase.  Tomamos cada clase de atracciones de la multitud y pasamos las im√°genes a trav√©s de la red.  Consiguieron incrustaciones y tomaron su vector medio, que se llamaba vector de referencia de clase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e9/8ef/f32/5e98eff32e669f6c3c0b7245c25bc4ea.png"><br><br>  Para determinar las vistas en la foto, ejecutamos la imagen de entrada a trav√©s de la red, y su incrustaci√≥n se compara con el vector de referencia de cada clase.  Si el resultado de la comparaci√≥n es menor que el umbral, entonces creemos que no hay ning√∫n punto de inter√©s en la imagen.  De lo contrario, tomamos la clase con el valor de comparaci√≥n m√°s alto. <br><br><h2>  Resultados de la prueba </h2><br><ul><li>  En la prueba de la nube, la precisi√≥n de las vistas fue de 0.616, no de las vistas: 0.981 </li><li>  La precisi√≥n promedio de 0.669 se obtuvo en la prueba de b√∫squeda, y la completitud promedio fue de 0.576. </li></ul><br>  En B√∫squeda, no obtuvieron muy buenos resultados, pero esto se explica por el hecho de que el primero es bastante "sucio" y el segundo tiene caracter√≠sticas: entre las atracciones hay diferentes jardines bot√°nicos que son similares en todas las ciudades. <br><br>  Hubo una idea para el reconocimiento de escenas para entrenar primero la red, que determinar√° la m√°scara de escena, es decir, eliminar√° los objetos del primer plano y luego la introducir√° en el modelo mismo, que reconoce escenas de im√°genes sin estas √°reas, donde el fondo est√° obstruido.  Pero no est√° muy claro qu√© es exactamente lo que se debe eliminar de la capa frontal, qu√© m√°scara se necesita. <br><br>  Ser√° algo bastante complicado e inteligente, porque no todos entienden qu√© objetos pertenecen a la escena y cu√°les son superfluos.  Por ejemplo, se puede necesitar gente en un restaurante.  Esta es una decisi√≥n no trivial, tratamos de hacer algo similar, pero no dio buenos resultados. <br><br>  Aqu√≠ hay un ejemplo de trabajo en fotograf√≠as de combate. <br><br>  Ejemplos de trabajo exitoso: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/ca4/413/c53ca44130bc3b9a845a7f21a681a8e0.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/774/c01/642774c0174cb07eeb52c055cb92b8d4.png"><br><br>  Pero el trabajo fallido: no se encontraron lugares de inter√©s.  El principal problema de nuestro modelo en este momento no es que la red confunda las vistas, sino que no las encuentra en la foto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c9/aca/7ab/6c9aca7ab564401b4c9b5cea126fc68a.png"><br><br>  En el futuro, planeamos reunir una base para un n√∫mero a√∫n mayor de ciudades, encontrar nuevos m√©todos para capacitar a la red para esta tarea y determinar las posibilidades de aumentar el n√∫mero de clases sin volver a capacitar a la red. <br><br><h2>  Conclusiones </h2><br>  Hoy nosotros: <br><br><ul><li>  Observamos qu√© conjuntos de datos est√°n disponibles para el reconocimiento de escena; <br></li><li>  Vimos que la Red Residual Amplia es el mejor modelo; <br></li><li>  Discuti√≥ otras posibilidades para aumentar la calidad de este modelo; <br></li><li>  Examinamos la tarea de reconocer vistas, qu√© dificultades surgen; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Describimos el algoritmo para recopilar la base y los m√©todos de ense√±anza del modelo para reconocer atracciones. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Puedo decir que las tareas son interesantes, pero poco estudiadas en la comunidad. </font><font style="vertical-align: inherit;">Es interesante tratar con ellos, porque puede aplicar enfoques no est√°ndar que no se aplican en el reconocimiento habitual de objetos.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minuto de publicidad. </font><font style="vertical-align: inherit;">Si le gust√≥ este informe de la conferencia SmartData, tenga en cuenta que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></b></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> se llevar√° a cabo en San Petersburgo el 15 de octubre, una </font><font style="vertical-align: inherit;">conferencia para aquellos que est√°n inmersos en el mundo del aprendizaje autom√°tico, el an√°lisis y el procesamiento de datos. </font><font style="vertical-align: inherit;">El programa tendr√° muchas cosas interesantes, el sitio ya tiene sus primeros oradores e informes.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es419501/">https://habr.com/ru/post/es419501/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es419483/index.html">PowerShell y Shift + Ins, o c√≥mo ganar velocidad de Hermes cuando se trabaja con GPP</a></li>
<li><a href="../es419491/index.html">C√≥mo funciona STP</a></li>
<li><a href="../es419493/index.html">¬øPor qu√© necesitas Splunk? An√°lisis de eventos de seguridad</a></li>
<li><a href="../es419495/index.html">Qui√©n "invent√≥" la conducci√≥n √≥sea, por qu√© se usa y qu√© tan seguro es para escuchar</a></li>
<li><a href="../es419497/index.html">Revisi√≥n de impresora grande 3D Hercules Strong</a></li>
<li><a href="../es419503/index.html">El libro "Algoritmos y estructuras de datos. Recuperaci√≥n de informaci√≥n de Java ¬ª</a></li>
<li><a href="../es419507/index.html">Descripci√≥n general de la impresora rusa 3D PICASO 3D Designer X de 3Dtool</a></li>
<li><a href="../es419509/index.html">Red neuronal artificial fot√≥nica</a></li>
<li><a href="../es419511/index.html">tipo de (T) vs. TypeOf‚ü®T‚ü©</a></li>
<li><a href="../es419513/index.html">Configurar la pol√≠tica de seguridad de contrase√±a en Zimbra</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>