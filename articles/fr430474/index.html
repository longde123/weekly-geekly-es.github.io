<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëµ üçö üîí CephFS vs GlusterFS üë©üèª‚Äçüíº üë∂üèø üåà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En tant qu'ing√©nieur d'infrastructure dans l'√©quipe de d√©veloppement de la plateforme cloud , j'ai eu l'opportunit√© de travailler avec de nombreux sys...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CephFS vs GlusterFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/430474/"><p>  En tant qu'ing√©nieur d'infrastructure dans l'√©quipe de d√©veloppement de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plateforme cloud</a> , j'ai eu l'opportunit√© de travailler avec de nombreux syst√®mes de stockage distribu√©s, y compris ceux qui sont indiqu√©s dans l'en-t√™te.  Il semble qu'il y ait une compr√©hension de leurs forces et de leurs faiblesses, et je vais essayer de partager mes r√©flexions avec vous √† ce sujet.  Pour ainsi dire, voyons qui a la fonction de hachage plus longtemps. </p><br><p><img src="https://habrastorage.org/webt/fb/qn/vj/fbqnvjedf1ujf2hknyuxqbv82qy.png"></p><a name="habracut"></a><br><p>  Avertissement: Plus t√¥t dans ce blog, vous pouvez voir des articles sur GlusterFS.  Je n'ai rien √† voir avec ces articles.  Il s'agit du blog de l'auteur de l'√©quipe de projet de notre cloud et chacun de ses membres peut raconter son histoire.  L'auteur de ces articles est ing√©nieur de notre groupe op√©rationnel et il a ses propres t√¢ches et son exp√©rience, qu'il a partag√©es.  Veuillez en tenir compte si vous constatez soudainement une diff√©rence d'opinion.  J'en profite pour transmettre mes salutations √† l'auteur de ces articles! </p><br><h2 id="o-chem-poydet-rech">  Ce qui sera discut√© </h2><br><p> Parlons des syst√®mes de fichiers qui peuvent √™tre construits sur la base de GlusterFS et CephFS.  Nous allons discuter de l'architecture de ces deux syst√®mes, les regarder sous des angles diff√©rents, et √† la fin je risquerai m√™me de tirer des conclusions.  Les autres fonctionnalit√©s de Ceph, telles que RBD et RGW, ne seront pas affect√©es. </p><br><h2 id="terminologiya">  Terminologie </h2><br><p>  Pour rendre l'article complet et compr√©hensible pour tout le monde, regardons la terminologie de base des deux syst√®mes: </p><br><p>  Terminologie Ceph: </p><br><p>  <strong>RADOS</strong> (Reliable Autonomic Distributed Object Store) est un stockage d'objets autonome, qui est la base du projet Ceph. <br>  <strong>CephFS</strong> , <strong>RBD</strong> (RADOS Block Device), <strong>RGW</strong> (RADOS Gateway) sont des gadgets de haut niveau pour RADOS qui fournissent aux utilisateurs finaux diverses interfaces avec RADOS. <br>  Plus pr√©cis√©ment, CephFS fournit une interface de syst√®me de fichiers compatible POSIX.  En fait, les donn√©es CephFS sont stock√©es dans RADOS. <br>  <strong>OSD</strong> (Object Storage Daemon) est un processus desservant un stockage disque / objet s√©par√© dans un cluster RADOS. <br>  <strong>RADOS Pool</strong> (pool) - plusieurs <strong>OSD</strong> unis par un ensemble commun de r√®gles, comme, par exemple, une politique de r√©plication.  Du point de vue de la hi√©rarchie des donn√©es, un pool est un r√©pertoire ou un espace de noms s√©par√© (plat, sans sous-r√©pertoires) pour les objets. <br>  <strong>PG</strong> (Placement Group) - Je pr√©senterai le concept de PG un peu plus tard, dans le contexte, pour une meilleure compr√©hension. </p><br><p>  RADOS √©tant la base sur laquelle CephFS est construit, j'en parlerai souvent et cela s'appliquera automatiquement √† CephFS. </p><br><p>  Terminologie de GlusterFS (ci-apr√®s gl): </p><br><p>  <strong>La brique</strong> est un processus desservant un seul disque, un analogue de l'OSD dans la terminologie RADOS. <br>  <strong>Volume</strong> - volume dans lequel les briques sont unies.  Tom est un analogue de pool dans RADOS, il a √©galement une topologie de r√©plication sp√©cifique entre les briques. </p><br><h2 id="raspredelenie-dannyh">  Distribution des donn√©es </h2><br><p>  Pour le rendre plus clair, consid√©rons un exemple simple qui peut √™tre impl√©ment√© par les deux syst√®mes. </p><br><p>  La configuration √† utiliser comme exemple: </p><br><ul><li>  2 serveurs (S1, S2) avec 3 disques de volume √©gal (sda, sdb, sdc) dans chacun; </li><li>  volume / pool avec r√©plication 2. </li></ul><br><p>  Les deux syst√®mes n√©cessitent au moins 3 serveurs pour un fonctionnement normal.  Mais nous fermons les yeux sur ce point, car ce n'est qu'un exemple pour un article. </p><br><p>  Dans le cas de gl, ce sera un volume <strong>distribu√©-r√©pliqu√©</strong> compos√© de 3 groupes de r√©plication: </p><br><p><img src="https://habrastorage.org/webt/ai/k_/pg/aik_pgd6mwqy1wlfyhjx-mdsf6u.png"></p><br><p>  Chaque groupe de r√©plication est compos√© de deux briques sur des serveurs diff√©rents. <br>  En fait, il s'av√®re que le volume combine les trois RAID-1. <br>  Lorsque vous le montez, obtenez le syst√®me de fichiers souhait√© et commencez √† y √©crire des fichiers, vous constaterez que chaque fichier que vous √©crivez appartient √† l'un de ces groupes de r√©plication dans son ensemble. <br>  La distribution des fichiers entre ces groupes distribu√©s se fait par <strong>DHT</strong> (Distributed Hash Tables), qui est essentiellement une fonction de hachage (nous y reviendrons plus tard). </p><br><p>  Sur le "diagramme", cela ressemblera √† ceci: </p><br><p><img src="https://habrastorage.org/webt/d1/-8/uk/d1-8ukcptl3owiyuqw11v0plxqw.png"></p><br><p>  Comme si les premi√®res caract√©ristiques architecturales √©taient d√©j√† manifest√©es: </p><br><ul><li>  placer en groupes est in√©galement √©limin√©, cela d√©pend de la taille des fichiers; </li><li>  lors de l'√©criture d'un fichier, IO va √† un seul groupe, les autres sont inactifs; </li><li>  Vous ne pouvez pas obtenir l'IO de l'int√©gralit√© du volume lors de l'√©criture d'un seul fichier; </li><li>  s'il n'y a pas assez d'espace dans le groupe pour √©crire le fichier, vous obtiendrez une erreur, le fichier ne sera pas √©crit et ne sera pas redistribu√© √† un autre groupe. </li></ul><br><p>  Si vous utilisez d'autres types de volumes, par exemple, Distributed-Striped-Replicated ou m√™me Dispersed (Erasure Coding), alors seule la m√©canique de distribution des donn√©es au sein d'un groupe changera fondamentalement.  DHT d√©composera √©galement les fichiers enti√®rement dans ces groupes, et √† la fin nous aurons tous les m√™mes probl√®mes.  Oui, si le volume se compose d'un seul groupe ou si vous avez tous les fichiers de la m√™me taille, il n'y aura pas de probl√®me.  Mais nous parlons de syst√®mes normaux, sous des centaines de t√©raoctets de donn√©es, y compris des fichiers de diff√©rentes tailles, nous pensons donc qu'il y a un probl√®me. </p><br><p>  Voyons maintenant CephFS.  La RADOS mentionn√©e ci-dessus entre en sc√®ne.  Dans RADOS, chaque disque est servi par un processus distinct - OSD.  Sur la base de notre configuration, nous n'en obtenons que 6, 3 sur chaque serveur.  Ensuite, nous devons cr√©er un pool pour les donn√©es et d√©finir le nombre de PG et le facteur de r√©plication des donn√©es dans ce pool - dans notre cas 2. <br>  Disons que nous avons cr√©√© un pool avec 8 PG.  Ces PG seront distribu√©s √† peu pr√®s √©galement sur l'OSD: </p><br><p><img src="https://habrastorage.org/webt/cn/ea/hs/cneahsczaws7dzuqtu1syubb4cy.png"></p><br><p>  Il est temps de pr√©ciser que PG est un groupe logique qui combine un certain nombre d'objets.  Depuis que nous avons d√©fini le fait de r√©plication 2, chaque PG a une r√©plique sur un autre OSD sur un autre serveur (par d√©faut).  Par exemple, PG1, qui est sur OSD-1 sur le serveur S1, a un jumeau sur S2 sur OSD-6.  Dans chaque paire de PG (ou triple, si la r√©plication 3) est PRIMARY PG, qui est en cours d'enregistrement.  Par exemple, PRIMARY for PG4 est sur S1, mais PRIMARY for PG3 est sur S2. </p><br><p>  Maintenant que vous savez comment fonctionne RADOS, nous pouvons passer √† l'√©criture de fichiers dans notre tout nouveau pool.  Bien que RADOS soit un stockage √† part enti√®re, il n'est pas possible de le monter en tant que syst√®me de fichiers ou de l'utiliser comme p√©riph√©rique de bloc.  Pour y √©crire directement des donn√©es, vous devez utiliser un utilitaire ou une biblioth√®que sp√©ciale. </p><br><p>  Nous √©crivons les trois m√™mes fichiers que dans l'exemple ci-dessus: </p><br><p><img src="https://habrastorage.org/webt/ut/0z/zd/ut0zzd20fmocwke70q9rj-0snog.png"></p><br><p>  Dans le cas de RADOS, tout est devenu en quelque sorte plus compliqu√©, d'accord. </p><br><p>  Puis CRUSH (r√©plication contr√¥l√©e sous hachage √©volutif) est apparu dans la cha√Æne.  CRUSH est l'algorithme sur lequel repose RADOS (nous y reviendrons plus tard).  Dans ce cas particulier, en utilisant cet algorithme, il est d√©termin√© o√π le fichier doit √™tre √©crit dans quelle PG.  Ici CRUSH remplit la m√™me fonction que DHT dans gl.  √Ä la suite de cette distribution pseudo-al√©atoire de fichiers sur PG, nous avons eu les m√™mes probl√®mes que gl, uniquement sur un sch√©ma plus complexe. </p><br><p>  Mais j'ai d√©lib√©r√©ment gard√© le silence sur un point important.  Presque personne n'utilise RADOS sous sa forme pure.  Pour un travail pratique avec RADOS, les couches suivantes ont √©t√© d√©velopp√©es: RBD, CephFS, RGW, que j'ai d√©j√† mentionn√©. </p><br><p>  Tous ces traducteurs (clients RADOS) fournissent une interface client diff√©rente, mais ils sont similaires dans leur travail avec RADOS.  La similitude la plus importante est que toutes les donn√©es qui les traversent sont coup√©es en morceaux et plac√©es dans RADOS en tant qu'objets RADOS s√©par√©s.  Par d√©faut, les clients officiels coupent le flux d'entr√©e en morceaux de 4 Mo.  Pour RBD, la taille de bande peut √™tre d√©finie lors de la cr√©ation du volume.  Dans le cas de CephFS, il s'agit de l'attribut (xattr) du fichier et peut √™tre g√©r√© au niveau des fichiers individuels ou pour tous les fichiers de catalogue.  Eh bien, RGW a √©galement un param√®tre correspondant. </p><br><p>  Supposons maintenant que nous empilions CephFS au-dessus du pool RADOS pr√©sent√© dans l'exemple pr√©c√©dent.  D√©sormais, les syst√®mes en question sont sur un pied d'√©galit√© et offrent une interface d'acc√®s aux fichiers identique. </p><br><p>  Si nous r√©√©crivons nos fichiers de test sur le tout nouveau CephFS, nous trouverons une distribution des donn√©es compl√®tement diff√©rente et presque uniforme sur l'OSD.  Par exemple, le fichier 2 de 2 Go sera divis√© en 512 morceaux, qui seront r√©partis sur diff√©rentes PG et, par cons√©quent, sur diff√©rents OSD presque uniform√©ment, et cela r√©sout pratiquement les probl√®mes de distribution de donn√©es d√©crits ci-dessus. </p><br><p>  Dans notre exemple, seulement 8 PG sont utilis√©s, bien qu'il soit recommand√© d'avoir ~ 100 PG sur un OSD.  Et vous avez besoin de 2 pools pour que CephFS fonctionne. Vous avez √©galement besoin de quelques d√©mons de service pour que RADOS fonctionne en principe.  Ne pensez pas que tout est si simple, j'en omet sp√©cifiquement beaucoup, pour ne pas sortir de l'essence. </p><br><p>  Alors maintenant, CephFS semble plus int√©ressant, non?  Mais je n'ai pas mentionn√© un autre point important, cette fois √† propos de gl.  Gl a √©galement un m√©canisme pour couper des fichiers en morceaux et ex√©cuter ces morceaux via DHT.  Le soi-disant sharding (Sharding). </p><br><p>  Cinq minutes d'histoire </p><br><blockquote>  Le 21 avril 2016, l'√©quipe de d√©veloppement de Ceph a publi√© "Jewel", la premi√®re version de Ceph dans laquelle CephFS est consid√©r√© comme stable. </blockquote><p>  Ce sont maintenant tous des cris de gauche et de droite √† propos de CephFS!  Et il y a 3-4 ans, l'utiliser serait au moins une d√©cision douteuse.  Nous avons cherch√© d'autres solutions, et gl avec l'architecture d√©crite ci-dessus n'√©tait pas bonne.  Mais nous y croyions plus qu'en CephFS, et attendions le partage, qui se pr√©parait pour la sortie. </p><br><p>  Et ici, c'est le jour X: </p><br><blockquote>  4 juin 2015 - La communaut√© Gluster a annonc√© aujourd'hui la disponibilit√© g√©n√©rale du logiciel de stockage d√©fini par logiciel ouvert GlusterFS 3.7. </blockquote><p>  3.7 - la premi√®re version de gl, dans laquelle le sharding a √©t√© annonc√© comme une opportunit√© exp√©rimentale.  Ils avaient presque un an avant la sortie stable de CephFS afin de prendre pied sur le podium ... </p><br><p>  Donc sharding signifie.  Comme tout dans gl, cela est impl√©ment√© dans un traducteur s√©par√©, qui se tenait au-dessus du DHT (√©galement traducteur) sur la pile.  Puisqu'il est sup√©rieur √† DHT, DHT re√ßoit des fragments pr√™ts √† l'emploi √† l'entr√©e et les distribue parmi les groupes de r√©plication sous forme de fichiers normaux.  Le partage est activ√© au niveau du volume individuel.  La taille du fragment peut √™tre d√©finie, par d√©faut - 4 Mo, comme les lotions Ceph. </p><br><p>  Lorsque j'ai effectu√© les premiers tests, j'√©tais ravi!  J'ai dit √† tout le monde que gl est maintenant la chose la plus importante et maintenant nous allons vivre!  Lorsque le partage est activ√©, l'enregistrement d'un fichier se fait en parall√®le avec diff√©rents groupes de r√©plication.  La d√©compression apr√®s la compression ¬´en √©criture¬ª peut √™tre incr√©mentielle au niveau du fragment.  En pr√©sence de prises de vue en cache ici aussi, tout devient bon et des fragments s√©par√©s sont d√©plac√©s vers le cache, et non les fichiers entiers.  En g√©n√©ral, je me r√©jouissais, car  il semblait qu'il avait entre les mains un instrument tr√®s cool. </p><br><p>  Il restait √† attendre les premi√®res corrections de bugs et le statut de "pr√™t pour la production".  Mais tout s'est av√©r√© moins rose ... Afin de ne pas √©tirer l'article avec une liste de bugs critiques li√©s au sharding, apparaissant de temps en temps dans les prochaines versions, je ne peux que dire que le dernier "probl√®me majeur" avec la description suivante: </p><br><blockquote>  L'extension d'un volume gluster qui est fragment√© peut endommager les fichiers.  Les volumes √©clat√©s sont g√©n√©ralement utilis√©s pour les images de machine virtuelle, si ces volumes sont √©tendus ou √©ventuellement contract√©s (c'est-√†-dire ajouter / supprimer des briques et r√©√©quilibrer), il y a des rapports d'images corrompues. </blockquote><p>  a √©t√© ferm√© dans la version 3.13.2 du 20 janvier 2018 ... ce n'est peut-√™tre pas le dernier? </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Commentaire</a> sur l'un de nos articles √† ce sujet, pour ainsi dire, de premi√®re main. </p><br><p>  RedHat dans sa documentation pour l'actuel RedHat Gluster Storage 3.4 note que le seul cas de partage qu'ils prennent en charge est le stockage pour les disques VM. </p><br><blockquote>  Sharding a un cas d'utilisation pris en charge: dans le contexte de la fourniture de Red Hat Gluster Storage en tant que domaine de stockage pour Red Hat Enterprise Virtualization, afin de fournir un stockage pour les images de machine virtuelle en direct.  Notez que le partitionnement est √©galement une exigence pour ce cas d'utilisation, car il fournit des am√©liorations de performances significatives par rapport aux impl√©mentations pr√©c√©dentes. </blockquote><p>  Je ne sais pas pourquoi une telle restriction, mais vous devez admettre que c'est alarmant. </p><br><h2 id="seychas-ya-tebe-tut-vse-zaheshiruyu">  Maintenant j'ai tout ici pour toi </h2><br><p>  Les deux syst√®mes utilisent une fonction de hachage pour distribuer de mani√®re pseudo-al√©atoire les donn√©es sur les disques. </p><br><p>  Pour RADOS, cela ressemble √† ceci: </p><br><pre><code class="plaintext hljs">PG = pool_id + "." + jenkins_hash(object_name) % pg_coun # eg pool id=5 =&gt; pg = 5.1f OSD = crush_hash_based_on_jenkins(PG) # eg pg=5.1f =&gt; OSD = 12</code> </pre> <br><p>  Gl utilise le <strong>hachage</strong> dit <strong>coh√©rent</strong> .  Chaque brique obtient une "plage dans un espace de hachage 32 bits".  Autrement dit, toutes les briques partagent tout l'espace de hachage d'adresse lin√©aire sans intersection de plages ou de trous.  Le client ex√©cute le nom de fichier via la fonction de hachage, puis d√©termine dans quelle plage de hachage le hachage re√ßu tombe.  Ainsi, la brique est s√©lectionn√©e.  S'il y a plusieurs briques dans le groupe de r√©plication, elles ont toutes la m√™me plage de hachage.  Quelque chose comme √ßa: </p><br><p><img src="https://habrastorage.org/webt/o_/y5/ye/o_y5yeby9vn5enx7r-5xa3zfwuq.png"></p><br><p>  Si nous apportons le travail de deux syst√®mes √† une certaine forme logique, il en r√©sultera quelque chose comme ceci: </p><br><pre> <code class="plaintext hljs">file -&gt; HASH -&gt; placement_unit</code> </pre> <br><p>  o√π placement_unit dans le cas de RADOS est PG, et dans le cas de gl c'est un groupe de r√©plication de plusieurs briques. </p><br><p>  Donc, une fonction de hachage, puis celle-ci distribue, distribue des fichiers, et tout √† coup, il s'av√®re que l'un placement_unit est utilis√© plus que l'autre.  Telle est la caract√©ristique fondamentale des syst√®mes de distribution de hachage.  Et nous sommes confront√©s √† une t√¢che tr√®s courante: d√©s√©quilibrer les donn√©es. </p><br><p>  Gl est capable de reconstruire, mais en raison de l'architecture avec les plages de hachage d√©crites ci-dessus, vous pouvez ex√©cuter la reconstruction autant que vous le souhaitez, mais aucune plage de hachage (et, par cons√©quent, les donn√©es) ne bougera.  Le seul crit√®re de redistribution des plages de hachage est un changement de capacit√© volumique.  Et il vous reste une option: ajouter des briques.  Et si nous parlons d'un volume avec r√©plication, alors nous devons ajouter un groupe de r√©plication entier, c'est-√†-dire deux nouvelles briques dans notre configuration.  Apr√®s avoir augment√© le volume, vous pouvez commencer la reconstruction - les plages de hachage seront redistribu√©es en tenant compte du nouveau groupe et les donn√©es seront distribu√©es.  Lorsqu'un groupe de r√©plication est supprim√©, les plages de hachage sont allou√©es automatiquement. </p><br><p>  RADOS a toute une voiture de possibilit√©s.  Dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de</a> Ceph, je me plaignais beaucoup du concept de PG, mais ici, comparant avec gl, bien s√ªr, RADOS √† cheval.  Chaque OSD a son propre poids, il est g√©n√©ralement d√©fini en fonction de la taille du disque.  √Ä leur tour, les PG sont distribu√©s par OSD en fonction du poids de ce dernier.  Tout, alors nous modifions simplement le poids de l'OSD vers le haut ou vers le bas et la PG (avec les donn√©es) commence √† se d√©placer vers d'autres OSD.  De plus, chaque OSD a un poids d'ajustement suppl√©mentaire, qui vous permet d'√©quilibrer les donn√©es entre les disques d'un serveur.  Tout cela est inh√©rent √† CRUSH.  Le principal b√©n√©fice est qu'il n'est pas n√©cessaire d'augmenter la capacit√© du pool pour mieux d√©s√©quilibrer les donn√©es.  Et il n'est pas n√©cessaire d'ajouter des disques en groupes, vous ne pouvez ajouter qu'un seul OSD et une partie de PG y sera transf√©r√©e. </p><br><p>  Oui, il est possible que lors de la cr√©ation d'un pool, ils n'aient pas cr√©√© suffisamment de PG et il s'est av√©r√© que chacune des PG est assez volumineuse, et o√π qu'elles se d√©placent, le d√©s√©quilibre restera.  Dans ce cas, vous pouvez augmenter le nombre de PG, et ils sont divis√©s en plus petits.  Oui, si le cluster est plein de donn√©es, cela fait mal, mais l'essentiel de notre comparaison est qu'il existe une telle opportunit√©.  Maintenant, seule une augmentation du nombre de PG est autoris√©e et avec cela, vous devez √™tre plus prudent, mais dans la prochaine version de Ceph - Nautilus, il y aura un soutien pour r√©duire le nombre de PG (fusion de pg). </p><br><h2 id="replikaciya-dannyh">  R√©plication de donn√©es </h2><br><p>  Nos pools et volumes de test ont un facteur de r√©plication de 2. Fait int√©ressant, les syst√®mes en question utilisent diff√©rentes approches pour atteindre ce nombre de r√©pliques. </p><br><p>  Dans le cas de RADOS, le sch√©ma d'enregistrement ressemble √† ceci: </p><br><p><img src="https://habrastorage.org/webt/lx/vb/q-/lxvbq-niuingzw76ad7aqanp2pg.png"></p><br><p>  Le client conna√Æt la topologie de l'ensemble du cluster, utilise CRUSH (√©tape 0) pour s√©lectionner une PG sp√©cifique pour l'√©criture, √©crit sur PRIMARY PG sur OSD-0 (√©tape 1), puis OSD-0 r√©plique de mani√®re synchrone les donn√©es sur SECONDARY PG (√©tape 2), et seulement apr√®s √©tape 2 r√©ussie / √©chou√©e, l'OSD confirme / ne confirme pas l'op√©ration au client (√©tape 3).  La r√©plication des donn√©es entre deux OSD est transparente pour le client.  Les OSD peuvent g√©n√©ralement utiliser un ¬´cluster¬ª distinct, un r√©seau plus rapide pour la r√©plication des donn√©es. </p><br><p>  Si la r√©plication triple est configur√©e, elle s'ex√©cute √©galement de mani√®re synchrone entre OSD PRIMAIRE et deux SECONDAIRES, transparente pour le client ... eh bien, seule cette latence est plus √©lev√©e. </p><br><p>  Gl fonctionne diff√©remment: </p><br><p><img src="https://habrastorage.org/webt/ll/zz/q-/llzzq-m2jhfaf83-dtk_fokavw0.png"></p><br><p>  Le client conna√Æt la topologie du volume, utilise DHT (√©tape 0) pour d√©terminer la brique souhait√©e, puis y √©crit (√©tape 1).  Tout est simple et clair.  Mais ici, nous rappelons que toutes les briques du groupe de r√©plication ont la m√™me plage de hachage.  Et cette caract√©ristique mineure fait toute la f√™te.  Le client √©crit en parall√®le sur toutes les briques qui ont une plage de hachage appropri√©e. </p><br><p>  Dans notre cas, avec double r√©plication, le client effectue un double enregistrement en parall√®le sur deux briques diff√©rentes.  Au cours de la triple r√©plication, un triple enregistrement sera effectu√©, respectivement, et 1 Mo de donn√©es se transformeront approximativement en 3 Mo de trafic r√©seau du client vers le c√¥t√© des serveurs gl.  D'accord, les concepts de syst√®mes sont perpendiculaires. </p><br><p>  Dans un tel sch√©ma, plus de travail est assign√© au client gl, et, par cons√©quent, il a besoin de plus de CPU, eh bien, j'ai d√©j√† dit √† propos du r√©seau. </p><br><p>  La r√©plication est effectu√©e par le traducteur AFP (r√©plication automatique de fichiers) - Un xlator c√¥t√© client qui effectue une r√©plication synchrone.  R√©plique les √©critures sur toutes les briques de la r√©plique ‚Üí Utilise un mod√®le de transaction. </p><br><p>  Si n√©cessaire, synchronisez les r√©pliques dans le groupe (gu√©rison), par exemple, apr√®s une indisponibilit√© temporaire d'une brique, les d√©mons gl le font par eux-m√™mes en utilisant l'AFP int√©gr√©, transparent pour les clients et sans leur participation. </p><br><p>  Il est int√©ressant de noter que si vous ne travaillez pas via le client gl natif, mais √©crivez via le serveur NFS int√©gr√© dans gl, nous obtiendrons le m√™me comportement que RADOS.  Dans ce cas, AFP sera utilis√© dans les d√©mons gl pour r√©pliquer les donn√©es sans intervention du client.  Mais le NFS int√©gr√© est s√©curis√© dans gl v4, et si vous voulez ce comportement, il est recommand√© d'utiliser NFS-Ganesha. </p><br><p>  Soit dit en passant, en raison de comportements si diff√©rents lors de l'utilisation de NFS et du client natif, vous pouvez voir des indicateurs de performances compl√®tement diff√©rents. </p><br><h2 id="a-u-vas-est-takoy-zhe-klaster-tolko-na-kolenke">  Avez-vous le m√™me cluster, uniquement "sur le genou"? </h2><br><p>  Je vois souvent sur Internet des discussions sur toutes sortes de configurations de rotule, o√π un cluster de donn√©es est construit √† partir de ce qui est √† port√©e de main.  Dans ce cas, une solution bas√©e sur RADOS peut vous donner plus de libert√© lors du choix de vos disques.  Dans RADOS, vous pouvez ajouter des lecteurs de presque toutes les tailles.  Chaque disque aura un poids correspondant √† sa taille (g√©n√©ralement) et les donn√©es seront r√©parties sur les disques presque proportionnellement √† leur poids.  Dans le cas de gl, il n'y a pas de concept de ¬´disques s√©par√©s¬ª dans les volumes avec r√©plication.  Les disques sont ajout√©s par paires √† double r√©plication ou triples √† triple.  S'il existe des disques de tailles diff√©rentes dans un groupe de r√©plication, vous ex√©cuterez alors un emplacement sur le plus petit disque du groupe et d√©ploierez la capacit√© des grands disques.  Dans un tel sch√©ma, gl supposera que la capacit√© d'un groupe de r√©plication est √©gale √† la capacit√© du plus petit disque du groupe, ce qui est logique.  En m√™me temps, il est permis d'avoir des groupes de r√©plication compos√©s de disques de tailles diff√©rentes - des groupes de tailles diff√©rentes.  Les groupes plus importants peuvent recevoir une plus grande plage de hachage par rapport aux autres groupes et, par cons√©quent, recevoir plus de donn√©es. </p><br><p>  Nous vivons avec Ceph depuis la cinqui√®me ann√©e.  Nous avons commenc√© avec des disques du m√™me volume, maintenant nous introduisons des disques plus volumineux.  Avec Ceph, vous pouvez retirer le disque et le remplacer par un autre plus grand ou l√©g√®rement plus petit sans aucune difficult√© architecturale.  Avec gl, tout est plus compliqu√© - a sorti un disque de 2 To - mettez le m√™me, s'il vous pla√Æt.  Eh bien, ou retirer l'ensemble du groupe dans son ensemble, ce qui n'est pas tr√®s bon, d'accord. </p><br><h2 id="obrabotka-otkazov">  Basculement </h2><br><p>  Nous connaissions d√©j√† un peu l'architecture des deux solutions et nous pouvons maintenant parler de la fa√ßon de vivre avec et des fonctionnalit√©s lors de la maintenance. </p><br><p>  Supposons que sda ‚Äã‚Äãsur s1 soit refus√© - une chose courante. </p><br><p>  Dans le cas de gl: </p><br><ul><li>  une copie des donn√©es sur le disque actif restant dans le groupe n'est pas automatiquement redistribu√©e aux autres groupes; </li><li>  jusqu'√† ce que le disque soit remplac√©, il ne reste qu'une copie des donn√©es; </li><li>  lors du remplacement d'un disque d√©fectueux par un nouveau, la r√©plication est effectu√©e √† partir d'un disque de travail vers un nouveau (1 sur 1). </li></ul><br><p>  C'est comme servir une √©tag√®re avec plusieurs RAID-1.  Oui, avec une triple r√©plication, si un disque tombe en panne, il ne reste plus une copie, mais deux, mais cette approche pr√©sente n√©anmoins de s√©rieux inconv√©nients, et je vais les montrer avec un bon exemple avec RADOS. </p><br><p>  Supposons que nous ayons √©chou√© sda sur S1 (OSD-0) - une chose courante: </p><br><ul><li>  Les PG qui √©taient sur OSD-0 seront automatiquement remapp√©es vers d'autres OSD apr√®s 10 minutes (par d√©faut).  Dans notre exemple, sur OSD 1 et 2. S'il y avait plus de serveurs, alors sur un plus grand nombre d'OSD. </li><li>  Les PG qui stockent la deuxi√®me copie survivante des donn√©es les r√©pliqueront automatiquement sur les OSD o√π les PG restaur√©es sont transf√©r√©es.  Il s'av√®re que la r√©plication plusieurs √† plusieurs, pas la r√©plication un √† un comme gl. </li><li>  Lorsqu'un nouveau disque est introduit, au lieu d'un disque cass√©, certaines PG seront accumul√©es en fonction de son poids dans le nouvel OSD et les donn√©es d'autres OSD seront redistribu√©es. </li></ul><br><p>  Je pense que cela n'a aucun sens d'expliquer les avantages architecturaux de RADOS.  Vous ne pouvez pas trembler lorsque vous recevez une lettre disant que le disque est tomb√© en panne.  Et lorsque vous venez travailler le matin, constatez que toutes les copies manquantes ont d√©j√† √©t√© restaur√©es sur des dizaines d'autres OSD ou en cours de traitement.  Sur les grands clusters, o√π des centaines de PG sont r√©parties sur un tas de disques, la r√©cup√©ration de donn√©es d'un OSD peut avoir lieu √† des vitesses beaucoup plus √©lev√©es que la vitesse d'un disque en raison du fait que des dizaines d'OSD sont impliqu√©s (lecture et √©criture).  Eh bien, vous ne devez pas non plus oublier l'√©quilibrage de charge. </p><br><h2 id="masshtabirovanie">  Mise √† l'√©chelle </h2><br><p>  Dans ce contexte, je donnerai probablement le pi√©destal gl.  Dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> sur Ceph, j'ai d√©j√† √©crit sur certaines des complexit√©s de la mise √† l'√©chelle RADOS associ√©es au concept PG.  Si l'augmentation de PG avec la croissance du cluster peut encore √™tre ressentie, alors qu'en est-il de Ceph MDS n'est pas clair.  CephFS fonctionne au-dessus de RADOS et utilise un pool s√©par√© pour les m√©tadonn√©es et un processus sp√©cial, le serveur de m√©tadonn√©es ceph (MDS), pour la maintenance des m√©tadonn√©es du syst√®me de fichiers et la coordination de toutes les op√©rations avec le FS.  Je ne dis pas qu'avoir MDS met fin √† l'√©volutivit√© de CephFS, non, d'autant plus que vous pouvez ex√©cuter plusieurs MDS en mode actif-actif.  Je veux juste noter que gl est architecturalement d√©pourvu de tout cela.  Il n'a pas d'√©quivalent PG, rien de tel que MDS.  Gl √©volue vraiment parfaitement en ajoutant simplement des groupes de r√©plication, presque lin√©airement. </p><br><p>  Dans les jours qui ont pr√©c√©d√© CephFS, nous avons con√ßu la solution pour les p√©taoctets de donn√©es et examin√© GL.  Ensuite, nous avions des doutes sur l'√©volutivit√© de gl et nous l'avons d√©couvert par le biais de la liste de diffusion.  Voici une des r√©ponses (Q: ma question): </p><br><blockquote>  J'utilise 60 serveurs chacun a des disques 26x8TB total 1560 disque 16 + 4 volume EC avec 9PB d'espace utilisable. <br><br>  Q: Utilisez-vous libgfapi ou FUSE ou NFS c√¥t√© client? <br><br>  J'utilise FUSE et j'ai pr√®s de 1000 clients. <br><br>  Q: Combien de fichiers avez-vous dans votre volume? <br>  Q: Les fichiers sont plus gros ou plus petits? <br><br>  J'ai plus de 1 million de fichiers et% 13 du cluster est utilis√©, ce qui fait une taille de fichier moyenne de 1 Go. <br>  La taille de fichier minimale / maximale est de 100 Mo / 2 Go.  Chaque jour, 10 √† 20 To de nouvelles donn√©es entrent dans le volume. <br><br>  Q: √Ä quelle vitesse "ls" fonctionne-t-il)? <br><br>  Les op√©rations de m√©tadonn√©es sont lentes comme vous vous y attendez.  J'essaye de ne pas mettre plus de 2-3K fichiers dans un r√©pertoire.  Mon cas d'utilisation est pour la sauvegarde / l'archivage, donc je fais rarement des op√©rations de m√©tadonn√©es. </blockquote><br><h2 id="pereimenovanie-faylov">  Renommer des fichiers </h2><br><p>  Retour aux fonctions de hachage √† nouveau.  Nous avons compris comment des fichiers sp√©cifiques sont achemin√©s vers des disques sp√©cifiques, et maintenant la question devient pertinente, mais que se passera-t-il lors du changement de nom des fichiers? </p><br><p>  Apr√®s tout, si nous changeons le nom du fichier, le hachage en son nom changera √©galement, ce qui signifie la place de ce fichier sur un autre disque (dans une plage de hachage diff√©rente) ou sur une autre PG / OSD en cas de RADOS.  Oui, nous pensons correctement, et ici, sur deux syst√®mes, tout est √† nouveau perpendiculaire. </p><br><p>  Dans le cas de gl, lorsque vous renommez un fichier, le nouveau nom est ex√©cut√© via une fonction de hachage, une nouvelle brique est d√©finie et un lien sp√©cial est cr√©√© sur celle-ci vers l'ancienne brique, o√π le fichier reste comme avant.  Topovka, c'est √ßa?  Pour que les donn√©es se d√©placent vraiment vers un nouvel endroit, et que le client n'a pas cliqu√© sur le lien inutilement, vous devez faire une r√©bellion. </p><br><p>  Mais RADOS n'a g√©n√©ralement pas de m√©thode pour renommer les objets simplement en raison de la n√©cessit√© de leur d√©placement ult√©rieur.  Il est propos√© d'utiliser une copie √©quitable pour renommer, ce qui entra√Æne un mouvement synchrone de l'objet.  Et CephFS, qui fonctionne au-dessus de RADOS, a un atout dans sa manche sous la forme d'un pool de m√©tadonn√©es et de MDS.  La modification du nom de fichier n'affecte pas le contenu du fichier dans le pool de donn√©es. </p><br><h2 id="replikaciya-25">  R√©plication 2.5 </h2><br><p>  Gl a une fonctionnalit√© tr√®s int√©ressante que je voudrais mentionner s√©par√©ment.  Tout le monde comprend que la r√©plication 2 n'est pas une configuration fiable, mais n√©anmoins elle a lieu p√©riodiquement pour √™tre tout √† fait justifi√©e.  Pour vous prot√©ger contre le split-brain dans de tels sch√©mas et pour garantir la coh√©rence des donn√©es, gl vous permet de cr√©er des volumes avec la r√©plique 2 et un arbitre suppl√©mentaire.  L'arbitre est applicable pour la r√©plication de 3 ou plus.  Il s'agit de la m√™me brique du groupe que les deux autres, mais elle ne cr√©e en fait qu'une structure de fichiers √† partir de fichiers et de r√©pertoires.  Les fichiers sur une telle brique sont de taille nulle, mais leurs attributs √©tendus du syst√®me de fichiers (attributs √©tendus) sont conserv√©s en √©tat synchronis√© avec les fichiers de taille normale dans la m√™me r√©plique.  Je pense que l'id√©e est claire.  Je pense que c'est une bonne opportunit√©. </p><br><p>  Le seul moment ... la taille de l'emplacement dans le groupe de r√©plication est d√©termin√©e par la taille de la plus petite brique, ce qui signifie que l'arbitre doit glisser un disque au moins de la m√™me taille que le reste du groupe.  Pour ce faire, il est recommand√© de cr√©er des LV minces (fines) fictives, de grandes tailles, afin de ne pas utiliser un vrai disque. </p><br><h2 id="a-che-po-klientam">  Et les clients? </h2><br><p>  L'API native des deux syst√®mes est impl√©ment√©e sous la forme des biblioth√®ques libgfapi (gl) et libcephfs (CephFS).  Des liaisons pour les langues populaires sont √©galement disponibles.  En g√©n√©ral, avec les biblioth√®ques, tout est √† peu pr√®s aussi bon.  Le NFS-Ganesha omnipr√©sent prend en charge les deux biblioth√®ques en tant que FSAL, ce qui est √©galement la norme.  Qemu prend √©galement en charge l'API native gl via libgfapi. </p><br><p>  Mais fio (Flexible I / O Tester) prend en charge libgfapi depuis longtemps et avec succ√®s, mais il ne prend pas en charge libcephfs.  C'est un plus, car  utiliser fio est vraiment sympa pour tester gl directement.  En travaillant uniquement depuis l'espace utilisateur via libgfapi, vous obtiendrez tout ce que gl peut faire de gl. </p><br><p>  Mais si nous parlons du syst√®me de fichiers POSIX et de la fa√ßon de le monter, alors gl ne peut proposer que le client FUSE et l'impl√©mentation CephFS dans le noyau en amont.  Il est clair que dans le module du noyau, vous pouvez faire une telle astuce que FUSE affichera de meilleures performances.  Mais dans la pratique, FUSE est toujours une surcharge pour le changement de contexte.  J'ai personnellement vu plus d'une fois comment FUSE a pli√© un serveur √† double socket avec des CS uniquement. <br>  D'une mani√®re ou d'une autre, Linus a d√©clar√©: </p><br><blockquote>  Syst√®me de fichiers de l'espace utilisateur?  Le probl√®me est l√†.  Depuis toujours.  Les gens qui pensent que les syst√®mes de fichiers de l'espace utilisateur sont r√©alistes pour tout, mais les jouets sont tout simplement malavis√©s. </blockquote><p>  Les d√©veloppeurs de Gl, au contraire, pensent que FUSE est cool.  On dit que cela donne plus de flexibilit√© et se d√©tache des versions du noyau.  Quant √† moi, ils utilisent FUSE car gl n'est pas une question de vitesse.  D'une certaine mani√®re, c'est √©crit - eh bien, c'est normal, et s'emb√™ter avec l'impl√©mentation dans le noyau est vraiment √©trange. </p><br><h2 id="proizvoditelnost">  Performances </h2><br><p>  Il n'y aura pas de comparaisons). </p><br><p>  C'est trop compliqu√©.  M√™me sur une configuration identique, il est trop difficile de r√©aliser des tests objectifs.  Quoi qu'il en soit, il y aura quelqu'un dans les commentaires qui donnera 100500 param√®tres qui ¬´acc√©l√®rent¬ª l'un des syst√®mes et disent que les tests sont des conneries.  Par cons√©quent, si vous √™tes int√©ress√©, testez-vous, s'il vous pla√Æt. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  RADOS et CephFS, en particulier, sont une solution plus complexe √† la fois dans la compr√©hension, la configuration et la maintenance. </p><br><p>  Mais personnellement, j'aime l'architecture de RADOS et son fonctionnement sur CephFS plus que sur GlusterFS.  Plus de poign√©es (PG, poids OSD, hi√©rarchie CRUSH, etc.), les m√©tadonn√©es CephFS augmentent la complexit√©, mais donnent plus de flexibilit√© et rendent cette solution plus efficace, √† mon avis. </p><br><p>  Ceph est beaucoup mieux adapt√© aux crit√®res SDS actuels et me semble plus prometteur.  Mais c'est mon avis, qu'en pensez-vous? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr430474/">https://habr.com/ru/post/fr430474/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr430462/index.html">En Russie, un projet de loi est apparu sur la fourniture de donn√©es des utilisateurs des r√©seaux sociaux √† un cercle illimit√© de personnes. R√©seaux sociaux contre</a></li>
<li><a href="../fr430466/index.html">Mini AI Cup # 3: √âcrire un top bot</a></li>
<li><a href="../fr430468/index.html">Sensibiliser les citoyens</a></li>
<li><a href="../fr430470/index.html">Pourquoi maintenir le contexte sur le compte client - honn√™tement et de mani√®re rentable</a></li>
<li><a href="../fr430472/index.html">R√©seau DECT sans couture bricolage</a></li>
<li><a href="../fr430476/index.html">NCBI Genome Workbench: Endangered Research</a></li>
<li><a href="../fr430478/index.html">Trading de bots pour le march√© des crypto-monnaies. Par o√π commencer?</a></li>
<li><a href="../fr430480/index.html">Au moment o√π nous √©crivions l'application au hackathon du NASA Space Apps Challenge</a></li>
<li><a href="../fr430482/index.html">Le th√®me des plaques d'armure dans la culture de l'Est et de l'Ouest</a></li>
<li><a href="../fr430484/index.html">Sc√©narios d'impl√©mentation NGFW typiques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>