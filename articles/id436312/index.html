<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ½â€âš–ï¸ ğŸ•°ï¸ ğŸ’ˆ Sintesis pidato jaringan saraf menggunakan arsitektur Tacotron 2, atau "Dapatkan keselarasan atau mati coba" ğŸ…¿ï¸ â™ï¸ ğŸ¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tim kami diberi tugas: mengulangi hasil kerja jaringan syaraf tiruan sintesis bicara Tacotron2 kepengarangan DeepMind. Ini adalah kisah tentang jalan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sintesis pidato jaringan saraf menggunakan arsitektur Tacotron 2, atau "Dapatkan keselarasan atau mati coba"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/436312/"><img src="https://habrastorage.org/webt/yu/y0/2f/yuy02fd0i4fodpxxadfkf0k2ori.jpeg"><br><br>  Tim kami diberi tugas: mengulangi hasil kerja jaringan syaraf tiruan sintesis bicara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tacotron2</a> kepengarangan DeepMind.  Ini adalah kisah tentang jalan setapak yang kami lewati saat pelaksanaan proyek. <br><a name="habracut"></a><br>  Tugas sintesis pidato komputer telah lama menarik bagi para ilmuwan dan pakar teknis.  Namun, metode klasik tidak memungkinkan sintesis ucapan, tidak dapat dibedakan dari manusia.  Dan di sini, seperti di banyak daerah lain, pembelajaran mendalam datang untuk menyelamatkan. <br><br>  Mari kita lihat metode sintesis klasik. <br><br><h2>  Sintesis Bicara Konsatenatif </h2><br>  Metode ini didasarkan pada fragmen audio pendek pra-perekaman, yang kemudian digabungkan untuk menciptakan ucapan yang koheren.  Ternyata sangat bersih dan jelas, tetapi sama sekali tanpa komponen emosional dan intonasional, yaitu, itu terdengar tidak wajar.  Dan semuanya karena tidak mungkin mendapatkan rekaman audio dari semua kata yang mungkin diucapkan dalam semua kombinasi emosi dan prosodi yang mungkin.  Sistem concatenative membutuhkan basis data besar dan kombinasi hard-coding untuk membentuk kata-kata.  Mengembangkan sistem yang andal membutuhkan banyak waktu. <br><br><h2>  Sintesis pidato parametrik </h2><br>  Aplikasi TTS concatenational terbatas karena persyaratan data yang tinggi dan waktu pengembangan.  Oleh karena itu, metode statistik dikembangkan yang mengeksplorasi sifat data.  Ini menghasilkan ucapan dengan menggabungkan parameter seperti frekuensi, spektrum amplitudo, dll. <br><br>  Sintesis parametrik terdiri dari dua tahap. <br><br><ol><li>  Pertama, fitur linguistik, seperti fonem, durasi, dll., Diekstraksi dari teks. </li><li>  Kemudian, untuk vocoder (sistem yang menghasilkan bentuk gelombang), fitur diekstraksi yang mewakili sinyal ucapan yang sesuai: cepstrum, frekuensi, spektogram linier, spektrogram kapur. </li><li>  Parameter yang dikonfigurasi secara manual ini, bersama dengan fitur linguistik, ditransfer ke model vocoder, dan melakukan banyak transformasi kompleks untuk menghasilkan gelombang suara.  Pada saat yang sama, vocoder mengevaluasi parameter bicara, seperti fase, prosodi, intonasi, dan lainnya. </li></ol><br>  Jika kita dapat memperkirakan parameter yang menentukan ucapan pada setiap unitnya, maka kita dapat membuat model parametrik.  Sintesis parametrik membutuhkan data dan kerja keras yang jauh lebih sedikit daripada sistem concatenative. <br><br>  Secara teoritis, semuanya sederhana, tetapi dalam praktiknya ada banyak artefak yang mengarah pada suara teredam dengan suara "berdengung", yang sama sekali tidak terdengar seperti suara alami. <br><br>  Faktanya adalah bahwa pada setiap tahap sintesis kami membuat kode yang sulit untuk beberapa fitur dan berharap untuk mendapatkan pidato yang terdengar realistis.  Tetapi data yang dipilih didasarkan pada pemahaman kita tentang bicara, dan pengetahuan manusia tidak mutlak, oleh karena itu, tanda-tanda yang diambil tidak akan selalu menjadi solusi terbaik. <br><br>  Dan di sini Deep Learning memasuki lokasi dengan segala keindahannya. <br><br>  Deep neural networks adalah alat yang ampuh yang, secara teoritis, dapat memperkirakan fungsi kompleks yang berubah-ubah, yaitu, membawa beberapa ruang data input X ke dalam ruang data output Y. Dalam konteks tugas kami, masing-masing akan berupa teks dan audio dengan ucapan. <br><br><h2>  Pra-pemrosesan data </h2><br>  Untuk memulainya, kita akan menentukan apa yang kita miliki sebagai input dan apa yang ingin kita dapatkan pada output. <br><br>  Input akan berupa teks, dan output akan menjadi spektrogram <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kapur</a> .  Ini adalah representasi tingkat rendah yang diperoleh dengan menerapkan transformasi Fourier cepat ke sinyal audio diskrit.  Harus segera dicatat bahwa spektogram yang diperoleh dengan cara ini masih <b>perlu dinormalisasi</b> dengan mengompresi rentang dinamis.  Ini memungkinkan Anda mengurangi hubungan alami antara suara paling keras dan paling tenang dalam rekaman.  Dalam percobaan kami, penggunaan spektrogram dikurangi hingga <b>kisaran [-4; 4]</b> terbukti menjadi yang terbaik. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/849/8fe/eb1/8498feeb1bd525506739c85bc5230c65.png"><br>  <i>Gambar 1: Spektrum kapur dari sinyal audio ucapan berkurang hingga kisaran [-4; 4].</i> <br><br>  Sebagai set data pelatihan, kami memilih <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dataset LJSpeech</a> , yang berisi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">13.100</a> trek audio selama 2-10 detik.  dan file dengan teks yang sesuai dengan ucapan bahasa Inggris yang direkam pada audio. <br><br>  Suara menggunakan transformasi di atas dikodekan ke dalam spektrogram kapur.  Teks tersebut dipatok dan diubah. <br><br>  menjadi urutan bilangan bulat.  Saya harus segera menekankan bahwa teks dinormalisasi: semua angka ditulis secara verbal, dan kemungkinan singkatan diuraikan, misalnya: "Mrs.  Robinson "-" Missis Robinson ". <br><br>  Jadi, setelah preprocessing, kita mendapatkan set array numpy dari urutan numerik dan spektogram kapur yang direkam dalam file npy pada disk. <br><br>  Sehingga pada tahap pelatihan semua dimensi dalam tensor patch bertepatan, kami akan menambahkan paddings ke urutan pendek.  Untuk urutan dalam bentuk teks, ini akan disediakan untuk padding 0, dan untuk spektrogram, bingkai yang nilainya sedikit lebih rendah dari spektogram minimum yang ditentukan oleh kami.  Ini direkomendasikan untuk mengisolasi bantalan ini, memisahkannya dari kebisingan dan keheningan. <br><br>  Sekarang kami memiliki data yang mewakili teks dan audio yang cocok untuk diproses oleh jaringan saraf tiruan.  Mari kita lihat arsitektur Net prediksi fitur, yang dengan nama elemen pusat dari seluruh sistem sintesis akan disebut Tacotron2. <br><br><h2>  Arsitektur </h2><br>  Tacotron 2 bukan satu jaringan, tetapi dua: Net prediksi fitur dan NN-vocoder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">WaveNet</a> .  Artikel asli, serta visi kami sendiri tentang pekerjaan yang dilakukan, memungkinkan kami untuk mempertimbangkan Net prediksi fitur sebagai biola pertama, sementara vocoder WaveNet berperan sebagai sistem periferal. <br><br>  Tacotron2 adalah arsitektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">urutan ke urutan</a> .  Ini terdiri dari <b>encoder</b> (encoder), yang menciptakan beberapa representasi internal dari sinyal input (simbol token), dan <b>decoder</b> (decoder), yang mengubah representasi ini menjadi spektrogram kapur.  Juga elemen yang sangat penting dari jaringan adalah apa yang disebut <b>PostNet</b> , yang dirancang untuk meningkatkan spektrogram yang dihasilkan oleh decoder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/744/8c3/587/7448c35878c1640d1e156e745fa2bd96.png"><br>  <i>Gambar 2: Arsitektur Jaringan Tacotron 2.</i> <br><br>  Mari kita pertimbangkan lebih detail blok-blok jaringan dan modul-modulnya. <br><br>  Lapisan <b>encoder</b> pertama adalah lapisan Embedding.  Berdasarkan urutan bilangan alami yang mewakili karakter, ia menciptakan vektor multidimensi (512 dimensi). <br><br>  Selanjutnya, vektor embedding dimasukkan ke dalam blok tiga lapisan konvolusional satu dimensi.  Setiap lapisan menyertakan 512 filter dengan panjang 5. Nilai ini adalah ukuran filter yang baik dalam konteks ini, karena menangkap karakter tertentu, serta dua tetangganya sebelumnya dan dua tetangganya berikutnya.  Setiap lapisan konvolusional diikuti oleh normalisasi mini-batch dan aktivasi ReLU. <br><br>  Tensor yang diperoleh setelah blok konvolusional diterapkan pada lapisan LSTM dua arah, masing-masing 256 neuron.  Meneruskan dan mengembalikan hasil digabungkan. <br><br>  <b>Dekoder</b> memiliki arsitektur berulang, yaitu, pada setiap langkah berikutnya, output dari langkah sebelumnya digunakan.  Di sini mereka akan menjadi satu bingkai spektogram.  Elemen penting lain, jika bukan kunci, dari sistem ini adalah mekanisme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perhatian yang</a> lembut (terlatih) - teknik yang relatif baru yang semakin populer.  Pada setiap langkah dekoder, perhatian untuk membentuk vektor konteks dan memperbarui bobot perhatian menggunakan: <br><br><ul><li>  proyeksi keadaan tersembunyi sebelumnya dari jaringan RNN dari decoder pada lapisan yang terhubung penuh, </li><li>  proyeksi output encoder ke lapisan yang terhubung penuh, </li><li>  serta aditif (terakumulasi pada setiap langkah waktu decoder) menjadi perhatian. </li></ul><br>  Gagasan perhatian harus dipahami sebagai berikut: "bagian mana dari data encoder yang harus digunakan pada langkah dekoder saat ini". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/f4d/410/e47f4d41040e5926c719ef909d21cc99.png"><br>  <i>Gambar 3: Skema mekanisme perhatian.</i> <br><br>  Pada setiap langkah dekoder, vektor konteks <i>C <sub>i</sub></i> dihitung (ditunjukkan sebagai "output encoder yang dihadiri" pada gambar di atas), yang merupakan produk dari output encoder ( <i>h</i> ) dan bobot perhatian ( <i>Î±</i> ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b27/8dc/00e/b278dc00e38919c05351ff95c03dc309.png"><br><br>  di mana <i>Î± <sub>ij</sub></i> adalah bobot perhatian yang dihitung dengan rumus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3cc/344/b9f/3cc344b9f86ba736045a83a7cae8b77e.png"><br><br>  di mana <i>e <sub>ij</sub></i> adalah apa yang disebut "energi", rumus perhitungan yang tergantung pada jenis mekanisme perhatian yang Anda gunakan (dalam kasus kami, itu akan menjadi tipe hibrida, menggunakan baik perhatian berbasis lokasi dan perhatian berbasis konten).  Energi dihitung dengan rumus: <br><br>  <i>e <sub>ij</sub> = v <sub>aT</sub> tanh (Ws <sub>i-1</sub> + Vh <sub>j</sub> + Uf <sub>i, j</sub> + b)</i> <br><br>  dimana: <br><ul><li>  <i>s <sub>i-1</sub></i> - status tersembunyi sebelumnya dari jaringan LSTM dari decoder, </li><li>  <i>Î± <sub>i-1</sub></i> - bobot perhatian sebelumnya, </li><li>  <i>h <sub>j</sub></i> adalah status tersembunyi dari enkoder, </li><li>  <i>W</i> , <i>V</i> , <i>U</i> , <i>v <sub>a</sub></i> dan <i>b</i> adalah parameter pelatihan, </li><li>  <i>f <sub>i, j</sub></i> - tanda-lokasi yang dihitung dengan rumus: <br><br>  <i>f <sub>i</sub> = F * Î± <sub>i-1</sub></i> <br><br>  di mana <i>F</i> adalah operasi konvolusi. </li></ul><br><br>  Untuk pemahaman yang jelas tentang apa yang terjadi, kami menambahkan bahwa beberapa modul yang dijelaskan di bawah mengasumsikan penggunaan informasi dari langkah sebelumnya dari decoder.  Tetapi jika ini adalah langkah pertama, maka informasi akan menjadi tensor dari nilai nol, yang merupakan praktik umum ketika membuat struktur perulangan. <br><br>  Sekarang pertimbangkan <b>algoritma operasi</b> . <br><br>  Pertama, output dekoder dari langkah waktu sebelumnya dimasukkan ke dalam modul PreNet kecil, yang merupakan tumpukan dua lapisan 256 neuron yang terhubung penuh, bergantian dengan lapisan putus dengan kecepatan 0,5.  Fitur khusus dari modul ini adalah bahwa dropout digunakan di dalamnya tidak hanya pada tahap pelatihan model, tetapi juga pada tahap output. <br><br>  Output PreNet dalam gabungan dengan vektor konteks yang diperoleh sebagai hasil dari mekanisme perhatian dimasukkan ke input ke jaringan LSTM dua-lapisan searah, 1024 neuron di setiap lapisan. <br><br>  Kemudian, gabungan output dari lapisan LSTM dengan vektor konteks yang sama (dan mungkin berbeda) dimasukkan ke dalam lapisan yang terhubung penuh dengan 80 neuron, yang sesuai dengan jumlah saluran spektogram.  Lapisan terakhir dari dekoder ini membentuk bingkai spektrogram yang diprediksi oleh bingkai.  Dan hasilnya sudah disediakan sebagai input ke langkah waktu berikutnya dari decoder di PreNet. <br><br>  Mengapa kami menyebutkan dalam paragraf sebelumnya bahwa vektor konteks mungkin sudah berbeda?  Salah satu pendekatan yang mungkin adalah menghitung ulang vektor konteks setelah keadaan laten jaringan LSTM diperoleh pada langkah ini.  Namun, dalam percobaan kami, pendekatan ini tidak membenarkan dirinya sendiri. <br><br>  Selain memproyeksikan ke lapisan 80-neuron yang sepenuhnya terhubung, rangkaian output dari lapisan LSTM dengan vektor konteks dimasukkan ke dalam lapisan yang sepenuhnya terhubung dengan satu neuron, diikuti oleh aktivasi sigmoid - ini adalah lapisan "stop token prediksi".  Dia memprediksi kemungkinan bahwa frame yang dibuat pada langkah dekoder ini adalah final.  Lapisan ini dirancang untuk menghasilkan spektrogram tidak tetap, tetapi panjang sewenang-wenang pada tahap output model.  Yaitu, pada tahap output, elemen ini menentukan jumlah langkah dari decoder.  Ini dapat dianggap sebagai classifier biner. <br><br>  Output dari decoder dari semua langkahnya akan menjadi spektrogram yang diprediksi.  Namun, ini belum semuanya.  Untuk meningkatkan kualitas spektrogram, ia dilewatkan melalui modul PostNet, yang merupakan tumpukan lima lapisan konvolusional satu dimensi dengan masing-masing 512 filter dan dengan ukuran filter 5. Normalisasi batch dan aktivasi tangen mengikuti setiap lapisan (kecuali yang terakhir).  Untuk kembali ke dimensi spektrogram, kami melewatkan data output post-net melalui lapisan yang terhubung penuh dengan 80 neuron dan menambahkan data yang diterima dengan hasil awal dari decoder.  Kami mendapatkan spektrogram kapur yang dihasilkan dari teks.  Untung <br><br>  Semua modul konvolusional diregulasi dengan lapisan putus dengan laju 0,5, dan lapisan berulang dengan metode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Zoneout yang</a> lebih baru dengan laju 0,1.  Ini cukup sederhana: alih-alih menerapkan keadaan laten dan keadaan sel yang diperoleh pada langkah saat ini ke langkah waktu berikutnya dari jaringan LSTM, kami mengganti bagian dari data dengan nilai-nilai dari langkah sebelumnya.  Ini dilakukan pada tahap pelatihan dan pada tahap penarikan.  Dalam kasus ini, hanya keadaan tersembunyi (yang diteruskan ke langkah LSTM berikutnya) yang diekspos ke metode Zoneout di setiap langkah, sedangkan output sel LSTM pada langkah saat ini tetap tidak berubah. <br><br>  Kami memilih PyTorch sebagai kerangka pembelajaran yang mendalam.  Meskipun pada saat implementasi jaringan itu dalam keadaan pra-rilis, tetapi sudah merupakan alat yang sangat kuat untuk membangun dan melatih jaringan saraf tiruan.  Dalam pekerjaan kami, kami menggunakan kerangka kerja lain seperti TensorFlow dan Keras.  Namun, yang terakhir dibuang karena kebutuhan untuk menerapkan struktur kustom non-standar, dan jika kita membandingkan TensorFlow dan PyTorch, maka ketika menggunakan yang kedua, tidak ada perasaan bahwa model tersebut diambil dari bahasa Python.  Namun, kami tidak berusaha untuk menyatakan bahwa salah satu dari mereka lebih baik dan yang lain lebih buruk.  Penggunaan kerangka kerja tertentu mungkin tergantung pada berbagai faktor. <br><br>  Jaringan dilatih oleh metode backpropagation.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ADAM</a> digunakan sebagai pengoptimal, Mean Square Error sebelum dan sesudah PostNet, serta Binary Cross Entropy di atas nilai aktual dan prediksi lapisan Stop Token Prediction, digunakan sebagai fungsi kesalahan.  Kesalahan yang dihasilkan adalah jumlah sederhana dari ketiganya. <br><br>  Model ini dilatih pada GPU GeForce 1080Ti tunggal dengan memori 11 GB. <br><br><h2>  Visualisasi </h2><br>  Ketika bekerja dengan model sebesar itu, penting untuk melihat bagaimana proses pembelajaran berjalan.  Dan di sini TensorBoard menjadi alat yang praktis.  Kami melacak nilai kesalahan di kedua pelatihan dan iterasi validasi.  Selain itu, kami menampilkan target spektogram, spektogram prediksi pada tahap pelatihan, prediksi spektogram pada tahap validasi, dan penyelarasan, yang merupakan bobot perhatian terakumulasi secara aditif dari semua langkah pelatihan. <br><br>  Ada kemungkinan bahwa pada awalnya perhatian Anda tidak akan terlalu informatif: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4ce/600/7be/4ce6007bef5b7fe17aa8df56472904bc.png"><br>  <i>Gambar 4: Contoh skala perhatian yang kurang terlatih.</i> <br><br>  Tetapi setelah semua modul Anda mulai bekerja seperti arloji Swiss, Anda akhirnya akan mendapatkan sesuatu seperti ini: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8e7/5d9/ff0/8e75d9ff0e0da78915c1a16f623a99e8.png"><br>  <i>Gambar 5: Contoh skala perhatian yang berhasil dilatih.</i> <br><br>  Apa artinya bagan ini?  Pada setiap langkah dari decoder, kami mencoba untuk memecahkan kode satu frame spektogram.  Namun, tidak jelas informasi apa yang perlu digunakan pembuat enkode pada setiap langkah dekoder.  Dapat diasumsikan bahwa korespondensi ini akan langsung.  Misalnya, jika kita memiliki urutan teks input 200 karakter dan spektrogram 800 frame yang sesuai, maka akan ada 4 frame untuk setiap karakter.  Namun, Anda harus mengakui bahwa pidato yang dihasilkan berdasarkan spektogram semacam itu akan sepenuhnya tanpa kealamian.  Kami mengucapkan beberapa kata lebih cepat, beberapa lebih lambat, di suatu tempat kami jeda, tetapi di suatu tempat kami tidak.  Dan menganggap semua konteks yang mungkin tidak mungkin.  Itulah sebabnya perhatian adalah elemen kunci dari keseluruhan sistem: ia mengatur korespondensi antara langkah dekoder dan informasi dari pembuat enkode untuk mendapatkan informasi yang diperlukan untuk menghasilkan bingkai tertentu.  Dan semakin besar nilai bobot perhatian, semakin "perhatian harus dibayarkan" ke bagian yang sesuai dari data enkoder saat membuat bingkai spektrogram. <br><br>  Pada tahap pelatihan, ini juga akan berguna untuk menghasilkan audio, dan tidak hanya secara visual mengevaluasi kualitas spektogram dan perhatian.  Namun, mereka yang telah bekerja dengan WaveNet akan setuju bahwa menggunakannya sebagai vocoder pada tahap pelatihan akan menjadi kemewahan yang tidak dapat diterima dalam hal waktu.  Oleh karena itu, disarankan untuk menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">algoritma Griffin-Lim</a> , yang memungkinkan rekonstruksi sebagian sinyal setelah transformasi Fourier yang cepat.  Kenapa sebagian?  Faktanya adalah bahwa ketika kita mengubah sinyal menjadi spektrogram, kita kehilangan informasi fase.  Namun, kualitas audio yang diperoleh akan cukup untuk memahami ke arah mana Anda bergerak. <br><br><h2>  Pelajaran yang dipetik </h2><br>  Di sini kita akan berbagi beberapa pemikiran tentang membangun proses pengembangan, mengirimkannya dalam format tips.  Beberapa dari mereka cukup umum, yang lain lebih spesifik. <br><br>  <b>Tentang organisasi alur kerja</b> : <br><br><ul><li>  Gunakan sistem kontrol versi, jelaskan dan jelaskan semua perubahan.  Ini mungkin tampak seperti rekomendasi yang jelas, tetapi tetap saja.  Saat mencari arsitektur optimal, perubahan terus-menerus terjadi.  Dan setelah menerima beberapa hasil perantara yang memuaskan, pastikan untuk membuat diri Anda sebagai pos pemeriksaan sehingga Anda dapat dengan aman melakukan perubahan berikutnya. <br></li><li>  Dari sudut pandang kami, dalam arsitektur seperti itu orang harus mematuhi prinsip-prinsip enkapsulasi: satu kelas - satu modul Python.  Pendekatan ini tidak umum dalam tugas ML, tetapi ini akan membantu Anda menyusun kode dan mempercepat proses debug dan pengembangan.  Baik dalam kode dan visi arsitektur Anda, membaginya menjadi blok, blok menjadi modul, dan modul menjadi beberapa lapisan.  Jika modul memiliki kode yang menjalankan peran tertentu, maka gabungkan menjadi metode kelas modul.  Ini adalah kebenaran umum, tetapi kami tidak terlalu malas untuk mengatakannya lagi. <br></li><li>  Berikan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi bergaya numpy</a> dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> .  Ini akan sangat menyederhanakan pekerjaan untuk Anda dan kolega Anda yang akan membaca kode Anda. <br></li><li>  Selalu gambar arsitektur model Anda.  Pertama, ini akan membantu Anda memahaminya, dan kedua, pandangan sisi arsitektur dan hiperparameter model akan memungkinkan Anda untuk dengan cepat mengidentifikasi ketidakakuratan dalam pendekatan Anda. <br></li><li>  Lebih baik bekerja sebagai tim.  Jika Anda bekerja sendirian, tetap kumpulkan rekan kerja dan diskusikan pekerjaan Anda.  Paling tidak, mereka dapat mengajukan pertanyaan yang akan mengarahkan Anda pada beberapa pemikiran, tetapi secara maksimal mereka akan menunjuk ke ketidakakuratan spesifik yang tidak memungkinkan Anda untuk berhasil melatih model. <br></li><li>  Trik lain yang bermanfaat sudah dikaitkan dengan preprocessing data.  Misalkan Anda memutuskan untuk menguji beberapa hipotesis dan membuat perubahan yang sesuai dengan model.  Tetapi memulai kembali pelatihan, terutama sebelum akhir pekan, akan berisiko.  Pendekatan awalnya mungkin salah dan Anda akan membuang waktu.  Lalu apa yang harus dilakukan?  Menambah ukuran jendela Fast Fourier Transform.  Parameter default adalah 1024;  tingkatkan 4, atau bahkan 8 kali.  Ini akan "memeras" spektogram dalam jumlah waktu yang tepat dan sangat mempercepat pembelajaran.  Audio yang dipulihkan dari mereka akan memiliki kualitas yang lebih rendah, tetapi ini bukan tugas Anda sekarang?  Dalam 2-3 jam Anda sudah bisa mendapatkan keselarasan ("alignment" dari skala perhatian, seperti yang ditunjukkan pada gambar di atas), ini akan menunjukkan kebenaran arsitektur dari pendekatan dan itu sudah dapat diuji pada data besar. <br></li></ul><br>  <b>Model bangunan dan pelatihan</b> : <br><br><ul><li>  Kami berhipotesis bahwa jika bets tidak dibentuk secara acak, tetapi berdasarkan panjangnya, mereka akan mempercepat proses pelatihan model dan membuat spektogram yang dihasilkan lebih baik.  Asumsi logis, yang didasarkan pada hipotesis bahwa semakin banyak sinyal yang bermanfaat (dan bukan padding) diumpankan ke jaringan pelatihan, semakin baik.  Namun, pendekatan ini tidak membenarkan dirinya sendiri, dalam percobaan kami, kami tidak dapat melatih jaringan dengan cara ini.  Ini mungkin karena hilangnya keacakan dalam pemilihan contoh untuk pelatihan. <br></li><li>  Gunakan algoritma inisialisasi parameter jaringan modern dengan beberapa kondisi awal yang dioptimalkan.  Misalnya, dalam percobaan kami, kami menggunakan Inisialisasi Berat Seragam Xavier.  Jika dalam modul Anda, Anda perlu menggunakan normalisasi dengan mini-batch dan beberapa fungsi aktivasi, maka mereka harus bergantian satu sama lain dalam urutan ini.  Memang, jika kita menerapkan, misalnya, aktivasi ReLU, maka kita segera kehilangan semua sinyal negatif yang harus terlibat dalam proses normalisasi data batch tertentu. <br></li><li>  Dari langkah pembelajaran tertentu, gunakan tingkat pembelajaran yang dinamis.  Ini sangat membantu mengurangi nilai kesalahan dan meningkatkan kualitas spektogram yang dihasilkan. <br></li><li>  Setelah membuat model dan upaya yang gagal untuk melatihnya pada batch dari seluruh kumpulan data, akan berguna untuk mencoba melatihnya pada satu batch.    ,   alignment,           (    ).  ,      ,      . <br><br>    .        .  ,        â€“      .    ,            .       ,       . </li><li>    RNN-              .      . ,           .        ?             LSTM-     -. <br></li><li>       ,   LSTM-,      Â« Â»: Â« <i>       ,         LSTM-.      Â«Â»  bf.   ,        ,    ,   LSTM-     ft  1/2.   ,        :    ,        Â«Â»  1/2,         .    bf    ,  1   2:     ft                 </i> Â». <br></li><li>   seq2seq-         .       â€”       ,         .           ?           ,        ( ). <br></li><li> Sekarang rekomendasi spesifik untuk kerangka PyTorch.  Meskipun lapisan LSTM dalam dekoder, pada kenyataannya, adalah sel LSTM-nya, yang menerima informasi hanya untuk satu elemen urutan pada setiap langkah dekoder, disarankan untuk menggunakan kelas <code>torch.nn.LSTMCell</code> daripada <code>torch.nn.LSTMCell</code> .  Alasannya adalah bahwa backend LSTM diimplementasikan di perpustakaan CUDNN di C, dan LSTMCell dalam Python.  Trik ini akan memungkinkan Anda untuk meningkatkan kecepatan sistem secara signifikan. </li></ul><br>  Dan di akhir artikel, kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">akan membagikan contoh-contoh generasi bicara dari teks-teks yang tidak terkandung dalam set pelatihan.</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id436312/">https://habr.com/ru/post/id436312/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id436302/index.html">Pengalaman substitusi impor nyata menggunakan sistem penyimpanan AERODISK Rusia</a></li>
<li><a href="../id436304/index.html">Zimbra Collaboration Suite dan perjuangan melawan phishing</a></li>
<li><a href="../id436306/index.html">Pembelajaran Mesin untuk Vertica</a></li>
<li><a href="../id436308/index.html">Rostelecom dapat menjadi perusahaan monopoli di pasar pusat data</a></li>
<li><a href="../id436310/index.html">Seperti yang dilakukan metrik Ivan, DevOps. Objek pengaruh</a></li>
<li><a href="../id436314/index.html">Hotel robo-Jepang "menembak" setengah dari robot mereka karena masalah yang mereka buat</a></li>
<li><a href="../id436316/index.html">Bagaimana kartu pintar membantu mengarahkan proyek TI</a></li>
<li><a href="../id436318/index.html">Fitur Otomasi Jaringan Baru di Red Hat Ansible</a></li>
<li><a href="../id436320/index.html">Banyak properti atau properti-objek: kriteria pemilihan</a></li>
<li><a href="../id436322/index.html">@Pythonetc Desember 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>