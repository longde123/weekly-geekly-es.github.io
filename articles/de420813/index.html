<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§´ üíí ü§™ Backstage-Netzwerke bei Kubernetes üí¶ üññüèΩ ‚ú®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hinweis perev. : Der Autor des Originalartikels, Nicolas Leiva, ist ein L√∂sungsarchitekt von Cisco, der beschlossen hat, seinen Kollegen, Netzwerkinge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Backstage-Netzwerke bei Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Hinweis</b></i>  <i><b>perev.</b></i>  <i>: Der Autor des Originalartikels, Nicolas Leiva, ist ein L√∂sungsarchitekt von Cisco, der beschlossen hat, seinen Kollegen, Netzwerkingenieuren, die Funktionsweise des Kubernetes-Netzwerks von innen mitzuteilen.</i>  <i>Zu diesem Zweck untersucht er die einfachste Konfiguration im Cluster und nutzt dabei aktiv den gesunden Menschenverstand, sein Wissen √ºber Netzwerke und Standard-Linux / Kubernetes-Dienstprogramme.</i>  <i>Es stellte sich volumin√∂s heraus, aber sehr deutlich.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  Zus√§tzlich zu der Tatsache, dass Kelsey Hightowers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes The Hard Way-</a> Handbuch nur funktioniert ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sogar unter AWS!</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gefiel</a> mir, dass das Netzwerk sauber und einfach gehalten wurde.  Dies ist eine gro√üartige Gelegenheit, um beispielsweise die Rolle der Container Network Interface ( <a href="">CNI</a> ) zu verstehen.  Trotzdem m√∂chte ich hinzuf√ºgen, dass das Kubernetes-Netzwerk eigentlich nicht sehr intuitiv ist, insbesondere f√ºr Anf√§nger ... und vergessen Sie auch nicht, dass es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einfach kein</a> Netzwerk f√ºr Container gibt. <a name="habracut"></a><br><br>  Obwohl es zu diesem Thema bereits gute Materialien gibt (siehe Links <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> ), konnte ich kein solches Beispiel finden, dass ich alles Notwendige mit den Schlussfolgerungen der Teams kombinieren w√ºrde, die Netzwerktechniker lieben und hassen, um zu demonstrieren, was tats√§chlich hinter den Kulissen passiert.  Aus diesem Grund habe ich beschlossen, Informationen aus vielen Quellen zu sammeln. Ich hoffe, dies hilft und Sie verstehen besser, wie alles miteinander verbunden ist.  Dieses Wissen ist nicht nur wichtig, um sich selbst zu testen, sondern auch, um die Diagnose von Problemen zu vereinfachen.  Sie k√∂nnen dem Beispiel in Ihrem Cluster von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes The Hard Way</a> folgen: Alle IP-Adressen und Einstellungen werden von dort √ºbernommen (Stand der Commits f√ºr Mai 2018, bevor <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nabla-Container verwendet werden</a> ). <br><br>  Und wir werden am Ende beginnen, wenn wir drei Controller und drei Arbeitsknoten haben: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  Sie k√∂nnen feststellen, dass es hier auch mindestens drei private Subnetze gibt!  Ein wenig Geduld, und sie werden alle ber√ºcksichtigt.  Denken Sie daran, dass wir uns zwar auf sehr spezifische IP-Pr√§fixe beziehen, diese jedoch einfach aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes The Hard Way stammen</a> , sodass sie nur lokale Bedeutung haben und Sie gem√§√ü <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RFC 1918</a> einen anderen Adressblock f√ºr Ihre Umgebung ausw√§hlen k√∂nnen.  F√ºr den Fall von IPv6 wird es einen separaten Blog-Artikel geben. <br><br><h2>  Host-Netzwerk (10.240.0.0/24) </h2><br>  Dies ist ein internes Netzwerk, zu dem alle Knoten geh√∂ren.  Definiert durch das <code>--private-network-ip</code> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GCP</a> oder die Option <code>--private-ip-address</code> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS</a> beim <code>--private-ip-address</code> Computerressourcen. <br><br><h3>  Initialisieren von Controller-Knoten in GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Initialisieren von Controller-Knoten in AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Jede Instanz hat zwei IP-Adressen: privat vom Host-Netzwerk (Controller - <code>10.240.0.1${i}/24</code> , Worker - <code>10.240.0.2${i}/24</code> ) und eine √∂ffentliche, vom Cloud-Anbieter festgelegte, √ºber die wir sp√§ter sprechen werden wie man zu <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Alle Knoten m√ºssen in der Lage sein, sich gegenseitig zu pingen, wenn die <a href="">Sicherheitsrichtlinien korrekt sind</a> (und wenn <code>ping</code> auf dem Host installiert <code>ping</code> ). <br><br><h2>  Herdnetzwerk (10.200.0.0/16) </h2><br>  Dies ist das Netzwerk, in dem die Pods leben.  Jeder Arbeitsknoten verwendet ein Subnetz dieses Netzwerks.  In unserem Fall ist <code>POD_CIDR=10.200.${i}.0/24</code> f√ºr <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Um zu verstehen, wie alles konfiguriert ist, treten Sie einen Schritt zur√ºck und sehen Sie sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das Kubernetes-Netzwerkmodell an</a> , f√ºr das Folgendes erforderlich ist: <br><br><ul><li>  Alle Container k√∂nnen ohne Verwendung von NAT mit anderen Containern kommunizieren. </li><li>  Alle Knoten k√∂nnen mit allen Containern kommunizieren (und umgekehrt), ohne NAT zu verwenden. </li><li>  Die IP, die der Container sieht, muss mit der IP-Adresse √ºbereinstimmen, die andere sehen. </li></ul><br>  All dies kann auf viele Arten implementiert werden, und Kubernetes √ºbergibt das Netzwerk-Setup an das <a href="">CNI-Plugin</a> . <br><br><blockquote>  ‚ÄûDas CNI-Plugin ist daf√ºr verantwortlich, dem <b>Netzwerk-Namespace</b> des Containers eine Netzwerkschnittstelle hinzuzuf√ºgen (z. B. ein Ende eines <b>veth-Paares</b> ) und die erforderlichen √Ñnderungen auf dem Host vorzunehmen (z. B. das zweite Ende von veth mit einer Bridge zu verbinden).  Dann muss er eine IP-Schnittstelle zuweisen und die Routen gem√§√ü dem Abschnitt IP-Adressverwaltung konfigurieren, indem er das gew√ºnschte IPAM-Plugin aufruft. ‚Äú  <i>(aus der <a href="">Spezifikation der Containernetzwerkschnittstelle</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Netzwerk-Namespace </h3><br><blockquote>  ‚ÄûDer Namespace verpackt die globale Systemressource in eine Abstraktion, die f√ºr Prozesse in diesem Namespace so sichtbar ist, dass sie √ºber eine eigene isolierte Instanz der globalen Ressource verf√ºgen.  √Ñnderungen in der globalen Ressource sind f√ºr andere in diesem Namespace enthaltene Prozesse sichtbar, f√ºr andere Prozesse jedoch nicht. ‚Äú  <i>( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von der Namespaces-Manpage</a> )</i> </blockquote><br>  Linux bietet sieben verschiedene Namespaces ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Netzwerk-Namespaces ( <code>CLONE_NEWNET</code> ) definieren die Netzwerkressourcen, die dem Prozess zur Verf√ºgung stehen: ‚ÄûJeder Netzwerk-Namespace verf√ºgt √ºber eigene Netzwerkger√§te, IP-Adressen, IP-Routing-Tabellen, <code>/proc/net</code> Verzeichnis, Portnummern usw.‚Äú <i>( aus dem Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Namespaces in Betrieb</a> ‚Äú)</i> . <br><br><h3>  Virtuelle Ethernet-Ger√§te (Veth) </h3><br><blockquote>  ‚ÄûEin virtuelles Netzwerkpaar (veth) bietet eine Abstraktion in Form einer‚Äû Pipe ‚Äú, mit der Tunnel zwischen Netzwerk-Namespaces oder eine Br√ºcke zu einem physischen Netzwerkger√§t in einem anderen Netzwerkbereich erstellt werden k√∂nnen.  Wenn der Namespace freigegeben wird, werden alle darin enthaltenen Ger√§te zerst√∂rt. ‚Äú  <i>(von der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Manpage</a> der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-Namespaces</a> )</i> </blockquote><br>  Gehen Sie zu Boden und sehen Sie, wie sich alles auf den Cluster bezieht.  Erstens sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-Plugins</a> in Kubernetes vielf√§ltig, und CNI-Plugins sind eines davon ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">warum nicht CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubelet</a> auf jedem Knoten teilt der Container- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laufzeit mit,</a> welches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-Plug-In</a> verwendet werden soll.  Die Container Network Interface ( <a href="">CNI</a> ) befindet sich zwischen der Container-Laufzeit und der Netzwerkimplementierung.  Und schon baut das CNI-Plugin das Netzwerk auf. <br><br><blockquote>  ‚ÄûDas CNI-Plugin wird ausgew√§hlt, indem die Befehlszeilenoption <code>--network-plugin=cni</code> an Kubelet √ºbergeben wird.  Kubelet liest die Datei aus <code>--cni-conf-dir</code> (der Standardwert ist <code>/etc/cni/net.d</code> ) und verwendet die CNI-Konfiguration aus dieser Datei, um das Netzwerk f√ºr jede Datei zu konfigurieren. "  <i>(aus den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anforderungen</a> des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-Plugins</a> )</i> </blockquote><br>  Die realen Bin√§rdateien des CNI-Plugins befinden sich in <code>-- cni-bin-dir</code> (der Standardwert ist <code>/opt/cni/bin</code> ). <br><br>  Bitte beachten Sie, dass die <a href=""><code>kubelet.service</code></a> von <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  Zun√§chst erstellt Kubernetes einen Netzwerk-Namespace f√ºr den Herd, noch bevor Plugins aufgerufen werden.  Dies wird mithilfe des speziellen <code>pause</code> implementiert, der ‚Äûals‚Äû √ºbergeordneter Container ‚Äúf√ºr alle Herdcontainer dient‚Äú <i>(aus dem Artikel ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der allm√§chtige Pausencontainer</a> ‚Äú)</i> .  Kubernetes f√ºhrt dann das CNI-Plugin aus, um den <code>pause</code> an das Netzwerk anzuh√§ngen.  Alle Pod-Container verwenden den <code>netns</code> dieses <code>netns</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  Die verwendete <a href="">CNI-Konfiguration</a> gibt die Verwendung des <code>bridge</code> Plugins zum Konfigurieren der Linux (L2) -Softwarebr√ºcke im Root-Namespace <code>cnio0</code> (der <a href="">Standardname</a> ist <code>cni0</code> ) an, der als Gateway fungiert ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Ein veth-Paar wird ebenfalls konfiguriert, um den Herd mit der neu erstellten Br√ºcke zu verbinden: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Um L3-Informationen wie IP-Adressen zuzuweisen, wird das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IPAM-Plugin</a> ( <code>ipam</code> ) aufgerufen.  In diesem Fall wird der <code>host-local</code> Typ verwendet, "der den Status lokal im Hostdateisystem speichert, wodurch die Eindeutigkeit der IP-Adressen auf einem Host sichergestellt wird" <i>(aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code> host-local</code></a> )</i> .  Das IPAM-Plugin gibt diese Informationen an das vorherige Plugin ( <code>bridge</code> ) zur√ºck, sodass alle in der Konfiguration angegebenen Routen konfiguriert werden k√∂nnen ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Wenn <code>gw</code> nicht angegeben ist, <a href="">wird es aus dem Subnetz √ºbernommen</a> .  Die Standardroute wird auch im Netzwerk-Namespace des Herdes konfiguriert und zeigt auf die Bridge (die als erstes IP-Subnetz des Herdes konfiguriert ist). <br><br>  Und das letzte wichtige Detail: Wir haben Maskerading ( <code>"ipMasq": true</code> ) f√ºr den Datenverkehr aus dem <code>"ipMasq": true</code> angefordert.  Wir brauchen NAT hier nicht wirklich, aber dies ist die Konfiguration in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes The Hard Way</a> .  Der Vollst√§ndigkeit halber muss ich daher erw√§hnen, dass die Eintr√§ge in den <code>iptables</code> <code>bridge</code> Plugins f√ºr dieses spezielle Beispiel konfiguriert sind.  Alle Pakete vom Herd, deren Empf√§nger nicht im Bereich <code>224.0.0.0/4</code> , befinden <a href="">sich hinter NAT</a> , was die Anforderung "Alle Container k√∂nnen mit jedem anderen Container ohne Verwendung von NAT kommunizieren" nicht ganz erf√ºllt.  Nun, wir werden beweisen, warum NAT nicht ben√∂tigt wird ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Herdf√ºhrung </h3><br>  Jetzt k√∂nnen wir die Pods anpassen.  Schauen wir uns alle Netzwerkbereiche der Namen eines der Arbeitsknoten an und analysieren Sie einen davon, nachdem Sie <a href="">von hier aus</a> die Bereitstellung von <code>nginx</code> <a href="">haben</a> .  Wir werden <code>lsns</code> mit der Option <code>-t</code> , um den gew√ºnschten Typ des Namespace (d. H. <code>net</code> ) auszuw√§hlen: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  Mit der Option <code>-i</code> zu <code>ls</code> k√∂nnen wir ihre Inode-Nummern finden: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Sie k√∂nnen auch alle Netzwerk-Namespaces mit <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Um alle Prozesse <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> , die im Netzwerkbereich <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>4026532426</code> ) ausgef√ºhrt werden, k√∂nnen Sie beispielsweise den folgenden Befehl ausf√ºhren: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  Es ist zu sehen, dass wir zus√§tzlich zur <code>pause</code> in diesem Pod Nginx gestartet haben.  Der <code>pause</code> teilt die <code>net</code> und <code>ipc</code> Namespaces mit allen anderen Pod-Containern.  Erinnern Sie sich an die PID aus <code>pause</code> - 27255;  wir werden darauf zur√ºckkommen. <br><br>  Nun wollen wir sehen, was <code>kubectl</code> √ºber diesen Pod erz√§hlt: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  Weitere Details: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Wir sehen den Namen des Pods - <code>nginx-65899c769f-wxdx6</code> - und die ID eines seiner Container ( <code>nginx</code> ), aber √ºber die <code>pause</code> nichts gesagt.  Graben Sie einen tieferen Arbeitsknoten, um alle Daten abzugleichen.  Denken Sie daran, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes The Hard Way</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Docker</a> nicht verwendet. Einzelheiten zum Container finden Sie im Konsolen-Dienstprogramm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Containerd</a> - ctr <i>(siehe auch Artikel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Integration von Containerd mit Kubernetes, Ersetzen von Docker, produktionsbereit</a> " - <b>ca. √úbertragung</b> ).</i> :: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Wenn Sie den Containerd- <code>k8s.io</code> ( <code>k8s.io</code> ) kennen, k√∂nnen Sie die <code>nginx</code> Container-ID <code>k8s.io</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... und auch <code>pause</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  Die <code>nginx</code> Container-ID mit der Endung <code>‚Ä¶983c7</code> stimmt mit der von <code>kubectl</code> .  Mal sehen, ob wir herausfinden k√∂nnen, welcher <code>pause</code> zum <code>nginx</code> Pod geh√∂rt: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  Denken Sie daran, dass Prozesse mit PID 27331 und 27355 im Netzwerk-Namespace <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> werden. <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... und: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Jetzt wissen wir sicher, welche Container in diesem Pod ( <code>nginx-65899c769f-wxdx6</code> ) und im Netzwerk-Namespace ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ) ausgef√ºhrt werden: <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  Pause (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  Wie ist dies unter ( <code>nginx-65899c769f-wxdx6</code> ) mit dem Netzwerk verbunden?  Wir verwenden die zuvor von <code>pause</code> empfangene PID 27255, um Befehle in ihrem Netzwerk-Namespace <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  F√ºr diese Zwecke verwenden wir <code>nsenter</code> mit der Option <code>-t</code> , die die Ziel-PID definiert, und <code>-n</code> ohne eine Datei anzugeben, um in den Netzwerk-Namespace des <code>nsenter</code> zu gelangen (27255).  Hier ist, was <code>ip link show</code> sagen wird: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... und <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Dies best√§tigt, dass die zuvor √ºber <code>kubectl get pod</code> erhaltene IP-Adresse auf der <code>eth0</code> Schnittstelle konfiguriert ist.  Diese Schnittstelle ist Teil eines <b>Veth-Paares</b> , von dem sich ein Ende im Herd und das andere im Root-Namespace befindet.  Um die Schnittstelle des zweiten Endes herauszufinden, verwenden wir <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Wir sehen, dass der <code>ifindex</code> Festes 7 ist. √úberpr√ºfen Sie, ob er sich im Root-Namespace befindet.  Dies kann √ºber den <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Um sicherzugehen, lassen Sie uns sehen: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  Gro√üartig, jetzt ist mit dem virtuellen Link alles klar.  <code>brctl</code> Sie uns mit <code>brctl</code> sehen, wer noch mit der Linux-Bridge verbunden ist: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Das Bild sieht also wie folgt aus: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Routing-Pr√ºfung </h3><br>  Wie leiten wir den Verkehr tats√§chlich weiter?  Schauen wir uns die Routing-Tabelle im Netzwerk-Namespace-Pod an: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Zumindest wissen wir, wie man zum Root-Namespace kommt ( <code>default via 10.200.0.1</code> ).  Nun sehen wir uns die Host-Routing-Tabelle an: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Wir wissen, wie Pakete an einen VPC-Router weitergeleitet werden (VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verf√ºgt √ºber einen</a> ‚Äûimpliziten‚Äú Router, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">normalerweise eine zweite Adresse</a> aus dem Haupt-IP-Adressraum des Subnetzes hat).  Jetzt: Wei√ü der VPC-Router, wie er zum Netzwerk jedes Herdes gelangt?  Nein, das tut er nicht. Daher wird davon ausgegangen, dass die Routen vom CNI-Plug-In oder <a href="">manuell</a> (wie im Handbuch) konfiguriert werden.  Anscheinend macht das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AWS CNI-Plugin</a> genau das f√ºr uns bei AWS.  Denken Sie daran, dass es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele CNI-Plugins gibt</a> , und wir betrachten ein Beispiel f√ºr eine <b>einfache Netzwerkkonfiguration</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Tiefes Eintauchen in NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> zwei identische <code>busybox</code> Container mit Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Wir bekommen: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Pings von einem Container zum anderen sollten erfolgreich sein: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Um die Bewegung des Datenverkehrs zu verstehen, k√∂nnen Sie Pakete mit <code>tcpdump</code> oder <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  Die Quell-IP von Pod 10.200.0.21 wird in die IP-Adresse des Hosts 10.240.0.20 √ºbersetzt. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  In iptables k√∂nnen Sie sehen, dass die Anzahl steigt: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  Wenn Sie andererseits <code>"ipMasq": true</code> aus der CNI-Plugin-Konfiguration entfernen, sehen Sie Folgendes (dieser Vorgang wird ausschlie√ülich zu Bildungszwecken ausgef√ºhrt - wir empfehlen, die Konfiguration in einem funktionierenden Cluster nicht zu √§ndern!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping sollte noch bestehen: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  Und in diesem Fall - ohne NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Daher haben wir √ºberpr√ºft, dass "alle Container ohne Verwendung von NAT mit anderen Containern kommunizieren k√∂nnen". <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Clusternetzwerk (10.32.0.0/24) </h2><br>  M√∂glicherweise haben Sie im <code>busybox</code> Beispiel <code>busybox</code> dass die der <code>busybox</code> zugewiesenen IP-Adressen jeweils <code>busybox</code> waren.  Was w√§re, wenn wir diese Container f√ºr die Kommunikation von anderen Herden zur Verf√ºgung stellen wollten?  Man k√∂nnte die aktuellen IP-Adressen des Pods nehmen, aber sie werden sich √§ndern.  Aus diesem Grund m√ºssen Sie die Serviceressource konfigurieren, die Anfragen an viele kurzlebige Herde weiterleitet. <br><br><blockquote>  "Service in Kubernetes ist eine Abstraktion, die den logischen Satz von Herden und die Richtlinien definiert, √ºber die auf sie zugegriffen werden kann."  <i>(aus der Dokumentation zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Services</a> )</i> </blockquote><br>  Es gibt verschiedene M√∂glichkeiten, einen Dienst zu ver√∂ffentlichen.  Der Standardtyp ist <code>ClusterIP</code> , der die IP-Adresse aus dem CIDR-Block des Clusters festlegt (d. h. nur √ºber den Cluster zug√§nglich).  Ein solches Beispiel ist das in Kubernetes The Hard Way konfigurierte DNS-Cluster-Add-on. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> zeigt, dass der <code>Service</code> Endpunkte <code>kubectl</code> und √ºbersetzt: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  Wie genau? .. wieder <code>iptables</code> .  Lassen Sie uns die f√ºr dieses Beispiel erstellten Regeln durchgehen.  Ihre vollst√§ndige Liste kann mit dem Befehl <code>iptables-save</code> angezeigt werden. <br><br>  Sobald Pakete vom Prozess erstellt werden ( <code>OUTPUT</code> ) oder auf der Netzwerkschnittstelle ankommen ( <code>PREROUTING</code> ), durchlaufen sie die folgenden <code>iptables</code> Ketten: <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Die folgenden Ziele entsprechen TCP-Paketen, die um 10.32.0.10 an den 53. Port gesendet und mit dem 53. Port an den Empf√§nger 10.200.0.27 √ºbertragen werden: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Gleiches gilt f√ºr UDP-Pakete (Empf√§nger 10.32.0.10:53 ‚Üí 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Es gibt andere Arten von <code>Services</code> in Kubernetes.  Insbesondere <code>NodePort</code> Kubernetes The Hard Way √ºber <code>NodePort</code> - siehe <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> ver√∂ffentlicht den Dienst unter der IP-Adresse jedes Knotens und platziert ihn an einem statischen Port (er hei√üt <code>NodePort</code> ).  <code>NodePort</code> kann <code>NodePort</code> von au√üerhalb des Clusters zugegriffen werden.  Sie k√∂nnen den dedizierten Port (in diesem Fall - 31088) mit <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under ist jetzt im Internet unter <code>http://${EXTERNAL_IP}:31088/</code> .  Hier ist <code>EXTERNAL_IP</code> die √∂ffentliche IP-Adresse <b>einer Arbeitsinstanz</b> .  In diesem Beispiel habe ich die √∂ffentliche IP-Adresse von <b>worker-0 verwendet</b> .  Die Anforderung wird von einem Host mit einer internen IP-Adresse von 10.240.0.20 empfangen (der Cloud-Anbieter ist an √∂ffentlichem NAT beteiligt). Der Dienst wird jedoch tats√§chlich auf einem anderen Host gestartet ( <b>Worker-1</b> , erkennbar an der IP-Adresse des Endpunkts - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  Das Paket wird von <b>Worker-0</b> an <b>Worker-1</b> gesendet, wo es seinen Empf√§nger findet: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Ist eine solche Schaltung ideal?  Vielleicht nicht, aber es funktioniert.  In diesem Fall lauten die programmierten <code>iptables</code> Regeln wie folgt: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  Mit anderen Worten, die Adresse f√ºr den Empf√§nger von Paketen mit Port 31088 wird am 10.200.1.18 gesendet.  Der Port sendet auch von 31088 bis 80. <br><br>  Wir haben keinen anderen <code>LoadBalancer</code> - <code>LoadBalancer</code> - der den Dienst mithilfe eines Load Balancers eines Cloud-Anbieters √∂ffentlich verf√ºgbar macht, aber der Artikel hat sich bereits als umfangreich herausgestellt. <br><br><h2>  Fazit </h2><br>  Es scheint, dass es viele Informationen gibt, aber wir haben nur die Spitze des Eisbergs ber√ºhrt.  In Zukunft werde ich √ºber IPv6, IPVS, eBPF und einige interessante aktuelle CNI-Plugins sprechen. <br><br><h2>  PS vom √úbersetzer </h2><br>  Lesen Sie auch in unserem Blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Illustrierter Leitfaden zur Vernetzung bei Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vergleich der Netzwerkleistung f√ºr Kubernetes</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experimente mit Kube-Proxy und Host-Unzug√§nglichkeit in Kubernetes</a> "; </li><li>  ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbesserung der Zuverl√§ssigkeit von Kubernetes: Wie kann man schnell feststellen, dass ein Knoten gefallen ist?</a> ‚Äú; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Play with Kubernetes ‚Äî      K8s</a> ¬ª; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   Kubernetes   </a> ¬ª <i>( ,        Kubernetes)</i> ; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Container Networking Interface (CNI) ‚Äî      Linux-</a> ¬ª. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de420813/">https://habr.com/ru/post/de420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de420799/index.html">MPS 2018.2: Generatortests, GitHub-Plugin, VCS-Aspekt, Migrationsbenachrichtigungen und mehr</a></li>
<li><a href="../de420803/index.html">3D-Druckunterricht. Speichern von Kunststoff beim Drucken nicht funktionsf√§higer Modelle aus 3Dtool</a></li>
<li><a href="../de420805/index.html">[√úbersetzung] Wann werden parallele Streams verwendet?</a></li>
<li><a href="../de420809/index.html">Sicherheitswoche 31: F√ºnfzig Schattierungen von Unsicherheit unter Android</a></li>
<li><a href="../de420811/index.html">Dezentrales Messenger- und Telefonnetz der neuen Generation</a></li>
<li><a href="../de420815/index.html">Wie "die digitale Welt entschl√ºsseln" die Halle in die Luft jagte: Top 10 Berichte von DotNext 2018 Piter</a></li>
<li><a href="../de420819/index.html">Top 10 Python-Tools f√ºr maschinelles Lernen und Datenwissenschaft</a></li>
<li><a href="../de420821/index.html">Regel 10: 1 beim Programmieren und Schreiben</a></li>
<li><a href="../de420825/index.html">Heute ist das erste Spiel zwischen OpenAI- und Dota 2-Profis (Menschen haben gewonnen). Wir verstehen, wie der Bot funktioniert</a></li>
<li><a href="../de420827/index.html">Erstellen Sie ein einfaches Maven-Projekt mit Java EE + WildFly10 + JPA (Ruhezustand) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>