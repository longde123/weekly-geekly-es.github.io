<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí™üèº üï∞Ô∏è üõåüèæ Les r√©seaux de neurones pr√©f√®rent les textures et comment y faire face. üë©üèº‚Äçü§ù‚Äçüë®üèæ üë©‚Äçüë¶‚Äçüë¶ ü§±üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="R√©cemment, plusieurs articles ont critiqu√© ImageNet, peut-√™tre l'ensemble d'images le plus c√©l√®bre utilis√© pour entra√Æner les r√©seaux de neurones. 


...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les r√©seaux de neurones pr√©f√®rent les textures et comment y faire face.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  R√©cemment, plusieurs articles ont critiqu√© ImageNet, peut-√™tre l'ensemble d'images le plus c√©l√®bre utilis√© pour entra√Æner les r√©seaux de neurones. </p><br><p>  Dans le premier article, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Approximation des CNN avec des mod√®les de caract√©ristiques de sac local fonctionne √©tonnamment bien sur ImageNet, les</a> auteurs prennent un mod√®le similaire √† un sac de mots et utilisent des fragments de l'image comme ¬´mots¬ª.  Ces fragments peuvent mesurer jusqu'√† 9x9 pixels.  Et en m√™me temps, sur un tel mod√®le, o√π toute information sur la disposition spatiale de ces fragments est compl√®tement absente, les auteurs obtiennent une pr√©cision de 70 √† 86% (par exemple, la pr√©cision d'un ResNet-50 r√©gulier est de ~ 93%). </p><br><p>  Dans le deuxi√®me article des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CNN form√©s par ImageNet sont biais√©s vers la texture, les</a> auteurs concluent que l'ensemble de donn√©es ImageNet lui-m√™me et la fa√ßon dont les personnes et les r√©seaux de neurones per√ßoivent les images en sont coupables et sugg√®rent d'utiliser un nouvel ensemble de donn√©es - Stylized-ImageNet. </p><br><p>  Plus en d√©tail sur ce que les gens voient sur les images et sur les r√©seaux de neurones <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  L'ensemble de donn√©es ImageNet a commenc√© √† √™tre cr√©√© en 2006 par les efforts du professeur Fei-Fei Li et continue d'√©voluer √† ce jour.  √Ä l'heure actuelle, il contient environ 14 millions d'images appartenant √† plus de 20 000 cat√©gories diff√©rentes. </p><br><p>  Depuis 2010, un sous-ensemble de cet ensemble de donn√©es, connu sous le nom d'ImageNet 1K avec environ 1 million d'images et des milliers de classes, a √©t√© utilis√© dans le cadre du d√©fi de reconnaissance visuelle √† grande √©chelle d'ImageNet (ILSVRC).  Dans ce concours, en 2012, AlexNet, un r√©seau de neurones √† convolution, a tir√© avec une pr√©cision de 60% dans le top 1 et de 80% dans le top 5. <br>  C'est sur ce sous-ensemble de l'ensemble de donn√©es que les personnes du milieu universitaire mesurent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">leur SOTA</a> lorsqu'elles proposent de nouvelles architectures de r√©seau. </p><br><p>  Un peu sur le processus d'apprentissage sur cet ensemble de donn√©es.  Nous parlerons du protocole de formation sur ImageNet en milieu acad√©mique.  Autrement dit, lorsque les r√©sultats d'un r√©seau SE block, ResNeXt ou DenseNet nous sont pr√©sent√©s dans l'article, le processus ressemble √† ceci: le r√©seau apprend pendant 90 √©poques, la vitesse d'apprentissage diminue des 30e et 60e √©poques, toutes les 10 fois, en tant qu'optimiseur un SGD ordinaire avec une petite d√©croissance de poids est s√©lectionn√©, seuls RandomCrop et HorizontalFlip sont utilis√©s √† partir des augmentations, l'image est g√©n√©ralement redimensionn√©e √† 224x224 pixels. </p><br><p>  Voici un exemple de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">script pytorch</a> pour la formation sur ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Revenons aux articles mentionn√©s pr√©c√©demment.  Dans le premier d'entre eux, les auteurs voulaient un mod√®le plus facile √† interpr√©ter que les r√©seaux profonds ordinaires.  Inspir√©s par l'id√©e des mod√®les de sac de fonctionnalit√©, ils cr√©ent leur propre famille de mod√®les - BagNets.  En utilisant comme base le r√©seau ResNet-50 habituel. </p><br><p>  Rempla√ßant certaines convolutions 3x3 par 1x1 dans ResNet-50, ils garantissent que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">champ r√©cepteur des</a> neurones sur la derni√®re couche convolutionnelle est consid√©rablement r√©duit, jusqu'√† 9x9 pixels.  Ainsi, ils limitent les informations disponibles √† un neurone individuel √† un tr√®s petit fragment de l'image enti√®re - un patch de plusieurs pixels.  Il convient de noter que pour le ResNet-50 vierge, la taille du champ r√©cepteur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est sup√©rieure √† 400 pixels,</a> ce qui recouvre compl√®tement l'image, qui est g√©n√©ralement redimensionn√©e √† 224x224 pixels. </p><br><p>  Ce patch est le fragment <strong>maximum</strong> de l'image √† partir duquel le mod√®le pourrait extraire des donn√©es spatiales.  √Ä la fin du mod√®le, toutes les donn√©es √©taient simplement r√©sum√©es et le mod√®le ne pouvait en aucun cas savoir o√π se trouvait chaque patch par rapport aux autres patchs. <br>  Au total, trois variantes de r√©seaux √† champ r√©cepteur 9x9, 17x17 et 33x33 ont √©t√© test√©es.  Et, malgr√© le manque total d'informations spatiales, ces mod√®les ont pu obtenir une bonne pr√©cision dans la classification sur ImageNet.  La pr√©cision du Top 5 pour les patchs 9x9 √©tait de 70%, pour 17x17 - 80%, pour 33x33 - 86%.  A titre de comparaison, la pr√©cision ResNet-50 top-5 est d'environ 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  La structure du mod√®le est illustr√©e dans la figure ci-dessus.  Chaque patch de qxqx3 pixels d√©coup√©s dans l'image est transform√© en vecteur 2048 par le r√©seau. Ensuite, ce vecteur est aliment√© √† l'entr√©e d'un classificateur lin√©aire, qui produit des scores pour chacune des 1000 classes.  En collectant les scores de chaque patch dans un tableau 2D, vous pouvez obtenir une carte thermique pour chaque classe et chaque pixel de l'image d'origine.  Les notes finales pour l'image ont √©t√© obtenues en additionnant la carte thermique de chaque classe. </p><br><p>  Exemples de cartes thermiques pour certaines classes: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Cartes thermiques"><br>  Comme vous pouvez le voir, la plus grande contribution au b√©n√©fice d'une classe particuli√®re est apport√©e par les correctifs situ√©s sur les bords des objets.  Les correctifs de l'arri√®re-plan sont presque ignor√©s.  Jusqu'√† pr√©sent, tout va bien. </p><br><p>  Regardons les correctifs les plus informatifs: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="Patchs informatifs"></p><br><p>  Par exemple, les auteurs ont suivi quatre cours.  Pour chacun d'eux, 2x7 patchs les plus significatifs ont √©t√© s√©lectionn√©s (c'est-√†-dire les patchs o√π le score de cette classe √©tait le plus √©lev√©).  La rang√©e sup√©rieure de 7 patchs est extraite des images de la classe correspondante uniquement, celle du bas - de l'√©chantillon entier d'images. </p><br><p>  Ce que l'on peut voir sur ces photos est remarquable.  Par exemple, pour la classe des tanches (tanche, poisson), les doigts sont une caract√©ristique.  Oui, des doigts humains ordinaires sur fond vert.  Et tout cela parce qu'il y a un p√™cheur dans presque toutes les images avec cette classe, qui, en fait, tient ce poisson dans ses mains, exhibant un troph√©e. </p><br><div class="spoiler">  <b class="spoiler_title">Exemples d'ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Pour les ordinateurs portables, les touches de lettre sont une caract√©ristique.  Les cl√©s de machine √† √©crire comptent √©galement pour cette classe. </p><br><p>  Une caract√©ristique d'une couverture de livre est constitu√©e de lettres sur fond color√©.  Que ce soit m√™me une inscription sur un T-shirt ou sur un sac. </p><br><p>  Il semblerait que ce probl√®me ne devrait pas nous d√©ranger.  Puisqu'il n'est inh√©rent qu'√† une classe √©troite de r√©seaux avec un champ r√©cepteur tr√®s limit√©.  Mais en outre, les auteurs ont calcul√© la corr√©lation entre les logits (sorties r√©seau avant le softmax final) attribu√©s √† chaque classe BagNet avec un champ r√©cepteur diff√©rent, et les logits de VGG-16, qui a un champ r√©cepteur assez important.  Et ils l'ont trouv√©e assez √©lev√©e. </p><br><div class="spoiler">  <b class="spoiler_title">Corr√©lation entre BagNets et VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Les auteurs se sont demand√© si BagNet contient des indices sur la fa√ßon dont les autres r√©seaux prennent des d√©cisions. </p><br><p>  Pour l'un des tests, ils ont utilis√© une technique comme le brouillage d'image.  Ce qui a consist√© √† utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un g√©n√©rateur de texture bas√© sur des matrices de gramme</a> pour composer une image o√π les textures sont enregistr√©es, mais les informations spatiales manquent. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p>  VGG-16, form√© sur des images ordinaires √† part enti√®re, a plut√¥t bien g√©r√© ces images brouill√©es.  Sa pr√©cision dans le top 5 est pass√©e de 90% √† 80%.  C'est-√†-dire que m√™me les r√©seaux avec un champ r√©cepteur assez large pr√©f√®rent toujours se souvenir des textures et ignorer les informations spatiales.  Par cons√©quent, leur pr√©cision n'est pas tomb√©e lourdement sur les images brouill√©es. </p><br><p>  Les auteurs ont men√© une s√©rie d'exp√©riences o√π ils ont compar√© les parties des images les plus importantes pour BagNet et d'autres r√©seaux (VGG-16, ResNet-50, ResNet-152 et DenseNet-169).  Tout laissait entendre que d'autres r√©seaux, comme BagNet, s'appuient sur de petits fragments d'images et commettent √† peu pr√®s les m√™mes erreurs lors de la prise de d√©cisions.  Cela √©tait particuli√®rement visible pour les r√©seaux peu profonds tels que VGG. </p><br><p>  Cette tendance des r√©seaux √† prendre des d√©cisions bas√©es sur les textures, contrairement √† nous, les gens qui pr√©f√®rent la forme (voir la figure ci-dessous), a incit√© les auteurs du deuxi√®me article √† cr√©er un nouvel ensemble de donn√©es bas√© sur ImageNet. </p><br><h3 id="stylized-imagenet">  ImageNet stylis√© </h3><br><p>  Tout d'abord, les auteurs de l'article utilisant le transfert de style ont cr√©√© un ensemble d'images o√π la forme (donn√©es spatiales) et les textures d'une image se contredisaient.  Et nous avons compar√© les r√©sultats de personnes et de r√©seaux de convolution profonds de diff√©rentes architectures sur un ensemble de donn√©es synth√©tis√©es de 16 classes. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  Dans l'extr√™me droite, les gens voient un chat, un r√©seau - un √©l√©phant. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Comparaison des r√©sultats des personnes et des r√©seaux de neurones. </p><br><p>  Comme vous pouvez le voir, les gens lors de l'attribution d'un objet √† une classe particuli√®re se sont appuy√©s sur la forme des objets, les r√©seaux de neurones sur les textures.  Dans la figure ci-dessus, les gens ont vu un chat, un r√©seau - un √©l√©phant. </p><br><blockquote>  Oui, ici, vous pouvez trouver √† redire au fait que les filets sont √©galement un peu √† droite et cela, par exemple, pourrait √™tre un √©l√©phant photographi√© de pr√®s avec le tatouage d'un chat bien-aim√©.  Mais le fait que les r√©seaux lors de la prise de d√©cisions se comportent diff√©remment des gens, les auteurs ont consid√©r√© le probl√®me et ont commenc√© √† chercher des moyens de le r√©soudre. </blockquote><p>  Comme mentionn√© ci-dessus, en s'appuyant uniquement sur les textures, le r√©seau est capable d'obtenir un bon r√©sultat avec une pr√©cision de 86% dans le top 5.  Et il ne s'agit pas de plusieurs classes, o√π les textures aident √† classer correctement les images, mais de la plupart des classes. </p><br><p>  Le probl√®me est dans ImageNet lui-m√™me, car il sera montr√© plus tard que le r√©seau est capable d'apprendre la forme, mais ne le fait pas, car les textures sont suffisantes pour cet ensemble de donn√©es et les neurones responsables des textures sont sur des couches peu profondes, qui sont beaucoup plus faciles √† former. </p><br><p>  En utilisant cette fois un m√©canisme de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transfert de style AdaIN</a> l√©g√®rement diff√©rent, les auteurs ont cr√©√© un nouvel ensemble de donn√©es - Stylized ImageNet.  La forme des objets a √©t√© tir√©e d'ImageNet, et l'ensemble des textures de ce <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">concours sur Kaggle</a> .  Le script de g√©n√©ration est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le lien</a> . </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  De plus, par souci de concision, ImageNet sera appel√© <strong>IN</strong> , ImageNet stylis√© <strong>SIN</strong> . </p><br><p>  Les auteurs ont pris ResNet-50 et trois BagNet avec un champ r√©cepteur diff√©rent et se sont entra√Æn√©s sur un mod√®le distinct pour chacun des ensembles de donn√©es. </p><br><p>  Et voici ce qu'ils ont fait: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  Ce que nous voyons ici.  ResNet-50 form√© sur IN est compl√®tement inapte sur SIN.  Ce qui confirme en partie que lors de l'entra√Ænement sur IN, le r√©seau s'adapte aux textures et ignore la forme des objets.  Dans le m√™me temps, le ResNet-50 form√© sur SIN fait parfaitement face √† la fois √† SIN et √† IN.  Autrement dit, s'il est priv√© d'un chemin simple, le r√©seau suit un chemin difficile - il enseigne la forme des objets. <br>  BagNet a finalement commenc√© √† se comporter comme pr√©vu, en particulier sur les petits correctifs, car il n'a rien √† quoi se raccrocher - les informations de texture manquent simplement dans le NAS. </p><br><p>  Dans les seize classes mentionn√©es pr√©c√©demment, ResNet-50, form√© sur le NAS, a commenc√© √† donner des r√©ponses plus similaires √† celles que les gens donnent: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  En plus de simplement former ResNet-50 sur le SIN, les auteurs ont essay√© de former le r√©seau sur un ensemble mixte de SIN et IN, y compris un r√©glage fin s√©par√©ment sur le IN pur. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Comme vous pouvez le voir, lorsque vous utilisez SIN + IN pour la formation, les r√©sultats se sont am√©lior√©s non seulement sur la t√¢che principale - la classification des images sur ImageNet, mais √©galement sur la t√¢che de d√©tection des objets sur le jeu de donn√©es PASCAL VOC 2007. </p><br><p>  De plus, les r√©seaux form√©s au SIN sont devenus plus r√©sistants √† divers bruits dans les donn√©es. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  M√™me maintenant, en 2019, apr√®s sept ans de succ√®s avec AlexNet, lorsque les r√©seaux de neurones sont largement utilis√©s en vision par ordinateur, lorsque ImageNet 1K est devenu de facto la norme pour √©valuer les performances des mod√®les dans l'environnement universitaire, le m√©canisme de prise de d√©cision des r√©seaux de neurones n'est pas tout √† fait clair. .  Et comment les ensembles de donn√©es sur lesquels ces r√©seaux ont √©t√© form√©s influencent cela. </p><br><p>  Les auteurs du premier article ont tent√© de faire la lumi√®re sur la fa√ßon dont ces d√©cisions sont prises dans les r√©seaux avec une architecture de sac de fonctionnalit√©s avec un champ r√©cepteur limit√©, ce qui est plus facile √† interpr√©ter.  Et, en comparant les r√©ponses de BagNet et les r√©seaux de neurones profonds habituels, nous sommes arriv√©s √† la conclusion que les processus d√©cisionnels en eux sont assez similaires. </p><br><p>  Les auteurs du deuxi√®me article ont compar√© la fa√ßon dont les gens et les r√©seaux de neurones per√ßoivent les images dans lesquelles la forme et les textures se contredisent.  Et ils ont sugg√©r√© d'utiliser un nouvel ensemble de donn√©es, Stylized ImageNet, pour r√©duire les diff√©rences de perception.  Ayant re√ßu en prime une augmentation de la pr√©cision de la classification sur ImageNet et de la d√©tection sur des ensembles de donn√©es tiers. </p><br><p>  La principale conclusion peut √™tre tir√©e comme suit: les r√©seaux qui √©tudient en images, ayant la capacit√© de se souvenir des propri√©t√©s spatiales de plus haut niveau des objets, pr√©f√®rent un moyen plus facile d'atteindre l'objectif - de s'ajuster aux textures.  Si l'ensemble de donn√©es sur lequel ils s'entra√Ænent le permet. </p><br><p>  En plus de l'int√©r√™t acad√©mique, le probl√®me du sur-ajustement de la texture est important pour nous tous qui utilisons des mod√®les pr√©-form√©s pour transf√©rer l'apprentissage dans leurs t√¢ches. <br>  Une cons√©quence importante de tout cela pour nous est que vous ne devriez pas faire confiance au poids des mod√®les qui sont g√©n√©ralement pr√©-form√©s sur ImageNet, car pour la plupart d'entre eux, des augmentations assez simples ont √©t√© utilis√©es qui ne contribuent pas √† √©liminer le sur-ajustement.  Et il est pr√©f√©rable, si possible, d'avoir des mod√®les form√©s avec des augmentations plus s√©rieuses ou StylNet ImageNet + ImageNet dans le nid.  Pour toujours pouvoir comparer celle qui convient le mieux √† notre t√¢che actuelle. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr453788/">https://habr.com/ru/post/fr453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr453776/index.html">Nous pompons des designers dans l'entreprise: du junior au directeur artistique</a></li>
<li><a href="../fr453778/index.html">Comment nous avons cr√©√© une banque en ligne pour les entreprises. Premi√®re partie: changement de marque</a></li>
<li><a href="../fr453780/index.html">Comment choisir un t√©l√©phone Grandstream SIP - en g√©n√©ral et en particulier?</a></li>
<li><a href="../fr453782/index.html">Infinite UIScrollView</a></li>
<li><a href="../fr453784/index.html">Comment le sp√©cialiste DevOps est victime de l'automatisation</a></li>
<li><a href="../fr453790/index.html">"Le client est parti - est-ce pour toujours?" Comment compter le taux de d√©sabonnement des clients en SaaS et ce qui ne va pas avec les mesures de base</a></li>
<li><a href="../fr453792/index.html">Syst√®mes de recommandation: id√©es, approches, t√¢ches</a></li>
<li><a href="../fr453796/index.html">Les gens ont-ils besoin de math√©matiques?</a></li>
<li><a href="../fr453800/index.html">Comment r√©soudre "D√©mineur" (et le rendre meilleur)</a></li>
<li><a href="../fr453804/index.html">Le livre "Comp√©titivit√© et concurrence sur la plate-forme .NET. Mod√®les de conception efficaces "</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>