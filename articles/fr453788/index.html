<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💪🏼 🕰️ 🛌🏾 Les réseaux de neurones préfèrent les textures et comment y faire face. 👩🏼‍🤝‍👨🏾 👩‍👦‍👦 🤱🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Récemment, plusieurs articles ont critiqué ImageNet, peut-être l'ensemble d'images le plus célèbre utilisé pour entraîner les réseaux de neurones. 


...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les réseaux de neurones préfèrent les textures et comment y faire face.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  Récemment, plusieurs articles ont critiqué ImageNet, peut-être l'ensemble d'images le plus célèbre utilisé pour entraîner les réseaux de neurones. </p><br><p>  Dans le premier article, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Approximation des CNN avec des modèles de caractéristiques de sac local fonctionne étonnamment bien sur ImageNet, les</a> auteurs prennent un modèle similaire à un sac de mots et utilisent des fragments de l'image comme «mots».  Ces fragments peuvent mesurer jusqu'à 9x9 pixels.  Et en même temps, sur un tel modèle, où toute information sur la disposition spatiale de ces fragments est complètement absente, les auteurs obtiennent une précision de 70 à 86% (par exemple, la précision d'un ResNet-50 régulier est de ~ 93%). </p><br><p>  Dans le deuxième article des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CNN formés par ImageNet sont biaisés vers la texture, les</a> auteurs concluent que l'ensemble de données ImageNet lui-même et la façon dont les personnes et les réseaux de neurones perçoivent les images en sont coupables et suggèrent d'utiliser un nouvel ensemble de données - Stylized-ImageNet. </p><br><p>  Plus en détail sur ce que les gens voient sur les images et sur les réseaux de neurones <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  L'ensemble de données ImageNet a commencé à être créé en 2006 par les efforts du professeur Fei-Fei Li et continue d'évoluer à ce jour.  À l'heure actuelle, il contient environ 14 millions d'images appartenant à plus de 20 000 catégories différentes. </p><br><p>  Depuis 2010, un sous-ensemble de cet ensemble de données, connu sous le nom d'ImageNet 1K avec environ 1 million d'images et des milliers de classes, a été utilisé dans le cadre du défi de reconnaissance visuelle à grande échelle d'ImageNet (ILSVRC).  Dans ce concours, en 2012, AlexNet, un réseau de neurones à convolution, a tiré avec une précision de 60% dans le top 1 et de 80% dans le top 5. <br>  C'est sur ce sous-ensemble de l'ensemble de données que les personnes du milieu universitaire mesurent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">leur SOTA</a> lorsqu'elles proposent de nouvelles architectures de réseau. </p><br><p>  Un peu sur le processus d'apprentissage sur cet ensemble de données.  Nous parlerons du protocole de formation sur ImageNet en milieu académique.  Autrement dit, lorsque les résultats d'un réseau SE block, ResNeXt ou DenseNet nous sont présentés dans l'article, le processus ressemble à ceci: le réseau apprend pendant 90 époques, la vitesse d'apprentissage diminue des 30e et 60e époques, toutes les 10 fois, en tant qu'optimiseur un SGD ordinaire avec une petite décroissance de poids est sélectionné, seuls RandomCrop et HorizontalFlip sont utilisés à partir des augmentations, l'image est généralement redimensionnée à 224x224 pixels. </p><br><p>  Voici un exemple de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">script pytorch</a> pour la formation sur ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Revenons aux articles mentionnés précédemment.  Dans le premier d'entre eux, les auteurs voulaient un modèle plus facile à interpréter que les réseaux profonds ordinaires.  Inspirés par l'idée des modèles de sac de fonctionnalité, ils créent leur propre famille de modèles - BagNets.  En utilisant comme base le réseau ResNet-50 habituel. </p><br><p>  Remplaçant certaines convolutions 3x3 par 1x1 dans ResNet-50, ils garantissent que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">champ récepteur des</a> neurones sur la dernière couche convolutionnelle est considérablement réduit, jusqu'à 9x9 pixels.  Ainsi, ils limitent les informations disponibles à un neurone individuel à un très petit fragment de l'image entière - un patch de plusieurs pixels.  Il convient de noter que pour le ResNet-50 vierge, la taille du champ récepteur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est supérieure à 400 pixels,</a> ce qui recouvre complètement l'image, qui est généralement redimensionnée à 224x224 pixels. </p><br><p>  Ce patch est le fragment <strong>maximum</strong> de l'image à partir duquel le modèle pourrait extraire des données spatiales.  À la fin du modèle, toutes les données étaient simplement résumées et le modèle ne pouvait en aucun cas savoir où se trouvait chaque patch par rapport aux autres patchs. <br>  Au total, trois variantes de réseaux à champ récepteur 9x9, 17x17 et 33x33 ont été testées.  Et, malgré le manque total d'informations spatiales, ces modèles ont pu obtenir une bonne précision dans la classification sur ImageNet.  La précision du Top 5 pour les patchs 9x9 était de 70%, pour 17x17 - 80%, pour 33x33 - 86%.  A titre de comparaison, la précision ResNet-50 top-5 est d'environ 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  La structure du modèle est illustrée dans la figure ci-dessus.  Chaque patch de qxqx3 pixels découpés dans l'image est transformé en vecteur 2048 par le réseau. Ensuite, ce vecteur est alimenté à l'entrée d'un classificateur linéaire, qui produit des scores pour chacune des 1000 classes.  En collectant les scores de chaque patch dans un tableau 2D, vous pouvez obtenir une carte thermique pour chaque classe et chaque pixel de l'image d'origine.  Les notes finales pour l'image ont été obtenues en additionnant la carte thermique de chaque classe. </p><br><p>  Exemples de cartes thermiques pour certaines classes: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Cartes thermiques"><br>  Comme vous pouvez le voir, la plus grande contribution au bénéfice d'une classe particulière est apportée par les correctifs situés sur les bords des objets.  Les correctifs de l'arrière-plan sont presque ignorés.  Jusqu'à présent, tout va bien. </p><br><p>  Regardons les correctifs les plus informatifs: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="Patchs informatifs"></p><br><p>  Par exemple, les auteurs ont suivi quatre cours.  Pour chacun d'eux, 2x7 patchs les plus significatifs ont été sélectionnés (c'est-à-dire les patchs où le score de cette classe était le plus élevé).  La rangée supérieure de 7 patchs est extraite des images de la classe correspondante uniquement, celle du bas - de l'échantillon entier d'images. </p><br><p>  Ce que l'on peut voir sur ces photos est remarquable.  Par exemple, pour la classe des tanches (tanche, poisson), les doigts sont une caractéristique.  Oui, des doigts humains ordinaires sur fond vert.  Et tout cela parce qu'il y a un pêcheur dans presque toutes les images avec cette classe, qui, en fait, tient ce poisson dans ses mains, exhibant un trophée. </p><br><div class="spoiler">  <b class="spoiler_title">Exemples d'ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Pour les ordinateurs portables, les touches de lettre sont une caractéristique.  Les clés de machine à écrire comptent également pour cette classe. </p><br><p>  Une caractéristique d'une couverture de livre est constituée de lettres sur fond coloré.  Que ce soit même une inscription sur un T-shirt ou sur un sac. </p><br><p>  Il semblerait que ce problème ne devrait pas nous déranger.  Puisqu'il n'est inhérent qu'à une classe étroite de réseaux avec un champ récepteur très limité.  Mais en outre, les auteurs ont calculé la corrélation entre les logits (sorties réseau avant le softmax final) attribués à chaque classe BagNet avec un champ récepteur différent, et les logits de VGG-16, qui a un champ récepteur assez important.  Et ils l'ont trouvée assez élevée. </p><br><div class="spoiler">  <b class="spoiler_title">Corrélation entre BagNets et VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Les auteurs se sont demandé si BagNet contient des indices sur la façon dont les autres réseaux prennent des décisions. </p><br><p>  Pour l'un des tests, ils ont utilisé une technique comme le brouillage d'image.  Ce qui a consisté à utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un générateur de texture basé sur des matrices de gramme</a> pour composer une image où les textures sont enregistrées, mais les informations spatiales manquent. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p>  VGG-16, formé sur des images ordinaires à part entière, a plutôt bien géré ces images brouillées.  Sa précision dans le top 5 est passée de 90% à 80%.  C'est-à-dire que même les réseaux avec un champ récepteur assez large préfèrent toujours se souvenir des textures et ignorer les informations spatiales.  Par conséquent, leur précision n'est pas tombée lourdement sur les images brouillées. </p><br><p>  Les auteurs ont mené une série d'expériences où ils ont comparé les parties des images les plus importantes pour BagNet et d'autres réseaux (VGG-16, ResNet-50, ResNet-152 et DenseNet-169).  Tout laissait entendre que d'autres réseaux, comme BagNet, s'appuient sur de petits fragments d'images et commettent à peu près les mêmes erreurs lors de la prise de décisions.  Cela était particulièrement visible pour les réseaux peu profonds tels que VGG. </p><br><p>  Cette tendance des réseaux à prendre des décisions basées sur les textures, contrairement à nous, les gens qui préfèrent la forme (voir la figure ci-dessous), a incité les auteurs du deuxième article à créer un nouvel ensemble de données basé sur ImageNet. </p><br><h3 id="stylized-imagenet">  ImageNet stylisé </h3><br><p>  Tout d'abord, les auteurs de l'article utilisant le transfert de style ont créé un ensemble d'images où la forme (données spatiales) et les textures d'une image se contredisaient.  Et nous avons comparé les résultats de personnes et de réseaux de convolution profonds de différentes architectures sur un ensemble de données synthétisées de 16 classes. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  Dans l'extrême droite, les gens voient un chat, un réseau - un éléphant. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Comparaison des résultats des personnes et des réseaux de neurones. </p><br><p>  Comme vous pouvez le voir, les gens lors de l'attribution d'un objet à une classe particulière se sont appuyés sur la forme des objets, les réseaux de neurones sur les textures.  Dans la figure ci-dessus, les gens ont vu un chat, un réseau - un éléphant. </p><br><blockquote>  Oui, ici, vous pouvez trouver à redire au fait que les filets sont également un peu à droite et cela, par exemple, pourrait être un éléphant photographié de près avec le tatouage d'un chat bien-aimé.  Mais le fait que les réseaux lors de la prise de décisions se comportent différemment des gens, les auteurs ont considéré le problème et ont commencé à chercher des moyens de le résoudre. </blockquote><p>  Comme mentionné ci-dessus, en s'appuyant uniquement sur les textures, le réseau est capable d'obtenir un bon résultat avec une précision de 86% dans le top 5.  Et il ne s'agit pas de plusieurs classes, où les textures aident à classer correctement les images, mais de la plupart des classes. </p><br><p>  Le problème est dans ImageNet lui-même, car il sera montré plus tard que le réseau est capable d'apprendre la forme, mais ne le fait pas, car les textures sont suffisantes pour cet ensemble de données et les neurones responsables des textures sont sur des couches peu profondes, qui sont beaucoup plus faciles à former. </p><br><p>  En utilisant cette fois un mécanisme de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transfert de style AdaIN</a> légèrement différent, les auteurs ont créé un nouvel ensemble de données - Stylized ImageNet.  La forme des objets a été tirée d'ImageNet, et l'ensemble des textures de ce <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">concours sur Kaggle</a> .  Le script de génération est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le lien</a> . </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  De plus, par souci de concision, ImageNet sera appelé <strong>IN</strong> , ImageNet stylisé <strong>SIN</strong> . </p><br><p>  Les auteurs ont pris ResNet-50 et trois BagNet avec un champ récepteur différent et se sont entraînés sur un modèle distinct pour chacun des ensembles de données. </p><br><p>  Et voici ce qu'ils ont fait: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  Ce que nous voyons ici.  ResNet-50 formé sur IN est complètement inapte sur SIN.  Ce qui confirme en partie que lors de l'entraînement sur IN, le réseau s'adapte aux textures et ignore la forme des objets.  Dans le même temps, le ResNet-50 formé sur SIN fait parfaitement face à la fois à SIN et à IN.  Autrement dit, s'il est privé d'un chemin simple, le réseau suit un chemin difficile - il enseigne la forme des objets. <br>  BagNet a finalement commencé à se comporter comme prévu, en particulier sur les petits correctifs, car il n'a rien à quoi se raccrocher - les informations de texture manquent simplement dans le NAS. </p><br><p>  Dans les seize classes mentionnées précédemment, ResNet-50, formé sur le NAS, a commencé à donner des réponses plus similaires à celles que les gens donnent: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  En plus de simplement former ResNet-50 sur le SIN, les auteurs ont essayé de former le réseau sur un ensemble mixte de SIN et IN, y compris un réglage fin séparément sur le IN pur. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Comme vous pouvez le voir, lorsque vous utilisez SIN + IN pour la formation, les résultats se sont améliorés non seulement sur la tâche principale - la classification des images sur ImageNet, mais également sur la tâche de détection des objets sur le jeu de données PASCAL VOC 2007. </p><br><p>  De plus, les réseaux formés au SIN sont devenus plus résistants à divers bruits dans les données. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  Même maintenant, en 2019, après sept ans de succès avec AlexNet, lorsque les réseaux de neurones sont largement utilisés en vision par ordinateur, lorsque ImageNet 1K est devenu de facto la norme pour évaluer les performances des modèles dans l'environnement universitaire, le mécanisme de prise de décision des réseaux de neurones n'est pas tout à fait clair. .  Et comment les ensembles de données sur lesquels ces réseaux ont été formés influencent cela. </p><br><p>  Les auteurs du premier article ont tenté de faire la lumière sur la façon dont ces décisions sont prises dans les réseaux avec une architecture de sac de fonctionnalités avec un champ récepteur limité, ce qui est plus facile à interpréter.  Et, en comparant les réponses de BagNet et les réseaux de neurones profonds habituels, nous sommes arrivés à la conclusion que les processus décisionnels en eux sont assez similaires. </p><br><p>  Les auteurs du deuxième article ont comparé la façon dont les gens et les réseaux de neurones perçoivent les images dans lesquelles la forme et les textures se contredisent.  Et ils ont suggéré d'utiliser un nouvel ensemble de données, Stylized ImageNet, pour réduire les différences de perception.  Ayant reçu en prime une augmentation de la précision de la classification sur ImageNet et de la détection sur des ensembles de données tiers. </p><br><p>  La principale conclusion peut être tirée comme suit: les réseaux qui étudient en images, ayant la capacité de se souvenir des propriétés spatiales de plus haut niveau des objets, préfèrent un moyen plus facile d'atteindre l'objectif - de s'ajuster aux textures.  Si l'ensemble de données sur lequel ils s'entraînent le permet. </p><br><p>  En plus de l'intérêt académique, le problème du sur-ajustement de la texture est important pour nous tous qui utilisons des modèles pré-formés pour transférer l'apprentissage dans leurs tâches. <br>  Une conséquence importante de tout cela pour nous est que vous ne devriez pas faire confiance au poids des modèles qui sont généralement pré-formés sur ImageNet, car pour la plupart d'entre eux, des augmentations assez simples ont été utilisées qui ne contribuent pas à éliminer le sur-ajustement.  Et il est préférable, si possible, d'avoir des modèles formés avec des augmentations plus sérieuses ou StylNet ImageNet + ImageNet dans le nid.  Pour toujours pouvoir comparer celle qui convient le mieux à notre tâche actuelle. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr453788/">https://habr.com/ru/post/fr453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr453776/index.html">Nous pompons des designers dans l'entreprise: du junior au directeur artistique</a></li>
<li><a href="../fr453778/index.html">Comment nous avons créé une banque en ligne pour les entreprises. Première partie: changement de marque</a></li>
<li><a href="../fr453780/index.html">Comment choisir un téléphone Grandstream SIP - en général et en particulier?</a></li>
<li><a href="../fr453782/index.html">Infinite UIScrollView</a></li>
<li><a href="../fr453784/index.html">Comment le spécialiste DevOps est victime de l'automatisation</a></li>
<li><a href="../fr453790/index.html">"Le client est parti - est-ce pour toujours?" Comment compter le taux de désabonnement des clients en SaaS et ce qui ne va pas avec les mesures de base</a></li>
<li><a href="../fr453792/index.html">Systèmes de recommandation: idées, approches, tâches</a></li>
<li><a href="../fr453796/index.html">Les gens ont-ils besoin de mathématiques?</a></li>
<li><a href="../fr453800/index.html">Comment résoudre "Démineur" (et le rendre meilleur)</a></li>
<li><a href="../fr453804/index.html">Le livre "Compétitivité et concurrence sur la plate-forme .NET. Modèles de conception efficaces "</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>