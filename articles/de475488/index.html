<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëçüèΩ ‚ùáÔ∏è üôéüèª Einf√ºhrung in PyTorch: Deep Learning in der Verarbeitung nat√ºrlicher Sprachen üî¢ üé∞ üë®üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, habrozhiteli! Natural Language Processing (NLP) ist eine √§u√üerst wichtige Aufgabe im Bereich der k√ºnstlichen Intelligenz. Eine erfolgreiche Implem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einf√ºhrung in PyTorch: Deep Learning in der Verarbeitung nat√ºrlicher Sprachen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/475488/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/gj/ib/ak/gjibakead8idlzldl2vxqq3gmuc.jpeg" align="left" alt="Bild"></a>  Hi, habrozhiteli!  Natural Language Processing (NLP) ist eine √§u√üerst wichtige Aufgabe im Bereich der k√ºnstlichen Intelligenz.  Eine erfolgreiche Implementierung erm√∂glicht Produkte wie Amazon und Google Translate.  Dieses Buch hilft Ihnen dabei, PyTorch, eine umfassende Lernbibliothek f√ºr die Python-Sprache, zu lernen, eines der f√ºhrenden Tools f√ºr Datenwissenschaftler und NLP-Softwareentwickler.  Delip Rao und Brian McMahan bringen Sie mit NLP und Deep Learning-Algorithmen auf den neuesten Stand.  Und zeigen Sie, wie Sie mit PyTorch Anwendungen implementieren k√∂nnen, die Textanalysen verwenden. <br><br>  In diesem Buch ‚Ä¢ Computergrafiken und das Paradigma des Lernens mit einem Lehrer.  ‚Ä¢ Grundlagen der optimierten PyTorch-Bibliothek zum Arbeiten mit Tensoren.  ‚Ä¢ Ein √úberblick √ºber traditionelle NLP-Konzepte und -Methoden.  ‚Ä¢ Proaktive neuronale Netze (Multilayer-Perceptron und andere).  ‚Ä¢ Verbesserung der RNN mit Langzeit-Kurzzeitged√§chtnis (LSTM) und kontrollierten Wiederholungsbl√∂cken ‚Ä¢ Vorhersage- und Sequenztransformationsmodelle.  ‚Ä¢ Entwurfsmuster der in der Produktion verwendeten NLP-Systeme. <br><a name="habracut"></a><br><h3>  Auszug  Verschachtelungsw√∂rter und andere Arten </h3><br>  Bei der L√∂sung von Problemen bei der Verarbeitung von Texten in nat√ºrlichen Sprachen m√ºssen verschiedene Arten von diskreten Datentypen behandelt werden.  Das offensichtlichste Beispiel sind W√∂rter.  Viele W√∂rter (W√∂rterbuch) nat√ºrlich.  Unter anderem Symbole, Beschriftungen von Wortarten, benannte Entit√§ten, benannte Entit√§tstypen, mit dem Parsen verbundene Attribute, Positionen im Produktkatalog usw. Tats√§chlich wird jedes Eingabemerkmal von einem endlichen (oder unendlichen, aber nicht von einem endlichen) Objekt √ºbernommen abz√§hlbar) setzt. <br><br>  Die Grundlage f√ºr die erfolgreiche Anwendung von Deep Learning in NLP ist die Darstellung diskreter Datentypen (z. B. W√∂rter) in Form dichter Vektoren.  Die Begriffe "Repr√§sentationslernen" und "Einbetten" bedeuten das Lernen, einen diskreten Datentyp bis zu einem Punkt in einem Vektorraum anzuzeigen / darzustellen.  Wenn diskrete Typen W√∂rter sind, wird eine dichte Vektordarstellung als Worteinbettung bezeichnet.  In Kapitel 2 haben wir bereits Beispiele f√ºr Verschachtelungsmethoden basierend auf der Anzahl der Vorkommen gesehen, z. B. TF-IDF (‚ÄûTermh√§ufigkeit ist die inverse H√§ufigkeit eines Dokuments‚Äú) Artikel von Baroni et al. (Baroni et al., 2014), in dem Leistungstraining durchgef√ºhrt wird, indem die Zielfunktion f√ºr eine bestimmte Lernaufgabe maximiert wird;  Zum Beispiel das Vorhersagen eines Wortes anhand des Kontexts.  Trainingsbasierte Anlagemethoden sind aufgrund ihrer breiten Anwendbarkeit und hohen Effizienz derzeit der Standard.  Tats√§chlich ist die Einbettung von W√∂rtern in NLP-Aufgaben so weit verbreitet, dass sie als ‚ÄûSriracha von NLP‚Äú bezeichnet werden, da zu erwarten ist, dass ihre Verwendung in jeder Aufgabe die Effizienz der L√∂sung erh√∂ht.  Dieser Spitzname ist jedoch etwas irref√ºhrend, da Anh√§nge im Gegensatz zu Syraci in der Regel nicht nachtr√§glich zum Modell hinzugef√ºgt werden, sondern dessen Grundbestandteil sind. <br><br>  In diesem Kapitel werden Vektordarstellungen im Zusammenhang mit Worteinbettungen behandelt: Worteinbettungsmethoden, Methoden zur Optimierung der Worteinbettung f√ºr Unterrichtsaufgaben mit und ohne Lehrer, Methoden zur visuellen Einbettung von Visualisierungsmethoden sowie Methoden zur Worteinbettung von Kombinationss√§tzen und Dokumenten.  Vergessen Sie jedoch nicht, dass die hier beschriebenen Methoden f√ºr jeden diskreten Typ gelten. <br><br><h3>  Warum Investitionstraining? </h3><br>  In den vorherigen Kapiteln haben wir Ihnen die √ºblichen Methoden zum Erstellen von Vektordarstellungen von W√∂rtern gezeigt.  Sie haben n√§mlich gelernt, wie man einheitliche Darstellungen verwendet - Vektoren mit einer L√§nge, die der Gr√∂√üe des W√∂rterbuchs entspricht, mit Nullen an allen Positionen, au√üer einer, die einen Wert von 1 enth√§lt, der einem bestimmten Wort entspricht.  Au√üerdem haben Sie Darstellungen der Anzahl der Vorkommen gefunden - Vektoren mit einer L√§nge, die der Anzahl der eindeutigen W√∂rter im Modell entspricht, die die Anzahl der Vorkommen von W√∂rtern im Satz an den entsprechenden Positionen enthalten.  Solche Darstellungen werden auch Verteilungsdarstellungen genannt, da sich ihr sinnvoller Inhalt / ihre Bedeutung in mehreren Dimensionen des Vektors widerspiegelt.  Die Geschichte der distributiven Repr√§sentation hat viele Jahrzehnte gedauert (siehe Firths Artikel [Firth, 1935]) und eignet sich hervorragend f√ºr viele Modelle des maschinellen Lernens und neuronaler Netze.  Diese Darstellungen sind heuristisch aufgebaut1 und nicht auf Daten trainiert. <br><br>  Die verteilte Darstellung hat ihren Namen erhalten, weil die W√∂rter in ihnen durch einen dichten Vektor mit einer viel kleineren Dimension dargestellt werden (z. B. d = 100 anstelle der Gr√∂√üe des gesamten W√∂rterbuchs, das in der Gr√∂√üenordnung liegen kann <img src="https://habrastorage.org/webt/vr/yx/0r/vryx0rlwodoo2krhnfdldvleqaq.png" alt="Bild">  ), und die Bedeutung und andere Eigenschaften des Wortes sind √ºber mehrere Dimensionen dieses dichten Vektors verteilt. <br><br>  Die niederdimensionalen, dichten Darstellungen, die als Ergebnis des Trainings erhalten wurden, haben mehrere Vorteile im Vergleich zu den einheitlichen Vektoren, die die Anzahl der Vorkommen enthalten, auf die wir in den vorhergehenden Kapiteln gesto√üen sind.  Erstens ist die Dimensionsreduktion rechnerisch effizient.  Zweitens f√ºhren Darstellungen, die auf der Anzahl der Vorkommen basieren, zu hochdimensionalen Vektoren mit √ºberm√§√üiger Codierung derselben Information in verschiedenen Dimensionen, und ihre statistische Aussagekraft ist nicht zu gro√ü.  Drittens kann eine zu gro√üe Dimension der Eingabedaten zu Problemen beim maschinellen Lernen und Optimieren f√ºhren - ein Ph√§nomen, das h√§ufig als Dimensionsfluch bezeichnet wird ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://bit.ly/2CrhQXm</a> ).  Um dieses Problem mit der Dimensionalit√§t zu l√∂sen, werden verschiedene Methoden zum Reduzieren der Dimension verwendet, beispielsweise die Singul√§rwertzerlegung (SVD) und die Hauptkomponentenanalyse (PCA). Ironischerweise lassen sich diese Ans√§tze jedoch nicht gut auf Dimensionen in der Gr√∂√üenordnung von Millionen skalieren ( typischer Fall in NLP).  Viertens sind die Darstellungen, die aus problemspezifischen Daten gelernt (oder auf deren Grundlage angepasst) wurden, f√ºr diese spezielle Aufgabe optimal geeignet.  Bei heuristischen Algorithmen wie TF-IDF und Dimensionsreduktionsmethoden wie SVD ist nicht klar, ob die objektive Optimierungsfunktion f√ºr eine bestimmte Aufgabe mit dieser Einbettungsmethode geeignet ist. <br><br><h3>  Investitionseffizienz </h3><br>  Um zu verstehen, wie Einbettungen funktionieren, betrachten Sie ein Beispiel eines einheitlichen Vektors, mit dem die Gewichtsmatrix in einer linearen Schicht multipliziert wird, wie in Abb.  5.1.  In den Kapiteln 3 und 4 stimmte die Gr√∂√üe der Einheitsvektoren mit der Gr√∂√üe des W√∂rterbuchs √ºberein.  Ein Vektor wird als einheitlich bezeichnet, da er an der Position, die einem bestimmten Wort entspricht, eine 1 enth√§lt und somit seine Anwesenheit anzeigt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4q/ag/or4qagtvn_dh4sxqok-q0tcsfmi.png" alt="Bild"></div><br>  Abb.  5.1.  Ein Beispiel f√ºr eine Matrixmultiplikation f√ºr den Fall eines Einheitsvektors und einer Matrix von Gewichten einer linearen Schicht.  Da der Einheitsvektor alle Nullen und nur eine Einheit enth√§lt, spielt die Position dieser Einheit die Rolle des Auswahloperators beim Multiplizieren der Matrix.  Dies ist in der Figur als Verdunkelung der Zellen der Gewichtsmatrix und des resultierenden Vektors dargestellt.  Obwohl diese Suchmethode funktioniert, erfordert sie einen hohen Verbrauch an Rechenressourcen und ist ineffizient, da der Einheitsvektor mit jeder der Zahlen in der Gewichtsmatrix multipliziert und die Summe in Zeilen berechnet wird <br><br>  Per Definition sollte die Anzahl der Zeilen der Gewichtsmatrix einer linearen Schicht, die am Eingang einen Einheitsvektor empf√§ngt, gleich der Gr√∂√üe dieses Einheitsvektors sein.  Beim Multiplizieren der Matrix, wie in Abb.  In 5.1 ist der resultierende Vektor tats√§chlich eine Zeichenfolge, die einem Nicht-Null-Element eines einheitlichen Vektors entspricht.  Basierend auf dieser Beobachtung k√∂nnen Sie den Multiplikationsschritt √ºberspringen und einen ganzzahligen Wert als Index verwenden, um die gew√ºnschte Zeile zu extrahieren. <br><br>  Ein letzter Hinweis zur Anlageperformance: Trotz des Beispiels in Abbildung  5.1, wo die Dimension der Gewichtsmatrix mit der Dimension des Eingabe-Einheitsvektors √ºbereinstimmt, ist dies bei weitem nicht immer der Fall.  In der Tat werden Anh√§nge oft verwendet, um W√∂rter aus einem Raum mit geringerer Dimension darzustellen, als dies bei Verwendung eines einheitlichen Vektors oder der Darstellung der Anzahl der Vorkommen erforderlich w√§re.  Eine typische Investitionsgr√∂√üe in wissenschaftlichen Artikeln liegt bei 25 bis 500 Messungen, und die Auswahl eines bestimmten Werts wird auf die Menge des verf√ºgbaren GPU-Speichers reduziert. <br><br><h3>  Lernans√§tze f√ºr Anh√§nge </h3><br>  Der Zweck dieses Kapitels besteht nicht darin, Ihnen bestimmte Techniken zum Investieren von W√∂rtern beizubringen, sondern Ihnen dabei zu helfen, herauszufinden, was Investitionen sind, wie und wo sie angewendet werden k√∂nnen, wie sie am besten in Modellen verwendet werden k√∂nnen und welche Einschr√§nkungen sie haben.  Tatsache ist, dass man in der Praxis selten neue Lernalgorithmen f√ºr Worteinbettungen schreiben muss.  In diesem Unterabschnitt geben wir jedoch einen kurzen √úberblick √ºber die modernen Ans√§tze f√ºr eine solche Ausbildung.  Das Lernen in allen Methoden zum Verschachteln von W√∂rtern erfolgt nur mit W√∂rtern (d. H. Nicht beschrifteten Daten), jedoch mit einem Lehrer.  Dies ist m√∂glich, weil mit dem Lehrer zus√§tzliche Unterrichtsaufgaben erstellt werden, bei denen die Daten implizit markiert sind, da die f√ºr die L√∂sung der zus√§tzlichen Aufgabe optimierte Darstellung viele statistische und sprachliche Eigenschaften des Textkorpus erfassen sollte, um zumindest einen gewissen Nutzen zu erzielen.  Hier einige Beispiele f√ºr solche Hilfsaufgaben. <br><br><ul><li>  Sagen Sie das n√§chste Wort in einer bestimmten Folge von W√∂rtern voraus.  Es tr√§gt auch den Namen des Sprachmodellierungsproblems. </li><li>  Sagen Sie ein fehlendes Wort anhand der W√∂rter vorher und nachher voraus. </li><li>  Sagen Sie W√∂rter innerhalb eines bestimmten Fensters unabh√§ngig von ihrer Position f√ºr ein bestimmtes Wort voraus. </li></ul><br>  Nat√ºrlich ist diese Liste nicht vollst√§ndig und die Wahl eines Hilfsproblems h√§ngt von der Intuition des Algorithmusentwicklers und den Berechnungskosten ab.  Beispiele hierf√ºr sind GloVe, Continuous Bag-of-Words (CBOW), Skipgrams usw. Details finden Sie in Kapitel 10 von Goldbergs Buch (Goldberg, 2017), aber wir werden das CBOW-Modell hier kurz diskutieren.  In den meisten F√§llen ist es jedoch v√∂llig ausreichend, vorgefertigte Wortanh√§nge zu verwenden und sie an die vorhandene Aufgabe anzupassen. <br><br><h3>  Praktische Anwendung von vorgefertigten Wortanh√§ngen </h3><br>  Der Gro√üteil dieses Kapitels, wie auch der Rest des Buches, befasst sich mit der Verwendung von vorgefertigten Wortanh√§ngen.  Mit einer der vielen oben beschriebenen Methoden auf einem gro√üen Textk√∂rper vortrainiert - zum Beispiel auf dem gesamten Textk√∂rper von Google News, Wikipedia oder Common Crawl1 - k√∂nnen Wortanh√§nge kostenlos heruntergeladen und verwendet werden.  Im weiteren Verlauf des Kapitels werden wir zeigen, wie Sie diese Anh√§nge korrekt finden und laden, einige Eigenschaften von Worteinbettungen untersuchen und Beispiele f√ºr die Verwendung von vorgefertigten Worteinbettungen in NLP-Aufgaben geben. <br><br><h3>  Anh√§nge herunterladen </h3><br>  Anh√§nge von W√∂rtern sind so beliebt und weit verbreitet, dass viele verschiedene Optionen zum Herunterladen zur Verf√ºgung stehen, vom urspr√ºnglichen Word2Vec2 bis zum Stanford GloVe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://stanford.io/2PSIvPZ</a> ), einschlie√ülich Facebooks FastText3 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://fasttext.cc</a> ) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">/</a> ) und viele andere.  Normalerweise werden Anh√§nge in folgendem Format geliefert: Jede Zeile beginnt mit einem Wort / Typ, gefolgt von einer Folge von Zahlen (d. H. Einer Vektordarstellung).  Die L√§nge dieser Sequenz entspricht der Dimension der Pr√§sentation (Dimension des Anhangs).  Die Dimension der Investitionen liegt normalerweise in der Gr√∂√üenordnung von Hunderten.  Die Anzahl der Tokentypen entspricht meist der Gr√∂√üe des W√∂rterbuchs und betr√§gt etwa eine Million.  Hier sind zum Beispiel die ersten sieben Dimensionen der Hunde- und Katzenvektoren von GloVe. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n5/g7/x0/n5g7x0mos74bxchmiuexmcl7yw8.png" alt="Bild"></div><br>  Um Anh√§nge effizient zu laden und zu handhaben, beschreiben wir die Helferklasse PreTrainedEmbeddings (Beispiel 5.1).  Es erstellt einen Index aller im RAM gespeicherten Wortvektoren, um die schnelle Suche und Abfrage der n√§chsten Nachbarn mit Hilfe des Berechnungspakets f√ºr die n√§chsten Nachbarn zu vereinfachen. <br><br>  Beispiel 5.1.  Verwenden von vorbereiteten Word-Anh√§ngen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nx/hh/hh/nxhhhhz9wsdjl1tzms5ur8e4bvk.png" alt="Bild"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/8u/vb/y18uvbmsjpbvwno_wp5dsui8_38.png" alt="Bild"></div><br>  In diesen Beispielen verwenden wir die Einbettung der W√∂rter GloVe.  Sie m√ºssen sie herunterladen und eine Instanz der PreTrainedEmbeddings-Klasse erstellen, wie in Eingabe [1] aus Beispiel 5.1 gezeigt. <br><br><h3>  Beziehungen zwischen Wortanh√§ngen </h3><br>  Die Schl√ºsseleigenschaft von Worteinbettungen ist die Kodierung syntaktischer und semantischer Beziehungen, die sich in Form von Wortgebrauchsmustern manifestieren.  Zum Beispiel wird √ºber Katzen und Hunde normalerweise sehr √§hnlich gesprochen (sie besprechen ihre Haustiere, F√ºtterungsgewohnheiten usw.).  Infolgedessen liegen die Anh√§nge f√ºr die W√∂rter Katzen und Hunde viel n√§her beieinander als die Anh√§nge f√ºr die Namen anderer Tiere, beispielsweise Enten und Elefanten. <br><br>  Es gibt viele M√∂glichkeiten, semantische Beziehungen zu untersuchen, die in Wortanh√§ngen kodiert sind.  Eine der beliebtesten Methoden ist die Verwendung der Analogie-Aufgabe (eine der h√§ufigsten Arten logischer Denkaufgaben in Pr√ºfungen wie SAT): <br><br>  Word1: Word2 :: Word3: ______ <br><br>  Bei dieser Aufgabe ist es erforderlich, den vierten Wert bei gegebener Verbindung zwischen den ersten beiden durch die gegebenen drei W√∂rter zu bestimmen.  Mit Hilfe von verschachtelten W√∂rtern kann dieses Problem r√§umlich codiert werden.  Subtrahieren Sie zun√§chst Word2 von Word1.  Der Differenzvektor zwischen ihnen codiert die Beziehung zwischen Word1 und Word2.  Diese Differenz kann dann zu Slovo3 addiert werden und das Ergebnis ist der Vektor, der dem vierten fehlenden Wort am n√§chsten kommt.  Um das Analogieproblem zu l√∂sen, ist es ausreichend, die n√§chsten Nachbarn unter Verwendung dieses erhaltenen Vektors nach Index abzufragen.  Die in Beispiel 5.2 gezeigte entsprechende Funktion f√ºhrt genau das aus, was oben beschrieben wurde: Sie verwendet Vektorarithmetik und einen ungef√§hren Index der n√§chsten Nachbarn, um das fehlende Element in der Analogie zu finden. <br><br>  Beispiel 5.2.  L√∂sen eines Analogieproblems mithilfe von Worteinbettungen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q3/qv/sl/q3qvslovjualenm1jkjn6sftbtk.png" alt="Bild"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1u/i_/hi/1ui_hizmvkfh5xqy1aqkegqqjfc.png" alt="Bild"></div><br><br>  Interessanterweise kann anhand einer einfachen verbalen Analogie gezeigt werden, wie Worteinbettungen eine Vielzahl von semantischen und syntaktischen Beziehungen erfassen k√∂nnen (Beispiel 5.3). <br><br>  Beispiel 5.3  Codierung mit Hilfe von Worteinbettungen vieler sprachlicher Zusammenh√§nge am Beispiel von Aufgaben nach der Analogie des SAT <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/rf/v0/q1rfv0qixjwl9mob64rfru-2vgw.png" alt="Bild"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hk/wt/j0/hkwtj0_ea88pxvm-2q3zelcjlfq.png" alt="Bild"></div><br>  Obwohl es den Anschein hat, dass Zusammenh√§nge das Funktionieren der Sprache deutlich widerspiegeln, ist nicht alles so einfach.  Wie Beispiel 5.4 zeigt, k√∂nnen Verbindungen falsch definiert werden, da Wortvektoren auf der Grundlage ihres gemeinsamen Auftretens bestimmt werden. <br><br>  Beispiel 5.4.  Ein Beispiel, das die Gefahr der Kodierung der Bedeutung von W√∂rtern auf der Grundlage des gemeinsamen Vorkommens veranschaulicht - manchmal funktioniert es nicht! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b1/xf/t3/b1xft37n3jlu6lwivpjpi_shnmm.png" alt="Bild"></div><br>  Beispiel 5.5 zeigt eine der h√§ufigsten Kombinationen beim Codieren von Geschlechterrollen. <br><br>  Beispiel 5.5  Seien Sie vorsichtig mit gesch√ºtzten Attributen wie Geschlecht, die durch Wortanh√§nge verschl√ºsselt sind.  Sie k√∂nnen in zuk√ºnftigen Modellen zu unerw√ºnschten Verzerrungen f√ºhren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ga/5y/cl/ga5yclk4wb32h5rpwujbl96yry0.png" alt="Bild"></div><br>  Es stellt sich heraus, dass es ziemlich schwierig ist, zwischen Sprachmustern und tief verwurzelten kulturellen Vorurteilen zu unterscheiden.  Zum Beispiel sind √Ñrzte keineswegs immer M√§nner und Krankenschwestern sind nicht immer Frauen, aber solche Vorurteile sind so hartn√§ckig, dass sie sich in der Sprache und folglich in den Wortvektoren widerspiegeln, wie in Beispiel 5.6 gezeigt. <br><br>  Beispiel 5.6.  Kulturelle Vorurteile in Wortvektoren ‚Äûeingen√§ht‚Äú <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oz/ap/xz/ozapxzj5jxnwwxdcq33sc_dig9q.png" alt="Bild"></div><br>  Wir sollten m√∂gliche systematische Fehler bei Investitionen nicht vergessen, da diese in NLP-Anwendungen immer beliebter werden und h√§ufiger vorkommen.  Die Beseitigung systematischer Fehler bei der Worteinbettung ist ein neues und sehr interessantes Gebiet der wissenschaftlichen Forschung (siehe den Artikel von Bolukbashi et al. [Bolukbasi et al., 2016]).  Wir empfehlen Ihnen, auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ethicsinnlp.org nachzuschlagen,</a> wo Sie aktuelle Informationen zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Querschnittsethik</a> und NLP finden. <br><br><h3>  √úber Autoren </h3><br>  <b>Delip Rao</b> ist der Gr√ºnder des in San Francisco ans√§ssigen Beratungsunternehmens Joostware, das sich auf maschinelles Lernen und NLP-Forschung spezialisiert hat.  Einer der Mitbegr√ºnder der Fake News Challenge - eine Initiative, die Hacker und Forscher auf dem Gebiet der KI √ºber die Aufgaben der √úberpr√ºfung von Fakten in den Medien zusammenbringen soll.  Zuvor arbeitete Delip bei Twitter und Amazon (Alexa) an NLP-bezogenen Forschungs- und Softwareprodukten. <br><br>  <b>Brian McMahan</b> ist wissenschaftlicher Mitarbeiter bei Wells Fargo und konzentriert sich haupts√§chlich auf NLP.  Zuvor bei Joostware gearbeitet. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  25% <b>Rabatt</b> Gutschein f√ºr <b>H√§ndler</b> - <b>PyTorch</b> <br><br>  Nach Bezahlung der Papierversion des Buches wird ein elektronisches Buch per E-Mail verschickt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475488/">https://habr.com/ru/post/de475488/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475476/index.html">Datennetz: So arbeiten Sie mit Daten ohne Monolithen</a></li>
<li><a href="../de475478/index.html">Netflix Experience: Netflix Inside</a></li>
<li><a href="../de475480/index.html">Was bist du Wie wir Parodie von Mensch unterschieden - und sogar gewonnen haben</a></li>
<li><a href="../de475482/index.html">Wie wurde aus der Testaufgabe eine Produktionsbibliothek?</a></li>
<li><a href="../de475486/index.html">AR-Macher: die Entstehung eines neuen Berufes</a></li>
<li><a href="../de475490/index.html">Unter Druck arbeiten</a></li>
<li><a href="../de475494/index.html">"Gibt es ein Leben nach Signor?" Oder wor√ºber werden wir auf der SECR-2019 sprechen</a></li>
<li><a href="../de475496/index.html">So ermitteln Sie die Adresse eines Smart-Vertrags vor der Bereitstellung: Verwenden von CREATE2 f√ºr den Krypto-Austausch</a></li>
<li><a href="../de475498/index.html">Windows Server Core vs. GUI und Softwarekompatibilit√§t</a></li>
<li><a href="../de475506/index.html">Interview mit Mikhail Chinkov √ºber Arbeit und Leben in Berlin</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>