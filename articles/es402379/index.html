<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêé ‚úíÔ∏è üôçüèº Lo que piensan los investigadores de IA sobre los posibles riesgos asociados con √©l ü¶Ü üî∞ ü§¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Me interes√© por los riesgos asociados con la IA en 2007. En ese momento, la reacci√≥n de la mayor√≠a de las personas a este tema fue algo como esto: "Es...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lo que piensan los investigadores de IA sobre los posibles riesgos asociados con √©l</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402379/"> Me interes√© por los riesgos asociados con la IA en 2007.  En ese momento, la reacci√≥n de la mayor√≠a de las personas a este tema fue algo como esto: "Es muy divertido, vuelve cuando alguien que no sea idiota de Internet crea en √©l". <br><br>  En los a√±os siguientes, varias figuras extremadamente inteligentes e influyentes, incluidos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bill Gates</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Stephen Hawking</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Elon Musk</a> , compartieron p√∫blicamente sus preocupaciones sobre los riesgos de la IA, seguidos por cientos de otros intelectuales, desde fil√≥sofos de Oxford hasta cosm√≥logos del MIT e inversores de Silicon Valley. .  Y estamos de vuelta. <br><br>  Luego la reacci√≥n cambi√≥ a: "Bueno, un par de cient√≠ficos y empresarios pueden creer en ello, pero es poco probable que sean verdaderos expertos en este campo que est√©n realmente versados ‚Äã‚Äãen la situaci√≥n". <br><br>  De aqu√≠ surgieron declaraciones como el art√≠culo de Popular Science " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bill Gates le teme a la IA, pero los investigadores de IA deber√≠an saberlo</a> ": <br><blockquote>  Habiendo hablado con investigadores de IA: investigadores reales, que apenas hacen que tales sistemas funcionen, sin mencionar que funcionan bien, queda claro que no temen que la superinteligencia se les acerque repentinamente, ni ahora ni en el futuro. .  A pesar de todas las historias aterradoras contadas por Mask, los investigadores no tienen prisa por construir salas de protecci√≥n y autodestrucci√≥n con una cuenta regresiva. </blockquote><a name="habracut"></a><br>  O, como escribieron en Fusion.net en el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Objeci√≥n sobre robots asesinos de una persona que realmente est√° desarrollando IA</a> ": <br><blockquote>  Andrew Angie desarrolla profesionalmente sistemas de inteligencia artificial.  Ense√±√≥ un curso de IA en Stanford, desarroll√≥ AI en Google, y luego se mud√≥ al motor de b√∫squeda chino Baidu para continuar su trabajo a la vanguardia de la aplicaci√≥n de AI a problemas del mundo real.  Entonces, cuando se entera de c√≥mo Elon Musk o Stephen Hawking, personas que no est√°n familiarizadas directamente con la tecnolog√≠a moderna, est√°n hablando de IA, que podr√≠a destruir a la humanidad, casi se puede escuchar que se cubre la cara con las manos. </blockquote><br>  Ramez Naam de Marginal Revolution repite m√°s o menos lo mismo en el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øQu√© piensan los investigadores sobre los riesgos de la IA?</a> ": <br><blockquote>  Elon Musk, Stephen Hawking y Bill Gates han expresado recientemente su preocupaci√≥n de que el desarrollo de la IA podr√≠a implementar el escenario "asesino de la IA" y potencialmente conducir a la extinci√≥n de la humanidad.  No son investigadores de IA, y que yo sepa, no han trabajado directamente con la IA.  ¬øQu√© piensan los verdaderos investigadores de IA sobre los riesgos de la IA? </blockquote><br>  Cita las palabras de investigadores de IA especialmente seleccionados, como los autores de otras historias, y luego se detiene, sin mencionar ninguna opini√≥n diferente a esta. <br><br>  Pero ellos existen.  Los investigadores de IA, incluidos los l√≠deres en el campo, han expresado activamente sus preocupaciones sobre los riesgos de la IA y m√°s all√° de la inteligencia, y desde el principio.  Comenzar√© enumerando a estas personas, a pesar de la lista de Naam, y luego pasar√© a por qu√© no considero esto como una "discusi√≥n" en el sentido cl√°sico que se espera al enumerar las estrellas. <br><br>  Los criterios de mi lista son los siguientes: menciono solo a los investigadores m√°s prestigiosos, o profesores de ciencias en buenos institutos con muchas citas de art√≠culos cient√≠ficos, o cient√≠ficos muy respetados de la industria que trabajan para grandes empresas y tienen un buen historial.  Se dedican a la inteligencia artificial y al aprendizaje autom√°tico.  Tienen varias declaraciones fuertes en apoyo de un cierto punto de vista con respecto al inicio de una singularidad o riesgo grave de la IA en el futuro cercano.  Algunos de ellos han escrito obras o libros sobre este tema.  Otros simplemente expresaron sus pensamientos, creyendo que este es un tema importante digno de estudio. <br><br>  Si alguien no est√° de acuerdo con la inclusi√≥n de una persona en esta lista o cree que olvid√© algo importante, av√≠seme. <br><br>  * * * * * * * * * * <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Stuart Russell</a> es profesor de ciencias de la computaci√≥n en Berkeley, ganador del Premio IJCAI Computadoras y Pensamiento, investigador de la Asociaci√≥n de Mecanizaci√≥n de Computadoras, investigador de la Academia Americana de Investigaci√≥n Cient√≠fica Avanzada, director del Centro de Sistemas Inteligentes, ganador del Premio Blaise Pascal, etc.  etc.  Coautor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI: A Modern Approach</a> , un libro de texto cl√°sico utilizado en 1.200 universidades de todo el mundo.  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">su sitio web,</a> escribe: <br><blockquote>  En el campo de la IA, se han llevado a cabo 50 a√±os de investigaci√≥n bajo la bandera de la suposici√≥n de que cuanto m√°s inteligente, mejor.  La preocupaci√≥n por el beneficio de la humanidad debe combinarse con esto.  El argumento es simple: <br><br>  1. Es probable que la IA se cree con √©xito. <br>  2. El √©xito ilimitado conlleva grandes riesgos y grandes beneficios. <br>  3. ¬øQu√© podemos hacer para aumentar las posibilidades de obtener beneficios y evitar riesgos? <br><br>  Algunas organizaciones ya est√°n trabajando en estos temas, incluido el Instituto para el Futuro de la Humanidad en Oxford, el Centro de Estudios de Riesgo Existencial en Cambridge (CSER), el Instituto para el Estudio de la Inteligencia de M√°quinas en Berkeley y el Instituto para la Vida Futura en Harvard / MIT (FLI).  Estoy en consejos consultivos con CSER y FLI. <br><br>  As√≠ como los investigadores en fusi√≥n nuclear consideraron el problema de limitar las reacciones nucleares como uno de los problemas m√°s importantes en su campo, el desarrollo del campo de la IA inevitablemente plantear√° cuestiones de control y seguridad.  Los investigadores ya est√°n comenzando a plantear preguntas, desde cuestiones puramente t√©cnicas (los principales problemas de racionalidad y utilidad, etc.) hasta cuestiones ampliamente filos√≥ficas. </blockquote><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">edge.org,</a> describe un punto de vista similar: <br><blockquote>  Como explicaron Steve Omohandro, Nick Bostrom y otros, una discrepancia en los valores con los sistemas de toma de decisiones, cuyas posibilidades est√°n en constante crecimiento, puede generar problemas, tal vez incluso problemas de la escala de extinci√≥n, si las m√°quinas son m√°s capaces que las personas.  Algunos creen que no hay riesgos previsibles para la humanidad en los pr√≥ximos siglos, tal vez olvidando que la diferencia horaria entre la declaraci√≥n de confianza de Rutherford de que la energ√≠a at√≥mica nunca puede extraerse y transcurren menos de 24 horas por la invenci√≥n de la reacci√≥n en cadena nuclear iniciada por neutrones . </blockquote><br>  Tambi√©n trat√≥ de convertirse en un representante de estas ideas en la comunidad acad√©mica, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">indicando</a> : <br><blockquote>  Me parece que las personas principales en esta industria, que nunca antes han expresado sus temores, piensan que este problema debe tomarse muy en serio, y cuanto antes lo tomemos en serio, mejor. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">David McAllister</a> es profesor y miembro principal del Instituto de Tecnolog√≠a de Toyota, afiliado a la Universidad de Chicago, quien trabaj√≥ anteriormente en las facultades del MIT y el Instituto Cornell.  Es miembro de la American AI Association, ha publicado m√°s de cien trabajos, llevado a cabo investigaciones en los campos del aprendizaje autom√°tico, la teor√≠a de la programaci√≥n, la toma de decisiones autom√°ticas, la planificaci√≥n de la inteligencia artificial, la ling√º√≠stica computacional y ha tenido un gran impacto en los algoritmos de la famosa computadora de ajedrez Deep Blue.  Seg√∫n un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> en el Pittsburgh Tribune Review: <br><blockquote>  El profesor de Chicago, David McAllister, considera inevitable el surgimiento de la capacidad de las m√°quinas inteligentes totalmente autom√°ticas para dise√±ar y crear versiones m√°s inteligentes de s√≠ mismos, es decir, el inicio de un evento conocido como singularidad [tecnol√≥gica].  La singularidad permitir√° que las m√°quinas se vuelvan infinitamente inteligentes, lo que lleva a un "escenario incre√≠blemente peligroso", dice. </blockquote><br>  En su blog, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pensamientos sobre autom√≥viles</a> , escribe: <br><blockquote>  La mayor√≠a de los inform√°ticos se niegan a hablar sobre √©xitos reales en IA.  Creo que ser√≠a m√°s razonable decir que nadie puede predecir cu√°ndo se recibir√° una IA comparable a la mente humana.  John MacArthy me dijo una vez que cuando le preguntan qu√© tan pronto se crear√° la IA de nivel humano, responde que tiene entre quinientos y quinientos a√±os.  MacArthy era inteligente.  Dadas las incertidumbres en esta √°rea, es razonable considerar el problema de la IA amigable ... <br><br>  En las primeras etapas, la IA generalizada estar√° a salvo.  Sin embargo, las primeras etapas de OII ser√°n un excelente sitio de prueba para AI como servidor u otras opciones amigables de AI.  Ben Goertzel tambi√©n anuncia un enfoque experimental en una buena publicaci√≥n en su blog.  Si nos espera la era de las OII seguras y no demasiado inteligentes, entonces tendremos tiempo para pensar en tiempos m√°s peligrosos. </blockquote><br>  Fue miembro del grupo de expertos AAAI Panel On AI a largo plazo dedicado a las perspectivas a largo plazo de AI, presidi√≥ el comit√© de control a largo plazo y se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">describe de la siguiente manera</a> : <br><blockquote>  Makalister me habl√≥ sobre el enfoque de la "singularidad", un evento en el que las computadoras se vuelven m√°s inteligentes que las personas.  No mencion√≥ la fecha exacta de su ocurrencia, pero dijo que esto podr√≠a suceder en las pr√≥ximas dos d√©cadas, y que al final definitivamente suceder√≠a.  Aqu√≠ est√°n sus puntos de vista sobre la singularidad.  Ocurrir√°n dos eventos importantes: inteligencia operativa, en la que podemos hablar f√°cilmente con las computadoras, y una reacci√≥n en cadena de la inteligencia artificial, en la que la computadora puede mejorar sin ayuda, y luego repetirla nuevamente.  El primer evento lo notaremos en sistemas de asistencia autom√°tica que realmente nos ayudar√°n.  M√°s tarde se volver√° realmente interesante comunicarse con las computadoras.  Y para que las computadoras puedan hacer todo lo que las personas pueden hacer, debe esperar a que ocurra el segundo evento. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hans Moravek</a> es un ex profesor del Instituto de Rob√≥tica de la Universidad Carnegie Mellon, que lleva su nombre en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">para√≠so de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Moravec</a> , fundador de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SeeGrid Corporation</a> , que se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">especializa</a> en sistemas de visi√≥n artificial para aplicaciones industriales.  Su trabajo, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">S√≠ntesis de sensores en las rejillas de certeza de robots m√≥viles</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">,</a> ha sido citado m√°s de mil veces, y fue invitado a escribir un art√≠culo para la Enciclopedia Brit√°nica de Rob√≥tica, en un momento en que los art√≠culos en enciclopedias fueron escritos por expertos mundiales en este campo, y no cientos de comentaristas an√≥nimos en Internet. <br><br>  Tambi√©n es autor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Robot: From a Simple Machine to a Transcendental Mind</a> , que Amazon describe de la siguiente manera: <br><blockquote>  En este emocionante libro, Hans Moravek predice que para 2040, las m√°quinas se acercar√°n al nivel intelectual de las personas, y para 2050 nos superar√°n.  Pero mientras Moravec predice el final de una era de dominaci√≥n humana, su visi√≥n de este evento no es tan sombr√≠a.  No est√° cercado de un futuro en el que las m√°quinas dominen el mundo, sino que lo acepta y describe un punto de vista sorprendente seg√∫n el cual los robots inteligentes se convertir√°n en nuestros descendientes evolutivos.  Moravec cree que al final de este proceso, "el vasto ciberespacio se unir√° con la supermente inhumana y se ocupar√° de asuntos que est√©n lejos de las personas y de los humanos y las bacterias". </blockquote><br>  Shane Leg es cofundador de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepMind Technologies</a> , una startup de IA comprada en 2014 por $ 500 millones por Google.  Recibi√≥ su doctorado del Instituto de IA con el nombre de  Dale Moul en Suiza, y tambi√©n trabaj√≥ en la Divisi√≥n de Neurobiolog√≠a Computacional.  Gatsby en Londres.  Al final de su disertaci√≥n, "m√°quina de superinteligencia", <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escribe</a> : <br><blockquote>  Si alguna vez aparece algo que pueda acercarse al poder absoluto, ser√° una m√°quina s√∫per inteligente.  Por definici√≥n, podr√° lograr una gran cantidad de objetivos en una amplia variedad de entornos.  Si nos preparamos cuidadosamente para tal oportunidad de antemano, no solo podemos evitar el desastre, sino tambi√©n comenzar una era de prosperidad, a diferencia de todo lo que exist√≠a antes. </blockquote><br>  En una entrevista posterior, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dice</a> : <br><blockquote>  AI es ahora donde estaba Internet en 1988.  Las necesidades de aprendizaje autom√°tico se requieren en aplicaciones especiales (motores de b√∫squeda como Google, fondos de cobertura y bioinform√°tica), y su n√∫mero crece cada a√±o.  Creo que a mediados de la pr√≥xima d√©cada este proceso se volver√° masivo y notable.  El auge de la IA tendr√° lugar alrededor de 2020, seguido de una d√©cada de r√°pido progreso, posiblemente despu√©s de las correcciones del mercado.  La IA a nivel humano se crear√° a mediados de 2020, aunque muchas personas no aceptar√°n el inicio de este evento.  Despu√©s de eso, los riesgos asociados con la IA avanzada se pondr√°n en pr√°ctica.  No dir√© acerca de la "singularidad", pero esperan que en alg√∫n momento despu√©s de la creaci√≥n de la OII las cosas locas comenzar√°n a suceder.  Est√° en alg√∫n lugar entre 2025 y 2040. </blockquote><br>  √âl y sus cofundadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Demis Khasabis</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mustafa Suleiman</a> firmaron una petici√≥n para el Instituto para la Vida Futura con respecto a los riesgos de IA, y una de sus condiciones para unirse a Google fue que la compa√±√≠a acuerda organizar un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">consejo de √©tica de IA</a> para estudiar estos temas. <br><br>  Steve Omohundro es un ex profesor de ciencias de la computaci√≥n en la Universidad de Illinois, fundador del grupo de visi√≥n y capacitaci√≥n en computaci√≥n en el Centro para el Estudio de Sistemas Complejos, e inventor de varios desarrollos importantes en aprendizaje autom√°tico y visi√≥n por computadora.  Trabaj√≥ en robots leyendo labios, el lenguaje de programaci√≥n paralelo StarLisp, algoritmos de aprendizaje geom√©trico.  Actualmente dirige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Self-Aware Systems</a> , "un equipo de cient√≠ficos que trabaja para garantizar que las tecnolog√≠as inteligentes beneficien a la humanidad".  Su trabajo, "Los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fundamentos de la motivaci√≥n de la IA</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">,</a> ayud√≥ a generar el dominio de la √©tica de la m√°quina, ya que se√±al√≥ que los sistemas s√∫per inteligentes se dirigir√°n hacia objetivos potencialmente peligrosos.  El escribe: <br><blockquote>  Hemos demostrado que es probable que todos los sistemas avanzados de IA tengan un conjunto de motivaciones centrales.  Es imperativo comprender estas motivaciones para crear tecnolog√≠as que aseguren un futuro positivo para la humanidad.  Yudkovsky pidi√≥ una "IA amigable".  Para hacer esto, necesitamos desarrollar un enfoque cient√≠fico para el "desarrollo utilitario", que nos permitir√° desarrollar funciones socialmente √∫tiles que conduzcan a las secuencias que deseamos.  Los r√°pidos avances en el progreso tecnol√≥gico sugieren que estos problemas pronto pueden volverse cr√≠ticos. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Puede</a> encontrar sus art√≠culos sobre el tema "IA racional para el bien com√∫n" en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Murray Shanahan</a> tiene un doctorado en ciencias de la computaci√≥n de Cambridge, y ahora es profesor de rob√≥tica cognitiva en el Imperial College de Londres.  Public√≥ trabajos en campos como la rob√≥tica, la l√≥gica, los sistemas din√°micos, la neurobiolog√≠a computacional y la filosof√≠a de la mente.  Actualmente est√° trabajando en el libro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Singularidad tecnol√≥gica</a> , que se publicar√° en agosto.  La anotaci√≥n promocional de Amazon es la siguiente: <br><blockquote>  Shanahan describe los avances tecnol√≥gicos en IA, ambos realizados bajo la influencia del conocimiento de la biolog√≠a, y desarrollados desde cero.  Explica que cuando se crea la IA del nivel humano, una tarea te√≥ricamente posible pero dif√≠cil, la transici√≥n a la IA superinteligente ser√° muy r√°pida.  Shanahan reflexiona sobre lo que puede conducir la existencia de m√°quinas superinteligentes en √°reas como la personalidad, la responsabilidad, los derechos y la individualidad.  Algunos representantes de la inteligencia artificial superinteligente pueden crearse en beneficio del hombre, otros pueden salirse de control.  (¬øEs decir, Siri o HAL?) La singularidad representa para la humanidad tanto una amenaza existencial como una oportunidad existencial para superar sus limitaciones.  Shanahan deja en claro que si queremos lograr un mejor resultado, debemos imaginar ambas posibilidades. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Marcus Hutter</a> es profesor de investigaci√≥n en inform√°tica en la Universidad Nacional de Australia.  Antes de eso, trabaj√≥ en el Instituto de IA con el nombre de  Dale Mole en Suiza y en el Instituto Nacional de Inform√°tica y Comunicaciones de Australia, y tambi√©n trabaj√≥ en el aprendizaje estimulado, los hallazgos bayesianos, la teor√≠a de la complejidad computacional, la teor√≠a de predicciones inductivas de Solomon, la visi√≥n por computadora y los perfiles gen√©ticos.  Tambi√©n escribi√≥ mucho sobre la singularidad.  En el art√≠culo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øPuede la inteligencia explotar?</a> " √âl escribe: <br><blockquote>  Este siglo puede ser testigo de una explosi√≥n tecnol√≥gica, cuya escala merece el nombre de singularidad.  El escenario predeterminado es una comunidad de individuos inteligentes que interact√∫an en el mundo virtual, simulados en computadoras con recursos inform√°ticos que aumentan hiperb√≥licamente.  Esto inevitablemente va acompa√±ado de una explosi√≥n de velocidad, medida por el tiempo f√≠sico, pero no necesariamente por una explosi√≥n de inteligencia.  Si el mundo virtual est√° poblado por individuos libres e interactivos, la presi√≥n evolutiva conducir√° a la aparici√≥n de individuos con inteligencia creciente que competir√°n por los recursos inform√°ticos.  El punto final de esta aceleraci√≥n evolutiva de la inteligencia puede ser la comunidad de los individuos m√°s inteligentes.  Algunos aspectos de esta comunidad singular pueden estudiarse te√≥ricamente con la ayuda de herramientas cient√≠ficas modernas.  Mucho antes de la aparici√≥n de esta singularidad, incluso colocando esta comunidad virtual en nuestra imaginaci√≥n, uno puede imaginar la aparici√≥n de diferencias, como, por ejemplo, una fuerte ca√≠da en el valor de un individuo, que puede tener consecuencias radicales. </blockquote><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">J√ºrgen Schmidhuber</a> es profesor de IA en la Universidad de Lugano y ex profesor de rob√≥tica cognitiva en la Universidad Tecnol√≥gica de Munich.  Desarrolla algunas de las redes neuronales m√°s avanzadas del mundo, trabaja en rob√≥tica evolutiva y la teor√≠a de la complejidad computacional, y se desempe√±a como investigador en la Academia Europea de Ciencias y Artes.  En su libro, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hip√≥tesis de singularidades</a> ", argumenta que "con la continuaci√≥n de las tendencias existentes, enfrentaremos una explosi√≥n intelectual en las pr√≥ximas d√©cadas".  Cuando se le pregunt√≥ directamente en Reddit AMA sobre los riesgos asociados con la IA, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">respondi√≥</a> : <br><blockquote>        .    - ,    ?  ,    ,  :   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>       .     -  .         ,    ,    .  ,           .           .   .              ,  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">encontrar un nicho para la supervivencia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ".</font></font></blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Richard Saton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es profesor y miembro del Comit√© iCORE de la Universidad de Alberta. </font><font style="vertical-align: inherit;">Se desempe√±a como investigador en la Asociaci√≥n para el Desarrollo de la IA, coautor del libro de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">texto m√°s popular sobre aprendizaje estimulado</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , el pionero del m√©todo de diferencias de tiempo, uno de los m√°s importantes en este campo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En su </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">informe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en una conferencia sobre IA organizada por el Instituto para el Futuro de la Vida, Saton argument√≥ que "existe una posibilidad real de que incluso con nuestras vidas" se cree una IA que sea intelectualmente comparable a los humanos, y agreg√≥ que esta IA "no nos obedecer√°", "lo har√° para competir y cooperar con nosotros "y que" si creamos esclavos s√∫per inteligentes, obtendremos oponentes s√∫per inteligentes ". En conclusi√≥n, dijo que "necesitamos pensar a trav√©s de mecanismos (sociales, legales, pol√≠ticos, culturales) para asegurar el resultado deseado", pero que "inevitablemente la gente com√∫n ser√° menos importante". Tambi√©n mencion√≥ problemas similares en la </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">presentaci√≥n del</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Instituto Gadsby. Tambi√©n en el libro de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glenn Beck</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">existen tales l√≠neas: "Richard Saton, uno de los mayores expertos en IA, predice una explosi√≥n de inteligencia en alg√∫n lugar a mediados de siglo". </font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrew Davison</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> es profesor de visi√≥n artificial en el Imperial College London, l√≠der en equipos de visi√≥n rob√≥tica y el Laboratorio de Rob√≥tica Dyson, e inventor del sistema de marcado y localizaci√≥n computarizado MonoSLAM. En su sitio web, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">escribe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><blockquote>         ,  ,   ,  ,  2006           :            ,          (,   20-30 ).         ¬´ ¬ª (   ,    ),           ,    ,        ,     .    , ,      ,        ,     ,    ,      . <br><br>       ,   ,      ,     (       ).   ,        .    ¬´ ¬ª    .        ,    ,         ,         . </blockquote><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alan Turing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no necesitan ser presentados. Turing invent√≥ los fundamentos matem√°ticos de la ciencia computacional y le puso el nombre de m√°quina de Turing, integridad de Turing y prueba de Turing. Goode trabaj√≥ con Turing en Bletchley Park, ayud√≥ a crear una de las primeras computadoras e invent√≥ muchos algoritmos conocidos, por ejemplo, el algoritmo de transformaci√≥n de Fourier discreto r√°pido, conocido como el algoritmo FFT. En su trabajo, ¬øpueden pensar los coches digitales? Turing escribe:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Asumamos que tales m√°quinas pueden ser creadas, y consideremos las consecuencias de su creaci√≥n. </font><font style="vertical-align: inherit;">Tal acto, sin duda, ser√° recibido con hostilidad, a menos que hayamos avanzado en la tolerancia religiosa desde la √©poca de Galileo. </font><font style="vertical-align: inherit;">La oposici√≥n consistir√° en intelectuales temerosos de perder sus trabajos. </font><font style="vertical-align: inherit;">Pero es probable que los intelectuales se equivoquen. </font><font style="vertical-align: inherit;">Ser√° posible hacer muchas cosas en un intento por mantener su intelecto al nivel de los est√°ndares establecidos por las m√°quinas, ya que despu√©s del inicio del m√©todo de la m√°quina, no lleva mucho tiempo hasta el momento en que las m√°quinas superan nuestras capacidades insignificantes. </font><font style="vertical-align: inherit;">En alg√∫n momento, debemos esperar que las m√°quinas tomen el control.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mientras trabajaba en Atlas Computer Lab en los a√±os 60, Goode desarroll√≥ esta idea en " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Razonamiento de la primera m√°quina ultrainteligente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ":</font></font><br><blockquote>   ,  ,       .     ‚Äì     ,       .  ,   ,   ¬´ ¬ª,      . ,    ‚Äì   ,    . </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">* * * * * * * * * * </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Me molesta que esta lista pueda dar la impresi√≥n de una cierta disputa entre "creyentes" y "esc√©pticos" en esta √°rea, durante la cual se aplastan mutuamente. Pero no lo creo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuando leo art√≠culos sobre esc√©pticos, siempre encuentro dos argumentos. En primer lugar, todav√≠a estamos muy lejos de la IA del nivel humano, sin mencionar la superinteligencia, y no hay una forma obvia de alcanzar tales alturas. En segundo lugar, si exige prohibiciones en la investigaci√≥n de IA, es un idiota. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estoy completamente de acuerdo con ambos puntos. Como los l√≠deres del movimiento de riesgo de IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encuesta entre investigadores de IA ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Muller y Bostrom, 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) mostraron que, en promedio, dan el 50% por el hecho de que la IA de nivel humano aparecer√° en 2040 oda, y el 90%, que aparecer√° en 2075. En promedio, el 75% de ellos cree que la superinteligencia ("inteligencia de m√°quina, excede seriamente las capacidades de cada persona en la mayor√≠a de las profesiones ") aparecer√° dentro de los 30 a√±os posteriores al advenimiento de la IA de nivel humano. Y aunque la t√©cnica de esta encuesta plantea algunas dudas, si acepta sus resultados, resulta que la mayor√≠a de los investigadores de IA est√°n de acuerdo en que algo de lo que vale la pena preocuparse aparecer√° en una o dos generaciones. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero Luke Muelhauser, director del Instituto de Inteligencia de M√°quinas, y Nick Bostrom, director del Instituto para el Futuro de la Humanidad, declararon que sus predicciones para el desarrollo de la IA son mucho m√°s tard√≠as que las predichas por los cient√≠ficos involucrados en la encuesta. Si estudias</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A partir de los datos sobre las predicciones de IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de Stuart Armstrong, se puede ver que, en general, las estimaciones sobre el momento de la aparici√≥n de AI realizadas por los partidarios de AI no difieren de las estimaciones realizadas por los esc√©pticos de AI. Adem√°s, la predicci√≥n a largo plazo en esta tabla pertenece al propio Armstrong. Sin embargo, Armstrong trabaja actualmente en el Instituto para el Futuro de la Humanidad, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">llamando la atenci√≥n sobre los riesgos de la IA</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y la necesidad de investigar los objetivos de la superinteligencia. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La diferencia entre los partidarios y los esc√©pticos no est√° en sus evaluaciones de cu√°ndo deber√≠amos esperar la aparici√≥n de la IA a nivel humano, sino en cu√°ndo debemos comenzar a prepararnos para esto.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lo que nos lleva al segundo punto. Parece que la posici√≥n de los esc√©pticos es que, aunque probablemente deber√≠amos enviar a un par de personas inteligentes a trabajar en una evaluaci√≥n preliminar del problema, no hay necesidad de entrar en p√°nico o prohibir la investigaci√≥n de IA. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los fan√°ticos de la IA insisten en que, aunque no necesitamos entrar en p√°nico o prohibir la investigaci√≥n de la IA, probablemente valga la pena enviar a un par de personas inteligentes a trabajar en una evaluaci√≥n preliminar del problema. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jan Lekun es posiblemente el esc√©ptico m√°s ardiente de los riesgos de IA. Fue citado abundantemente en un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo sobre Popular Science</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , en una </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">publicaci√≥n sobre Marginal Revolution</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y tambi√©n habl√≥ con </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sobre los "problemas inevitables de la singularidad", que √©l mismo describe como "estar tan lejos que se puede escribir ciencia ficci√≥n sobre ellos". </font><font style="vertical-align: inherit;">Pero cuando se le pidi√≥ que aclarara su posici√≥n, declar√≥:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elon Musk est√° muy preocupado por las amenazas existenciales para la humanidad (por eso construye cohetes para enviar personas a colonizar otros planetas). </font><font style="vertical-align: inherit;">Y aunque el riesgo de una rebeli√≥n de la IA es muy peque√±o y muy lejano al futuro, debemos pensar en ello, desarrollar medidas y reglas de precauci√≥n. </font><font style="vertical-align: inherit;">As√≠ como el comit√© de bio√©tica apareci√≥ en los a√±os setenta y ochenta, antes del uso generalizado de la gen√©tica, necesitamos comit√©s de √©tica de inteligencia artificial. </font><font style="vertical-align: inherit;">Pero, como escribi√≥ Yoshua Benjio, todav√≠a tenemos mucho tiempo.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz es otro experto a menudo referido como el principal portavoz del escepticismo y las limitaciones. Su punto de vista se describi√≥ en art√≠culos como "El </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Director de Investigaci√≥n de Microsoft cree que la IA fuera de control no nos matar√°</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " y " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz de Microsoft cree que la IA no debe tener miedo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ". Pero aqu√≠ est√° lo que dijo en una entrevista m√°s larga con NPR:</font></font><br><blockquote> Keist: Horvitz duda que las secretarias virtuales se conviertan en algo que conquiste el mundo.  √âl dice que esto es de esperarse para que una cometa evolucione en un Boeing 747. ¬øSignifica esto que se burla de una singularidad? <br><br>  Horvitz: No.  Creo que hubo una mezcla de conceptos, y yo tambi√©n tengo sentimientos encontrados. <br><br>  Keist: En particular, debido a ideas como la singularidad, Horvits y otros expertos en inteligencia artificial est√°n tratando cada vez m√°s de lidiar con los problemas √©ticos que pueden surgir con la inteligencia artificial espec√≠fica en los pr√≥ximos a√±os.  Tambi√©n hacen preguntas m√°s futuristas.  Por ejemplo, ¬øc√≥mo puedo hacer un bot√≥n de apagado de emergencia para una computadora que puede cambiar por s√≠ misma? <br><br>  Horvits: Realmente creo que hay mucho en juego para gastar tiempo y energ√≠a buscando soluciones activamente, incluso si la probabilidad de tales eventos es peque√±a. </blockquote><br>  Esto generalmente coincide con la posici√≥n de muchos de los agitadores de riesgo de IA m√°s ardientes.  Con tales amigos, no se necesitan enemigos. <br><br>  El art√≠culo de Slate, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">No tengas miedo de la IA</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">,</a> tambi√©n sorprendentemente pone las cosas en la luz correcta: <br><blockquote>  Como afirma el propio Musk, la soluci√≥n al riesgo de IA radica en la colaboraci√≥n sobria y racional de cient√≠ficos y legisladores.  Sin embargo, es bastante dif√≠cil entender c√≥mo hablar sobre "demonios" puede ayudar a lograr este noble objetivo.  Ella incluso puede obstaculizarla. <br><br>  En primer lugar, hay grandes agujeros en la idea del script Skynet.  Aunque los investigadores en el campo de las ciencias de la computaci√≥n creen que el razonamiento de Mask "no es completamente una locura", todav√≠a est√°n demasiado lejos de un mundo en el que el entusiasmo por la IA oculta una realidad de IA un poco menos a la que se enfrentan nuestros cient√≠ficos inform√°ticos. <br><br>  Ian Lekun, el jefe del laboratorio de IA en Facebook, resumi√≥ esta idea en una publicaci√≥n en Google+ en 2013: la exageraci√≥n est√° da√±ando a la IA.  En las √∫ltimas cinco d√©cadas, la exageraci√≥n ha matado a la IA cuatro veces.  Ella necesita ser detenida ". Lekun y otros tienen miedo de exagerar. El incumplimiento de las altas expectativas impuestas por la ciencia ficci√≥n conduce a recortes serios en los presupuestos para la investigaci√≥n de IA. </blockquote><br>  Los cient√≠ficos que trabajan en IA son personas inteligentes.  No est√°n interesados ‚Äã‚Äãen caer en trampas pol√≠ticas cl√°sicas, en las que se dividir√≠an en campos y se acusar√≠an mutuamente de p√°nico u ostrichism.  Aparentemente, est√°n tratando de encontrar un equilibrio entre la necesidad de comenzar el trabajo preliminar relacionado con la amenaza que se avecina en alg√∫n lugar en la distancia, y el riesgo de causar una sensaci√≥n tan fuerte que los golpear√°. <br><br>  No quiero decir que no hay diferencia de opini√≥n sobre qu√© tan pronto debe comenzar a abordar este problema.  B√°sicamente, todo se reduce a si es posible decir que "resolveremos el problema cuando lo encontremos", o esperar un despegue tan inesperado, debido a lo cual todo se saldr√° de control y para lo cual, por lo tanto, debemos prepararnos por adelantado  Veo menos evidencia de la que me gustar√≠a que la mayor√≠a de los investigadores de IA con sus propias opiniones entiendan la segunda posibilidad.  ¬øQu√© puedo decir, incluso si en un art√≠culo sobre Revoluci√≥n Marginal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se cita a</a> un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">experto</a> que dice que la superinteligencia no representa una gran amenaza, porque "las computadoras inteligentes no pueden establecerse objetivos por s√≠ mismas", aunque cualquiera que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">lea Bostrom</a> sabe que Todo el problema es. <br><br>  Todav√≠a hay una monta√±a de trabajo por hacer.  Pero no para seleccionar espec√≠ficamente art√≠culos en los que "los verdaderos expertos en IA no se preocupen por la superinteligencia". </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es402379/">https://habr.com/ru/post/es402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es402367/index.html">C√≥mo dejar de pagar por roaming, o con un n√∫mero en todo el mundo</a></li>
<li><a href="../es402369/index.html">C√≥mo medir la velocidad de una impresora 3D: su extremo activo. Y no solo velocidad</a></li>
<li><a href="../es402373/index.html">Lo que da la "Gen√©tica de la Microbiota"</a></li>
<li><a href="../es402375/index.html">Conmutador de CA de 8 kilovatios y 4 canales con medici√≥n de consumo. Parte 1</a></li>
<li><a href="../es402377/index.html">¬øQu√© piensan sus tel√©fonos inteligentes sobre la carga USB del autom√≥vil?</a></li>
<li><a href="../es402381/index.html">C√≥mo reclutar astronautas</a></li>
<li><a href="../es402383/index.html">Saw, Shura: c√≥mo dise√±amos la aplicaci√≥n m√≥vil de seguimiento de perros Mishiko</a></li>
<li><a href="../es402385/index.html">¬øPor qu√© deber√≠a esperar un boom en el campo de la creaci√≥n de robots para locales comerciales?</a></li>
<li><a href="../es402387/index.html">Bol√≠grafo 3D para impresoras 3D</a></li>
<li><a href="../es402389/index.html">MPAA y RIAA planean recuperar datos de discos duros fallidos de compartir archivos Megaupload</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>