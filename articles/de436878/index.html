<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë®‚Äçüëß üë©üèæ‚Äçüè≠ üöö BERT ist ein hochmodernes Sprachmodell f√ºr 104 Sprachen. Tutorial zum lokalen Starten von BERT und in Google Colab ü§† üßòüèº üÖøÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERT ist ein neuronales Netzwerk von Google, das bei einer Reihe von Aufgaben mit gro√üem Abstand die neuesten Ergebnisse zeigte. Mit BERT k√∂nnen Sie K...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>BERT ist ein hochmodernes Sprachmodell f√ºr 104 Sprachen. Tutorial zum lokalen Starten von BERT und in Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="Bild"></p><br><p>  BERT ist ein neuronales Netzwerk von Google, das bei einer Reihe von Aufgaben mit gro√üem Abstand die neuesten Ergebnisse zeigte.  Mit BERT k√∂nnen Sie KI-Programme zur Verarbeitung einer nat√ºrlichen Sprache erstellen: Beantworten Sie Fragen in beliebiger Form, erstellen Sie Chat-Bots, automatische √úbersetzer, analysieren Sie Text usw. </p><br><p>  Google hat vorab geschulte BERT-Modelle ver√∂ffentlicht, aber wie es normalerweise beim maschinellen Lernen der Fall ist, leiden sie unter einem Mangel an Dokumentation.  In diesem Tutorial erfahren Sie daher, wie Sie das neuronale BERT-Netzwerk auf dem lokalen Computer sowie auf der GPU des kostenlosen Servers in Google Colab ausf√ºhren. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Warum ist es √ºberhaupt notwendig? </h2><br><p>  Um Text an die Eingabe eines neuronalen Netzwerks zu senden, m√ºssen Sie ihn irgendwie in Form von Zahlen darstellen.  Es ist am einfachsten, dies Buchstabe f√ºr Buchstabe zu tun und einen Buchstaben auf jeden Eingang des neuronalen Netzwerks anzuwenden.  Dann wird jeder Buchstabe mit einer Zahl von 0 bis 32 (plus einer Art Rand f√ºr Satzzeichen) codiert.  Dies ist die sogenannte Zeichenebene. </p><br><p>  Viel bessere Ergebnisse werden jedoch erzielt, wenn wir Vorschl√§ge nicht mit einem Buchstaben pr√§sentieren, sondern indem wir jedem Eingang des neuronalen Netzes sofort ein ganzes Wort (oder zumindest Silben) √ºbermitteln.  Es wird bereits eine Wortebene sein.  Am einfachsten ist es, ein W√∂rterbuch mit allen vorhandenen W√∂rtern zu kompilieren und dem Netzwerk die Anzahl der W√∂rter in diesem W√∂rterbuch mitzuteilen.  Befindet sich beispielsweise das Wort "Hund" 1678 in diesem W√∂rterbuch, geben wir die Nummer 1678 f√ºr die Eingabe des neuronalen Netzwerks f√ºr dieses Wort ein. </p><br><p>  Aber nur in einer nat√ºrlichen Sprache mit dem Wort ‚ÄûHund‚Äú tauchen in einer Person viele Assoziationen gleichzeitig auf: ‚Äûflauschig‚Äú, ‚Äûb√∂se‚Äú, ‚ÄûFreund einer Person‚Äú.  Ist es m√∂glich, dieses Merkmal unseres Denkens in der Pr√§sentation f√ºr das neuronale Netzwerk irgendwie zu kodieren?  Es stellt sich heraus, dass Sie k√∂nnen.  Dazu reicht es aus, die Wortnummern neu anzuordnen, damit die bedeutungsnahen W√∂rter nebeneinander stehen.  Sei es zum Beispiel f√ºr "Hund" die Nummer 1678 und f√ºr das Wort "flauschig" die Nummer 1680. Und f√ºr das Wort "Teekanne" ist die Nummer 9000. Wie Sie sehen k√∂nnen, sind die Nummern 1678 und 1680 viel n√§her beieinander als die Nummer 9000. </p><br><p>  In der Praxis wird jedem Wort nicht eine, sondern mehrere Zahlen zugewiesen - beispielsweise ein Vektor mit 32 Zahlen.  Die Abst√§nde werden als Abst√§nde zwischen den Punkten gemessen, auf die diese Vektoren im Raum der entsprechenden Dimension zeigen (f√ºr einen 32-stelligen Vektor ist dies ein Raum mit 32 Dimensionen oder mit 32 Achsen).  Auf diese Weise k√∂nnen Sie ein Wort gleichzeitig mit mehreren W√∂rtern vergleichen, deren Bedeutung nahe beieinander liegt (abh√§ngig davon, welche Achse gez√§hlt werden soll).  Dar√ºber hinaus k√∂nnen arithmetische Operationen mit Vektoren durchgef√ºhrt werden.  Ein klassisches Beispiel: Wenn Sie den Vektor ‚ÄûMann‚Äú von dem Vektor subtrahieren, der das Wort ‚ÄûK√∂nig‚Äú bezeichnet, und den Vektor f√ºr das Wort ‚ÄûFrau‚Äú hinzuf√ºgen, erhalten Sie einen bestimmten Ergebnisvektor.  Und er wird auf wundersame Weise dem Wort "K√∂nigin" entsprechen.  Und in der Tat: "K√∂nig ist Mann + Frau = K√∂nigin."  Die Magie!  Und dies ist kein abstraktes Beispiel, aber es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">passiert wirklich</a> .  In Anbetracht der Tatsache, dass neuronale Netze f√ºr mathematische Transformationen √ºber ihre Eingaben gut geeignet sind, bietet dies anscheinend eine so hohe Effizienz dieser Methode. </p><br><p>  Dieser Ansatz wird als Einbettung bezeichnet.  Alle maschinellen Lernpakete (TensorFlow, PyTorch) erm√∂glichen es der ersten Schicht des neuronalen Netzwerks, eine spezielle Schicht der Einbettungsschicht zu erstellen, die dies automatisch ausf√ºhrt.  Das hei√üt, am Eingang des neuronalen Netzwerks geben wir die √ºbliche Wortnummer im W√∂rterbuch ein, und die selbstlernende Einbettungsschicht √ºbersetzt jedes Wort in einen Vektor der angegebenen L√§nge, beispielsweise 32 Zahlen. </p><br><p>  Sie erkannten jedoch schnell, dass es viel rentabler ist, eine solche Vektordarstellung von W√∂rtern auf einem riesigen Textkorpus, beispielsweise in der gesamten Wikipedia, vorab zu trainieren und vorgefertigte Wortvektoren in bestimmten neuronalen Netzen zu verwenden, anstatt sie jedes Mal neu zu trainieren. </p><br><p>  Es gibt verschiedene M√∂glichkeiten, W√∂rter als Vektoren darzustellen, die sich allm√§hlich weiterentwickelten: word2vec, GloVe, Elmo. </p><br><p>  Im Sommer 2018 stellte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI fest,</a> dass ein neuronales Netzwerk, das die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformer-</a> Architektur mit gro√üen Textmengen vorab trainiert, unerwartet und mit gro√üem Abstand hervorragende Ergebnisse bei vielen verschiedenen Arten von Aufgaben zur Verarbeitung nat√ºrlicher Sprache zeigt.  Tats√§chlich erzeugt ein solches neuronales Netzwerk an seiner Ausgabe Vektordarstellungen f√ºr W√∂rter und sogar ganze Phrasen.  Und indem Sie einen kleinen Block aus zwei zus√§tzlichen Schichten von Neuronen auf ein solches Sprachmodell h√§ngen, k√∂nnen Sie dieses neuronale Netzwerk f√ºr alle Aufgaben trainieren. </p><br><p>  BERT von Google ist ein erweitertes GPA-Netzwerk von OpenAI (bidirektional statt unidirektional usw.), das ebenfalls auf der Transformer-Architektur basiert.  Derzeit ist BERT bei fast allen g√§ngigen NLP-Benchmarks auf dem neuesten Stand der Technik. </p><br><h2 id="kak-oni-eto-sdelali">  Wie haben sie das gemacht? </h2><br><p>  Die Idee hinter BERT ist sehr einfach: Lassen Sie uns das neuronale Netzwerk mit Phrasen versorgen, in denen wir 15% der W√∂rter durch [MASK] ersetzen und das neuronale Netzwerk trainieren, um diese maskierten W√∂rter vorherzusagen. </p><br><p>  Wenn wir beispielsweise den Satz "Ich bin zu [MASK] gekommen und habe [MASK] gekauft" an den Eingang des neuronalen Netzwerks senden, sollten am Ausgang die W√∂rter "speichern" und "Milch" angezeigt werden.  Dies ist ein vereinfachtes Beispiel von der offiziellen BERT-Seite: Bei l√§ngeren S√§tzen wird der Bereich m√∂glicher Optionen kleiner und die Reaktion des neuronalen Netzwerks ist eindeutig. </p><br><p>  Und damit das neuronale Netzwerk lernen kann, die Beziehung zwischen verschiedenen S√§tzen zu verstehen, werden wir es zus√§tzlich trainieren, um vorherzusagen, ob die zweite Phrase eine logische Fortsetzung der ersten ist.  Oder ist es eine zuf√§llige Phrase, die nichts mit der ersten zu tun hat? </p><br><p>  Also f√ºr zwei S√§tze: "Ich ging in den Laden."  und "Und dort Milch gekauft.", sollte das neuronale Netzwerk antworten, dass dies logisch ist.  Und wenn der zweite Satz "Crucian Sky Pluto" ist, dann muss ich antworten, dass dieser Vorschlag nichts mit dem ersten zu tun hat.  Wir werden unten mit diesen beiden BERT-Modi herumspielen. </p><br><p>  Nachdem wir das neuronale Netzwerk 4 Tage lang mit 16 TPU auf dem Korpus von Texten aus Wikipedia und der B√ºchersammlung BookCorpus trainiert hatten, erhielten wir BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Installation und Einrichtung </h2><br><p>  <em><strong>Hinweis</strong> : In diesem Abschnitt werden wir BERT auf dem lokalen Computer starten und damit spielen.</em>  <em>Um dieses neuronale Netzwerk auf einer lokalen GPU auszuf√ºhren, ben√∂tigen Sie eine NVidia GTX 970 mit 4 GB Videospeicher oder h√∂her.</em>  <em>Wenn Sie BERT nur in einem Browser ausf√ºhren m√∂chten (daf√ºr ben√∂tigen Sie nicht einmal eine GPU auf Ihrem Computer), gehen Sie zum Abschnitt "Google Colab".</em> </p><br><p>  Installieren Sie zuerst TensorFlow, falls Sie es noch nicht haben. Befolgen Sie dazu die Anweisungen unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.tensorflow.org/install</a> .  Um die GPU zu unterst√ºtzen, m√ºssen Sie zuerst das CUDA Toolkit 9.0, dann das cuDNN SDK 7.2 und erst dann TensorFlow mit GPU-Unterst√ºtzung installieren: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Grunds√§tzlich reicht dies aus, um BERT auszuf√ºhren.  Es gibt jedoch keine Anweisung als solche. Sie k√∂nnen sie selbst <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erstellen,</a> indem Sie die Quellen in der Datei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">run_classifier.py sortieren</a> (die √ºbliche Situation beim maschinellen Lernen besteht darin, dass Sie anstelle der Dokumentation in die Quellen gehen m√ºssen).  Wir werden es jedoch einfacher machen und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Keras BERT-</a> Shell verwenden (sie kann auch zur sp√§teren Feinabstimmung des Netzwerks n√ºtzlich sein, da sie eine praktische Keras-Schnittstelle bietet). </p><br><p>  Installieren Sie dazu Keras selbst: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  Und nach Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Wir ben√∂tigen auch die Datei tokenization.py aus dem urspr√ºnglichen Github BERT.  Klicken Sie entweder auf die Schaltfl√§che Raw und speichern Sie sie im Ordner mit dem zuk√ºnftigen Skript, oder laden Sie das gesamte Repository herunter und nehmen Sie die Datei von dort, oder nehmen Sie eine Kopie aus dem Repository mit diesem Code <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Jetzt ist es Zeit, das vorab trainierte neuronale Netzwerk herunterzuladen.  Es gibt verschiedene Optionen f√ºr BERT, die alle auf der offiziellen Seite <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/google-research/bert aufgef√ºhrt</a> sind.  Wir werden die universelle mehrsprachige "BERT-Base, Multilingual Cased" f√ºr 104 Sprachen verwenden.  Laden Sie die Datei <a href="">multi_cased_L-12_H-768_A-12.zip</a> (632 MB) herunter und entpacken Sie sie mit dem zuk√ºnftigen Skript in den Ordner. </p><br><p>  Alles ist fertig, erstellen Sie die BERT.py-Datei, dann wird es ein bisschen Code geben. </p><br><p>  Importieren Sie die erforderlichen Bibliotheken und legen Sie die Pfade fest </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Da wir gew√∂hnliche Textzeilen in ein spezielles Format von Token √ºbersetzen m√ºssen, werden wir daf√ºr ein spezielles Objekt erstellen.  Achten Sie auf do_lower_case = False, da wir das Cased BERT-Modell verwenden, bei dem zwischen Gro√ü- und Kleinschreibung unterschieden wird. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Modell laden </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT kann in zwei Modi arbeiten: Erraten der in der Phrase fehlenden W√∂rter oder Erraten, ob die zweite Phrase nach der ersten logisch ist.  Wir werden beide Optionen ausf√ºhren. </p><br><p>  F√ºr den ersten Modus m√ºssen Sie eine Phrase im folgenden Format einreichen: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  Das neuronale Netzwerk sollte einen vollst√§ndigen Satz mit den Worten anstelle der Masken zur√ºckgeben: "Ich bin in den Laden gekommen und habe Milch gekauft." </p><br><p>  F√ºr den zweiten Modus m√ºssen beide durch ein Trennzeichen getrennten Phrasen dem Eingang des neuronalen Netzwerks zugef√ºhrt werden: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  Das neuronale Netzwerk muss antworten, ob die zweite Phrase eine logische Fortsetzung der ersten ist.  Oder ist es eine zuf√§llige Phrase, die nichts mit der ersten zu tun hat? </p><br><p>  Damit BERT funktioniert, m√ºssen Sie drei Vektoren mit einer L√§nge von jeweils 512 Zahlen vorbereiten: token_input, seg_input und mask_input. </p><br><p>  <strong>Token_input</strong> speichert unseren Quellcode, der mit dem Tokenizer in Token √ºbersetzt wurde.  Die Phrase in Form von Indizes im W√∂rterbuch befindet sich am Anfang dieses Vektors, und der Rest wird mit Nullen gef√ºllt. </p><br><p>  In <strong>mask_input m√ºssen</strong> wir 1 f√ºr alle Positionen setzen, an denen sich die [MASK] -Maske befindet, und den Rest mit Nullen f√ºllen. </p><br><p>  In <strong>seg_input m√ºssen</strong> wir <strong>die</strong> erste Phrase (einschlie√ülich des Start-CLS- und SEP-Trennzeichens) als 0, die zweite Phrase (einschlie√ülich der End-SEP) als 1 bezeichnen und den Rest bis zum Ende des Vektors mit Nullen f√ºllen. </p><br><p>  BERT verwendet kein W√∂rterbuch ganzer W√∂rter, sondern der h√§ufigsten Silben.  Obwohl es auch ganze W√∂rter hat.  Sie k√∂nnen die Datei vocab.txt im heruntergeladenen neuronalen Netzwerk √∂ffnen und sehen, welche W√∂rter das neuronale Netzwerk an seiner Eingabe verwendet.  Es gibt ganze W√∂rter wie Frankreich.  Aber die meisten russischen W√∂rter m√ºssen in Silben zerlegt werden.  Das Wort "kam" sollte also in "mit" und "## ging" unterteilt werden.  Um die Konvertierung regul√§rer Textzeilen in das von BERT geforderte Format zu erleichtern, verwenden wir das Modul tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Modus 1: Vorhersage von W√∂rtern, die durch Token [MASK] in einer Phrase geschlossen wurden </h2><br><p>  Die Eingabephrase, die dem Eingang des neuronalen Netzwerks zugef√ºhrt wird </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Konvertiere es in Token.  Das Problem ist, dass der Tokenizer keine Dienstmarken wie [CLS] und [MASK] verarbeiten kann, obwohl vocab.txt sie im W√∂rterbuch enth√§lt.  Daher m√ºssen wir unsere Linie manuell mit [MASK] -Markierungen unterbrechen und Klartextst√ºcke ausw√§hlen, um sie mithilfe des Tokenizers in BERT-Token zu konvertieren.  F√ºgen Sie au√üerdem [CLS] am Anfang und [SEP] am Ende der Phrase hinzu. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Token verf√ºgen jetzt √ºber Token, die garantiert in Indizes im W√∂rterbuch konvertiert werden.  Lass es uns tun: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Jetzt gibt es in token_input eine Reihe von Zahlen (Wortnummern im Vokabular "vocab.txt"), die dem Eingang des neuronalen Netzwerks zugef√ºhrt werden m√ºssen.  Es bleibt nur, diesen Vektor auf eine L√§nge von 512 Elementen zu erweitern.  Das Python-Konstrukt [0] * length erstellt ein Array mit L√§ngenl√§ngen, das mit Nullen gef√ºllt ist.  F√ºgen Sie es einfach unseren Token hinzu, die in Python zwei Arrays zu einem kombinieren. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Erstellen Sie nun eine Maskenmaske mit einer L√§nge von 512, setzen Sie √ºberall 1, wobei die Zahl 103 in Token erscheint (was der Markierung [MASK] im Vokabular "vocab.txt" entspricht), und f√ºllen Sie den Rest mit 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  F√ºr den ersten BERT-Betriebsmodus muss seg_input vollst√§ndig mit Nullen gef√ºllt sein: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Im letzten Schritt m√ºssen Sie Python-Arrays in Numpy-Arrays mit der Form (1.512) konvertieren, f√ºr die wir sie in ein Subarray [] einf√ºgen: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  OK, fertig.  F√ºhren Sie nun die Vorhersage des neuronalen Netzwerks aus! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Formatieren Sie nun das Ergebnis von Token zur√ºck in eine durch Leerzeichen getrennte Zeichenfolge </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  Und geben Sie das Ergebnis aus: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  In unserem Beispiel f√ºr den Satz "Ich bin zu [MASK] gekommen und habe [MASK] gekauft."  Das neuronale Netz erzeugte das Ergebnis "Haus" und "es": "Ich kam zum Haus und kaufte es."  Zum ersten Mal nicht so schlimm.  Ein Haus zu kaufen ist definitiv besser als Milch. </p><br><div class="spoiler">  <b class="spoiler_title">Andere Beispiele (ich gebe keine erfolglosen an, es gibt viel mehr als erfolgreiche. In den meisten F√§llen gibt das Netzwerk eine leere Antwort):</b> <div class="spoiler_text"><p>  Die Erde ist die dritte [MASKE] von der Sonne <br>  Ergebnis: Stern </p><br><p>  bestes Sandwich [MASKE] mit Butter <br>  Ergebnis: Erf√ºllt </p><br><p>  Nach [MASKE] soll das Mittagessen schlafen <br>  Ergebnis: davon </p><br><p>  Geh weg von [MASKE] <br>  Ergebnis: ## oh - ist das eine Art Fluch?  ) </p><br><p>  [MASKE] von der T√ºr <br>  Ergebnis: Ansicht </p><br><p>  Mit [MASKE] k√∂nnen Hammer und N√§gel Schrank machen <br>  Ergebnis: Hilfe </p><br><p>  Und wenn morgen nicht ist?  Heute ist es zum Beispiel nicht [MASKE]! <br>  Ergebnis: wird sein </p><br><p>  Wie k√∂nnen Sie es leid werden, [MASKE] zu ignorieren? <br>  Ergebnis: sie </p><br><p>  Es gibt allt√§gliche Logik, es gibt weibliche Logik, aber √ºber die m√§nnliche [MASKE] ist nichts bekannt. <br>  Ergebnis: Philosophie </p><br><p>  Bei Frauen wird im Alter von 30 Jahren ein Bild des Prinzen gebildet, das zu jeder [MASKE] passt. <br>  Ergebnis: Mann </p><br><p>  Mit einer Mehrheit stimmten Schneewittchen und die sieben Zwerge mit einer Gegenstimme f√ºr [MASKE]. <br>  Ergebnis: Dorf - der erste Buchstabe ist korrekt </p><br><p>  Bewerten Sie Ihre Langeweile auf einer 10-Punkte-Skala: [MASKE] Punkte <br>  Ergebnis: 10 </p><br><p>  Deine [MASKE], [MASKE] und [MASKE]! <br>  Ergebnis: Lieb mich, ich - nein, BERT, ich habe es √ºberhaupt nicht so gemeint </p></div></div><br><p>  Sie k√∂nnen englische Phrasen eingeben (und jede in 104 Sprachen, von denen eine Liste <a href="">hier ist</a> ) </p><br><p>  [MASKE] muss weitergehen! <br>  Ergebnis: I. </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Modus 2: √úberpr√ºfen der Konsistenz von zwei Phrasen </h2><br><p>  Wir setzen zwei aufeinanderfolgende Phrasen, die dem Eingang des neuronalen Netzwerks zugef√ºhrt werden </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Wir werden Token im Format [CLS] Phrase_1 [SEP] Phrase_2 [SEP] erstellen und mit dem Tokenizer einfachen Text in Token konvertieren: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Wir konvertieren String-Token in numerische Indizes (Wortnummern im Vokabular "vocab.txt") und erweitern den Vektor auf 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Die Wortmaske ist in diesem Fall vollst√§ndig mit Nullen gef√ºllt </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Die Vorschlagsmaske sollte jedoch unter der zweiten Phrase (einschlie√ülich der endg√ºltigen SEP) mit Einheiten und alles andere mit Nullen ausgef√ºllt werden: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Wir leiten die Phrasen durch das neuronale Netzwerk (diesmal ist das Ergebnis in [1] und nicht in [0], wie oben). </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  Und wir leiten die Wahrscheinlichkeit ab, dass die zweite Phrase eine normale und keine zuf√§llige Menge von W√∂rtern ist </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  In zwei S√§tze: </p><br><p>  Ich kam in den Laden.  -&gt; Und kaufte Milch. </p><br><p>  Antwort des neuronalen Netzwerks: </p><br><p>  Satz ist ok: 99% </p><br><p>  Und wenn der zweite Satz "Crucian Sky Pluto" lautet, lautet die Antwort: </p><br><p>  Satz ist ok: 4% </p><br><h2 id="google-colab">  Google Colab </h2><br><p>  Google bietet eine kostenlose Tesla K80-Server-GPU mit 12 GB Videospeicher an (TPUs sind jetzt verf√ºgbar, ihre Konfiguration ist jedoch etwas komplizierter).  Der gesamte Code f√ºr Colab sollte als Jupyter-Notizbuch konzipiert sein.  Um BERT in einem Browser zu starten, √∂ffnen Sie einfach den Link </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  <strong>W√§hlen Sie im</strong> Men√º <strong>Laufzeit</strong> die <strong>Option</strong> <strong>Alle ausf√ºhren aus</strong> , damit zum ersten Mal alle Zellen gestartet werden, die Modelldownloads und die erforderlichen Bibliotheken verbunden werden.  Stimmen Sie zu, bei Bedarf alle Runtime zur√ºckzusetzen. </p><br><div class="spoiler">  <b class="spoiler_title">Wenn etwas schief gelaufen ist ...</b> <div class="spoiler_text"><p>  Stellen Sie sicher, dass GPU und Python 3 im Men√º Laufzeit -&gt; Laufzeittyp √§ndern ausgew√§hlt sind </p><br><p>  Wenn die Schaltfl√§che Verbinden nicht aktiv ist, klicken Sie darauf, um eine Verbindung herzustellen. </p></div></div><br><p>  √Ñndern Sie nun die Eingabezeilen <strong>Satz</strong> , <strong>Satz_1</strong> und <strong>Satz_2</strong> und klicken Sie links auf das Wiedergabesymbol, um nur die aktuelle Zelle zu starten.  Das Ausf√ºhren des gesamten Notebooks ist nicht mehr erforderlich. </p><br><p>  Sie k√∂nnen BERT in Google Colab auch von einem Smartphone aus ausf√ºhren. Wenn es jedoch nicht ge√∂ffnet wird, m√ºssen Sie m√∂glicherweise das Kontrollk√§stchen Vollversion in Ihren Browsereinstellungen aktivieren. </p><br><h2 id="chto-dalshe">  Was weiter? </h2><br><p>  Um BERT f√ºr eine bestimmte Aufgabe zu trainieren, m√ºssen Sie eine oder zwei Schichten eines einfachen Feed-Forward-Netzwerks hinzuf√ºgen und nur trainieren, ohne das Haupt-BERT-Netzwerk zu ber√ºhren.  Dies kann entweder auf nacktem TensorFlow oder √ºber die Keras BERT-Shell erfolgen.  Ein solches zus√§tzliches Training f√ºr eine bestimmte Dom√§ne erfolgt sehr schnell und ist der Feinabstimmung in Faltungsnetzwerken v√∂llig √§hnlich.  F√ºr die SQuAD-Aufgabe k√∂nnen Sie also in nur 30 Minuten ein neuronales Netzwerk auf einer TPU trainieren (verglichen mit 4 Tagen auf 16 TPU f√ºr das Training von BERT selbst). </p><br><p>  Dazu m√ºssen Sie untersuchen, wie die letzten Ebenen in BERT dargestellt werden, und √ºber einen geeigneten Datensatz verf√ºgen.  Auf der offiziellen BERT-Seite <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/google-research/bert</a> finden Sie mehrere Beispiele f√ºr verschiedene Aufgaben sowie Anweisungen zum Starten der Umschulung f√ºr Cloud-TPUs.  Und alles andere muss in der Quelle in den Dateien <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">run_classifier.py</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">extract_features.py nachsehen</a> . </p><br><h3 id="ps">  PS </h3><br><p>  Der hier vorgestellte Code und das Jupyter-Notizbuch f√ºr Google Colab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>werden im Repository gehostet</strong></a> . </p><br><p>  Wunder sind nicht zu erwarten.  Erwarten Sie nicht, dass BERT wie eine Person spricht.  Der Status des Standes der Technik bedeutet keineswegs, dass die Fortschritte in der NLP ein akzeptables Niveau erreicht haben.  Es bedeutet nur, dass BERT besser ist als fr√ºhere Modelle, die noch schlechter waren.  Starke Konversations-KI ist noch sehr weit weg.  Dar√ºber hinaus ist BERT in erster Linie ein Sprachmodell und kein vorgefertigter Chat-Bot. Daher werden gute Ergebnisse erst nach einer Umschulung f√ºr eine bestimmte Aufgabe angezeigt. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436878/">https://habr.com/ru/post/de436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436866/index.html">Wie man gef√§lschte agile Projekte erkennt</a></li>
<li><a href="../de436868/index.html">Betten Sie statische Analysen in den Prozess ein und suchen Sie nicht nach Fehlern</a></li>
<li><a href="../de436872/index.html">PGConf.Russia 2019 kommt bald</a></li>
<li><a href="../de436874/index.html">Neujahrst√§nze rund um den FC-Adapter oder eine Geschichte dar√ºber, wie weit die Ursachen des Problems von den Symptomen entfernt sind</a></li>
<li><a href="../de436876/index.html">[SAP] SAPUI5 f√ºr Dummies Teil 1: Eine vollst√§ndige Schritt-f√ºr-Schritt-√úbung</a></li>
<li><a href="../de436880/index.html">Grundlagen der C ++ - Vorlage: Funktionsvorlagen</a></li>
<li><a href="../de436884/index.html">Wir beherrschen Async / Warten auf ein echtes Beispiel</a></li>
<li><a href="../de436886/index.html">Verwenden von Babel und Webpack zum Einrichten eines React-Projekts von Grund auf neu</a></li>
<li><a href="../de436888/index.html">Geschichte zum Entwerfen einer API</a></li>
<li><a href="../de436890/index.html">React Tutorial Teil 10: Workshop zum Arbeiten mit Komponenteneigenschaften und zum Styling</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>