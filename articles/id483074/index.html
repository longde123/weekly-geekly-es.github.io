<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>◀️ 🎷 🐖 TensorRT 6.xxx - inferensi kinerja tinggi untuk model pembelajaran dalam (Deteksi Objek dan Segmentasi) 💆🏽 🔮 🤱🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Itu hanya menyakitkan untuk pertama kalinya! 

 Halo semuanya! Teman-teman yang terkasih, dalam artikel ini saya ingin berbagi pengalaman menggunakan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>TensorRT 6.xxx - inferensi kinerja tinggi untuk model pembelajaran dalam (Deteksi Objek dan Segmentasi)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/483074/"><img src="https://habrastorage.org/webt/qr/je/yu/qrjeyup390z-iv5uyduultmmxcs.png" alt="gambar"><br>  <i>Itu hanya menyakitkan untuk pertama kalinya!</i> <br><br>  Halo semuanya!  Teman-teman yang terkasih, dalam artikel ini saya ingin berbagi pengalaman menggunakan TensorRT, RetinaNet berdasarkan repositori <a href="https://github.com/aidonchuk/retinanet-examples">github.com/aidonchuk/retinanet-examples</a> (ini adalah percabangan turnkey resmi dari <a href="https://github.com/NVIDIA/retinanet-examples">nvidia</a> , yang akan memungkinkan kita untuk mulai menggunakan model yang dioptimalkan dalam produksi sesegera mungkin).  <a href="https://ods.ai/">Menggulir melalui saluran</a> komunitas <a href="https://ods.ai/">ods.ai</a> , saya menemukan pertanyaan tentang menggunakan TensorRT, dan sebagian besar pertanyaan diulang, jadi saya memutuskan untuk menulis panduan selengkap <i>mungkin</i> untuk menggunakan inferensi cepat berdasarkan TensorRT, RetinaNet, Unet, dan docker. <br><a name="habracut"></a><br>  <b>Deskripsi tugas</b> <br><br>  Saya mengusulkan pengaturan tugas dengan cara ini: kita perlu menandai dataset, melatih jaringan RetinaNet / Unet pada Pytorch1.3 + di atasnya, mengonversi bobot yang diterima menjadi ONNX, kemudian mengonversinya ke mesin TensorRT dan menjalankan semua ini di buruh pelabuhan, terutama di Ubuntu 18 dan sangat lebih disukai pada arsitektur ARM (Jetson) *, sehingga meminimalkan penyebaran lingkungan secara manual.  Sebagai hasilnya, kita akan mendapatkan wadah yang siap tidak hanya untuk ekspor dan pelatihan RetinaNet / Unet, tetapi juga untuk pengembangan penuh dan pelatihan klasifikasi, segmentasi dengan semua ikatan yang diperlukan. <br><cut></cut><br>  <b>Tahap 1. Persiapan lingkungan</b> <br><br>  Penting untuk dicatat di sini bahwa baru-baru ini saya benar-benar meninggalkan penggunaan dan penyebaran setidaknya beberapa perpustakaan di mesin desktop, serta pada devbox.  Satu-satunya hal yang harus Anda buat dan instal adalah lingkungan virtual python dan cuda 10.2 (Anda dapat membatasi diri pada satu driver nvidia) dari deb. <br><br>  Misalkan Anda memiliki Ubuntu yang baru diinstal 18. Instal cuda 10.2 (deb), saya tidak akan membahas proses instalasi secara rinci, dokumentasi resmi cukup. <br><br>  Sekarang kita akan menginstal buruh pelabuhan, panduan pemasangan buruh pelabuhan dapat dengan mudah ditemukan, berikut adalah contoh <a href="https://www.digitalocean.com/community/tutorials/docker-ubuntu-18-04-1-ru">www.digitalocean.com/community/tutorials/docker-ubuntu-18-04-1-en</a> , versi 19+ sudah tersedia - taruh saja.  Nah, jangan lupa untuk memungkinkan menggunakan buruh pelabuhan tanpa sudo, itu akan lebih nyaman.  Setelah semuanya berubah, kami melakukan ini: <br><br><pre><code class="bash hljs">distribution=$(. /etc/os-release;<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$ID</span></span><span class="hljs-variable"><span class="hljs-variable">$VERSION_ID</span></span>) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/<span class="hljs-variable"><span class="hljs-variable">$distribution</span></span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker</code> </pre> <br>  Dan Anda bahkan tidak dapat melihat ke repositori resmi <a href="https://github.com/NVIDIA/nvidia-docker">github.com/NVIDIA/nvidia-docker</a> . <br><br>  Sekarang lakukan git clone <a href="https://github.com/aidonchuk/retinanet-examples">github.com/aidonchuk/retinanet-examples</a> . <br><br>  Tetap sedikit, untuk mulai menggunakan buruh pelabuhan dengan nvidia-image, kita perlu mendaftar di NGC Cloud dan masuk.  Kami pergi ke sini <a href="https://ngc.nvidia.com/">ngc.nvidia.com</a> , mendaftar dan setelah kami masuk ke dalam NGC Cloud, tekan SETUP di sudut kiri atas layar atau ikuti tautan ini <a href="https://ngc.nvidia.com/setup/api-key">ngc.nvidia.com/setup/api-key</a> .  Klik "buat kunci."  Saya sarankan menyimpannya, jika tidak saat berikutnya Anda mengunjunginya, Anda harus membuat ulang dan, karenanya, menyebarkannya pada mobil baru, jalankan kembali operasi ini. <br><br>  Jalankan: <br><br><pre> <code class="bash hljs">docker login nvcr.io Username: <span class="hljs-variable"><span class="hljs-variable">$oauthtoken</span></span> Password: &lt;Your Key&gt; -  </code> </pre><br>  Nama pengguna cukup salin.  Nah, pertimbangkan, lingkungan sudah dikerahkan! <br><cut></cut><br>  <b>Tahap 2. Merakit wadah buruh pelabuhan</b> <br><br>  Pada tahap kedua dari pekerjaan kami, kami akan mengumpulkan buruh pelabuhan dan berkenalan dengan bagian dalamnya. <br>  Mari kita pergi ke folder root relatif terhadap proyek contoh retina dan jalankan <br><br><pre> <code class="bash hljs">docker build --build-arg USER=<span class="hljs-variable"><span class="hljs-variable">$USER</span></span> --build-arg UID=<span class="hljs-variable"><span class="hljs-variable">$UID</span></span> --build-arg GID=<span class="hljs-variable"><span class="hljs-variable">$GID</span></span> --build-arg PW=alex -t retinanet:latest retinanet/</code> </pre><br>  Kami mengumpulkan buruh pelabuhan dengan melemparkan pengguna saat ini ke dalamnya - ini sangat berguna jika Anda menulis sesuatu pada VOLUME yang dipasang dengan hak pengguna saat ini, jika tidak akan ada root dan sakit. <br><br>  Sementara buruh pelabuhan akan pergi, mari kita menjelajahi Dockerfile: <br><br><pre> <code class="powershell hljs">FROM nvcr.io/nvidia/pytorch:<span class="hljs-number"><span class="hljs-number">19.10</span></span><span class="hljs-literal"><span class="hljs-literal">-py3</span></span> ARG USER=alex ARG UID=<span class="hljs-number"><span class="hljs-number">1000</span></span> ARG GID=<span class="hljs-number"><span class="hljs-number">1000</span></span> ARG PW=alex RUN useradd <span class="hljs-literal"><span class="hljs-literal">-m</span></span> <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} -<span class="hljs-literal"><span class="hljs-literal">-uid</span></span>=<span class="hljs-variable"><span class="hljs-variable">$</span></span>{UID} &amp;&amp; echo <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$</span></span></span><span class="hljs-string">{USER}:</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$</span></span></span><span class="hljs-string">{PW}"</span></span> | chpasswd RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> update &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install curl &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install wget &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install git &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install automake &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> sudo &amp;&amp; adduser <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} sudo RUN pip install git+https://github.com/bonlime/pytorch<span class="hljs-literal"><span class="hljs-literal">-tools</span></span>.git@master COPY . retinanet/ RUN pip install -<span class="hljs-literal"><span class="hljs-literal">-no</span></span><span class="hljs-literal"><span class="hljs-literal">-cache</span></span><span class="hljs-literal"><span class="hljs-literal">-dir</span></span> <span class="hljs-literal"><span class="hljs-literal">-e</span></span> retinanet/ RUN pip install /workspace/retinanet/extras/tensorrt<span class="hljs-literal"><span class="hljs-literal">-6</span></span>.<span class="hljs-number"><span class="hljs-number">0.1</span></span>.<span class="hljs-number"><span class="hljs-number">5</span></span><span class="hljs-literal"><span class="hljs-literal">-cp36</span></span><span class="hljs-literal"><span class="hljs-literal">-none</span></span><span class="hljs-literal"><span class="hljs-literal">-linux_x86_64</span></span>.whl RUN pip install tensorboardx RUN pip install albumentations RUN pip install setproctitle RUN pip install paramiko RUN pip install flask RUN pip install mem_top RUN pip install arrow RUN pip install pycuda RUN pip install torchvision RUN pip install pretrainedmodels RUN pip install efficientnet<span class="hljs-literal"><span class="hljs-literal">-pytorch</span></span> RUN pip install git+https://github.com/qubvel/segmentation_models.pytorch RUN pip install pytorch_toolbelt RUN chown <span class="hljs-literal"><span class="hljs-literal">-R</span></span> <span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER}:<span class="hljs-variable"><span class="hljs-variable">$</span></span>{USER} retinanet/ RUN cd /workspace/retinanet/extras/cppapi &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake <span class="hljs-literal"><span class="hljs-literal">-DCMAKE_CUDA_FLAGS</span></span>=<span class="hljs-string"><span class="hljs-string">"--expt-extended-lambda -std=c++14"</span></span> .. &amp;&amp; make &amp;&amp; cd /workspace RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> openssh<span class="hljs-literal"><span class="hljs-literal">-server</span></span> &amp;&amp; apt install <span class="hljs-literal"><span class="hljs-literal">-y</span></span> tmux &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install bison flex &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-cache</span></span> search pcre &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install net<span class="hljs-literal"><span class="hljs-literal">-tools</span></span> &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nmap RUN apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install libpcre3 libpcre3<span class="hljs-literal"><span class="hljs-literal">-dev</span></span> &amp;&amp; apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install iputils<span class="hljs-literal"><span class="hljs-literal">-ping</span></span> RUN mkdir /var/run/sshd RUN echo <span class="hljs-string"><span class="hljs-string">'root:pass'</span></span> | chpasswd RUN sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/'</span></span> /etc/ssh/sshd_config RUN sed <span class="hljs-string"><span class="hljs-string">'s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g'</span></span> <span class="hljs-literal"><span class="hljs-literal">-i</span></span> /etc/pam.d/sshd ENV NOTVISIBLE <span class="hljs-string"><span class="hljs-string">"in users profile"</span></span> RUN echo <span class="hljs-string"><span class="hljs-string">"export VISIBLE=now"</span></span> &gt;&gt; /etc/profile CMD [<span class="hljs-string"><span class="hljs-string">"/usr/sbin/sshd"</span></span>, <span class="hljs-string"><span class="hljs-string">"-D"</span></span>]</code> </pre><br>  Seperti yang Anda lihat dari teks, kami mengambil semua yang favorit kami, mengkompilasi retinanet, mendistribusikan alat-alat dasar untuk kenyamanan bekerja dengan Ubuntu, dan mengkonfigurasi server openssh.  Baris pertama hanyalah pewarisan gambar nvidia, yang untuknya kami membuat login di NGC Cloud dan yang berisi Pytorch1.3, TensorRT6.xxx dan sekelompok lib yang memungkinkan kami untuk mengkompilasi kode sumber cpp untuk detektor kami. <br><cut></cut><br>  <b>Tahap 3. Memulai dan men-debug kontainer buruh pelabuhan</b> <br><br>  Mari kita beralih ke kasus utama menggunakan wadah dan lingkungan pengembangan, untuk memulai, jalankan nvidia docker.  Jalankan: <br><br><pre> <code class="bash hljs">docker run --gpus all --net=host -v /home/&lt;your_user_name&gt;:/workspace/mounted_vol -d -P --rm --ipc=host -it retinanet:latest</code> </pre> <br>  Sekarang wadah tersedia di ssh &lt;curr_user_name&gt; @localhost.  Setelah peluncuran yang berhasil, buka proyek di PyCharm.  Selanjutnya, buka <br><br><pre> <code class="bash hljs">Settings-&gt;Project Interpreter-&gt;Add-&gt;Ssh Interpreter</code> </pre> <br>  <i>Langkah 1</i> <br><img src="https://habrastorage.org/webt/g0/qc/e4/g0qce4xw2pe0arglt4b4iu4jfle.png" alt="gambar"><br><br>  <i>Langkah 2</i> <br><img src="https://habrastorage.org/webt/nf/m_/cu/nfm_cuj84kymlgofajbo-go8dwy.png" alt="gambar"><br><br>  <i>Langkah 3</i> <br><img src="https://habrastorage.org/webt/w6/nn/eg/w6nnegihsdou1fhoy75p7g9evhi.png" alt="gambar"><br><br>  Kami memilih semuanya seperti pada tangkapan layar, <br><br><pre> <code class="bash hljs">Interpreter -&gt; /opt/conda/bin/python</code> </pre> <br>  - ini akan berada di Python3.6 dan <br><br><pre> <code class="bash hljs">Sync folder -&gt; /workspace/retinanet</code> </pre> <br>  Kami menekan garis finish, kami mengharapkan pengindeksan, dan hanya itu, lingkungan siap digunakan! <br><br>  <i><b>PENTING !!!</b></i>  Segera setelah pengindeksan, ekstrak file yang dikompilasi untuk Retinanet dari buruh pelabuhan.  Di menu konteks di root proyek, pilih <br><br><pre> <code class="bash hljs">Deployment-&gt;Download</code> </pre> <br>  Satu file dan dua folder build, retinanet.egg-info dan _so akan muncul <br><br><img src="https://habrastorage.org/webt/ti/uo/pp/tiuoppum1j_wx2yfxlkl8i-gdle.png" alt="gambar"><br><br>  Jika proyek Anda terlihat seperti ini, maka lingkungan melihat semua file yang diperlukan dan kami siap mempelajari RetinaNet. <br><br>  <b>Tahap 4. Menandai data dan melatih detektor</b> <br><br>  Untuk markup, saya terutama menggunakan <a href="https://supervise.ly/">supervise.ly</a> - alat yang bagus dan nyaman, di saat terakhir sekelompok kusen diperbaiki dan menjadi berperilaku lebih baik. <br><br>  Misalkan Anda menandai dataset dan mengunduhnya, tetapi tidak akan langsung berfungsi untuk memasukkannya ke RetinaNet kami, karena itu dalam formatnya sendiri dan untuk ini kami perlu mengubahnya menjadi COCO.  Alat konversi ada di: <br><br><pre> <code class="bash hljs">markup_utils/supervisly_to_coco.py</code> </pre> <br>  Harap perhatikan bahwa Kategori dalam skrip adalah contoh dan Anda harus memasukkannya sendiri (Anda tidak perlu menambahkan kategori latar belakang) <br><br><pre> <code class="json hljs">categories = [{'id': <span class="hljs-number"><span class="hljs-number">1</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">1</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">2</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">2</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">3</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">3</span></span>'}, {'id': <span class="hljs-number"><span class="hljs-number">4</span></span>, 'name': '<span class="hljs-number"><span class="hljs-number">4</span></span>'}]</code> </pre> <br>  Untuk beberapa alasan, penulis repositori asli memutuskan bahwa Anda tidak akan melatih apa pun kecuali COCO / VOC untuk deteksi, jadi saya harus sedikit memodifikasi file sumber <br><br><pre> <code class="bash hljs">retinanet/dataset.py</code> </pre> <br>  Dengan menambahkan tutda album tambahan augmentasi <a href="https://albumentations.readthedocs.io/en/latest/">Anda.readthedocs.io/en/latest</a> dan hilangkan kategori terprogram dari COCO.  Dimungkinkan juga untuk memercikkan area deteksi besar jika Anda mencari objek kecil dalam gambar besar, Anda memiliki dataset kecil =), dan tidak ada yang berhasil, tetapi lebih pada waktu lain. <br><br>  Secara umum, loop kereta juga lemah, awalnya tidak menyimpan pos-pos pemeriksaan, menggunakan semacam penjadwal yang buruk, dll.  Tapi sekarang yang harus Anda lakukan adalah memilih backbone dan mengeksekusi <br><br><pre> <code class="bash hljs">/opt/conda/bin/python retinanet/main.py</code> </pre> <br>  dengan parameter: <br><br><pre> <code class="bash hljs">train retinanet_rn34fpn.pth --backbone ResNet34FPN --classes 12 --val-iters 10 --images /workspace/mounted_vol/dataset/train/images --annotations /workspace/mounted_vol/dataset/train_12_class.json --val-images /workspace/mounted_vol/dataset/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>/images_small --val-annotations /workspace/mounted_vol/dataset/val_10_class_cropped.json --jitter 256 512 --max-size 512 --batch 32</code> </pre><br>  Di konsol Anda akan melihat: <br><br><pre> <code class="plaintext hljs">Initializing model... model: RetinaNet backbone: ResNet18FPN classes: 2, anchors: 9 Selected optimization level O0: Pure FP32 training. Defaults for this optimization level are: enabled : True opt_level : O0 cast_model_type : torch.float32 patch_torch_functions : False keep_batchnorm_fp32 : None master_weights : False loss_scale : 1.0 Processing user overrides (additional kwargs that are not None)... After processing overrides, optimization options are: enabled : True opt_level : O0 cast_model_type : torch.float32 patch_torch_functions : False keep_batchnorm_fp32 : None master_weights : False loss_scale : 128.0 Preparing dataset... loader: pytorch resize: [1024, 1280], max: 1280 device: 4 gpus batch: 4, precision: mixed Training model for 20000 iterations... [ 1/20000] focal loss: 0.95619, box loss: 0.51584, 4.042s/4-batch (fw: 0.698s, bw: 0.459s), 1.0 im/s, lr: 0.0001 [ 12/20000] focal loss: 0.76191, box loss: 0.31794, 0.187s/4-batch (fw: 0.055s, bw: 0.133s), 21.4 im/s, lr: 0.0001 [ 24/20000] focal loss: 0.65036, box loss: 0.30269, 0.173s/4-batch (fw: 0.045s, bw: 0.128s), 23.1 im/s, lr: 0.0001 [ 36/20000] focal loss: 0.46425, box loss: 0.23141, 0.178s/4-batch (fw: 0.047s, bw: 0.131s), 22.4 im/s, lr: 0.0001 [ 48/20000] focal loss: 0.45115, box loss: 0.23505, 0.180s/4-batch (fw: 0.047s, bw: 0.133s), 22.2 im/s, lr: 0.0001 [ 59/20000] focal loss: 0.38958, box loss: 0.25373, 0.184s/4-batch (fw: 0.049s, bw: 0.134s), 21.8 im/s, lr: 0.0001 [ 71/20000] focal loss: 0.37733, box loss: 0.23988, 0.174s/4-batch (fw: 0.049s, bw: 0.125s), 22.9 im/s, lr: 0.0001 [ 83/20000] focal loss: 0.39514, box loss: 0.23878, 0.181s/4-batch (fw: 0.048s, bw: 0.133s), 22.1 im/s, lr: 0.0001 [ 94/20000] focal loss: 0.39947, box loss: 0.23817, 0.185s/4-batch (fw: 0.050s, bw: 0.134s), 21.6 im/s, lr: 0.0001 [ 105/20000] focal loss: 0.37343, box loss: 0.20238, 0.182s/4-batch (fw: 0.048s, bw: 0.134s), 22.0 im/s, lr: 0.0001 [ 116/20000] focal loss: 0.19689, box loss: 0.17371, 0.183s/4-batch (fw: 0.050s, bw: 0.132s), 21.8 im/s, lr: 0.0001 [ 128/20000] focal loss: 0.20368, box loss: 0.16538, 0.178s/4-batch (fw: 0.046s, bw: 0.131s), 22.5 im/s, lr: 0.0001 [ 140/20000] focal loss: 0.22763, box loss: 0.15772, 0.176s/4-batch (fw: 0.050s, bw: 0.126s), 22.7 im/s, lr: 0.0001 [ 148/20000] focal loss: 0.21997, box loss: 0.18400, 0.585s/4-batch (fw: 0.047s, bw: 0.144s), 6.8 im/s, lr: 0.0001 Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.52674 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.91450 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.35172 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61881 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.58824 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.61765 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.00000 Saving model: 148</code> </pre> <br>  Untuk mempelajari seluruh rangkaian parameter, lihat <br><br><pre> <code class="bash hljs">retinanet/main.py</code> </pre> <br>  Secara umum, mereka standar untuk deteksi, dan mereka memiliki deskripsi.  Jalankan pelatihan dan tunggu hasilnya.  Contoh inferensi dapat ditemukan di: <br><br><pre> <code class="bash hljs">retinanet/infer_example.py</code> </pre> <br>  atau jalankan perintah: <br><br><pre> <code class="bash hljs">/opt/conda/bin/python retinanet/main.py infer retinanet_rn34fpn.pth --images /workspace/mounted_vol/dataset/<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>/images --annotations /workspace/mounted_vol/dataset/val.json --output result.json --resize 256 --max-size 512 --batch 32</code> </pre><br>  Kehilangan fokus dan beberapa tulang punggung sudah dibangun ke dalam repositori, dan mereka <br><br><pre> <code class="bash hljs">retinanet/backbones/*.py</code> </pre> <br>  Para penulis memberikan beberapa karakteristik dalam papan nama: <br><br><img src="https://habrastorage.org/webt/z2/rj/3c/z2rj3cuo4rvxgnqfdx-rntclfm0.png" alt="gambar"><br><br>  Ada juga tulang punggung ResNeXt50_32x4dFPN dan ResNeXt101_32x8dFPN, yang diambil dari torchvision. <br>  Saya harap kami sedikit mengetahui pendeteksiannya, tetapi Anda harus membaca dokumentasi resmi untuk <i><b>memahami mode ekspor dan logging</b></i> . <br><br>  <b>Tahap 5. Ekspor dan inferensi model Unet dengan encnet Resnet</b> <br><br>  Seperti yang mungkin Anda perhatikan, pustaka untuk segmentasi dipasang di Dockerfile, dan khususnya pustaka hebat <a href="https://github.com/qubvel/segmentation_models.pytorch">github.com/qubvel/segmentation_models.pytorch</a> .  Dalam paket Yunet, Anda dapat menemukan contoh inferensi dan ekspor pos pemeriksaan pytorch di mesin TensorRT. <br><br>  Masalah utama ketika mengekspor model Unet-like dari ONNX ke TensoRT adalah kebutuhan untuk menetapkan ukuran Upsample yang tetap atau menggunakan ConvTranspose2D: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.onnx.symbolic_opset9 <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> onnx_symbolic <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">upsample_nearest2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(g, input, output_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Currently, TRT 5.1/6.0 ONNX Parser does not support all ONNX ops # needed to support dynamic upsampling ONNX forumlation # Here we hardcode scale=2 as a temporary workaround scales = g.op("Constant", value_t=torch.tensor([1., 1., 2., 2.])) return g.op("Upsample", input, scales, mode_s="nearest") onnx_symbolic.upsample_nearest2d = upsample_nearest2d</span></span></code> </pre><br>  Dengan menggunakan konversi ini, Anda dapat melakukan ini secara otomatis saat mengekspor ke ONNX, tetapi sudah dalam versi 7 dari TensorRT masalah ini diselesaikan, dan kami harus menunggu sangat sedikit. <br><br>  <b>Kesimpulan</b> <br><br>  Ketika saya mulai menggunakan buruh pelabuhan, saya ragu tentang kinerjanya untuk tugas-tugas saya.  Di salah satu unit saya, sekarang ada cukup banyak lalu lintas jaringan yang dibuat oleh beberapa kamera. <br><br><img src="https://habrastorage.org/webt/3p/b9/1q/3pb91qmq_vcanxwqlskw3aervtu.png" alt="gambar"><br><br>  Berbagai tes di Internet mengungkapkan overhead yang relatif besar untuk interaksi dan perekaman jaringan pada VOLUME, ditambah GIL yang tidak diketahui dan mengerikan, dan karena memotret bingkai, operasi driver dan mentransmisikan bingkai melalui jaringan adalah operasi atom dalam mode <i>waktu-keras yang keras</i> , penundaan. online sangat penting bagi saya. <br><br>  Tapi tidak ada yang terjadi =) <br><br>  PS Masih menambahkan loop kereta favorit Anda untuk segmentasi dan produksi! <br><br>  <b>Terima kasih</b> <br><br>  Berkat komunitas <a href="http://ods.ai/">ods.ai</a> , tidak mungkin berkembang tanpa itu!  Terima kasih banyak kepada <a href="https://habr.com/ru/users/n01z3/" class="user_link">n01z3</a> , DL, yang berharap agar saya menerima DL, atas nasihatnya yang tak ternilai dan profesionalisme yang luar biasa! <br><br>  <i>Gunakan model yang dioptimalkan dalam produksi!</i> <br><br><img src="https://habrastorage.org/webt/xv/x3/yv/xvx3yvszd_twqjbrtk3rjaxrxpk.png">  Aurorai, llc </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id483074/">https://habr.com/ru/post/id483074/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id483056/index.html">Bidang logis dalam database, apakah ada penangkal?</a></li>
<li><a href="../id483058/index.html">Hasil Survei Liburan</a></li>
<li><a href="../id483064/index.html">Vue untuk blog kecil alias terkecil di semua kanon</a></li>
<li><a href="../id483066/index.html">The 5 Buku Tradisional oleh Bill Gates</a></li>
<li><a href="../id483068/index.html">Membalikkan rekayasa BattlEye anti-cheat populer</a></li>
<li><a href="../id483076/index.html">Bahasa pemrograman teratas untuk pengembangan Aplikasi Android pada tahun 2020</a></li>
<li><a href="../id483078/index.html">Pembelajaran Penguatan Mendalam: Cara Mengajari Laba-laba Berjalan</a></li>
<li><a href="../id483082/index.html">Perburuan Kerentanan 7% Lebih Efektif</a></li>
<li><a href="../id483084/index.html">Kamera dengan fungsi pelacakan</a></li>
<li><a href="../id483086/index.html">Hasil 2019: aset mana yang ternyata paling menguntungkan bagi investor Rusia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>