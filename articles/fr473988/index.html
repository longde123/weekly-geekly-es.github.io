<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äçüè´ üñêÔ∏è ‚úçüèæ Optimisation de la distribution des serveurs sur les racks üë¥üèΩ ü§µüèª üë∏üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="R√©cemment, un coll√®gue m'a demand√© lors d'un chat: 

 - Existe-t-il un article sur la fa√ßon d'emballer correctement les serveurs dans les racks? 

 J'...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Optimisation de la distribution des serveurs sur les racks</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473988/">  R√©cemment, un coll√®gue m'a demand√© lors d'un chat: <br><br>  - Existe-t-il un article sur la fa√ßon d'emballer correctement les serveurs dans les racks? <br><br>  J'ai r√©alis√© que je ne le savais pas.  J'ai donc d√©cid√© d'√©crire mon texte. <br><br>  Tout d'abord, il s'agit d'un article sur les serveurs bare metal dans les installations du centre de donn√©es (DC).  Deuxi√®mement, nous estimons qu'il y a beaucoup de serveurs (des centaines ou des milliers);  l'article n'a pas de sens pour moins de quantit√©s.  Troisi√®mement, nous consid√©rons qu'il y a trois contraintes dans les racks: l'espace physique, l'alimentation √©lectrique par chacun et les armoires restent dans les rang√©es adjacentes les unes aux autres, nous pouvons donc utiliser un seul commutateur ToR pour y connecter des serveurs. <br><a name="habracut"></a><br>  La r√©ponse √† la question d'origine d√©pend consid√©rablement du param√®tre que nous optimisons et de ce que nous pouvons changer pour obtenir un meilleur r√©sultat.  Par exemple, nous devons utiliser moins d'espace pour en laisser plus pour la croissance future.  Ou peut-√™tre que nous avons la libert√© de choisir la hauteur de l'armoire, l'alimentation par rack, le nombre de prises par PDU, le nombre d'armoires par groupe de commutateurs (un commutateur par 1, 2 ou 3 racks), les longueurs de c√¢ble et les travaux de c√¢blage.  Le dernier composant est essentiel pour les rang√©es de fin de rack o√π nous devons tirer les c√¢bles dans l'autre rang√©e ou laisser des ports sous-utilis√©s dans le commutateur.  Les histoires compl√®tement diff√©rentes sont la s√©lection du serveur et la s√©lection du centre de donn√©es.  Nous devons consid√©rer que nous les avons d√©j√† choisis. <br><br>  Il est bon de comprendre certaines nuances et d√©tails, en particulier la consommation √©lectrique moyenne / maximale du serveur et la fa√ßon dont notre fournisseur fournit l'√©lectricit√©.  Donc, si nous avons une alimentation de 230 V monophas√©e, un disjoncteur de 32 A peut supporter jusqu'√† 7 kW.  Disons que nous payons formellement 6 kW par rack.  Si un fournisseur mesure notre consommation d'√©nergie par rang√©e de 10 armoires, pas par une seule, et si les disjoncteurs limitent la puissance √† 7 kW, nous pouvons utiliser 6,9 kW dans un rack et 5,1 kW dans un autre.  Ce sera ok et impunissable. <br><br>  Habituellement, notre objectif principal est de minimiser les d√©penses.  Le meilleur crit√®re de mesure est la r√©duction du co√ªt total de possession (TCO).  Il se compose des parties suivantes: <br><br><ul><li>  CAPEX: achat d'infrastructures de centre de donn√©es, serveurs, p√©riph√©riques r√©seau, c√¢blage </li><li>  OPEX: loyer DC, consommation √©lectrique, entretien.  OPEX d√©pend de la dur√©e de vie.  Il est raisonnable de supposer qu'une dur√©e de vie est √©gale √† 3 ans. </li></ul><br><img src="https://habrastorage.org/webt/ub/ur/zi/uburzia_hkzpiosfd3k6i2agrwq.jpeg" alt="Graphique TCO"><br><br>  Nous devons optimiser les parties les plus ch√®res du g√¢teau.  Tout le reste devrait utiliser les ressources restantes aussi efficacement que possible. <br><br>  Soi-disant, nous avons un DC existant, une hauteur de rack d'unit√©s H (par exemple H = 47), une puissance par rack P <sub>rack</sub> (P <sub>rack</sub> = 6kW), et nous avons d√©cid√© d'utiliser des serveurs √† deux unit√©s h = 2U.  Supprimons 2 √† 4 unit√©s du rack pour les commutateurs, les panneaux de brassage et les gestionnaires de c√¢bles.  Ensuite, nous pouvons placer S <sub>h</sub> = arrondi ((H-2..4) / h) serveurs dans un rack (c'est-√†-dire S <sub>h</sub> = arrondi ((47-4) / 2) = 21 serveurs par rack).  M√©morisons S <sub>h</sub> . <br><br>  Dans un cas simple, tous les serveurs sont identiques.  Donc, si nous remplissons un rack par des serveurs, nous pouvons d√©penser par serveur une puissance moyenne de P <sub>serv</sub> = P <sub>rack</sub> / S <sub>h</sub> (P <sub>serv</sub> = 6000W / 21 = 287W).  Nous ignorons la consommation √©lectrique du commutateur ici. <br><br>  √âcartons-nous et d√©finissons la consommation d'√©nergie maximale du serveur P <sub>max</sub> .  La mani√®re simple, totalement s√ªre et tr√®s inefficace consiste √† lire ce que dit une √©tiquette sur le bloc d'alimentation du serveur.  Voici P <sub>max</sub> . <br><br>  Une approche plus compliqu√©e et efficace consiste √† prendre le TDP de tous les composants et √† les r√©sumer.  Ce n'est pas exact, mais nous pouvons le faire de cette fa√ßon. <br><br>  Habituellement, nous ne connaissons pas le TDP des composants en dehors du CPU.  Ainsi, l'approche la plus correcte et la plus compliqu√©e consiste √† prendre un serveur exp√©rimental correctement configur√©, √† le charger, par exemple, par / Linpack / (CPU et m√©moire) et / fio / (disques), et √† mesurer la consommation d'√©nergie.  Nous avons besoin d'un laboratoire dans ce cas.  Si nous prenons les choses au s√©rieux, nous devons cr√©er un environnement chaleureux dans l'all√©e froide car une temp√©rature plus √©lev√©e affecte √† la fois les ventilateurs et la consommation d'√©nergie du processeur.  Ainsi, nous obtenons la consommation √©lectrique maximale du serveur d'√©chantillons avec cette configuration particuli√®re dans l'environnement actuel sous la charge sp√©cifique.  N'oubliez pas qu'un nouveau micrologiciel, une nouvelle version du logiciel et d'autres conditions peuvent affecter le r√©sultat. <br><br>  Revenons maintenant √† P <sub>serv</sub> et comment comparer avec P <sub>max</sub> .  Il s'agit de comprendre le fonctionnement des services et la force des nerfs de notre CTO. <br><br>  Si nous n'acceptons aucun risque, nous devons supposer que tous les serveurs pourraient commencer √† consommer leur maximum potentiel simultan√©ment.  Dans le m√™me temps, l'une des alimentations CC peut √©galement √©chouer.  L'infrastructure devrait toujours fournir le service.  Donc, P <sub>serv</sub> ‚â° P <sub>max</sub> .  C'est l'approche lorsque la fiabilit√© est tr√®s importante. <br><br>  Si le DSI prend en compte non seulement la s√©curit√© id√©ale mais aussi l'argent de l'entreprise, s'il est assez courageux, alors il peut d√©cider que <br><br><ul><li>  nous commen√ßons √† g√©rer nos fournisseurs, en particulier, nous interdisons toute maintenance planifi√©e dans les p√©riodes de notre charge √©lev√©e pr√©vue pour minimiser la panne de courant </li><li>  et ou notre architecture nous permet de perdre un rack / ligne / DC pendant que les services continuent leurs op√©rations </li><li>  et ou nous r√©partissons la charge horizontalement sur les racks si bien que nos serveurs dans une seule armoire ne consommeront jamais leur maximum th√©orique tous ensemble. </li></ul><br>  Il est avantageux non seulement de deviner ici, mais de surveiller la consommation d'√©nergie et de comprendre comment les serveurs consomment de l'√©nergie pendant les charges habituelles et de pointe.  Ainsi et ainsi de suite apr√®s une certaine analyse, le CIO se d√©place et dit: <br>  "Je commande que la moyenne maximale r√©alisable de toute la consommation √©lectrique maximale du serveur est <b>bien</b> inf√©rieure √† la consommation maximale d'un seul serveur."  Soit P <sub>serv</sub> = 0,8 * P <sub>max</sub> <br><br>  Et puis un rack de 6kW peut accueillir non pas 16 serveurs de P <sub>max</sub> = 375W mais 20 serveurs de P <sub>serv</sub> = 375W * 0.8 = 300W.  Soit 25% de serveurs en plus.  C'est une vraie √©conomie car nous avons besoin de 25% de racks en moins.  Et nous pouvons √©conomiser sur les PDU en rack, les commutateurs et le c√¢blage.  Un s√©rieux inconv√©nient de la solution est la n√©cessit√© de v√©rifier en permanence que nos hypoth√®ses sont toujours valables.  Nous devons nous assurer qu'un nouveau firmware ne modifie pas de mani√®re significative le fonctionnement et la consommation d'√©nergie des ventilateurs, que l'√©quipe de d√©veloppement n'a pas commenc√© √† utiliser les serveurs beaucoup plus efficacement (cela signifie qu'ils ont r√©ussi √† augmenter l'utilisation et la consommation d'√©nergie).  Ensuite, les hypoth√®ses et les conclusions initiales deviennent fausses.  C'est donc le risque d'√™tre accept√© de fa√ßon responsable.  Ou le risque peut √™tre √©vit√© et alors l'entreprise paie pour les racks √©videmment sous-charg√©s. <br><br>  Une remarque importante: il vaut la peine d'essayer de distribuer diff√©rents serveurs de services sur les racks horizontalement si possible.  Il est n√©cessaire d'√©viter les cas o√π un groupe de serveurs pour le service arrive et est install√© dans des armoires verticalement pour am√©liorer la "densit√©" (simplement parce que c'est plus facile √† faire de cette fa√ßon).  En effet, cela conduit √† la situation o√π un rack est rempli avec les m√™mes serveurs √† faible charge alors que tous les serveurs hautement charg√©s r√©sident dans un autre.  Lorsque le profil de charge est le m√™me et que tous les serveurs commencent √† consommer autant simultan√©ment en raison d'une charge √©lev√©e, la probabilit√© de perdre le deuxi√®me rack devient beaucoup plus √©lev√©e. <br><br>  Revenons √† la distribution des serveurs dans les racks.  Nous avons consid√©r√© les contraintes physiques dans les armoires et les limitations de puissance.  Consid√©rons maintenant le r√©seau.  On peut utiliser des commutateurs N = 24/32/48 ports (en supposant des commutateurs ToR √† 48 ports).  Heureusement, il n'y a pas tellement d'options si nous ignorons les c√¢bles de d√©rivation.  Nous consid√©rons les options d'un commutateur dans chaque rack, un commutateur par deux ou par trois armoires par groupe (R <sub>net</sub> ).  Je pense que le groupe ne devrait pas √™tre compos√© de trois personnes.  Sinon, cela entra√Æne des probl√®mes de c√¢blage. <br><br>  Ainsi, nous r√©partissons les serveurs sur les racks pour chaque sc√©nario de r√©seau (1, 2 ou 3 racks par groupe): <br><br>  <sub>Rack</sub> S = min (S <sub>h</sub> , rounddown (P <sub>rack</sub> / P <sub>serv</sub> ), rounddown (N / R <sub>net</sub> )) <br><br>  Ainsi, un sc√©nario de groupe de deux racks est <br><br>  <sub>Rack</sub> S <sup>2</sup> = min (21, arrondi (6000/300), arrondi (48/2)) = min (21, 20, 24) = 20 serveurs par rack <br><br>  De m√™me, nous comptons les autres sc√©narios: <br><br>  <sub>Rack</sub> S <sup>1</sup> = 20 <br><br>  <sub>Rack</sub> S <sup>3</sup> = 16 <br><br>  Nous avons presque fini.  Il faut compter le nombre total de racks pour distribuer tous les serveurs S (soit 1000 serveurs): <br><br>  R = arrondi (S / ( <sub>rack</sub> S * R <sub>net</sub> )) * R <sub>net</sub> <br><br>  R <sub>1</sub> = arrondi (1000 / (20 * 1)) * 1 = 50 * 1 = 50 racks <br><br>  R <sub>2</sub> = arrondi (1000 / (20 * 2)) * 2 = 25 * 2 = 50 racks <br><br>  R <sub>2</sub> = arrondi (1000 / (16 * 3)) * 3 = 21 * 3 = 63 racks <br><br>  Ensuite, nous devons compter le TCO pour chaque option en fonction du nombre de racks, des commutateurs requis, du c√¢blage, etc.  Nous choisissons le sc√©nario avec le TCO le plus bas.  Profit! <br><br>  Veuillez noter que bien que le nombre de racks pour les sc√©narios 1 et 2 soit le m√™me, le co√ªt total de possession est diff√©rent en raison de deux fois moins de commutateurs et de c√¢bles plus longs pour le deuxi√®me sc√©nario. <br><br>  PS Si la puissance par rack ou la hauteur de rack peut varier, la variabilit√© augmente.  Mais la s√©lection peut √™tre r√©duite √† la m√©thode ci-dessus en force brute les options.  Il y aura plus de sc√©narios, mais leur quantit√© sera limit√©e.  Nous pouvons augmenter la puissance par rack par pas de 1 kW, et il existe un nombre limit√© de types de rack standard: de 42U, 45U, 47U, 48U.  Il peut √™tre utile d'utiliser l'analyse What-If d'Excel en mode Tableau de donn√©es.  Nous devons examiner le tableau r√©sultant et s√©lectionner la meilleure option. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr473988/">https://habr.com/ru/post/fr473988/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr473978/index.html">Une telle douleur, une telle douleur, une caisse enregistreuse 2: 0</a></li>
<li><a href="../fr473980/index.html">La technologie et le monde r√©el: 4 start-ups qui changent l'avenir du design d'int√©rieur</a></li>
<li><a href="../fr473982/index.html">NB-IoT: comment √ßa marche? Partie 3: SCEF - une fen√™tre d'acc√®s unique aux services de l'op√©rateur</a></li>
<li><a href="../fr473984/index.html">Data Science Digest (octobre 2019)</a></li>
<li><a href="../fr473986/index.html">¬´Mon r√™ve est de voir une personne √† travers les arbres¬ª - fondatrice de Lisa Alert sur les technologies de recherche modernes</a></li>
<li><a href="../fr473990/index.html">Apprenez OpenGL. Le√ßon 7.2 - Dessin de texte</a></li>
<li><a href="../fr473992/index.html">Aper√ßu des protocoles modernes dans les syst√®mes d'automatisation industrielle</a></li>
<li><a href="../fr473994/index.html">Chargement de script moderne</a></li>
<li><a href="../fr473998/index.html">Pays-Bas, ou aller-retour</a></li>
<li><a href="../fr474000/index.html">IQBX - concepteur √©lectrom√©canique pour les cercles et les amateurs de bricolage [id√©e conceptuelle]</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>