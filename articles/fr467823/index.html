<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüè´ üì¢ üëêüèø Analyse: MOO sur Kubernetes ü•ì üë©üèø‚Äçüåæ üçâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les probl√®mes dans l'environnement de travail sont toujours une catastrophe. Cela arrive quand vous rentrez chez vous, et la raison semble toujours st...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse: MOO sur Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/467823/"><p><img src="https://habrastorage.org/webt/m8/of/zv/m8ofzvtlhfh8mkwo-i04c5ren38.jpeg"></p><br><p>  Les probl√®mes dans l'environnement de travail sont toujours une catastrophe.  Cela arrive quand vous rentrez chez vous, et la raison semble toujours stupide.  R√©cemment, nous avons manqu√© de m√©moire sur les n≈ìuds du cluster Kubernetes, bien que le n≈ìud se soit imm√©diatement r√©tabli, sans interruption visible.  Aujourd'hui, nous parlerons de cette affaire, des dommages que nous avons subis et de la mani√®re dont nous avons l'intention d'√©viter un probl√®me similaire √† l'avenir. </p><br><h2 id="sluchay-pervyy">  Premier cas </h2><br><h3 id="subbota-15-iyunya-2019-g-1712">  Samedi 15 juin 2019 17 h 12 </h3><a name="habracut"></a><br><p>  Blue Matador (oui, nous nous surveillons!) G√©n√®re une alerte: un √©v√©nement sur l'un des noeuds du cluster de production de Kubernetes - SystemOOM. </p><br><h3 id="1716">  17:16 </h3><br><p>  Blue Matador g√©n√®re un avertissement: EBS Burst Balance est faible sur le volume racine du n≈ìud - celui o√π l'√©v√©nement SystemOOM a eu lieu.  Bien qu'un avertissement concernant le Burst Balance soit venu apr√®s une notification concernant SystemOOM, les donn√©es CloudWatch r√©elles montrent que le Burst Balance a atteint un niveau minimum √† 17:02.  La raison de ce retard est que les m√©triques EBS sont constamment en retard de 10 √† 15 minutes, et notre syst√®me ne capture pas tous les √©v√©nements en temps r√©el. </p><br><p><img src="https://habrastorage.org/webt/xw/gb/u3/xwgbu3jxukrpz_qmakfhvsb5bm4.png"></p><br><h3 id="1718">  17:18 </h3><br><p> En ce moment, j'ai vu une alerte et un avertissement.  Je lance rapidement <strong>kubectl get pods</strong> pour voir les dommages que nous avons subis, et je suis surpris de constater que les pods dans l'application sont morts exactement √† 0. <strong>J'ex√©cute kubectl top nodes</strong> , mais cette v√©rification montre √©galement que le n≈ìud suspect a un probl√®me de m√©moire;  Certes, il a d√©j√† r√©cup√©r√© et utilise environ 60% de sa m√©moire.  Il est 17 heures et la bi√®re artisanale se r√©chauffe d√©j√†.  Apr√®s m'√™tre assur√© que le n≈ìud √©tait op√©rationnel et qu'aucun pod n'√©tait endommag√©, j'ai d√©cid√© qu'un accident s'√©tait produit.  Si quoi que ce soit, je le d√©couvrirai lundi. </p><br><p>  Voici notre correspondance avec la station-service de Slack ce soir-l√†: </p><br><p><img src="https://habrastorage.org/webt/ib/fp/f0/ibfpf05vnjd2uaukxr0rwjgcjj0.png"></p><br><h2 id="sluchay-vtoroy">  Deuxi√®me cas </h2><br><h3 id="subbota-16-iyunya-2019-g-1802">  Samedi 16 juin 2019 18h02 </h3><br><p>  Blue Matador g√©n√®re une alerte: l'√©v√©nement est d√©j√† sur un autre noeud, √©galement SystemOOM.  Il devait √™tre que la station-service √† ce moment-l√† ne faisait que regarder l'√©cran du smartphone, car elle m'√©crivait et me faisait imm√©diatement reprendre l'√©v√©nement, je ne peux pas moi-m√™me allumer l'ordinateur (est-il temps de r√©installer Windows?).  Et encore une fois, tout semble normal.  Pas un seul pod n'est tu√©, et le n≈ìud consomme √† peine 70% de la m√©moire. </p><br><h3 id="1806">  18:06 </h3><br><p>  Blue Matador g√©n√®re √† nouveau un avertissement: EBS Burst Balance.  La deuxi√®me fois en une journ√©e, ce qui signifie que je ne peux pas lib√©rer ce probl√®me sur les freins.  Avec CloudWatch inchang√©, Burst Balance a d√©vi√© de la norme 2 heures ou plus avant que le probl√®me ne soit identifi√©. </p><br><h3 id="1811">  18:11 </h3><br><p>  Je vais √† Datalog et regarde les donn√©es sur la consommation de m√©moire.  Je vois que juste avant l'√©v√©nement SystemOOM, le n≈ìud suspect prend vraiment beaucoup de m√©moire.  Le sentier m√®ne √† nos pods fluentd-sumologic. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/s-/k-/aes-k-fwfx5qtsm7ehvhkgnpofe.png"></a> </p><br><p>  Vous pouvez clairement voir une forte d√©viation de la consommation de m√©moire, √† peu pr√®s au m√™me moment que les √©v√©nements SystemOOM se sont produits.  Ma conclusion: ce sont ces pods qui ont pris toute la m√©moire, et quand SystemOOM s'est produit, Kubernetes s'est rendu compte que ces pods pouvaient √™tre tu√©s et red√©marr√©s pour retourner la m√©moire n√©cessaire sans affecter mes autres pods.  Bien jou√©, Kubernetes! </p><br><p>  Alors, pourquoi n'ai-je pas vu cela samedi lorsque j'ai compris quels modules ont red√©marr√©?  Le fait est que j'ex√©cute des pods fluentd-sumologic dans un espace de noms s√©par√© et press√© je n'ai tout simplement pas pens√© √† y jeter un ≈ìil. </p><br><blockquote>  Conclusion 1: Lorsque vous recherchez des pods red√©marr√©s, v√©rifiez tous les espaces de noms. </blockquote><p>  Ayant re√ßu ces donn√©es, j'ai calcul√© que le lendemain, la m√©moire sur les autres n≈ìuds ne se terminerait pas, cependant, j'ai continu√© et red√©marr√© tous les pods sumologic afin qu'ils commencent √† travailler avec une faible consommation de m√©moire.  Le lendemain matin, je pr√©vois de concocter comment int√©grer le travail sur le probl√®me dans un plan pour la semaine et de ne pas trop charger le dimanche soir. </p><br><h3 id="2300">  23h00 </h3><br><p>  J'ai regard√© la prochaine s√©rie de "Black Mirror" (soit dit en passant, j'ai aim√© Miley) et j'ai d√©cid√© de regarder comment allait le cluster.  La consommation de m√©moire est normale, alors n'h√©sitez pas √† tout laisser comme pour la nuit. </p><br><h2 id="pochinka">  Fix </h2><br><p>  Lundi, j'ai pris du temps pour ce probl√®me.  √áa ne fait pas de mal de chasser avec elle tous les soirs.  Ce que je sais en ce moment: </p><br><ul><li>  Les conteneurs Fluentd-sumologic engloutissaient une tonne de m√©moire; </li><li>  L'√©v√©nement SystemOOM est pr√©c√©d√© d'une activit√© √©lev√©e du disque, mais je ne sais pas lequel. </li></ul><br><p>  Au d√©but, je pensais que les conteneurs fluentd-sumologic sont accept√©s pour manger de la m√©moire lors d'un afflux soudain de journaux.  Cependant, apr√®s avoir v√©rifi√© Sumologic, j'ai vu que les journaux √©taient utilis√©s de mani√®re stable, et en m√™me temps qu'il y avait des probl√®mes, il n'y avait pas d'augmentation de ces journaux. </p><br><p>  Un peu sur Google, j'ai trouv√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cette t√¢che sur github</a> , ce qui sugg√®re d'ajuster certains param√®tres Ruby - pour r√©duire la consommation de m√©moire.  J'ai d√©cid√© de l'essayer, d'ajouter une variable d'environnement √† la sp√©cification du pod et de l'ex√©cuter: </p><br><pre><code class="plaintext hljs">env: - name: RUBY_GC_HEAP_OLDOBJECT_LIMIT_FACTOR value: "0.9"</code> </pre> <br><p>  En parcourant le manifeste fluentd-sumologic, j'ai remarqu√© que je n'avais pas d√©fini de demandes de ressources et de restrictions.  Je commence √† soup√ßonner que le correctif RUBY_GCP_HEAP fera une sorte de miracle, alors maintenant il est logique de d√©finir des limites de consommation de m√©moire.  M√™me si je ne r√®gle pas le probl√®me de m√©moire, il sera au moins possible de limiter sa consommation √† cet ensemble de pods.  Utilisation des <strong>pods sup√©rieurs de kubectl |</strong>  <strong>grep fluentd-sumologic</strong> , je sais d√©j√† combien de ressources demander: </p><br><pre> <code class="plaintext hljs">resources: requests: memory: "128Mi" cpu: "100m" limits: memory: "1024Mi" cpu: "250m"</code> </pre> <br><blockquote>  Conclusion 2: d√©finir des limites de ressources, en particulier pour les applications tierces. </blockquote><br><h2 id="proverka-ispolneniya">  V√©rification d'ex√©cution </h2><br><p>  Apr√®s quelques jours, je confirme que la m√©thode ci-dessus fonctionne.  La consommation de m√©moire √©tait stable et - aucun probl√®me avec aucun composant de Kubernetes, EC2 et EBS.  Maintenant, il est clair √† quel point il est important de d√©terminer les demandes de ressources et les restrictions pour tous les pods que j'ex√©cute, et voici ce qui doit √™tre fait: appliquer une combinaison de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">limites de ressources par d√©faut et de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quotas de</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ressources</a> . </p><br><p>  Le dernier myst√®re non r√©solu est EBS Burst Balance, qui a co√Øncid√© avec l'√©v√©nement SystemOOM.  Je sais que lorsqu'il y a peu de m√©moire, le syst√®me d'exploitation utilise l'espace de swap pour ne pas √™tre laiss√© compl√®tement sans m√©moire.  Mais je ne suis pas n√© hier et je suis conscient que Kubernetes ne d√©marrera m√™me pas sur les serveurs o√π le fichier d'√©change est activ√©.  Je voulais juste m'assurer, j'ai grimp√© dans mes n≈ìuds via SSH - pour v√©rifier si le fichier d'√©change a √©t√© activ√©;  J'ai utilis√© √† la fois de la m√©moire libre et celle de la zone de swap.  Le fichier n'a pas √©t√© activ√©. </p><br><p>  Et puisque l'√©change n'est pas au travail, j'ai plus d'indices sur ce qui a provoqu√© la croissance des flux d'E / S, c'est pourquoi le n≈ìud a failli manquer de m√©moire, non.  En g√©n√©ral, j'ai une intuition: le pod fluentd-sumologic lui-m√™me √©crivait une tonne de messages de journal √† ce moment-l√†, peut-√™tre m√™me un message de journal li√© √† la configuration de Ruby GC.  Il est √©galement possible qu'il existe d'autres sources de messages Kubernetes ou journald qui deviennent excessivement productifs lorsque la m√©moire est √©puis√©e, et je les ai √©limin√©s lors de la configuration de fluentd.  Malheureusement, je n'ai plus acc√®s aux fichiers journaux enregistr√©s imm√©diatement avant le dysfonctionnement, et maintenant je ne peux plus creuser. </p><br><blockquote>  Conclusion 3: Bien qu'il y ait une opportunit√©, approfondissez votre analyse des causes profondes, quel que soit le probl√®me. </blockquote><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  Et bien que je ne sois pas all√© √† la racine des causes, je suis s√ªr qu'elles ne sont pas n√©cessaires pour √©viter les m√™mes dysfonctionnements √† l'avenir.  Le temps, c'est de l'argent, mais je suis occup√© depuis trop longtemps, et apr√®s cela, j'ai √©galement √©crit ce post pour vous.  Et puisque nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Blue Matador</a> , de tels dysfonctionnements sont trait√©s en d√©tail, donc je me permets de lib√©rer quelque chose sur les freins, sans √™tre distrait du projet principal. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr467823/">https://habr.com/ru/post/fr467823/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr467811/index.html">Architecture et programmation Fairchild Channel F</a></li>
<li><a href="../fr467813/index.html">R√©vision des changements dans le 17e ordre du FSTEC</a></li>
<li><a href="../fr467815/index.html">Les m√©dias ont paniqu√© que "les adresses IP s'√©puisent en Russie". Comment vraiment?</a></li>
<li><a href="../fr467817/index.html">Un peu sur les mod√®les de conception g√©n√©ratifs</a></li>
<li><a href="../fr467821/index.html">Simplifiez et √©liminez les besoins: entretien avec John Romero, cr√©ateur de Doom</a></li>
<li><a href="../fr467825/index.html">Algorithmes d'apprentissage automatique indispensables</a></li>
<li><a href="../fr467827/index.html">Comment nous avons fait notre petite unit√© √† partir de z√©ro</a></li>
<li><a href="../fr467831/index.html">La voie √©pineuse de la programmation</a></li>
<li><a href="../fr467837/index.html">MCU ¬´Terrible¬ª √† trois cents - un bref aper√ßu des microcontr√¥leurs co√ªtant moins de 0,1 $</a></li>
<li><a href="../fr467841/index.html">Rendez-vous plus facile: Entretien avec John Romero, d√©veloppeur de Doom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>