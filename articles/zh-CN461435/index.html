<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍔 🍊 🚊 使用卷积神经网络进行情感识别 〽️ 🌔 😜</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="对于科学家而言，认识情感一直是令人兴奋的挑战。 最近，我正在进行一个实验性SER项目（语音情感识别），以了解这项技术的潜力-为此，我选择了Github上最受欢迎的存储库，并将其作为我项目的基础。 

 在我们开始理解该项目之前，最好记住SER有什么样的瓶颈。 

 主要障碍 


- 情绪是主观的，...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>使用卷积神经网络进行情感识别</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br> 对于科学家而言，认识情感一直是令人兴奋的挑战。 最近，我正在进行一个实验性SER项目（语音情感识别），以了解这项技术的潜力-为此，我选择了<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Github</a>上最受欢迎的存储库，并将其作为我项目的基础。 <br><br> 在我们开始理解该项目之前，最好记住SER有什么样的瓶颈。 <br><a name="habracut"></a><br><h2> 主要障碍 </h2><br><ul><li> 情绪是主观的，即使人们对情绪的理解也不同。 很难定义“情感”的概念。 </li><li> 对音频发表评论很难。 我们应该以某种方式标记每个单词，句子或整个交流吗？ 识别中使用什么样的情感？ </li><li> 收集数据也不容易。 可以从电影和新闻中收集很多音频数据。 但是，这两个消息来源都是“有偏见的”，因为新闻必须是中立的，并且演员的情感也会被播放。 很难找到音频数据的“客观”来源。 </li><li> 标记数据需要大量的人力和时间资源。 与在图像上绘制框架不同，它需要经过专门培训的人员来收听整个音频记录，对其进行分析并提供评论。 然后，这些评论必须受到<b>许多</b>其他人的赞赏，因为评分是主观的。 </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2> 项目说明 </h2><br> 使用卷积神经网络识别录音中的情绪。 是的，存储库的所有者未引用任何来源。 <br><br><h2> 资料说明 </h2><br>  RAVDESS和SAVEE存储库中使用了两个数据集，我只是在模型中改编了RAVDESS。  RAVDESS上下文中有两种类型的数据：语音和歌曲。 <br><br> 数据集<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">RAVDESS（情感言语和歌曲的Ryerson视听数据库）</a> ： <br><br><ul><li>  12位演员和12位女演员录制了表演中的讲话和歌曲； </li><li>  18号演员没有录制歌曲； </li><li> 情绪在“歌曲”数据中不存在厌恶（厌恶），中立（中立）和惊奇（惊奇）。 </li></ul><br> 情绪细分： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br> 情绪分布图： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3> 特征提取 </h3><br> 当我们处理语音识别任务时，尽管<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">倒谱系数（MFCC）</a>出现在80年代，但它却是一项先进的技术。 <br><br> 引用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">MFCC教程</a> ： <br><blockquote> 此形状确定输出声音是什么。 如果我们可以查明表格的形式，它将使我们能够准确表示所<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">读音素</a> 。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">声道</a>的形状在短频谱的包络中表现出来，而MFCC的工作是准确显示该包络。 </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">波形图</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">频谱图</font> <br><br> 我们使用MFCC作为输入功能。 如果您有兴趣了解有关MFCC的更多信息，那么本<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">教程</a>适合您。 使用librosa Python软件包可以轻松下载数据并将其转换为MFCC格式。 <br><br><h3> 默认模型架构 </h3><br> 作者使用Keras软件包开发了CNN模型，创建了7层-六个Con1D层和一个密度层（Dense）。 <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote> 作者在最新版本（2018年9月18日）中对第4层和第5层进行了评论，该模型的最终文件大小不适合所提供的网络，因此我无法获得相同的准确度结果-72％。 </blockquote><br> 只需使用参数<code>batch_size=16</code>和<code>epochs=700</code>即可训练模型，而无需任何训练时间表等。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br> 此处<code>categorical_crossentropy</code>是损失的函数，评估的方法是准确性。 <br><br><h2> 我的实验 </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">探索性数据分析</a> </h3><br> 在RAVDESS数据集中，每个演员都显示8种情绪，每个句子发音和唱歌2个句子，两次。 结果，从每个演员获得每种情绪的4个示例，除了上述中性情绪，厌恶和惊奇之外。 每个音频持续约4秒钟，在最初和最后几秒钟通常是静音。 <br><br>  <b>典型报价</b> ： <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3> 观察 </h3><br> 在从1位演员和1位女演员中选择一个数据集，然后听听他们的所有记录后，我意识到男人和女人表达自己的情感的方式有所不同。 例如： <br><br><ul><li> 男性愤怒（愤怒）声音更大； </li><li> 男人的快乐（Happy）和挫败感（Sad）-“沉默”时笑声和哭声的特征； </li><li> 女性的快乐（快乐），愤怒（愤怒）和沮丧（悲伤）更大； </li><li> 女恶心（Disgust）包含呕吐的声音。 </li></ul><br><h3> 实验重复 </h3><br> 作者删除了中立，令人反感和惊讶的类，以使RAVDESS对数据集进行10类识别。 尝试重复作者的经历，我得到了以下结果： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br> 但是，我发现，当用于验证的数据集与测试数据集相同时，会发生数据泄漏。 因此，我重复了数据的分离过程，将两个演员和两个女演员的数据集隔离开来，以使它们在测试期间不可见： <br><br><ul><li> 演员1至20以8：2的比例用于训练/有效场景; </li><li> 参与者21至24与测试隔离； </li><li> 火车设置参数：（1248，216，1）; </li><li> 有效设置参数：（312，216，1）; </li><li> 测试设置参数：（320，216，1）-（隔离）。 </li></ul><br> 我重新训练了模型，结果如下： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3> 性能测试 </h3><br> 从“火车有效毛额”图可以明显看出，所选的10个班级没有收敛。 因此，我决定降低模型的复杂性，只保留男性情感。 我在测试集中隔离了两个演员，其余的以8：2的比例放在火车/有效集中。 这样可以确保数据集中没有不平衡。 然后，我分别指导了男性和女性数据进行测试。 <br><br>  <b>男性数据集</b> <br><br><ul><li> 训练集-演员1-10中的640个样本 </li><li> 有效集-来自参与者1-10的160个样本； </li><li> 测试集-来自演员11-12的160个样本。 </li></ul><br>  <b>参考线：男</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>女性数据集</b> <br><br><ul><li> 火车套装-女演员1-10名的608个样本； </li><li> 有效集-女演员1-10中的152个样本； </li><li> 测试集-从女演员11-12中获得160个样本。 </li></ul><br>  <b>参考线：女士</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br> 如您所见，错误矩阵是不同的。 <br><br> 男人：愤怒和快乐是模型中主要的预测类，但两者并不相同。 <br><br> 妇女：失调（悲伤）和欢乐（快乐）-模型中基本上预测的类别； 愤怒和喜悦很容易混淆。 <br><br> 回顾《 <b>情报数据分析》中</b>的观察结果，我怀疑女性愤怒和快乐与混淆点相似，因为它们的表达方式只是为了表达自己的声音。 <br><br> 最重要的是，我很好奇，如果我进一步简化模型，只留下正，中性和负性类。 或只有正面和负面。 简而言之，我将情绪分别分为2类和3类。 <br><br>  <b>2节课：</b> <br><br><ul><li> 积极：喜悦（快乐），镇定（平静）； </li><li> 负面：愤怒，恐惧（恐惧），沮丧（悲伤）。 </li></ul><br>  <b>3个班级：</b> <br><br><ul><li> 积极：快乐（快乐）； </li><li> 中立：平静（Calm），中立（Neutral）； </li><li> 负面：愤怒，恐惧（恐惧），沮丧（悲伤）。 </li></ul><br> 在开始实验之前，我使用男性数据建立模型架构，进行5级识别。 <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br> 我添加了2层Conv1D，一层MaxPooling1D和2层BarchNormalization； 我也将辍学值更改为0.25。 最后，我以0.0001的学习速度将优化器更改为SGD。 <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br> 为了训练模型，我减少了“训练平稳期”，只保存了具有最小<code>val_loss</code>的最佳模型。 这是不同目标类别的结果。 <br><br><h2> 新车型性能 </h2><br>  <b>男子5节课</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>5年级女性</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>2年级男士</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>男子三班</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2> 增加（增强） </h2><br> 当我加强模型的架构，优化器和训练的速度时，事实证明该模型仍然无法在训练模式下收敛。 我建议这是一个数据量问题，因为我们只有800个样本。 这使我想到了增加音频的方法，最后我将数据集增加了一倍。 让我们看一下这些方法。 <br><br><h3> 男子5节课 </h3><br>  <b>动态增量</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>音高调整</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>偏移量</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>添加白噪声</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br> 值得注意的是，增强极大地提高了准确性，在通常情况下可达70 +％。 特别是在添加白色的情况下，将精度提高到87.19％-但是，测试精度和<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">F1测量值</a>下降了5％以上。 然后我想到了结合几种增强方法以获得更好结果的想法。 <br><br><h3> 结合几种方法 </h3><br>  <b>白噪声+偏差</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2> 测试男性增强 </h2><br><h3>  2年级男士 </h3><br>  <b>白噪声+偏差</b> <br><br> 对于所有样品 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>白噪声+偏差</b> <br><br> 仅针对阳性样本，因为2类组不平衡（朝向阴性样本）。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>音高+白噪声</b> <br> 对于所有样品 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>音高+白噪声</b> <br><br> 仅适用于阳性样品 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2> 结论 </h2><br> 最后，我只能使用男性数据集进行实验。 我重新划分了数据，以避免不平衡，从而避免数据泄漏。 我设置模型以尝试男性声音，因为我想尽可能简化模型。 我还使用不同的增强方法进行了测试； 白噪声和偏置的添加在不平衡数据上效果很好。 <br><br><h2> 结论 </h2><br><ul><li> 情绪是主观的，难以解决； </li><li> 有必要事先确定哪些情绪适合该项目； </li><li> 即使内容众多，也不要总是信任Github的内容； </li><li> 数据共享-记住这一点； </li><li> 探索性数据分析总是有一个好主意，但是在处理音频数据时，您需要耐心等待； </li><li> 确定要为模型输入提供什么：句子，整个记录还是感叹号？ </li><li> 缺少数据是SER成功的重要因素，但是创建具有情感的良好数据集是一项复杂而昂贵的任务； </li><li> 缺少数据时简化模型。 </li></ul><br><h2> 进一步改善 </h2><br><ul><li> 我仅使用前3秒作为输入来减小整体数据大小-原始项目使用了2.5秒。 我想尝试全尺寸的录音； </li><li> 您可以对数据进行预处理：修剪静音，通过用零填充来标准化长度等。 </li><li> 为此任务尝试递归神经网络。 </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN461435/">https://habr.com/ru/post/zh-CN461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN461421/index.html">投资交易所时如何确保自己免受可能的损失：结构性产品</a></li>
<li><a href="../zh-CN461423/index.html">11个技巧：如何向“非设计人员”展示UI / UX工作</a></li>
<li><a href="../zh-CN461425/index.html">如何成为产品经理并进一步发展</a></li>
<li><a href="../zh-CN461431/index.html">“喜欢和不喜欢”：通过HTTPS进行DNS</a></li>
<li><a href="../zh-CN461433/index.html">在Net Core 3.0中使用Identity Server 4</a></li>
<li><a href="../zh-CN461437/index.html">370个灯泡</a></li>
<li><a href="../zh-CN461439/index.html">启动React和TypeScript组件库</a></li>
<li><a href="../zh-CN461441/index.html">使用R.并行计算，图形，xlsx，电子邮件和所有这些报告存储状态</a></li>
<li><a href="../zh-CN461443/index.html">分析后：有关SKS Keyserver加密密钥服务器网络的最新攻击的已知信息</a></li>
<li><a href="../zh-CN461447/index.html">关于系统管理员是濒临灭绝的物种的史诗</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>