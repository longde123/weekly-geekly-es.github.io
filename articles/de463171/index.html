<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèø‚Äçüéì ‚óºÔ∏è ü§∏üèΩ Neuronale Netze und Deep Learning: Ein Online-Tutorial, Kapitel 6, Teil 1: Deep Learning üö¶ üôÄ üë®üèø‚Äç‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inhalt 

- Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen 
- Kapitel 2: Funktionsweise des Backpropagation-Algorithmus 
-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und Deep Learning: Ein Online-Tutorial, Kapitel 6, Teil 1: Deep Learning</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463171/"><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  Im letzten Kapitel haben wir gelernt, dass tiefe neuronale Netze (GNSs) oft schwieriger zu trainieren sind als flache.  Und das ist schlecht, denn wir haben allen Grund zu der Annahme, dass wenn wir die STS trainieren k√∂nnten, sie die Aufgaben viel besser erledigen k√∂nnten.  Die Nachrichten aus dem vorherigen Kapitel sind zwar entt√§uschend, werden uns aber nicht aufhalten.  In diesem Kapitel werden wir Techniken entwickeln, mit denen wir tiefe Netzwerke trainieren und in die Praxis umsetzen k√∂nnen.  Wir werden auch die Situation allgemeiner betrachten und uns kurz mit den j√ºngsten Fortschritten bei der Verwendung von GNS f√ºr die Bilderkennung, Sprache und f√ºr andere Anwendungen vertraut machen.  Und √ºberlegen Sie auch oberfl√§chlich, welche Zukunft die neuronalen Netze und die KI erwarten k√∂nnen. <br><br>  Dies wird ein langes Kapitel sein, also gehen wir etwas √ºber das Inhaltsverzeichnis.  Die Abschnitte sind nicht stark miteinander verbunden. Wenn Sie also einige grundlegende Konzepte zu neuronalen Netzen haben, k√∂nnen Sie mit dem Abschnitt beginnen, der Sie mehr interessiert. <br><br>  Der Hauptteil des Kapitels ist eine Einf√ºhrung in eine der beliebtesten Arten von tiefen Netzwerken: Deep Convolution Networks (GSS).  Wir werden mit einem detaillierten Beispiel f√ºr die Verwendung eines Faltungsnetzwerks mit einem Code und anderen Dingen arbeiten, um das Problem der Klassifizierung handgeschriebener Ziffern aus dem MNIST-Datensatz zu l√∂sen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><a name="habracut"></a><br>  Wir beginnen unsere √úberpr√ºfung von Faltungsnetzwerken mit flachen Netzwerken, mit denen wir dieses Problem weiter oben in diesem Buch gel√∂st haben.  In mehreren Schritten werden wir immer leistungsf√§higere Netzwerke schaffen.  Auf dem Weg werden wir viele leistungsstarke Technologien kennenlernen: Faltungen, Pooling, Verwendung von GPUs, um den Trainingsaufwand im Vergleich zu flachen Netzwerken erheblich zu erh√∂hen, algorithmische Erweiterung der Trainingsdaten (um √úberanpassung zu reduzieren) mithilfe der Dropout-Technologie (auch um die Umschulung zu reduzieren), unter Verwendung von Ensembles von Netzwerken und anderen.  Infolgedessen werden wir zu einem System kommen, dessen F√§higkeiten fast auf menschlicher Ebene liegen.  Von den 10.000 MNIST-√úberpr√ºfungsbildern, die das System w√§hrend des Trainings nicht gesehen hat, kann es 9967 korrekt erkennen. Und hier sind einige der Bilder, die nicht korrekt erkannt wurden.  In der oberen rechten Ecke befinden sich die richtigen Optionen.  Was unser Programm gezeigt hat, ist in der unteren rechten Ecke angegeben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6e/2d7/69a/b6e2d769a802b1ae5f249932789f2dff.png"><br><br>  Viele von ihnen sind f√ºr Menschen schwer zu klassifizieren.  Nehmen Sie zum Beispiel die dritte Ziffer in der obersten Zeile.  Es scheint mir eher "9" als die offizielle Version von "8".  Unser Netzwerk entschied auch, dass es "9" war.  Zumindest k√∂nnen solche Fehler vollst√§ndig verstanden und vielleicht sogar genehmigt werden.  Wir schlie√üen unsere Diskussion √ºber die Bilderkennung mit einem √úberblick √ºber die enormen Fortschritte, die das neuronale Netzwerk k√ºrzlich erzielt hat (insbesondere Faltungsnetzwerke). <br><br>  Der Rest des Kapitels widmet sich einer Diskussion √ºber tiefes Lernen aus einer breiteren und weniger detaillierten Perspektive.  Wir werden kurz auf andere NS-Modelle eingehen, insbesondere auf wiederkehrende NS und Einheiten des Langzeit-Kurzzeitged√§chtnisses, und wie diese Modelle verwendet werden k√∂nnen, um Probleme bei der Spracherkennung, der Verarbeitung nat√ºrlicher Sprache und anderen zu l√∂sen.  Wir werden die Zukunft von NS und Zivilschutz diskutieren, von Ideen wie absichtsgesteuerten Benutzeroberfl√§chen bis hin zur Rolle des tiefen Lernens in der KI. <br><br>  Dieses Kapitel basiert auf Material aus fr√ºheren Kapiteln des Buches, in dem Ideen wie Backpropagation, Regularisierung, Softmax usw. verwendet und integriert werden.  Um dieses Kapitel zu lesen, ist es jedoch nicht erforderlich, das Material aller vorherigen Kapitel zu erl√§utern.  Es tut jedoch nicht weh, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1</a> zu lesen und die Grundlagen der Nationalversammlung kennenzulernen.  Wenn ich die Konzepte aus den Kapiteln 2 bis 5 verwende, gebe ich bei Bedarf die erforderlichen Links zum Material. <br><br>  Es ist erw√§hnenswert, dass dieses Kapitel dies nicht tut.  Dies ist kein Schulungsmaterial zu den neuesten und coolsten Bibliotheken f√ºr die Arbeit mit NS.  Wir werden STS nicht mit Dutzenden von Schichten trainieren, um Probleme auf dem neuesten Stand der Forschung zu l√∂sen.  Wir werden versuchen, einige der Grundprinzipien zu verstehen, die GNS zugrunde liegen, und sie auf den einfachen und leicht verst√§ndlichen Kontext von MNIST-Aufgaben anwenden.  Mit anderen Worten, dieses Kapitel bringt Sie nicht an die Spitze der Region.  Der Wunsch dieses und der vorhergehenden Kapitel ist es, sich auf die Grundlagen zu konzentrieren und Sie darauf vorzubereiten, eine breite Palette zeitgen√∂ssischer Werke zu verstehen. <br><br><h2>  Einf√ºhrung in Faltungs-Neuronale Netze </h2><br>  In den vorherigen Kapiteln haben wir unseren neuronalen Netzen beigebracht, dass es ziemlich gut ist, Bilder von handgeschriebenen Zahlen zu erkennen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/839/d0b/543/839d0b54370af70f06b3f097897de457.png"><br><br>  Wir haben dazu Netzwerke verwendet, in denen benachbarte Schichten vollst√§ndig miteinander verbunden waren.  Das hei√üt, jedes Neuron des Netzwerks wurde jedem Neuron der benachbarten Schicht zugeordnet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/248/73a/b05/24873ab052991e684b9ff0650c11a1c4.png"><br><br>  Insbesondere haben wir die Intensit√§t jedes Pixels im Bild als Wert f√ºr das entsprechende Neuron der Eingangsschicht codiert.  F√ºr Bilder mit einer Gr√∂√üe von 28 x 28 Pixel bedeutet dies, dass das Netzwerk 784 (= 28 √ó 28) eingehende Neuronen hat.  Dann haben wir die Gewichte und Offsets des Netzwerks so trainiert, dass die Ausgabe (es gab eine solche Hoffnung) das eingehende Bild korrekt identifizierte: '0', '1', '2', ..., '8' oder '9'. <br><br>  Unsere fr√ºhen Netzwerke funktionieren ziemlich gut: Mit Trainings- und Testdaten aus den handgeschriebenen MNIST-Ziffern haben wir eine Klassifizierungsgenauigkeit von √ºber 98% erreicht.  Wenn Sie diese Situation jetzt bewerten, erscheint es seltsam, ein Netzwerk mit vollst√§ndig verbundenen Ebenen zur Klassifizierung von Bildern zu verwenden.  Tatsache ist, dass ein solches Netzwerk die r√§umliche Struktur von Bildern nicht ber√ºcksichtigt.  Beispielsweise gilt dies genau f√ºr Pixel, die weit voneinander entfernt sind, sowie f√ºr benachbarte Pixel.  Es wird davon ausgegangen, dass Schlussfolgerungen zu solchen Konzepten der r√§umlichen Struktur auf der Grundlage der Untersuchung von Trainingsdaten gezogen werden sollten.  Was aber, wenn wir, anstatt die Netzwerkstruktur von vorne zu beginnen, eine Architektur verwenden, die versucht, die r√§umliche Struktur zu nutzen?  In diesem Abschnitt beschreibe ich Faltungs-Neuronale Netze (SNA).  Sie verwenden eine spezielle Architektur, die sich besonders zur Klassifizierung von Bildern eignet.  Durch die Verwendung einer solchen Architektur lernen SNAs schneller.  Dies hilft uns dabei, tiefere und vielschichtigere Netzwerke zu trainieren, mit denen sich Bilder gut klassifizieren lassen.  Heutzutage wird in den meisten F√§llen der Bilderkennung Deep SNA oder eine √§hnliche Variante verwendet. <br><br>  Die Urspr√ºnge der SNA reichen bis in die 1970er Jahre zur√ºck.  Die erste Arbeit, die ihre moderne Verbreitung begann, war die Arbeit von 1998, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gradient Learning for Recognizing Documents</a> ".  Lekun machte eine interessante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bemerkung</a> zur in der SNA verwendeten Terminologie: ‚ÄûDie Verbindung von Modellen wie Faltungsnetzwerken mit der Neurobiologie ist sehr oberfl√§chlich.  Deshalb nenne ich sie Faltungsnetzwerke, keine Faltungsnetzwerke, und deshalb nennen wir ihre Knotenelemente, nicht Neuronen. "  Trotzdem verwendet die SNA viele Ideen aus der NS-Welt, die wir bereits untersucht haben: R√ºckausbreitung, Gradientenabstieg, Regularisierung, nichtlineare Aktivierungsfunktionen usw.  Daher werden wir die allgemein akzeptierte Vereinbarung befolgen und sie als eine Art NA betrachten.  Ich werde sie sowohl Netzwerke als auch neuronale Netzwerke und ihre Knoten nennen - sowohl Neuronen als auch Elemente. <br><br>  SNA verwendet drei Grundideen: lokale Empfangsfelder, Gesamtgewichte und Pooling.  Schauen wir uns diese Ideen der Reihe nach an. <br><br><h3>  Lokale Empfangsfelder </h3><br>  In vollst√§ndig verbundenen Netzwerkschichten werden Eingabeschichten durch vertikale Linien von Neuronen angezeigt.  In der SNA ist es bequemer, die Eingabeschicht in Form eines Quadrats von Neuronen mit einer Dimension von 28 √ó 28 darzustellen, deren Werte den Pixelintensit√§ten des Bildes 28 √ó 28 entsprechen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/da3/848/9d0/da38489d04325743131546e76f99396d.png"><br><br>  Wie √ºblich verkn√ºpfen wir eingehende Pixel mit einer Schicht versteckter Neuronen.  Wir werden jedoch nicht jedes Pixel mit jedem versteckten Neuron verkn√ºpfen.  Wir organisieren die Kommunikation in kleinen, lokalisierten Bereichen des eingehenden Bildes. <br><br>  Genauer gesagt wird jedes Neuron der ersten verborgenen Schicht einem kleinen Teil eingehender Neuronen zugeordnet, beispielsweise einer 5 √ó 5-Region, die 25 eingehenden Pixeln entspricht.  F√ºr ein verstecktes Neuron sieht die Verbindung m√∂glicherweise folgenderma√üen aus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf9/71d/5dc/cf971d5dc7106f1c56832c8416d7847a.png"><br><br>  Dieser Teil des eingehenden Bildes wird als lokales Empfangsfeld f√ºr dieses versteckte Neuron bezeichnet.  Dies ist ein kleines Fenster, in dem die eingehenden Pixel angezeigt werden.  Jede Bindung lernt ihr Gewicht.  Ein verstecktes Neuron untersucht auch die allgemeine Verschiebung.  Wir k√∂nnen annehmen, dass dieses bestimmte Neuron lernt, sein spezifisches lokales Rezeptionsfeld zu analysieren. <br><br>  Dann bewegen wir das lokale Empfangsfeld durch das eingehende Bild.  Jedes lokale Empfangsfeld hat in der ersten verborgenen Schicht ein eigenes verstecktes Neuron.  Beginnen Sie f√ºr eine genauere Darstellung mit dem lokalen Empfangsfeld in der oberen linken Ecke: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c41/5cd/64d/c415cd64dfc93b81b89395ae360026c1.png"><br><br>  Bewegen Sie das lokale Empfangsfeld um ein Pixel nach rechts (ein Neuron), um es dem zweiten versteckten Neuron zuzuordnen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db1/285/a7a/db1285a7a7009e0210e97253061054f3.png"><br><br>  Also bauen wir die erste versteckte Ebene.  Beachten Sie, dass wenn unser eingehendes Bild 28x28 ist und das lokale Empfangsfeld 5x5 ist, sich 24x24 Neuronen in der verborgenen Schicht befinden.  Dies liegt daran, dass wir das lokale Empfangsfeld nur um 23 Neuronen nach rechts (oder unten) bewegen k√∂nnen und dann auf die rechte (oder untere) Seite des eingehenden Bildes treffen. <br><br>  In diesem Beispiel bewegen sich die lokalen Empfangsfelder jeweils um ein Pixel.  Manchmal wird jedoch eine andere Schrittgr√∂√üe verwendet.  Zum Beispiel k√∂nnten wir das lokale Empfangsfeld um 2 Pixel zur Seite verschieben, und in diesem Fall k√∂nnen wir √ºber die Gr√∂√üe von Schritt 2 sprechen. In diesem Kapitel werden wir haupts√§chlich Schritt 1 verwenden, aber Sie sollten wissen, dass manchmal Experimente mit Schritten einer anderen Gr√∂√üe durchgef√ºhrt werden .  Sie k√∂nnen mit der Schrittgr√∂√üe wie mit anderen Hyperparametern experimentieren.  Sie k√∂nnen auch die Gr√∂√üe des lokalen Empfangsfelds √§ndern, es stellt sich jedoch normalerweise heraus, dass eine gr√∂√üere Gr√∂√üe des lokalen Empfangsfelds bei Bildern, die deutlich gr√∂√üer als 28 x 28 Pixel sind, besser funktioniert. <br><br><h3>  Gesamtgewichte und Offsets </h3><br>  Ich erw√§hnte, dass jedes versteckte Neuron einen Versatz und 5x5 Gewichte hat, die mit seinem lokalen Empfangsfeld verbunden sind.  Aber ich habe nicht erw√§hnt, dass wir f√ºr alle versteckten 24x24-Neuronen die gleichen Gewichte und Verschiebungen verwenden werden.  Mit anderen Worten, f√ºr ein verstecktes Neuron j, k ist die Ausgabe gleich: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>b</mi><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></msubsup><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></msubsup><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi><mo>,</mo><mi>m</mi></mrow></msub><msub><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>+</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>+</mo><mi>m</mi></mrow></msub><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>125</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="58.874ex" height="3.021ex" viewBox="0 -883.9 25348.3 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-69" x="719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-67" x="1065" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="1545" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-61" x="2424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6C" x="3203" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-65" x="3502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-66" x="3968" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-74" x="4519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-28" x="4880" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-62" x="5270" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-2B" x="5921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-73" x="7172" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-75" x="7641" y="0"></use><g transform="translate(8214,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-34" x="1242" y="488"></use><g transform="translate(878,-328)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-3D" x="298" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-30" x="1077" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-73" x="10558" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-75" x="11027" y="0"></use><g transform="translate(11600,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-34" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-3D" x="878" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-30" x="1657" y="0"></use></g></g><g transform="translate(14104,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-77" x="0" y="0"></use><g transform="translate(716,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-2C" x="298" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="577" y="0"></use></g></g><g transform="translate(15950,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-61" x="0" y="0"></use><g transform="translate(529,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-2B" x="412" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6C" x="1191" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-2C" x="1489" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6B" x="1768" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-2B" x="2289" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-6D" x="3068" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-72" x="19620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-69" x="20071" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-67" x="20417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-68" x="20897" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-74" x="21474" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-29" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-74" x="22475" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-61" x="22836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMATHI-67" x="23366" y="0"></use><g transform="translate(23846,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-32" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/463171/&amp;usg=ALkJrhhtUlUCh7KrTf8RO8_OTvZm6d_MtA#MJMAIN-35" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy="false">(</mo><mi>b</mi><mo>+</mo><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></msubsup><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mn>4</mn></msubsup><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>l</mi><mo>,</mo><mi>m</mi></mrow></msub><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>+</mo><mi>l</mi><mo>,</mo><mi>k</mi><mo>+</mo><mi>m</mi></mrow></msub><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>125</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> \ sigma \ left (b + \ sum_ {l = 0} ^ 4 \ sum_ {m = 0} ^ 4 w_ {l, m} a_ {j + l, k + m} \ right) \ tag {125} </script></p><br><br>  Hier ist œÉ die Aktivierungsfunktion, m√∂glicherweise ein Sigmoid aus fr√ºheren Kapiteln.  b ist der Gesamtversatzwert.  w <sub>l, m</sub> - Anordnung der Gesamtgewichte 5x5.  Und schlie√ülich bezeichnet a <sub>x, y</sub> die Eingangsaktivierung an Position x, y. <br><br>  Dies bedeutet, dass alle Neuronen in der ersten verborgenen Schicht dasselbe Zeichen erkennen, das sich nur in verschiedenen Teilen des Bildes befindet.  Ein von einem versteckten Neuron erkanntes Zeichen ist eine bestimmte eingehende Sequenz, die zur Aktivierung eines Neurons f√ºhrt: m√∂glicherweise der Bildrand oder eine Form.  Um zu verstehen, warum dies sinnvoll ist, nehmen wir an, dass unsere Gewichte und Verschiebungen so sind, dass ein verstecktes Neuron beispielsweise eine vertikale Fl√§che in einem bestimmten lokalen Empfangsfeld erkennen kann.  Diese F√§higkeit ist wahrscheinlich an anderer Stelle im Bild n√ºtzlich.  Daher ist es n√ºtzlich, denselben Merkmaldetektor √ºber den gesamten Bildbereich zu verwenden.  Noch abstrakter ist die SNA gut an die translatorische Invarianz von Bildern angepasst: Bewegen Sie beispielsweise das Bild der Katze ein wenig zur Seite, und es bleibt weiterhin das Bild der Katze.  Richtig, die Bilder aus dem MNIST-Ziffernklassifizierungsproblem sind alle zentriert und in der Gr√∂√üe normalisiert.  Daher hat MNIST eine geringere Translationsinvarianz als zuf√§llige Bilder.  Dennoch sind Merkmale wie Gesichter und Winkel wahrscheinlich auf der gesamten Oberfl√§che des eingehenden Bildes n√ºtzlich. <br><br>  Aus diesem Grund wird die Zuordnung eines eingehenden Layers und eines verborgenen Layers manchmal als Feature-Map bezeichnet.  Gewichte, die die Merkmalskarte definieren, nennen wir Gesamtgewichte.  Und die Vorspannung, die die Merkmalskarte definiert, ist die allgemeine Vorspannung.  Es wird oft gesagt, dass Gesamtgewichte und Verschiebung einen Kernel oder Filter bestimmen.  Aber in der Literatur werden diese Begriffe manchmal aus einem etwas anderen Grund verwendet, und deshalb werde ich nicht weiter auf die Terminologie eingehen.  Schauen wir uns besser einige konkrete Beispiele an. <br><br>  Die von mir beschriebene Netzwerkstruktur kann nur ein lokalisiertes Attribut einer Art erkennen.  Um Bilder zu erkennen, ben√∂tigen wir mehr Feature-Maps.  Daher besteht die fertige Faltungsschicht aus mehreren verschiedenen Merkmalskarten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f84/5df/a57/f845dfa572668e27590c5bd1c057f849.png"><br><br>  Das Beispiel zeigt 3 Feature-Maps.  Jede Karte wird durch einen Satz von 5x5 Gesamtgewichten und einen gemeinsamen Versatz bestimmt.  Infolgedessen kann ein solches Netzwerk drei verschiedene Arten von Zeichen erkennen, und jedes Zeichen kann in jedem Teil des Bildes gefunden werden. <br><br>  Der Einfachheit halber habe ich drei Attributkarten gezogen.  In der Praxis kann der SNA mehr (m√∂glicherweise viel mehr) Feature-Maps verwenden.  Einer der fr√ºhen SNSs, LeNet-5, verwendete 6 Feature-Karten, von denen jede einem 5x5-Empfangsfeld zugeordnet war, um MNIST-Ziffern zu erkennen.  Daher ist das obige Beispiel LeNet-5 sehr √§hnlich.  In den Beispielen, die wir unabh√§ngig voneinander weiterentwickeln werden, werden wir Faltungsschichten verwenden, die 20 und 40 Feature-Karten enthalten.  Werfen wir einen kurzen Blick auf die Zeichen, die wir untersuchen werden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fad/a16/6b7/fada166b767b58edbff262944ee6488b.png"><br><br>  Diese 20 Bilder entsprechen 20 verschiedenen Attributzuordnungen (Filter oder Kernel).  Jede Karte wird durch ein 5x5-Bild dargestellt, das 5x5-Gewichten des lokalen Empfangsfeldes entspricht.  Wei√üe Pixel bedeuten ein geringes (normalerweise negativeres) Gewicht, und die Feature-Map reagiert weniger auf die entsprechenden Pixel.  Dunkle Pixel bedeuten mehr Gewicht und die Feature-Map reagiert st√§rker auf die entsprechenden Pixel.  Grob gesagt zeigen diese Bilder die Zeichen, auf die die Faltungsschicht reagiert. <br><br>  Welche Schlussfolgerungen k√∂nnen aus diesen Attributkarten gezogen werden?  Die r√§umlichen Strukturen hier erschienen offensichtlich nicht zuf√§llig - viele Zeichen zeigen klare helle und dunkle Bereiche.  Dies deutet darauf hin, dass unser Netzwerk wirklich etwas in Bezug auf r√§umliche Strukturen lernt.  Abgesehen davon ist es jedoch ziemlich schwierig zu verstehen, was diese Zeichen sind.  Wir untersuchen offensichtlich keine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gabor-Filter</a> , die in vielen traditionellen Ans√§tzen zur Mustererkennung verwendet wurden.  Tats√§chlich wird jetzt viel Arbeit geleistet, um besser zu verstehen, welche Zeichen von der SNA genau untersucht werden.  Wenn Sie interessiert sind, empfehle ich ab <a href="">2013 zu beginnen</a> . <br><br>  Der gro√üe Vorteil allgemeiner Gewichte und Offsets besteht darin, dass dadurch die Anzahl der f√ºr den SNA verf√ºgbaren Parameter drastisch reduziert wird.  F√ºr jede Feature-Map ben√∂tigen wir 5 √ó 5 = 25 Gesamtgewichte und einen gemeinsamen Versatz.  Daher sind f√ºr jede Feature-Map 26 Parameter erforderlich.  Wenn wir 20 Feature-Maps haben, haben wir insgesamt 20 √ó 26 = 520 Parameter, die die Faltungsschicht definieren.  Nehmen wir zum Vergleich an, wir haben eine vollst√§ndig verbundene erste Schicht mit 28 √ó 28 = 784 eingehenden Neuronen und relativ bescheidenen 30 versteckten Neuronen - wir haben dieses Schema in vielen Beispielen fr√ºher verwendet.  Es ergeben sich 784 √ó 30 Gewichte plus 30 Offsets, insgesamt 23.550 Parameter.  Mit anderen Worten, eine vollst√§ndig verbundene Schicht hat mehr als 40-mal mehr Parameter als eine Faltungsschicht. <br><br>  Nat√ºrlich k√∂nnen wir die Anzahl der Parameter nicht direkt vergleichen, da sich diese beiden Modelle radikal unterscheiden.  Intuitiv scheint es jedoch so zu sein, dass die Verwendung der Faltungs-Translationsinvarianz die Anzahl der Parameter verringert, die erforderlich sind, um eine Effizienz zu erzielen, die mit der eines vollst√§ndig verbundenen Modells vergleichbar ist.  Dies wiederum wird das Training des Faltungsmodells beschleunigen und uns letztendlich helfen, mithilfe von Faltungsschichten tiefere Netzwerke aufzubauen. <br><br>  Der Name "Faltung" stammt √ºbrigens aus der Operation in Gleichung (125), die manchmal als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltung bezeichnet wird</a> .  Genauer gesagt, manchmal schreiben Menschen diese Gleichung als <sup>1</sup> = œÉ (b + w ‚àó a <sup>0</sup> ), wobei eine <sup>1</sup> einen Satz von Ausgangsaktivierungen einer Merkmalskarte, eine <sup>0</sup> - einen Satz von Eingabeaktivierungen und * eine Faltungsoperation bezeichnet.  Wir werden uns nicht eingehend mit der Mathematik der Windungen befassen, sodass Sie sich √ºber diesen Zusammenhang nicht besonders Gedanken machen m√ºssen.  Es lohnt sich jedoch nur zu wissen, woher der Name stammt. <br><br><h3>  Schichten b√ºndeln </h3><br>  Zus√§tzlich zu den in der SNA beschriebenen Faltungsschichten gibt es auch Pooling-Schichten.  Sie werden normalerweise unmittelbar nach der Faltung verwendet.  Sie sind bestrebt, Informationen aus der Ausgabe der Faltungsschicht zu vereinfachen. <br><br>  Hier verwende ich den Ausdruck "Feature Map" nicht im Sinne der von der Faltungsschicht berechneten Funktion, sondern um die Aktivierung der Ausgabe von Neuronen der verborgenen Schicht anzuzeigen.  Eine solche freie Verwendung von Begriffen findet sich h√§ufig in der Forschungsliteratur. <br><br>  Die Pooling-Schicht akzeptiert die Ausgabe jeder Faltungsschicht-Feature-Map und erstellt eine komprimierte Feature-Map.  Beispielsweise kann jedes Element der Pooling-Schicht einen Abschnitt von beispielsweise 2 √ó 2 Neuronen der vorherigen Schicht zusammenfassen.  Fallstudie: Ein g√§ngiges Pooling-Verfahren ist das Max-Pooling.  Beim maximalen Pooling gibt das Pooling-Element einfach die maximale Aktivierung aus dem 2x2-Abschnitt an, wie in der Abbildung gezeigt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dd0/f6b/86c/dd0f6b86c374504de4ae58056a0f7008.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Da die Ausgabe von Faltungsschichtneuronen 24x24-Werte ergibt, erhalten wir nach dem Ziehen 12x12 Neuronen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie oben erw√§hnt, impliziert eine Faltungsschicht normalerweise etwas mehr als eine einzelne Merkmalskarte. Wir wenden das maximale Pooling auf jede Feature-Map einzeln an. Wenn wir also drei Feature-Maps haben, sehen die kombinierten Faltungs- und Max-Pooling-Ebenen folgenderma√üen aus:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a95/68f/8d1/a9568f8d10dd7dced2f682fe259aed48.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Max-Pulling kann als eine M√∂glichkeit des Netzwerks angesehen werden, zu fragen, ob sich an einer beliebigen Stelle des Bildes ein bestimmtes Zeichen befindet. Und dann verwirft sie Informationen √ºber den genauen Standort. Es ist intuitiv klar, dass wenn ein Zeichen gefunden wird, seine genaue Position nicht mehr so ‚Äã‚Äãwichtig ist wie seine ungef√§hre Position relativ zu anderen Zeichen. Der Vorteil besteht darin, dass die Anzahl der durch Pooling erhaltenen Features viel geringer ist, und dies hilft, die Anzahl der in den n√§chsten Schichten erforderlichen Parameter zu reduzieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Max Pooling ist nicht die einzige Pooling-Technologie. </font><font style="vertical-align: inherit;">Ein weiterer g√§ngiger Ansatz ist das L2-Pooling. </font><font style="vertical-align: inherit;">Anstatt die maximale Aktivierung der Region von 2x2-Neuronen zu nehmen, nehmen wir darin die Quadratwurzel der Summe der Quadrate der Aktivierung der 2x2-Region. </font><font style="vertical-align: inherit;">Details der Ans√§tze unterscheiden sich, aber intuitiv √§hnelt es dem Max-Pooling: L2-Pooling ist eine M√∂glichkeit, Informationen aus einer Faltungsschicht zu komprimieren. </font><font style="vertical-align: inherit;">In der Praxis werden h√§ufig beide Technologien eingesetzt. </font><font style="vertical-align: inherit;">Manchmal verwenden Menschen andere Arten von Pooling. </font><font style="vertical-align: inherit;">Wenn Sie Schwierigkeiten haben, die Qualit√§t des Netzwerks zu optimieren, k√∂nnen Sie die unterst√ºtzenden Daten verwenden, um verschiedene Ans√§tze zum Ziehen zu vergleichen und den besten auszuw√§hlen. </font><font style="vertical-align: inherit;">Wir werden uns jedoch nicht um eine so detaillierte Optimierung k√ºmmern.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zusammenfassend </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jetzt k√∂nnen wir alle Informationen zusammenf√ºhren und eine vollst√§ndige SNA erhalten. Es √§hnelt der Architektur, die wir k√ºrzlich √ºberpr√ºft haben, verf√ºgt jedoch √ºber eine zus√§tzliche Schicht von 10 Ausgangsneuronen, die 10 m√∂glichen Werten der MNIST-Ziffern entsprechen ('0', '1', '2', ..): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b2d/ba1/8ee/b2dba18ee40b3f642fb9f4e9cbda772b.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Netzwerk beginnt mit 28x28 verwendeten Eingangsneuronen um die Pixelintensit√§t des MNIST-Bildes zu codieren. Danach folgt eine Faltungsschicht unter Verwendung der lokalen Empfangsfelder 5x5 und 3 Feature Maps. Das Ergebnis ist eine Schicht von Neuronen mit 3 x 24 x 24 versteckten Merkmalen. Der n√§chste Schritt ist eine maximale Pooling-Ebene, die auf 2x2 Bereiche auf jeder der drei Feature-Maps angewendet wird. Das Ergebnis ist eine Schicht von Neuronen mit 3 x 12 x 12 versteckten Merkmalen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die letzte Verbindungsebene im Netzwerk ist vollst√§ndig verbunden. Das hei√üt, es verbindet jedes Neuron der Max-Pooling-Schicht mit jedem der 10 Ausgangsneuronen. Wir haben fr√ºher eine solche vollst√§ndig verbundene Architektur verwendet. Bitte beachten Sie, dass ich im obigen Diagramm der Einfachheit halber einen einzelnen Pfeil verwendet habe, der nicht alle Links anzeigt. Sie k√∂nnen sich alle leicht vorstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Faltungsarchitektur unterscheidet sich sehr von dem, was wir zuvor verwendet haben. Das Gesamtbild ist jedoch √§hnlich: Ein Netzwerk, das aus vielen einfachen Elementen besteht, deren Verhalten durch Gewichte und Offsets bestimmt wird. Das Ziel bleibt dasselbe: Verwenden Sie Trainingsdaten, um das Netzwerk in Gewichten und Offsets zu trainieren, damit das Netzwerk eingehende Nummern gut klassifiziert.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Insbesondere werden wir wie in den vorherigen Kapiteln unser Netzwerk mit stochastischem Gradientenabstieg und R√ºckausbreitung trainieren. Der Vorgang l√§uft fast genauso ab wie zuvor. Wir m√ºssen jedoch einige √Ñnderungen am Backpropagation-Verfahren vornehmen. Tatsache ist, dass unsere Derivate f√ºr die R√ºckausbreitung f√ºr ein Netzwerk mit vollst√§ndig verbundenen Schichten gedacht waren. Gl√ºcklicherweise ist das √Ñndern von Ableitungen f√ºr Faltungs- und Max-Pooling-Schichten recht einfach. Wenn Sie die Details verstehen m√∂chten, lade ich Sie ein, das folgende Problem zu l√∂sen. Ich werde Sie warnen, dass es viel Zeit in Anspruch nehmen wird, es sei denn, Sie haben die fr√ºhen Fragen der Differenzierung der R√ºckausbreitung gr√ºndlich verstanden.</font></font><br><br><h3>  Herausforderung </h3><br><ul><li>     .            (BP1)-(BP4). ,     ,  -     ,     .      ? </li></ul><br><h2>      </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben die Ideen hinter der SNA diskutiert. Lassen Sie uns sehen, wie sie in der Praxis funktionieren, indem einige SNAs implementiert und auf das MNIST-Problem der Ziffernklassifizierung angewendet werden. Wir werden das Programm network3.py verwenden, eine verbesserte Version der Programme network.py und network2.py, die in den vorherigen Kapiteln erstellt wurden. Das Programm network3.py verwendet Ideen aus der Dokumentation der Theano-Bibliothek (insbesondere der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LeNet-5- </font></font></a><font style="vertical-align: inherit;"></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementierung</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) aus der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Implementierung der Ausnahme</font></a><font style="vertical-align: inherit;"> von Misha Denil und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chris Olah</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Der Programmcode ist auf GitHub verf√ºgbar. Im n√§chsten Abschnitt werden wir den Code des Programms network3.py untersuchen und in diesem Abschnitt als Bibliothek zum Erstellen des SNA verwenden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Programme network.py und network2.py wurden unter Verwendung der Numpy-Matrixbibliothek in Python geschrieben. Sie arbeiteten auf der Grundlage erster Prinzipien und erreichten die detailliertesten Details der R√ºckausbreitung, des stochastischen Gradientenabfalls usw. Wenn wir diese Details verstehen, verwenden wir f√ºr network3.py die Theano-Bibliothek f√ºr maschinelles Lernen (siehe die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wissenschaftliche Arbeit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mit ihrer Beschreibung). Theano ist auch die Basis der beliebten Bibliotheken f√ºr NS </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pylearn2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keras</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sowie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Torch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Verwendung von Theano erleichtert die Implementierung der Backpropagation in der SNA, da automatisch alle Karten gez√§hlt werden. Theano ist auch deutlich schneller als unser vorheriger Code (der zum besseren Verst√§ndnis geschrieben wurde und nicht f√ºr Hochgeschwindigkeitsarbeiten). Daher ist es sinnvoll, ihn zum Trainieren komplexerer Netzwerke zu verwenden. Eine der gro√üartigen Funktionen von Theano besteht insbesondere darin, Code sowohl auf der CPU als auch auf der GPU auszuf√ºhren, sofern verf√ºgbar. Das Ausf√ºhren auf einer GPU erh√∂ht die Geschwindigkeit erheblich und hilft beim Trainieren komplexerer Netzwerke. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um parallel zum Buch zu arbeiten, m√ºssen Sie Theano auf Ihrem System installieren. Befolgen Sie dazu die Anweisungen auf </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der Projekthomepage</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Zum Zeitpunkt des Schreibens und Startens der Beispiele war Theano 0.7 verf√ºgbar. Ich habe einige Experimente unter Mac OS X Yosemite ohne GPU durchgef√ºhrt. Einige unter Ubuntu 14.04 mit einer NVIDIA-GPU. Und einige sind da und da. Setzen Sie zum Starten von network3.py das GPU-Flag im Code auf True oder False. Dar√ºber hinaus k√∂nnen die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">folgenden Anweisungen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ihnen helfen, Theano auf Ihrer GPU auszuf√ºhren </font><font style="vertical-align: inherit;">. Es ist auch einfach, Schulungsunterlagen online zu finden. Wenn Sie keine eigene GPU haben, k√∂nnen Sie sich an </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Amazon Web Services EC2 G2 wenden</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Aber selbst mit einer GPU funktioniert unser Code nicht sehr schnell. Viele Experimente dauern einige Minuten bis mehrere Stunden. Die komplexesten auf einer einzelnen CPU werden mehrere Tage lang ausgef√ºhrt. Wie in den vorherigen Kapiteln empfehle ich, das Experiment zu starten und weiterzulesen, wobei die Funktionsweise regelm√§√üig √ºberpr√ºft wird. Ohne Verwendung einer GPU empfehle ich, die Anzahl der Trainingszeiten f√ºr die komplexesten Experimente zu reduzieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um grundlegende Vergleichsergebnisse zu erhalten, beginnen wir mit einer flachen Architektur mit einer verborgenen Schicht, die 100 verborgene Neuronen enth√§lt. Wir werden 60 Epochen studieren, die Lerngeschwindigkeit Œ∑ = 0,1 verwenden, die Gr√∂√üe des Minipakets betr√§gt 10 und wir werden ohne Regularisierung lernen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Abschnitt habe ich eine bestimmte Anzahl von Trainingsperioden festgelegt. </font><font style="vertical-align: inherit;">Ich mache dies aus Gr√ºnden der Klarheit im Lernprozess. </font><font style="vertical-align: inherit;">In der Praxis ist es n√ºtzlich, fr√ºhe Stopps zu verwenden, die Genauigkeit des Best√§tigungssatzes zu verfolgen und das Training zu beenden, wenn wir davon √ºberzeugt sind, dass sich die Genauigkeit der Best√§tigung nicht mehr verbessert:</font></font><br><br><pre><script type="text/javascript">function gtElInit() {var lib = new google.translate.TranslateService();lib.translatePage('ru', 'de', function () {});}</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=gtElInit&amp;client=wt"></script><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network3 &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> network3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Network &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> network3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer &gt;&gt;&gt; training_data, validation_data, test_data = network3.load_data_shared() &gt;&gt;&gt; mini_batch_size = <span class="hljs-number"><span class="hljs-number">10</span></span> &gt;&gt;&gt; net = Network([ FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">784</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, validation_data, test_data)</code> </pre> <br>  Die beste Klassifizierungsgenauigkeit betrug 97,80%.  Dies ist die Klassifizierungsgenauigkeit test_data, gesch√§tzt aus dem Trainingszeitalter, in der wir die beste Klassifizierungsgenauigkeit f√ºr Daten aus validation_data erhalten haben.  Durch die Verwendung validierender Daten zur Entscheidung √ºber die Genauigkeitsbewertung kann eine Umschulung vermieden werden.  Dann werden wir es tun.  Ihre Ergebnisse k√∂nnen geringf√ºgig variieren, da Netzwerkgewichte und Offsets zuf√§llig initialisiert werden. <br><br>  Die Genauigkeit von 97,80% liegt ziemlich nahe an der Genauigkeit von 98,04%, die in Kapitel 3 unter Verwendung einer √§hnlichen Netzwerkarchitektur und Trainingshyperparametern erhalten wurde.  Insbesondere verwenden beide Beispiele flache Netzwerke mit einer verborgenen Schicht, die 100 verborgene Neuronen enth√§lt.  Beide Netzwerke lernen 60 Epochen mit einer Minipaketgr√∂√üe von 10 und einer Lernrate von Œ∑ = 0,1. <br><br>  Im fr√ºheren Netzwerk gab es jedoch zwei Unterschiede.  Zun√§chst f√ºhrten wir eine Regularisierung durch, um die Auswirkungen der Umschulung zu verringern.  Durch die Regularisierung des aktuellen Netzwerks wird die Genauigkeit verbessert, jedoch nicht wesentlich. Daher werden wir vorerst nicht dar√ºber nachdenken.  Zweitens, obwohl die letzte Schicht des fr√ºhen Netzwerks Sigmoid-Aktivierungen und die Cross-Entropy-Kostenfunktion verwendete, verwendet das aktuelle Netzwerk die letzte Schicht mit Softmax und die logarithmische Wahrscheinlichkeitsfunktion als Kostenfunktion.  Wie in Kapitel 3 beschrieben, ist dies keine wesentliche √Ñnderung.  Ich habe aus irgendeinem Grund nicht von einem zum anderen gewechselt - haupts√§chlich, weil Softmax und die logarithmische Wahrscheinlichkeitsfunktion in modernen Netzwerken h√§ufiger zur Klassifizierung von Bildern verwendet werden. <br><br>  K√∂nnen wir die Ergebnisse mithilfe einer tieferen Netzwerkarchitektur verbessern? <br><br>  Beginnen wir mit dem Einf√ºgen einer Faltungsschicht ganz am Anfang des Netzwerks.  Wir werden das lokale Empfangsfeld 5x5 verwenden, eine Schrittl√§nge von 1 und 20 Feature-Karten.  Wir werden auch eine maximale Pooling-Ebene einf√ºgen, die Funktionen mithilfe von 2x2-Pooling-Fenstern kombiniert.  Die gesamte Netzwerkarchitektur sieht also √§hnlich aus wie im vorherigen Abschnitt, jedoch mit einer zus√§tzlichen vollst√§ndig verbundenen Schicht: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ca/178/8d2/7ca1788d2206313b37a6f8896086b582.png"><br><br>  In dieser Architektur werden die Faltungs- und Pooling-Schichten in der lokalen r√§umlichen Struktur trainiert, die im eingehenden Trainingsbild enthalten ist, und die letzte vollst√§ndig verbundene Schicht wird auf einer abstrakteren Ebene trainiert, wobei globale Informationen aus dem gesamten Bild integriert werden.  Dies ist ein h√§ufig verwendetes Schema in der SNA. <br><br>  Lassen Sie uns ein solches Netzwerk trainieren und sehen, wie es sich verh√§lt. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">20</span></span>*<span class="hljs-number"><span class="hljs-number">12</span></span>*<span class="hljs-number"><span class="hljs-number">12</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, validation_data, test_data)</code> </pre> <br>  Wir erhalten eine Genauigkeit von 98,78%, was deutlich √ºber den vorherigen Ergebnissen liegt.  Wir haben den Fehler um mehr als ein Drittel reduziert - ein hervorragendes Ergebnis. <br><br>  Bei der Beschreibung der Netzwerkstruktur betrachtete ich Faltungs- und Pooling-Schichten als eine einzige Schicht.  Betrachten Sie sie als separate Schichten oder als einzelne Schicht - eine Frage der Pr√§ferenz.  network3.py betrachtet sie als eine Ebene, da der Code auf diese Weise kompakter ist.  Es ist jedoch einfach, network3.py so zu √§ndern, dass die Ebenen einzeln festgelegt werden k√∂nnen. <br><br><h3>  √úbung </h3><br><ul><li>  Welche Klassifizierungsgenauigkeit erhalten wir, wenn wir die vollst√§ndig verbundene Schicht absenken und nur die Faltungs- / Poolschicht und die Softmax-Schicht verwenden?  Hilft die Aufnahme einer vollst√§ndig verbundenen Schicht? </li></ul><br>  K√∂nnen wir das Ergebnis um 98,78% verbessern? <br><br>  Versuchen wir, die zweite Faltungs- / Pooling-Schicht einzuf√ºgen.  Wir werden es zwischen der vorhandenen Faltung / Pooling und den vollst√§ndig verbundenen verborgenen Schichten einf√ºgen.  Wir verwenden wieder das lokale 5x5-Empfangsfeld und den Pool in 2x2-Abschnitten.  Mal sehen, was passiert, wenn wir ein Netzwerk mit ungef√§hr den gleichen Hyperparametern wie zuvor trainieren: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)), ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">40</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, validation_data, test_data)</code> </pre> <br>  Und wieder haben wir eine Verbesserung: Jetzt erhalten wir eine Genauigkeit von 99,06%! <br><br>  Im Moment stellen sich zwei nat√ºrliche Fragen.  Erstens: Was bedeutet es, die zweite Faltungs- / Pooling-Schicht zu verwenden?  Sie k√∂nnen davon ausgehen, dass auf der zweiten Faltungs- / Pooling-Ebene "12 x 12" -Bilder zur Eingabe gelangen, deren "Pixel" das Vorhandensein (oder Fehlen) bestimmter lokalisierter Merkmale im eingehenden Originalbild darstellen.  Das hei√üt, wir k√∂nnen davon ausgehen, dass eine bestimmte Version des eingehenden Originalbilds zur Eingabe dieser Ebene gelangt.  Dies wird eine abstraktere und pr√§gnantere Version sein, hat aber immer noch gen√ºgend r√§umliche Struktur, so dass es sinnvoll ist, eine zweite Faltungs- / Ziehschicht zu verwenden, um sie zu verarbeiten. <br><br>  Eine angenehme Sichtweise, die jedoch eine zweite Frage aufwirft.  Bei der Ausgabe von der vorherigen Schicht werden 20 separate KPs erhalten, daher kommen 20 √ó 12 √ó 12 Gruppen von Eingabedaten zur zweiten Faltungs- / Pooling-Schicht.  Es stellt sich heraus, dass in der Faltungs- / Pooling-Schicht 20 separate Bilder enthalten sind und nicht ein Bild, wie dies bei der ersten Faltungs- / Pooling-Ebene der Fall war.  Wie m√ºssen Neuronen aus der zweiten Faltungs- / Poolschicht auf viele dieser eingehenden Bilder reagieren?  Tats√§chlich erlauben wir einfach, dass jedes Neuron dieser Schicht auf der Basis aller 20x5x5 Neuronen trainiert wird, die in sein lokales Empfangsfeld eintreten.  In weniger formalen Begriffen haben Merkmaldetektoren in der zweiten Faltungs- / Poolschicht Zugriff auf alle Merkmale der ersten Schicht, jedoch nur innerhalb ihrer spezifischen lokalen Empfangsfelder. <br><br>  Ein solches Problem w√§re √ºbrigens in der ersten Schicht aufgetreten, wenn die Bilder farbig w√§ren.  In diesem Fall h√§tten wir 3 Eingabeattribute f√ºr jedes Pixel, die den roten, gr√ºnen und blauen Kan√§len des Originalbilds entsprechen.  Und dann w√ºrden wir auch Zeichendetektoren Zugriff auf alle Farbinformationen gew√§hren, jedoch nur im Rahmen ihres lokalen Empfangsfeldes. <br><br><h3>  Herausforderung </h3><br><ul><li>  Verwendung der Aktivierungsfunktion in Form einer hyperbolischen Tangente.  Zu Beginn dieses Buches erw√§hnte ich mehrmals Beweise daf√ºr, dass die Tanh-Funktion, eine hyperbolische Tangente, m√∂glicherweise besser als Aktivierungsfunktion als als Sigmoid geeignet ist.  Wir haben nichts damit gemacht, da wir mit dem Sigmoid gute Fortschritte gemacht haben.  Aber versuchen wir einige Experimente mit Tanh als Aktivierungsfunktion.  Versuchen Sie, ein Tang-aktiviertes Netzwerk mit Faltungsschichten und vollst√§ndig verbundenen Ebenen zu trainieren (Sie k√∂nnen activity_fn = tanh als Parameter an die Klassen ConvPoolLayer und FullyConnectedLayer √ºbergeben).  Beginnen Sie mit denselben Hyperparametern wie das Sigmoid-Netzwerk, trainieren Sie jedoch das Netzwerk aus 20 Epochen, nicht aus 60. Wie verh√§lt sich das Netzwerk?  Was wird passieren, wenn wir bis zur 60. √Ñra weitermachen?  Versuchen Sie, ein Diagramm √ºber die Genauigkeit der Best√§tigung der Arbeit durch Epochen f√ºr Tangente und Sigmoid bis zur 60. √Ñra zu erstellen.  Wenn Ihre Ergebnisse meinen √§hnlich sind, werden Sie feststellen, dass das tangentiale Netzwerk etwas schneller lernt, die resultierende Genauigkeit beider Netzwerke jedoch gleich ist.  K√∂nnen Sie erkl√§ren, warum dies passiert?  Ist es m√∂glich, mit einem Sigmoid die gleiche Lerngeschwindigkeit zu erreichen - zum Beispiel durch √Ñndern der Lerngeschwindigkeit oder durch Skalieren (denken Sie daran, dass œÉ (z) = (1 + tanh (z / 2)) / 2)?  Probieren Sie f√ºnf oder sechs verschiedene Hyperparameter oder Netzwerkarchitekturen aus und suchen Sie, wo die Tangente vor dem Sigmoid liegen kann.  Ich stelle fest, dass diese Aufgabe offen ist.  Pers√∂nlich habe ich beim Umschalten auf die Tangente keine ernsthaften Vorteile festgestellt, obwohl ich keine umfassenden Experimente durchgef√ºhrt habe, und vielleicht werden Sie sie finden.  In jedem Fall werden wir bald einen Vorteil beim Umschalten auf eine begradigte lineare Aktivierungsfunktion finden, so dass wir uns nicht mehr mit dem Thema hyperbolischer Tangente befassen werden. </li></ul><br><h3>  Mit geraden linearen Elementen </h3><br>  Das Netzwerk, das wir derzeit entwickelt haben, ist eine der Netzwerkoptionen, die in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fruchtbaren Arbeit von 1998 verwendet wurden</a> , in der die Aufgabe von MNIST, einem Netzwerk namens LeNet-5, erstmals vorgestellt wurde.  Dies ist eine gute Grundlage f√ºr weitere Experimente, um das Verst√§ndnis des Problems und der Intuition zu verbessern.  Insbesondere gibt es viele M√∂glichkeiten, wie wir unser Netzwerk √§ndern k√∂nnen, um die Ergebnisse zu verbessern. <br><br>  Lassen Sie uns zun√§chst unsere Neuronen so √§ndern, dass wir anstelle der Sigmoid-Aktivierungsfunktion begradigte lineare Elemente (ReLU) verwenden k√∂nnen.  Das hei√üt, wir werden die Aktivierungsfunktion der Form f (z) ‚â° max (0, z) verwenden.  Wir werden ein Netzwerk von 60 Epochen mit einer Geschwindigkeit von Œ∑ = 0,03 trainieren.  Ich fand auch, dass es etwas bequemer ist, die L2-Regularisierung mit dem Regularisierungsparameter Œª = 0,1 zu verwenden: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> network3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReLU &gt;&gt;&gt; net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">40</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>, activation_fn=ReLU), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, validation_data, test_data, lmbda=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br>  Ich habe eine Klassifizierungsgenauigkeit von 99,23%.  Eine bescheidene Verbesserung gegen√ºber Sigmoid-Ergebnissen (99,06%).  In all meinen Experimenten stellte ich jedoch fest, dass Netzwerke, die auf ReLU basieren, Netzwerken voraus sind, die auf der Sigmoid-Aktivierungsfunktion mit beneidenswerter Konstanz basieren.  Anscheinend bietet der Wechsel zu ReLU echte Vorteile, um dieses Problem zu l√∂sen. <br><br>  Was macht die ReLU-Aktivierungsfunktion besser als die Sigmoid- oder hyperbolische Tangente?  Im Moment verstehen wir das nicht besonders.  Es wird normalerweise gesagt, dass die Funktion max (0, z) im Gegensatz zu Sigmoid-Neuronen bei gro√üem z nicht ges√§ttigt ist, und dies hilft ReLU-Neuronen, weiter zu lernen.  Ich argumentiere nicht, aber diese Rechtfertigung kann nicht als umfassend bezeichnet werden, sondern ist nur eine Art Beobachtung (ich erinnere Sie daran, dass wir in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2</a> √ºber die S√§ttigung gesprochen haben). <br><br>  ReLU wurde in den letzten Jahren aktiv eingesetzt.  Sie wurden aus empirischen Gr√ºnden √ºbernommen: Einige Leute versuchten es mit ReLU, oft einfach aufgrund von Vermutungen oder heuristischen Argumenten.  Sie haben gute Ergebnisse erzielt und die Praxis hat sich verbreitet.  In einer idealen Welt h√§tten wir eine Theorie, die uns sagt, welche Anwendungen welche Aktivierungsfunktionen f√ºr welche Anwendungen am besten sind.  Aber vorerst haben wir noch einen langen Weg vor uns.  Es wird mich √ºberhaupt nicht wundern, wenn durch die Auswahl einer noch geeigneteren Aktivierungsfunktion weitere Verbesserungen im Betrieb der Netzwerke erzielt werden k√∂nnen.  Ich erwarte auch, dass in den kommenden Jahrzehnten eine gute Theorie der Aktivierungsfunktionen entwickelt wird.  Aber heute m√ºssen wir uns auf schlecht studierte Faust- und Erfahrungsregeln verlassen. <br><br><h3>  Erweiterung der Trainingsdaten </h3><br>  Eine andere M√∂glichkeit, die uns m√∂glicherweise dabei helfen kann, unsere Ergebnisse zu verbessern, besteht darin, die Trainingsdaten algorithmisch zu erweitern.  Der einfachste Weg, die Trainingsdaten zu erweitern, besteht darin, jedes Trainingsbild um ein Pixel nach oben, unten, rechts oder links zu verschieben.  Dies kann durch Ausf√ºhren des Programms <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">expand_mnist.py erfolgen</a> . <br><br><pre> <code class="bash hljs">$ python expand_mnist.py</code> </pre> <br>  Mit dem Start des Programms werden 50.000 Trainingsbilder von MNIST in einen erweiterten Satz von 250.000 Trainingsbildern umgewandelt.  Dann k√∂nnen wir diese Trainingsbilder verwenden, um das Netzwerk zu trainieren.  Wir werden das gleiche Netzwerk wie zuvor mit ReLU verwenden.  In meinen ersten Experimenten habe ich die Anzahl der Trainingsperioden reduziert - es war sinnvoll, weil wir f√ºnfmal mehr Trainingsdaten haben.  Durch die Erweiterung des Datensatzes wurde jedoch der Effekt der Umschulung erheblich reduziert.  Daher kehrte ich nach mehreren Experimenten zur Anzahl der Epochen 60 zur√ºck. Lassen Sie uns auf jeden Fall trainieren: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>expanded_training_data, _, _ = network3.load_data_shared( <span class="hljs-string"><span class="hljs-string">"../data/mnist_expanded.pkl.gz"</span></span>) &gt;&gt;&gt; net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">40</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>, activation_fn=ReLU), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(expanded_training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, validation_data, test_data, lmbda=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br>  Mit fortgeschrittenen Trainingsdaten erhielt ich eine Genauigkeit von 99,37%.  Eine solche fast triviale √Ñnderung f√ºhrt zu einer signifikanten Verbesserung der Klassifizierungsgenauigkeit.  Und wie bereits erw√§hnt, kann die algorithmische Datenerweiterung weiterentwickelt werden.  Zur Erinnerung: 2003 haben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Simard, Steinkraus und Platt</a> die Genauigkeit ihres Netzwerks auf 99,6% verbessert.  Ihr Netzwerk war unserem √§hnlich, sie verwendeten zwei Faltungs- / Poolschichten, gefolgt von einer vollst√§ndig verbundenen Schicht mit 100 Neuronen.  Die Details ihrer Architektur waren unterschiedlich - sie hatten beispielsweise keine Gelegenheit, ReLU zu nutzen -, aber der Schl√ºssel zur Verbesserung der Arbeitsqualit√§t war die Erweiterung der Schulungsdaten.  Dies wurde erreicht, indem MNIST-Trainingsbilder gedreht, √ºbertragen und verzerrt wurden.  Sie entwickelten auch den Prozess der ‚Äûelastischen Verzerrung‚Äú, bei dem die zuf√§lligen Vibrationen der Armmuskeln beim Schreiben nachgeahmt werden.  Durch die Kombination all dieser Prozesse haben sie das effektive Volumen ihrer Trainingsdatenbank erheblich erh√∂ht und dadurch eine Genauigkeit von 99,6% erreicht. <br><br><h3>  Herausforderung </h3><br><ul><li>  Die Idee von Faltungsschichten besteht darin, unabh√§ngig von der Position im Bild zu arbeiten.  Aber dann mag es seltsam erscheinen, dass unser Netzwerk besser trainiert ist, wenn wir einfach Eingabebilder verschieben.  K√∂nnen Sie erkl√§ren, warum dies eigentlich ganz vern√ºnftig ist? </li></ul><br><br><h3>  Hinzuf√ºgen einer zus√§tzlichen vollst√§ndig verbundenen Ebene </h3><br>  Ist es m√∂glich, die Situation zu verbessern?  Eine M√∂glichkeit besteht darin, genau das gleiche Verfahren anzuwenden, aber gleichzeitig die Gr√∂√üe der vollst√§ndig verbundenen Schicht zu erh√∂hen.  Ich habe das Programm mit 300 und 1000 Neuronen ausgef√ºhrt und Ergebnisse in 99,46% bzw. 99,43% erzielt.  Dies ist interessant, aber nicht besonders √ºberzeugend als das vorherige Ergebnis (99,37%). <br><br>  Was ist mit dem Hinzuf√ºgen einer zus√§tzlichen vollst√§ndig verbundenen Ebene?  Versuchen wir, eine zus√§tzliche vollst√§ndig verbundene Schicht hinzuzuf√ºgen, sodass wir zwei versteckte vollst√§ndig verbundene Schichten mit 100 Neuronen haben: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">40</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>, activation_fn=ReLU), FullyConnectedLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">100</span></span>, activation_fn=ReLU), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(expanded_training_data, <span class="hljs-number"><span class="hljs-number">60</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, validation_data, test_data, lmbda=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br>  Somit erreichte ich eine Verifizierungsgenauigkeit von 99,43%.  Das erweiterte Netzwerk hat die Leistung erneut nicht wesentlich verbessert.  Nachdem ich √§hnliche Experimente mit vollst√§ndig verbundenen Schichten von 300 und 100 Neuronen durchgef√ºhrt hatte, erhielt ich eine Genauigkeit von 99,48% und 99,47%.  Inspirierend, aber nicht wie ein echter Gewinn. <br><br>  Was ist los?  Ist es m√∂glich, dass erweiterte oder zus√§tzliche vollst√§ndig verbundene Schichten nicht zur L√∂sung des MNIST-Problems beitragen?  Oder kann unser Netzwerk besser abschneiden, aber wir entwickeln es in die falsche Richtung?  Vielleicht k√∂nnten wir zum Beispiel eine strengere Regularisierung verwenden, um die Umschulung zu reduzieren.  Eine M√∂glichkeit ist die in Kapitel 3 erw√§hnte Dropout-Technik. Denken Sie daran, dass die Grundidee des Ausschlusses darin besteht, einzelne Aktivierungen beim Training des Netzwerks zuf√§llig zu entfernen.  Infolgedessen wird das Modell widerstandsf√§higer gegen den Verlust einzelner Beweise, und daher ist es weniger wahrscheinlich, dass es sich auf einige kleine nicht standardm√§√üige Merkmale der Trainingsdaten st√ºtzt.  Versuchen wir, die Ausnahme auf die letzte vollst√§ndig verbundene Ebene anzuwenden: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = Network([ ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>), filter_shape=(<span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), poolsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), activation_fn=ReLU), FullyConnectedLayer( n_in=<span class="hljs-number"><span class="hljs-number">40</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>*<span class="hljs-number"><span class="hljs-number">4</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">1000</span></span>, activation_fn=ReLU, p_dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>), FullyConnectedLayer( n_in=<span class="hljs-number"><span class="hljs-number">1000</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">1000</span></span>, activation_fn=ReLU, p_dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>), SoftmaxLayer(n_in=<span class="hljs-number"><span class="hljs-number">1000</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>, p_dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)], mini_batch_size) &gt;&gt;&gt; net.SGD(expanded_training_data, <span class="hljs-number"><span class="hljs-number">40</span></span>, mini_batch_size, <span class="hljs-number"><span class="hljs-number">0.03</span></span>, validation_data, test_data)</code> </pre> <br>  Mit diesem Ansatz erreichen wir eine Genauigkeit von 99,60%, was viel besser ist als die vorherigen, insbesondere unsere grundlegende Einsch√§tzung - ein Netzwerk mit 100 versteckten Neuronen, das eine Genauigkeit von 99,37% ergibt. <br><br>  Zwei √Ñnderungen sind hier erw√§hnenswert. <br><br>  Zuerst habe ich die Anzahl der Trainingszeiten auf 40 reduziert: Ausnahme reduziert die Umschulung und wir lernen schneller. <br><br>  Zweitens enthalten vollst√§ndig verbundene verborgene Schichten 1000 Neuronen und nicht wie zuvor 100.  Nat√ºrlich eliminiert die Ausnahme tats√§chlich viele Neuronen w√§hrend des Trainings, daher sollten wir eine Art Expansion erwarten.  Tats√§chlich f√ºhrte ich Experimente mit 300 und 1000 Neuronen durch und erhielt bei 1000 Neuronen eine etwas bessere Best√§tigung. <br><br><h3>  Verwenden von Network Ensemble </h3><br>  Eine einfache M√∂glichkeit, die Effizienz zu verbessern, besteht darin, mehrere neuronale Netze zu erstellen und sie dann dazu zu bringen, f√ºr eine bessere Klassifizierung zu stimmen.  Nehmen wir zum Beispiel an, wir haben 5 verschiedene NS nach dem obigen Rezept trainiert und jeder von ihnen hat eine Genauigkeit von nahezu 99,6% erreicht.  Obwohl alle Netzwerke eine √§hnliche Genauigkeit aufweisen, k√∂nnen sie aufgrund unterschiedlicher zuf√§lliger Initialisierung unterschiedliche Fehler aufweisen.  Es ist anzunehmen, dass bei einer Abstimmung mit 5 NA ihre allgemeine Klassifizierung besser ist als die eines separaten Netzwerks. <br><br>  Es klingt zu sch√∂n, um wahr zu sein, aber das Zusammenstellen solcher Ensembles ist ein √ºblicher Trick sowohl f√ºr die Nationalversammlung als auch f√ºr andere MO-Techniken.  Und es gibt tats√§chlich eine Verbesserung der Effizienz: Wir erhalten eine Genauigkeit von 99,67%.  Mit anderen Worten, unser Netzwerkensemble klassifiziert alle 10.000 Verifizierungsbilder mit Ausnahme von 33 korrekt. <br><br>  Die verbleibenden Fehler sind unten aufgef√ºhrt.  Das Etikett in der oberen rechten Ecke ist die korrekte Klassifizierung gem√§√ü MNIST-Daten, und in der unteren rechten Ecke ist das Etikett, das vom Netzwerkensemble empfangen wurde: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6e/2d7/69a/b6e2d769a802b1ae5f249932789f2dff.png"><br><br>  Es lohnt sich, sich mit den Bildern zu befassen.  Die ersten beiden Ziffern 6 und 5 sind die wirklichen Fehler unseres Ensembles.  Sie k√∂nnen jedoch verstanden werden, ein solcher Fehler k√∂nnte vom Menschen gemacht werden.  Diese 6 ist 0 sehr √§hnlich, und 5 ist 3 sehr √§hnlich. Das dritte Bild, angeblich 8, sieht wirklich eher wie 9 aus. Ich stehe auf der Seite des Netzwerkensembles: Ich denke, dass er die Arbeit besser gemacht hat als die Person, die diese Figur geschrieben hat.  Andererseits wird das vierte Bild, 6, von Netzwerken wirklich falsch klassifiziert. <br><br>  Usw.  In den meisten F√§llen erscheint die Netzwerkl√∂sung plausibel, und in einigen F√§llen wurde die Zahl besser klassifiziert als von der Person, die sie geschrieben hat.  Insgesamt weisen unsere Netzwerke eine au√üergew√∂hnliche Effizienz auf, insbesondere wenn wir uns daran erinnern, dass sie 9967 Bilder korrekt klassifiziert haben, die wir hier nicht pr√§sentieren.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Zusammenhang k√∂nnen mehrere offensichtliche Fehler verstanden werden. </font><font style="vertical-align: inherit;">Sogar eine vorsichtige Person irrt sich manchmal. </font><font style="vertical-align: inherit;">Daher kann ich nur von einer √§u√üerst genauen und methodischen Person ein besseres Ergebnis erwarten. </font><font style="vertical-align: inherit;">Unser Netzwerk n√§hert sich der menschlichen Leistung.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Warum haben wir die Ausnahme nur auf vollst√§ndig verbundene Ebenen angewendet? </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie sich den obigen Code genau ansehen, werden Sie feststellen, dass wir die Ausnahme nur auf vollst√§ndig verbundene Netzwerkschichten angewendet haben, nicht jedoch auf Faltungsschichten. </font><font style="vertical-align: inherit;">Im Prinzip kann ein √§hnliches Verfahren auf Faltungsschichten angewendet werden. </font><font style="vertical-align: inherit;">Dies ist jedoch nicht erforderlich: Faltungsschichten weisen einen erheblichen Widerstand gegen Umschulungen auf. </font><font style="vertical-align: inherit;">Dies liegt daran, dass die Faltungsfilter durch die Gesamtgewichte gleichzeitig √ºber das gesamte Bild lernen. </font><font style="vertical-align: inherit;">Infolgedessen ist es weniger wahrscheinlich, dass sie √ºber einige lokale Verzerrungen in den Trainingsdaten stolpern. </font><font style="vertical-align: inherit;">Daher besteht keine besondere Notwendigkeit, andere Regularisierer auf sie anzuwenden, wie z. B. Ausnahmen.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Weitermachen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen die Effizienz der L√∂sung des MNIST-Problems noch weiter verbessern. Rodrigo Benenson stellte eine </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">informative Tafel zusammen,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die den Fortschritt im Laufe der Jahre und Links zur Arbeit zeigt. Viele der Werke verwenden GSS √§hnlich wie wir. Wenn Sie in Ihrer Arbeit st√∂bern, werden Sie viele interessante Techniken finden, und Sie k√∂nnen einige davon implementieren. In diesem Fall ist es ratsam, die Implementierung mit einem einfachen Netzwerk zu beginnen, das schnell trainiert werden kann. Auf diese Weise k√∂nnen Sie schnell verstehen, was passiert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zum gr√∂√üten Teil werde ich nicht versuchen, die j√ºngsten Arbeiten zu √ºberpr√ºfen. Aber ich kann einer Ausnahme nicht widerstehen. Es geht um eine </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeit im Jahr 2010</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Ich mag ihre Einfachheit in ihr. Das Netzwerk ist mehrschichtig und verwendet nur vollst√§ndig verbundene Schichten (ohne Windungen). In ihrem erfolgreichsten Netzwerk gibt es versteckte Schichten mit 2500, 2000, 1500, 1000 bzw. 500 Neuronen. Sie verwendeten √§hnliche Ideen, um Trainingsdaten zu erweitern. Abgesehen davon wendeten sie einige weitere Tricks an, einschlie√ülich des Fehlens von Faltungsschichten: Es war das einfachste Vanille-Netzwerk, das mit der richtigen Geduld und der Verf√ºgbarkeit geeigneter Computerf√§higkeiten bereits in den 1980er Jahren h√§tte unterrichtet werden k√∂nnen (wenn das MNIST-Set damals existiert h√§tte). Sie erreichten eine Klassifizierungsgenauigkeit von 99,65%, was in etwa unserer entspricht. Die Hauptsache in ihrer Arbeit ist die Verwendung eines sehr gro√üen und tiefen Netzwerks und die Verwendung von GPUs zur Beschleunigung des Lernens. Dadurch konnten sie viele Epochen lernen. Sie nutzten auch die langen Trainingsintervalle,und schrittweise die Lerngeschwindigkeit von 10 reduziert</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-3</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bis 10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">-6</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Der Versuch, mit einer Architektur wie der ihren √§hnliche Ergebnisse zu erzielen, ist eine interessante √úbung.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Warum lernen wir? </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im vorigen Kapitel haben wir grundlegende Hindernisse f√ºr das Erlernen von Deep Multilayer NS gesehen. </font><font style="vertical-align: inherit;">Insbesondere haben wir gesehen, dass der Gradient sehr instabil wird: Beim √úbergang von der Ausgangsschicht zur vorherigen neigt der Gradient dazu, entweder zu verschwinden (das Problem des verschwindenden Gradienten) oder explosives Wachstum (das Problem des explosiven Gradientenwachstums). </font><font style="vertical-align: inherit;">Da der Gradient das Signal ist, das wir f√ºr das Training verwenden, verursacht dies Probleme. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie haben wir es geschafft, sie zu vermeiden?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Antwort lautet nat√ºrlich: Wir konnten sie nicht vermeiden. Stattdessen haben wir einige Dinge getan, die es uns trotzdem erm√∂glichten, weiterzuarbeiten. Insbesondere: (1) die Verwendung von Faltungsschichten reduziert die Anzahl der darin enthaltenen Parameter erheblich, was das Lernproblem erheblich erleichtert; (2) die Verwendung effizienterer Regularisierungstechniken (Ausschluss- und Faltungsschichten); (3) Verwendung von ReLU anstelle von Sigmoidneuronen zur Beschleunigung des Lernens - empirisch bis zu 3-5 mal; (4) die Verwendung der GPU und die F√§higkeit, im Laufe der Zeit zu lernen. Insbesondere haben wir in j√ºngsten Experimenten 40 Epochen mit einem Datensatz untersucht, der f√ºnfmal gr√∂√üer ist als die Standard-MNIST-Trainingsdaten. Zu Beginn des Buches haben wir haupts√§chlich 30 Epochen mit Standard-Trainingsdaten untersucht. Die Kombination der Faktoren (3) und (4) ergibt einen solchen Effekt,als ob wir 30 mal l√§nger als zuvor studiert h√§tten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie sagen wahrscheinlich: "Ist das alles?" Ist das alles, was man braucht, um tiefe neuronale Netze zu trainieren? Und wegen was hat dann die Aufregung Feuer gefangen? ‚Äú </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich haben wir andere Ideen verwendet: ausreichend gro√üe Datens√§tze (um Umschulungen zu vermeiden); korrekte Kostenfunktion (um Lernverlangsamungen zu vermeiden); gute Initialisierung der Gewichte (auch um eine Verlangsamung des Lernens aufgrund der S√§ttigung der Neuronen zu vermeiden); algorithmische Erweiterung des Trainingsdatensatzes. Wir haben diese und andere Ideen in fr√ºheren Kapiteln besprochen und hatten normalerweise die M√∂glichkeit, sie mit kleinen Notizen in diesem Kapitel wiederzuverwenden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nach allen Angaben ist dies ein ziemlich einfacher Satz von Ideen. Einfach, aber in der Lage, viel zu tun, wenn es in einem Komplex verwendet wird. Es stellte sich heraus, dass der Einstieg in das tiefe Lernen ziemlich einfach war!</font></font><br><br><h3>       ? </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir Faltungs- / Pooling-Schichten als eine betrachten, gibt es in unserer endg√ºltigen Architektur 4 versteckte Schichten. Hat ein solches Netzwerk einen tiefen Titel verdient? Nat√ºrlich sind 4 versteckte Schichten viel mehr als in flachen Netzwerken, die wir zuvor untersucht haben. Die meisten Netzwerke hatten eine verborgene Schicht, manchmal 2. Auf der anderen Seite haben moderne fortgeschrittene Netzwerke manchmal Dutzende von verborgenen Schichten. Manchmal habe ich Leute getroffen, die dachten, je tiefer das Netzwerk, desto besser. Wenn Sie nicht gen√ºgend versteckte Ebenen verwenden, bedeutet dies, dass Sie nicht wirklich tief lernen. Ich denke nicht, insbesondere weil ein solcher Ansatz die Definition von Deep Learning in ein Verfahren verwandelt, das von momentanen Ergebnissen abh√§ngt. Ein echter Durchbruch in diesem Bereich war die Idee der Praktikabilit√§t, √ºber Netzwerke mit einer oder zwei verborgenen Schichten hinauszugehen.Mitte der 2000er Jahre vorherrschend. Dies war ein echter Durchbruch und er√∂ffnete ein Forschungsfeld mit ausdrucksst√§rkeren Modellen. Nun, eine bestimmte Anzahl von Schichten ist nicht von grundlegendem Interesse. Die Verwendung tiefer Netzwerke ist ein Werkzeug, um andere Ziele zu erreichen, beispielsweise die Verbesserung der Klassifizierungsgenauigkeit.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Verfahrensfrage </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Abschnitt haben wir reibungslos von flachen Netzwerken mit einer verborgenen Schicht zu mehrschichtigen Faltungsnetzwerken gewechselt. </font><font style="vertical-align: inherit;">Alles schien so einfach! </font><font style="vertical-align: inherit;">Wir haben eine √Ñnderung vorgenommen und eine Verbesserung erhalten. </font><font style="vertical-align: inherit;">Wenn Sie anfangen zu experimentieren, garantiere ich, dass normalerweise nicht alles so reibungslos verl√§uft. </font><font style="vertical-align: inherit;">Ich habe Ihnen eine gek√§mmte Geschichte vorgestellt, in der viele Experimente weggelassen wurden, auch erfolglose. </font><font style="vertical-align: inherit;">Ich hoffe, dass diese gek√§mmte Geschichte Ihnen hilft, die Grundideen besser zu verstehen. </font><font style="vertical-align: inherit;">Er riskiert jedoch, einen unvollst√§ndigen Eindruck zu vermitteln. </font><font style="vertical-align: inherit;">Um ein gutes, funktionierendes Netzwerk zu erhalten, ist viel Versuch und Irrtum erforderlich, durchsetzt mit Frustration. </font><font style="vertical-align: inherit;">In der Praxis k√∂nnen Sie mit einer Vielzahl von Experimenten rechnen. </font><font style="vertical-align: inherit;">Um den Prozess zu beschleunigen, k√∂nnen Ihnen die Informationen in Kapitel 3 zur Auswahl der Netzwerkhyperparameter sowie die dort erw√§hnte zus√§tzliche Literatur helfen.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Code f√ºr unsere Faltungsnetzwerke </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Okay, schauen wir uns jetzt den Code f√ºr unser Programm network3.py an. Strukturell √§hnelt es network2.py, das wir in Kapitel 3 entwickelt haben, aber die Details unterscheiden sich aufgrund der Verwendung der Theano-Bibliothek. Beginnen wir mit der FullyConnectedLayer-Klasse, √§hnlich den zuvor untersuchten Ebenen.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">FullyConnectedLayer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, n_in, n_out, activation_fn=sigmoid, p_dropout=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.n_in = n_in self.n_out = n_out self.activation_fn = activation_fn self.p_dropout = p_dropout <span class="hljs-comment"><span class="hljs-comment"># Initialize weights and biases self.w = theano.shared( np.asarray( np.random.normal( loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)), dtype=theano.config.floatX), name='w', borrow=True) self.b = theano.shared( np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)), dtype=theano.config.floatX), name='b', borrow=True) self.params = [self.w, self.b] def set_inpt(self, inpt, inpt_dropout, mini_batch_size): self.inpt = inpt.reshape((mini_batch_size, self.n_in)) self.output = self.activation_fn( (1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b) self.y_out = T.argmax(self.output, axis=1) self.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout) self.output_dropout = self.activation_fn( T.dot(self.inpt_dropout, self.w) + self.b) def accuracy(self, y): "Return the accuracy for the mini-batch." return T.mean(T.eq(y, self.y_out))</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Gro√üteil der __init__ -Methode spricht f√ºr sich selbst, aber ein paar Hinweise k√∂nnen helfen, den Code zu verdeutlichen. Wie √ºblich initialisieren wir Gewichte und Offsets zuf√§llig mit normalen Zufallswerten mit geeigneten Standardabweichungen. Diese Zeilen sehen etwas unverst√§ndlich aus. Der gr√∂√üte Teil des seltsamen Codes l√§dt jedoch Gewichte und Offsets in das, was die Theano-Bibliothek als gemeinsam genutzte Variablen bezeichnet. Dadurch wird sichergestellt, dass Variablen auf der GPU verarbeitet werden k√∂nnen, sofern verf√ºgbar. Wir werden uns nicht mit diesem Thema befassen - wenn Sie interessiert sind, lesen Sie die Dokumentation f√ºr Theano. Beachten Sie auch, dass diese Initialisierung von Gewichten und Offsets f√ºr die Sigmoid-Aktivierungsfunktion gilt. Idealerweise w√ºrden wir f√ºr Funktionen wie hyperbolische Tangente und ReLU Gewichte und Offsets unterschiedlich initialisieren. Dieses Problem wird in zuk√ºnftigen Aufgaben behandelt.Die Methode __init__ endet mit der Anweisung self.params = [self.w, self.b]. Dies ist eine bequeme M√∂glichkeit, alle mit einer Ebene verbundenen Lernparameter zusammenzuf√ºhren. Network.SGD verwendet sp√§ter die params-Attribute, um herauszufinden, welche Variablen in der Network-Klasseninstanz trainiert werden k√∂nnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die set_inpt-Methode wird verwendet, um Eingaben an eine Ebene zu √ºbergeben und die entsprechende Ausgabe zu berechnen. Ich schreibe inpt anstelle von Eingabe, da Eingabe eine integrierte Python-Funktion ist. Wenn Sie mit ihnen spielen, kann dies zu unvorhersehbarem Programmverhalten und schwer zu diagnostizierenden Fehlern f√ºhren. Tats√§chlich √ºbergeben wir Eingaben auf zwei Arten: √ºber self.inpt und self.inpt_dropout. Dies geschieht, da wir w√§hrend des Trainings m√∂glicherweise Ausnahmen verwenden m√∂chten. Und dann m√ºssen wir einen Teil der self.p_dropout-Neuronen entfernen. Dies ist, was die Funktion dropout_layer in der vorletzten Zeile der Methode set_inpt tut. Self.inpt_dropout und self.output_dropout werden also w√§hrend des Trainings verwendet, und self.inpt und self.output werden f√ºr alle anderen Zwecke verwendet, z. B. zur Bewertung der Genauigkeit von Validierungs- und Testdaten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Klassendefinitionen f√ºr ConvPoolLayer und SoftmaxLayer √§hneln FullyConnectedLayer. So √§hnlich, dass ich den Code nicht einmal zitiere. Wenn Sie interessiert sind, k√∂nnen Sie den vollst√§ndigen Code des Programms sp√§ter in diesem Kapitel lesen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist erw√§hnenswert, einige verschiedene Details zu erw√§hnen. In ConvPoolLayer und SoftmaxLayer berechnen wir die Ausgabeaktivierungen nat√ºrlich so, dass sie dem Layertyp entsprechen. Gl√ºcklicherweise ist Theano einfach zu handhaben und verf√ºgt √ºber integrierte Operationen zur Berechnung der Faltung, des Max-Pooling und der Softmax-Funktion.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist weniger offensichtlich, wie Gewichte und Offsets in der Softmax-Ebene initialisiert werden - wir haben dies nicht diskutiert. Wir haben erw√§hnt, dass es f√ºr sigmoidale Gewichtsschichten notwendig ist, entsprechend parametrisierte normale Zufallsverteilungen zu initialisieren. Dieses heuristische Argument galt jedoch f√ºr Sigmoid-Neuronen (und mit geringf√ºgigen Korrekturen f√ºr Tang-Neuronen). Es gibt jedoch keinen besonderen Grund f√ºr dieses Argument, auf Softmax-Ebenen anzuwenden. Daher gibt es keinen Grund, diese Initialisierung von vornherein erneut anzuwenden. Stattdessen initialisiere ich alle Gewichte und Offsets auf 0. Die Option ist spontan, funktioniert aber in der Praxis ziemlich gut. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben also alle Schichtenklassen untersucht. Was ist mit der Netzwerkklasse? Beginnen wir mit der __init__ -Methode:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, layers, mini_batch_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   layers,   ,   mini_batch_size          """</span></span> self.layers = layers self.mini_batch_size = mini_batch_size self.params = [param <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> layer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.layers <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> param <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer.params] self.x = T.matrix(<span class="hljs-string"><span class="hljs-string">"x"</span></span>) self.y = T.ivector(<span class="hljs-string"><span class="hljs-string">"y"</span></span>) init_layer = self.layers[<span class="hljs-number"><span class="hljs-number">0</span></span>] init_layer.set_inpt(self.x, self.x, self.mini_batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(self.layers)): prev_layer, layer = self.layers[j<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.layers[j] layer.set_inpt( prev_layer.output, prev_layer.output_dropout, self.mini_batch_size) self.output = self.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].output self.output_dropout = self.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].output_dropout</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der gr√∂√üte Teil des Codes spricht f√ºr sich. Die Zeile self.params = [Parameter f√ºr Ebene in ...] sammelt alle Parameter f√ºr jede Ebene in einer einzigen Liste. Wie bereits vorgeschlagen, verwendet die Network.SGD-Methode self.params, um herauszufinden, aus welchen Parametern das Netzwerk lernen kann. Die Linien self.x = T.matrix ("x") und self.y = T.ivector ("y") definieren die symbolischen Variablen Theano x und y. Sie repr√§sentieren die Eingabe und die gew√ºnschte Ausgabe des Netzwerks. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist kein Tutorial zur Verwendung von Theano, daher werde ich nicht auf die Bedeutung symbolischer Variablen eingehen (siehe </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dokumentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und auch eines der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tutorials)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) Grob gesagt bezeichnen sie mathematische Variablen, keine spezifischen. Mit ihnen k√∂nnen Sie viele normale Operationen ausf√ºhren: Addieren, Subtrahieren, Multiplizieren, Anwenden von Funktionen usw. Theano bietet viele M√∂glichkeiten, solche symbolischen Variablen zu manipulieren, zu falten, maximal zu ziehen und so weiter. Die Hauptsache ist jedoch die M√∂glichkeit einer schnellen symbolischen Differenzierung unter Verwendung einer sehr allgemeinen Form des Backpropagation-Algorithmus. Dies ist √§u√üerst n√ºtzlich, um einen stochastischen Gradientenabstieg auf eine Vielzahl von Netzwerkarchitekturen anzuwenden. Insbesondere definieren die folgenden Codezeilen die symbolische Ausgabe des Netzwerks. Wir beginnen mit der Zuordnung der Eingabe zur ersten Ebene:</font></font><br><br><pre> <code class="python hljs"> init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Eingabedaten werden jeweils einzeln √ºbertragen, daher wird dort ihre Gr√∂√üe angegeben. Wir √ºbergeben die Eingabe von self.x zweimal: Tatsache ist, dass wir das Netzwerk auf zwei verschiedene Arten verwenden k√∂nnen (mit oder ohne Ausnahme). Die for-Schleife verbreitet die symbolische Variable self.x durch die Netzwerkschichten. Auf diese Weise k√∂nnen wir die endg√ºltigen Attribute output und output_dropout definieren, die symbolisch die Ausgabe des Netzwerks darstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nachdem wir uns mit der Initialisierung des Netzwerks befasst haben, schauen wir uns dessen Training mit der SGD-Methode an. Der Code sieht lang aus, ist aber recht einfach aufgebaut. Erkl√§rungen folgen dem Code:</font></font><br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SGD</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    -    ."""</span></span> training_x, training_y = training_data validation_x, validation_y = validation_data test_x, test_y = test_data <span class="hljs-comment"><span class="hljs-comment">#   -  ,    num_training_batches = size(training_data)/mini_batch_size num_validation_batches = size(validation_data)/mini_batch_size num_test_batches = size(test_data)/mini_batch_size #    ,     l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers]) cost = self.layers[-1].cost(self)+\ 0.5*lmbda*l2_norm_squared/num_training_batches grads = T.grad(cost, self.params) updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)] #     -    #      -. i = T.lscalar() # mini-batch index train_mb = theano.function( [i], cost, updates=updates, givens={ self.x: training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) validate_mb_accuracy = theano.function( [i], self.layers[-1].accuracy(self.y), givens={ self.x: validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) test_mb_accuracy = theano.function( [i], self.layers[-1].accuracy(self.y), givens={ self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) self.test_mb_predictions = theano.function( [i], self.layers[-1].y_out, givens={ self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) #    best_validation_accuracy = 0.0 for epoch in xrange(epochs): for minibatch_index in xrange(num_training_batches): iteration = num_training_batches*epoch+minibatch_index if iteration print("Training mini-batch number {0}".format(iteration)) cost_ij = train_mb(minibatch_index) if (iteration+1) validation_accuracy = np.mean( [validate_mb_accuracy(j) for j in xrange(num_validation_batches)]) print("Epoch {0}: validation accuracy {1:.2 epoch, validation_accuracy)) if validation_accuracy &gt;= best_validation_accuracy: print("This is the best validation accuracy to date.") best_validation_accuracy = validation_accuracy best_iteration = iteration if test_data: test_accuracy = np.mean( [test_mb_accuracy(j) for j in xrange(num_test_batches)]) print('The corresponding test accuracy is {0:.2 test_accuracy)) print("Finished training network.") print("Best validation accuracy of {0:.2 best_validation_accuracy, best_iteration)) print("Corresponding test accuracy of {0:.2</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die ersten Zeilen sind klar, sie unterteilen die Datens√§tze in die Komponenten x und y und berechnen die Anzahl der in jedem Datensatz verwendeten Minipakete. Die folgenden Zeilen sind interessanter und zeigen, warum es so interessant ist, mit der Theano-Bibliothek zu arbeiten. Ich werde sie hier zitieren:</font></font><br><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#    ,     l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers]) cost = self.layers[-1].cost(self)+\ 0.5*lmbda*l2_norm_squared/num_training_batches grads = T.grad(cost, self.params) updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)]</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesen Zeilen definieren wir symbolisch die regulierte Kostenfunktion basierend auf der logarithmischen Wahrscheinlichkeitsfunktion, berechnen die entsprechenden Ableitungen in der Gradientenfunktion und auch die entsprechenden Parameteraktualisierungen. Mit Theano k√∂nnen wir dies alles in wenigen Zeilen erledigen. Das einzige, was verborgen bleibt, ist, dass bei der Berechnung der Kosten die Kostenmethode f√ºr die Ausgabeschicht aufgerufen wird. Dieser Code befindet sich an anderer Stelle in network3.py. Aber es ist kurz und einfach. Mit der Definition all dessen ist alles bereit, die Funktion train_mb zu definieren, die symbolische Theano-Funktion, die Aktualisierungen verwendet, um Netzwerkparameter durch Minipaketindex zu aktualisieren. In √§hnlicher Weise berechnen die Funktionen validate_mb_accuracy und test_mb_accuracy die Netzwerkgenauigkeit f√ºr ein bestimmtes Minipaket von Validierungs- oder Verifizierungsdaten. Mittelung √ºber diese Funktionen,Wir k√∂nnen die Genauigkeit f√ºr die gesamten Validierungs- und Verifizierungsdatens√§tze berechnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der Rest der SGD-Methode spricht f√ºr sich selbst - wir gehen einfach nacheinander durch die Epochen, trainieren das Netzwerk immer wieder anhand von Minipaketen mit Trainingsdaten und berechnen die Genauigkeit der Best√§tigung und √úberpr√ºfung. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jetzt verstehen wir die wichtigsten Teile des Jahres network3.py. Lassen Sie uns kurz das gesamte Programm durchgehen. Es ist nicht notwendig, alles im Detail zu studieren, aber Sie m√∂chten vielleicht √ºber die Ma√üen gehen und sich vielleicht mit einigen besonders beliebten Passagen befassen. Aber der beste Weg, das Programm zu verstehen, besteht nat√ºrlich darin, es zu √§ndern, etwas Neues hinzuzuf√ºgen und die Teile zu √ºberarbeiten, die Ihrer Meinung nach verbessert werden k√∂nnen. Nach dem Code stelle ich einige Aufgaben vor, die eine Reihe von ersten Vorschl√§gen enthalten, was hier getan werden kann. Hier ist der Code.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network3.py ~~~~~~~~~~~~~~     Theano      .     (, , -, softmax)    (,  , ReLU;   ).    CPU     ,  network.py  network2.py. ,    ,      GPU,    .     Theano,       network.py  network2.py.  ,       .  , API   network2.py.       ,  ,     .   ,     ,    .      Theano   (http://deeplearning.net/tutorial/lenet.html ),       (https://github.com/mdenil/dropout )      (http://colah.github.io ).   Theano 0.6  0.7,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import cPickle import gzip #  import numpy as np import theano import theano.tensor as T from theano.tensor.nnet import conv from theano.tensor.nnet import softmax from theano.tensor import shared_randomstreams from theano.tensor.signal import downsample #    def linear(z): return z def ReLU(z): return T.maximum(0.0, z) from theano.tensor.nnet import sigmoid from theano.tensor import tanh ####  GPU = True if GPU: print "Trying to run under a GPU. If this is not desired, then modify "+\ "network3.py\nto set the GPU flag to False." try: theano.config.device = 'gpu' except: pass # it's already set theano.config.floatX = 'float32' else: print "Running with a CPU. If this is not desired, then the modify "+\ "network3.py to set\nthe GPU flag to True." ####   MNIST def load_data_shared(filename="../data/mnist.pkl.gz"): f = gzip.open(filename, 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() def shared(data): """    .   Theano    GPU,   . """ shared_x = theano.shared( np.asarray(data[0], dtype=theano.config.floatX), borrow=True) shared_y = theano.shared( np.asarray(data[1], dtype=theano.config.floatX), borrow=True) return shared_x, T.cast(shared_y, "int32") return [shared(training_data), shared(validation_data), shared(test_data)] ####        class Network(object): def __init__(self, layers, mini_batch_size): """   layers,   ,   mini_batch_size         . """ self.layers = layers self.mini_batch_size = mini_batch_size self.params = [param for layer in self.layers for param in layer.params] self.x = T.matrix("x") self.y = T.ivector("y") init_layer = self.layers[0] init_layer.set_inpt(self.x, self.x, self.mini_batch_size) for j in xrange(1, len(self.layers)): prev_layer, layer = self.layers[j-1], self.layers[j] layer.set_inpt( prev_layer.output, prev_layer.output_dropout, self.mini_batch_size) self.output = self.layers[-1].output self.output_dropout = self.layers[-1].output_dropout def SGD(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda=0.0): """    -    .""" training_x, training_y = training_data validation_x, validation_y = validation_data test_x, test_y = test_data #   -  ,    num_training_batches = size(training_data)/mini_batch_size num_validation_batches = size(validation_data)/mini_batch_size num_test_batches = size(test_data)/mini_batch_size #    ,     l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers]) cost = self.layers[-1].cost(self)+\ 0.5*lmbda*l2_norm_squared/num_training_batches grads = T.grad(cost, self.params) updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)] #     -    #      -. i = T.lscalar() # mini-batch index train_mb = theano.function( [i], cost, updates=updates, givens={ self.x: training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) validate_mb_accuracy = theano.function( [i], self.layers[-1].accuracy(self.y), givens={ self.x: validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) test_mb_accuracy = theano.function( [i], self.layers[-1].accuracy(self.y), givens={ self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size], self.y: test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) self.test_mb_predictions = theano.function( [i], self.layers[-1].y_out, givens={ self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size] }) #    best_validation_accuracy = 0.0 for epoch in xrange(epochs): for minibatch_index in xrange(num_training_batches): iteration = num_training_batches*epoch+minibatch_index if iteration % 1000 == 0: print("Training mini-batch number {0}".format(iteration)) cost_ij = train_mb(minibatch_index) if (iteration+1) % num_training_batches == 0: validation_accuracy = np.mean( [validate_mb_accuracy(j) for j in xrange(num_validation_batches)]) print("Epoch {0}: validation accuracy {1:.2%}".format( epoch, validation_accuracy)) if validation_accuracy &gt;= best_validation_accuracy: print("This is the best validation accuracy to date.") best_validation_accuracy = validation_accuracy best_iteration = iteration if test_data: test_accuracy = np.mean( [test_mb_accuracy(j) for j in xrange(num_test_batches)]) print('The corresponding test accuracy is {0:.2%}'.format( test_accuracy)) print("Finished training network.") print("Best validation accuracy of {0:.2%} obtained at iteration {1}".format( best_validation_accuracy, best_iteration)) print("Corresponding test accuracy of {0:.2%}".format(test_accuracy)) ####    class ConvPoolLayer(object): """     - .        ,         ,    ,   . """ def __init__(self, filter_shape, image_shape, poolsize=(2, 2), activation_fn=sigmoid): """`filter_shape` -   4,   ,    ,     . `image_shape` -   4,   -,    ,    . `poolsize` -   2,    y  x. """ self.filter_shape = filter_shape self.image_shape = image_shape self.poolsize = poolsize self.activation_fn=activation_fn # initialize weights and biases n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize)) self.w = theano.shared( np.asarray( np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape), dtype=theano.config.floatX), borrow=True) self.b = theano.shared( np.asarray( np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)), dtype=theano.config.floatX), borrow=True) self.params = [self.w, self.b] def set_inpt(self, inpt, inpt_dropout, mini_batch_size): self.inpt = inpt.reshape(self.image_shape) conv_out = conv.conv2d( input=self.inpt, filters=self.w, filter_shape=self.filter_shape, image_shape=self.image_shape) pooled_out = downsample.max_pool_2d( input=conv_out, ds=self.poolsize, ignore_border=True) self.output = self.activation_fn( pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')) self.output_dropout = self.output # no dropout in the convolutional layers class FullyConnectedLayer(object): def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0): self.n_in = n_in self.n_out = n_out self.activation_fn = activation_fn self.p_dropout = p_dropout # Initialize weights and biases self.w = theano.shared( np.asarray( np.random.normal( loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)), dtype=theano.config.floatX), name='w', borrow=True) self.b = theano.shared( np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)), dtype=theano.config.floatX), name='b', borrow=True) self.params = [self.w, self.b] def set_inpt(self, inpt, inpt_dropout, mini_batch_size): self.inpt = inpt.reshape((mini_batch_size, self.n_in)) self.output = self.activation_fn( (1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b) self.y_out = T.argmax(self.output, axis=1) self.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout) self.output_dropout = self.activation_fn( T.dot(self.inpt_dropout, self.w) + self.b) def accuracy(self, y): "Return the accuracy for the mini-batch." return T.mean(T.eq(y, self.y_out)) class SoftmaxLayer(object): def __init__(self, n_in, n_out, p_dropout=0.0): self.n_in = n_in self.n_out = n_out self.p_dropout = p_dropout #     self.w = theano.shared( np.zeros((n_in, n_out), dtype=theano.config.floatX), name='w', borrow=True) self.b = theano.shared( np.zeros((n_out,), dtype=theano.config.floatX), name='b', borrow=True) self.params = [self.w, self.b] def set_inpt(self, inpt, inpt_dropout, mini_batch_size): self.inpt = inpt.reshape((mini_batch_size, self.n_in)) self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b) self.y_out = T.argmax(self.output, axis=1) self.inpt_dropout = dropout_layer( inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout) self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b) def cost(self, net): "   ." return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y]) def accuracy(self, y): "  -." return T.mean(T.eq(y, self.y_out)) ####  def size(data): "    `data`." return data[0].get_value(borrow=True).shape[0] def dropout_layer(layer, p_dropout): srng = shared_randomstreams.RandomStreams( np.random.RandomState(0).randint(999999)) mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape) return layer*T.cast(mask, theano.config.floatX)</span></span></code> </pre> <br><h3>  Die Aufgaben </h3><br><ul><li>     SGD       .            ,  .  network3.py ,      . </li><li>   Network ,       . </li><li>  SGD ,       Œ∑      (   , , ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ). </li><li>              ,   .  network3.py,      . ,         ,      .    . </li><li>      . </li><li>    ‚Äì     .    ,    ,  ,   ?  . </li><li>    ReLU    ,     ( -) .       .  ,    ReLU ( ). ,        c&gt;0     c <sup>L‚àí1</sup> ,  L ‚Äì  .  ,     softmax?         ReLU?       ? ,    ,       .        ,   ReLU. </li><li>         .     ,      ReLU?         ,        ? :  ¬´¬ª   .        ‚Äì       ,   - -  . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de463171/">https://habr.com/ru/post/de463171/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de463157/index.html">ESP32-CAM-Video-Streaming-Server zum Verbinden von I2C- und SPI-Displays</a></li>
<li><a href="../de463159/index.html">√úber Sicherheit, Zahlen, E-Mails und einiges √ºber Werbung</a></li>
<li><a href="../de463165/index.html">Telegramm schl√§gt DPI zur√ºck und sperrt - Fake TLS</a></li>
<li><a href="../de463167/index.html">Notwendige Materialien, um mit der Entwicklung eines VR-Schulungsprojekts zu beginnen</a></li>
<li><a href="../de463169/index.html">Open Source H√∂rger√§t - wie es funktioniert</a></li>
<li><a href="../de463175/index.html">Visualisierung von Abh√§ngigkeiten und Vererbung zwischen maschinellen Lernmodellen</a></li>
<li><a href="../de463177/index.html">Service Desk zu Hause Guthaben. Und was ist drin? ...</a></li>
<li><a href="../de463179/index.html">Big Data Big Billing: √úber BigData in der Telekommunikation</a></li>
<li><a href="../de463181/index.html">Figma - eine einfache L√∂sung f√ºr einen Designer, eine schwierige L√∂sung f√ºr einen Layoutdesigner</a></li>
<li><a href="../de463183/index.html">Schulung Cisco 200-125 CCNA v3.0. Tag 13. Konfigurieren Sie das VLAN</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>