<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⏩ 🚡 🈶 Livy - tautan yang hilang dalam rantai Python Hadoop Spark Airflow ♂️ 👋 👜</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hai semuanya, beberapa informasi "dari bawah tenda" adalah tanggal lokakarya teknik Alfastrakhovaniya - yang menggairahkan pikiran teknis kami. 





...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livy - tautan yang hilang dalam rantai Python Hadoop Spark Airflow</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/alfastrah/blog/466017/"><p>  Hai semuanya, beberapa informasi "dari bawah tenda" adalah tanggal lokakarya teknik Alfastrakhovaniya - yang menggairahkan pikiran teknis kami. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/956/f04/a6a/956f04a6ae545bee58a8c34f2938a850.png" alt="gambar"></p><br><p>  Apache Spark adalah alat luar biasa yang memungkinkan Anda dengan cepat dan mudah memproses data dalam jumlah besar pada sumber daya komputasi yang cukup sederhana (maksud saya pemrosesan kluster). </p><br><p>  Secara tradisional, notebook jupyter digunakan dalam pemrosesan data ad hoc.  Dalam kombinasi dengan Spark, ini memungkinkan kita untuk memanipulasi frame data berumur panjang (Spark berurusan dengan alokasi sumber daya, frame data tinggal di suatu tempat di cluster, masa hidup mereka dibatasi oleh masa hidup konteks Spark). </p><br><p>  Setelah mentransfer pemrosesan data ke Apache Airflow, masa pakai frame sangat berkurang - konteks Spark "hidup" dalam pernyataan Airflow yang sama.  Bagaimana menyiasati ini, mengapa berkeliling dan apa yang harus dilakukan Livy dengan itu - baca di bawah potongan. </p><a name="habracut"></a><br><p>  Mari kita lihat contoh yang sangat, sangat sederhana: misalkan kita perlu mendenormalisasi data dalam tabel besar dan menyimpan hasilnya dalam tabel lain untuk diproses lebih lanjut (elemen khas dari pipa pemrosesan data). </p><br><p>  Bagaimana kita melakukan ini: </p><br><ul><li>  memuat data ke dalam kerangka data (pemilihan dari tabel besar dan direktori) </li><li>  melihat dengan "mata" pada hasilnya (apakah itu bekerja dengan benar) </li><li>  dataframe disimpan ke tabel Hive (misalnya) </li></ul><br><p>  Berdasarkan hasil analisis, kita mungkin perlu memasukkan pada langkah kedua beberapa pemrosesan spesifik (penggantian kamus atau yang lain).  Dalam hal logika, kami memiliki tiga langkah </p><br><ul><li>  langkah 1: unduh </li><li>  langkah 2: pemrosesan </li><li>  langkah 3: simpan </li></ul><br><p>  Di jupyter notebook, ini adalah bagaimana kami melakukannya - kami dapat memproses data yang diunduh untuk waktu yang lama, memberikan kontrol sumber daya Spark. </p><br><p> Adalah logis untuk mengharapkan bahwa partisi seperti itu dapat ditransfer ke Airflow.  Artinya, memiliki grafik semacam ini </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/312/30b/d7e/31230bd7e4c3beb62f92aebb709e2010.png" alt="gambar"></p><br><p>  Sayangnya, ini tidak mungkin ketika menggunakan kombinasi Airflow + Spark: setiap pernyataan Airflow dieksekusi dalam interpreter python, oleh karena itu, di antara hal-hal lain, setiap pernyataan harus entah bagaimana "bertahan" hasil dari kegiatannya.  Dengan demikian, pemrosesan kami "dikompresi" dalam satu langkah - "mendenormalkan data". </p><br><p>  Bagaimana fleksibilitas jupyter notebook dapat dikembalikan ke Airflow?  Jelas bahwa contoh di atas adalah "tidak layak" (mungkin, sebaliknya, ternyata langkah pemrosesan yang bisa dimengerti baik).  Tapi tetap - bagaimana membuat pernyataan Airflow dieksekusi dalam konteks Spark yang sama pada ruang dataframe umum? </p><br><h2 id="privetstvuem-livy">  Selamat datang Livy </h2><br><p>  Produk ekosistem Hadoop lain datang untuk menyelamatkan - Apache Livy. </p><br><p>  Saya tidak akan mencoba menggambarkan di sini "binatang" macam apa itu.  Jika sangat singkat dan hitam dan putih - Livy memungkinkan Anda untuk "menyuntikkan" kode python ke program yang dijalankan oleh driver: </p><br><ul><li>  pertama kita buat sesi Livy </li><li>  setelah itu kita memiliki kemampuan untuk mengeksekusi kode python sewenang-wenang dalam sesi ini (sangat mirip dengan ideologi jupyter / ipython) </li></ul><br><p>  Dan untuk semua ini ada REST API. </p><br><p>  Kembali ke tugas sederhana kita: dengan Livy kita dapat menyimpan logika asli dari denasionalisasi kita </p><br><ul><li>  pada langkah pertama (pernyataan pertama dari grafik kami) kami akan memuat dan mengeksekusi kode pemuatan data dalam kerangka data </li><li>  pada langkah kedua (pernyataan kedua) - jalankan kode untuk pemrosesan tambahan yang diperlukan dari kerangka data ini </li><li>  pada langkah ketiga - kode untuk menyimpan dataframe ke tabel </li></ul><br><p>  Seperti apa aliran udara yang terlihat seperti ini: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b9b/255/bcd/b9b255bcd00525201a000ef1a3fbafa3.png" alt="gambar"></p><br><p>  (karena gambar tersebut adalah tangkapan layar yang sangat nyata, ditambahkan "realitas" tambahan - menciptakan konteks Spark menjadi operasi terpisah dengan nama yang aneh, "pemrosesan" data menghilang karena tidak diperlukan, dll.) </p><br><p>  Untuk meringkas, kita dapatkan </p><br><ul><li>  pernyataan aliran udara universal yang mengeksekusi kode python dalam sesi Livy </li><li>  kemampuan untuk "mengatur" kode python ke dalam grafik yang cukup kompleks (Aliran udara untuk itu) </li><li>  kemampuan untuk mengatasi optimasi tingkat yang lebih tinggi, misalnya, dalam urutan mana kita perlu melakukan transformasi sehingga Spark dapat menyimpan data umum dalam memori cluster selama </li></ul><br><p>  Sebuah pipa khusus untuk menyiapkan data untuk pemodelan berisi sekitar 25 pertanyaan lebih dari 10 tabel, jelas bahwa beberapa tabel digunakan lebih sering daripada yang lain (sangat "data umum") dan ada sesuatu untuk dioptimalkan. </p><br><h2 id="chto-dalshe">  Apa selanjutnya </h2><br><p>  Kemampuan teknis telah diuji, kami berpikir lebih jauh - bagaimana menerjemahkan transformasi kami ke dalam paradigma ini secara lebih teknologi.  Dan bagaimana cara mendekati optimasi yang disebutkan di atas.  Kami masih berada di awal bagian dari perjalanan kami - ketika ada sesuatu yang menarik, kami pasti akan membagikannya. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id466017/">https://habr.com/ru/post/id466017/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id465991/index.html">Pekerja Arsitektur Swift Bersih</a></li>
<li><a href="../id465993/index.html">Tidak perlu menghemat keamanan digital</a></li>
<li><a href="../id465995/index.html">LDC - Ekskursi</a></li>
<li><a href="../id466001/index.html">Feng Shui "Bergerak", atau kita tidur dengan benar (kopi, kecoak, dan intoleransi terhadap Habré)</a></li>
<li><a href="../id466015/index.html">Sedikit lagi tentang trigonometri dalam komputasi</a></li>
<li><a href="../id466019/index.html">ABBYY Mobile Web Capture: Foto dokumen berkualitas tinggi tepat di browser ponsel cerdas Anda</a></li>
<li><a href="../id466021/index.html">Bagaimana saya mengajar Yandex. Baik untuk berbicara tentang mainan seks</a></li>
<li><a href="../id466027/index.html">Buku "The Way of Python. Sabuk hitam untuk pengembangan, penskalaan, pengujian dan penyebaran ”</a></li>
<li><a href="../id466029/index.html">Cara mengubah komputer kuantum menjadi generator angka acak sempurna</a></li>
<li><a href="../id466031/index.html">Misi epik DeepMind untuk memecahkan masalah sains yang paling kompleks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>