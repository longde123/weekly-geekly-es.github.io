<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõãÔ∏è ü§¶ üöé Cr√©ation d'une infrastructure informatique tol√©rante aux pannes. Partie 2. Installation et configuration du cluster oVirt 4.3 üë©üèª‚Äçüé§ üö¢ üöå</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cet article est la suite du pr√©c√©dent - ¬´ Cr√©ation d'une infrastructure informatique tol√©rante aux pannes. Partie 1 - pr√©parer le d√©ploiement du clust...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cr√©ation d'une infrastructure informatique tol√©rante aux pannes. Partie 2. Installation et configuration du cluster oVirt 4.3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/485208/"><p>  Cet article est la suite du pr√©c√©dent - ¬´ <a href="https://habr.com/ru/company/lenvendo/blog/483980/">Cr√©ation d'une infrastructure informatique tol√©rante aux pannes.</a>  <a href="https://habr.com/ru/company/lenvendo/blog/483980/">Partie 1 - pr√©parer le d√©ploiement du cluster oVirt 4.3</a> . ¬ª </p><br><p>  Il examinera le processus d'installation et de configuration de base du cluster oVirt 4.3 pour l'h√©bergement de machines virtuelles hautement accessibles, en tenant compte du fait que toutes les √©tapes pr√©liminaires de pr√©paration de l'infrastructure sont d√©j√† termin√©es. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  Pr√©sentation </h2><br><p>  L'objectif principal de l'article n'est pas de donner des instructions pas √† pas du formulaire ¬´ <strong>Suivant</strong> -&gt; <strong>Oui</strong> -&gt; <strong>Terminer</strong> ¬ª, mais de montrer certaines fonctionnalit√©s lors de son installation et de sa configuration.  Le processus de d√©ploiement de votre cluster peut ne pas toujours co√Øncider avec celui qui y est d√©crit, en raison des particularit√©s de l'infrastructure et de l'environnement, mais les principes g√©n√©raux seront les m√™mes. </p><br><p>  D'un point de vue subjectif, <a href="https://www.ovirt.org/">oVirt 4.3</a> est similaire en fonctionnalit√©s √† VMware vSphere version 5.x, mais bien s√ªr avec ses propres fonctionnalit√©s de configuration et de fonctionnement. </p><br><p>  Pour ceux qui sont int√©ress√©s, toutes les diff√©rences entre RHEV (aka oVirt) et VMware vSphere peuvent √™tre trouv√©es sur Internet, par exemple <a href="https://www.redhat.com/cms/managed-files/vi-virtualization-vmware-competitive-review-f10106jm-201803_0_0.pdf">ici</a> , mais je noterai occasionnellement certaines de leurs diff√©rences ou similitudes, comme le dit l'article. </p><br><p>  S√©par√©ment, je voudrais comparer un peu de travail avec les r√©seaux pour les machines virtuelles.  OVirt impl√©mente un principe similaire de gestion de r√©seau pour les machines virtuelles (ci-apr√®s d√©nomm√© VM), comme dans VMware vSphere: </p><br><ul><li>  en utilisant le pont Linux standard (dans VMware - <em>vSwitch standard</em> ) ex√©cut√© sur des h√¥tes de virtualisation; </li><li>  L'utilisation d'Open vSwitch (OVS) (dans VMware, <em>Distributed vSwitch</em> ) est un commutateur virtuel distribu√© qui se compose de deux composants principaux: un serveur OVN central et des contr√¥leurs OVN sur des h√¥tes g√©r√©s. </li></ul><br><p>  Il convient de noter qu'en raison de la simplicit√© de mise en ≈ìuvre, l'article d√©crira la configuration r√©seau dans oVirt pour les machines virtuelles √† l'aide du pont Linux standard, qui est le choix standard lors de l'utilisation de l'hyperviseur KVM. </p><br><p> √Ä cet √©gard, il existe plusieurs r√®gles de base pour travailler avec un r√©seau dans un cluster qu'il vaut mieux ne pas violer: </p><br><ul><li>  Tous les param√®tres r√©seau sur les h√¥tes doivent √™tre identiques avant de les ajouter √† oVirt, √† l'exception des adresses IP. </li><li>  Une fois que l'h√¥te est pris sous le contr√¥le de oVirt, il est fortement d√©conseill√© de modifier quelque chose dans les param√®tres r√©seau avec vos mains, sans une confiance totale dans vos actions, car l'agent oVirt les ram√®nera simplement aux pr√©c√©dents, apr√®s avoir red√©marr√© l'h√¥te ou l'agent. </li><li>  L'ajout d'un nouveau r√©seau pour la machine virtuelle, ainsi que l'utilisation de celui-ci, doivent √™tre effectu√©s uniquement √† partir de la console de gestion oVirt. </li></ul><br><p>  Autre <u>point important</u> - pour un environnement tr√®s critique (tr√®s sensible aux pertes d'argent), il serait toujours recommand√© d'utiliser un support payant et d'utiliser <a href="https://access.redhat.com/products/red-hat-virtualization">Red Hat Virtualization 4.3</a> .  Pendant le fonctionnement du cluster oVirt, il peut y avoir certains points sur lesquels il est conseill√© d'obtenir une aide qualifi√©e d√®s que possible, plut√¥t que de les traiter vous-m√™me. </p><br><p>  Et enfin, <u>il est recommand√©</u> qu'avant de d√©ployer le cluster oVirt, familiarisez-vous avec la <a href="https://www.ovirt.org/documentation/">documentation officielle</a> afin de conna√Ætre au moins les concepts et d√©finitions de base, sinon il sera un peu difficile de lire l'article plus loin. </p><br><p>  Les documents suivants sont fondamentaux pour comprendre l'article et les principes de fonctionnement du cluster oVirt: </p><br><ul><li>  <a href="https://www.ovirt.org/documentation/install-guide/Installation_Guide/">Guide d'installation oVirt</a> </li><li>  <a href="https://www.ovirt.org/documentation/self-hosted/Self-Hosted_Engine_Guide/">Guide du moteur auto-h√©berg√© oVirt</a> </li><li>  <a href="https://www.ovirt.org/documentation/admin-guide/administration-guide/">Guide d'administration oVirt</a> <a href="https://www.ovirt.org/documentation/admin-guide/administration-guide/"><br></a> </li></ul><br><p>  Le volume l√†-bas n'est pas tr√®s important, en une heure ou deux, il est tout √† fait possible de ma√Ætriser les principes de base, et pour les fans de d√©tails, il est recommand√© de lire la <a href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/">documentation produit de Red Hat Virtualization 4.3</a> - RHEV et oVirt sont essentiellement la m√™me chose. </p><br><p>  Donc, si tous les param√®tres de base sur les h√¥tes, les commutateurs et le stockage sont termin√©s, nous proc√©dons directement au d√©ploiement de oVirt. </p><br><h2 id="chast-2-ustanovka-i-nastroyka-klastera-ovirt-43">  Partie 2. Installation et configuration du cluster oVirt 4.3 </h2><br><p>  Pour faciliter l'orientation, je vais √©num√©rer les principales sections de cet article, qui doivent √™tre effectu√©es √† tour de r√¥le: </p><br><ol><li>  Installation du serveur de gestion oVirt </li><li>  Cr√©ation d'un nouveau centre de donn√©es </li><li>  Cr√©er un nouveau cluster </li><li>  Installation d'h√¥tes suppl√©mentaires dans un environnement auto-h√©berg√© </li><li>  Cr√©er une zone de stockage ou des domaines de stockage </li><li>  Cr√©er et configurer des r√©seaux pour des machines virtuelles </li><li>  Cr√©ation d'une image d'installation pour d√©ployer une machine virtuelle </li><li>  Cr√©ation d'une machine virtuelle </li></ol><br><h3 id="ustanovka-upravlyayuschego-servera-ovirt">  Installation du serveur de gestion oVirt </h3><br><p>  <strong>Le serveur de gestion oVirt</strong> est l'√©l√©ment le plus important de l'infrastructure oVirt, sous la forme d'une machine virtuelle, d'un h√¥te ou d'un p√©riph√©rique virtuel qui g√®re l'int√©gralit√© de l'infrastructure oVirt. </p><br><p>  Ses analogues proches du monde de la virtualisation: </p><br><ul><li>  VMware vSphere - vCenter Server </li><li>  Microsoft Hyper-V - System Center Virtual Machine Manager (VMM). </li></ul><br><p>  Pour installer le serveur de gestion oVirt, nous avons deux options: </p><br><p>  <u>Option 1</u> <br>  D√©ploiement d'un serveur en tant que machine virtuelle ou h√¥te sp√©cialis√©. </p><br><p>  Cette option fonctionne bien, mais √† condition qu'une telle machine virtuelle fonctionne ind√©pendamment du cluster, c'est-√†-dire  ne s'ex√©cutant sur aucun h√¥te du cluster en tant que machine virtuelle standard ex√©cutant KVM. </p><br><p>  Pourquoi ne pouvez-vous pas d√©ployer une telle machine virtuelle sur les h√¥tes de cluster? </p><br><p>  Au tout d√©but du processus de d√©ploiement du serveur de gestion oVirt, nous avons un dilemme: vous devez installer la machine virtuelle de gestion, mais le cluster lui-m√™me n'est pas encore r√©ellement, et donc, que pouvez-vous trouver tout de suite?  Il est juste d'installer KVM sur le futur n≈ìud de cluster, puis de cr√©er une machine virtuelle sur celui-ci, par exemple, avec CentOS OS et d'y d√©ployer le moteur oVirt.  Cela peut g√©n√©ralement √™tre fait pour des raisons de contr√¥le complet sur une telle machine virtuelle, mais c'est une intention erron√©e, car dans ce cas, √† l'avenir, il y aura 100% de probl√®mes avec une telle machine virtuelle de contr√¥le: </p><br><ul><li>  il ne peut pas √™tre migr√© dans la console oVirt entre les h√¥tes (n≈ìuds) du cluster; </li><li>  lors de la migration √† l'aide des outils KVM via <em>virsh migrate</em> , cette machine virtuelle sera inaccessible √† la gestion depuis la console oVirt. </li><li>  les h√¥tes de cluster ne peuvent pas √™tre mis en <strong>mode Maintenance</strong> si vous migrez cette machine virtuelle d'h√¥te √† h√¥te √† l'aide de <em>virsh migrate</em> . </li></ul><br><p>  Faites donc tout selon les r√®gles - utilisez un h√¥te s√©par√© ou une machine virtuelle ind√©pendante ex√©cut√©e sur celui-ci pour le serveur de contr√¥le oVirt, ou plut√¥t faites comme il est √©crit dans la deuxi√®me version. </p><br><p>  <u>Option 2</u> <br>  Installation de oVirt Engine Appliance sur l'h√¥te de cluster qu'il g√®re. </p><br><p>  C'est cette option qui sera consid√©r√©e ci-dessous, comme plus correcte et adapt√©e dans notre cas. <br>  Les exigences pour une telle machine virtuelle sont d√©crites ci-dessous. J'ajouterai seulement qu'il est recommand√© d'avoir au moins deux h√¥tes dans l'infrastructure sur laquelle la machine virtuelle de gestion peut √™tre ex√©cut√©e pour la rendre tol√©rante aux pannes.  Ici, je voudrais ajouter que, comme je l'ai d√©j√† √©crit dans les commentaires de l'article pr√©c√©dent, je n'ai pas pu obtenir <em>splitbrain</em> sur le cluster oVirt √† partir de deux h√¥tes, avec la possibilit√© d'ex√©cuter la machine virtuelle du moteur h√©berg√© sur eux. </p><br><h4 id="ustanovka-ovirt-engine-appliance-na-pervyy-host-klastera">  Installer oVirt Engine Appliance sur le premier h√¥te de cluster </h4><br><p>  Lien vers la documentation officielle - <a href="https://www.ovirt.org/documentation/self-hosted/Self-Hosted_Engine_Guide/">Guide du moteur auto-h√©berg√© oVirt</a> , chapitre ¬´ <a href="https://www.ovirt.org/documentation/self-hosted/chap-Deploying_Self-Hosted_Engine.html">D√©ploiement du moteur auto-h√©berg√© √† l'aide de la ligne de commande</a> ¬ª </p><br><p>  Le document indique les conditions pr√©alables qui doivent √™tre remplies avant de d√©ployer la machine virtuelle du moteur h√©berg√©, ainsi que le processus d'installation est d√©crit en d√©tail, donc le r√©p√©ter mot pour mot n'a pas beaucoup de sens, nous nous concentrons donc sur certains d√©tails importants. </p><br><ul><li>  Avant de commencer toutes les √©tapes, assurez-vous d'activer la prise en charge de la virtualisation dans les param√®tres du BIOS sur l'h√¥te. </li><li>  Installez le package d'installation du moteur h√©berg√© sur l'h√¥te: </li></ul><br><pre><code class="plaintext hljs">yum -y install http://resources.ovirt.org/pub/yum-repo/ovirt-release43.rpm yum -y install epel-release yum install screen ovirt-hosted-engine-setup</code> </pre> <br><ul><li>  Nous commen√ßons la proc√©dure de d√©ploiement de oVirt Hosted Engine dans l'√©cran de l'h√¥te (vous pouvez le quitter via Ctrl-A + D, le fermer via Ctrl-D): </li></ul><br><pre> <code class="plaintext hljs">screen hosted-engine --deploy</code> </pre> <br><p>  Si vous le souhaitez, vous pouvez d√©marrer l'installation avec un fichier de r√©ponses pr√©par√©: </p><br><pre> <code class="plaintext hljs">hosted-engine --deploy --config-append=/var/lib/ovirt-hosted-engine-setup/answers/answers-ohe.conf</code> </pre> <br><ul><li>  Lors du d√©ploiement du moteur h√©berg√©, sp√©cifiez tous les param√®tres n√©cessaires: </li></ul><br><pre> <code class="plaintext hljs">-   -  vCPU  vRAM ( 4 vCPU  16 ) -  -    hosted engine  ‚Äì    FC -  LUN   hosted engine -       hosted engine ‚Äì     Local (  PostgreSQL    )  . .</code> </pre> <br><ul><li>  Pour installer une machine virtuelle hautement accessible avec un moteur h√©berg√©, nous avons cr√©√© un num√©ro de LUN sp√©cial de 4 et 150 Go sur le syst√®me de stockage, qui a ensuite √©t√© pr√©sent√© aux h√¥tes du cluster - voir l' <a href="https://habr.com/ru/company/lenvendo/blog/483980/">article pr√©c√©dent</a> . </li></ul><br><p>  Plus t√¥t, nous avons √©galement v√©rifi√© sa visibilit√© sur les h√¥tes: </p><br><pre> <code class="plaintext hljs">multipath -ll ‚Ä¶ 3600a098000e4b4b3000003c95d171065 dm-3 DELL , MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> <br><ul><li>  Le processus de d√©ploiement du moteur h√©berg√© lui-m√™me ne comporte rien de compliqu√©, √† la fin, nous devrions obtenir quelque chose comme ceci: </li></ul><br><pre> <code class="plaintext hljs">[ INFO ] Generating answer file '/var/lib/ovirt-hosted-engine-setup/answers/answers-20191129131846.conf' [ INFO ] Generating answer file '/etc/ovirt-hosted-engine/answers.conf' [ INFO ] Stage: Pre-termination [ INFO ] Stage: Termination [ INFO ] Hosted Engine successfully deployed</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">V√©rification de la disponibilit√© des services oVirt sur l'h√¥te:</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/qv/kd/c8/qvkdc8tkfhqkxar1v13jxt4ozxs.png"></p></div></div><br><p>  Si tout a √©t√© fait correctement, une fois l'installation termin√©e, acc√©dez √† <em><a href="https://ovirt_hostname/ovirt-engine">https: // ovirt_hostname / ovirt-engine</a></em> depuis l'ordinateur de l'administrateur √† l'aide d'un navigateur Web et cliquez sur [ <strong>Administration Portal</strong> ]. </p><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran ¬´Portail d'administration¬ª</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/m7/sq/xp/m7sqxpd7qkl3dugl4oj7_5bga5g.png"></p></div></div><br><p>  Apr√®s avoir entr√© le login et le mot de passe (d√©finis lors de l'installation) dans la fen√™tre comme dans la capture d'√©cran, nous arrivons au panneau de configuration d'Open Virtualization Manager, dans lequel vous pouvez effectuer toutes les actions avec l'infrastructure virtuelle: </p><br><ol><li>  ajouter un centre de donn√©es </li><li>  ajouter et configurer un cluster </li><li>  ajouter des h√¥tes et les g√©rer </li><li>  ajouter des zones de stockage ou des domaines de stockage pour les disques de machine virtuelle </li><li>  ajouter et configurer des r√©seaux pour les machines virtuelles </li><li>  ajouter des machines virtuelles, des images d'installation, des mod√®les de VM et les g√©rer </li></ol><br><p><img src="https://habrastorage.org/webt/4r/t_/qg/4rt_qg6irvl-mf6bsorclnmr36e.png"></p><br><p>  Toutes ces actions seront examin√©es plus tard, quelque chose dans une grande cellule, quelque chose de plus en d√©tail et avec des nuances. <br>  Mais d'abord, je recommanderais de lire ce module compl√©mentaire, dont beaucoup peuvent √™tre utiles. </p><br><p>  <strong>Addition</strong> </p><br><p>  <strong>1)</strong> En principe, s'il y a un tel besoin, rien ne vous emp√™che de pr√©-installer l'hyperviseur KVM sur les n≈ìuds de cluster √† l'aide des <strong><em>packages libvirt</em></strong> et <strong><em>qemu-kvm</em></strong> (ou <strong><em>qemu-kvm-ev</em></strong> ) de la version souhait√©e, bien que lors du d√©ploiement du n≈ìud de cluster oVirt, il puisse faites-le vous-m√™me. </p><br><p>  Mais si <strong><em>libvirt</em></strong> et <strong><em>qemu-kvm</em></strong> n'ont pas √©t√© install√©s avec la derni√®re version, vous pouvez obtenir une telle erreur lors du d√©ploiement du moteur h√©berg√©: </p><br><pre> <code class="plaintext hljs">error: unsupported configuration: unknown CPU feature: md-clear</code> </pre> <br><p>  C'est-√†-dire  vous devez avoir <a href="https://centos.pkgs.org/7/centos-updates-x86_64/libvirt-4.5.0-10.el7_6.12.x86_64.rpm.html">une version mise √† jour de</a> <strong><em>libvirt</em></strong> avec protection <a href="https://wiki.ubuntu.com/SecurityTeam/KnowledgeBase/MDS">MDS</a> qui prend en charge cette politique: </p><br><pre> <code class="plaintext hljs">&lt;feature policy='require' name='md-clear'/&gt;</code> </pre> <br><p>  Installez libvirt v.4.5.0-10.el7_6.12, avec le support md-clear: </p><br><pre> <code class="plaintext hljs">yum-config-manager --disable mirror.centos.org_centos-7_7_virt_x86_64_libvirt-latest_ yum install centos-release-qemu-ev yum update yum install qemu-kvm qemu-img virt-manager libvirt libvirt-python libvirt-client virt-install virt-viewer libguestfs libguestfs-tools dejavu-lgc-sans-fonts virt-top libvirt libvirt-python libvirt-client systemctl enable libvirtd systemctl restart libvirtd &amp;&amp; systemctl status libvirtd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">V√©rifiez la prise en charge de md-clear:</b> <div class="spoiler_text"><br><pre> <code class="plaintext hljs">virsh domcapabilities kvm | grep require &lt;feature policy='require' name='ss'/&gt; &lt;feature policy='require' name='hypervisor'/&gt; &lt;feature policy='require' name='tsc_adjust'/&gt; &lt;feature policy='require' name='clflushopt'/&gt; &lt;feature policy='require' name='pku'/&gt; &lt;feature policy='require' name='md-clear'/&gt; &lt;feature policy='require' name='stibp'/&gt; &lt;feature policy='require' name='ssbd'/&gt; &lt;feature policy='require' name='invtsc'/&gt;</code> </pre> </div></div><br><p>  Apr√®s cela, vous pouvez continuer √† installer le moteur h√©berg√©. </p><br><p>  <strong>2)</strong> Dans oVirt 4.3, la pr√©sence et l'utilisation du pare <strong>-</strong> feu <strong>firewalld</strong> est une condition pr√©alable. </p><br><div class="spoiler">  <b class="spoiler_title">Si lors du d√©ploiement de la machine virtuelle pour le moteur h√©berg√©, nous obtenons l'erreur suivante:</b> <div class="spoiler_text"><br><pre> <code class="plaintext hljs">[ ERROR ] fatal: [localhost]: FAILED! =&gt; {"changed": false, "msg": "firewalld is required to be enabled and active in order to correctly deploy hosted-engine. Please check, fix accordingly and re-deploy.\n"} [ ERROR ] Failed to execute stage 'Closing up': Failed executing ansible-playbook [https://bugzilla.redhat.com/show_bug.cgi?id=1608467</code> </pre> </div></div><br><p>  Ensuite, vous devez d√©sactiver un autre pare-feu (s'il est utilis√©), installer et ex√©cuter le <strong>pare</strong> - <strong>feu</strong> : </p><br><pre> <code class="plaintext hljs">yum install firewalld systemctl enable firewalld systemctl start firewalld firewall-cmd --state firewall-cmd --get-default-zone firewall-cmd --get-active-zones firewall-cmd --get-zones</code> </pre> <br><p>  Plus tard, lors de l'installation de l'agent ovirt sur un nouvel h√¥te pour le cluster, il configurera automatiquement les ports requis dans <strong>firewalld</strong> . </p><br><p>  <strong>3)</strong> Red√©marrage d'un h√¥te avec une machine virtuelle en cours d'ex√©cution avec un moteur h√©berg√©. </p><br><p>  Comme d'habitude, <a href="https://www.ovirt.org/documentation/self-hosted/chap-Maintenance_and_Upgrading_Resources.html">r√©f√©rence 1</a> et <a href="https://www.ovirt.org/documentation/self-hosted/chap-Troubleshooting.html">r√©f√©rence 2</a> aux documents d'orientation. </p><br><p>  Toute la gestion du moteur h√©berg√© de la VM se fait UNIQUEMENT √† l'aide de la commande de <strong>moteur h√©berg√©</strong> sur l'h√¥te o√π il fonctionne, vous devez oublier <strong>virsh</strong> , ainsi que le fait que vous pouvez vous connecter √† cette VM via SSH et ex√©cuter la <strong>commande</strong> ¬´ <strong>shutdown</strong> ¬ª dessus. </p><br><div class="spoiler">  <b class="spoiler_title">La proc√©dure pour mettre la VM en mode service:</b> <div class="spoiler_text"><br><pre> <code class="plaintext hljs">hosted-engine --set-maintenance --mode=global hosted-engine --vm-status !! Cluster is in GLOBAL MAINTENANCE mode !! --== Host host1.test.local (id: 1) status ==-- conf_on_shared_storage : True Status up-to-date : True Hostname : host1.test.local Host ID : 1 Engine status : {"health": "good", "vm": "up", "detail": "Up"} Score : 3400 stopped : False Local maintenance : False crc32 : dee1a774 local_conf_timestamp : 1821 Host timestamp : 1821 Extra metadata (valid at timestamp): metadata_parse_version=1 metadata_feature_version=1 timestamp=1821 (Sat Nov 29 14:25:19 2019) host-id=1 score=3400 vm_conf_refresh_time=1821 (Sat Nov 29 14:25:19 2019) conf_on_shared_storage=True maintenance=False state=GlobalMaintenance stopped=False hosted-engine --vm-shutdown</code> </pre> </div></div><br><p>  Nous red√©marrons l'h√¥te avec l'agent moteur h√©berg√© et faisons ce dont nous avons besoin. </p><br><p>  Apr√®s le red√©marrage, v√©rifiez l'√©tat de la machine virtuelle avec le moteur h√©berg√©: </p><br><pre> <code class="plaintext hljs">hosted-engine --vm-status</code> </pre> <br><p>  Si notre machine virtuelle avec moteur h√©berg√© ne d√©marre pas et si nous voyons des erreurs similaires dans le journal de service: </p><br><div class="spoiler">  <b class="spoiler_title">Erreur dans le journal de service:</b> <div class="spoiler_text"><br><pre> <code class="plaintext hljs">journalctl -u ovirt-ha-agent ... Jun 29 14:34:44 host1 journal: ovirt-ha-agent ovirt_hosted_engine_ha.agent.hosted_engine.HostedEngine ERROR Failed to start necessary monitors Jun 29 14:34:44 host1 journal: ovirt-ha-agent ovirt_hosted_engine_ha.agent.agent.Agent ERROR Traceback (most recent call last):#012 File "/usr/lib/python2.7/site-packages/ovirt_hosted_engine_ha/agent/agent.py", line 131, in _run_agent#012 return action(he)#012 File "/usr/lib/python2.7/site-packages/ovirt_hosted_engine_ha/agent/agent.py", line 55, in action_proper#012 return he.start_monitoring()#012 File "/usr/lib/python2.7/site-packages/ovirt_hosted_engine_ha/agent/hosted_engine.py", line 413, in start_monitoring#012 self._initialize_broker()#012 File "/usr/lib/python2.7/site-packages/ovirt_hosted_engine_ha/agent/hosted_engine.py", line 537, in _initialize_broker#012 m.get('options', {}))#012 File "/usr/lib/python2.7/site-packages/ovirt_hosted_engine_ha/lib/brokerlink.py", line 86, in start_monitor#012 ).format(t=type, o=options, e=e)#012RequestError: brokerlink - failed to start monitor via ovirt-ha-broker: [Errno 2] No such file or directory, [monitor: 'ping', options: {'addr': '172.20.32.32'}] Jun 29 14:34:44 host1 journal: ovirt-ha-agent ovirt_hosted_engine_ha.agent.agent.Agent ERROR Trying to restart agent</code> </pre> </div></div><br><p>  Ensuite, nous connectons le stockage et red√©marrons l'agent: </p><br><pre> <code class="plaintext hljs">hosted-engine --connect-storage systemctl restart ovirt-ha-agent systemctl status ovirt-ha-agent hosted-engine --vm-start hosted-engine --vm-status</code> </pre> <br><p>  Apr√®s avoir d√©marr√© la machine virtuelle avec le moteur h√©berg√©, nous la supprimons du mode de maintenance: </p><br><div class="spoiler">  <b class="spoiler_title">Proc√©dure pour supprimer une machine virtuelle du mode maintenance:</b> <div class="spoiler_text"><br><pre> <code class="plaintext hljs">hosted-engine --check-liveliness hosted-engine --set-maintenance --mode=none hosted-engine --vm-status --== Host host1.test.local (id: 1) status ==-- conf_on_shared_storage : True Status up-to-date : True Hostname : host1.test.local Host ID : 1 Engine status : {"health": "good", "vm": "up", "detail": "Up"} Score : 3400 stopped : False Local maintenance : False crc32 : 6d1eb25f local_conf_timestamp : 6222296 Host timestamp : 6222296 Extra metadata (valid at timestamp): metadata_parse_version=1 metadata_feature_version=1 timestamp=6222296 (Fri Jan 17 11:40:43 2020) host-id=1 score=3400 vm_conf_refresh_time=6222296 (Fri Jan 17 11:40:43 2020) conf_on_shared_storage=True maintenance=False state=EngineUp stopped=False</code> </pre> </div></div><br><p>  <strong>4)</strong> Suppression du moteur h√©berg√© et de tout ce qui s'y rapporte. </p><br><p>  Parfois, il peut √™tre n√©cessaire de supprimer correctement un moteur h√©berg√© pr√©c√©demment install√© - un <a href="https://www.ovirt.org/documentation/self-hosted/chap-Troubleshooting.html">lien</a> vers un document d'orientation. </p><br><p>  Ex√©cutez simplement la commande sur l'h√¥te: </p><br><pre> <code class="plaintext hljs">/usr/sbin/ovirt-hosted-engine-cleanup</code> </pre> <br><p>  Ensuite, supprimez les packages inutiles, en sauvegardant certaines configurations avant cela, si n√©cessaire: </p><br><pre> <code class="plaintext hljs">yum autoremove ovirt* qemu* virt* libvirt* libguestfs</code> </pre> <br><h4 id="sozdanie-novogo-datacentra">  Cr√©ation d'un nouveau centre de donn√©es </h4><br><p>  Documentation de r√©f√©rence - Guide d'administration oVirt.  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Data_Centers.html">Chapitre 4: Centres de donn√©es</a> </p><br><p>  Tout d'abord, nous d√©finissons ce qu'est un <strong>centre de donn√©es</strong> (je cite l'aide) - il s'agit d'une entit√© logique qui d√©finit un ensemble de ressources utilis√©es dans un environnement sp√©cifique. </p><br><p>  Un centre de donn√©es est un type de conteneur compos√© de: </p><br><ul><li>  ressources logiques sous forme de clusters et d'h√¥tes </li><li>  mettre en r√©seau les ressources du r√©seau sous forme de r√©seaux logiques et d'adaptateurs physiques sur les h√¥tes, </li><li>  les ressources de stockage (pour les disques VM, les mod√®les, les images) sous forme de zones de stockage (domaines de stockage). </li></ul><br><p>  Un centre de donn√©es peut comprendre plusieurs clusters constitu√©s de plusieurs h√¥tes sur lesquels s'ex√©cutent des machines virtuelles, il peut √©galement √™tre associ√© √† plusieurs zones de stockage. <br>  Il peut y avoir plusieurs centres de donn√©es, ils fonctionnent ind√©pendamment les uns des autres.  Ovirt a une s√©paration des pouvoirs par r√¥le, et vous pouvez configurer les autorisations personnellement, √† la fois au niveau du centre de donn√©es et sur ses √©l√©ments logiques individuels. </p><br><p>  Le centre de donn√©es, ou les centres de donn√©es, s'il y en a plusieurs, sont g√©r√©s √† partir d'une console d'administration ou d'un portail unique. </p><br><p>  Pour cr√©er un centre de donn√©es, acc√©dez au portail d'administration et cr√©ez un nouveau centre de donn√©es: <br>  <strong>Calculer</strong> &gt;&gt; <strong>Centres de donn√©es</strong> &gt;&gt; <strong>Nouveau</strong> </p><br><p>  √âtant donn√© que nous utilisons le stockage partag√© sur le syst√®me de stockage, le type de stockage doit √™tre partag√©: </p><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran avec l'assistant du centre de donn√©es</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/bf/eg/id/bfegid-tcvqsfjjxa1oqzop_9iw.png"></p></div></div><br><p>  Lors de l'installation d'une machine virtuelle avec moteur h√©berg√©, par d√©faut, un centre de donn√©es est cr√©√© - <strong>Datacenter1</strong> , puis, si n√©cessaire, vous pouvez changer le type de stockage en un autre. </p><br><p>  La cr√©ation d'un centre de donn√©es est une t√¢che simple, sans nuances d√©licates, et toutes les actions suppl√©mentaires avec celui-ci sont d√©crites dans la documentation.  Je noterai seulement que les h√¥tes uniques qui n'ont que du stockage local (disque) pour la machine virtuelle ne peuvent pas entrer dans le centre de donn√©es avec le type de stockage - partag√© (vous ne pouvez pas les ajouter l√†-bas), et pour eux, vous devez cr√©er un centre de donn√©es distinct - c'est-√†-dire  chaque h√¥te individuel avec stockage local a besoin de son propre centre de donn√©es distinct. </p><br><h4 id="sozdanie-novogo-klastera">  Cr√©er un nouveau cluster </h4><br><p>  Lien de documentation - Guide d'administration oVirt.  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Clusters.html">Chapitre 5: Clusters</a> </p><br><p>  Sans d√©tails inutiles, un <strong>cluster</strong> est un regroupement logique d'h√¥tes qui ont une zone de stockage commune (sous la forme de lecteurs partag√©s sur un syst√®me de stockage, comme dans notre cas).  Il est √©galement souhaitable que les h√¥tes du cluster soient identiques en mat√©riel et aient le m√™me type de processeur (Intel ou AMD).  Il est bien s√ªr pr√©f√©rable que les serveurs du cluster soient compl√®tement identiques. </p><br><p>  Le cluster fait partie du centre de donn√©es (avec un type de stockage sp√©cifique - <em>local</em> ou <em>partag√©</em> ), et tous les h√¥tes doivent n√©cessairement appartenir √† un cluster, selon qu'ils disposent ou non d'un stockage commun. </p><br><p>  Lors de l'installation d'une machine virtuelle avec moteur h√©berg√© sur l'h√¥te, par d√©faut, un centre de donn√©es est cr√©√© - <strong>Datacenter1</strong> , avec un cluster - <strong>Cluster1</strong> , et √† l'avenir, vous pouvez configurer ses param√®tres, activer des options suppl√©mentaires, y ajouter des h√¥tes, etc. </p><br><p>  Comme d'habitude, pour plus de d√©tails sur tous les param√®tres du cluster, il est conseill√© de se r√©f√©rer √† la documentation officielle.  Parmi certaines fonctionnalit√©s de la configuration du cluster, j'ajouterai seulement que lors de sa cr√©ation, il suffit de configurer uniquement les param√®tres de base de l'onglet <strong><em>G√©n√©ral</em></strong> . </p><br><p>  Je noterai les param√®tres les plus importants: </p><br><ul><li>  <em>Type de processeur</em> - il est s√©lectionn√© en fonction des processeurs install√©s sur les h√¥tes du cluster, de quel fabricant ils sont et du processeur sur les h√¥tes le plus ancien afin que, selon cela, toutes les instructions de processeur disponibles dans le cluster soient utilis√©es. </li><li>  <em>Type de commutateur</em> - dans notre cluster, nous utilisons uniquement un pont Linux, nous le s√©lectionnons donc. </li><li>  <em>Type de pare-feu</em> - tout est clair ici, c'est firewalld, qui doit √™tre activ√© et configur√© sur les h√¥tes. </li></ul><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran avec les param√®tres du cluster</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/os/vn/qy/osvnqyj3srg8vjpflkdy9l4e84e.png"></p></div></div><br><h4 id="ustanovka-dopolnitelnyh-hostov-v-self-hosted-okruzhenii">  Installation d'h√¥tes suppl√©mentaires dans un environnement auto-h√©berg√© </h4><br><p>  <a href="https://www.ovirt.org/documentation/self-hosted/chap-Installing_Additional_Hosts_to_a_Self-Hosted_Environment.html">Lien</a> vers la documentation. </p><br><p>  Des h√¥tes suppl√©mentaires pour l'environnement auto-h√©berg√© sont ajout√©s de la m√™me mani√®re qu'un h√¥te standard, avec le point suppl√©mentaire pour le d√©ploiement d'une machine virtuelle avec un moteur h√©berg√© - <strong><em>Choisissez l'action de d√©ploiement du moteur h√©berg√©</em></strong> &gt;&gt; <strong><em>D√©ployer</em></strong> .  √âtant donn√© que le LUN de la machine virtuelle avec le moteur h√©berg√© doit √©galement √™tre pr√©sent√© √† l'h√¥te suppl√©mentaire, cela signifie que cet h√¥te peut √™tre utilis√© si n√©cessaire pour h√©berger la machine virtuelle avec le moteur h√©berg√© sur celui-ci. <br>  Pour la tol√©rance aux pannes, il est fortement recommand√© qu'il existe au moins deux h√¥tes sur lesquels une machine virtuelle de moteur h√©berg√©e peut √™tre h√©berg√©e. </p><br><p>  Sur l'h√¥te suppl√©mentaire, d√©sactivez iptables (si activ√©), activez le pare-feu </p><br><pre> <code class="plaintext hljs">systemctl stop iptables systemctl disable iptables systemctl enable firewalld systemctl start firewalld</code> </pre> <br><p>  Installez la version requise de KVM (si n√©cessaire): </p><br><pre> <code class="plaintext hljs">yum-config-manager --disable mirror.centos.org_centos-7_7_virt_x86_64_libvirt-latest_ yum install centos-release-qemu-ev yum update yum install qemu-kvm qemu-img virt-manager libvirt libvirt-python libvirt-client virt-install virt-viewer libguestfs libguestfs-tools dejavu-lgc-sans-fonts virt-top libvirt libvirt-python libvirt-client systemctl enable libvirtd systemctl restart libvirtd &amp;&amp; systemctl status libvirtd virsh domcapabilities kvm | grep md-clear</code> </pre> <br><p>  Installez les r√©f√©rentiels n√©cessaires et le programme d'installation du moteur h√©berg√©: </p><br><pre> <code class="plaintext hljs">yum -y install http://resources.ovirt.org/pub/yum-repo/ovirt-release43.rpm yum -y install epel-release yum update yum install screen ovirt-hosted-engine-setup</code> </pre> <br><p>  Ensuite, acc√©dez √† la <strong><em>console Open Virtualization Manager</em></strong> , ajoutez un nouvel h√¥te et faites tout √©tape par √©tape, comme cela est √©crit dans la <a href="https://www.ovirt.org/documentation/self-hosted/chap-Installing_Additional_Hosts_to_a_Self-Hosted_Environment.html">documentation</a> . </p><br><p>  Par cons√©quent, apr√®s avoir ajout√© un h√¥te suppl√©mentaire, nous devrions obtenir environ une image dans la console d'administration, comme dans la capture d'√©cran. </p><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran du portail administratif - h√¥tes</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/wh/hr/nc/whhrnchajorr5ogqxinvecjdxi0.png"></p></div></div><br><p>  L'h√¥te sur lequel la machine virtuelle h√©berg√©e est actuellement active a une couronne en or et l'inscription ¬´ <em>Running the Hosted Engine VM</em> ¬ª, l'h√¥te sur lequel cette machine virtuelle peut √™tre ex√©cut√©e si n√©cessaire est l'inscription ¬´ <em>Can run the Hosted Engine VM</em> ¬ª. </p><br><p>  En cas de d√©faillance d'un h√¥te sur lequel " <em>Ex√©cution de la machine virtuelle du moteur h√©berg√©</em> ", il red√©marrera automatiquement sur le deuxi√®me h√¥te.  En outre, cette machine virtuelle peut √™tre migr√©e de l'h√¥te actif vers celui de sauvegarde, pour sa maintenance. </p><br><p>  <u>Configurer la gestion de l'alimentation / l'escrime sur les h√¥tes oVirt</u> </p><br><p>  Liens de documentation: </p><br><ul><li>  Red Hat Virtualization 4.3 -&gt; R√©f√©rence technique -&gt; <a href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/html/technical_reference/chap-power_management">Chapitre 4. Gestion de l'alimentation</a> </li><li>  Guide d'administration oVirt -&gt; <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html">Chapitre 7: H√¥tes</a> </li></ul><br><p>  Bien qu'il puisse sembler que l'ajout et la configuration de l'h√¥te soient termin√©s, ce n'est pas enti√®rement vrai. <br>  Pour le fonctionnement normal des h√¥tes et l'identification / l'√©limination des pannes avec l'un d'entre eux, le param√®tre Gestion de l'alimentation / cl√¥ture est requis. </p><br><p>  <strong>L'escrime</strong> , ou bo√Ætier, est le processus d'exclusion temporaire d'un h√¥te d√©faillant ou d√©faillant d'un cluster, au cours duquel les services oVirt sur celui-ci ou l'h√¥te lui-m√™me sont red√©marr√©s. </p><br><p>  Tous les d√©tails sur les d√©finitions et les param√®tres de gestion de l'alimentation / cl√¥ture sont donn√©s, comme d'habitude dans la documentation, je ne donnerai qu'un exemple de la fa√ßon de configurer ce param√®tre important lorsqu'il est appliqu√© aux serveurs Dell R640 avec iDRAC 9. </p><br><ol><li>  Nous allons dans le portail administratif, cliquez sur <strong>Calculer</strong> &gt;&gt; <strong>H√¥tes,</strong> s√©lectionnez l'h√¥te. </li><li>  Cliquez sur <strong>Modifier</strong> . </li><li>  Cliquez sur l'onglet <strong>Gestion de</strong> l' <strong>alimentation</strong> . </li><li>  Cochez la case en regard de l'option <strong>Activer la gestion de l'alimentation</strong> . </li><li>  Nous <em>cochons</em> la case √† c√¥t√© de l'option d' <em>int√©gration de Kdump</em> afin que l'h√¥te ne passe pas en mode d'escrime lors de l'√©criture d'un vidage sur <em>incident du</em> noyau. </li></ol><br><div class="spoiler">  <b class="spoiler_title">Remarque</b> <div class="spoiler_text"><p>  Apr√®s avoir activ√© l'int√©gration de Kdump sur un h√¥te d√©j√† en cours d'ex√©cution, il doit √™tre r√©install√© conform√©ment √† la proc√©dure dans le Guide d'administration oVirt -&gt; <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html">Chapitre 7: H√¥tes</a> -&gt; R√©installer les h√¥tes. </p></div></div><br><ol><li>  Facultativement, vous pouvez cocher la case <em>D√©sactiver</em> le contr√¥le de la <em>politique de gestion de l'alimentation</em> si nous ne voulons pas que la gestion de l'alimentation de l'h√¥te soit contr√¥l√©e par la politique de planification du cluster. </li><li>  Cliquez sur le bouton ( <em>+</em> ) pour ajouter un nouveau dispositif de gestion de l'alimentation, la fen√™tre de modification des propri√©t√©s de l'agent s'ouvrira. <br>  Pour iDRAC9, remplissez les champs: <br><ul><li>  <em>Adresse</em> - adresse iDRAC9 </li><li>  <em>Nom d'utilisateur / mot de passe</em> - respectivement identifiant et mot de passe pour entrer dans iDRAC9 </li><li>  <em>Type</em> - drac5 </li><li>  marque <em>Secure</em> </li><li>  ajoutez les options suivantes: <em>cmd_prompt =&gt;, login_timeout = 30</em> </li></ul></li></ol><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran avec les param√®tres ¬´Gestion de l'alimentation¬ª dans les propri√©t√©s de l'h√¥te</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/kz/be/ky/kzbekyoxr6cromqq0epltd2cjbo.png"></p></div></div><br><h4 id="sozdanie-oblasti-hraneniya-ili-storage-domains">  Cr√©er une zone de stockage ou des domaines de stockage </h4><br><p>  Lien de documentation - Guide d'administration oVirt, <a href="https://www.ovirt.org/documentation/admin-guide/chap-Storage.html">Chapitre 8: Stockage</a> . </p><br><p>  <strong>Un domaine de stockage</strong> , ou zone de stockage, est un emplacement centralis√© pour le stockage des disques de machine virtuelle, des images d'installation, des mod√®les et des instantan√©s. </p><br><p>  Les zones de stockage peuvent √™tre connect√©es √† un centre de donn√©es √† l'aide de divers protocoles, grappes et syst√®mes de fichiers r√©seau. </p><br><p>  OVirt dispose de trois types de zones de stockage: </p><br><ul><li>  <strong>Data Domain</strong> - pour stocker toutes les donn√©es associ√©es aux machines virtuelles (disques, mod√®les).  Data Domain ne peut pas √™tre partag√© entre diff√©rents centres de donn√©es. </li><li>  <strong>Domaine ISO</strong> (type de zone de stockage obsol√®te) - pour stocker les images d'installation du syst√®me d'exploitation.  Le domaine ISO peut √™tre partag√© entre diff√©rents centres de donn√©es. </li><li>  <strong>Domaine d'exportation</strong> (type de zone de stockage obsol√®te) - pour le stockage temporaire des images d√©plac√©es entre les centres de donn√©es. </li></ul><br><p>  Dans notre cas particulier, un domaine de stockage de type Data Domain utilise le protocole Fibre Channel (FCP) pour se connecter aux LUN du syst√®me de stockage. </p><br><p>  Du point de vue de oVirt, lorsque vous utilisez des syst√®mes de stockage (FC ou iSCSI), chaque disque virtuel, instantan√© ou mod√®le est un disque logique. <br>  Les p√©riph√©riques de bloc sont assembl√©s en une seule unit√© (sur les h√¥tes de cluster) √† l'aide du groupe de volumes, puis divis√©s √† l'aide de LVM en volumes logiques utilis√©s comme disques virtuels pour les machines virtuelles. </p><br><p>  Tous ces groupes et de nombreux volumes LVM peuvent √™tre vus sur l'h√¥te du cluster √† l'aide des commandes <strong>vgs</strong> et <strong>lvs</strong> .  Naturellement, toutes les actions avec de tels disques doivent √™tre effectu√©es uniquement √† partir de la console oVirt, sauf dans des cas particuliers. </p><br><p>  Les disques virtuels pour machines virtuelles peuvent √™tre de deux types - QCOW2 ou RAW.  Les disques peuvent √™tre <em>minces</em> ou <em>√©pais</em> .  Les instantan√©s sont toujours cr√©√©s aussi <em>fins</em> . </p><br><p>  La fa√ßon de g√©rer les domaines de stockage ou les zones de stockage accessibles via FC est assez logique - pour chaque disque virtuel d'une machine virtuelle, il existe un volume logique distinct qui n'est accessible en √©criture que sur un seul h√¥te.  Dans le cas de connexions via FC, oVirt utilise quelque chose comme LVM en cluster. </p><br><p>  Les machines virtuelles situ√©es sur la m√™me zone de stockage peuvent √™tre migr√©es entre des h√¥tes appartenant au m√™me cluster. </p><br><p>  Comme vous pouvez le voir dans la description, le cluster dans oVirt, comme le cluster dans VMware vSphere ou Hyper-V, signifie essentiellement la m√™me chose - il s'agit d'un regroupement logique d'h√¥tes, de pr√©f√©rence identiques en mat√©riel, et ayant un stockage commun pour les disques de machines virtuelles. </p><br><p>  Nous proc√©dons directement √† la cr√©ation d'une zone de stockage pour les donn√©es (disques VM), car sans elle, le centre de donn√©es ne sera pas initialis√©. <br>  Permettez-moi de vous rappeler que tous les LUN pr√©sent√©s aux h√¥tes du cluster sur le stockage doivent leur √™tre visibles √† l'aide de la commande " <strong>multipath -ll</strong> ". </p><br><p>  Selon la <a href="https://www.ovirt.org/documentation/admin-guide/chap-Storage.html">documentation</a> , acc√©dez au portail, acc√©dez √† <strong><em>Stockage</em></strong> &gt;&gt; <strong><em>Domaines</em></strong> -&gt; <strong><em>Nouveau domaine</em></strong> et suivez les instructions de la section "Ajout de stockage FCP". </p><br><p>  Apr√®s avoir d√©marr√© l'assistant, remplissez les champs obligatoires: </p><br><ul><li>  <em>Nom</em> - d√©finissez le nom du cluster </li><li>  <em>Fonction de domaine</em> - Donn√©es </li><li>  <em>Type de stockage</em> - Fibre Channel </li><li>  <em>H√¥te √† utiliser</em> : s√©lectionnez l'h√¥te sur lequel le LUN requis est disponible. </li></ul><br><p>  Dans la liste des LUN, nous s√©lectionnons celle dont nous avons besoin, cliquez sur <em>Ajouter</em> puis sur <em>OK</em> .  Si n√©cessaire, vous pouvez ajuster des param√®tres suppl√©mentaires de la zone de stockage en cliquant sur <em>Param√®tres avanc√©s</em> . </p><br><div class="spoiler">  <b class="spoiler_title">Capture d'√©cran de l'assistant de compl√©ment ¬´Domaine de stockage¬ª</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/tk/tz/gq/tktzgqwyqioqf45ere-qi2ll6bc.png"></p></div></div><br><p>  Selon les r√©sultats de l'assistant, nous devrions obtenir une nouvelle zone de stockage et notre centre de donn√©es devrait passer √† l'√©tat <strong>UP</strong> ou initialis√©: </p><br><div class="spoiler">  <b class="spoiler_title">Captures d'√©cran du centre de donn√©es et des zones de stockage:</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/dj/4j/zm/dj4jzmtuzoyu4ustvkcaqxgywoo.png"></p><br><p><img src="https://habrastorage.org/webt/5l/tn/gd/5ltngdb3ipqfrfkkvktqriegwzq.png"></p></div></div><br><h4 id="sozdanie-i-nastroyka-setey-dlya-virtualnyh-mashin">  Cr√©er et configurer des r√©seaux pour des machines virtuelles </h4><br><p>  Lien de documentation - Guide d'administration oVirt, <a href="https://www.ovirt.org/documentation/admin-guide/chap-Logical_Networks.html">Chapitre 6: R√©seaux logiques</a> </p><br><p>  Les r√©seaux, ou r√©seaux, sont utilis√©s pour regrouper les r√©seaux logiques utilis√©s dans l'infrastructure virtuelle oVirt. </p><br><p>  Les interfaces logiques telles que le pont Linux sont utilis√©es pour interagir avec une carte r√©seau dans une machine virtuelle, avec une carte physique sur l'h√¥te. </p><br><p>  Pour regrouper et r√©partir le trafic entre les r√©seaux, les VLAN sont configur√©s sur les commutateurs. </p><br><p>         oVirt,     ,   VLAN  ,        ,        . </p><br><p>                <a href="https://habr.com/ru/company/lenvendo/blog/483980/"> </a> ‚Äì    <strong>bond1</strong> ,           oVirt. </p><br><p>     hosted-engine,      ,           ‚Äì <strong>ovritmgmt</strong> ,      . </p><br><p>        <strong>ovritmgmt</strong>   ,    ,      oVirt. </p><br><div class="spoiler"> <b class="spoiler_title">   ovritmgmt</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/li/ym/4o/liym4ottpmognydqgsc7mi3lvfi.png"></p></div></div><br><p>        ,      <strong><em>Network</em></strong> &gt;&gt; <strong><em>Networks</em></strong> &gt;&gt; <strong><em>New</em></strong> ,    <strong><em>General</em></strong>      VLAN,      ¬´ <em>VM Network</em> ¬ª,          . </p><br><div class="spoiler"> <b class="spoiler_title">    VLAN32</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/gx/3x/nd/gx3xndfoeo-cniaf-cjlnjcefq4.png"></p></div></div><br><p>   <strong><em>Cluster</em></strong> ,       <strong>Cluster1</strong> . </p><br><p>     <strong><em>Compute</em></strong> &gt;&gt; <strong><em>Hosts</em></strong> ,      ,   <strong><em>Network interfaces</em></strong> ,    <em>Setup host networks</em> ,       . </p><br><div class="spoiler"> <b class="spoiler_title">  ¬´Setup host networks¬ª</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/l8/87/me/l887meiynekpssdplkxid4_11yo.png"></p></div></div><br><p>  oVirt         ‚Äì  VLAN  BRIDGE. </p><br><div class="spoiler"> <b class="spoiler_title">       :</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat ifcfg-bond1 # Generated by VDSM version 4.30.17.1 DEVICE=bond1 BONDING_OPTS='mode=1 miimon=100' MACADDR=00:50:56:82:57:52 ONBOOT=yes MTU=1500 DEFROUTE=no NM_CONTROLLED=no IPV6INIT=no cat ifcfg-bond1.432 # Generated by VDSM version 4.30.17.1 DEVICE=bond1.432 VLAN=yes BRIDGE=ovirtvm-vlan432 ONBOOT=yes MTU=1500 DEFROUTE=no NM_CONTROLLED=no IPV6INIT=no cat ifcfg-ovirtvm-vlan432 # Generated by VDSM version 4.30.17.1 DEVICE=ovirtvm-vlan432 TYPE=Bridge DELAY=0 STP=off ONBOOT=yes MTU=1500 DEFROUTE=no NM_CONTROLLED=no IPV6INIT=no</code> </pre> </div></div><br><p>   ,     <u> </u>      <strong>ifcfg-bond1.432</strong>  <strong>ifcfg-ovirtvm-vlan432</strong> . </p><br><p>            c hosted engine,      . </p><br><h4 id="sozdanie-ustanovochnogo-obraza-dlya-razvyortyvaniya-virtualnoy-mashiny">        </h4><br><p>    ‚Äî oVirt Administration Guide, <a href="https://www.ovirt.org/documentation/admin-guide/chap-Storage.html">Chapter 8: Storage</a> ,  Uploading Images to a Data Storage Domain. </p><br><p>    ,     ,       ,    , , <a href="https://cobbler.github.io/">Cobbler</a>    . </p><br><p>      ,        oVirt. ,     ISO Domain,     oVirt    ,         Storage domain   . </p><br><p>      <strong>Storage</strong> &gt;&gt; <strong>Disks</strong> &gt;&gt; <strong>Upload</strong> &gt;&gt; <strong>Start</strong> <br>       ISO ,     ,    " <em>Test connection</em> ". </p><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/mf/f6/mq/mff6mqownqd8xt9zhyesnobmt2y.png"></p></div></div><br><div class="spoiler"> <b class="spoiler_title">    :</b> <div class="spoiler_text"><p> <code>Unable to upload image to disk d6d8fd10-c1e0-4f2d-af15-90f8e636dadc due to a network error. Ensure that ovirt-imageio-proxy service is installed and configured and that ovirt-engine's CA certificate is registered as a trusted CA in the browser. The certificate can be fetched from https://ovirt.test.local/ovirt-engine/services/pki-resource?resource=ca-certificate&amp;format=X509-PEM-CA`</code> </p> </div></div><br><p>     oVirt  ¬´ <strong>  </strong> ¬ª (Trusted Root CA)    ,    . </p><br><p>     Trusted Root CA,   " <em>Test connection</em> ",  : </p><br><pre> <code class="plaintext hljs">Connection to ovirt-imageio-proxy was successful.</code> </pre> <br><p>     ,      ISO  Storage Domain. </p><br><p>  ,    Storage Domain   Data,         ,      Storage Domain  hosted engine,      . </p><br><div class="spoiler"> <b class="spoiler_title">   ISO  Storage Domain  hosted engine</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/1c/eg/vl/1cegvlrj_jio2lyrw8bzwwxqik0.png"></p></div></div><br><h4 id="sozdanie-virtualnoy-mashiny">    </h4><br><p>   : <br> oVirt Virtual Machine Management Guide ‚Äì&gt; <a href="https://www.ovirt.org/documentation/vmm-guide/chap-Installing_Linux_Virtual_Machines.html">Chapter 2: Installing Linux Virtual Machines <br> Console Clients Resources</a> </p><br><p>    oVirt    ,       .    ,       ,       ‚Äì        .       ‚Äì      -   ,   . </p><br><p>      CentOS 7,       . </p><br><p>    ,   <strong>Compute</strong> &gt;&gt; <strong>Virtual Machines</strong> ,     .     ,   <em></em> .   ,   . </p><br><p>   ,       ,   ,   ,      : </p><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/b-/ax/5r/b-ax5rq6w4tfzjp26f0x_oassb8.png"></p><br><p><img src="https://habrastorage.org/webt/zf/og/yc/zfogycv0ghsys-pjj6xl4f6uj64.png"></p><br><p><img src="https://habrastorage.org/webt/da/k9/9s/dak99sg1l7ktd-_kujmswznaveq.png"></p><br><p><img src="https://habrastorage.org/webt/ej/ra/rr/ejrarrisp1wubzm2-cdsawkq0kq.png"></p><br><p><img src="https://habrastorage.org/webt/ka/gi/mv/kagimvvhxi2bei9i-n6gp-ozbse.png"></p></div></div><br><p>     ,  ,        . <br>          : </p><br><div class="spoiler"> <b class="spoiler_title">        </b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/ph/lz/vv/phlzvvb-isibuzvomxhg-ligxtm.png"></p></div></div><br><p>     ,        . </p><br><div class="spoiler"> <b class="spoiler_title">  ,  ¬´Console¬ª</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/ay/w8/7h/ayw87h-2eubjlamhixkkap2j2ag.png"></p></div></div><br><p>       , , <a href="https://virt-manager.org/">Virtual Machine Viewer</a> . </p><br><p>         ,       : </p><br><p><img src="https://habrastorage.org/webt/wu/yj/nm/wuyjnmj_wk_qm_hmzffxkrji0dw.png"></p><br><p>     ,   oVirt guest agent: </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum install -y ovirt-guest-agent-common systemctl enable ovirt-guest-agent.service &amp;&amp; systemctl restart ovirt-guest-agent.service systemctl status ovirt-guest-agent.service</code> </pre> <br><p>  ,    ,    , ..     ,    , oVirt      .           ,   . </p><br><h4 id="zaklyuchenie">  Conclusion </h4><br><p> ,     ,  oVirt ‚Äì       ,       ‚Äî      ,    ,    . </p><br><p> -          ,           ,   - ,  ..          ,     , -         .   ‚Äì   ,     ,             . </p><br><p>     ,           : ,  ,  ,     . </p><br><p>           ,       ‚Äî     VyOS      (  ,           oVirt). </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr485208/">https://habr.com/ru/post/fr485208/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr485198/index.html">Une autre fa√ßon de mesurer les performances des m√©thodes d'application .NET</a></li>
<li><a href="../fr485200/index.html">D√©sordre au d√©part: post-mortem sur la vitesse de lancement d'une application iOS</a></li>
<li><a href="../fr485202/index.html">Syst√®me de suppression</a></li>
<li><a href="../fr485204/index.html">Retour en haut: pourquoi la capitalisation d'Amazon d√©passera bient√¥t encore 1 billion de dollars</a></li>
<li><a href="../fr485206/index.html">Comment Typescript m'a-t-il d√©√ßu et cela en vaut-il la peine?</a></li>
<li><a href="../fr485210/index.html">Tireur de zombies simple sur Unity</a></li>
<li><a href="../fr485214/index.html">CLRium # 7: Pratique. S√©minaire, devoirs avec v√©rification, mentorat</a></li>
<li><a href="../fr485218/index.html">R√©servation de constantes et de crochets Git en C #</a></li>
<li><a href="../fr485220/index.html">L'√©volution du pare-feu d'applications Web: des pare-feu aux syst√®mes de s√©curit√© bas√©s sur le cloud d'apprentissage automatique</a></li>
<li><a href="../fr485222/index.html">Comment travailler avec des leaders d'opinion en Chine? Cinq conseils pratiques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>