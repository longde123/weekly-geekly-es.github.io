<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì¥ üéâ üçå Pr√©sentation de NeurIPS-2018 (ex. NIPS) üéø üë®üèø‚Äç‚úàÔ∏è üëµüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Au d√©but de d√©cembre, Montr√©al a accueilli la 32e conf√©rence annuelle des syst√®mes de traitement de l'information neuronale sur l'apprentissage automa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©sentation de NeurIPS-2018 (ex. NIPS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/434694/"> Au d√©but de d√©cembre, Montr√©al a accueilli la 32e conf√©rence annuelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des syst√®mes de traitement de l'information neuronale</a> sur l'apprentissage automatique.  Selon un tableau de classement non officiel, cette conf√©rence est le premier √©v√©nement de ce format au monde.  Tous les billets de conf√©rence de cette ann√©e ont √©t√© vendus en un temps record de 13 minutes.  Nous avons une grande √©quipe de scientifiques des donn√©es MTS, mais une seule d'entre elles - Marina Yaroslavtseva ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">magoli</a> ) - a eu la chance de se rendre √† Montr√©al.  Avec Danila Savenkov ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">danila_savenkov</a> ), rest√©e sans visa et qui a suivi la conf√©rence de Moscou, nous parlerons des travaux qui nous ont paru les plus int√©ressants.  Cet √©chantillon est tr√®s subjectif, mais j'esp√®re qu'il vous int√©ressera. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c10/868/6af/c108686af497e18c338a44c475d6d64e.png" alt="image"><br><a name="habracut"></a><br>  <b>R√©seaux neuronaux r√©currents relationnels</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  Lorsque vous travaillez avec des s√©quences, il est souvent tr√®s important de savoir comment les √©l√©ments de la s√©quence sont li√©s les uns aux autres.  L'architecture standard des r√©seaux de r√©currence (GRU, LSTM) peut difficilement mod√©liser la relation entre deux √©l√©ments assez √©loign√©s l'un de l'autre.  Dans une certaine mesure, l'attention aide √† y faire face ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://youtu.be/SysgYptB198</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://youtu.be/quoGRI-1l0A</a> ), mais ce n'est toujours pas tout √† fait raison.  L'attention vous permet de d√©terminer le poids avec lequel l'√©tat cach√© de chacune des √©tapes de la s√©quence affectera l'√©tat cach√© final et, par cons√©quent, la pr√©diction.  Nous nous int√©ressons √† la relation des √©l√©ments de la s√©quence. <br><br>  L'ann√©e derni√®re, toujours sur NIPS, Google a sugg√©r√© d'abandonner compl√®tement la r√©cidive et d'utiliser l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">auto-attention</a> .  L'approche s'est av√©r√©e tr√®s bonne, mais principalement sur les t√¢ches seq2seq (l'article fournit des r√©sultats sur la traduction automatique). <br><br>  L'article de cette ann√©e utilise l'id√©e de l'attention personnelle dans le cadre du LSTM.  Il n'y a pas beaucoup de changements: <br><br><ol><li>  Nous changeons le vecteur d'√©tat cellulaire en matrice ¬´m√©moire¬ª M. Dans une certaine mesure, la matrice m√©moire est constitu√©e de nombreux vecteurs d'√©tat cellulaire (de nombreuses cellules m√©moire).  En obtenant un nouvel √©l√©ment de la s√©quence, nous d√©terminons combien cet √©l√©ment doit mettre √† jour chacune des cellules de m√©moire. </li><li>  Pour chaque √©l√©ment de la s√©quence, nous mettrons √† jour cette matrice en utilisant l'attention du produit scalaire √† plusieurs t√™tes (MHDPA, vous pouvez en savoir plus sur cette m√©thode dans l'article mentionn√© de Google).  Le r√©sultat MHPDA pour l'√©l√©ment courant de la s√©quence et de la matrice M est ex√©cut√© √† travers un maillage enti√®rement connect√©, le sigmo√Øde puis la matrice M est mis √† jour de la m√™me mani√®re que l'√©tat de cellule dans LSTM </li></ol><br>  On fait valoir que c'est par le biais de MHDPA que le maillage peut prendre en compte l'interconnexion des √©l√©ments de s√©quence m√™me lorsqu'ils sont retir√©s les uns des autres. <br><br>  En tant que probl√®me de jouet, le mod√®le est demand√© dans la s√©quence de vecteurs pour trouver le N√®me vecteur par distance du M√®me en termes de distance euclidienne.  Par exemple, il y a une s√©quence de 10 vecteurs et nous vous demandons d'en trouver un qui est √† la troisi√®me place √† proximit√© du cinqui√®me.  Il est clair que pour r√©pondre √† cette question du mod√®le, il faut en quelque sorte √©valuer les distances de tous les vecteurs au cinqui√®me et les trier.  Ici, le mod√®le propos√© par les auteurs bat en toute confiance le LSTM et le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DNC</a> .  De plus, les auteurs comparent leur mod√®le avec d'autres architectures sur Learning to Execute (nous obtenons quelques lignes de code √† saisir, donnons le r√©sultat), Mini-Pacman, Language Modeling et partout rapportent les meilleurs r√©sultats. <br><br>  <b>Imputation de s√©ries chronologiques multivari√©es avec des r√©seaux contradictoires g√©n√©ratifs</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> (bien qu'ils ne soient pas li√©s ici dans l'article) <br><br>  Dans les s√©ries chronologiques multidimensionnelles, en r√®gle g√©n√©rale, il y a un grand nombre d'omissions, ce qui emp√™che l'utilisation de m√©thodes statistiques avanc√©es.  Les solutions standard - remplir avec une moyenne / z√©ro, supprimer des cas incomplets, restaurer des donn√©es bas√©es sur des expansions matricielles dans cette situation, ne fonctionnent souvent pas, car elles ne peuvent pas reproduire les d√©pendances temporelles et la distribution complexe de s√©ries chronologiques multidimensionnelles. <br><br>  La capacit√© des r√©seaux contradictoires g√©n√©ratifs (GAN) √† imiter toute distribution de donn√©es, en particulier dans les t√¢ches de ¬´dessin¬ª de visages et de g√©n√©ration de phrases, est largement connue.  Mais, en r√®gle g√©n√©rale, ces mod√®les n√©cessitent soit une formation initiale sur un ensemble complet de donn√©es sans lacunes, soit ne tiennent pas compte de la nature coh√©rente des donn√©es. <br><br>  Les auteurs proposent de compl√©ter le GAN avec un nouvel √©l√©ment - le Gated Recurrent Unit for Imputation (GRUI).  La principale diff√©rence avec le GRU habituel est que le GRUI peut apprendre des donn√©es √† des intervalles de diff√©rentes longueurs entre les observations et ajuster l'effet des observations en fonction de leur distance dans le temps du point actuel.  Un param√®tre d'att√©nuation sp√©cial Œ≤ est calcul√©, dont la valeur varie de 0 √† 1 et plus elle est petite, plus le d√©calage temporel entre l'observation en cours et la pr√©c√©dente non vide est grand. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ab/c71/63b/6abc7163bf3ff1b16310104100b53236.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/203/a64/599/203a6459932c852ad827db0d9574ef2c.png" alt="image"><br><br>  Le discriminateur et le g√©n√©rateur GAN sont tous deux constitu√©s d'une couche GRUI et d'une couche enti√®rement connect√©e.  Comme d'habitude dans les GAN, le g√©n√©rateur apprend √† simuler les donn√©es source (dans ce cas, il suffit de combler les lacunes dans les lignes), et le discriminateur apprend √† distinguer les lignes remplies avec le g√©n√©rateur des vraies. <br><br>  Il s'est av√©r√© que cette approche restitue de mani√®re tr√®s ad√©quate les donn√©es m√™me dans les s√©ries chronologiques avec une tr√®s grande part d'omissions (dans le tableau ci-dessous - R√©cup√©ration des donn√©es MSE dans l'ensemble de donn√©es KDD en fonction du pourcentage d'omissions et de la m√©thode de r√©cup√©ration. Dans la plupart des cas, la m√©thode bas√©e sur GAN donne la plus grande pr√©cision r√©cup√©ration). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/188/737/e57/188737e575173058d1c0a79482b8679d.png" alt="image"><br><br>  <b>Sur la dimensionnalit√© des int√©grations de mots</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  L'incorporation de mots / repr√©sentation vectorielle de mots est une approche largement utilis√©e pour diverses applications de PNL: des syst√®mes de recommandation √† l'analyse de la coloration √©motionnelle des textes et de la traduction automatique. <br><br>  De plus, la question de savoir comment r√©gler de mani√®re optimale un hyperparam√®tre aussi important que la dimension des vecteurs reste ouverte.  En pratique, il est le plus souvent s√©lectionn√© par recherche empirique exhaustive ou fix√© par d√©faut, par exemple au niveau de 300. Dans le m√™me temps, une dimension trop petite ne permet pas de refl√©ter toutes les relations significatives entre les mots, et trop grande peut conduire √† une reconversion. <br><br>  Les auteurs de l'√©tude proposent leur solution √† ce probl√®me en minimisant le param√®tre de perte PIP, une nouvelle mesure de la diff√©rence entre les deux options d'int√©gration. <br>  Le calcul est bas√© sur des matrices PIP qui contiennent les produits scalaires de toutes les paires de repr√©sentations vectorielles de mots dans le corpus.  La perte PIP est calcul√©e comme la norme Frobenius entre les matrices PIP de deux plongements: entra√Æn√©e sur les donn√©es (entra√Æn√©e par l'int√©gration E_hat) et id√©ale, entra√Æn√©e sur les donn√©es bruyantes (incorpor√©e par l'oracle E). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/135/a0a/1f2/135a0a1f216137c64ab73f9d2b86f0dc.png" alt="image" width="300" height="200"></div><br><br>  Cela semblerait simple: vous devez choisir une dimension qui minimise la perte de PIP, le seul moment incompr√©hensible est o√π obtenir l'int√©gration d'Oracle.  En 2015-2017, un certain nombre d'ouvrages ont √©t√© publi√©s dans lesquels il a √©t√© montr√© que diff√©rentes m√©thodes de construction des plongements (word2vec, GloVe, LSA) factorisent implicitement (abaissent la dimension) la matrice de signal de l'affaire.  Dans le cas de word2vec (skip-gram), la matrice de signal est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PMI</a> , dans le cas de GloVe, c'est la matrice de log-count.  Il est propos√© de prendre un dictionnaire de taille pas tr√®s grande, de construire une matrice de signaux et d'utiliser SVD pour obtenir l'int√©gration d'Oracle.  Ainsi, la dimension d'int√©gration d'oracle est √©gale au rang de la matrice du signal (en pratique, pour un dictionnaire de 10k mots, la dimension sera de l'ordre de 2k).  Cependant, notre matrice de signaux empiriques est toujours bruyante et nous devons recourir √† des sch√©mas d√©licats pour obtenir l'int√©gration d'Oracle et estimer la perte de PIP par une matrice bruyante. <br><br>  Les auteurs soutiennent que pour s√©lectionner la dimension d'int√©gration optimale, il suffit d'utiliser un dictionnaire de 10 000 mots, ce qui n'est pas beaucoup et vous permet d'ex√©cuter cette proc√©dure dans un d√©lai raisonnable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/454/806/a20/454806a208dd7c10df8b9cbd365c440a.png" alt="image"><br><br>  Il s'est av√©r√© que la dimension d'int√©gration calcul√©e de cette mani√®re dans la plupart des cas avec une erreur allant jusqu'√† 5% co√Øncide avec la dimension optimale d√©termin√©e sur la base d'estimations d'experts.  Il s'est av√©r√© (attendu) que Word2Vec et GloVe ne se sont pratiquement pas recycl√©s (la perte de PIP ne chute pas √† de tr√®s grandes dimensions), mais LSA est recycl√© assez fortement. <br><br>  En utilisant le code affich√© sur le github par les auteurs, on peut rechercher la dimension optimale de Word2Vec (skip-gram), GloVe, LSA. <br><br>  <b>FRAGE: Repr√©sentation des mots agnostiques en fr√©quence</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  Les auteurs expliquent comment les int√©grations fonctionnent diff√©remment pour les mots rares et populaires.  Par populaire, je veux dire non pas des mots (nous ne les consid√©rons pas du tout), mais des mots informatifs qui ne sont pas tr√®s rares. <br><br>  Les observations sont les suivantes: <br><br>  Si nous parlons de mots populaires, leur proximit√© en mesure de cosinus se refl√®te tr√®s bien <br><br><ol><li>  leur affinit√© s√©mantique.  Pour les mots rares, ce n'est pas le cas (ce qui est attendu), et (ce qui est moins attendu) les n premiers des mots cosinus les plus proches d'un mot rare sont √©galement rares et en m√™me temps s√©mantiquement sans rapport.  Autrement dit, les mots rares et fr√©quents dans l'espace des plongements vivent dans des endroits diff√©rents (dans des c√¥nes diff√©rents, si nous parlons de cosinus) </li><li>  Pendant la formation, les vecteurs de mots populaires sont mis √† jour beaucoup plus souvent et, en moyenne, sont deux fois plus loin de l'initialisation que les vecteurs de mots rares.  Cela conduit au fait que l'incorporation de mots rares est en moyenne plus proche de l'origine.  Pour √™tre honn√™te, j'ai toujours cru que, au contraire, les incorporations de mots rares sont en moyenne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plus longues</a> et je ne sais pas comment me rapporter √† la d√©claration des auteurs =) </li></ol><br>  Quelle que soit la relation entre les normes L2 des plongements, la s√©parabilit√© des mots populaires et rares n'est pas un tr√®s bon ph√©nom√®ne.  Nous voulons que les plongements refl√®tent la s√©mantique d'un mot, pas sa fr√©quence. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7e/f4d/e70/c7ef4de7034a8347269fa40d6bdc7005.png" alt="image"><br><br>  L'image montre les mots Word2Vec populaires (rouges) et rares (bleus) apr√®s SVD.  Populaire ici fait r√©f√©rence aux 20% de mots les plus fr√©quents. <br><br>  Si le probl√®me ne concernait que les normes L2 des plongements, nous pourrions les normaliser et vivre heureux, mais, comme je l'ai dit dans le premier paragraphe, les mots rares sont √©galement s√©par√©s des mots populaires par la proximit√© cosinus (en coordonn√©es polaires). <br><br>  Les auteurs sugg√®rent, bien s√ªr, le GAN.  Faisons la m√™me chose qu'avant, mais ajoutons un discriminateur qui essaiera de faire la distinction entre les mots populaires et les mots rares (encore une fois, nous consid√©rons que les n-premiers% des mots en fr√©quence sont populaires). <br><br>  Cela ressemble √† ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/0a3/0b7/f0e0a30b77a9071ff63fa208401fad56.png" alt="image"><br><br>  Les auteurs testent l'approche sur les t√¢ches de similitude de mots, de traduction automatique, de classification de texte et de mod√©lisation de langage et partout o√π elles fonctionnent mieux que la ligne de base.  Dans la similitude des mots, il est indiqu√© que la qualit√© augmente particuli√®rement sur les mots rares. <br><br>  Un exemple: la citoyennet√©.  Probl√®mes de saut de gramme: bonheur, pakistans, rejet, renforcement.  Probl√®mes FRAGE: population, st√§dtischen, dignit√©, b√ºrger.  Les mots citoyen et citoyens dans FRAGE sont respectivement en 79e et 7e places (√† proximit√© de la citoyennet√©), en saut de gramme ils ne sont pas dans le top 10000. <br><br>  Pour une raison quelconque, les auteurs ont publi√© le code uniquement pour la traduction automatique et la mod√©lisation de la langue, la similitude des mots et les t√¢ches de classification de texte dans le r√©f√©rentiel, malheureusement, ne sont pas repr√©sent√©s. <br><br>  <b>Alignement intermodal non supervis√© des espaces d'int√©gration de la parole et du texte</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  Code: pas de code, mais j'aimerais <br><br>  Des √©tudes r√©centes ont montr√© que deux espaces vectoriels form√©s √† l'aide d'algorithmes d'int√©gration (par exemple, word2vec) sur des corps de texte dans deux langues diff√©rentes peuvent √™tre mis en correspondance sans marquage ni correspondance de contenu entre les deux b√¢timents.  En particulier, cette approche est utilis√©e pour la traduction automatique sur Facebook.  L'une des propri√©t√©s cl√©s de l'int√©gration des espaces est utilis√©e: √† l'int√©rieur, des mots similaires doivent √™tre g√©om√©triquement proches et des mots diff√©rents, au contraire, doivent √™tre √©loign√©s les uns des autres.  On suppose que, en g√©n√©ral, la structure de l'espace vectoriel est pr√©serv√©e quelle que soit la langue dans laquelle le corpus √©tait destin√© √† l'enseignement. <br><br>  Les auteurs de l'article sont all√©s plus loin et ont appliqu√© une approche similaire dans le domaine de la reconnaissance et de la traduction automatiques de la parole.  Il est propos√© de former l'espace vectoriel s√©par√©ment pour le corpus de texte dans la langue d'int√©r√™t (par exemple, Wikipedia), s√©par√©ment pour le corpus de la parole enregistr√©e (au format audio), √©ventuellement dans une autre langue, pr√©alablement d√©compos√©e en mots, puis de comparer ces deux espaces de la m√™me mani√®re qu'avec deux cas de texte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d07/950/b6f/d07950b6f9bac05e9ce48dd80bb24838.png" alt="image"><br><br>  Pour le corpus de texte, word2vec est utilis√©, et pour la parole, une approche similaire, appel√©e par Speech2vec, est bas√©e sur LSTM et les m√©thodologies utilis√©es pour word2vec (CBOW / skip-gram), il est donc suppos√© qu'il combine les mots pr√©cis√©ment par des caract√©ristiques contextuelles et s√©mantiques, et ne sonne pas. <br><br>  Une fois que les deux espaces vectoriels ont √©t√© form√©s et qu'il y a deux ensembles de plongements - S (sur le corps du discours), compos√© de n incorporations de dimension d1 et T (sur le corps du texte), compos√© de m incorporations de dimension d2, vous devez les comparer.  Id√©alement, nous avons un dictionnaire qui d√©termine quel vecteur de S correspond √† quel vecteur de T. Ensuite, deux matrices sont form√©es pour comparaison: k plongements sont s√©lectionn√©s parmi S, qui forment une matrice X de taille d1 xk;  √† partir de T, k plongements correspondants (selon le dictionnaire) pr√©c√©demment s√©lectionn√©s √† partir de S sont √©galement s√©lectionn√©s, et une matrice Y de taille d2 x k est obtenue.  Ensuite, vous devez trouver un mappage lin√©aire W tel que: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db3/59e/e40/db359ee40ad9f907c6461d378db3d7aa.png" alt="image" width="300" height="200"></div><br><br>  Mais comme l'article consid√®re l'approche non supervis√©e, il n'y a initialement pas de dictionnaire, par cons√©quent, une proc√©dure pour g√©n√©rer un dictionnaire synth√©tique, compos√© de deux parties, est propos√©e.  Premi√®rement, nous obtenons la premi√®re approximation de W en utilisant l'entra√Ænement par domaine (un mod√®le comp√©titif comme GAN, mais au lieu du g√©n√©rateur - une cartographie lin√©aire de W, avec laquelle nous essayons de rendre S et T indiscernables les uns des autres, et le discriminateur essaie de d√©terminer l'origine r√©elle de l'int√©gration).  Ensuite, sur la base des mots dont les plongements ont montr√© la meilleure correspondance entre eux et se trouvent le plus souvent dans les deux b√¢timents, un dictionnaire est form√©.  Apr√®s cela, le raffinement de W conform√©ment √† la formule ci-dessus se produit. <br><br>  Cette approche donne des r√©sultats comparables √† l'apprentissage sur des donn√©es √©tiquet√©es, ce qui peut √™tre tr√®s utile dans la t√¢che de reconnaissance et de traduction de la parole de langues rares pour lesquelles il y a trop peu de cas de parole / texte parall√®les, ou ils sont absents. <br><br>  <b>D√©tection d'anomalies profondes √† l'aide de transformations g√©om√©triques</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  Une approche assez inhabituelle dans la d√©tection des anomalies, qui, selon les auteurs, d√©fait grandement les autres approches. <br><br>  L'id√©e est la suivante: imaginons K diff√©rentes transformations g√©om√©triques (combinaison de d√©calages, rotation √† 90 degr√©s et r√©flexion) et appliquons-les √† chaque image de l'ensemble de donn√©es d'origine.  L'image obtenue √† la suite de la i-√®me transformation appartiendra d√©sormais √† la classe i, c'est-√†-dire qu'il y aura K classes au total, chacune d'entre elles sera repr√©sent√©e par le nombre d'images qui √©taient √† l'origine dans l'ensemble de donn√©es.  Nous allons maintenant enseigner une classification multiclasse sur un tel balisage (les auteurs ont choisi un large resnet). <br><br>  Nous pouvons maintenant obtenir K vecteurs y (Ti (x)) de dimension K pour une nouvelle image, o√π Ti est la i-√®me transformation, x est l'image, y est la sortie du mod√®le.  La d√©finition de base de la ¬´normalit√©¬ª est la suivante: <br><br>  Ici, pour l'image x, nous avons ajout√© les probabilit√©s pr√©dites des classes correctes pour toutes les transformations.  Plus la ¬´normalit√©¬ª est √©lev√©e, plus il est probable que l'image soit tir√©e de la m√™me distribution que l'√©chantillon d'apprentissage.  Les auteurs affirment que cela fonctionne d√©j√† tr√®s bien, mais offrent n√©anmoins un moyen plus complexe qui fonctionne encore un peu mieux.  Nous supposerons que le vecteur y (Ti (x)) pour chaque transformation de Ti est distribu√© par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dirichlet</a> et nous prendrons le logarithme de vraisemblance comme mesure de la ¬´normalit√©¬ª de l'image.  Les param√®tres de distribution de Dirichlet sont estim√©s sur un ensemble d'apprentissage. <br><br>  Les auteurs rapportent l'incroyable augmentation des performances par rapport √† d'autres approches. <br><br>  <b>Un cadre unifi√© simple pour d√©tecter les √©chantillons hors distribution et les attaques contradictoires</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  L'identification dans l'√©chantillon pour l'application du mod√®le de cas significativement diff√©rent de la distribution de l'√©chantillon de formation est l'une des principales exigences pour obtenir des r√©sultats de classification fiables.  Dans le m√™me temps, les r√©seaux de neurones sont connus pour leur fonctionnalit√© avec un haut degr√© de confiance (et de mani√®re incorrecte) pour classer les objets qui n'ont pas √©t√© rencontr√©s lors de l'entra√Ænement ou intentionnellement corrompus (exemples contradictoires). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1e0/780/f35/1e0780f3540fcb4ba894f38a40c31cbb.png" alt="image"><br><br>  Les auteurs de l'article proposent une nouvelle m√©thode pour identifier √† la fois ces cas et d'autres ¬´mauvais¬ª cas.  L'approche est mise en ≈ìuvre comme suit: d'abord, un r√©seau de neurones avec la sortie softmax habituelle est form√©, puis la sortie de son avant-derni√®re couche est prise, et le classificateur g√©n√©ratif est form√© sur elle.  Soit x - qui est appliqu√© √† l'entr√©e du mod√®le pour un objet de classification particulier, y - l'√©tiquette de classe correspondante, puis supposons que nous avons un classificateur softmax pr√©-form√© de la forme: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/223/898/826/223898826445e2677c03a2078b182208.png" alt="image" width="300" height="200"></div><br><br>  O√π wc et bc sont les poids et constantes de la couche softmax pour la classe c, et f (.) Est la sortie de l'avant-dernier soya DNN. <br><br>  De plus, sans aucune modification du classificateur pr√©-form√©, une transition est effectu√©e vers le classificateur g√©n√©ratif, √† savoir l'analyse discriminante.  On suppose que les entit√©s issues de l'avant-derni√®re couche du classificateur softmax ont une distribution normale multidimensionnelle, dont chaque composante correspond √† une classe.  Ensuite, la distribution conditionnelle peut √™tre sp√©cifi√©e √† travers le vecteur de moyennes de la distribution multidimensionnelle et sa matrice de covariance: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ada/0d8/e70/ada0d8e70536c7993a6061b154f3c0eb.png" alt="image" width="300" height="200"></div><br><br>  Pour √©valuer les param√®tres du classificateur g√©n√©ratif, des moyennes empiriques sont calcul√©es pour chaque classe, ainsi que la covariance des cas de l'√©chantillon d'apprentissage {(x1, y1), ..., (xN, yN)}: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ada/0d8/e70/ada0d8e70536c7993a6061b154f3c0eb.png" alt="image" width="300" height="200"></div><br><br>  o√π N est le nombre de cas de la classe correspondante dans l'ensemble d'apprentissage.  Ensuite, une mesure de fiabilit√© est calcul√©e sur l'√©chantillon de test - la distance de Mahalanobis entre le cas de test et la distribution de classe normale la plus proche de ce cas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/095/928/03b/09592803b18f664fad5940fc81d47f77.png" alt="image" width="400" height="300"></div><br><br>  Il s'est av√©r√© qu'une telle m√©trique fonctionnait de mani√®re beaucoup plus fiable sur des objets atypiques ou endommag√©s, sans donner d'estimations √©lev√©es, comme la couche softmax.  Dans la plupart des comparaisons sur diff√©rentes donn√©es, la m√©thode propos√©e a montr√© des r√©sultats qui d√©passaient l'√©tat actuel de la technique pour trouver les deux cas qui n'√©taient pas dans la formation et intentionnellement g√¢t√©s. <br><br>  De plus, les auteurs envisagent une autre application int√©ressante de leur m√©thodologie: utiliser le classificateur g√©n√©ratif pour mettre en √©vidence de nouvelles classes qui n'√©taient pas en formation sur le test, puis mettre √† jour les param√®tres du classificateur lui-m√™me afin qu'il puisse d√©terminer cette nouvelle classe √† l'avenir. <br><br>  <b>Exemples contradictoires qui trompent √† la fois la vision par ordinateur et les humains limit√©s dans le temps</b> <br>  R√©sum√©: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://arxiv.org/abs/1802.08195</a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les auteurs examinent quels sont les exemples contradictoires en termes de perception humaine. Aujourd'hui, il n'est √©tonnant pour personne que vous ne puissiez presque jamais changer l'image pour que le r√©seau fasse des erreurs dessus. Cependant, il n'est pas tr√®s clair √† quel point l'image originale diff√®re de l'exemple contradictoire pour une personne et si elle diff√®re du tout. Il est clair qu'aucune personne n'appellera l'image de droite une autruche, mais peut-√™tre que l'image de droite pour une personne n'est pas compl√®tement identique √† l'image de gauche et, si tel est le cas, une personne peut √©galement faire l'objet d'attaques accusatoires. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/b13/cf0/859b13cf0bb3c14ac58eeab39b5a0945.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les auteurs tentent d'√©valuer dans quelle mesure une personne sera capable de classer des exemples contradictoires. Pour obtenir des exemples contradictoires, on utilise une technique qui n'a pas acc√®s √† l'architecture du r√©seau source (la logique des auteurs est qu'ils n'auront de toute fa√ßon pas acc√®s √† l'architecture du cerveau humain).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ainsi, une personne se voit montrer un exemple contradictoire, comme dans l'image ci-dessus, et est invit√©e √† le classer. Il est clair que dans des conditions normales, le r√©sultat serait pr√©visible, mais ici, une image est montr√©e √† une personne dans les 63 millisecondes, apr√®s quoi elle doit choisir l'une des deux classes. Dans de telles conditions, la pr√©cision des images source √©tait 10% plus √©lev√©e que celle de la confrontation. En principe, cela pourrait s'expliquer par le fait que l'image contradictoire est simplement bruyante et donc, dans des conditions de pression temporelle, les gens la classent incorrectement, mais cela r√©fute l'exp√©rience suivante. Si avant d'ajouter une perturbation √† l'image, nous refl√©tons cette perturbation verticalement, alors la pr√©cision ne changera gu√®re par rapport √† l'image d'origine.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/f4d/e03/966f4de0389283dd83f73a1be2cf36cb.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sur l'histogramme, adv est un exemple contradictoire, l'image est l'image originale, flip est l'image originale + perturbation contradictoire, r√©fl√©chie verticalement. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Contr√¥les de sant√© mentale pour les cartes de saillance </font></font></b> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L'</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> interpr√©tation du mod√®le </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">abstrait</font></a><font style="vertical-align: inherit;"> est l'un des sujets les plus discut√©s aujourd'hui. </font><font style="vertical-align: inherit;">En ce qui concerne l'apprentissage en profondeur, ils parlent g√©n√©ralement de cartes de saillance. </font><font style="vertical-align: inherit;">Les cartes de saillance tentent de r√©pondre √† la question de savoir comment la valeur change √† l'une des sorties de la grille lorsque les valeurs d'entr√©e changent. </font><font style="vertical-align: inherit;">Cela peut ressembler √† une carte de saillance, qui montre quels pixels ont influenc√© le fait que l'image a √©t√© class√©e comme un ¬´chien¬ª. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa5/2c5/659/aa52c5659d4c00d7666661797671c4b9.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les auteurs posent une question tr√®s raisonnable: "Comment validerions-nous les m√©thodes de construction des cartes de saillance?" </font><font style="vertical-align: inherit;">Deux points √©vidents sont avanc√©s et propos√©s √† v√©rifier:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La carte de saillance devrait d√©pendre des poids de la grille </font></font></li><li> Saliency map    ,     </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous v√©rifierons la premi√®re th√®se en rempla√ßant les poids dans la grille form√©e par un al√©atoire: randomisation en cascade (randomiser les couches √† partir de la derni√®re et voir comment la carte de saillance change) et randomisation ind√©pendante (randomiser une couche sp√©cifique). Nous allons v√©rifier la deuxi√®me th√®se comme ceci: m√©langer au hasard toutes les √©tiquettes sur le train, √©quiper le train et regarder les cartes de saillance. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si la m√©thode de construction d'une carte de saillance est vraiment bonne et vous permet de comprendre comment fonctionne le mod√®le, de telles randomisations devraient changer consid√©rablement les cartes de saillance. Cependant: ¬´√Ä notre grande surprise, certaines m√©thodes de saillance largement d√©ploy√©es sont ind√©pendantes √† la fois des donn√©es sur lesquelles le mod√®le a √©t√© form√© et des param√®tres du mod√®le¬ª, d√©clarent les auteurs. Ici, par exemple, ressemble √† des cartes de saillance obtenues √† l'aide de divers algorithmes apr√®s randomisation en cascade:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/965/eaa/ca7/965eaaca7902d24ef5369a63d493b4ff.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Notez le fait amusant que la derni√®re colonne correspond √† une grille avec des poids al√©atoires dans toutes les couches. Autrement dit, la grille pr√©dit au hasard, mais certaines cartes de saillance dessinent toujours un oiseau. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les auteurs disent √† juste titre que - une √©valuation des cartes de saillance par leur compr√©hensibilit√© et leur logique et une attention insuffisante √† la mani√®re dont le r√©sultat est g√©n√©ralement li√© au fonctionnement du mod√®le conduit √† un biais de confirmation. Apparemment, y compris pour cette raison, il s'av√®re que les approches communes √† l'interpr√©tation des mod√®les ne les interpr√®tent pas du tout. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Une d√©faillance intrigante des r√©seaux de neurones convolutifs et de la solution CoordConv</font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> R√©sum√©: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://arxiv.org/abs/1807.03247</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Code: il existe d√©j√† de nombreuses impl√©mentations et en g√©n√©ral l'id√©e est si belle et simple qu'elle est √©crite litt√©ralement en 10 lignes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Facile √† mettre en ≈ìuvre et id√©e prometteuse d'Uber. Les r√©seaux convolutifs ont √©t√© initialement affin√©s pour l'invariance de cisaillement, de sorte que les t√¢ches associ√©es √† la d√©termination des coordonn√©es d'un objet sont tr√®s difficiles pour de tels r√©seaux. Les r√©seaux convolutionnels conventionnels ne sont m√™me pas capables de r√©soudre des probl√®mes de jouets tels que la d√©termination des coordonn√©es d'un point dans une image ou le dessin d'un point par des coordonn√©es: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ed/408/c14/6ed408c1456a509c86a6260df5c5a23b.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un hack tr√®s √©l√©gant est propos√©: ajoutez deux matrices i et j √† l'image (en g√©n√©ral, √† l'entr√©e de la couche CoodrConv), dans laquelle contient les coordonn√©es verticales et horizontales des pixels correspondants: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/87e/8cb/a2a87e8cb6b7d283396a0fb3ef11fe30.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On pr√©tend que:</font></font><br><br><ol><li>       ImageNet'.         , ,   ,    ,         </li><li> CoordConv   object detection.        MNIST,      Faster R-CNN,    IoU  21% </li><li>   CoordConv  GAN    . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fee/a18/425/feea18425335abbb7d784669a1b82bcd.png" alt="image"><br><br>  GAN'   :                 LSUN.       ,     ‚Äî     c.  ,   GAN'    , ,         .   CoordConv         ,      .    LSUN   d ,     ,  CoordConv GAN,    <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. L'utilisation de CoordConv dans </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A2C</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> donne une augmentation dans certains jeux (pas tous).</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Personnellement, je suis le plus int√©ress√© par ce deuxi√®me point, j'aimerais voir les r√©sultats sur de vrais ensembles de donn√©es, mais rien n'est googl√© tout de suite. </font><font style="vertical-align: inherit;">Pendant ce temps, CoordConv est d√©j√† tr√®s activement int√©gr√© dans le </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">r√©seau</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Internet </font><font style="vertical-align: inherit;">: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://arxiv.org/abs/1812.01429,%2520"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://arxiv.org/abs/1812.01429, https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69274</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https: //github.com/mjDelta/Kaggle-RSNA-Pneumonia-Detection-Challenge</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il y a une bonne </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vid√©o</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> plus d√©taill√©e </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">de l'auteur</font></a><font style="vertical-align: inherit;"> . </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R√©gularisation par la variance du </font></font></b> <font style="vertical-align: inherit;"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;">code </font></a></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">abstrait d' </font></font></a> <font style="vertical-align: inherit;"><b><font style="vertical-align: inherit;">√©chantillon-variances des activations</font></b></font><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les auteurs offrent une alternative amusante √† la normalisation par lots. </font><font style="vertical-align: inherit;">Nous affinerons la grille pour la variabilit√© de la dispersion des activations sur une couche. </font><font style="vertical-align: inherit;">En pratique, ils l'impl√©mentent comme ceci: prenez deux sous-ensembles disjoints S1 et S2 du lot et calculez une telle chose:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e5f/2ea/97a/e5f2ea97aaebf6af472880bc2190b2cc.png" alt="image" width="300" height="200"></div><br><br>  o√π œÉ2 sont des variances d'√©chantillon dans S1 et S2, respectivement, Œ≤ est le coefficient positif entra√Æn√©.  Les auteurs appellent cette chose la perte de constance de la variance (VCL) et l'ajoutent √† la perte totale. <br><br>  Dans la section sur les exp√©riences, les auteurs se plaignent de la fa√ßon dont les r√©sultats des articles d'autres personnes ne sont pas reproduits et s'engagent √† mettre en place un code reproductible (pr√©sent√©).  Tout d'abord, ils ont exp√©riment√© avec un petit maillage √† 11 couches sur l'ensemble de donn√©es de petites images (CIFAR-10 et CIFAR-100).  Nous avons obtenu que VCL prouve, si vous utilisez Leaky ReLU ou ELU comme activations, mais la normalisation par lots fonctionne mieux avec ReLU.  Ensuite, ils augmentent le nombre de couches de 2 fois et passent √† Tiny Imagenet - une version simplifi√©e d'Imagenet avec 200 classes et une r√©solution de 64x64.  En validation, VCL surpasse la normalisation par lots sur la grille avec ELU, ainsi que ResNet-110 et DenseNet-40, mais surpasse Wide-ResNet-32.  Un point int√©ressant est que les meilleurs r√©sultats sont obtenus lorsque les sous-ensembles S1 et S2 sont constitu√©s de deux √©chantillons. <br><br>  De plus, les auteurs testent la VCL dans les r√©seaux √† action directe et la VCL gagne un peu plus souvent qu'un r√©seau avec normalisation par lots ou sans r√©gularisation. <br><br>  <b>DropMax: Softmax variationnel adaptatif</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©sum√©</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  Il est propos√© dans le probl√®me de classification multiclasse √† chaque it√©ration de la descente de gradient pour chaque √©chantillon de supprimer al√©atoirement un certain nombre de classes incorrectes.  De plus, la probabilit√© avec laquelle nous abandonnons l'une ou l'autre classe pour tel ou tel objet est √©galement en cours d'apprentissage.  En cons√©quence, il s'av√®re que le r√©seau ¬´se concentre¬ª sur la distinction entre les classes les plus difficiles √† s√©parer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b2d/f45/914/b2df45914519d3b6dd1d4f12d5513775.png" alt="image"><br><br>  Des exp√©riences sur les sous-ensembles MNIST, CIFAR et Imagenet montrent que DropMax fonctionne mieux que SoftMax standard et certaines de ses modifications. <br><br>  <b>Mod√®les intelligibles pr√©cis avec interactions par paires</b> <br>  (Les amis ne laissent pas les amis d√©ployer des mod√®les de bo√Æte noire: l'importance de l'intelligibilit√© dans l'apprentissage automatique) <br><br>  R√©sum√©: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf</a> <br>  Code: il n'est pas l√†.  Je suis tr√®s int√©ress√© par la fa√ßon dont les auteurs impr√®gnent un nom aussi imp√©ratif avec un manque de code.  Acad√©miciens, monsieur =) <br><br>  Vous pouvez consulter ce package, par exemple: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/dswah/pyGAM</a> .  Des interactions de fonctionnalit√©s y ont √©t√© ajout√©es il n'y a pas si longtemps (ce qui distingue en fait GAM de GA2M). <br><br>  Cet article a √©t√© pr√©sent√© dans le cadre de l'atelier ¬´Interpr√©tabilit√© et robustesse de l'audio, de la parole et du langage¬ª, bien qu'il soit consacr√© √† l'interpr√©tabilit√© des mod√®les en g√©n√©ral, et non au domaine de l'analyse du son et de la parole. Probablement, tout le monde a √©t√© confront√© dans une certaine mesure au dilemme de choisir entre l'interpr√©tabilit√© et sa pr√©cision.  Si nous utilisons la r√©gression lin√©aire habituelle, nous pouvons comprendre par les coefficients comment chaque variable ind√©pendante affecte la personne √† charge.  Si nous utilisons des mod√®les √† bo√Æte noire, par exemple, l'augmentation du gradient sans restrictions sur la complexit√© ou les r√©seaux de neurones profonds, un mod√®le correctement r√©gl√© sur des donn√©es appropri√©es sera tr√®s pr√©cis, mais le suivi et l'explication de tous les mod√®les que le mod√®le trouv√© dans les donn√©es sera probl√©matique.  Par cons√©quent, il sera difficile d'expliquer le mod√®le au client et de savoir s'il a appris quelque chose que nous n'aimerions pas.  Le tableau ci-dessous fournit des estimations de l'interpr√©tabilit√© et de la pr√©cision relatives de divers types de mod√®les. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fca/429/548/fca42954836ae74af472d4baacf732f1.png" alt="image"><br><br>  Un exemple de situation o√π une mauvaise interpr√©tabilit√© du mod√®le est associ√©e √† de grands risques: sur l'un des ensembles de donn√©es m√©dicales, le probl√®me de pr√©dire la probabilit√© du patient de mourir d'une pneumonie a √©t√© r√©solu.  Le sch√©ma int√©ressant suivant a √©t√© trouv√© dans les donn√©es: si une personne souffre d'asthme bronchique, la probabilit√© de mourir d'une pneumonie est plus faible que chez les personnes sans cette maladie.  Lorsque les chercheurs se sont tourn√©s vers les m√©decins praticiens, il s'est av√©r√© qu'un tel sch√©ma existe vraiment, car les personnes souffrant d'asthme dans le cas de la pneumonie re√ßoivent l'aide la plus rapide et des m√©dicaments puissants.  Si nous avions form√© xgboost sur cet ensemble de donn√©es, il aurait probablement d√©tect√© ce sch√©ma, et notre mod√®le classerait les patients asthmatiques comme un groupe √† faible risque et, en cons√©quence, leur recommanderait une priorit√© et une intensit√© de traitement inf√©rieures. <br><br>  Les auteurs de l'article proposent une alternative √† la fois interpr√©table et pr√©cise - il s'agit du GA2M, une sous-esp√®ce de mod√®les additifs g√©n√©ralis√©s. <br><br>  Le GAM classique peut √™tre consid√©r√© comme une g√©n√©ralisation suppl√©mentaire du GLM: un mod√®le est une somme dont chaque terme refl√®te l'influence d'une seule variable ind√©pendante sur la personne √† charge, mais l'influence n'est pas exprim√©e par un coefficient de pond√©ration, comme dans le GLM, mais par une fonction non param√©trique lisse (en r√®gle g√©n√©rale, d√©finie par morceaux fonctions - cannelures ou arbres de faible profondeur, y compris les "souches").  Gr√¢ce √† cette fonctionnalit√©, les GAM peuvent mod√©liser des relations plus complexes qu'un simple mod√®le lin√©aire.  D'un autre c√¥t√©, les d√©pendances (fonctions) apprises peuvent √™tre visualis√©es et interpr√©t√©es. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/643/828/ca8/643828ca8aaeb0f0728ba2d094de7e43.png" alt="image"><br><br>  Cependant, les GAM standard n'atteignent toujours pas la pr√©cision des algorithmes de bo√Æte noire.  Pour y rem√©dier, les auteurs de l'article proposent un compromis - ajouter √† l'√©quation du mod√®le, en plus des fonctions d'une variable, un petit nombre de fonctions de deux variables - des couples soigneusement s√©lectionn√©s dont l'interaction est significative pour pr√©dire la variable d√©pendante.  Ainsi, GA2M est obtenu. <br><br>  Tout d'abord, un GAM standard est construit (sans tenir compte de l'interaction des variables), puis des paires de variables sont ajout√©es √©tape par √©tape (le GAM restant est utilis√© comme variable cible).  Dans le cas o√π il y a beaucoup de variables et la mise √† jour du mod√®le apr√®s chaque √©tape est difficile √† calculer, un algorithme de classement FAST est propos√©, avec lequel vous pouvez pr√©s√©lectionner des paires potentiellement utiles et √©viter une √©num√©ration compl√®te. <br><br>  Cette approche nous permet d'atteindre une qualit√© proche de mod√®les d'une complexit√© illimit√©e.  Le tableau montre le taux d'erreur des mod√®les additifs g√©n√©ralis√©s par rapport √† une for√™t al√©atoire pour r√©soudre le probl√®me de classification sur diff√©rents ensembles de donn√©es, et dans la plupart des cas, la qualit√© de la pr√©vision pour GA2M avec FAST et pour les for√™ts al√©atoires n'est pas significativement diff√©rente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/809/a50/e5b809a50067cbfcd2f7bbb04f98630e.png" alt="image"><br><br>  Je voudrais attirer l'attention sur les caract√©ristiques du travail des universitaires qui proposent d'envoyer ces boostings et lerings profonds au four.  Veuillez noter que les jeux de donn√©es sur lesquels les r√©sultats sont pr√©sent√©s ne contiennent pas plus de 20 000 objets (tous les jeux de donn√©es du r√©f√©rentiel UCI).  Une question naturelle se pose: n'y a-t-il vraiment pas de jeu de donn√©es ouvert de taille normale pour de telles exp√©riences en 2018?  Vous pouvez aller plus loin et comparer sur un ensemble de donn√©es de 50 objets - il est possible que le mod√®le constant ne diff√®re pas de mani√®re significative d'une for√™t al√©atoire. <br><br>  Le point suivant est la r√©gularisation.  Sur un grand nombre de signes, il est tr√®s facile de se recycler m√™me sans interactions.  Les auteurs peuvent croire que ce probl√®me n'existe pas, et le seul probl√®me est le mod√®le de bo√Æte noire.  Au moins dans l'article, la r√©gularisation n'est √©voqu√©e nulle part, bien qu'elle soit √©videmment n√©cessaire. <br><br>  Et le dernier, sur l'interpr√©tabilit√©.  M√™me les mod√®les lin√©aires ne sont pas interpr√©tables si nous avons beaucoup de fonctionnalit√©s.  Lorsque vous avez 10 000 poids normalement distribu√©s (dans le cas de l'utilisation de la r√©gularisation L2, ce sera quelque chose comme √ßa), il est impossible de dire exactement quels signes sont responsables du fait que predite_proba donne 0,86.  Pour l'interpr√©tabilit√©, nous voulons non seulement un mod√®le lin√©aire, mais un mod√®le lin√©aire avec des poids clairsem√©s.  Il semblerait que cela puisse √™tre r√©alis√© par la r√©gularisation L1, mais ici aussi, ce n'est pas si simple.  √Ä partir d'un ensemble de caract√©ristiques fortement corr√©l√©es, la r√©gularisation L1 en choisira une presque par accident.  Le reste aura un poids de 0, bien que si l'une de ces fonctionnalit√©s a une capacit√© pr√©dictive, les autres ne sont clairement pas uniquement du bruit.  En termes d'interpr√©tation du mod√®le, cela peut √™tre OK, en termes de compr√©hension de la relation entre les caract√©ristiques et la variable cible, c'est tr√®s mauvais.  Autrement dit, m√™me avec des mod√®les lin√©aires, tout n'est pas si simple, plus de d√©tails sur les mod√®les interpr√©tables et cr√©dibles peuvent √™tre trouv√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  <b>Visualisation pour l'apprentissage automatique: UMAP</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Absract</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Code</a> <br><br>  Le jour des tutoriels, l'un des premiers √† √™tre ex√©cut√© √©tait "Visualisation pour Machine Learning" par Google Brain.  Dans le cadre du didacticiel, nous avons √©t√© inform√©s de l'histoire des visualisations, √† partir du cr√©ateur des premiers graphiques, ainsi que des diff√©rentes caract√©ristiques du cerveau humain et de la perception et des techniques qui peuvent √™tre utilis√©es pour attirer l'attention sur la chose la plus importante de l'image, contenant m√™me de nombreux petits d√©tails - par exemple, la mise en √©vidence forme, couleur, cadre, etc., comme dans l'image ci-dessous.  Je vais sauter cette partie, mais il y a une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bonne critique</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/751/f0f/c31/751f0fc318a4bb3edadf59e35ecc7ba9.png" alt="image"><br><br>  Personnellement, j'√©tais le plus int√©ress√© par le sujet de la visualisation d'ensembles de donn√©es multidimensionnels, en particulier, l'approche Uniform Manifold Approximation and Projection (UMAP) - une nouvelle m√©thode non lin√©aire de r√©duction de dimension.  Il a √©t√© propos√© en f√©vrier de cette ann√©e, donc peu de gens l'utilisent encore, mais il semble prometteur tant en termes de temps de travail qu'en termes de qualit√© de s√©paration des classes dans les visualisations bidimensionnelles.  Ainsi, sur diff√©rents ensembles de donn√©es, l'UMAP est 2 √† 10 fois plus rapide que le t-SNE et les autres m√©thodes en termes de vitesse, et plus la dimension des donn√©es est grande, plus l'√©cart de performances est important: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b82/045/a12/b82045a12b1cb8782c5c9fb64dfa46b9.png" alt="image"><br><br>  De plus, contrairement au t-SNE, le temps de fonctionnement UMAP est presque ind√©pendant de la dimension du nouvel espace dans lequel nous allons int√©grer notre jeu de donn√©es (voir la figure ci-dessous), ce qui en fait un outil appropri√© pour d'autres t√¢ches (en plus de la visualisation) - en particulier, r√©duire la dimension avant d'entra√Æner le mod√®le. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0c6/f14/56a/0c6f1456a68537b5e12f58f6e1dfb740.png" alt="image"><br><br>  Dans le m√™me temps, les tests sur diff√©rents ensembles de donn√©es ont montr√© que l'UMAP ne fonctionnait pas moins bien pour la visualisation, et t-SNE est meilleur par endroits: par exemple, sur les ensembles de donn√©es MNIST et Fashion MNIST, les classes sont mieux s√©par√©es dans la version avec UMAP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f35/4b4/869/f354b48691226fd262e3f92af9fe398f.png" alt="image"><br><br>  Un avantage suppl√©mentaire est une impl√©mentation pratique: la classe UMAP h√©rite des classes sklearn, vous pouvez donc l'utiliser comme un transformateur normal dans le pipeline sklearn.  En outre, il est avanc√© que l'UMAP est plus interpr√©table que le t-SNE, car  maintient mieux une structure de donn√©es globale. <br><br>  √Ä l'avenir, les auteurs pr√©voient d'ajouter un support pour la formation semi-supervis√©e - c'est-√†-dire que si nous avons des balises pour au moins certains des objets, nous pouvons construire UMAP sur la base de ces informations. <br><br>  Quels articles avez-vous aim√©?  √âcrivez des commentaires, posez des questions, nous y r√©pondrons. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr434694/">https://habr.com/ru/post/fr434694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr434684/index.html">Rouille 2019 et au-del√†: restrictions de croissance</a></li>
<li><a href="../fr434686/index.html">Cours magistral sur JavaScript et Node.js dans KPI</a></li>
<li><a href="../fr434688/index.html">FreeBSD pr√©voit de passer √† ZFSonLinux</a></li>
<li><a href="../fr434690/index.html">Syst√®me d'exploitation Haiku: portage d'applications et cr√©ation de packages</a></li>
<li><a href="../fr434692/index.html">Les 25 startups am√©ricaines les plus ch√®res √† mourir en 2018</a></li>
<li><a href="../fr434696/index.html">Les employ√©s des g√©ants de l'informatique ont compris comment influencer les politiques de leurs entreprises</a></li>
<li><a href="../fr434698/index.html">Pessimisme √† propos du multithreading</a></li>
<li><a href="../fr434700/index.html">Avantages de suivre les guides de style lors du d√©veloppement d'applications angulaires</a></li>
<li><a href="../fr434702/index.html">Pourquoi le SSD moderne me plante</a></li>
<li><a href="../fr434704/index.html">Raisons de la baisse du co√ªt du trafic mobile en Russie et pr√©visions pour 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>