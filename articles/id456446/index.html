<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòù üò∏ üë©üèæ‚Äçü§ù‚Äçüë©üèª Ceph - dari "di lutut" ke "produksi" ‚óªÔ∏è üé≥ üö£üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pilihan CEPH. Bagian 1 


 Kami memiliki lima rak, sepuluh sakelar optik, BGP yang dikonfigurasi, beberapa lusin SSD, dan sekelompok disk SAS dari sem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - dari "di lutut" ke "produksi"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456446/"><h1 id="vybor-ceph-chast-1">  Pilihan CEPH.  Bagian 1 </h1><br><p>  <em>Kami memiliki lima rak, sepuluh sakelar optik, BGP yang dikonfigurasi, beberapa lusin SSD, dan sekelompok disk SAS dari semua warna dan ukuran, dan juga proxmox dan keinginan untuk meletakkan semua statis di penyimpanan S3 kami sendiri.</em>  <em>Bukan berarti semua ini diperlukan untuk virtualisasi, tetapi begitu Anda mulai menggunakan opensource, maka habiskan hobi Anda sampai akhir.</em>  <em>Satu-satunya hal yang menggangguku adalah BGP.</em>  <em>Tidak ada seorang pun di dunia ini yang lebih tidak berdaya, tidak bertanggung jawab, dan tidak bermoral daripada rute internal BGP.</em>  <em>Dan saya tahu bahwa kami akan segera terjun ke dalamnya.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/09e/a36/178/09ea3617814a9598a6aa9784abc14a76.jpg"></p><br><p>  Tugas itu lumrah - ada CEPH, tidak bekerja dengan baik.  Itu perlu untuk melakukan "dengan baik." <br>  Cluster yang saya dapatkan heterogen, dikocok dan praktis tidak disetel.  Ini terdiri dari dua kelompok node yang berbeda, dengan satu grid umum memainkan peran cluster dan jaringan publik.  Node-node tersebut dikemas dengan empat jenis disk - dua jenis SSD, dirangkai menjadi dua aturan penempatan terpisah dan dua jenis HDD dengan ukuran berbeda, dirakit menjadi kelompok ketiga.  Masalah dengan ukuran yang berbeda dipecahkan oleh bobot OSD yang berbeda. </p><br><p>  Pengaturan itu sendiri dibagi menjadi dua bagian - <strong>menyetel sistem operasi</strong> dan <strong>menyetel CEPH itu sendiri</strong> dan pengaturannya. </p><a name="habracut"></a><br><h2 id="prokachka-os">  Leveling OS </h2><br><h3 id="network">  Jaringan </h3><br><p>  Latensi tinggi memengaruhi saat merekam dan saat menyeimbangkan.  Saat merekam - karena klien tidak menerima respons tentang rekaman yang berhasil hingga replika data dalam grup penempatan lain mengonfirmasi keberhasilan.  Karena aturan untuk mendistribusikan replika di peta CRUSH kami memiliki satu replika per host, jaringan selalu digunakan. </p><br><p>  Oleh karena itu, hal pertama yang saya putuskan untuk sedikit mengkonfigurasi jaringan saat ini, ketika mencoba meyakinkan saya untuk pindah ke jaringan yang terpisah. </p><br><p>  Untuk memulai, memutar pengaturan kartu jaringan.  Dimulai dengan mengatur antrian: </p><br><p>  apa itu: </p><br><div class="spoiler">  <b class="spoiler_title">ethtool -l ens1f1</b> <div class="spoiler_text"><pre><code class="plaintext hljs">root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1 root@ceph01:~# ethtool -g ens1f1 Ring parameters for ens1f1: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 root@ceph01:~# ethtool -l ens1f1 Channel parameters for ens1f1: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 63 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 1</code> </pre> </div></div><br><p>  Dapat dilihat bahwa parameter saat ini jauh dari maksimum.  Meningkat: </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ethtool -G ens1f0 rx 4096 root@ceph01:~#ethtool -G ens1f0 tx 4096 root@ceph01:~#ethtool -L ens1f0 combined 63</code> </pre> <br><p>  Dipandu oleh artikel yang luar biasa </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a> </p><br><p>  menambah panjang <strong>antrian</strong> pengiriman <strong>txqueuelen</strong> dari 1.000 menjadi 10.000 </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set ens1f0 txqueuelen 10000</code> </pre> <br><p>  Nah, mengikuti dokumentasi ceph sendiri </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://ceph.com/geen-categorie/ceph-loves-jumbo-frames/</a> </p><br><p>  meningkatkan <strong>MTU</strong> menjadi 9000. </p><br><pre> <code class="plaintext hljs">root@ceph01:~#ip link set dev ens1f0 mtu 9000</code> </pre> <br><p>  Ditambahkan ke / etc / network / interfaces, sehingga semua hal di atas dimuat saat startup </p><br><div class="spoiler">  <b class="spoiler_title">cat / etc / network / interfaces</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01:~# cat /etc/network/interfaces auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet manual post-up /sbin/ethtool -G ens1f0 rx 4096 post-up /sbin/ethtool -G ens1f0 tx 4096 post-up /sbin/ethtool -L ens1f0 combined 63 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 auto ens1f1 iface ens1f1 inet manual post-up /sbin/ethtool -G ens1f1 rx 4096 post-up /sbin/ethtool -G ens1f1 tx 4096 post-up /sbin/ethtool -L ens1f1 combined 63 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000</code> </pre> </div></div><br><p>  Setelah itu, mengikuti artikel yang sama, ia mulai dengan hati-hati menyelesaikan 4.15 kernel.  Menimbang bahwa pada node 128G RAM, kami mendapat file konfigurasi tertentu untuk <strong>sysctl</strong> </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/50-ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">net.core.rmem_max = 56623104 #        54M net.core.wmem_max = 56623104 #        54M net.core.rmem_default = 56623104 #        . 54M net.core.wmem_default = 56623104 #         54M #    net.ipv4.tcp_rmem = 4096 87380 56623104 # (,  , )    tcp_rmem #  3  ,      TCP. # :   TCP       #   .     #       (moderate memory pressure). #       8  (8192). #  :  ,    #   TCP  .     #  /proc/sys/net/core/rmem_default,   . #       ( ) #  87830 .     65535  #     tcp_adv_win_scale  tcp_app_win = 0, #  ,       tcp_app_win. # :   ,     #     TCP.     , #    /proc/sys/net/core/rmem_max.  ¬´¬ª #     SO_RCVBUF     . net.ipv4.tcp_wmem = 4096 65536 56623104 net.core.somaxconn = 5000 #    ,  . net.ipv4.tcp_timestamps=1 #     (timestamps),    RFC 1323. net.ipv4.tcp_sack=1 #     TCP net.core.netdev_max_backlog=5000 ( 1000) #       ,  #    ,     . net.ipv4.tcp_max_tw_buckets=262144 #   ,    TIME-WAIT . #     ‚Äì ¬´¬ª     #    . net.ipv4.tcp_tw_reuse=1 #   TIME-WAIT   , #     . net.core.optmem_max=4194304 #   - ALLOCATABLE #    (4096 ) net.ipv4.tcp_low_latency=1 #  TCP/IP      #     . net.ipv4.tcp_adv_win_scale=1 #          , #    TCP-    . #   tcp_adv_win_scale ,     #   : # Bytes- bytes\2  -tcp_adv_win_scale #  bytes ‚Äì     .   tcp_adv_win_scale # ,       : # Bytes- bytes\2  tcp_adv_win_scale #    .  - ‚Äì 2, # ..     ¬º  ,   # tcp_rmem. net.ipv4.tcp_slow_start_after_idle=0 #    ,     # ,       . #   SSR  ,    #  . net.ipv4.tcp_no_metrics_save=1 #    TCP      . net.ipv4.tcp_syncookies=0 #   syncookie net.ipv4.tcp_ecn=0 #Explicit Congestion Notification (   )  # TCP-.      ¬´¬ª #       .     # -        #    . net.ipv4.conf.all.send_redirects=0 #   ICMP Redirect ‚Ä¶  .    #   ,        . #    . net.ipv4.ip_forward=0 #  .   ,     , #    . net.ipv4.icmp_echo_ignore_broadcasts=1 #   ICMP ECHO ,    net.ipv4.tcp_fin_timeout=10 #      FIN-WAIT-2   #   .  60 net.core.netdev_budget=600 # ( 300) #        , #          #  .    NIC ,    . # ,     SoftIRQs # ( )  CPU.    netdev_budget. #    300.    SoftIRQ  # 300   NIC     CPU net.ipv4.tcp_fastopen=3 # TFO TCP Fast Open #        TFO,      #    TCP .     ,  #  )</code> </pre> </div></div><br><p>  Dengan <strong>jaringan kilau,</strong> dialokasikan pada antarmuka jaringan 10Gbps yang terpisah untuk jaringan datar yang terpisah.  Kartu jaringan dual-port <strong>Mellanox</strong> 10/25 Gbps yang <strong>dikirimkan</strong> dalam dua sakelar 10Gbps terpisah dikirimkan pada setiap mesin.  Agregasi dilakukan menggunakan OSPF, karena ikatan dengan lacp untuk beberapa alasan menunjukkan total bandwidth maksimum 16 Gbps, sementara ospf berhasil memanfaatkan sepenuhnya kedua lusinan pada setiap mesin.  Rencana masa depan adalah menggunakan ROCE pada melanox ini untuk mengurangi latensi.  Cara mengonfigurasi bagian jaringan ini: </p><br><ol><li>  Karena mesin itu sendiri memiliki IP eksternal pada BGP, kami memerlukan perangkat lunak - <em>(atau lebih tepatnya, pada saat penulisan ini, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">frr = 6.0-1</a> )</em> sudah ada. </li><li>  Secara total, mesin memiliki dua antarmuka jaringan dengan dua antarmuka - total 4 port.  Satu kartu jaringan melihat pabrik dengan dua port dan BGP dikonfigurasikan di atasnya, kartu kedua - di dua port melihat dua switch berbeda dan OSPF dipasang di atasnya </li></ol><br><p>  Detail pengaturan OSPF: Tugas utama adalah untuk menggabungkan dua tautan dan memiliki toleransi kesalahan. <br>  dua antarmuka jaringan dikonfigurasikan dalam dua jaringan datar sederhana - 10.10.10.0/24 dan 10.10.20.0/24 </p><br><pre> <code class="plaintext hljs">1: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.10.2/24 brd 10.10.10.255 scope global ens1f0 2: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP group default qlen 1000 inet 10.10.20.2/24 brd 10.10.20.255 scope global ens1f1</code> </pre> <br><p>  dimana mobil saling melihat. </p><br><h3 id="disk">  DISK </h3><br><p>  Langkah selanjutnya adalah mengoptimalkan kinerja disk.  Untuk SSD saya mengubah scheduler menjadi <strong>noop</strong> , untuk HDD - <strong>deadline</strong> .  Jika kira-kira - maka NOOP bekerja berdasarkan prinsip "siapa yang pertama kali bangun - dan sandal", yang dalam bahasa Inggris terdengar seperti "FIFO (First In, First Out)".  Permintaan diantrikan saat tersedia.  DEADLINE lebih bersifat read-only, ditambah proses dari antrian mendapatkan akses hampir eksklusif ke disk pada saat operasi.  Ini bagus untuk sistem kami - setelah semua, hanya satu proses yang bekerja dengan setiap disk - daemon OSD. <br>  (Mereka yang ingin membenamkan diri dalam penjadwal I / O dapat membacanya di sini: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> </p><br><p>  Lebih suka membaca dalam bahasa Rusia: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://www.opennet.ru/base/sys/linux_shedulers.txt.html</a> ) </p><br><p>  Dalam rekomendasi untuk penyetelan Linux disarankan untuk juga meningkatkan nr_request </p><br><blockquote>  <em>nr_requests</em> <em><br></em>  <em>Nilai nr_requests menentukan jumlah permintaan I / O yang mendapatkan buffer sebelum penjadwal I / O mengirim / menerima data ke perangkat blok, jika Anda menggunakan kartu RAID / Perangkat Blok yang dapat menangani antrian yang lebih besar daripada apa yang saya / O scheduler diatur ke, menaikkan nilai nr_requests dapat membantu meningkatkan seluruh dan mengurangi beban server ketika sejumlah besar I / O terjadi di server.</em>  <em>Jika Anda menggunakan Tenggat Waktu atau CFQ sebagai penjadwal, disarankan agar Anda mengatur nilai nr_request menjadi 2 kali dari nilai kedalaman antrian.</em> </blockquote><p>  TAPI!  Warga sendiri adalah pengembang CEPH yang meyakinkan kami bahwa sistem prioritas mereka berfungsi lebih baik </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b7e/fd3/bd0/b7efd3bd03fedc88307e200905c8c6a9.gif"></p><br><div class="spoiler">  <b class="spoiler_title">WBThrottle dan / atau nr_requests</b> <div class="spoiler_text"><blockquote>  WBThrottle dan / atau nr_requests <br>  Penyimpanan file menggunakan buffered I / O untuk menulis;  ini membawa sejumlah manfaat jika log penyimpanan file ada di media yang lebih cepat.  Permintaan klien diberitahukan segera setelah data ditulis ke log, dan kemudian dibilas ke disk data itu sendiri di lain waktu menggunakan fungsionalitas Linux standar.  Ini memungkinkan disk spindle OSD untuk menyediakan latensi tulis yang mirip dengan SSD saat menulis dalam paket kecil.  Tertunda menulis ini tertunda juga memungkinkan kernel itu sendiri untuk membangun kembali permintaan I / O ke disk dengan harapan baik menggabungkan mereka bersama-sama atau memungkinkan kepala disk yang ada untuk memilih beberapa jalur yang lebih optimal di atas piring mereka.  Efek akhirnya adalah Anda dapat menekan I / O sedikit lebih banyak dari setiap disk daripada yang mungkin dilakukan dengan I / O langsung atau sinkron. </blockquote><p>  Namun, masalah tertentu muncul jika volume rekaman yang masuk dalam cluster Ceph yang diberikan berada di depan semua kemampuan disk yang mendasarinya.  Dalam skenario seperti itu, jumlah total pending I / O yang tertunda tulis ke disk dapat tumbuh tidak terkendali dan menghasilkan antrian operasi I / O yang mengisi seluruh disk dan antrian Ceph.  Permintaan baca bekerja sangat buruk, karena mereka terjebak di antara permintaan menulis, yang mungkin memerlukan beberapa detik untuk masuk ke disk utama. </p><br><p>  Untuk mengatasi masalah ini, Ceph memiliki mekanisme pelambatan write-back yang disebut WBThrottle yang dibangun ke dalam penyimpanan file.  Ia dirancang untuk membatasi jumlah total operasi penulisan I / O yang tertunda yang dapat di-antri dan memulai proses reset mereka lebih awal daripada yang akan terjadi secara alami karena penyertaan oleh kernel itu sendiri.  Sayangnya, pengujian menunjukkan bahwa nilai default mungkin masih tidak mengurangi perilaku yang ada ke tingkat yang dapat mengurangi efek ini pada latensi operasi baca.  Penyesuaian dapat mengubah perilaku ini dan mengurangi keseluruhan panjang antrian perekaman dan membuat dampak seperti itu tidak kuat.  Namun, ada tradeoff: dengan mengurangi jumlah maksimum entri yang diizinkan untuk antri, Anda dapat mengurangi kemampuan kernel itu sendiri untuk memaksimalkan efisiensinya dalam memesan permintaan yang masuk.  Perlu sedikit pemikiran bahwa Anda membutuhkan lebih banyak untuk aplikasi spesifik Anda, beban kerja dan menyesuaikan agar sesuai dengan mereka. </p><br><p>  Untuk mengontrol kedalaman antrian penulisan yang tertunda, Anda dapat mengurangi jumlah maksimum operasi I / O yang gagal menggunakan pengaturan WBThrottle, atau mengurangi nilai maksimum untuk operasi yang gagal pada level blok kernel Anda.  Baik itu dan yang lain dapat secara efektif mengontrol perilaku yang sama dan itu adalah preferensi Anda yang akan menjadi dasar untuk penerapan pengaturan ini. <br>  Juga harus dicatat bahwa sistem prioritas operasi Ceph lebih efisien untuk kueri yang lebih pendek pada level disk.  Ketika mengurangi antrian keseluruhan ke disk yang diberikan, lokasi antrian utama pindah ke Ceph, di mana ia memiliki kontrol lebih besar atas prioritas apa yang dimiliki operasi I / O.  Perhatikan contoh berikut: </p><br><pre> <code class="plaintext hljs">echo 8 &gt; /sys/block/sda/queue/nr_requests</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> </p></div></div><br><h3 id="common">  UMUM </h3><br><p>  Dan beberapa pengaturan kernel untuk dibuat <del>  mobil Anda lembut dan halus </del>  memeras beberapa kinerja lagi dari besi </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/sysctl.d/60-ceph2.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> kernel.pid_max = 4194303 #     25,       kernel.threads-max=2097152 # , , . vm.max_map_count=524288 #      . #        #         # malloc,    mmap, mprotect  madvise,     #  . fs.aio-max-nr=50000000 #   input-output #  Linux     - (AIO), #       - # ,    -  . #     , #      -. #  aio-max-nr     #  . vm.min_free_kbytes=1048576 #       . #  1Gb,       , #    OOM Killer   OSD.     #    ,      vm.swappiness=10 #       10% . #   128G ,  10%  12 .     . #    60%   ,   , #       vm.vfs_cache_pressure=1000 #    100.     #     . vm.zone_reclaim_mode=0 #         #  ,     . #     ,     . #       # ,    , zone_reclaim_mode #  ,   , # ,   ,   . vm.dirty_ratio=20 #   ,     ""  #    : #   128  . #   20  SSD,     CEPH  #     3G . #   40  HDD,      1G # 20%  128  25.6 . ,     , #    2.4G .         #    -    DevOps   . vm.dirty_background_ratio=3 #   ,    dirty pages  , #    pdflush/flush/kdmflush     fs.file-max=524288 #      ,,   ,    .</code> </pre> </div></div><br><h2 id="pogruzhenie-v--ceph">  Menyelam di CEPH </h2><br><p>  Pengaturan yang ingin saya bahas lebih detail: </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">osd: journal_aio: true #  ,  journal_block_align: true #  i/o journal_dio: true #   journal_max_write_bytes: 1073714824 #     #      journal_max_write_entries: 10000 #      journal_queue_max_bytes: 10485760000 journal_queue_max_ops: 50000 rocksdb_separate_wal_dir: true #    wal #       # NVMe bluestore_block_db_create: true #       bluestore_block_db_size: '5368709120 #5G' bluestore_block_wal_create: true bluestore_block_wal_size: '1073741824 #1G' bluestore_cache_size_hdd: '3221225472 # 3G' #     #     bluestore_cache_size_ssd: '9663676416 # 9G' keyring: /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap: '1073741824 #1G' osd_disk_thread_ioprio_class: idle osd_disk_thread_ioprio_priority: 7 osd_disk_threads: 2 #        osd_failsafe_full_ratio: 0.95 osd_heartbeat_grace: 5 osd_heartbeat_interval: 3 osd_map_dedup: true osd_max_backfills: 2 #       . osd_max_write_size: 256 osd_mon_heartbeat_interval: 5 osd_op_threads: 16 osd_op_num_threads_per_shard: 1 osd_op_num_threads_per_shard_hdd: 2 osd_op_num_threads_per_shard_ssd: 2 osd_pool_default_min_size: 1 #  .    osd_pool_default_size: 2 #  ,    #     #   osd_recovery_delay_start: 10.000000 osd_recovery_max_active: 2 osd_recovery_max_chunk: 1048576 osd_recovery_max_single_start: 3 osd_recovery_op_priority: 1 osd_recovery_priority: 1 #       osd_recovery_sleep: 2 osd_scrub_chunk_max: 4</code> </pre> </div></div><br><p>  Beberapa parameter yang diuji pada QA pada versi 12.2.12 hilang dalam ceph versi 12.2.2, misalnya, <strong>osd_recovery_threads.</strong>  Oleh karena itu, paket termasuk pembaruan pada prod ke 12.2.12.  Praktik telah menunjukkan kompatibilitas dalam satu kelompok versi 12.2.2 dan 12.2.12, yang memungkinkan pembaruan bergulir. </p><br><h3 id="testovyy-klaster">  Kelompok uji </h3><br><p>  Secara alami, untuk pengujian perlu memiliki versi yang sama seperti dalam pertempuran, tetapi pada saat itu saya mulai bekerja dengan cluster di repositori hanya ada yang lebih baru.  Melihat bahwa dalam versi minor Anda tidak terlalu besar ( <strong>1393</strong> baris dalam konfigurasi versus <strong>1436</strong> baris dalam versi baru), kami memutuskan untuk mulai menguji yang baru (masih diperbarui, mengapa pergi ke sampah lama) </p><br><p>  Satu-satunya hal yang mereka coba tinggalkan versi lama adalah paket <strong>ceph-deploy,</strong> karena sebagian dari utilitas (dan sebagian dari karyawan) dipertajam oleh sintaksnya.  Versi baru sangat berbeda, tetapi itu tidak mempengaruhi operasi cluster itu sendiri, dan versi <strong>1.5.39</strong> meninggalkannya </p><br><p>  Karena perintah ceph-disk dengan jelas mengatakan bahwa itu sudah usang dan digunakan, Sayang, perintah ceph-volume - kami mulai membuat OSD dengan perintah ini, tanpa membuang waktu untuk yang sudah usang. </p><br><p>  Rencananya adalah untuk membuat mirror dari dua disk SSD, di mana kita menempatkan log OSD, yang, pada gilirannya, terletak di spindle SAS.  Jadi kita bisa melindungi diri dari masalah data saat disk dengan log jatuh. </p><br><p>  Buat sekelompok dokumentasi baja </p><br><div class="spoiler">  <b class="spoiler_title">cat /etc/ceph/ceph.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">root@ceph01-qa:~# cat /etc/ceph/ceph.conf #     [client] rbd_cache = true rbd_cache_max_dirty = 50331648 rbd_cache_max_dirty_age = 2 rbd_cache_size = 67108864 rbd_cache_target_dirty = 33554432 rbd_cache_writethrough_until_flush = true rbd_concurrent_management_ops = 10 rbd_default_format = 2 [global] auth_client_required = cephx auth_cluster_required = cephx auth_service_required = cephx cluster network = 10.10.10.0/24 debug_asok = 0/0 debug_auth = 0/0 debug_buffer = 0/0 debug_client = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_filer = 0/0 debug_filestore = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_journal = 0/0 debug_journaler = 0/0 debug_lockdep = 0/0 debug_mon = 0/0 debug_monc = 0/0 debug_ms = 0/0 debug_objclass = 0/0 debug_objectcatcher = 0/0 debug_objecter = 0/0 debug_optracker = 0/0 debug_osd = 0/0 debug_paxos = 0/0 debug_perfcounter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_rgw = 0/0 debug_throttle = 0/0 debug_timer = 0/0 debug_tp = 0/0 fsid = d0000000d-4000-4b00-b00b-0123qwe123qwf9 mon_host = ceph01-q, ceph02-q, ceph03-q mon_initial_members = ceph01-q, ceph02-q, ceph03-q public network = 8.8.8.8/28 #  ,  )) rgw_dns_name = s3-qa.mycompany.ru #     rgw_host = s3-qa.mycompany.ru #    [mon] mon allow pool delete = true mon_max_pg_per_osd = 300 #     #     #  , ,    , #     OSD.     PG #     -    mon_osd_backfillfull_ratio = 0.9 mon_osd_down_out_interval = 5 mon_osd_full_ratio = 0.95 #   SSD     #   -      #   5%   (   1.2Tb) #   ,     # bluestore_block_db_size     #   mon_osd_nearfull_ratio = 0.9 mon_pg_warn_max_per_osd = 520 [osd] bluestore_block_db_create = true bluestore_block_db_size = 5368709120 #5G bluestore_block_wal_create = true bluestore_block_wal_size = 1073741824 #1G bluestore_cache_size_hdd = 3221225472 # 3G bluestore_cache_size_ssd = 9663676416 # 9G journal_aio = true journal_block_align = true journal_dio = true journal_max_write_bytes = 1073714824 journal_max_write_entries = 10000 journal_queue_max_bytes = 10485760000 journal_queue_max_ops = 50000 keyring = /var/lib/ceph/osd/ceph-$id/keyring osd_client_message_size_cap = 1073741824 #1G osd_disk_thread_ioprio_class = idle osd_disk_thread_ioprio_priority = 7 osd_disk_threads = 2 osd_failsafe_full_ratio = 0.95 osd_heartbeat_grace = 5 osd_heartbeat_interval = 3 osd_map_dedup = true osd_max_backfills = 4 osd_max_write_size = 256 osd_mon_heartbeat_interval = 5 osd_op_num_threads_per_shard = 1 osd_op_num_threads_per_shard_hdd = 2 osd_op_num_threads_per_shard_ssd = 2 osd_op_threads = 16 osd_pool_default_min_size = 1 osd_pool_default_size = 2 osd_recovery_delay_start = 10.0 osd_recovery_max_active = 1 osd_recovery_max_chunk = 1048576 osd_recovery_max_single_start = 3 osd_recovery_op_priority = 1 osd_recovery_priority = 1 osd_recovery_sleep = 2 osd_scrub_chunk_max = 4 osd_scrub_chunk_min = 2 osd_scrub_sleep = 0.1 rocksdb_separate_wal_dir = true</code> </pre> </div></div><br><pre> <code class="plaintext hljs">#   root@ceph01-qa:~#ceph-deploy mon create ceph01-q #        root@ceph01-qa:~#ceph-deploy gatherkeys ceph01-q #   .       - ,       # mon_initial_members = ceph01-q, ceph02-q, ceph03-q #         root@ceph01-qa:~#ceph-deploy mon create-initial #        root@ceph01-qa:~#cat ceph.bootstrap-osd.keyring &gt; /var/lib/ceph/bootstrap-osd/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-mgr.keyring &gt; /var/lib/ceph/bootstrap-mgr/ceph.keyring root@ceph01-qa:~#cat ceph.bootstrap-rgw.keyring &gt; /var/lib/ceph/bootstrap-rgw/ceph.keyring #      root@ceph01-qa:~#ceph-deploy admin ceph01-q #  ,   root@ceph01-qa:~#ceph-deploy mgr create ceph01-q</code> </pre> <br><p> ,        ceph-deploy    12.2.12 ‚Äî      OSD  db    - </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 blkid could not detect a PARTUUID for device: /dev/md1</code> </pre> <br><p> , blkid   PARTUUID,    : </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#parted /dev/md0 mklabel GPT #   , #  GPT     #        = bluestore_block_db_size: '5368709120 #5G' #    20  OSD,     #    root@ceph01-qa:~#for i in {1..20}; do echo -e "n\n\n\n+5G\nw" | fdisk /dev/md0; done</code> </pre> <br><p>   ,     OSD     (, ,    ) </p><br><p>   OSD  bluestore     WAL,    db </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sde --block.db /dev/md0 stderr: 2019-04-12 10:39:27.211242 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid stderr: 2019-04-12 10:39:27.213185 7eff461b6e00 -1 bdev(0x55824c273680 /var/lib/ceph/osd/ceph-0//block.wal) open open got: (22) Invalid argument stderr: 2019-04-12 10:39:27.213201 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _open_db add block device(/var/lib/ceph/osd/ceph-0//block.wal) returned: (22) Invalid argument stderr: 2019-04-12 10:39:27.999039 7eff461b6e00 -1 bluestore(/var/lib/ceph/osd/ceph-0/) mkfs failed, (22) Invalid argument stderr: 2019-04-12 10:39:27.999057 7eff461b6e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (22) Invalid argument stderr: 2019-04-12 10:39:27.999141 7eff461b6e00 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0/: (22) Invalid argumen</code> </pre> <br><p>     -  (   ,  )      WAL      OSD ‚Äî     (    WAL,  , ,   ). </p><br><p> ,         WAL  NVMe,     . </p><br><pre> <code class="plaintext hljs">root@ceph01-qa:~#ceph-volume lvm create --bluestore --data /dev/sdf --block.wal /dev/md0p2 --block.db /dev/md1p2</code> </pre> <br><p>  ,   OSD.      ,        ‚Äî    SSD  ,     SAS. </p><br><p>       20 ,     ,  ‚Äî . <br> , ,   : </p><br><div class="spoiler"> <b class="spoiler_title">ceph osd tree</b> <div class="spoiler_text"><p> root@eph01-q:~# ceph osd tree <br> ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF <br> -1 14.54799 root default <br> -3 9.09200 host ceph01-q <br> 0 ssd 1.00000 osd.0 up 1.00000 1.00000 <br> 1 ssd 1.00000 osd.1 up 1.00000 1.00000 <br> 2 ssd 1.00000 osd.2 up 1.00000 1.00000 <br> 3 ssd 1.00000 osd.3 up 1.00000 1.00000 <br> 4 hdd 1.00000 osd.4 up 1.00000 1.00000 <br> 5 hdd 0.27299 osd.5 up 1.00000 1.00000 <br> 6 hdd 0.27299 osd.6 up 1.00000 1.00000 <br> 7 hdd 0.27299 osd.7 up 1.00000 1.00000 <br> 8 hdd 0.27299 osd.8 up 1.00000 1.00000 <br> 9 hdd 0.27299 osd.9 up 1.00000 1.00000 <br> 10 hdd 0.27299 osd.10 up 1.00000 1.00000 <br> 11 hdd 0.27299 osd.11 up 1.00000 1.00000 <br> 12 hdd 0.27299 osd.12 up 1.00000 1.00000 <br> 13 hdd 0.27299 osd.13 up 1.00000 1.00000 <br> 14 hdd 0.27299 osd.14 up 1.00000 1.00000 <br> 15 hdd 0.27299 osd.15 up 1.00000 1.00000 <br> 16 hdd 0.27299 osd.16 up 1.00000 1.00000 <br> 17 hdd 0.27299 osd.17 up 1.00000 1.00000 <br> 18 hdd 0.27299 osd.18 up 1.00000 1.00000 <br> 19 hdd 0.27299 osd.19 up 1.00000 1.00000 <br> -5 5.45599 host ceph02-q <br> 20 ssd 0.27299 osd.20 up 1.00000 1.00000 <br> 21 ssd 0.27299 osd.21 up 1.00000 1.00000 <br> 22 ssd 0.27299 osd.22 up 1.00000 1.00000 <br> 23 ssd 0.27299 osd.23 up 1.00000 1.00000 <br> 24 hdd 0.27299 osd.24 up 1.00000 1.00000 <br> 25 hdd 0.27299 osd.25 up 1.00000 1.00000 <br> 26 hdd 0.27299 osd.26 up 1.00000 1.00000 <br> 27 hdd 0.27299 osd.27 up 1.00000 1.00000 <br> 28 hdd 0.27299 osd.28 up 1.00000 1.00000 <br> 29 hdd 0.27299 osd.29 up 1.00000 1.00000 <br> 30 hdd 0.27299 osd.30 up 1.00000 1.00000 <br> 31 hdd 0.27299 osd.31 up 1.00000 1.00000 <br> 32 hdd 0.27299 osd.32 up 1.00000 1.00000 <br> 33 hdd 0.27299 osd.33 up 1.00000 1.00000 <br> 34 hdd 0.27299 osd.34 up 1.00000 1.00000 <br> 35 hdd 0.27299 osd.35 up 1.00000 1.00000 <br> 36 hdd 0.27299 osd.36 up 1.00000 1.00000 <br> 37 hdd 0.27299 osd.37 up 1.00000 1.00000 <br> 38 hdd 0.27299 osd.38 up 1.00000 1.00000 <br> 39 hdd 0.27299 osd.39 up 1.00000 1.00000 <br> -7 6.08690 host ceph03-q <br> 40 ssd 0.27299 osd.40 up 1.00000 1.00000 <br> 41 ssd 0.27299 osd.41 up 1.00000 1.00000 <br> 42 ssd 0.27299 osd.42 up 1.00000 1.00000 <br> 43 ssd 0.27299 osd.43 up 1.00000 1.00000 <br> 44 hdd 0.27299 osd.44 up 1.00000 1.00000 <br> 45 hdd 0.27299 osd.45 up 1.00000 1.00000 <br> 46 hdd 0.27299 osd.46 up 1.00000 1.00000 <br> 47 hdd 0.27299 osd.47 up 1.00000 1.00000 <br> 48 hdd 0.27299 osd.48 up 1.00000 1.00000 <br> 49 hdd 0.27299 osd.49 up 1.00000 1.00000 <br> 50 hdd 0.27299 osd.50 up 1.00000 1.00000 <br> 51 hdd 0.27299 osd.51 up 1.00000 1.00000 <br> 52 hdd 0.27299 osd.52 up 1.00000 1.00000 <br> 53 hdd 0.27299 osd.53 up 1.00000 1.00000 <br> 54 hdd 0.27299 osd.54 up 1.00000 1.00000 <br> 55 hdd 0.27299 osd.55 up 1.00000 1.00000 <br> 56 hdd 0.27299 osd.56 up 1.00000 1.00000 <br> 57 hdd 0.27299 osd.57 up 1.00000 1.00000 <br> 58 hdd 0.27299 osd.58 up 1.00000 1.00000 <br> 59 hdd 0.89999 osd.59 up 1.00000 1.00000 </p></div></div><br><p>          : </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket rack01 root #  root root@ceph01-q:~#ceph osd crush add-bucket ceph01-q host #   root@ceph01-q:~#ceph osd crush move ceph01-q root=rack01 #     root@ceph01-q:~#osd crush add 28 1.0 host=ceph02-q #     #       root@ceph01-q:~# ceph osd crush remove osd.4 root@ceph01-q:~# ceph osd crush remove rack01</code> </pre> <br><p> ,      <strong></strong> ,            ‚Äî  <strong>ceph osd crush move ceph01-host root=rack01</strong> ,      .    CTRL+C     . </p><br><p>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://tracker.ceph.com/issues/23386</a> </p><br><p>    crushmap     <strong>rule replicated_ruleset</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd getcrushmap -o crushmap.row #     root@ceph01-prod:~#crushtool -d crushmap.row -o crushmap.txt #   root@ceph01-prod:~#vim crushmap.txt #,  rule replicated_ruleset root@ceph01-prod:~#crushtool -c crushmap.txt -o new_crushmap.row #  root@ceph01-prod:~#ceph osd setcrushmap -i new_crushmap.row #  </code> </pre> <br><p> <strong>:</strong>      placement group  OSD.    ,   . </p><br><p>  ,        ‚Äî  ,     OSD ,        ,    root default. <br>  ,   ,      root  ssd     ,          default root.   OSD     . <br> <em>     ,     .     </em> </p><br><h3 id="kak-my-delali-razlichnye-gruppy-po-tipam-diskov">        . </h3><br><p>     root- ‚Äî  ssd   hdd </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#ceph osd crush add-bucket ssd-root root root@ceph01-q:~#ceph osd crush add-bucket hdd-root root</code> </pre> <br><p>        ‚Äî          </p><br><pre> <code class="plaintext hljs"># : root@ceph01-q:~#ceph osd crush add-bucket ssd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack02 rack root@ceph01-q:~#ceph osd crush add-bucket ssd-rack03 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack root@ceph01-q:~#ceph osd crush add-bucket hdd-rack01 rack #  root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket ssd-ceph03-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph01-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host root@ceph01-q:~#ceph osd crush add-bucket hdd-ceph02-q host</code> </pre> <br><p>          </p><br><pre> <code class="plaintext hljs">root@ceph01-q:~#   0  3  SSD,   ceph01-q,     root@ceph01-q:~# ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 0 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 1 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 2 1 host=ssd-ceph01-q root@ceph01-q:~#ceph osd crush add 3 1 host=ssd-ceph01-q root-ceph01-q:~#    </code> </pre> <br><p>     ssd-root  hdd-root   root-default ,     </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush remove default</code> </pre> <br><p>     ,        ‚Äî      root          ‚Äî        ,     (    root,    ) </p><br><p>        : <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://docs.ceph.com/docs/jewel/rados/operations/crush-map/#crushmaprules</a> </p><br><pre> <code class="plaintext hljs">root-ceph01-q:~#ceph osd crush rule create-simple rule-ssd ssd-root host firstn root-ceph01-q:~#ceph osd crush rule create-simple rule-hdd hdd-root host firstn root-ceph01-q:~#    ,     root-ceph01-q:~#   -        , root-ceph01-q:~#       root-ceph01-q:~#  ,   ,    root-ceph01-q:~#        : root-ceph01-q:~# ##ceph osd crush rule create-simple rule-ssd ssd-root rack firstn</code> </pre> <br><p>    ,            ‚Äî PROXMOX: </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~# #ceph osd pool create {NAME} {pg_num} {pgp_num} root-ceph01-q:~# ceph osd pool create ssd_pool 1024 1024 root-ceph01-q:~# ceph osd pool create hdd_pool 1024 1024</code> </pre> <br><p>         </p><br><pre> <code class="plaintext hljs"> root-ceph01-q:~#ceph osd crush rule ls #    root-ceph01-q:~#ceph osd crush rule dump rule-ssd | grep rule_id # ID  root-ceph01-q:~#ceph osd pool set ssd_pool crush_rule 2</code> </pre><br><p>               ‚Äî     ,    (    )   ,    . </p><br><p>      300    ,        ‚Äî        10 Tb    10 PG ‚Äî      (pg)   ‚Äî           ). </p><br><p>        PG ‚Äî         ‚Äî     . </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="> </a> ,    CEPH. </p><br><p>  : </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://www.admin-magazine.com/HPC/Articles/Linux-IO-Schedulers</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">http://onreader.mdl.ru/MasteringCeph/content/Ch09.html#030202</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://tracker.ceph.com/issues/23386</a> <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://ceph.com/pgcalc/</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id456446/">https://habr.com/ru/post/id456446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id456432/index.html">Minggu Keamanan 25: Evernote kerentanan dan ratusan toko online yang diretas</a></li>
<li><a href="../id456434/index.html">Profesi Masa Depan: "Siapa yang Akan Anda Kerjakan di Mars?"</a></li>
<li><a href="../id456436/index.html">Tugas JS pendek untuk hari Senin</a></li>
<li><a href="../id456440/index.html">Petualangan Malvari yang Elusif, Bagian I</a></li>
<li><a href="../id456442/index.html">Masuk ke program sarjana St. Petersburg State University dengan dukungan Yandex dan JetBrains</a></li>
<li><a href="../id456448/index.html">Aturan untuk memilih kerangka kerja JS</a></li>
<li><a href="../id456450/index.html">DO-RA.Avia untuk memantau radiasi kosmik dalam penerbangan</a></li>
<li><a href="../id456452/index.html">Contoh kode C ++ sebelum dan sesudah Rentang</a></li>
<li><a href="../id456462/index.html">Merakit perpustakaan komponen sudut sebagai komponen web</a></li>
<li><a href="../id456466/index.html">PHP generik hari ini (yah, hampir)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>