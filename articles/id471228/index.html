<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∂üèº ‚úäüèø ‚èÆÔ∏è Grokay PyTorch üåè üöß üêò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! 

 Kami telah memesan sebelumnya buku yang sudah lama ditunggu-tunggu tentang perpustakaan PyTorch . 



 Karena Anda akan mempelajari sem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Halo, Habr! <br><br>  Kami telah memesan sebelumnya buku yang sudah lama ditunggu-tunggu tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perpustakaan PyTorch</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Karena Anda akan mempelajari semua materi dasar yang diperlukan tentang PyTorch dari buku ini, kami mengingatkan Anda tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">manfaat dari</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">proses yang</a> disebut "grokking" atau "pemahaman mendalam" dari topik yang ingin Anda pelajari.  Dalam posting hari ini, kami akan memberi tahu Anda bagaimana Kai Arulkumaran membanting PyTorch (tidak ada gambar).  Selamat datang di kucing. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PyTorch</a> adalah kerangka pembelajaran dalam fleksibel yang secara otomatis membedakan antara objek menggunakan jaringan saraf dinamis (yaitu, jaringan menggunakan kontrol aliran dinamis, seperti <code>if</code> dan <code>while</code> loop).  PyTorch mendukung akselerasi GPU, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pelatihan terdistribusi</a> , berbagai jenis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">optimasi</a> dan banyak fitur menarik lainnya.  Di sini saya mengemukakan beberapa pemikiran tentang bagaimana, menurut pendapat saya, harus menggunakan PyTorch;  semua aspek perpustakaan dan praktik yang disarankan tidak dibahas di sini, tetapi semoga teks ini bermanfaat bagi Anda. <br><br>  Jaringan saraf adalah subkelas dari grafik komputasi.  Komputasi grafik menerima data sebagai input, maka data ini dirutekan (dan dapat dikonversi) pada node di mana mereka diproses.  Dalam pembelajaran mendalam, neuron (node) biasanya mentransformasikan data dengan menerapkan parameter dan fungsi terdiferensiasi padanya, sehingga parameter dapat dioptimalkan untuk meminimalkan kerugian dengan metode gradient descent.  Dalam arti yang lebih luas, saya perhatikan bahwa fungsi dapat menjadi stokastik dan grafik yang dinamis.  Dengan demikian, sementara jaringan saraf cocok dengan paradigma pemrograman aliran data, API PyTorch berfokus pada paradigma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pemrograman imperatif</a> , dan cara menafsirkan program yang sedang dibuat ini jauh lebih akrab.  Itulah sebabnya kode PyTorch lebih mudah dibaca, lebih mudah menilai desain program yang rumit, yang, bagaimanapun, tidak memerlukan kompromi serius pada kinerja: pada kenyataannya, PyTorch cukup cepat dan memberikan banyak optimisasi bahwa Anda, sebagai pengguna akhir, tidak dapat khawatir (Namun, jika Anda benar-benar tertarik pada mereka, Anda dapat menggali sedikit lebih dalam dan mengenal mereka). <br><br>  Sisa dari artikel ini adalah analisis dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">contoh resmi pada dataset MNIST</a> .  Di sini kita <i>bermain</i> PyTorch, jadi saya sarankan untuk memahami artikel hanya setelah berkenalan dengan manual <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pemula resmi</a> .  Untuk kenyamanan, kode disajikan dalam bentuk fragmen kecil yang dilengkapi dengan komentar, yaitu, tidak didistribusikan ke fungsi / file terpisah yang biasa Anda lihat dalam kode modular murni. <br><br><h4>  Impor </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  Semua ini adalah impor standar, dengan pengecualian modul <code>torchvision</code> , yang secara khusus digunakan secara aktif untuk menyelesaikan tugas yang berkaitan dengan visi komputer. <br><br><h4>  Kustomisasi </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> adalah cara standar untuk menangani argumen baris perintah dengan Python. <br><br>  Jika Anda perlu menulis kode yang dirancang untuk bekerja pada perangkat yang berbeda (menggunakan akselerasi GPU, ketika tersedia, tetapi jika itu tidak digulirkan kembali ke perhitungan pada CPU), lalu pilih dan simpan alat <code>torch.device</code> sesuai, dengan mana Anda dapat menentukan di mana Anda harus tensor disimpan.  Untuk informasi lebih lanjut tentang cara membuat kode seperti itu, lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a> .  Pendekatan PyTorch adalah membawa pemilihan perangkat ke kontrol pengguna, yang mungkin tampak tidak diinginkan dalam contoh sederhana.  Namun, pendekatan ini sangat menyederhanakan pekerjaan ketika Anda harus berurusan dengan tensor, yang a) nyaman untuk debugging b) memungkinkan Anda untuk secara efektif menggunakan perangkat secara manual. <br><br>  Untuk reproduksibilitas percobaan, Anda perlu menetapkan nilai awal acak untuk semua komponen yang menggunakan pembuatan angka acak (termasuk <code>random</code> atau <code>numpy</code> , jika Anda juga menggunakannya).  Harap dicatat: cuDNN menggunakan algoritma non-deterministik dan secara opsional dinonaktifkan menggunakan <code>torch.backends.cudnn.enabled = False</code> . <br><br><h4>  Data </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Karena model <code>torchvision</code> disimpan di bawah <code>~/.torch/models/</code> , saya lebih suka menyimpan set <code>torchvision</code> torchvision di bawah <code>~/.torch/datasets</code> .  Ini adalah perjanjian hak cipta saya, tetapi sangat nyaman digunakan dalam proyek yang dikembangkan atas dasar MNIST, CIFAR-10, dll.  Secara umum, kumpulan data harus disimpan secara terpisah dari kode jika Anda bermaksud untuk menggunakan kembali beberapa kumpulan data. <br><br>  <code>torchvision.transforms</code> berisi banyak opsi konversi yang nyaman untuk gambar individual, seperti pemotongan dan normalisasi. <br><br>  Ada banyak opsi di <code>batch_size</code> , tetapi selain <code>batch_size</code> dan <code>shuffle</code> , Anda juga harus ingat <code>num_workers</code> dan <code>pin_memory</code> , mereka membantu meningkatkan efisiensi.  <code>num_workers &gt; 0</code> menggunakan subproses untuk memuat data asinkron, dan tidak memblokir proses utama untuk ini.  Kasus penggunaan yang umum adalah memuat data (misalnya, gambar) dari disk dan, mungkin, mengubahnya;  semua ini dapat dilakukan secara paralel, bersamaan dengan pemrosesan data jaringan.  Tingkat pemrosesan mungkin perlu disesuaikan untuk a) meminimalkan jumlah pekerja dan, akibatnya, jumlah CPU dan RAM yang digunakan (masing-masing pekerja memuat bets terpisah, daripada sampel individu yang termasuk dalam bets) b) meminimalkan lamanya waktu data menunggu di jaringan.  <code>pin_memory</code> menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">memori yang disematkan</a> (sebagai lawan dari paged) untuk mempercepat operasi transfer data dari RAM ke GPU (dan tidak melakukan apa-apa dengan kode khusus untuk CPU). <br><br><h4>  Model </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  Inisialisasi jaringan biasanya meluas ke variabel anggota, lapisan yang berisi parameter pembelajaran dan, mungkin, parameter pembelajaran individu dan buffer yang tidak terlatih.  Kemudian, dengan pass langsung, mereka digunakan dalam kombinasi dengan fungsi-fungsi dari <code>F</code> yang murni fungsional dan tidak mengandung parameter.  Beberapa orang suka bekerja dengan jaringan yang berfungsi murni (mis. Menjaga parameter dan menggunakan <code>F.conv2d</code> alih-alih <code>nn.Conv2d</code> ) atau jaringan yang seluruhnya terdiri dari lapisan (mis. <code>nn.ReLU</code> alih-alih <code>F.relu</code> ). <br><br>  <code>.to(device)</code> adalah cara mudah untuk mengirim parameter perangkat (dan buffer) ke GPU jika <code>device</code> diatur ke GPU, karena jika tidak (jika perangkat diatur ke CPU) tidak ada yang akan dilakukan.  Penting untuk mentransfer parameter perangkat ke perangkat yang sesuai sebelum meneruskannya ke pengoptimal;  jika tidak, pengoptimal tidak akan dapat melacak parameter dengan benar! <br><br>  Baik neural networks ( <code>nn.Module</code> ) dan optimizer ( <code>optim.Optimizer</code> ) dapat menyimpan dan memuat keadaan internal mereka, dan disarankan untuk melakukan ini dengan <code>.load_state_dict(state_dict)</code> - perlu memuat ulang keadaan keduanya untuk melanjutkan pelatihan berdasarkan kamus yang disimpan sebelumnya. menyatakan.  Menyimpan seluruh objek mungkin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">penuh dengan kesalahan</a> .  Jika Anda menyimpan tensor pada GPU dan ingin memuatnya ke CPU atau GPU lain, maka cara termudah adalah memuatnya langsung ke CPU menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">opsi</a> <code>map_location</code> , mis. <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Berikut adalah beberapa poin lain yang tidak ditampilkan di sini, tetapi layak disebutkan, bahwa Anda dapat menggunakan aliran kontrol dengan operan langsung (misalnya, eksekusi <code>if</code> mungkin tergantung pada variabel anggota atau pada data itu sendiri. Selain itu, itu sangat valid di tengah proses untuk menghasilkan ( <code>print</code> ) tensor, yang sangat menyederhanakan debugging. Akhirnya, dengan pass langsung, banyak argumen dapat digunakan. Saya akan mengilustrasikan poin ini dengan daftar pendek yang tidak terkait dengan ide tertentu: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  Pelatihan </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Modul jaringan dimasukkan ke dalam mode pelatihan secara default - yang sedikit banyak mempengaruhi operasi modul, sebagian besar - penipisan dan normalisasi batch.  Dengan satu atau lain cara, lebih baik untuk mengatur hal-hal seperti itu secara manual menggunakan <code>.train()</code> , yang memfilter bendera "pelatihan" ke semua modul anak. <br><br>  Di sini, metode <code>.to()</code> tidak hanya menerima perangkat, tetapi juga menetapkan <code>non_blocking=True</code> , sehingga memastikan penyalinan data yang tidak sinkron ke GPU dari memori yang dikomit, memungkinkan CPU untuk tetap beroperasi selama transfer data;  jika tidak, <code>non_blocking=True</code> sekali bukan opsi. <br><br>  Sebelum Anda membangun satu set gradien baru menggunakan <code>loss.backward()</code> dan <code>optimiser.step()</code> menggunakan <code>optimiser.step()</code> , Anda harus secara manual mengatur ulang gradien parameter yang akan dioptimalkan menggunakan <code>optimiser.zero_grad()</code> .  Secara default, PyTorch mengakumulasi gradien, yang sangat nyaman jika Anda tidak memiliki sumber daya yang cukup untuk menghitung semua gradien yang Anda butuhkan dalam satu pass. <br><br>  PyTorch menggunakan sistem "pita" gradien otomatis - ia mengumpulkan informasi tentang operasi apa dan dalam urutan apa yang dilakukan pada tensor, dan kemudian memutarnya dalam arah yang berlawanan untuk melakukan diferensiasi dalam urutan terbalik (reverse-mode diferensiation).  Itu sebabnya sangat fleksibel dan memungkinkan grafik komputasi yang sewenang-wenang.  Jika tidak ada dari tensor ini yang memerlukan gradien (Anda harus menetapkan <code>requires_grad=True</code> , membuat tensor untuk tujuan ini), maka tidak ada grafik yang disimpan!  Namun, jaringan biasanya memiliki parameter yang memerlukan gradien, sehingga setiap perhitungan yang dilakukan berdasarkan output jaringan akan disimpan dalam grafik.  Jadi, jika Anda ingin menyimpan data yang dihasilkan dari langkah ini, Anda harus menonaktifkan gradien secara manual atau (pendekatan yang lebih umum), menyimpan informasi ini sebagai nomor Python (menggunakan <code>.item()</code> dalam skalar PyTorch) atau array <code>numpy</code> .  Baca lebih lanjut tentang <code>autograd</code> di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi</a> . <br><br>  Salah satu cara untuk mengurangi grafik komputasi adalah dengan menggunakan <code>.detach()</code> ketika keadaan tersembunyi dilewatkan saat mempelajari RNN dengan versi terpotong dari backpropagation-through-time.  Juga nyaman saat membedakan kerugian, ketika salah satu komponen adalah output dari jaringan lain, tetapi jaringan lain ini tidak boleh dioptimalkan sehubungan dengan kerugian.  Sebagai contoh, saya akan mengajarkan bagian diskriminatif pada pembuatan bahan keluaran saat bekerja dengan GAN, atau pelatihan kebijakan dalam algoritme aktor-kritik menggunakan fungsi objektif sebagai fungsi dasar (misalnya, A2C).  Teknik lain yang mencegah perhitungan gradien efektif dalam pelatihan GAN (pelatihan bagian pembangkit pada bahan diskriminan) dan tipikal dalam fine tuning adalah penghitungan siklus parameter jaringan yang <code>param.requires_grad = False</code> . <br><br>  Penting tidak hanya untuk mencatat hasil dalam file konsol / log, tetapi juga untuk menetapkan titik kontrol dalam parameter model (dan status pengoptimal) untuk berjaga-jaga.  Anda juga dapat menggunakan <code>torch.save()</code> untuk menyimpan objek Python biasa, atau menggunakan solusi standar lain - <code>pickle</code> . <br><br><h4>  Pengujian </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  Menanggapi <code>.train()</code> jaringan harus secara eksplisit dimasukkan ke mode evaluasi menggunakan <code>.eval()</code> . <br><br>  Seperti disebutkan di atas, ketika menggunakan jaringan, grafik komputasi biasanya dikompilasi.  Untuk mencegah hal ini, gunakan <code>no_grad</code> konteks <code>no_grad</code> dengan <code>with torch.no_grad()</code> . <br><br><h4>  Lagi </h4><br>  Ini adalah bagian tambahan, di mana saya membuat beberapa penyimpangan yang lebih bermanfaat. <br>  Berikut adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi resmi yang</a> menjelaskan bekerja dengan memori. <br><br>  Kesalahan CUDA?  Sulit untuk memperbaikinya, dan biasanya mereka terhubung dengan inkonsistensi logis, yang menurutnya pesan kesalahan yang lebih masuk akal ditampilkan pada CPU daripada pada GPU.  Yang terbaik dari semuanya, jika Anda berencana untuk bekerja dengan GPU, Anda dapat dengan cepat beralih antara CPU dan GPU.  Tip pengembangan yang lebih umum adalah mengatur kode sehingga dapat dengan cepat diperiksa sebelum memulai tugas penuh.  Misalnya, siapkan dataset kecil atau sintetis, jalankan uji satu kereta + era, dll.  Jika masalahnya adalah kesalahan CUDA, atau Anda tidak dapat beralih ke CPU sama sekali, setel CUDA_LAUNCH_BLOCKING = 1.  Ini akan membuat kernel CUDA meluncurkan sinkron, dan Anda akan menerima pesan kesalahan yang lebih akurat. <br><br>  Catatan tentang <code>torch.multiprocessing</code> atau hanya menjalankan beberapa skrip PyTorch secara bersamaan.  Karena PyTorch menggunakan pustaka BLAS multi-threaded untuk mempercepat perhitungan aljabar linier pada CPU, biasanya beberapa core terlibat.  Jika Anda ingin melakukan beberapa hal secara bersamaan, menggunakan pemrosesan multi-utas atau beberapa skrip, mungkin disarankan untuk mengurangi jumlah mereka secara manual dengan mengatur variabel lingkungan <code>OMP_NUM_THREADS</code> menjadi 1 atau nilai rendah lainnya.  Dengan demikian, kemungkinan tergelincirnya prosesor berkurang.  Dokumentasi resmi memiliki komentar lain mengenai pemrosesan multithread. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id471228/">https://habr.com/ru/post/id471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id471212/index.html">10 tips dan trik untuk membantu Anda menjadi pengembang terbaik di VueJS</a></li>
<li><a href="../id471216/index.html">Sejarah panjang buku panduan - bagaimana saya menulis layanan untuk jalur pendakian yang cerdas selama 5 tahun</a></li>
<li><a href="../id471220/index.html">Cockpit - menyederhanakan tugas administrasi tipikal di Linux melalui antarmuka web yang nyaman</a></li>
<li><a href="../id471222/index.html">Memahami kebijakan privasi aplikasi dan layanan akan membantu jaringan saraf</a></li>
<li><a href="../id471226/index.html">Linux memiliki banyak wajah: cara bekerja pada distribusi apa pun</a></li>
<li><a href="../id471232/index.html">Pengalaman saya menghubungkan LPS331AP ke Omega Onion2</a></li>
<li><a href="../id471236/index.html">Dosimeter untuk Seryozha. Bagian III. Radiometer nasional</a></li>
<li><a href="../id471240/index.html">"Bitchy Betty" dan antarmuka audio modern: mengapa mereka berbicara dengan suara wanita?</a></li>
<li><a href="../id471242/index.html">Pengantar Bash Shell</a></li>
<li><a href="../id471244/index.html">Kode Rosetta: mengukur panjang kode dalam sejumlah besar bahasa pemrograman, mempelajari kedekatan bahasa satu sama lain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>