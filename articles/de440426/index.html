<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÜüèº ü§Ωüèº üë≤üèª Ein neuer Ansatz zum Verst√§ndnis des maschinellen Denkens üçï üòπ üëçüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neuronale Netze sind f√ºr ihre Unverst√§ndlichkeit bekannt - der Computer kann eine gute Antwort geben, aber nicht erkl√§ren, was ihn zu dieser Schlussfo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ein neuer Ansatz zum Verst√§ndnis des maschinellen Denkens</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440426/"><h3>  Neuronale Netze sind f√ºr ihre Unverst√§ndlichkeit bekannt - der Computer kann eine gute Antwort geben, aber nicht erkl√§ren, was ihn zu dieser Schlussfolgerung gef√ºhrt hat.  Bin Kim entwickelt einen ‚Äûmenschlichen √úbersetzer‚Äú, damit wir ihn verstehen k√∂nnen, wenn k√ºnstliche Intelligenz ausf√§llt. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/069/0a0/2c1/0690a02c197e3af97ec6e5cec9754fd6.jpg"><br>  <i>Bean Kim, eine Forscherin bei Google Brain, entwickelt eine M√∂glichkeit, ein maschinelles Lernsystem √ºber ihre Entscheidungen zu befragen.</i> <br><br>  Wenn der Arzt Ihnen sagt, dass Sie operiert werden m√ºssen, m√∂chten Sie herausfinden, warum - und Sie werden erwarten, dass seine Erkl√§rung f√ºr Sie von Bedeutung ist, auch wenn Sie nicht als Arzt ausgebildet wurden.  Kim, ein Forscher bei Google Brain, glaubt, dass wir dasselbe von k√ºnstlicher Intelligenz (KI) erwarten k√∂nnen sollten.  Sie ist Spezialistin f√ºr ‚Äûinterpretiertes‚Äú maschinelles Lernen (MO) und m√∂chte eine KI erstellen, die jedem ihre Handlungen erkl√§ren kann. <br><a name="habracut"></a><br>  Seit vor zehn Jahren verbreitete sich die Technologie der neuronalen Netze hinter der KI immer mehr. Dank der F√§higkeit, aus den Daten zu lernen und nach Mustern in ihnen zu suchen, konnte sie alle Prozesse von der Sortierung von E-Mails bis zur Suche nach neuen Medikamenten transformieren.  Diese F√§higkeit hat jedoch einen unerkl√§rlichen Haken: Die Komplexit√§t, die es modernen neuronalen Netzen mit gr√ºndlicher Schulung erm√∂glicht, erfolgreich das Autofahren zu lernen und Betrug mit Versicherungen zu erkennen, macht es Experten fast unm√∂glich, die Prinzipien ihrer Arbeit zu verstehen.  Wenn ein neuronales Netzwerk darauf trainiert ist, nach Patienten mit einem Risiko f√ºr Leberkrebs oder Schizophrenie zu suchen - und ein solches System namens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Patient wurde 2015</a> im Mount Sinai Hospital in New York eingef√ºhrt -, gibt es keine M√∂glichkeit, herauszufinden, welche Datenmerkmale Das neuronale Netzwerk ‚Äûachtet darauf‚Äú.  Dieses ‚ÄûWissen‚Äú verteilt sich auf viele Schichten k√ºnstlicher Neuronen, von denen jede Verbindungen zu Hunderten oder Tausenden anderer Neuronen hat. <br><br>  Da immer mehr Branchen versuchen, ihre Entscheidungsprozesse mithilfe von KI zu automatisieren oder zu verbessern, scheint dieses ‚ÄûBlack-Box‚Äú -Problem weniger ein technologischer als vielmehr ein grundlegender Fehler zu sein.  Ein Projekt von DARPA namens XAI (Abk√ºrzung f√ºr ‚ÄûExplanable AI‚Äú, eXplainable AI) untersucht dieses Problem aktiv, und die Interpretierbarkeit r√ºckt von den Forschungslinien im Bereich MO n√§her an sein Zentrum heran.  "Die KI befindet sich in einem kritischen Moment, in dem wir Menschen versuchen herauszufinden, ob diese Technologie f√ºr uns geeignet ist", sagt Kim.  "Wenn wir das Problem der Interpretierbarkeit nicht l√∂sen, werden wir mit dieser Technologie wahrscheinlich nicht weitermachen k√∂nnen, und vielleicht werden wir sie einfach ablehnen." <br><br>  Kim und Kollegen von Google Brain haben k√ºrzlich das TCAV-System (Testing with Concept Activation Vectors) entwickelt, das sie als menschliche √úbersetzerin beschreibt und mit dem der Benutzer der AI-Blackbox eine Frage stellen kann Inwieweit war ein bestimmtes hochrangiges Konzept an der Entscheidungsfindung beteiligt?  Wenn das MO-System beispielsweise darauf trainiert ist, Zebrabilder zu finden, kann eine Person TCAV bitten, zu beschreiben, wie viel das Konzept der ‚ÄûStreifen‚Äú f√ºr den Entscheidungsprozess ausmacht. <br><br>  TCAV wurde urspr√ºnglich an Modellen getestet, die f√ºr die Erkennung von Bildern trainiert wurden, funktioniert jedoch auch mit Modellen, die f√ºr die Textverarbeitung oder bestimmte Aufgaben der Datenvisualisierung entwickelt wurden, z. B. EEG-Diagramme.  "Es ist verallgemeinert und einfach - es kann mit vielen verschiedenen Modellen verbunden werden", sagt Kim. <br><br>  Quanta sprach mit Kim dar√ºber, was Interpretierbarkeit bedeutet, wer sie braucht und warum sie wichtig ist. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4c8/f00/6c4/4c8f006c4ccacab3d668b366f5cbecee.jpg"><br><br>  <b>Sie haben sich in Ihrer Karriere auf die ‚ÄûInterpretierbarkeit‚Äú f√ºr das MO konzentriert.</b>  <b>Aber was genau bedeutet dieser Begriff?</b> <br><br>  Die Interpretierbarkeit hat zwei Zweige.  Eine davon ist die Interpretierbarkeit f√ºr die Wissenschaft: Wenn Sie das neuronale Netzwerk als Untersuchungsobjekt betrachten, k√∂nnen Sie wissenschaftliche Experimente durchf√ºhren, um die Vor- und Nachteile des Modells, die Gr√ºnde f√ºr seine Reaktion usw. wirklich zu verstehen. <br><br>  Der zweite Zweig, auf den ich mich haupts√§chlich konzentriere, ist die Interpretierbarkeit f√ºr die Schaffung einer KI, die Fragen beantworten kann.  Sie m√ºssen nicht jedes Detail des Modells verstehen.  Unser Ziel ist es jedoch, genug zu verstehen, damit dieses Tool sicher verwendet werden kann. <br><br>  <b>Aber wie kann man an ein System glauben, wenn man nicht ganz versteht, wie es funktioniert?</b> <br><br>  Ich werde Ihnen eine Analogie geben.  Angenommen, in meinem Garten gibt es einen Baum, den ich f√§llen m√∂chte.  Ich habe eine Kettens√§ge daf√ºr.  Ich verstehe nicht genau, wie eine Kettens√§ge funktioniert.  Aber die Anweisungen sagen: "Etwas, mit dem man vorsichtig umgehen muss, um sich nicht zu schneiden."  Wenn ich Anweisungen habe, verwende ich besser eine Kettens√§ge anstelle einer Hands√§ge - letztere ist leichter zu verstehen, aber ich m√ºsste sie f√ºnf Stunden lang sehen. <br><br>  <b>Sie verstehen, was es bedeutet, zu ‚Äûschneiden‚Äú, auch wenn Sie nicht alles √ºber den Mechanismus wissen, der dies erm√∂glicht.</b> <br><br>  Ja  Der zweite Zweig der Interpretierbarkeit hat folgenden Zweck: K√∂nnen wir das Tool so gut verstehen, dass es sicher verwendet werden kann?  Und wir k√∂nnen dieses Verst√§ndnis schaffen, indem wir best√§tigen, dass sich n√ºtzliches menschliches Wissen im Instrument widerspiegelt. <br><br>  <b>Aber wie macht die ‚ÄûReflexion menschlichen Wissens‚Äú die Black Box der KI verst√§ndlicher?</b> <br><br>  Hier ist ein weiteres Beispiel.  Wenn der Arzt das MO-Modell verwendet, um eine Krebsdiagnose zu stellen, muss der Arzt wissen, dass das Modell nicht einfach eine zuf√§llige Korrelation in den Daten ausw√§hlt, die wir nicht ben√∂tigen.  Eine M√∂glichkeit, dies zu √ºberpr√ºfen, besteht darin, zu best√§tigen, dass das MO-Modell ungef√§hr das Gleiche tut, was der Arzt tun w√ºrde.  Das hei√üt, um zu zeigen, dass sich das diagnostische Wissen des Arztes im Modell widerspiegelt. <br><br>  Wenn ein Arzt beispielsweise nach einer geeigneten Zellinstanz f√ºr die Krebsdiagnose sucht, sucht er nach etwas, das als ‚Äûverschmolzene Dr√ºse‚Äú bezeichnet wird.  Er wird auch Indikatoren wie das Alter des Patienten und die Frage ber√ºcksichtigen, ob er sich in der Vergangenheit einer Chemotherapie unterzogen hat.  Diese Faktoren oder Konzepte werden von einem Arzt ber√ºcksichtigt, der versucht, Krebs zu diagnostizieren.  Wenn wir zeigen k√∂nnen, dass das MO-Modell auch auf sie aufmerksam macht, ist das Modell verst√§ndlicher, da es das menschliche Wissen von √Ñrzten widerspiegelt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/8Bi-EhFPSLk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  <b>Damit befasst sich TCAV - zeigt, welche √ºbergeordneten Konzepte das MO-Modell f√ºr die Entscheidungsfindung verwendet?</b> <br><br>  Ja  Zuvor erkl√§rten Interpretierbarkeitsmethoden nur, was das neuronale Netzwerk in Bezug auf ‚ÄûEingabemerkmale‚Äú tut.  Was bedeutet das?  Wenn Sie ein Bild haben, ist jedes seiner Pixel eine Eingabefunktion.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yang Lekun</a> (Pionier des Deep Learning, Direktor f√ºr KI-Forschung bei Facebook) sagte, dass er diese Modelle als super interpretierbar betrachte, da Sie jeden Knoten des neuronalen Netzwerks betrachten und die numerischen Werte f√ºr jedes der Eingabefunktionen sehen k√∂nnen.  F√ºr Computer mag dies geeignet sein, aber die Leute denken anders.  Ich sage Ihnen nicht: "Sehen Sie sich Pixel von 100 bis 200 an, deren RGB-Werte 0,2 und 0,3 betragen."  Ich sage: "Dies ist ein Bild eines sehr zotteligen Hundes."  Menschen kommunizieren auf diese Weise - durch Konzepte. <br><br>  <b>Wie √ºbersetzt TCAV zwischen Eingabefunktionen und Konzepten?</b> <br><br>  Kehren wir zum Beispiel eines Arztes zur√ºck, der das MO-Modell verwendet, das bereits darauf trainiert wurde, Bilder von Zellproben nach Krebs zu klassifizieren.  Als Arzt m√ºssen Sie herausfinden, wie wichtig das Konzept der ‚Äûverschmolzenen Dr√ºsen‚Äú f√ºr das Modell war, um positive Vorhersagen f√ºr Krebs zu treffen.  Zun√§chst sammeln Sie beispielsweise 20 Bilder, die Beispiele f√ºr verschmolzene Dr√ºsen zeigen.  Anschlie√üend verbinden Sie diese beschrifteten Beispiele mit dem Modell. <br><br>  Dann f√ºhrt TCAV in sich das sogenannte aus  "Empfindlichkeitspr√ºfung".  Wie stark steigt die Wahrscheinlichkeit einer positiven Krebsvorhersage, wenn wir diese markierten Bilder der verschmolzenen Dr√ºsen hinzuf√ºgen?  Die Antwort kann durch eine Zahl von 0 bis 1 gesch√§tzt werden. Und dies sind Ihre Punkte in TCAV.  Wenn die Wahrscheinlichkeit zunahm, war dieses Konzept f√ºr das Modell wichtig.  Wenn nicht, ist dieses Konzept nicht wichtig. <br><br>  <b>"Konzept" ist ein vager Begriff.</b>  <b>Gibt es Konzepte, die mit TCAV nicht funktionieren?</b> <br><br>  Wenn Sie ein Konzept nicht mit einer Teilmenge Ihres Datensatzes beschreiben k√∂nnen, funktioniert es nicht.  Wenn Ihr MO-Modell in Bildern trainiert ist, sollte das Konzept visuell ausgedr√ºckt werden.  Wenn ich zum Beispiel das Konzept der Liebe visuell ausdr√ºcken m√∂chte, wird es ziemlich schwierig sein, dies zu tun. <br><br>  Wir pr√ºfen das Konzept ebenfalls sorgf√§ltig.  Wir haben ein statistisches √úberpr√ºfungsverfahren, das den Konzeptvektor ablehnt, wenn er einen zuf√§lligen Effekt auf das Modell hat.  Wenn Ihr Konzept diesen Test nicht besteht, sagt TCAV: "Ich wei√ü nicht, dieses Konzept scheint f√ºr das Modell nicht wichtig zu sein." <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e7d/2e4/bb5/e7d2e4bb5cb0093890c6938abdb5acdc.jpg"><br><br>  <b>Ist das TCAV-Projekt mehr auf den Aufbau von KI-Vertrauen ausgerichtet als auf die Verallgemeinerung seines Verst√§ndnisses?</b> <br><br>  Nein, - und ich werde erkl√§ren, warum, da dieser Unterschied sehr subtil ist. <br><br>  Aus vielen Studien auf dem Gebiet der Kognitionswissenschaft und Psychologie wissen wir, dass die Menschen sehr vertrauensvoll sind.  Dies bedeutet, dass es sehr leicht ist, einen Menschen zu t√§uschen, indem man ihn zwingt, an etwas zu glauben.  Das Ziel der MO-Interpretierbarkeit ist das Gegenteil.  Es besteht darin, eine Person dar√ºber zu informieren, dass die Verwendung eines bestimmten Systems nicht sicher ist.  Das Ziel ist es, die Wahrheit aufzudecken.  Daher ist ‚ÄûVertrauen‚Äú nicht das richtige Wort. <br><br>  <b>Das Ziel der Interpretierbarkeit ist es also, potenzielle Fehler in der KI-Argumentation aufzudecken?</b> <br><br>  Ja genau. <br><br>  <b>Wie kann sie die M√§ngel aufdecken?</b> <br><br>  TCAV kann verwendet werden, um dem Modell eine Frage zu Konzepten zu stellen, die nicht mit dem Forschungsbereich zusammenh√§ngen.  Zur√ºck zum Beispiel von √Ñrzten, die KI verwenden, um die Wahrscheinlichkeit von Krebs vorherzusagen.  √Ñrzte k√∂nnten pl√∂tzlich denken: ‚ÄûAnscheinend gibt die Maschine f√ºr viele Bilder, bei denen die Farbe leicht nach blau verschoben ist, positive Vorhersagen f√ºr das Vorhandensein von Krebs.  Wir glauben, dass dieser Faktor nicht ber√ºcksichtigt werden sollte. ‚Äú  Und wenn sie einen hohen TCAV-Wert f√ºr Blau erhalten, bedeutet dies, dass sie ein Problem in ihrem MO-Modell gefunden haben. <br><br>  <b>TCAV kann an vorhandene KI-Systeme geh√§ngt werden, die nicht interpretiert werden k√∂nnen.</b>  <b>Warum nicht sofort interpretierte Systeme anstelle von Black Boxes erstellen?</b> <br><br>  Es gibt einen Zweig der Studie zur Interpretierbarkeit, der sich auf die Erstellung urspr√ºnglich interpretierter Modelle konzentriert, die die Argumentation einer Person widerspiegeln.  Aber ich denke schon: Jetzt sind wir bereits voll mit vorgefertigten KI-Modellen, die bereits zur L√∂sung wichtiger Probleme verwendet werden, und bei der Erstellung haben wir zun√§chst nicht an Interpretierbarkeit gedacht.  So einfach zu essen.  Viele von ihnen arbeiten bei Google!  Sie k√∂nnen sagen: "Die Interpretierbarkeit ist so n√ºtzlich, dass wir ein anderes Modell f√ºr Sie erstellen k√∂nnen, um das zu ersetzen, das Sie haben."  Nun, viel Gl√ºck. <br><br>  Und was dann tun?  Wir m√ºssen diesen entscheidenden Moment noch durchlaufen, um zu entscheiden, ob diese Technologie f√ºr uns n√ºtzlich ist oder nicht.  Daher arbeite ich an Interpretierbarkeitsmethoden nach dem Training.  Wenn Ihnen jemand ein Modell gegeben hat und Sie es nicht √§ndern k√∂nnen, wie gehen Sie dann mit der Aufgabe um, Erkl√§rungen zu seinem Verhalten zu generieren, damit Sie es sicher verwenden k√∂nnen?  Genau das macht TCAV. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/05d/9ba/522/05d9ba52265cd534871c8f62f4ce05f7.jpg"><br><br>  <b>Mit TCAV k√∂nnen Menschen AI nach der Wichtigkeit bestimmter Konzepte fragen.</b>  <b>Aber was ist, wenn wir nicht wissen, was wir fragen sollen - was ist, wenn wir m√∂chten, dass die KI es nur erkl√§rt?</b> <br><br>  Derzeit arbeiten wir an einem Projekt, das automatisch Konzepte f√ºr Sie findet.  Wir nennen es DTCAV - das Er√∂ffnungs-TCAV.  Ich denke jedoch, dass das Hauptproblem der Interpretierbarkeit darin besteht, dass Menschen an diesem Prozess teilnehmen und dass wir Menschen und Maschinen die Kommunikation erm√∂glichen. <br><br>  In vielen F√§llen verf√ºgen Experten auf einem bestimmten Gebiet bereits √ºber eine Liste von Konzepten, die f√ºr sie wichtig sind, wenn sie mit Anwendungen arbeiten, von denen viel abh√§ngt.  Wir bei Google Brain sind in den medizinischen Anwendungen der KI st√§ndig damit konfrontiert.  Sie ben√∂tigen keine Reihe von Konzepten - sie m√∂chten Konzeptmodelle bereitstellen, die f√ºr sie interessant sind.  Wir arbeiten mit einem Arzt zusammen, der diabetische Retinopathie und Augenkrankheiten behandelt, und als wir ihr von TCAV erz√§hlten, war sie sehr gl√ºcklich, weil sie bereits eine ganze Reihe von Hypothesen dar√ºber hatte, was das Modell tun kann, und jetzt kann sie alle auftretenden Fragen √ºberpr√ºfen.  Dies ist ein gro√ües Plus und eine sehr benutzerorientierte M√∂glichkeit, kollaboratives maschinelles Lernen zu implementieren. <br><br>  <b>Sie denken, dass die Menschheit ohne Interpretierbarkeit die KI-Technologie einfach aufgeben kann.</b>  <b>Bewerten Sie eine solche Option angesichts der sich bietenden M√∂glichkeiten wirklich als real?</b> <br><br>  Ja  Genau das ist mit Expertensystemen passiert.  In den 1980er Jahren haben wir festgestellt, dass sie bei der L√∂sung einiger Probleme billiger sind als Menschen.  Und wer nutzt heute Expertensysteme?  Niemand.  Und danach kam der KI-Winter. <br><br>  Bisher scheint dies nicht wahrscheinlich, da so viel Hype und Geld in KI investiert wird.  Aber auf lange Sicht denke ich, dass die Menschheit - vielleicht aus Angst, vielleicht aus Mangel an Beweisen - entscheiden kann, dass diese Technologie nicht zu uns passt.  Das ist m√∂glich. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de440426/">https://habr.com/ru/post/de440426/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de440414/index.html">√úber Linter, Codequalit√§t, Qualit√§t im Allgemeinen und Qualit√§tsmanagement</a></li>
<li><a href="../de440416/index.html">Kolonie. Kapitel 25: Ausgehen</a></li>
<li><a href="../de440420/index.html">Willkommen zum Devleads Meetup am 21. Februar</a></li>
<li><a href="../de440422/index.html">Wenn Sie f√ºr die Qualit√§t des Geschenks verantwortlich sind. Die Geschichte eines Blockchain-Experiments</a></li>
<li><a href="../de440424/index.html">Algorithmus des Denkens und des Bewusstseins</a></li>
<li><a href="../de440428/index.html">SMAA: Verbesserte morphologische Gl√§ttung von Subpixeln</a></li>
<li><a href="../de440430/index.html">Woher kommt der Slogan ‚ÄûSei nicht b√∂se‚Äú?</a></li>
<li><a href="../de440432/index.html">Freitag SciFi √ºber die Berufe der Zukunft: "Real Girls"</a></li>
<li><a href="../de440434/index.html">Russische Autoindustrie: Der Weg zu additiven Technologien</a></li>
<li><a href="../de440436/index.html">Java Practical Tasks - f√ºr Kurse und andere Aktivit√§ten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>