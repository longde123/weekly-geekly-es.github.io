<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🖖🏽 📯 😅 HighLoad ++, Mikhail Makurov (Intersvyaz): pengalaman dalam membuat layanan Zabbix cadangan dan berkerumun 🖕🏻 ☝️ 🎢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zabbix adalah sistem pemantauan terbuka populer yang digunakan oleh sejumlah besar perusahaan. Saya akan berbicara tentang pengalaman membuat cluster ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>HighLoad ++, Mikhail Makurov (Intersvyaz): pengalaman dalam membuat layanan Zabbix cadangan dan berkerumun</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/485534/">  Zabbix adalah sistem pemantauan terbuka populer yang digunakan oleh sejumlah besar perusahaan.  Saya akan berbicara tentang pengalaman membuat cluster pemantauan. <br><br>  Dalam laporan itu, saya secara singkat menyebutkan perubahan yang dibuat sebelumnya (tambalan), yang secara signifikan memperluas kemampuan sistem dan menyiapkan dasar untuk klaster (mengunggah riwayat ke "Clickhouse", polling asinkron).  Dan saya akan mempertimbangkan secara rinci masalah yang muncul selama pengelompokan sistem - menyelesaikan konflik identitas dalam database, sedikit tentang teorema CAP dan pemantauan dengan database terdistribusi, tentang nuansa Zabbix yang bekerja dalam mode cluster: cadangan dan koordinasi server dan proksi, tentang "domain pemantauan" dan tampilan baru pada arsitektur sistem. <br><br>  Saya akan berbicara singkat tentang cara memulai sebuah cluster di rumah, di mana mendapatkan sumber, dan apa yang tambahan.  pengaturan akan diperlukan untuk cluster. <br><br><img src="https://habrastorage.org/webt/uv/cy/1r/uvcy1rw-6ttiumnzmjuizgl4ml4.jpeg"><br><br>  HighLoad ++ Siberia 2019. Tomsk Hall.  24 Juni, jam 5 malam  Abstrak dan <a href="https://www.highload.ru/siberia/2019/abstracts/5210">presentasi</a> .  Konferensi HighLoad ++ berikutnya akan diadakan pada 6 dan 7 April 2020 di St. Petersburg.  Detail dan tiket di <a href="http://bit.ly/2sSxgBx">sini</a> . <a name="habracut"></a><br><br>  <b>Mikhaili Makurov (selanjutnya - MM):</b> - Saya bekerja untuk perusahaan penyedia.  Penyedia disebut Intersvyaz, ia bekerja di kota Chelyabinsk.  Kami memiliki sekitar 1,5 juta orang.  Dan agar penyedia dapat bekerja, ada infrastruktur yang sangat besar.  Kami memiliki sekitar 70 ribu peralatan: sakelar, perangkat IoT ... - banyak hal yang perlu dipantau.  Secara khusus, laporan ini adalah tentang penggunaan Zabbix, tentang membangun sebuah cluster berdasarkan Zabbix untuk pemantauan infrastruktur. <br><br>  Saya 12 tahun di penyedia.  Sekarang saya tidak melakukan hal-hal teknis sama sekali, ini lebih tentang mengelola orang.  Dan ini (hal-hal teknis) sebenarnya adalah hobi saya.  Saya akan mengembangkan topik ini sedikit. <br><br><h3>  Memantau masalah </h3><br>  Saya pikir saya beruntung.  Sekitar satu setengah tahun yang lalu, saya berakhir di sebuah proyek yang terdengar seperti ini: "Kita perlu menyelesaikan beberapa masalah dengan pemantauan kita."  Saya mewarisi zona tanggung jawab (pemantauan), yang terdiri dari sekelompok server, khususnya 21 server: <br><br><img src="https://habrastorage.org/webt/eu/om/mx/euommxg6onyd7rdbncwqbnrhvtc.jpeg"><br><br>  Ada 4 server yang kuat dan 15 proxy - semuanya perangkat keras.  Ada beberapa keluhan tentang pemantauan ini.  Yang pertama adalah banyak.  Kami belum memiliki satu server dengan penyedia yang menghabiskan begitu banyak ruang.  Ini adalah uang, listrik ... Sebenarnya, ini bukan masalah besar. <br><br><img src="https://habrastorage.org/webt/ui/jf/u3/uijfu32ebg7upocsepwtvptenk4.jpeg"><br><br>  Masalah besar adalah bahwa pemantauan tidak sesuai dengan berapa banyak yang kami inginkan darinya.  Bagi mereka yang belum secara aktif menggunakan Zabbix, ini adalah dasbor yang menunjukkan keterlambatan pada pemeriksaan: <br><br><img src="https://habrastorage.org/webt/ig/jd/ti/igjdtilbejkchus711uvbzzeuyo.jpeg"><br><br>  Sebagian besar cek kami berada di zona merah.  Mereka berlari lebih dari 10 menit lebih lambat dari yang kita inginkan, yaitu, mereka terlambat 10 menit.  Itu tidak terlalu menyenangkan, tetapi masih mungkin untuk hidup lebih atau kurang.  Masalah terbesar adalah ini: <br><br><img src="https://habrastorage.org/webt/vf/ai/nw/vfainwua91q6pgxnj2msbx_tzle.jpeg"><br><br>  Itu adalah sistem pemantauan jaringan kerja.  Ketika pekerjaan yang direncanakan dilakukan, segmen ribuan jatuh pada lima sakelar.  Bersama-sama dengan sakelar ini, sakelar dan pemantauan dilupakan.  Ketika semuanya dipulihkan, dua jam kemudian dan pemantauan dipulihkan.  Itu sangat tidak menyenangkan, dan frasa ini harus ada di setiap laporan: <br><br><img src="https://habrastorage.org/webt/aw/o-/t3/awo-t3j-xbu181dyneis-q4ozjo.jpeg"><br><br><h3>  "Kita harus melakukan sesuatu dengan proyek ini!" </h3><br>  Dan di sini saya akan menceritakan dua kisah.  Kemudian kami mencoba untuk pergi secara bersamaan dalam dua cara.  Kami memiliki grup integrasi - ia memilih cara untuk membangun sistem modular (ada laporan yang sangat keren dari Avito ke Highload pada November tahun lalu di Moskow - mereka membicarakan hal ini): <br><br><img src="https://habrastorage.org/webt/qg/f5/nv/qgf5nvonycfltfd8vge1tbeloj8.jpeg"><br><br><h3>  Zabbix = orang + API + efisiensi </h3><br>  Orang-orang dari potongan-potongan kecil mulai membangun sistem.  Dan dengan beberapa peminat, saya terus mengerjakan Zabbix.  Ada alasan untuk itu.  Apa alasannya? <br><br><ul><li>  Pertama, ada API keren.  Dan ketika Anda memiliki 60-70 ribu elemen pemantauan, jelas bahwa ini semua hanya berfungsi secara otomatis - Anda tidak dapat menambahkan begitu banyak tangan tanpa kesalahan. </li><li>  Personil.  Ada shift pemantauan bertugas yang berlangsung 24/7.  Ini bukan spesialis IT, ini adalah orang-orang yang bertugas.  Kami menunjukkan "Grafan" beberapa sistem lain - sulit bagi mereka.  Ada admin yang terbiasa dengan keanekaragaman, kenyamanan pemantauan di Zabbix itu sendiri: template, deteksi otomatis - dan itu semua keren! </li><li>  Zabbix bisa efektif. </li></ul><br><h3>  Apakah database SQL melambat?  Satu jawaban - Clickhouse </h3><br>  Alasan pertama jelas.  Kami kemudian bekerja pada MySQL, dan kami berlari ke sekitar 6-7 ribu metrik per detik, kami melihat penundaan yang konstan pada disk. <br><br><img src="https://habrastorage.org/webt/kb/ke/wf/kbkewfpy7l6b2qdexbuljdwr6ia.jpeg"><br><br>  Hari ini sudah 100 kali terdengar: satu-satunya jawaban adalah Clickhouse: <br><br><img src="https://habrastorage.org/webt/w-/lm/iw/w-lmiwpxynx-rmj4spjyv_gc1cc.jpeg"><br><br>  Dalam struktur kueri, sebagian besar kueri (profil kami dalam beberapa jam) adalah catatan metrik.  Menulis metrik ke database SQL sangat mahal.  Di sini TimeScaleDB muncul ... Kemudian kami memiliki "Clickhouse" yang beroperasi selama sekitar satu tahun untuk tugas-tugas lain (kami mengerjakan data besar, kami memiliki aplikasi besar - secara umum, penyedia sekarang merupakan bisnis TI keseluruhan). <br><br>  Setelah melihat grafik yang indah dari Internet (bahwa "Clickhouse" ratusan kali lebih cepat, yang membutuhkan ruang sangat sedikit) dan memiliki pengalaman saat ini, kami menulis modul HistoryStorage kami untuk "Zabbix" sehingga dapat menyimpan data "Clickhouse" secara langsung (yaitu, bukan dari ekspor file, tetapi langsung on the fly). <br><br><img src="https://habrastorage.org/webt/tp/f4/p2/tpf4p2z5jftziqwqhtdtxsb99-g.jpeg"><br><br>  Selain itu, kami menulis modul untuk "depan".  Semua gambar indah ini di panel admin Zabbix dapat dibangun dari Clickhouse.  Jelas bahwa API juga berfungsi. <br><br>  Efeknya kira-kira sama - SQL server sebagai entitas yang berdedikasi tidak menjadi sepenuhnya, yaitu beban turun menjadi nol.  Apa yang paling luar biasa, kami sudah memiliki cluster "Clickhouse" khusus: ketika kami memberikan semua beban kami di sana, itu meningkat dari 6 menjadi 10 ribu metrik.  Orang-orang yang mengelola berkata: "Tapi kami tidak melihat sesuatu yang telah datang.  Tidak! ” <br><br><h3>  Bagaimana kami memperluas Clickhouse </h3><br>  Saya akan mengatakan lebih jauh: untuk pengujian kami mencoba memuat hingga 140-150 ribu metrik per detik (kami tidak bisa lagi memeras dari Zabbix, nanti saya akan mengatakan alasannya), dan Clickhouse juga tidak melihat beban ini.  Artinya, sangat nyaman, beban dingin.  Secara umum, ada modul seperti itu. <br><br>  Selain itu, kami sedikit mengembangkannya: <br><br><img src="https://habrastorage.org/webt/_u/ha/aj/_uhaajlqcp9mcjuh-ih8sc-dn3k.jpeg"><br><br>  Dalam versi kami, Anda dapat mematikan nanodetik.  Anda mungkin tahu: Zabbix menulis detik dan nanodetik dalam dua bidang.  Di bidang "Clickhouse" di mana variabilitasnya sangat besar, membutuhkan banyak ruang. <br><br>  Ngomong-ngomong, tentang tempat itu.  Satu metrik di Clickhouse (saat ini kami memiliki sekitar 700 miliar metrik yang tercatat) membutuhkan 2,9 byte.  Menurut dokumentasi Zabbix, satu metrik dalam database SQL memakan waktu dari 40 hingga 100 byte.  Mematikan nanodetik menghemat 40% lagi, yaitu sekitar 1,5 byte per metrik.  Artinya, "Clickhouse" sangat efektif dalam hal lokasi. <br><br>  Atas permintaan orang-orang kami yang terlibat dalam pembelajaran mesin, kami membuat pilihan sehingga kami dapat menulis host dan nama metrik.  Karena variabilitas data besar, ini tidak memakan banyak ruang tambahan, meskipun fakta bahwa data teks bisa signifikan (belum diverifikasi dengan tes panjang). <br><br>  Plus, kami membuat dua tambahan, karena kami mengembangkan Zabbix dan seringkali harus menariknya.  Tambahan yang sangat keren: di awal, karena "Clickhouse" memungkinkan Anda membaca jutaan catatan, kami dapat mengisi cache riwayat.  Pada awalnya, kami ditunda selama 30-40 detik tambahan, tetapi kami mendapatkan layanan segera diluncurkan dengan cache yang hangat. <br><br>  Dalam kasus di mana lebih mudah untuk mengumpulkan dari infrastruktur, masih ada opsi seperti itu: untuk melarang membaca dari cache untuk beberapa waktu.  Lebih baik bekerja cepat selama 5 menit, tidak menghitung pemicu, dan kemudian cache akan terisi - jika Anda tidak melakukan ini, stagnasi para penyelesai sejarah dimulai. <br><br>  Secara umum, ada modul "Clickhouse".  Itu bisa digunakan. <br><br><h3>  Efisiensi pemungutan suara </h3><br>  Terlepas dari kenyataan bahwa kami kemudian memecahkan masalah dengan pangkalan, rem dan masalah dengan lima belas proxy masih tetap ada.  Mereka terhubung dengan ini: <br><br><img src="https://habrastorage.org/webt/un/ec/u8/unecu8ujsgk7scstaonbt3dueim.jpeg"><br><br>  Ini adalah pipa pemrosesan data utama di Zabbix.  Ada tahap pengumpulan data, ada preprocessing, dan ada sinkronisasi sejarah yang melakukan semua pekerjaan (perhitungan pemicu, peringatan, menyimpan riwayat).  Hambatan cache ternyata adalah: <br><br><img src="https://habrastorage.org/webt/er/jg/ei/erjgein-xwdpitoa8eql3b5dw1g.jpeg"><br><br>  Mengapa pemungutan suara lambat?  Karena utas yang membuat permintaan masuk ke antrian dalam konfigurasi cache untuk metrik unit dan memblokirnya.  Ada tempat lain, tetapi mereka tidak begitu sempit.  Misalnya, ada preprocessing sendiri dan ada History Cache.  Pada SQL kami, kami mendapatkan batasan berikut: <br><br><img src="https://habrastorage.org/webt/-w/ys/-b/-wys-btlu1uxulrqcinsvbmac5a.jpeg"><br><br>  Mungkin ini disebabkan oleh fakta bahwa dalam kasus kami, basisnya sekitar 5 juta metrik, yang kami hapus.  Dengan semua optimasi yang kami lakukan, kami dapat memperoleh 70 ribu metrik dalam bottleneck (pada Cache Konfigurasi), tetapi hanya dalam kasus ketika kami memprosesnya secara massal. <br><br>  Apa itu pemrosesan massal?  Poller pergi ke Konfigurasi Cache dan mengambil tugas bukan untuk satu metrik, tetapi untuk 4 atau 8 ribu.  Pada saat yang sama, ia mendapat kesempatan luar biasa: ia sekarang dapat melakukan polling secara tidak sinkron, karena ia mendapatkan 4 ribu metrik ... Mengapa mereka melakukan satu demi satu?  Anda bisa langsung menanyakan semuanya! <br><br><h3>  Polling asinkron lebih efisien daripada proxy! </h3><br>  Untuk jenis utama yang digunakan oleh penyedia - ini adalah SNMP dan AGEN, kami menulis ulang polling ke mode asinkron, dan secara agregat ini memberikan peningkatan kecepatan dari 100 menjadi 200 kali.  Kami memiliki 15 proxy, kami membaginya menjadi 150 - mereka benar-benar hilang.  Akibatnya, semuanya berubah menjadi dua bank, yang hanya diperlukan untuk cadangan: <br><br><img src="https://habrastorage.org/webt/ax/7z/lk/ax7zlkk-yg9oafyuruz9blq7l-q.jpeg"><br><br>  Bank Uniprocessor (satu biaya Xeon 1280).  Inilah saatnya saya: <br><br><img src="https://habrastorage.org/webt/zq/qf/nc/zqqfncyu4fxo38fiegvr0kunhhw.jpeg"><br><br>  Sekitar 60% gratis, tetapi dering dari 60% hingga 40% ini menjalankan skrip periodik pada mesin itu sendiri (skrip eksternal).  Mereka dapat dioptimalkan sampai masalah dibuat. <br><br>  Skala adalah sesuatu seperti ini: <br><br><img src="https://habrastorage.org/webt/km/pa/nj/kmpanjjtpdm3660spp8haz7ypo0.jpeg"><br><br>  Ini adalah 62 ribu host, sekitar 5 juta metrik.  Kebutuhan kita saat ini adalah sekitar 20 ribu metrik per detik. <br><br>  Ya, seperti semuanya?  Kami memecahkan masalah kinerja, Riwayat diperluas, Polling luar biasa.  Apakah masalah teratasi?  Tidak juga ... Semuanya akan terlalu sederhana. <br><br>  Saya memainkan trik pada grafik sebelumnya (tidak semua ditampilkan): <br><br><img src="https://habrastorage.org/webt/ds/ag/k5/dsagk5zrh8bog_ogrq35w1uvawy.jpeg"><br><br>  Ada dua masalah.  Saya ingin mengatakan: "Bodoh, jalan."  Ada faktor manusia, ada peralatan. <br><br>  Satu server masih belum cukup.  Dalam sekitar satu tahun operasi, ada dua kasus dengan masalah perangkat keras - drive SSD dan yang lainnya.  Sebagian besar masalah adalah faktor manusia ketika orang melakukan semacam pengujian.  Di perusahaan kami, Zabbix digunakan sebagai layanan: semua departemen dapat menulis sesuatu sendiri di sana. <br><br>  Saya ingin berkembang.  Saya ingin tidak bergantung pada satu kaleng.  Saya ingin kami bisa tetap lebih kuat.  Dan saya ingin meningkatkan skala sesuai dengan prinsip Scale-out.  Bahkan tidak ada yang perlu dibicarakan di sini: untuk tumbuh, meningkatkan kapasitas satu kaleng, sudah tidak relevan selama 20 tahun. <br><br><img src="https://habrastorage.org/webt/yl/hy/vn/ylhyvn7za42rcr2allqhjzlmw2i.jpeg"><br><br><h3>  Cluster meminta ... </h3><br>  Di suatu tempat di bulan Desember, versi pertama muncul.  Unit gugus atom adalah apa yang diproses pada host yang terpisah.  Tuan rumah telah dipilih. <br><br><img src="https://habrastorage.org/webt/9j/yo/sy/9jyosycj6yj7ecx7k5mn0esrfnu.jpeg"><br><br>  Faktanya adalah bahwa di Zabbix ada koneksi yang cukup kuat antara item yang dapat berada di host yang sama, yaitu, pemicu dapat dihubungkan, mereka dapat diproses bersama dalam preprocessing.  Tetapi di antara host-host konektivitasnya tidak terlalu tinggi, jadi itu normal untuk menggunakan cluster ini antara node-node dari cluster - akan ada banyak lalu lintas di sana.  Tugas utama cluster adalah untuk menyetujui di antara mereka sendiri yang terlibat dalam host. <br><br>  Saya ingin mencapai batas maksimum 60-70 ribu metrik kami, karena selera makan datang bersama.  Kami memiliki orang yang terlibat dalam QoE ... Quality of Experience - sebuah analisis tentang bagaimana Internet bekerja untuk pelanggan berdasarkan metrik transit, yaitu, Anda memasok semua metrik TCP ke 1,5 juta orang, menuangkannya ke dalam pemantauan - ada banyak data. <br><br>  Dan saya menginginkan keandalan.  Saya menginginkannya jika sesuatu terjadi ... Petugas jaga shift menelepon, berkata, "Kami memiliki masalah dengan server," mematikannya, kami akan mencari tahu besok. <br><br><h3>  Cluster pertama </h3><br>  Versi pertama diimplementasikan berdasarkan etcd: <br><br><img src="https://habrastorage.org/webt/oh/kr/qu/ohkrqu9wh-gldd6q6yheyoztro4.jpeg"><br><br>  Etcd adalah penyimpanan nilai kunci terdistribusi yang digunakan dalam banyak proyek progresif (sejauh yang saya mengerti, di Kubernetes).  Semuanya luar biasa.  Etcd menyediakan alat yang sangat menarik - misalnya, itu memecahkan masalah memilih server utama.  Tapi masalah seperti itu ... <br><br>  Kami memiliki tiga tautan klasik "Zabbix": "web" - basis - server itu sendiri.  Dan kami menambahkan "Clickhouse" di sana, dan sekarang kami menambahkan etcd juga.  Admin mulai menggaruk di belakang kepala mereka: ada terlalu banyak dependensi di sini - mungkin tidak akan dapat diandalkan.  Dalam proses pengembangan, satu hal lagi menjadi jelas: di Zabbix sendiri sudah ada cara interserver komunikasi, itu hanya digunakan antara server dan proxy, yang disebut proses poller proxy: <br><br><img src="https://habrastorage.org/webt/4x/cf/jh/4xcfjhgqaf-2h33nnzrjduw8rn8.jpeg"><br><br>  Ini cukup keren untuk komunikasi interserver dengan perubahan minimal.  Ini memungkinkan etcd untuk tidak menggunakan (setidaknya sementara), sangat menyederhanakan kode, dan yang paling penting, bekerja pada kode yang telah diverifikasi (sepertinya 5 atau 7 tahun untuk kode ini). <br><br><h3>  Bagaimana server terkoordinasi dalam sebuah cluster? </h3><br>  Koordinasi dilakukan berdasarkan jenis, seperti protokol IGP.  Agar server mendapat prioritas (sekarang saya akan mengatakan mengapa ini perlu) dan untuk menghindari konflik dalam database SQL saat menulis log, setiap server diberi pengenal (sejauh ini secara manual) - ini adalah angka dari 0 hingga 63 (63 - itu hanya sebuah konstanta, mungkin lebih): <br><br><img src="https://habrastorage.org/webt/kb/ei/xf/kbeixf3xjk6fo4oucwf9yk9yi9c.jpeg"><br><br>  Server dengan pengidentifikasi maksimum menjadi "master".  Ketika kami meluncurkan kelompok uji pertama kami, hal pertama yang dikatakan admin kami adalah: “Wow!  Dan mari kita letakkan mereka di situs yang berbeda.  Baiklah, bagus! ”(Kami akan kembali ke sini).  Dan ketika seseorang telah mendistribusikan cluster, akan dimungkinkan untuk mengontrol bagaimana topologi didistribusikan kembali: ke mana peran "master" pergi jika terjadi penurunan pada server "Zabbix" utama: <br><br><img src="https://habrastorage.org/webt/fa/n8/nx/fan8nxkovjbepqscnyljtfbc00o.jpeg"><br><br>  Dalam hal ini, seperti ini: <br><br><img src="https://habrastorage.org/webt/5l/qu/ou/5lquouk5h0tkzky-e6_m8dfdkn4.jpeg"><br><br><h3>  Melangkah </h3><br>  Dalam Zabbix asli, ini dilakukan seperti ini: server itu sendiri bertanggung jawab untuk menghasilkan indeks kenaikan otomatis.  Untuk mencegah banyak kejadian menginjak tumit satu sama lain (agar tidak membuat log dengan indeks yang sama), melangkah digunakan: "Zabbix" dengan pengidentifikasi "1" akan menghasilkan kelipatan satu - 1, 11, 21;  dengan pengidentifikasi "7" - 7, 17, 27 (dengan nuansa). <br>  Kami melaju dengan pengubah. <br><br><img src="https://habrastorage.org/webt/t3/fo/2-/t3fo2-qoqlx0zwmxds7vc_7y2pw.jpeg"><br><br><h3>  Bagaimana server berinteraksi satu sama lain? </h3><br>  Ini adalah warisan Paket Hello IGP setiap 5 detik.  Jadi server tahu bahwa mereka memiliki tetangga.  Jadi "master" tahu bahwa ada tetangga di dekatnya, dan atas dasar ini, "master" memutuskan host mana yang dapat didistribusikan ke server mana. <br><br><img src="https://habrastorage.org/webt/um/vm/pn/umvmpn5x2hljbhbuzfp6b3umqpo.jpeg"><br><br>  Dengan demikian, ada konfigurasi.  Menurut ingatan lama, saya menyebutnya topologi.  Topologi pada dasarnya adalah daftar server dan host milik mereka. <br><br>  Protokolnya sederhana - ini JSON: <br><br><img src="https://habrastorage.org/webt/qd/xx/rz/qdxxrzw2rj3akue1h8nwcqum1cc.jpeg"><br><br>  Ini juga merupakan warisan dari Zabbix proxy dan komunikasi server Zabbix.  Secara umum, tidak masuk akal untuk menggunakan sesuatu yang lain.  Satu-satunya hal adalah bahwa dalam kasus Zabbix ada 4 byte (ZBXD), tetapi ini bukan intinya. <br><br>  Dalam paket halo, pengidentifikasi server ditransmisikan: ketika server mengirim paket, ia mengatakan pengidentifikasi dan versi topologinya - dengan cara ini server dengan cepat mengetahui bahwa ada versi baru dari topologi dan diperbarui dengan sangat cepat. <br><br>  Sebenarnya, topologi itu sendiri hanyalah sebuah pohon, daftar server.  Untuk setiap server, daftar host yang didukungnya: <br><br><img src="https://habrastorage.org/webt/xp/dv/ec/xpdvecth8pp7eaukwxmykgysqiu.jpeg"><br><br>  Dan kemudian muncul masalah yang menarik. <br><br><h3>  Ada frase ajaib - domain pemantauan </h3><br>  Apa gunanya  Dalam Zabbix klasik, semuanya sederhana - sikap yang tidak ambigu: host ini dimonitor oleh proxy ini, proxy ini memberikan data ke server.  Jika proxy belum diinstal (atau tidak diperlukan), maka server ini memonitor semua host: <br><br><img src="https://habrastorage.org/webt/x2/ly/fh/x2lyfh4prsystuyggxb0elwclx4.jpeg"><br><br>  Ketika kami memiliki banyak server, apa yang harus dilakukan?  Selain itu, mungkin ada masalah dengan fakta bahwa kami memiliki server yang didistribusikan secara geografis, dan server di beberapa kantor yang lambat bekerja di Kemerovo akan mulai mencoba memantau seluruh infrastruktur Novosibirsk. <br><br><img src="https://habrastorage.org/webt/ep/06/cy/ep06cyiqx3dx4kkyfer1wcgn8um.jpeg"><br><br>  Kami tidak menginginkan ini.  Kami ingin memiliki semacam mekanisme sehingga tidak semua server, tetapi yang kami pilih (mungkin berdasarkan geografi) dapat memonitor host tertentu.  Pada saat yang sama, kami ingin mengelola ini, dan kami ingin itu sederhana.  Untuk ini, ide pemantauan domain diciptakan.  Faktanya, ini adalah grup sederhana - hanya sudah ada grup dalam catatan. <br>  Dan ketika saya melakukan ini, orang-orang dari operasi berbicara dengan saya - mereka berkata: “Kelompok-kelompok itu sangat membingungkan kami.  Kami selalu mulai berpikir tentang kelompok normal. ”  Oleh karena itu, nama ini: domain pemantauan. <br><br>  Host secara jelas berhubungan: satu host - satu domain: <br><br><img src="https://habrastorage.org/webt/6x/hi/z5/6xhiz5ocnajnjzoxin17log0liq.jpeg"><br><br>  Domain host dapat menyertakan sejumlah server.  Server dapat berada di sejumlah domain.  Ini adalah hal yang sangat fleksibel.  Untuk memperluas fleksibilitas dan menghancurkan otak sepenuhnya, ada juga domain default: <br><br><img src="https://habrastorage.org/webt/ad/zl/2x/adzl2xgso7st_v9c7vg9-fyryvg.jpeg"><br><br>  Server yang menjadi anggota domain default dipantau oleh semua host yang tidak memiliki server langsung atau yang tidak memiliki domain pemantauan. <br><br>  Ini hanya memungkinkan kita untuk mengikat host secara topologis ke beberapa server dan mengontrol bagaimana host didistribusikan jika satu server jatuh: <br><br><img src="https://habrastorage.org/webt/fm/og/nr/fmognrgop74jgnsqkwtxdzddqgw.jpeg"><br><br>  Masalah berikutnya yang kami temui ... <br><br><h3>  Cluster: Berpikir Berbeda </h3><br>  Ketika kami memiliki banyak server, ada peluang baru untuk membangun sebuah cluster, untuk membangun topologi.  Ini sangat klasik ketika kami memiliki semacam situs pusat dan ada yang jauh;  atau, katakanlah, proksi tempat beban didelegasikan: <br><br><img src="https://habrastorage.org/webt/rn/ar/qd/rnarqdcbs1knledntrvqoibsozs.jpeg"><br><br>  Dalam kasus cluster Zabbix, ini dapat diimplementasikan dalam dua cara.  Anda dapat menggunakan cara klasik: cukup gandakan infrastruktur.  Di pusat, kami memiliki dua server yang membentuk sebuah cluster, dapat mengatur ulang host atau mengambil beban pada diri mereka sendiri jika tetangga jatuh.  Dengan demikian, Anda dapat meningkatkan proxy tambahan di server yang sama - kami mendapatkan cadangan ganda: <br><br><img src="https://habrastorage.org/webt/bx/es/g4/bxesg4ijxn4b9pirro22u3-omoa.jpeg"><br><br>  Anda dapat menggunakan "fitur" baru dan melakukan ini: <br><br><img src="https://habrastorage.org/webt/v0/-o/ki/v0-okiiibpq1yyurxipbugjqfwq.jpeg"><br><br>  Hal utama adalah tidak pergi ke situasi di mana server yang secara geografis jauh sedang memantau beberapa infrastruktur besar di tempat lain.  Ini lebih merupakan masalah administrasi (saya menyebutnya bisnis) karena ini adalah masalah konfigurasi. <br><br><h3>  Cluster: membagi otak dan sudut pandang </h3><br>       ,    : <br><br><ul><li> split brain; </li><li> point of view ( ). </li></ul><br>   . Split brain –       ,         .     ,  -  –     ? ,      ,            ( ). <br><br>  point of view  : ,      ,          ,    .     . ,   RTT ,    . <br>       : <br><br><img src="https://habrastorage.org/webt/ab/qa/gc/abqagc2w3ky3kufvm6omjfsjheo.jpeg"><br><br>      ,   .   ,   ,       .    ,   –   .    ,       ,  ,  . <br><br><h3>  SQL- </h3><br> ,     ,    ,       .       .  , ,       …      .    . <br><br> -,  , ,  -      –    . , Galera  MySQL. <br><br>        PostgreSQL.    «»   :    ,   ,           –   . «», ,    . <br><br><h3>    ? </h3><br>        ,    : <br><br><img src="https://habrastorage.org/webt/q4/u1/4s/q4u14s11rryz7afktbdfkn0s9sy.jpeg"><br><br><img src="https://habrastorage.org/webt/pj/0p/uq/pj0puqdn9zkhsm2uwolcaw40a1w.jpeg"><br><br>           –  .      : <br><br><ul><li>  -  (Logs),    .   problems, events  events recovery.  ,   –   ,   . </li><li>  15   (State).  –     (    –   – «»   ).        .   ,  ;   –        … </li><li>  -       (Configuration update). </li></ul><br>   «.    «»,    SQL-: <br><br><img src="https://habrastorage.org/webt/ne/if/cc/neifccswcjikrxfetvhuip-agva.jpeg"><br><br> -,       : <br><br><img src="https://habrastorage.org/webt/4f/o4/ti/4fo4ticljuykrkh6zjmop4qc5sg.jpeg"><br><br>  .          -,   ,   …    –   ,   2  !    : «  ,    ».  -  ,       ,    . <br><br> .         ,           : <br><br><img src="https://habrastorage.org/webt/lj/wh/ug/ljwhugg1tdfbehcxjcrloiolwks.jpeg"><br><br>   ,  .  SQL-     .     ,   SQL-.       (   -  ),        «» (        ). … <br><br><h3> .  </h3><br> , , «»  . .       ,   ? <br><br><img src="https://habrastorage.org/webt/ci/yo/ne/ciyonezibe3bfla-7xodyyx_zwg.jpeg"><br><br>    «»- (. .  «» daemon).       (   ):   (  1  63,      «»)    (   ,       ). <br><br>      ServerIP  IP-.    ,      ,     IP-   . -    ,         proxy poller,    trapper  hello-,  proxy poller  . <br><br>   . ,       ,    « »: <br><br><img src="https://habrastorage.org/webt/dr/1z/pi/dr1zpi2hbe9penikzazqcycw-oa.jpeg"><br><br>      : <br><br><img src="https://habrastorage.org/webt/kf/pg/jb/kfpgjbghzsonjmsizlbcatyo3ja.jpeg"><br><br>  ,       default.       .  –   ,     IP-,    ,      (  ).      «» –   default. <br><br>  -, . <br><br><ul><li>   . </li><li> ,      : «    ,    ».   . </li><li>  - ,   . </li><li>     ,    hello-time,  : «   »;    . </li><li>  . </li></ul><br> ,   , ,   .               30-40 .   ,      ,    ,   . <br><br><h3>    </h3><br>            ,      .      -   : «    ,  !»  -! <br><br><img src="https://habrastorage.org/webt/nq/ta/ah/nqtaahv3gipyew36m8nhrmwukew.jpeg"><br><br>   –  :  -  ,  - , ,  GitLab, CI/CD,   . , ,  –    . <br><br>  ,      ,       –  4.0.9  (4.2   ).   Roadmap –        -. -,    «»;   ,   RPM'. <br><br><img src="https://habrastorage.org/webt/er/kn/2r/erkn2r6h9law8l3xynd6i8_jwne.jpeg"><br><br>      (   )  «»       «»-.   .         ,    .   –   :   , -   …  ? !..   «»,  . <br><br>        SQL-   ,    ,   . History Storage. <br><br><h3>  Referensi </h3><br>     5 .      . <br> -, ,   ,      , . .      -. <br><br><img src="https://habrastorage.org/webt/zp/3a/gr/zp3agr9epncvnjndf-hgpwt_zke.jpeg"><br><br><h3>     </h3><br> ,     ?     !  , , ,  .   -   - ,         . ,         ,        .       ,    : <br><br><img src="https://habrastorage.org/webt/ac/fo/pf/acfopfqzuwvtfvtni2n9rjx-ywy.jpeg"><br><br>  . «»- ,      . <br><br><ul><li>    ,         ,     , . </li><li>   «»  :  ,       Configuration Cache,   . </li><li>   ,           ,     .        ,    ,   . </li><li>   -    ,   .      ,    ,    .    200     ,     –   . </li></ul><br><h3>   </h3><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Catatan: proksi pasif belum didukung! </font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Saya menghapus kode. </font><font style="vertical-align: inherit;">Ini disebabkan oleh kenyataan bahwa sulit bagi orang untuk membuat mekanisme lain, server mana yang masih akan bertanggung jawab untuk proxy ini. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Proxy aktif sendiri masuk ke server. </font><font style="vertical-align: inherit;">Ada opsi Server untuk ini (proxy standar). </font><font style="vertical-align: inherit;">Proxy yang dimodifikasi memiliki opsi Server: </font></font><br><br><img src="https://habrastorage.org/webt/jm/ym/6t/jmym6tfhwlg_ix6o5xnnkbwphri.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dan apa yang dilakukan oleh server yang dimodifikasi tersebut? </font><font style="vertical-align: inherit;">Itu membuat koneksi KPI dengan semua server yang ditentukan untuk itu; </font><font style="vertical-align: inherit;">meminta konfigurasi, mengirim data ke server pertama yang tersedia dari daftar. </font><font style="vertical-align: inherit;">Ini menyelesaikan masalah. </font><font style="vertical-align: inherit;">Misalkan jika Anda memiliki proxy yang dikonfigurasi pada server Zabbix dan server Zabbix telah jatuh, ada satu lagi di cluster sehingga tidak dibiarkan tanpa proxy; </font><font style="vertical-align: inherit;">maka proxy hanya kait ke yang lain.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pertanyaan </font></font></h3><br>  Pertanyaan dari audiens (selanjutnya - A): - Saya ingin mengklarifikasi keadaan di antara server?  Protokol apa yang mereka gunakan untuk berkomunikasi?  Apakah ada keamanan?  Karena tidak terlalu "aman" untuk membawa komunikasi antar server ke Internet ... Bagaimana kabarnya? <br><br>  <b>MM:</b> - Saya pikir ini adalah penantang untuk pertanyaan terbaik - to the point!  Bahkan, ketika kami beralih ke komunikasi standar, server untuk komunikasi interserver mereka mewarisi semua token protokol komunikasi yang ada antara server dan proksi.  Saya akan mengklarifikasi: ada enkripsi, kompresi data.  Tolong - dengan cara yang sama semuanya dikonfigurasikan melalui web, karena dikonfigurasi secara standar untuk server dan proksi;  semuanya akan bekerja. <br><br>  <b>A:</b> - Bagaimana cara kerja Hauskiper untuk Anda dalam kasus Clickhouse? <br><br>  <b>MM:</b> - Dalam standar "Zabbix" tidak ada antarmuka dari "Housekeeper" ke History Interface, yaitu, History Interface tidak mendukung rotasi data (ElasticSearch, misalnya, tidak mendukung).  Mungkin di 4.2 itu (saya tidak melihat), tetapi sejauh ini pada 4.0.9. <br><br>  Buat itu mudah!  "Clickhouse" baru memiliki partisi.  Saya ingin melakukannya dengan melepaskan partisi yang sudah usang.  Jelas bahwa tidak akan ada rotasi pada level masing-masing item, tetapi ada trik di Zabbix: Anda dapat menentukan nilai global (misalnya, menyimpan seluruh riwayat selama tidak lebih dari 90 hari) - Anda dapat menghapus semua item, seluruh riwayat dari nilai global ini .  Dan itu akan dilakukan!  Ada lebih banyak tentang topik ini di Gitlab. <br><br>  Kami ingin melakukan hak arsitektural: apakah akan memperluas History Interface, sehingga pada dasarnya akan menjadi ... Secara umum, saya tidak ingin meninggalkan hutang teknis, tetapi itu akan dilakukan.  Karena itu perlu, semakin banyak "Clickhouse" mulai mendukung. <br><br>  <b>A:</b> - Bagaimana perasaan Anda tentang ini?  Anda, ternyata, melakukan cukup banyak pekerjaan non-penyedia. <br><br>  <b>MM:</b> - Saya mungkin tidak mengatakannya dengan benar.  Ini hobi saya!  Saya bukan spesialis teknis - saya seorang manajer.  Di waktu senggang saya berlatih. <br><br>  <b>A:</b> - Saya pikir Anda melakukan ini sebagai bagian dari bisnis inti Anda ... <br><br>  <b>MM:</b> - Bisnis memberi saya tempat yang keren untuk menguji.  Bahkan, saya sangat merekomendasikan - ini menenangkan otak.  Di suatu tempat di "hal" manajerial saya akan mengatakan ini - ketika Anda dapat beralih dari masalah manusia ke ini.  Mereka sangat keren dipecahkan!  Ini adalah masalah teknis.  Anda memprogram, dan itu bekerja seperti yang Anda programkan!  Sayang sekali orang tidak seharusnya melakukan itu. <br><br>  <b>A:</b> - Apakah Anda menulis ke "Clickhouse" melalui beberapa proksi atau langsung? <br><br>  <b>MM:</b> - Langsung.  Bahkan, Antarmuka Sejarah yang diubah, yang digunakan untuk "Elastix", juga diwariskan.  Url digunakan, yaitu, melalui antarmuka http "Zabbiks" mengirimkan "Clickhouse".  Yang keren, Zabbix berkumpul ketika ada aliran besar sejarah, ribuan metrik dalam satu paket, dan ini dengan sangat keren jatuh di Clickhouse. <br><br>  <b>A:</b> - Sebenarnya, dia menulis bachi untuknya? <br><br>  <b>MM:</b> - Ya.  Satu permintaan SQL yang dijalankan oleh url biasanya berisi seribu metrik.  Admin "Clickhouse" hanya senang. <br><br>  Presenter: - Ini adalah akhir dari program di ruangan ini.  Ada program malam yang diselenggarakan, dan ada sesuatu yang hanya dapat Anda lakukan.  Dan saya sarankan, sementara Anda akan berkomunikasi satu sama lain, untuk memikirkan hal-hal menarik apa yang dapat Anda lakukan ... Ketika Anda saling memberi tahu tentang kasus Anda, ini kemungkinan besar adalah hal yang dapat Anda laporkan.  Dengan berdiskusi satu sama lain, Anda dapat menemukan hanya untuk menemukan garis besar - komite program akan menerima aplikasi Anda, mempertimbangkan dan membantu membuat cerita yang baik dan dikemas dari itu.  Mungkin Anda punya semacam cerita tentang bekerja dengan komite program? <br><br>  <b>MM:</b> - Sebenarnya, banyak umpan balik yang diberikan.  Saya sangat beruntung: seseorang dari komite program tinggal di Chelyabinsk saya, dan Highload adalah satu-satunya konferensi yang bekerja sangat dekat dengan para pembicara.  Saya belum pernah melihat yang seperti ini di tempat lain.  Ini sangat bermanfaat!  Tahap berbeda: orang-orang menonton video, memberikan komentar pada slide - itu benar-benar terjadi pada subjek (ejaan, kesalahan ketik).  Sangat keren!  Saya merekomendasikannya!  Coba sendiri! <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wbIpn44Z2_8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Sedikit iklan :) </h3><br>  Terima kasih telah tinggal bersama kami.  Apakah Anda suka artikel kami?  Ingin melihat materi yang lebih menarik?  Dukung kami dengan melakukan pemesanan atau merekomendasikan kepada teman Anda, <a href="https://ua-hosting.company/cloudvps/nl">VPS berbasis cloud untuk pengembang mulai $ 4,99</a> , <b>analog unik dari server entry-level yang diciptakan oleh kami untuk Anda:</b> <a href="https://habr.com/company/ua-hosting/blog/347386/">Seluruh kebenaran tentang VPS (KVM) E5-2697 v3 (6 Cores) 10GB DDR4 480GB SSD 1Gbps mulai dari $ 19 atau cara membagi server?</a>  (opsi tersedia dengan RAID1 dan RAID10, hingga 24 core dan hingga 40GB DDR4). <br><br>  <b>Dell R730xd 2 kali lebih murah di pusat data Equinix Tier IV di Amsterdam?</b>  Hanya kami yang memiliki <b><a href="https://ua-hosting.company/serversnl">2 x Intel TetraDeca-Core Xeon 2x E5-2697v3 2.6GHz 14C 64GB DDR4 4x960GB SSD 1Gbps 100 TV dari $ 199</a> di Belanda!</b>  <b><b>Dell R420 - 2x E5-2430 2.2Ghz 6C 128GB DDR3 2x960GB SSD 1Gbps 100TB - mulai dari $ 99!</b></b>  Baca tentang <a href="https://habr.com/company/ua-hosting/blog/329618/">Cara Membangun Infrastruktur Bldg.</a>  <a href="https://habr.com/company/ua-hosting/blog/329618/">kelas menggunakan server Dell R730xd E5-2650 v4 seharga 9.000 euro untuk satu sen?</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id485534/">https://habr.com/ru/post/id485534/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id485524/index.html">Intisari materi menarik untuk pengembang seluler # 330 (pada 20 - 26 Januari)</a></li>
<li><a href="../id485526/index.html">Siapa dan mengapa ingin membuat Internet “dibagikan”</a></li>
<li><a href="../id485528/index.html">Cara Menyelesaikan Proyek Pengembangan Perangkat Lunak dengan Cara yang Benar</a></li>
<li><a href="../id485530/index.html">Pembelajaran Terpandu</a></li>
<li><a href="../id485532/index.html">Panduan Wawancara untuk para programmer yang tidak memahaminya</a></li>
<li><a href="../id485536/index.html">Apakah mungkin untuk meretas pesawat - 2</a></li>
<li><a href="../id485538/index.html">Zabbix: memonitor semuanya secara berurutan (pada contoh Redis)</a></li>
<li><a href="../id485542/index.html">Tambahkan grafik ke Notion</a></li>
<li><a href="../id485544/index.html">Catur sebagai sistem yang dinamis</a></li>
<li><a href="../id485546/index.html">Kiamat akan datang</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>