<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§Ωüèæ üíï üêπ L'IA essayant d'√©viter les probl√®mes a appris un comportement complexe ü§º ü§§ üåΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'apprentissage par renforcement utilise souvent la curiosit√© comme motivation pour l'IA. Le for√ßant √† rechercher de nouvelles sensations et √† explore...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>L'IA essayant d'√©viter les probl√®mes a appris un comportement complexe</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481484/"><p><img src="https://habrastorage.org/webt/id/qs/te/idqste6ktiyfnkpzihpvgcnfhmq.png"></p><br><p>  L'apprentissage par renforcement utilise souvent la curiosit√© comme motivation pour l'IA.  Le for√ßant √† rechercher de nouvelles sensations et √† explorer le monde.  Mais la vie est pleine de mauvaises surprises.  Vous pouvez tomber d'une falaise et du point de vue de la curiosit√© ce seront toujours des sensations tr√®s nouvelles et int√©ressantes.  Mais √©videmment pas pour quoi lutter. </p><br><p>  Les d√©veloppeurs de Berkeley ont boulevers√© la t√¢che de l'agent virtuel: ce n'est pas la curiosit√© qui a fait la principale force de motivation, mais plut√¥t le d√©sir d'√©viter toute nouveaut√© par tous les moyens.  Mais "ne rien faire" √©tait plus difficile qu'il n'y para√Æt.  √âtant plac√©e dans un environnement en constante √©volution, l'IA devait apprendre des comportements complexes afin d'√©viter de nouvelles sensations. </p><a name="habracut"></a><br><p>  L'apprentissage par renforcement prend des mesures timides pour construire une IA solide.  Et tandis que tout est limit√© √† de tr√®s faibles dimensions, litt√©ralement les unit√©s dans lesquelles l'agent virtuel doit agir (de pr√©f√©rence raisonnablement), de nouvelles id√©es apparaissent de temps en temps pour am√©liorer la formation de l'intelligence artificielle. </p><br><p>  Mais non seulement les algorithmes d'apprentissage sont compliqu√©s.  L'environnement devient √©galement plus difficile.  La plupart des environnements d'apprentissage par renforcement sont tr√®s simples et motivent l'agent √† explorer le monde.  Il peut s'agir d'un labyrinthe qui doit √™tre compl√®tement contourn√© pour trouver une issue, ou d'un jeu informatique qui doit √™tre compl√©t√© jusqu'au bout. </p><br><p>  Mais √† long terme, les √™tres vivants (raisonnables et pas si) s'efforcent non seulement d'explorer le monde qui les entoure.  Mais aussi pour garder tout le bien qui est dans leur courte (ou pas) vie. </p><br><p>  C'est ce qu'on appelle l'hom√©ostasie - le d√©sir du corps de maintenir un √©tat constant.  Sous une forme ou une autre, cela est commun √† tous les √™tres vivants.  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Les d√©veloppeurs de Berkeley</a> donnent un exemple si √©trange: toutes les r√©alisations de l'humanit√©, dans l'ensemble, sont con√ßues pour se prot√©ger contre les mauvaises surprises.  Prot√©ger contre une entropie toujours croissante de l'environnement.  Nous construisons des maisons o√π nous maintenons une temp√©rature constante, √† l'abri des changements climatiques.  Nous utilisons des m√©dicaments pour √™tre constamment en bonne sant√© et ainsi de suite. </p><br><p>  On peut contester cela, mais il y a vraiment quelque chose dans cette analogie. </p><br><p>  Les gars ont pos√© la question - que se passera-t-il si la principale motivation de l'IA est d'essayer d'√©viter toute nouveaut√©?  En d'autres termes, minimisez le chaos en tant que fonction d'apprentissage objectif. </p><br><p>  Et ils ont plac√© l'agent dans un monde dangereux en constante √©volution. </p><br><p><img src="https://habrastorage.org/webt/ya/iy/d3/yaiyd3jxw0gnbaywrqyh4deyi8u.gif"></p><br><p><img src="https://habrastorage.org/webt/qc/ss/am/qcssam7pbjxhrwx4uhpzlqm6xq4.gif"></p><br><p>  Les r√©sultats √©taient int√©ressants.  Dans de nombreux cas, cet apprentissage a d√©pass√© l'apprentissage bas√© sur le curriculum et, le plus souvent, en termes de qualit√©, se rapproche de l'apprentissage avec un enseignant.  Autrement dit, √† une formation sp√©cialis√©e pour atteindre un objectif sp√©cifique - pour gagner le match, passez par le labyrinthe. </p><br><p>  C'est bien s√ªr logique, car si vous vous tenez sur un pont qui s'effondre, alors pour continuer √† y √™tre (pour maintenir la constance et √©viter que de nouvelles sensations ne tombent), vous devez constamment vous √©loigner du bord.  Fuyez de toutes ses forces pour rester immobile, comme l'a dit Alice. </p><br><p><img src="https://habrastorage.org/webt/vl/p2/h1/vlp2h1png42kowbspxvoxald2gc.gif"></p><br><p>  Et en fait, dans tout algorithme d'apprentissage par renforcement, il y a un tel moment.  Parce que les morts dans le jeu et la fin rapide de l'√©pisode sont p√©nalis√©es par une r√©compense n√©gative.  Ou, selon l'algorithme, en r√©duisant la r√©compense maximale qu'un agent pourrait recevoir s'il ne tombait pas continuellement de la falaise. </p><br><p><img src="https://habrastorage.org/webt/7h/le/ni/7hleniwgnqd_wu6deeihk078ptk.gif"></p><br><p>  Mais c'est dans une telle formulation, quand l'IA n'a d'autre objectif que le d√©sir d'√©viter la nouveaut√©, qu'il semble qu'elle ait √©t√© utilis√©e pour la premi√®re fois dans l'apprentissage renforc√©. </p><br><p>  Fait int√©ressant, avec une telle motivation, l'agent virtuel a appris √† jouer √† de nombreux jeux qui ont pour objectif de gagner.  Par exemple, tetris. </p><br><p><img src="https://habrastorage.org/webt/a0/7w/at/a07watp9unf5zcgbcojewuv_7wy.gif"></p><br><p>  Ou l'environnement de Doom, o√π vous devez esquiver des boules de feu volantes et tirer sur des adversaires qui approchent.  Parce que de nombreuses t√¢ches peuvent √™tre formul√©es comme des t√¢ches de maintien de la constance.  Pour Tetris, c'est le d√©sir de laisser le champ vide.  L'√©cran se remplit-il constamment?  Oh mon cher, que se passera-t-il quand il sera rempli √† la fin?  Non, non, nous n'avons pas besoin d'un tel bonheur.  Trop de choc. </p><br><p>  Du c√¥t√© technique, il est arrang√© tout simplement.  Lorsqu'un agent re√ßoit un nouvel √©tat, il √©value √† quel point cet √©tat est familier.  Autrement dit, combien le nouvel √âtat est inclus dans la r√©partition de l'√âtat qu'il a visit√© plus t√¥t.  L'agent entre dans un √©tat plus familier, plus la r√©compense est grande.  Et la t√¢che de la politique d'apprentissage (ce sont tous les termes de l'apprentissage par renforcement, si quelqu'un ne le sait pas) est de choisir des actions qui m√®neraient √† la transition vers l'√©tat le plus familier.  De plus, chaque nouvel √©tat obtenu est utilis√© pour mettre √† jour les statistiques des √©tats familiers avec lesquels de nouveaux √©tats sont compar√©s. </p><br><p>  Fait int√©ressant, dans le processus de l'IA, j'ai spontan√©ment appris √† comprendre que de nouveaux √âtats influencent ce qui est consid√©r√© comme une nouveaut√©.  Et que vous pouvez atteindre des √©tats familiers de deux mani√®res: soit passez √† un √©tat d√©j√† connu.  Ou entrer dans un √©tat qui mettra √† <em>jour le</em> concept m√™me de persistance / familiarit√© de l'environnement, et l'agent sera dans un nouvel √©tat, form√© par ses actions, familier. </p><br><p>  Cela oblige l'agent √† entreprendre des actions coordonn√©es complexes, ne serait-ce que pour ne rien faire dans la vie. </p><br><p>  Paradoxalement, cela conduit √† un analogue de la curiosit√© de l'apprentissage ordinaire et force l'agent √† explorer le monde qui l'entoure.  Soudain, quelque part, il y a un endroit encore plus s√ªr qu'ici et maintenant?  L√†, vous pouvez vous adonner compl√®tement √† la paresse et ne rien faire, √©vitant ainsi tout probl√®me et toute nouvelle sensation.  Il ne serait pas exag√©r√© de dire que de telles pens√©es sont probablement venues √† l'un d'entre nous.  Et pour beaucoup, c'est une v√©ritable force motrice dans la vie.  Bien que dans la vraie vie, aucun de nous n'ait d√ª faire face √† un remplissage de tetris jusqu'au sommet, bien s√ªr. </p><br><p>  Pour √™tre honn√™te, c'est une histoire compliqu√©e.  Mais la pratique montre que cela fonctionne.  Les chercheurs ont compar√© cet algorithme aux meilleurs repr√©sentants bas√©s sur la curiosit√©: <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">ICM</a> et <a href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/" rel="nofollow">RND</a> .  Le premier est un m√©canisme efficace de curiosit√© qui est d√©j√† devenu classique dans l'apprentissage par renforcement.  L'agent ne cherche pas simplement de nouveaux √©tats inconnus et donc int√©ressants.  La m√©connaissance de la situation dans de tels algorithmes est estim√©e par la capacit√© de l'agent √† la pr√©dire (au d√©but, il y avait litt√©ralement des compteurs d'√©tats visit√©s, mais maintenant tout se r√©sume √† l'estimation int√©grale fournie par le r√©seau neuronal).  Mais dans ce cas, les feuilles en mouvement sur les arbres ou le bruit blanc √† la t√©l√©vision auraient une nouveaut√© sans fin pour un tel agent, et auraient provoqu√© une curiosit√© sans fin.  Parce qu'il ne peut jamais pr√©dire tous les nouveaux √©tats possibles dans un environnement compl√®tement al√©atoire. </p><br><p>  Par cons√©quent, dans ICM, un agent recherche uniquement les nouveaux √©tats qu'il peut influencer par ses actions.  L'IA peut-elle affecter le bruit blanc √† la t√©l√©vision?  Non.  Si inint√©ressant.  Et cela peut-il affecter le ballon si vous le d√©placez?  Oui  Jouer avec le ballon est donc int√©ressant.  Pour ce faire, ICM utilise une id√©e tr√®s cool avec le mod√®le inverse, avec lequel le mod√®le avanc√© est compar√©.  Plus de d√©tails dans l' <a href="https://pathak22.github.io/noreward-rl/" rel="nofollow">oeuvre originale</a> . </p><br><p>  RND est un d√©veloppement plus r√©cent du m√©canisme de curiosit√©.  Ce qui dans la pratique a d√©pass√© l'ICM.  En bref, le r√©seau neuronal essaie de pr√©dire les sorties d'un autre r√©seau neuronal, qui est initi√© par des poids al√©atoires et ne change jamais.  Il est suppos√© que plus la situation est famili√®re (aliment√©e √† l'entr√©e des deux r√©seaux de neurones, actuels et initi√©s de mani√®re al√©atoire), plus souvent le r√©seau de neurones actuel sera en mesure de pr√©dire les sorties initi√©es de mani√®re al√©atoire.  Je ne sais pas qui invente tout cela.  D'une part, je veux serrer la main d'une telle personne et, d'autre part, donner un coup de pied √† de telles distorsions. </p><br><p>  Mais d'une mani√®re ou d'une autre, et la formation sur l'id√©e de maintenir l'hom√©ostasie et d'essayer d'√©viter toute nouveaut√©, dans de nombreux cas, dans la pratique, on a obtenu un meilleur r√©sultat final qu'un programme bas√© sur le CII ou le RND.  Ce qui se refl√®te dans les graphiques. </p><br><p><img src="https://habrastorage.org/webt/p0/uw/uc/p0uwucutpngtfjp5zunetzqbf0k.png"></p><br><p>  Mais ici, il est n√©cessaire de pr√©ciser que ce n'est que pour les environnements que les chercheurs ont utilis√©s dans leur travail.  Ils sont dangereux, al√©atoires, bruyants et avec une entropie croissante.  Il peut vraiment √™tre plus rentable de ne rien y faire.  Et ce n'est qu'occasionnellement qu'il se d√©place activement lorsqu'une boule de feu vole en vous ou que le pont derri√®re vous commence √† s'effondrer.  Cependant, les chercheurs de Berkeley insistent, apparemment √† cause de leur exp√©rience de vie difficile, que de tels environnements sont beaucoup plus proches de la vie r√©elle complexe que ceux utilis√©s auparavant dans la formation de renforcement.  Eh bien, je ne sais pas, je ne sais pas.  Dans ma vie, des boules de feu de monstres volant en moi et des labyrinthes inhabit√©s avec une seule sortie se trouvent √† peu pr√®s √† la m√™me fr√©quence.  Mais on ne peut nier que l'approche propos√©e, avec toute sa simplicit√©, a donn√© des r√©sultats √©tonnants.  Peut-√™tre qu'√† l'avenir, les deux approches devraient √™tre raisonnablement combin√©es - l'hom√©ostasie avec la pr√©servation d'une constance positive √† long terme et la curiosit√© pour les √©tudes environnementales actuelles. </p><br><p>  <a href="https://bair.berkeley.edu/blog/2019/12/18/smirl/" rel="nofollow">Lien vers l'≈ìuvre originale</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr481484/">https://habr.com/ru/post/fr481484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr481474/index.html">Nous venons d'un autre test - nous testons la base de donn√©es sur MSTest</a></li>
<li><a href="../fr481476/index.html">Comment j'ai commenc√© √† parler lors de conf√©rences et je ne peux pas m'arr√™ter</a></li>
<li><a href="../fr481478/index.html">STM32 + CMSIS + STM32CubeIDE</a></li>
<li><a href="../fr481480/index.html">C'est la norme: quelles sont les cartes normales et comment fonctionnent-elles</a></li>
<li><a href="../fr481482/index.html">Publication crois√©e sur une page Facebook √† l'aide du SDK PHP</a></li>
<li><a href="../fr481486/index.html">¬´Rester en vie, rester en vie¬ª: le nouveau protocole augmentera de 60 m√®tres le rayon de l'utilisation possible du Wi-Fi</a></li>
<li><a href="../fr481488/index.html">Comment les responsables de la r√©gion de Moscou mesurent la pollution atmosph√©rique</a></li>
<li><a href="../fr481490/index.html">Vitamine D. Une courte excursion</a></li>
<li><a href="../fr481492/index.html">Nous collectons de la musique en couleur pour le nouvel an</a></li>
<li><a href="../fr481496/index.html">Attaque DDoS via l'ing√©nierie sociale</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>