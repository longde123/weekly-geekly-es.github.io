<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∞üèΩ üíä ü§úüèΩ Th√©orie et pratique de l'utilisation de HBase ü§Ωüèº üåà ü§úüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bon apr√®s-midi Je m'appelle Danil Lipova, notre √©quipe de Sbertech a commenc√© √† utiliser HBase comme r√©f√©rentiel de donn√©es op√©rationnelles. Au cours ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Th√©orie et pratique de l'utilisation de HBase</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/420425/">  Bon apr√®s-midi  Je m'appelle Danil Lipova, notre √©quipe de Sbertech a commenc√© √† utiliser HBase comme r√©f√©rentiel de donn√©es op√©rationnelles.  Au cours de son √©tude, une exp√©rience a √©t√© acquise que je souhaitais syst√©matiser et d√©crire (nous esp√©rons qu'elle sera utile √† beaucoup).  Toutes les exp√©riences ci-dessous ont √©t√© men√©es avec des versions de HBase 1.2.0-cdh5.14.2 et 2.0.0-cdh6.0.0-beta1. <br><br><ol><li>  Architecture g√©n√©rale </li><li>  √âcriture de donn√©es dans HBASE </li><li>  Lecture des donn√©es de HBASE </li><li>  Mise en cache des donn√©es </li><li>  Traitement par lots MultiGet / MultiPut </li><li>  Strat√©gie de r√©partition des tableaux en r√©gions (d√©versement) </li><li>  Tol√©rance aux pannes, compactification et localisation des donn√©es </li><li>  Param√®tres et performances </li><li>  Test de charge </li><li>  Conclusions </li></ol><a name="habracut"></a><br><h2>  1. Architecture g√©n√©rale </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y9/wa/vl/y9wavltynzhs9r_7v5sn1d8ff68.png"></div><br>  Le ma√Ætre de secours √©coute le rythme cardiaque actif sur le n≈ìud ZooKeeper et, en cas de disparition, reprend les fonctions du ma√Ætre. <br><br><h2>  2. √âcriture de donn√©es dans HBASE </h2><br>  Tout d'abord, consid√©rons le cas le plus simple - √©crire un objet valeur-cl√© dans une certaine table en utilisant put (rowkey).  Le client doit d'abord savoir o√π se trouve le serveur de r√©gion racine (RRS) qui stocke la m√©tabase hbase :.  Il re√ßoit ces informations de ZooKeeper.  Il se tourne ensuite vers RRS et lit la table m√©ta hbase: √† partir de laquelle il r√©cup√®re les informations dont RegionServer (RS) est responsable du stockage des donn√©es pour la cl√© de ligne donn√©e dans la table qui l'int√©resse.  Pour une utilisation future, la m√©ta-table est mise en cache par le client et donc les appels suivants vont plus vite, directement vers RS. <br><br>  RS, apr√®s avoir re√ßu la demande, l'√©crit tout d'abord dans WriteAheadLog (WAL), ce qui est n√©cessaire pour la r√©cup√©ration en cas de panne.  Ensuite, il enregistre les donn√©es dans MemStore.  Il s'agit d'un tampon en m√©moire qui contient un ensemble tri√© de cl√©s pour une r√©gion donn√©e.  Le tableau peut √™tre divis√© en r√©gions (partitions), chacune contenant un jeu de cl√©s disjoint.  Cela permet de placer des r√©gions sur diff√©rents serveurs pour obtenir de meilleures performances.  Cependant, malgr√© l'√©vidence de cette d√©claration, nous verrons plus loin que cela ne fonctionne pas dans tous les cas. <br><br>  Apr√®s avoir plac√© l'enregistrement dans MemStore, le client re√ßoit une r√©ponse indiquant que l'enregistrement a √©t√© enregistr√© avec succ√®s.  En m√™me temps, il n'est r√©ellement stock√© que dans le tampon et n'atteint le disque qu'apr√®s un certain laps de temps ou lorsqu'il est rempli de nouvelles donn√©es. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/xi/p7/xtxip7moylyjdgqggsqiu8j_cm4.png"></div><br>  Lors de l'ex√©cution de l'op√©ration ¬´Supprimer¬ª, aucune suppression physique des donn√©es ne se produit.  Ils sont simplement marqu√©s comme supprim√©s et la destruction elle-m√™me se produit lorsque la fonction compacte principale est appel√©e, ce qui est d√©crit plus en d√©tail dans la section 7. <br><br>  Les fichiers au format HFile sont accumul√©s dans HDFS et de temps en temps le processus compact mineur d√©marre, qui colle simplement de petits fichiers en plus gros sans rien supprimer.  Au fil du temps, cela devient un probl√®me qui ne se manifeste que lors de la lecture des donn√©es (nous y reviendrons plus tard). <br><br>  En plus du processus de d√©marrage d√©crit ci-dessus, il existe une proc√©dure beaucoup plus efficace, qui est probablement le c√¥t√© le plus puissant de cette base de donn√©es - BulkLoad.  Il consiste dans le fait que nous cr√©ons ind√©pendamment des HFiles et les mettons sur le disque, ce qui nous permet de parfaitement √©voluer et d'atteindre des vitesses tr√®s d√©centes.  En fait, la limitation ici n'est pas HBase, mais les possibilit√©s du fer.  Vous trouverez ci-dessous les r√©sultats du chargement sur un cluster compos√© de 16 RegionServers et 16 NodeManager YARN (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 threads), version HBase 1.2.0-cdh5.14.2. <br><br><img src="https://habrastorage.org/webt/ro/bu/hf/robuhfegpwqyed6gmwg7he2bmfk.png"><br><br>  On peut voir qu'en augmentant le nombre de partitions (r√©gions) dans la table, ainsi que les ex√©cutables Spark, on obtient une augmentation de la vitesse de t√©l√©chargement.  De plus, la vitesse d√©pend de la quantit√© d'enregistrement.  Les gros blocs augmentent la mesure de Mo / sec, les petits le nombre d'enregistrements ins√©r√©s par unit√© de temps, toutes choses √©tant √©gales par ailleurs. <br><br>  Vous pouvez √©galement commencer √† charger dans deux tables en m√™me temps et obtenir un doublement de la vitesse.  On peut voir ci-dessous que des blocs de 10 Ko sont √©crits sur deux tables √† la fois avec une vitesse d'environ 600 Mb / s chacune (total 1275 Mb / s), ce qui co√Øncide avec la vitesse d'√©criture de 623 Mo / s sur une table (voir n ¬∞ 11 ci-dessus) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gr/05/vp/gr05vpmhauzclwmn310erbgj22u.png"></div><br>  Mais le deuxi√®me lancement avec des enregistrements de 50 Ko montre que la vitesse de t√©l√©chargement augmente d√©j√† l√©g√®rement, ce qui indique une approximation des valeurs limites.  Il convient de garder √† l'esprit qu'il n'y a pratiquement pas de charge sur HBASE lui-m√™me, tout ce qui est requis est de donner d'abord les donn√©es de hbase: meta, et apr√®s avoir doubl√© les HFiles, videz les donn√©es BlockCache et enregistrez le tampon MemStore sur le disque s'il ne l'est pas. vide. <br><br><h2>  3. Lecture des donn√©es de HBASE </h2><br>  Si nous supposons que toutes les informations de hbase: meta ont d√©j√† un client (voir la section 2), la demande va imm√©diatement au RS o√π la cl√© souhait√©e est stock√©e.  La recherche s'effectue d'abord dans MemCache.  Qu'il y ait ou non des donn√©es, la recherche est √©galement effectu√©e dans le tampon BlockCache et, si n√©cessaire, dans HFiles.  Si les donn√©es ont √©t√© trouv√©es dans un fichier, elles sont plac√©es dans BlockCache et seront renvoy√©es plus rapidement √† la prochaine demande.  Les recherches de fichiers H sont relativement rapides en raison de l'utilisation du filtre Bloom, c'est-√†-dire  Apr√®s avoir lu une petite quantit√© de donn√©es, il d√©termine imm√©diatement si ce fichier contient la cl√© souhait√©e et, sinon, passe √† la suivante. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8-/zz/wz/8-zzwzcoed7obxmlgzxl3ihrzzw.png"></div><br>  Ayant re√ßu des donn√©es de ces trois sources, RS forme une r√©ponse.  En particulier, il peut transf√©rer plusieurs versions de l'objet trouv√© √† la fois si le client a demand√© un contr√¥le de version. <br><br><h2>  4. Mise en cache des donn√©es </h2><br>  Les tampons MemStore et BlockCache occupent jusqu'√† 80% de la m√©moire RS allou√©e RS (le reste est r√©serv√© aux t√¢ches de service RS).  Si le mode d'utilisation typique est tel que les processus √©crivent et lisent imm√©diatement les m√™mes donn√©es, alors il est logique de r√©duire BlockCache et d'augmenter MemStore, car  lorsque l'√©criture de donn√©es dans le cache de lecture ne tombe pas, l'utilisation de BlockCache se produit moins fr√©quemment.  Le tampon BlockCache se compose de deux parties: LruBlockCache (toujours sur le tas) et BucketCache (g√©n√©ralement hors tas ou sur SSD).  BucketCache doit √™tre utilis√© lorsqu'il y a beaucoup de demandes de lecture et qu'elles ne rentrent pas dans LruBlockCache, ce qui conduit au travail actif de Garbage Collector.  Dans le m√™me temps, vous ne devez pas vous attendre √† une augmentation radicale des performances de l'utilisation du cache de lecture, mais nous y reviendrons dans la section 8 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/oe/nw/rqoenwgqtngb-a37gsof7sqbgn0.png"></div><br>  BlockCache est un pour l'ensemble du RS, et MemStore a le sien pour chaque table (un pour chaque famille de colonnes). <br><br>  Comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©crit</a> en th√©orie, lorsque l'√©criture de donn√©es ne tombe pas dans le cache, et en effet, ces param√®tres CACHE_DATA_ON_WRITE pour la table et "Cache DATA on Write" pour RS sont d√©finis sur false.  Cependant, dans la pratique, si vous √©crivez des donn√©es dans MemStore, puis les videz sur le disque (en les nettoyant de cette fa√ßon), puis supprimez le fichier r√©sultant, puis en ex√©cutant une requ√™te get, nous recevrons avec succ√®s les donn√©es.  Et m√™me si vous d√©sactivez compl√®tement BlockCache et remplissez le tableau avec de nouvelles donn√©es, puis r√©cup√©rez le MemStore sur le disque, supprimez-les et demandez √† une autre session, ils seront toujours r√©cup√©r√©s quelque part.  HBase stocke donc non seulement des donn√©es, mais aussi des √©nigmes myst√©rieuses. <br><br><pre><code class="bash hljs">hbase(main):001:0&gt; create <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf'</span></span> Created table ns:magic Took 1.1533 seconds hbase(main):002:0&gt; put <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf:c'</span></span>, <span class="hljs-string"><span class="hljs-string">'try_to_delete_me'</span></span> Took 0.2610 seconds hbase(main):003:0&gt; flush <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span> Took 0.6161 seconds hdfs dfs -mv /data/hbase/data/ns/magic/* /tmp/trash hbase(main):002:0&gt; get <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span> cf:c timestamp=1534440690218, value=try_to_delete_me</code> </pre> <br>  Cache DATA on Read est d√©fini sur false.  Si vous avez des id√©es, n'h√©sitez pas √† en discuter dans les commentaires. <br><br><h2>  5. Traitement par lots des donn√©es MultiGet / MultiPut </h2><br>  Le traitement de requ√™tes uniques (Get / Put / Delete) est une op√©ration assez co√ªteuse, vous devez donc les combiner autant que possible dans une liste ou une liste, ce qui vous permet d'obtenir une am√©lioration significative des performances.  Cela est particuli√®rement vrai de l'op√©ration d'√©criture, mais lors de la lecture, il y a le pi√®ge suivant.  Le graphique ci-dessous montre le temps de lecture de 50 000 enregistrements de MemStore.  La lecture a √©t√© effectu√©e en un seul flux et l'axe horizontal indique le nombre de cl√©s dans la demande.  On peut voir que lorsque vous augmentez √† un millier de cl√©s en une seule demande, le temps d'ex√©cution diminue, c.-√†-d.  la vitesse augmente.  Toutefois, lorsque le mode MSLAB est activ√© par d√©faut, apr√®s ce seuil, une baisse spectaculaire des performances commence et plus la quantit√© de donn√©es dans l'enregistrement est grande, plus la dur√©e est longue. <br><br><img src="https://habrastorage.org/webt/1k/ic/hj/1kichjm1xdpxskbx7avzppmlqty.png"><br><br>  Les tests ont √©t√© effectu√©s sur une machine virtuelle, 8 c≈ìurs, HBase version 2.0.0-cdh6.0.0-beta1. <br><br>  Le mode MSLAB est con√ßu pour r√©duire la fragmentation du segment de m√©moire, qui se produit en raison du m√©lange des donn√©es de nouvelle et ancienne g√©n√©ration.  Pour r√©soudre le probl√®me lorsque MSLAB est activ√©, les donn√©es sont plac√©es dans des cellules relativement petites (blocs) et trait√©es par lots.  Par cons√©quent, lorsque le volume dans le paquet de donn√©es demand√© d√©passe la taille allou√©e, les performances diminuent fortement.  En revanche, la d√©sactivation de ce mode est √©galement d√©conseill√©e, car elle entra√Ænera des arr√™ts dus au GC pendant les moments de travail intensif avec les donn√©es.  Une bonne solution consiste √† augmenter le volume de la cellule, dans le cas d'une √©criture active via put simultan√©ment avec la lecture.  Il convient de noter que le probl√®me ne se produit pas si, apr√®s l'enregistrement, ex√©cutez la commande flush qui vide MemStore sur le disque ou si le chargement est effectu√© √† l'aide de BulkLoad.  Le tableau ci-dessous montre que les requ√™tes √† partir de donn√©es MemStore d'un volume plus important (et de la m√™me quantit√©) entra√Ænent un ralentissement.  Cependant, l'augmentation de la taille des blocs ram√®ne le temps de traitement √† la normale. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zb/jq/s3/zbjqs3tou2ywnnzc16p93fttbva.png"></div><br>  En plus d'augmenter la taille des blocs, la fragmentation des donn√©es par r√©gion aide, c'est-√†-dire  fractionnement de table.  Cela conduit au fait que moins de demandes arrivent dans chaque r√©gion et si elles sont plac√©es dans une cellule, la r√©ponse reste bonne. <br><br><h2>  6. La strat√©gie de division des tableaux en r√©gions (d√©coupage) </h2><br>  √âtant donn√© que HBase est un stockage de valeurs-cl√©s et que le partitionnement est effectu√© par cl√©, il est extr√™mement important de partager les donn√©es de mani√®re uniforme dans toutes les r√©gions.  Par exemple, le partitionnement d'une telle table en trois parties entra√Ænera la division des donn√©es en trois r√©gions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/4c/9d/rg4c9dm-gbtodx0dqr3quc9h6we.png"></div><br>  Il arrive que cela entra√Æne un net ralentissement si les donn√©es charg√©es √† l'avenir ressembleront, par exemple, √† des valeurs longues, dont la plupart commencent par le m√™me chiffre, par exemple: <br><br>  1000001 <br>  1000002 <br>  ... <br>  1100003 <br><br>  √âtant donn√© que les cl√©s sont stock√©es sous la forme d'un tableau d'octets, elles d√©marreront toutes de la m√™me mani√®re et appartiendront √† la m√™me r√©gion # 1 qui stocke cette plage de cl√©s.  Il existe plusieurs strat√©gies divis√©es: <br><br>  HexStringSplit - Transforme la cl√© en une cha√Æne avec un codage hexad√©cimal dans la plage "00000000" =&gt; "FFFFFFFF" et en la remplissant de z√©ros √† gauche. <br><br>  UniformSplit - Transforme une cl√© en un tableau d'octets avec un codage hexad√©cimal dans la plage "00" =&gt; "FF" et en la remplissant de z√©ros √† droite. <br><br>  De plus, vous pouvez sp√©cifier n'importe quelle plage ou ensemble de cl√©s √† diviser et configurer la division automatique.  Cependant, l'une des approches les plus simples et les plus efficaces est UniformSplit et l'utilisation de la concat√©nation de hachage, par exemple, une paire √©lev√©e d'octets provenant de l'ex√©cution d'une cl√© via la fonction CRC32 (rowkey) et rowkey elle-m√™me: <br><br>  hash + rowkey <br><br>  Ensuite, toutes les donn√©es seront r√©parties uniform√©ment entre les r√©gions.  Lors de la lecture, les deux premiers octets sont simplement supprim√©s et la cl√© d'origine reste.  RS contr√¥le √©galement la quantit√© de donn√©es et de cl√©s dans la r√©gion et lorsque les limites sont d√©pass√©es, il les d√©compose automatiquement en morceaux. <br><br><h2>  7. Tol√©rance aux pannes et localisation des donn√©es </h2><br>  √âtant donn√© qu'une seule r√©gion est responsable de chaque jeu de cl√©s, la solution aux probl√®mes associ√©s aux plantages RS ou au d√©classement consiste √† stocker toutes les donn√©es n√©cessaires dans HDFS.  Lorsque RS se bloque, le ma√Ætre le d√©tecte par l'absence de pulsation sur le n≈ìud ZooKeeper.  Ensuite, il attribue la r√©gion desservie √† une autre RS et puisque les HFiles sont stock√©s dans un syst√®me de fichiers distribu√©, le nouvel h√¥te les lit et continue de servir les donn√©es.  Cependant, comme certaines des donn√©es peuvent se trouver dans MemStore et n'ont pas eu le temps d'entrer dans les HFiles, les WAL, qui sont √©galement stock√©s dans HDFS, sont utilis√©s pour restaurer l'historique des op√©rations.  Apr√®s le transfert des modifications, RS est en mesure de r√©pondre aux demandes, cependant, le d√©placement conduit au fait qu'une partie des donn√©es et leurs processus se trouvent sur diff√©rents n≈ìuds, c'est-√†-dire  localit√© r√©duite. <br><br>  La solution au probl√®me est un compactage majeur - cette proc√©dure d√©place les fichiers vers les n≈ìuds qui en sont responsables (o√π se trouvent leurs r√©gions), √† la suite de quoi la charge sur le r√©seau et les disques augmente fortement pendant cette proc√©dure.  Cependant, √† l'avenir, l'acc√®s aux donn√©es est sensiblement acc√©l√©r√©.  De plus, major_compaction combine tous les HFiles en un seul fichier dans la r√©gion et nettoie √©galement les donn√©es en fonction des param√®tres de la table.  Par exemple, vous pouvez sp√©cifier le nombre de versions d'un objet que vous souhaitez enregistrer ou sa dur√©e de vie, apr√®s quoi l'objet est physiquement supprim√©. <br><br>  Cette proc√©dure peut avoir un effet tr√®s positif sur HBase.  L'image ci-dessous montre comment les performances se sont d√©grad√©es suite √† l'enregistrement actif des donn√©es.  Ici, vous pouvez voir comment 40 flux ont √©t√© √©crits dans une table et 40 flux lire des donn√©es en m√™me temps.  L'√©criture de flux forme de plus en plus de HFiles, qui sont lus par d'autres flux.  En cons√©quence, de plus en plus de donn√©es doivent √™tre supprim√©es de la m√©moire et √† la fin, le GC commence √† fonctionner, ce qui paralyse pratiquement tout le travail.  Le lancement d'un compactage majeur a conduit au nettoyage des blocages qui en r√©sultent et √† la restauration des performances. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/ha/ga/x2haga1cohdfilxz5ffu_vatfzy.png"></div><br>  Le test a √©t√© effectu√© sur 3 DataNode et 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 threads).  HBase version 1.2.0-cdh5.14.2 <br><br>  Il convient de noter que le lancement du compactage majeur a √©t√© effectu√© sur une table ¬´en direct¬ª, dans laquelle les donn√©es ont √©t√© activement √©crites et lues.  Il y avait une d√©claration sur le r√©seau que cela pourrait conduire √† une r√©ponse incorrecte lors de la lecture des donn√©es.  Pour v√©rifier, un processus a √©t√© lanc√© qui a g√©n√©r√© de nouvelles donn√©es et les a √©crites dans la table.  Apr√®s quoi j'ai imm√©diatement lu et v√©rifi√© si la valeur obtenue co√Øncidait avec ce qui a √©t√© enregistr√©.  Au cours de ce processus, un compactage majeur a √©t√© lanc√© environ 200 fois et aucune d√©faillance n'a √©t√© enregistr√©e.  Peut-√™tre que le probl√®me appara√Æt rarement et uniquement pendant une charge √©lev√©e, il est donc plus s√ªr d'arr√™ter de mani√®re planifi√©e les processus d'√©criture et de lecture et d'effectuer le nettoyage sans autoriser ces rabattements GC. <br><br>  De plus, le compactage majeur n'affecte pas l'√©tat de MemStore, pour le vider sur le disque et le compacter, vous devez utiliser flush (connection.getAdmin (). Flush (TableName.valueOf (tblName))). <br><br><h2>  8. Param√®tres et performances </h2><br>  Comme d√©j√† mentionn√©, HBase montre le plus grand succ√®s o√π il n'a rien √† faire lors de l'ex√©cution de BulkLoad.  Cependant, cela s'applique √† la plupart des syst√®mes et des personnes.  Cependant, cet outil est plus adapt√© √† l'empilement en bloc de donn√©es en gros blocs, alors que si le processus n√©cessite beaucoup de demandes de lecture et d'√©criture concurrentes, les commandes Get et Put d√©crites ci-dessus sont utilis√©es.  Pour d√©terminer les param√®tres optimaux, des lancements ont √©t√© effectu√©s avec diff√©rentes combinaisons de param√®tres et de r√©glages de table: <br><br><ul><li>  10 threads ont √©t√© lanc√©s en m√™me temps 3 fois de suite (appelons cela un bloc de threads). </li><li>  La dur√©e de fonctionnement de tous les flux dans le bloc a √©t√© moyenn√©e et √©tait le r√©sultat final du fonctionnement du bloc. </li><li>  Tous les fils fonctionnaient avec la m√™me table. </li><li>  Avant chaque d√©marrage du bloc de threads, un compactage majeur a √©t√© ex√©cut√©. </li><li>  Chaque bloc n'a effectu√© qu'une seule des op√©rations suivantes: </li></ul><br>  - Mettez <br>  - obtenir <br>  - Get + Put <br><br><ul><li>  Chaque bloc a effectu√© 50 000 r√©p√©titions de son op√©ration. </li><li>  La taille d'enregistrement dans le bloc est de 100 octets, 1000 octets ou 10000 octets (al√©atoire). </li><li>  Les blocs ont √©t√© lanc√©s avec un nombre diff√©rent de cl√©s demand√©es (soit une cl√©, soit 10). </li><li>  Des blocs ont √©t√© lanc√©s √† diff√©rents param√®tres de table.  Param√®tres modifi√©s: </li></ul><br>  - BlockCache = activ√© ou d√©sactiv√© <br>  - BlockSize = 65 Ko ou 16 Ko <br>  - Cloisons = 1, 5 ou 30 <br>  - MSLAB = activ√© ou d√©sactiv√© <br><br>  Ainsi, le bloc ressemble √† ceci: <br><br>  a.  Le mode MSLAB est activ√© / d√©sactiv√©. <br>  b.  Une table a √©t√© cr√©√©e pour laquelle les param√®tres suivants ont √©t√© d√©finis: BlockCache = true / none, BlockSize = 65/16 Kb, Partitions = 1/5/30. <br>  c.  R√©glez la compression GZ. <br>  d.  10 threads ont √©t√© lanc√©s simultan√©ment effectuant 1/10 des op√©rations put / get / get + put dans ce tableau avec des enregistrements de 100/1000/10000 octets, ex√©cutant 50 000 requ√™tes cons√©cutives (cl√©s al√©atoires). <br>  e.  Le point d a √©t√© r√©p√©t√© trois fois. <br>  f.  La dur√©e de fonctionnement de tous les threads a √©t√© moyenn√©e. <br><br>  Toutes les combinaisons possibles ont √©t√© v√©rifi√©es.  Il est pr√©visible qu'√† mesure que la taille d'enregistrement augmente, la vitesse diminue ou que la d√©sactivation de la mise en cache ralentit.  Cependant, l'objectif √©tait de comprendre le degr√© et l'importance de l'influence de chaque param√®tre, par cons√©quent, les donn√©es collect√©es ont √©t√© introduites dans l'entr√©e de la fonction de r√©gression lin√©aire, ce qui permet d'√©valuer la fiabilit√© √† l'aide de statistiques t.  Voici les r√©sultats des blocs effectuant des op√©rations de vente.  Un ensemble complet de combinaisons 2 * 2 * 3 * 2 * 3 = 144 options + 72 depuis  certains ont √©t√© ex√©cut√©s deux fois.  Par cons√©quent, 216 lancements au total: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/5u/wj/ai5uwj0fvmyo9hyqkjg4-cigceq.png"></div><br>  Les tests ont √©t√© effectu√©s sur un mini-cluster compos√© de 3 DataNode et 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flux).  HBase version 1.2.0-cdh5.14.2. <br><br>  La vitesse d'insertion la plus √©lev√©e de 3,7 secondes a √©t√© obtenue lorsque le mode MSLAB a √©t√© d√©sactiv√©, sur une table avec une partition, avec BlockCache activ√©, BlockSize = 16, enregistrements de 100 octets de 10 pi√®ces par paquet. <br>  La vitesse d'insertion la plus faible de 82,8 secondes a √©t√© obtenue lorsque le mode MSLAB √©tait activ√©, sur une table avec une partition, avec BlockCache activ√©, BlockSize = 16, enregistrements de 10 000 octets chacun. <br><br>  Voyons maintenant le mod√®le.  Nous voyons un mod√®le de bonne qualit√© pour R2, mais il est clair que l'extrapolation est ici contre-indiqu√©e.  Le comportement r√©el du syst√®me lors de la modification des param√®tres ne sera pas lin√©aire, ce mod√®le n'est pas n√©cessaire pour les pr√©visions, mais pour comprendre ce qui s'est pass√© dans les param√®tres donn√©s.  Par exemple, ici nous voyons par le crit√®re de Student que pour l'op√©ration Put, les param√®tres BlockSize et BlockCache n'ont pas d'importance (ce qui est g√©n√©ralement pr√©visible): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/aq/vg/xt/aqvgxt9uyfs_l4m4crnm3adhliy.png"></div><br>  Mais le fait qu'une augmentation du nombre de partitions entra√Æne une baisse des performances est quelque peu inattendu (nous avons d√©j√† vu l'effet positif d'une augmentation du nombre de partitions avec BulkLoad), bien que cela soit compr√©hensible.  Tout d'abord, pour le traitement, il est n√©cessaire de former des requ√™tes sur 30 r√©gions au lieu d'une, et la quantit√© de donn√©es n'est pas telle qu'elle donne un gain.  Deuxi√®mement, le temps de fonctionnement total est d√©termin√© par le RS le plus lent, et puisque le nombre de DataNode est inf√©rieur au nombre de RS, certaines r√©gions ont une localit√© nulle.  Eh bien, regardons les cinq premiers: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/md/iu/bxmdiuzfdzqlfe1l8_s_2ktecb0.png"></div><br>  Maintenant, √©valuons les r√©sultats de l'ex√©cution des blocs Get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/2b/no/zl2bnojdyx-byfpty6yr9poebzg.png"></div><br>  Le nombre de partitions a perdu de son importance, ce qui est probablement d√ª au fait que les donn√©es sont bien mises en cache et que le cache de lecture est le param√®tre le plus significatif (statistiquement).  Naturellement, l'augmentation du nombre de messages dans une demande est √©galement tr√®s utile pour les performances.  Les meilleurs r√©sultats: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/6d/pu/f06dpurnzlck4po4jw1xpyrphl8.png"></div><br>  Eh bien, enfin, regardez le mod√®le du bloc qui a ex√©cut√© get first, puis mettez: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/fc/23/aefc23q9mcfbtbumdc5qjmw_qrg.png"></div><br>  Ici, tous les param√®tres sont significatifs.  Et les r√©sultats des dirigeants: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q0/vo/th/q0vothoybhc8k6m1ysefviz3tco.png"></div><br><h2>  9. Test de charge </h2><br>  Enfin, nous allons lancer une charge plus ou moins d√©cente, mais c'est toujours plus int√©ressant quand il y a quelque chose √† comparer.  Le site de DataStax, un d√©veloppeur cl√© de Cassandra, pr√©sente les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©sultats de</a> NT d'un certain nombre de r√©f√©rentiels NoSQL, dont HBase version 0.98.6-1.  Le chargement a √©t√© effectu√© par 40 flux, taille des donn√©es 100 octets, disques SSD.  Le r√©sultat des tests des op√©rations de lecture-modification-√©criture a montr√© ces r√©sultats. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ha/7b/bw/ha7bbwydc612f04jtwyqvdvg_ae.png"></div><br>  Si je comprends bien, la lecture a √©t√© effectu√©e par blocs de 100 enregistrements et pour 16 n≈ìuds HBase, le test DataStax a montr√© une performance de 10 000 op√©rations par seconde. <br><br>  Heureusement que notre cluster poss√®de √©galement 16 n≈ìuds, mais pas tr√®s ¬´chanceux¬ª que chacun a 64 c≈ìurs (threads), tandis que le test DataStax n'en a que 4. D'autre part, ils ont des disques SSD, et nous avons des disques durs et plus encore la nouvelle version de HBase et l'utilisation du CPU pendant la charge n'ont pratiquement pas augment√© de mani√®re significative (visuellement de 5 √† 10%).  N√©anmoins, nous allons essayer de d√©marrer sur cette configuration.  Param√®tres de table par d√©faut, la lecture est effectu√©e dans une plage de cl√©s de 0 √† 50 millions au hasard (c'est-√†-dire, en fait, √† chaque fois une nouvelle).  Dans le tableau, 50 millions d'entr√©es sont r√©parties en 64 partitions.  Les cl√©s sont hach√©es par crc32.  Les param√®tres de table sont d√©finis par d√©faut, MSLAB est activ√©.  √Ä partir de 40 threads, chaque thread lit un ensemble de 100 cl√©s al√©atoires et r√©√©crit imm√©diatement les 100 octets g√©n√©r√©s sur ces cl√©s. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/po/sd/el/posdel66zx7quvvo3kvrjb6uif8.png"></div><br>  Support: 16 DataNode et 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flux).  HBase version 1.2.0-cdh5.14.2. <br><br>  Le r√©sultat moyen est plus proche de 40 000 op√©rations par seconde, ce qui est nettement meilleur que dans le test DataStax.  Cependant, aux fins de l'exp√©rience, les conditions peuvent √™tre l√©g√®rement modifi√©es.  Il est peu probable que tous les travaux soient effectu√©s exclusivement avec une seule table, ainsi qu'avec des cl√©s uniques.  Supposons qu'il existe un certain jeu de cl√©s ¬´√† chaud¬ª qui g√©n√®re la charge principale.  Par cons√©quent, nous allons essayer de cr√©er une charge avec des enregistrements plus volumineux (10 Ko), √©galement en paquets de 100 chacun, dans 4 tableaux diff√©rents et en limitant la plage de cl√©s demand√©es √† 50 000. Le graphique ci-dessous montre le d√©but de 40 threads, chaque flux lit un ensemble de 100 cl√©s et √©crit imm√©diatement 10 al√©atoires KB sur ces cl√©s de retour. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/ec/mr/f7ecmrebgulvlcyru05c7jzytyi.png"></div><br>  Support: 16 DataNode et 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flux).  HBase version 1.2.0-cdh5.14.2. <br><br>  Pendant le chargement, un compactage majeur a √©t√© lanc√© plusieurs fois, comme illustr√© ci-dessus sans cette proc√©dure, les performances se d√©graderont progressivement, cependant, une charge suppl√©mentaire se produit √©galement lors de l'ex√©cution.  Les pr√©l√®vements sont caus√©s par diverses raisons.  Parfois, les threads se terminaient et pendant leur red√©marrage, il y avait une pause, parfois des applications tierces cr√©aient une charge sur le cluster. <br><br>  Lire et √©crire tout de suite est l'un des sc√©narios de travail les plus difficiles pour HBase.  Si vous ne mettez que des requ√™tes put d'une petite taille, par exemple 100 octets chacune, en les combinant en lots de 10 √† 50 000 pi√®ces, vous pouvez obtenir des centaines de milliers d'op√©rations par seconde et la situation est similaire avec des requ√™tes en lecture seule.  Il est √† noter que les r√©sultats sont radicalement meilleurs que ceux qui ont √©t√© obtenus chez DataStax surtout en raison de demandes en blocs de 50 000. <br><br><img src="https://habrastorage.org/webt/kv/_j/bv/kv_jbvizskwbod1nxapokckg9s8.png"><br>  Support: 16 DataNode et 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 flux).  HBase version 1.2.0-cdh5.14.2. <br><br><h2>  10. Conclusions </h2><br>  Ce syst√®me est suffisamment flexible pour √™tre configur√©, mais l'effet d'un grand nombre de param√®tres est encore inconnu.  Certains d'entre eux ont √©t√© test√©s, mais n'ont pas √©t√© inclus dans la suite de tests r√©sultante.  Par exemple, des exp√©riences pr√©liminaires ont montr√© l'insignifiance d'un param√®tre tel que DATA_BLOCK_ENCODING, qui code les informations en utilisant les valeurs des cellules voisines, ce qui est tout √† fait compr√©hensible pour les donn√©es g√©n√©r√©es de mani√®re al√©atoire.  Dans le cas de l'utilisation d'un grand nombre d'objets r√©p√©titifs, le gain peut √™tre important.  En g√©n√©ral, nous pouvons dire que HBase donne l'impression d'une base de donn√©es assez s√©rieuse et bien pens√©e, qui peut √™tre assez productive lorsqu'il s'agit de gros blocs de donn√©es.  Surtout s'il est possible d'√©taler les processus de lecture et d'√©criture dans le temps. <br><br>  Si, √† votre avis, quelque chose n'est pas suffisamment divulgu√©, je suis pr√™t √† le dire plus en d√©tail.  Nous vous sugg√©rons de partager votre exp√©rience ou de d√©battre si vous n'√™tes pas d'accord avec quelque chose. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420425/">https://habr.com/ru/post/fr420425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420409/index.html">Apprenez OpenGL. Le√ßon 5.7 - HDR</a></li>
<li><a href="../fr420413/index.html">SQLite et NW.js - instructions pas √† pas pour cr√©er des amiti√©s solides</a></li>
<li><a href="../fr420415/index.html">Tout ce que vous vouliez savoir sur les tests d'adaptateurs Wi-Fi, mais aviez peur de demander</a></li>
<li><a href="../fr420419/index.html">Coureurs pour ceux qui aiment l'humiliation ou comment nous avons chang√© et modifi√© PixJam</a></li>
<li><a href="../fr420423/index.html">Probl√®mes d'interface avec le passage au sol</a></li>
<li><a href="../fr420429/index.html">USE, RED, PgBouncer, ses param√®tres et surveillance</a></li>
<li><a href="../fr420431/index.html">Mars Guide pratique de la terraformation pour les femmes au foyer</a></li>
<li><a href="../fr420433/index.html">¬´Format du vendredi¬ª: routes musicales - qu'est-ce que c'est et pourquoi elles ne sont pas en Russie</a></li>
<li><a href="../fr420435/index.html">D√©marrage rapide avec ARM Mbed: d√©veloppement de microcontr√¥leurs modernes pour d√©butants</a></li>
<li><a href="../fr420437/index.html">Une introduction pratique au gestionnaire de paquets pour Kubernetes - Helm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>