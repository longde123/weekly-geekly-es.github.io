<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶Å üòÖ üõ´ Libro "Ingenier√≠a de confiabilidad del sitio. Fiabilidad y fiabilidad como en Google ¬ª ‚úãüèæ ‚è¨ ‚§¥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Durante casi 20 a√±os, Google ha estado proporcionando sistemas inimaginablemente complejos y a gran escala que son sensibles a las solicitudes de los ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Libro "Ingenier√≠a de confiabilidad del sitio. Fiabilidad y fiabilidad como en Google ¬ª</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/420139/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/dx/hl/8t/dxhl8tgecl0xwqtx3pnxztosrom.jpeg" align="left" alt="imagen"></a>  Durante casi 20 a√±os, Google ha estado proporcionando sistemas inimaginablemente complejos y a gran escala que son sensibles a las solicitudes de los usuarios.  El motor de b√∫squeda de Google encuentra la respuesta a cualquier pregunta en una fracci√≥n de segundo, los mapas de Google con la mayor precisi√≥n reflejan el paisaje terrestre, y el correo de Google est√° disponible en modo 365/24/7 y, en esencia, se ha convertido en el primer almacenamiento en la nube p√∫blica.  ¬øSon estos sistemas perfectos?  No, tambi√©n fallan, se rompen y se vuelven obsoletos, como cualquier equipo.  Simplemente no lo notamos.  La cuesti√≥n es que, durante m√°s de diez a√±os, Google ha estado desarrollando la tecnolog√≠a √∫nica de ingenier√≠a de confiabilidad del sitio, que garantiza el funcionamiento ininterrumpido y el desarrollo progresivo de sistemas de software de cualquier complejidad.  Este libro es un dep√≥sito de experiencia acumulada por Google a lo largo de muchos a√±os, el trabajo colectivo de muchos especialistas destacados y un recurso indispensable para cualquier ingeniero que desee desarrollar y mantener cualquier producto con la m√°s alta calidad y de la manera m√°s eficiente. <br><a name="habracut"></a><br><h3>  SRE de Google en t√©rminos de SRE </h3><br>  Los centros de datos de Google (centros de datos) son significativamente diferentes de los centros de datos tradicionales y las "granjas" de servidores peque√±os.  Estas diferencias introducen problemas adicionales y oportunidades adicionales.  Este cap√≠tulo analiza los desaf√≠os y oportunidades espec√≠ficos de los centros de datos de Google e introduce la terminolog√≠a que se utilizar√° en todo el libro. <br><br>  <b>Equipo</b> <br><br>  La mayor√≠a de los recursos inform√°ticos de Google se encuentran en centros de datos dise√±ados por la empresa, que tienen su propio sistema de suministro de energ√≠a, sistema de refrigeraci√≥n, red interna y equipo inform√°tico [Barroso et al., 2013].  A diferencia de los centros de datos t√≠picos proporcionados por los proveedores a sus clientes, todos los centros de datos de Google est√°n equipados con lo mismo1.  Para evitar confusiones entre el hardware del servidor y el software del servidor, en este libro usamos la siguiente terminolog√≠a: <br><br><ul><li>  <i>m√°quina (computadora)</i> : una unidad de equipo (o, posiblemente, una m√°quina virtual); </li><li>  <i>servidor</i> : una unidad de software que implementa un servicio. </li></ul><br>  Cualquier servidor puede iniciarse en m√°quinas, por lo tanto, no asignamos computadoras espec√≠ficas para programas de servidor espec√≠ficos.  Por ejemplo, no tenemos una m√°quina espec√≠fica en la que se est√© ejecutando el servidor de correo.  En cambio, los recursos son asignados por nuestro sistema de gesti√≥n de cl√∫ster Borg. <br><br>  Entendemos que tal uso del t√©rmino "servidor" no es est√°ndar.  Es m√°s habitual designar dos conceptos a la vez: un programa que sirve conexiones de red y, al mismo tiempo, una m√°quina que ejecuta dichos programas, pero cuando hablamos de la potencia inform√°tica de Google, la diferencia entre ambos es significativa.  Tan pronto como se acostumbre a nuestra interpretaci√≥n de la palabra "servidor", le resultar√° m√°s claro por qu√© es importante utilizar una terminolog√≠a tan especializada no solo directamente en Google, sino a lo largo de este libro. <br><br>  En la fig.  2.1 demostr√≥ la configuraci√≥n del centro de datos de Google. <br><br><ul><li>  Se colocan docenas de autos en bastidores. </li><li>  Los bastidores se colocan en filas. </li><li>  Una o m√°s filas forman un grupo. </li><li>  Por lo general, en la construcci√≥n de un centro de datos (DPC) o centro de datos, se ubican varios grupos. </li><li>  Varios edificios de centros de datos que est√°n cerca uno del otro conforman el campus. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/no/2i/-3no2ios8vkdcddiea5ynqaaz1s.png" alt="imagen"></div><br>  Dentro de cada centro de datos, todas las m√°quinas deber√≠an poder comunicarse efectivamente entre s√≠, por lo que creamos un conmutador virtual muy r√°pido (conmutador) con decenas de miles de puertos.  Esto fue posible conectando cientos de conmutadores desarrollados por Google en una "f√°brica" ‚Äã‚Äãbasada en la topolog√≠a de la red Clos [Clos, 1953], llamada J√∫piter [Singh et al., 2015].  En su configuraci√≥n m√°xima, J√∫piter admite un rendimiento de 1.3 Pb / s entre servidores. <br><br>  Los centros de datos se conectan entre s√≠ mediante nuestra red troncal B4 global [Jain et al., 2013].  B4 tiene una arquitectura de red configurable por software y utiliza el protocolo de comunicaci√≥n abierta OpenFlow.  B4 proporciona un ancho de banda amplio a un n√∫mero limitado de sistemas y utiliza un control de ancho de canal flexible para maximizar su valor promedio [Kumar et al., 2015]. <br><br><h3>  Software del sistema que "organiza" el equipo </h3><br>  El software que proporciona la gesti√≥n y administraci√≥n de nuestros equipos debe ser capaz de manejar grandes sistemas.  Las fallas de hardware son uno de los principales problemas resueltos con la ayuda del software.  Dada la gran cantidad de componentes de hardware en un cl√∫ster, suceden con bastante frecuencia.  En cada grupo, miles de m√°quinas suelen fallar en un a√±o y fallan miles de discos duros.  Si multiplica este n√∫mero por el n√∫mero de grupos que operan en todo el mundo, el resultado es asombroso.  Por lo tanto, queremos aislar a los usuarios de tales problemas, y los equipos involucrados en nuestros servicios tampoco quieren ser distra√≠dos por problemas de hardware.  Cada campus del centro de datos tiene equipos que son responsables de apoyar el equipo y la infraestructura del centro de datos. <br><br><h3>  Gesti√≥n de la m√°quina </h3><br>  Borg (Figura 2.2) es un sistema de gesti√≥n de cl√∫ster distribuido [Verma et al., 2015], similar a Apache Mesos.  Borg gestiona trabajos a nivel de cl√∫ster. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tu/bv/iu/tubviu0qs-kyfob6r1kgodbttye.png" alt="imagen"></div>  Borg es responsable de lanzar trabajos de usuario.  Estas tareas pueden ser servicios que se ejecutan constantemente o procesos por lotes como MapReduce [Dean y Ghemawat, 2004].  Pueden consistir en varias (a veces miles) de tareas (tareas) id√©nticas, tanto por razones de confiabilidad como porque un proceso, por regla general, no puede procesar todo el tr√°fico del cl√∫ster.  Cuando Borg comienza la tarea, encuentra las m√°quinas para realizar sus tareas y les ordena que inicien el programa del servidor.  Borg luego monitorea el estado de estas tareas.  Si la tarea no funciona correctamente, se destruye y se reinicia, posiblemente en otra m√°quina. <br><br>  Dado que las tareas se distribuyen libremente entre las m√°quinas, no podemos usar direcciones IP y n√∫meros de puerto para acceder a ellas.  Este problema se resuelve mediante un nivel adicional de abstracci√≥n: al iniciar una tarea, Borg asigna un nombre para la tarea y un n√∫mero (√≠ndice) para cada tarea utilizando el servicio de nombres Borg (BNS).  En lugar de usar la direcci√≥n IP y el n√∫mero de puerto, otros procesos se asocian con las tareas de Borg por su nombre BNS, que luego convierte el BNS en direcci√≥n IP y n√∫mero de puerto.  Por ejemplo, la ruta BNS puede ser una cadena como / bns / &lt;cluster&gt; / &lt;user&gt; / &lt;task_name&gt; / &lt;task_number&gt;, que luego se traduce (es habitual decir "permitido" en las redes) en el formato &lt;direcci√≥n IP&gt;: &lt;port&gt; . <br><br>  Borg tambi√©n es responsable de asignar recursos para las tareas.  Cada tarea debe indicar qu√© recursos se requieren para completarla (por ejemplo, tres n√∫cleos de procesador, 2 GB de RAM).  Utilizando la lista de requisitos para todas las tareas, Borg puede distribuir de manera √≥ptima las tareas entre las m√°quinas, teniendo en cuenta tambi√©n las consideraciones de tolerancia a fallas (por ejemplo, Borg no iniciar√° todas las tareas de una tarea en el mismo rack, ya que el cambio de este rack ser√° un punto cr√≠tico en caso de falla) tareas). <br><br>  Si una tarea intenta obtener m√°s recursos de los solicitados, Borg la destruye y luego se reinicia (ya que generalmente es preferible tener una tarea que a veces se bloquea y se reinicia que no se reinicia en absoluto). <br><br><h3>  Almacenamiento </h3><br>  Para un acceso m√°s r√°pido a los datos, las tareas pueden usar el disco local de las m√°quinas, pero tenemos varias opciones para organizar el almacenamiento persistente en el cl√∫ster (e incluso los datos almacenados localmente eventualmente se mover√°n al almacenamiento del cl√∫ster).  Se pueden comparar con Luster y el Sistema de archivos distribuidos de Hadoop (HDFS): sistemas de archivos en cl√∫ster con una implementaci√≥n de c√≥digo abierto. <br><br>  El almacenamiento proporciona a los usuarios la capacidad de acceder de manera f√°cil y confiable a los datos disponibles para el cl√∫ster.  Como se muestra en la fig.  2.3, el repositorio tiene varias capas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/yf/7c/atyf7cemd0e2gczkjfgdnltw7zy.png" alt="imagen"></div><br>  1. La capa m√°s baja se llama D (desde el disco, aunque el nivel D usa tanto discos duros tradicionales como unidades flash).  D es un servidor de archivos que se ejecuta en pr√°cticamente todas las m√°quinas de cl√∫ster.  Sin embargo, los usuarios que desean acceder a sus datos no querr√°n recordar en qu√© m√°quina est√°n almacenados, por lo que la siguiente capa est√° conectada aqu√≠. <br><br>  2. Sobre la capa D est√° la capa Coloso, que crea un sistema de archivos en el cl√∫ster que ofrece la sem√°ntica habitual del sistema de archivos, as√≠ como la replicaci√≥n y el cifrado.  Colossus es el sucesor de GFS, el Sistema de archivos de Google (Ghemawat et al., 2003). <br><br>  3. A continuaci√≥n, hay varios servicios similares a bases de datos creados por encima del nivel Coloso. <br><br><ul><li>  Bigtable [Chang et al., 2006] es un sistema de base de datos no relacional (NoSQL) capaz de trabajar con bases de datos del tama√±o de petabytes.  Bigtable es una base de datos ordenada, distribuida, tolerante a fallas, multidimensional, ordenada que est√° indexada por filas, columnas y claves de sello de tiempo;  cada valor de la base de datos es una matriz arbitraria no interpretada de bytes.  Bigtable tambi√©n admite la replicaci√≥n entre centros de datos. </li><li>  Spanner [Corbett et al., 2012] ofrece una interfaz similar a SQL para usuarios que requieren integridad y consistencia de datos al acceder desde cualquier parte del mundo. </li><li>  Varios otros sistemas de bases de datos est√°n disponibles, como Blobstore.  Todos tienen sus propias fortalezas y debilidades (ver cap√≠tulo 26). </li></ul><br><h3>  Red </h3><br>  Google Networking se gestiona de varias maneras.  Como se mencion√≥ anteriormente, utilizamos una red configurable por software basada en OpenFlow.  En lugar de enrutadores inteligentes, utilizamos conmutadores no tan caros en combinaci√≥n con un controlador central (duplicado), que calcula previamente la mejor ruta en la red.  Esto le permite utilizar un equipo de conmutaci√≥n m√°s simple, lo que lo libera de la b√∫squeda de rutas que lleva mucho tiempo. <br><br>  El ancho de banda de la red debe asignarse adecuadamente.  Como Borg limita los recursos inform√°ticos que una tarea puede usar, Bandwidth Enforcer (BwE) gestiona el ancho de banda disponible para maximizar el rendimiento promedio.  La optimizaci√≥n del ancho de banda no solo est√° relacionada con el costo: la gesti√≥n centralizada del tr√°fico resuelve una serie de problemas que son extremadamente dif√≠ciles de resolver mediante una combinaci√≥n de enrutamiento distribuido y gesti√≥n convencional del tr√°fico (Kumar, 2015). <br><br>  Algunos servicios tienen trabajos que se ejecutan en varios grupos ubicados en diferentes partes del mundo.  Para reducir el tiempo de retraso de los sistemas distribuidos globalmente, nos gustar√≠a dirigir a los usuarios al centro de datos m√°s cercano que tenga la capacidad adecuada para esto.  Nuestro Global Software Load Balancer (GSLB) realiza el equilibrio de carga en tres niveles: <br><br><ul><li>  el equilibrio de carga geogr√°fica para consultas DNS (por ejemplo, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">www.google.com</a> ), se describe en el cap√≠tulo 19; </li><li>  equilibrio de carga a nivel de servicios de usuario (por ejemplo, YouTube o Google Maps); </li><li>  equilibrio de carga en el nivel de Llamada a procedimiento remoto (RPC), descrito en el Cap√≠tulo 20. </li></ul><br>  Los propietarios de servicios especifican nombres simb√≥licos para ellos, una lista de direcciones BNS del servidor y el rendimiento disponible en cada sitio (generalmente se mide en consultas por segundo, consultas por segundo, QPS).  Posteriormente, el GSLB enruta el tr√°fico a las direcciones BNS especificadas. <br><br><h3>  Otro software del sistema </h3><br><br>  Hay otros componentes importantes para el software del centro de datos. <br><br>  <b>Servicio de bloqueo</b> <br><br>  El Servicio Chubby Lock [Burrows, 2006] proporciona una API similar al sistema de archivos para servir bloqueos.  Chubby maneja bloqueos en todos los centros de datos.  Utiliza el protocolo Paxos para acceder as√≠ncronamente al Consenso (ver cap√≠tulo 23). <br><br>  Chubby tambi√©n juega un papel importante en la elecci√≥n de un asistente.  Si para alg√∫n servicio se proporcionan cinco r√©plicas de una tarea con el fin de aumentar la confiabilidad, pero en un momento particular solo una de ellas hace el trabajo real, entonces Chubby se usa para seleccionar esta r√©plica. <br>  Chubby es ideal para datos que requieren confiabilidad de almacenamiento.  Por esta raz√≥n, BNS usa Chubby para almacenar la proporci√≥n de rutas BNS a la direcci√≥n IP: pares de puertos. <br><br>  <b>Monitoreo y Alertas</b> <br><br>  Queremos asegurarnos de que todos los servicios funcionen correctamente.  Por lo tanto, estamos lanzando muchas instancias del programa de monitoreo Borgmon (ver cap√≠tulo 10).  Borgmon recibe regularmente valores de referencia de los servicios supervisados.  Estos datos pueden usarse inmediatamente para notificaci√≥n o almacenarse para su posterior procesamiento y an√°lisis, por ejemplo, para construir gr√°ficos.  Tal monitoreo puede ser usado para prop√≥sitos tales como: <br><br><ul><li>  configurar alertas para problemas urgentes; </li><li>  comparaci√≥n de comportamiento: la actualizaci√≥n del software aceler√≥ el servidor; </li><li>  evaluaci√≥n de la naturaleza de los cambios en el consumo de recursos a lo largo del tiempo, lo cual es necesario para la planificaci√≥n de la capacidad. </li></ul><br><br><h3>  Nuestra infraestructura de software </h3><br>  La arquitectura de nuestro software est√° dise√±ada para que sea posible utilizar los recursos de hardware del sistema de manera m√°s eficiente.  Nuestro c√≥digo completo es multiproceso, por lo que una tarea puede usar f√°cilmente varios n√∫cleos.  Para admitir paneles, monitoreo y depuraci√≥n, cada servidor incluye una implementaci√≥n de servidor HTTP como interfaz a trav√©s de la cual se proporciona informaci√≥n de diagn√≥stico y estad√≠sticas para una tarea espec√≠fica. <br><br>  Todos los servicios de Google "se comunican" utilizando la infraestructura de llamada a procedimiento remoto (RPC) llamada Stubby.  Hay una versi√≥n de c√≥digo abierto, se llama gRPC (ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">grpc.io</a> ).  A menudo, se realiza una llamada RPC incluso para rutinas en el programa local.  Esto le permite reorientar el programa a las llamadas de otro servidor para lograr una mayor modularidad o a medida que crece el c√≥digo del servidor original.  GSLB puede realizar el equilibrio de carga RPC de la misma manera que para las interfaces de servicios externos. <br><br>  El servidor recibe solicitudes RPC desde el front end y env√≠a el RPC al backend.  Usando t√©rminos tradicionales, la interfaz se llama cliente y el servidor se llama servidor. <br>  Los datos se transfieren hacia y desde RPC utilizando el protocolo de serializaci√≥n, los llamados buffers de protocolo o, brevemente, protobufs.  Este protocolo es similar al Thrift de Apache y tiene varias ventajas sobre XML cuando se trata de serializar datos estructurados: es m√°s simple, tres a diez veces m√°s compacto, 20 a 100 veces m√°s r√°pido y m√°s √∫nico. <br><br><h3>  Nuestro entorno de desarrollo </h3><br>  La velocidad del desarrollo del producto es muy importante para Google, por lo que creamos un entorno especial que aprovecha al m√°ximo nuestra infraestructura [Morgenthaler et al., 2012]. <br><br>  Con la excepci√≥n de algunos grupos cuyos productos son de c√≥digo abierto y, por lo tanto, usan sus propios repositorios separados (por ejemplo, Android y Chrome), los ingenieros de software de Google trabajan en un repositorio com√∫n [Potvin, Levenberg, 2016].  Este enfoque tiene varias aplicaciones pr√°cticas que son importantes para nuestro proceso de producci√≥n. <br><br><ul><li>  Si un ingeniero encuentra un problema en un componente fuera de su proyecto, puede solucionar el problema, enviar los cambios propuestos ("lista de cambios" - lista de cambios, CL) al propietario para su consideraci√≥n y luego implementar los cambios realizados en la rama principal del programa. </li><li>  Los cambios en el c√≥digo fuente en el propio proyecto de un ingeniero requieren consideraci√≥n: realizar una auditor√≠a (revisi√≥n).  Todo el software pasa esta etapa antes de la adopci√≥n. </li></ul><br>  Cuando se ensambla el software, la solicitud de ensamblado se env√≠a a servidores de centros de datos especializados.  Incluso construir proyectos grandes es r√°pido porque puede usar m√∫ltiples servidores para compilaci√≥n paralela.  Dicha infraestructura tambi√©n se utiliza para pruebas continuas.  Cada vez que aparece una nueva lista de cambios (CL), se ejecutan pruebas de todo el software que puede verse afectado directa o indirectamente por esos cambios.  Si el marco detecta que los cambios interrumpieron la operaci√≥n de otras partes del sistema, notifica al propietario de estos cambios.  Algunos proyectos utilizan el sistema push-on-green ("env√≠o con √©xito"), seg√∫n el cual la nueva versi√≥n se env√≠a autom√°ticamente a operaci√≥n comercial despu√©s de pasar las pruebas. <br><br><h3>  Shakespeare: ejemplo de servicio </h3><br>  Para demostrar c√≥mo Google desarrolla un servicio en un entorno industrial, considere un ejemplo de un servicio hipot√©tico que interact√∫a con las tecnolog√≠as de Google.  Supongamos que queremos ofrecer un servicio que le permita determinar en qu√© obras de Shakespeare se produce la palabra que ha mencionado. <br><br>  Podemos dividir el sistema en dos partes. <br><br><ul><li>  Un componente de procesamiento por lotes que lee todos los textos de Shakespeare, crea un √≠ndice y lo escribe en Bigtable.  Esta tarea (m√°s precisamente, la tarea) se realiza una vez o, posiblemente, ocasionalmente (¬°despu√©s de todo, puede aparecer alg√∫n texto nuevo de Shakespeare!). </li><li>  Una aplicaci√≥n front-end que procesa las solicitudes de los usuarios finales.  Esta tarea siempre se est√° ejecutando, porque en cualquier momento, un usuario de cualquier zona horaria puede querer buscar en los libros de Shakespeare. </li></ul><br>  El componente de procesamiento por lotes ser√° el servicio MapReduce, cuyo trabajo se divide en tres fases. <br><br>  1. En la fase de mapeo, los textos de Shakespeare se leen y se dividen en palabras separadas.  Esta parte del trabajo se completar√° m√°s r√°pido si se inician varios procesos de trabajo (tareas) en paralelo. <br><br>  2. En la fase Aleatorio, las entradas se ordenan por palabra. <br><br>  3. En la fase Reducir, se crean tuplas del formulario (word, list_products). <br><br>  Cada tupla se escribe como una cadena en Bigtable, la clave es la palabra. <br><br><h3>  Solicitar ciclo de vida </h3><br>  En la fig.  2.4 muestra c√≥mo se atiende la solicitud del usuario.  Primero, el usuario hace clic en el enlace shakespeare.google.com en el navegador.  Para obtener la direcci√≥n IP adecuada, el dispositivo del usuario traduce ("resuelve") la direcci√≥n utilizando el servidor DNS (1).  La consulta DNS finalmente termina en el servidor DNS de Google, que interact√∫a con el GSLB.  Rastreando la carga de tr√°fico de todos los servidores front-end por regi√≥n, GSLB elige qu√© direcci√≥n IP de qu√© servidor devolver√° al usuario. <br><br>  El navegador se conecta al servidor HTTP en la direcci√≥n especificada.  Este servidor (se llama Google Frontend o GFE) es un servidor proxy "inverso" ubicado en el otro extremo de la conexi√≥n TCP del cliente (2).  GFE busca el servicio requerido (por ejemplo, puede ser un servicio de b√∫squeda, mapas o, en nuestro caso, el servicio de Shakespeare).  Accediendo repetidamente al GSLB, el servidor encuentra un servidor front-end Shakespeare disponible y accede a √©l a trav√©s de una llamada a procedimiento remoto (RPC), transmitiendo una solicitud HTTP recibida del usuario (3). <br><br>  El servidor de Shakespeare analiza la solicitud HTTP y crea un "b√∫fer de protocolo" (protobuf) que contiene las palabras que se encuentran.  Ahora, el servidor front-end de Shakespeare debe contactar al servidor back-end de Shakespeare: el primero se pone en contacto con el GSLB para obtener la direcci√≥n BNS de una instancia adecuada y descargada del segundo (4).  A continuaci√≥n, el servidor back-end de Shakespeare se pone en contacto con el servidor Bigtable para recibir los datos solicitados (5). <br><br>  El resultado se escribe en el protocolo de respuesta y se devuelve al servidor de back-end de Shakespeare.  El backend pasa el protocolo con el resultado del servicio al servidor front-end de Shakespeare, que crea un documento HTML y lo devuelve como respuesta al usuario. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6b/nd/yo/6bndyo9cfcxu5k5sx4lieg5bfw8.png" alt="imagen"></div><br>  Toda esta cadena de eventos se ejecuta en un abrir y cerrar de ojos, ¬°en solo unos cientos de milisegundos!  Como hay muchos componentes involucrados, hay muchos lugares donde puede ocurrir un error potencial;  en particular, una falla en el GSLB puede desorganizar todo el trabajo y llevar al colapso.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, la pol√≠tica de control estricto, pruebas exhaustivas e implementaci√≥n segura de nuevos programas de Google, adem√°s de nuestros m√©todos proactivos de recuperaci√≥n de errores (como la eliminaci√≥n gradual de las funciones), nos permite crear servicios confiables que satisfacen las expectativas de nuestros usuarios. </font><font style="vertical-align: inherit;">Al final, las personas visitan regularmente </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">www.google.com</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para verificar si tienen una conexi√≥n a Internet.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Organizaci√≥n de tareas y datos. </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Probar la carga mostr√≥ que nuestro servidor back-end puede procesar aproximadamente 100 solicitudes por segundo (QPS). </font><font style="vertical-align: inherit;">Las pruebas de campo con un n√∫mero limitado de usuarios han demostrado que la carga m√°xima puede alcanzar aproximadamente 3470 QPS, por lo que debemos crear al menos 35 tareas. </font><font style="vertical-align: inherit;">Sin embargo, las siguientes consideraciones dicen que necesitamos al menos 37 tareas, o N + 2.</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Durante la actualizaci√≥n, una tarea no estar√° disponible temporalmente, por lo que 36 tareas permanecer√°n activas. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Durante la actualizaci√≥n, puede ocurrir una falla de hardware, lo que resulta en solo 35 tareas restantes, exactamente tantas como sea necesario para mantener la carga m√°xima. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un estudio m√°s detallado del tr√°fico de usuarios revela la distribuci√≥n geogr√°fica de la carga m√°xima: se generan 1430 QPS de Am√©rica del Norte, 290 de Am√©rica del Sur, 1400 de Europa y 350 de Asia y Australia. En lugar de colocar todos los servidores de back-end en un solo lugar, los distribuimos por regi√≥n: en Estados Unidos, Sudam√©rica, Europa y Asia. Dado el principio de N + 2 en cada regi√≥n, tenemos 17 tareas en los Estados Unidos, 16 en Europa y seis en Asia. Sin embargo, en Am√©rica del Sur, decidimos utilizar cuatro tareas (en lugar de cinco) para reducir los costos, de N + 2 a N + 1. En este caso, estamos listos para correr un peque√±o riesgo de un mayor tiempo de retraso y reducir el costo del equipo: permitir GSLB Al recargar el centro de datos de Am√©rica del Sur, redirigir el tr√°fico de un continente a otro, podemos ahorrar el 20% de los recursos,eso se gastar√≠a en equipo. En regiones m√°s grandes, para mayor estabilidad, distribuimos tareas entre 2 y 3 grupos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dado que los servidores de fondo deben comunicarse con el almac√©n de datos de Bigtable, tambi√©n debemos considerar estrat√©gicamente este almacenamiento. </font><font style="vertical-align: inherit;">Si el servidor de back-end en Asia se comunica con Bigtable, ubicado en los EE. UU., Esto conducir√° a un aumento significativo en la latencia, por lo que duplicaremos Bigtable en cada regi√≥n. </font><font style="vertical-align: inherit;">Esto nos brinda mayor estabilidad en caso de que el servidor Bigtable falle y tambi√©n reduce la latencia del acceso a datos. </font><font style="vertical-align: inherit;">Aunque Bigtable no garantiza una correspondencia estricta de los datos entre las instancias en un momento dado, la duplicaci√≥n no se convierte en un problema grave, ya que no necesitamos actualizar los contenidos del repositorio con demasiada frecuencia.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces, en este cap√≠tulo te has familiarizado con muchos conceptos y t√©rminos. </font><font style="vertical-align: inherit;">Aunque no es necesario memorizarlos todos, pueden ser √∫tiles para explorar muchos otros sistemas, que discutiremos m√°s adelante.</font></font><br><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cup√≥n de 20% de descuento para Habrozavitel - </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ingenier√≠a de confiabilidad del sitio</font></font></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es420139/">https://habr.com/ru/post/es420139/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es420125/index.html">Automatizaci√≥n en finanzas: los empleados del banco pueden quedar sin trabajo debido a los robots</a></li>
<li><a href="../es420129/index.html">Patrones de rutina de Asyncio: afuera aguardan</a></li>
<li><a href="../es420131/index.html">M√©todo de miner√≠a probabil√≠stico de Bitcoin</a></li>
<li><a href="../es420133/index.html">Modelado de sistemas din√°micos: ¬øc√≥mo se mueve la luna?</a></li>
<li><a href="../es420135/index.html">Esto tambi√©n es Toshiba: productos inesperados de la corporaci√≥n japonesa</a></li>
<li><a href="../es420141/index.html">Desde el MPP DBMS cargado - Data Lake lleno de vida con herramientas anal√≠ticas: comparta los detalles de la creaci√≥n</a></li>
<li><a href="../es420143/index.html">Rendimiento de Kotlin en Android</a></li>
<li><a href="../es420145/index.html">¬øC√≥mo es el d√≠a de trabajo de los miembros de PC AppsConf?</a></li>
<li><a href="../es420147/index.html">OpenSource en Clojure</a></li>
<li><a href="../es420151/index.html">M√°s f√°cil de lo que parece. Cap√≠tulo 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>