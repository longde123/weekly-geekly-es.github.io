<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ûüèΩ üèâ üë©üèª‚Äçüç≥ Aprendizaje profundo vs sentido com√∫n: desarrollar un bot de chat üë®‚Äçüë®‚Äçüë¶ üì≠ ‚èÆÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cuantos m√°s usuarios de su servicio, mayor ser√° la probabilidad de que necesiten ayuda. El chat de soporte t√©cnico es una soluci√≥n obvia pero bastante...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje profundo vs sentido com√∫n: desarrollar un bot de chat</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/455652/">  Cuantos m√°s usuarios de su servicio, mayor ser√° la probabilidad de que necesiten ayuda.  El chat de soporte t√©cnico es una soluci√≥n obvia pero bastante costosa.  Pero si usa la tecnolog√≠a de aprendizaje autom√°tico, puede ahorrar algo de dinero. <br><br>  El bot ahora puede responder preguntas simples.  Adem√°s, se puede ense√±ar al chatbot a determinar las intenciones del usuario y a capturar el contexto para que pueda resolver la mayor√≠a de los problemas de los usuarios sin intervenci√≥n humana.  C√≥mo hacer esto, Vladislav Blinov y Valery Baranova, desarrolladores del popular asistente Oleg, ayudar√°n a resolverlo. <br><br><img src="https://habrastorage.org/webt/rr/do/kw/rrdokweqzrtdqiogus2vahosksg.png"><br><br>  Pasando de m√©todos simples a otros m√°s complicados en la tarea de desarrollar un bot de chat, analizaremos problemas pr√°cticos de implementaci√≥n y veremos qu√© ganancia de calidad puede obtener y cu√°nto costar√°. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eL3dkh-WaSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Vladislav Blinov</strong> es un desarrollador senior de sistemas de di√°logo en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tinkoff</a> , a menudo abreviaciones: ML, NLP, DL, etc.  Adem√°s, la escuela de posgrado examina el modelado del humor a trav√©s del aprendizaje autom√°tico y las redes neuronales. <br><br>  <strong>Valeria Baranova ha estado</strong> escribiendo cosas interesantes en el campo de la PNL en Python durante m√°s de 5 a√±os.  Ahora, en el equipo de sistemas interactivos, Tinkoff crea bots de chat y ense√±a un curso de Machine Learning para estudiantes.  Tambi√©n se dedica a la investigaci√≥n en el campo del humor computacional, es decir, ense√±a a la inteligencia artificial a comprender los chistes y crear nuevos. Valeria y Vladislav <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hablar√°n</a> sobre esto en UseData Conf. <br><br>  Los servicios de Tinkoff Bank son utilizados por millones de personas.  Para proporcionar asistencia las 24 horas del d√≠a a tal cantidad de usuarios, se necesita un gran personal, lo que conlleva un alto costo del servicio.  Parece l√≥gico que las preguntas populares de los usuarios puedan responderse autom√°ticamente usando el bot de chat. <br><br><h2>  Intenci√≥n o intenci√≥n del usuario </h2><br>  Lo primero que necesita un chatbot es entender <strong>lo que quiere el usuario</strong> .  Esta tarea se llama clasificaci√≥n de intenciones o intenciones.  Adem√°s, todos los modelos y enfoques ser√°n considerados en el marco de esta tarea. <br><br>  Veamos un ejemplo de clasificaci√≥n de intenciones.  Si escribe: "Transfiera cientos de Lera", el bot de chat Oleg comprender√° que esta es la intenci√≥n de una transferencia de dinero, es decir, la intenci√≥n del usuario de transferir dinero.  O m√°s bien, que Lera necesita transferir la cantidad de 100 rublos. <br><br>  Compararemos m√©todos y probaremos la calidad de su trabajo en una muestra de prueba, que consiste en di√°logos reales con los usuarios.  Nuestra muestra contiene m√°s de 30,000 ejemplos marcados y 170 intenciones, por ejemplo: ir al cine, buscar restaurantes, abrir o cerrar un dep√≥sito, etc.  Oleg tambi√©n tiene su propia opini√≥n sobre muchas cosas y solo puede chatear contigo. <br><br><h2>  Clasificaci√≥n de diccionario </h2><br>  Lo m√°s simple que se puede hacer en la tarea de clasificar intentos es <strong>usar un diccionario</strong> .  Por ejemplo, si la palabra "traducir" aparece en la frase de un usuario, considere que debe hacerse una transferencia de dinero. <br><br>  Veamos la calidad de un enfoque tan simple. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  precisi√≥n </td><td>  recordar </td><td>  puntaje f1 </td></tr><tr><td>  Transferencia de dinero </td><td>  0,88 </td><td>  0.23 </td><td>  0,36 </td></tr><tr><td>  El resto </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  Si el clasificador simplemente define la intenci√≥n del usuario como "transferencia de dinero" con la palabra "traducir", entonces la calidad ya ser√° bastante alta.  Precisi√≥n: 88%, mientras que la integridad es baja, igual a solo el 23%.  Esto es comprensible: la palabra "traducir" no describe todas las posibilidades de decir "transferir dinero a alguien". <br><br>  Sin embargo, este enfoque tiene ventajas: <br><br><ul><li>  No se necesita muestreo etiquetado (si no estudia el modelo, entonces no se necesita muestreo). </li><li>  Puede obtener una alta precisi√≥n si compila bien los diccionarios (pero llevar√° tiempo y recursos). </li></ul><br>  Sin embargo, la integridad de tal soluci√≥n es probable que sea baja, ya que todas las variaciones de cualquier clase son dif√≠ciles de describir. <br><br>  Considere un contraejemplo.  Si adem√°s de la intenci√≥n de transferencia de dinero, "transferencia" tambi√©n puede incluir la segunda intenci√≥n: "transferencia al operador".  Cuando agregamos una nueva intenci√≥n de traducci√≥n al operador, obtenemos resultados diferentes. <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  precisi√≥n </td><td>  recordar </td><td>  puntaje f1 </td></tr><tr><td>  Transferencia de dinero </td><td>  0,70 </td><td>  0.23 </td><td>  0,34 </td></tr><tr><td>  El resto </td><td>  0,97 </td><td>  0,99 </td><td>  0,98 </td></tr></tbody></table></div>  La precisi√≥n se reduce en 18 puntos, mientras que, por supuesto, la integridad no crece.  Esto muestra que se necesita un enfoque m√°s avanzado. <br><br><h2>  An√°lisis de texto </h2><br>  Antes de utilizar el aprendizaje autom√°tico, debe comprender c√≥mo presentar el texto como un vector.  Uno de los enfoques m√°s f√°ciles es <strong>usar un vector tf-idf</strong> . <br><br>  El vector tf-idf tiene en cuenta la aparici√≥n de cada palabra en la frase del usuario y tiene en cuenta la aparici√≥n total de palabras en la colecci√≥n.  Las palabras que a menudo se encuentran en diferentes textos tienen menos peso en esta representaci√≥n vectorial. <br><br>  Veamos la calidad del modelo lineal en representaciones tf-idf (en nuestro caso, regresi√≥n log√≠stica). <br><div class="scrollable-table"><table><tbody><tr><td></td><td>  precisi√≥n </td><td>  recordar </td><td>  puntaje f1 </td></tr><tr><td>  Transferencia de dinero </td><td>  0,74 </td><td>  0,86 </td><td>  0,80 </td></tr><tr><td>  El resto </td><td>  0,99 </td><td>  0,99 </td><td>  0,99 </td></tr></tbody></table></div>  Como resultado, <strong>la integridad aument√≥</strong> bruscamente y la precisi√≥n se mantuvo comparable con el uso del diccionario, la medida f1 (media arm√≥nica ponderada entre precisi√≥n e integridad) tambi√©n aument√≥.  Es decir, el modelo en s√≠ ya comprende qu√© palabras son importantes para cada intento; no es necesario que invente nada usted mismo. <br><br><h3>  Visualizaci√≥n de datos </h3><br>  La visualizaci√≥n de datos ayuda a comprender c√≥mo se ven los intentos, qu√© tan bien est√°n agrupados en el espacio.  Pero no podemos visualizar directamente las representaciones de tf-idf debido a la gran dimensi√≥n, por lo que utilizaremos <strong>el m√©todo de compresi√≥n de dimensiones - t-SNE</strong> . <br><br><img src="https://habrastorage.org/webt/zx/gh/dc/zxghdc_rwrmjjqojzp9b5lao6tq.png"><br><br>  La principal diferencia entre este m√©todo y PCA es que cuando se transfiere al espacio bidimensional, <strong>se preserva</strong> la <strong>distancia relativa entre los objetos</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1p/8w/xr/1p8wxr2sc5guuqktq4u7aqe88jk.png" width="400"></div><br>  <em>t-SNE en tf-idf (10 intentos principales), puntaje F1 0.92</em> <br><br>  Los 10 principales intentos por ocurrencia en nuestra colecci√≥n se presentan arriba.  Hay puntos verdes que no pertenecen a ninguna intenci√≥n, y 10 grupos que est√°n marcados con diferentes colores son intenciones diferentes.  Se puede ver que algunos de ellos est√°n muy bien agrupados.  <strong>La medida</strong> ponderada de <strong>f1 es 0,92</strong> ; esto es bastante, ya puedes trabajar con ella. <br><br>  Entonces, con un clasificador lineal sobre tf-idf: <br><br><ul><li>  integridad mucho mayor que el uso de un diccionario, con una precisi√≥n comparable; </li><li>  no es necesario pensar qu√© palabras corresponden a qu√© intenci√≥n. </li></ul><br>  Pero tambi√©n hay desventajas: <br><br><ul><li>  vocabulario limitado, puede obtener peso solo para aquellas palabras que est√°n presentes en la muestra de entrenamiento; </li><li>  la reformulaci√≥n no se tiene en cuenta; </li><li>  el orden en que aparecen las palabras en el texto no se tiene en cuenta. </li></ul><br><h2>  Reformulaci√≥n </h2><br>  Consideremos con m√°s detalle el problema de la reformulaci√≥n. <br><br>  Los vectores Tf-idf solo pueden estar cerca para textos que se cruzan en palabras.  La proximidad entre los vectores se puede calcular a trav√©s del coseno del √°ngulo entre ellos.  La proximidad del coseno en la representaci√≥n vectorial tf-idf se calcula para ejemplos espec√≠ficos. <br><br><img src="https://habrastorage.org/webt/gf/7c/eb/gf7ceblapupg3xysxiyxgcjwo3o.jpeg"><br><br>  Estas no son frases muy cercanas para la representaci√≥n vectorial tf-idf, aunque para nosotros es la misma intenci√≥n y la misma clase. <br><br>  ¬øQu√© se puede hacer al respecto?  Por ejemplo, en lugar de un n√∫mero, puede representar una palabra como un vector completo; esto se denomina "incrustaci√≥n de palabras". <br><br><img src="https://habrastorage.org/webt/rl/9n/xm/rl9nxmdp8sr_q7fwo7iokyj8hwy.png"><br><br>  Uno de los modelos m√°s populares para resolver este problema se propuso en 2013.  Se llama <strong>word2vec</strong> y se ha usado ampliamente desde entonces. <br><br>  Una de las formas de aprender Word2vec funciona aproximadamente de la siguiente manera: tomamos el texto, tomamos algunas palabras del contexto y las descartamos, luego tomamos otra palabra al azar del contexto y presentamos ambas palabras como vectores √∫nicos.  One-hot vector es un vector de acuerdo con la dimensi√≥n del diccionario, donde solo la coordenada correspondiente al √≠ndice de la palabra en el diccionario tiene el valor 1, el 0 restante. <br><br><img src="https://habrastorage.org/webt/ow/ta/vc/owtavcymm3e4a1k28xltaxdguds.png"><br><br>  A continuaci√≥n, entrenamos una red neuronal simple de una sola capa sin activaci√≥n en la capa interna para predecir la siguiente palabra en contexto, es decir, para predecir la palabra "en la noche" usando la palabra "cohete".  En la salida, obtenemos la distribuci√≥n de probabilidad para todas las palabras del diccionario de la siguiente manera.  Como sabemos cu√°l era realmente la palabra, podemos calcular el error, actualizar los pesos, etc. <br><br><img src="https://habrastorage.org/webt/x6/m-/_e/x6m-_exb0v8m5mmge-q-mr9ez4a.png"><br><br>  Los pesos actualizados obtenidos como resultado de la capacitaci√≥n en nuestra muestra son la palabra incrustaci√≥n. <br><br>  La ventaja de usar incrustaci√≥n en lugar de n√∫mero es, en primer lugar, <strong>que se tiene en cuenta ese contexto</strong> .  Un ejemplo popular: Trump y Putin son cercanos en word2vec porque ambos son presidentes y a menudo se usan juntos en textos. <br><br>  Para las palabras que se encontraron en la muestra de capacitaci√≥n, simplemente tome la matriz de incrustaci√≥n, tome su vector por el √≠ndice de la palabra y obtenga la incrustaci√≥n. <br><br>  Parece que todo est√° bien, excepto que algunas palabras en su matriz pueden no estarlo, porque el modelo no las vio durante el entrenamiento.  Para abordar el problema de las palabras desconocidas (fuera del vocabulario), en 2014 se les ocurri√≥ una modificaci√≥n de word2vec - <strong>fasttext</strong> . <br><br>  Fasttext funciona de la siguiente manera: si la palabra no est√° en el diccionario, se divide en n-gramos simb√≥licos, por cada incrustaci√≥n de n-gramas se toma de la matriz de incrustaciones de n-gramos (que se entrenan como word2vec), las incrustaciones se promedian y se obtiene un vector. <br><br><img src="https://habrastorage.org/webt/sz/ws/mx/szwsmx8vblcqrbumxrjbzz7nzvo.png"><br><br>  Total, obtenemos vectores para palabras que no est√°n en nuestro diccionario.  Ahora podemos <strong>calcular similitudes incluso para palabras desconocidas</strong> .  Y, lo que es m√°s importante, hay modelos entrenados para ruso, ingl√©s y chino, por ejemplo, Facebook y el proyecto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepPavlov</a> , por lo que puede incluirlo r√°pidamente en su cartera. <br><br>  <strong>Pero las desventajas permanecen:</strong> <br><br><ul><li>  El modelo no se utiliza para todo el vector de texto.  Para obtener un vector de texto com√∫n, debe pensar en algo: promedio o promedio con multiplicaci√≥n por pesos idf, y esto puede funcionar de manera diferente en diferentes tareas. </li><li>  El vector para una palabra sigue siendo uno, independientemente del contexto.  Word2vec entrena un vector de palabra para cualquier contexto en el que aparece la palabra.  Para palabras de valores m√∫ltiples (como, por ejemplo, lenguaje) habr√° un mismo vector. </li></ul><br><img src="https://habrastorage.org/webt/ob/i3/ap/obi3apyrva_kjktfpfjh7farkx8.png"><br><br>  De hecho, la proximidad del coseno en nuestro ejemplo en texto r√°pido es mayor que la proximidad del coseno en tf-idf, aunque las frases en estas frases son solo "in". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2p/lz/hq/2plzhqw0n1-8m4vrc71_zwokjla.png" width="400"></div><br>  <em>t-SNE en texto r√°pido (10 intentos principales), puntaje F1: 0.86</em> <br><br>  Sin embargo, al visualizar los resultados de texto r√°pido en la descomposici√≥n de t-SNE, los cl√∫steres de intenci√≥n se destacan mucho peor que para tf-idf.  La medida F1 aqu√≠ es 0.86 en lugar de 0.92. <br><br>  Realizamos un experimento: combinados tf-idf y vectores de texto r√°pido.  La calidad es absolutamente la misma que cuando se usa solo tf-idf.  Esto no es cierto para todas las tareas, hay problemas en los que tf-idf y fasttext combinados funcionan mejor que solo tf-idf, o donde fasttext funciona mejor que tf-idf.  Necesitas experimentar e intentarlo. <br><br>  Intentemos aumentar el n√∫mero de intenciones (recuerde que tenemos 170 de ellas).  A continuaci√≥n se presentan los cl√∫steres para las 30 principales intenciones en los vectores tf-idf. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2k/a3/q9/2ka3q9iuylym9_cqdizf4dbtbfu.png" width="400"></div><br>  <em>t-SNE en tf-idf (30 intenciones principales), puntaje F1 0, 85 (a 10 fue 0.92)</em> <br><br>  La calidad cae en 7 puntos, y ahora no vemos una estructura de cl√∫ster pronunciada. <br><br>  Veamos ejemplos de textos que comenzaron a confundirse, porque se agregaron m√°s intentos que se cruzan sem√°nticamente y en palabras. <br><br>  Por ejemplo: "Y si abre un dep√≥sito, ¬øcu√°les son los intereses?"  y "Y quiero abrir una contribuci√≥n al 7 por ciento".  Frases muy similares, pero estas son intenciones diferentes.  En el primer caso, una persona quiere saber las condiciones para los dep√≥sitos, y en el segundo caso, para abrir un dep√≥sito.  Para separar dichos textos en diferentes clases, necesitamos algo m√°s complejo: <strong>aprendizaje profundo</strong> . <br><br><h2>  Modelo de idioma </h2><br>  Queremos obtener un vector de texto y, en particular, un vector de una palabra, que depender√° del contexto de uso.  La forma est√°ndar de obtener dicho vector es <strong>usar incrustaciones del modelo de lenguaje</strong> . <br><br>  El modelo de lenguaje resuelve el problema del modelado de lenguaje.  ¬øY cu√°l es esta tarea?  Que haya una secuencia de palabras, por ejemplo: "Solo hablar√© en presencia de mi propia ...", y estamos tratando de predecir la siguiente palabra en la secuencia.  El modelo de lenguaje proporciona contexto para incrustaciones.  Habiendo obtenido incrustaciones contextuales y vectores para cada palabra, uno puede predecir la probabilidad de la siguiente palabra. <br><br>  Hay un vector de dimensi√≥n de diccionario, y a cada palabra se le asigna la probabilidad de ser el siguiente.  Nuevamente sabemos qu√© palabra era en realidad, consideramos un error y entrenamos el modelo. <br><br><img src="https://habrastorage.org/webt/1i/yd/f1/1iydf1xm3j97nwwuys6jmrq1ate.jpeg"><br><br>  Hay bastantes modelos de idiomas, ¬øhubo un auge el a√±o pasado?  y se han propuesto muchas arquitecturas diferentes.  Uno de ellos es <strong>ELMo</strong> . <br><br><h3>  ELMo </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La idea del modelo ELMo es</a> construir primero una incrustaci√≥n simb√≥lica de palabras para cada palabra en el texto, y luego aplicar una <strong>red LSTM</strong> para ellas de tal manera que se tengan en cuenta las incrustaciones que tengan en cuenta el contexto en el que se produce la palabra. <br><br>  Examinemos c√≥mo se obtiene la incrustaci√≥n simb√≥lica: dividimos la palabra en s√≠mbolos, aplicamos una capa de incrustaci√≥n para cada s√≠mbolo y obtenemos una matriz de incrustaci√≥n.  Cuando se trata solo de s√≠mbolos, la dimensi√≥n de dicha matriz es peque√±a.  Luego, se aplica una convoluci√≥n unidimensional a la matriz de incrustaci√≥n, como generalmente se hace en PNL, con una agrupaci√≥n m√°xima al final, se obtiene un vector.  Se aplica a este vector una llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red de autopistas de</a> dos capas, que calcula el <strong>vector general de una palabra</strong> . <br><br><img src="https://habrastorage.org/webt/in/zp/zw/inzpzwr-5-in2jiuszhjrkimczs.png"><br><br>  Adem√°s, el modelo construir√° alg√∫n tipo de hip√≥tesis de inclusi√≥n incluso para una palabra que no se encontr√≥ en el conjunto de entrenamiento. <br><br>  Despu√©s de haber recibido incrustaciones simb√≥licas para cada palabra, les aplicamos una red BiLSTM de dos capas. <br><br><img src="https://habrastorage.org/webt/vi/em/mb/viemmbyrjg0lboyuiq8bfbrgewy.png"><br><br>  Despu√©s de aplicar una red BiLSTM de dos capas, generalmente se toman los estados ocultos de la √∫ltima capa, y se cree que esto es incrustaci√≥n contextual.  Pero ELMo tiene dos caracter√≠sticas: <br><br><ul><li>  <strong>Conexi√≥n residual</strong> entre la entrada de la primera capa LSTM y su salida.  La entrada LSTM se agrega a la salida para evitar el problema de gradientes de desvanecimiento. </li><li>  Los autores de ELMo proponen combinar la incrustaci√≥n simb√≥lica para cada palabra, la salida de la primera capa LSTM y la salida de la segunda capa LSTM con algunos pesos seleccionados para cada tarea.  Esto es necesario para tener en cuenta las caracter√≠sticas de bajo nivel y las caracter√≠sticas de nivel superior que proporcionan la primera y segunda capa de LSTM. </li></ul><br>  En nuestro problema, utilizamos un promedio simple de estas tres incrustaciones y, por lo tanto, obtuvimos la incrustaci√≥n contextual para cada palabra. <br><br><img src="https://habrastorage.org/webt/8y/tl/pa/8ytlpa3lia0oxj461muadegcdyq.png"><br><br>  El modelo de lenguaje proporciona los siguientes beneficios: <br><br><ul><li>  El vector de una palabra depende del contexto en el que se usa la palabra.  Es decir, por ejemplo, para la palabra "lenguaje" en el significado de la parte del cuerpo y el t√©rmino ling√º√≠stico, obtenemos diferentes vectores. </li><li>  Como en el caso de word2vec y fasttext, hay muchos modelos entrenados, por ejemplo, del proyecto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepPavlov</a> .  Puede tomar el modelo terminado e intentar aplicarlo en su tarea. </li><li>  Ya no necesita pensar en c√≥mo promediar los vectores de palabras.  El modelo ELMo produce inmediatamente un vector de todo el texto. </li><li>  Puede volver a capacitar el modelo de idioma para su tarea, hay varias formas de hacerlo, por ejemplo, ULMFiT. </li></ul><br>  El √∫nico signo negativo sigue siendo que el <strong>modelo de lenguaje no garantiza</strong> que los textos que pertenecen a la misma clase, es decir, con un solo prop√≥sito, est√©n cerca en el espacio vectorial. <br><br><img src="https://habrastorage.org/webt/hv/09/qf/hv09qfkirx8mbcfl8rvbsd11aiu.png"><br><br>  En nuestro ejemplo de restaurante, los valores del coseno seg√∫n el modelo ELMo realmente se volvieron m√°s altos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m8/d0/ax/m8d0axjr1fysz33kaoi7ydg4kga.png" width="400"></div><br>  <em>t-SNE en ELMo (10 intenciones principales), puntaje F1 0.93 (0.92 por tf-idf)</em> <br><br>  Los grupos con 10 intenciones principales tambi√©n son m√°s pronunciados.  En la figura anterior, los 10 grupos son claramente visibles, mientras que la precisi√≥n ha aumentado ligeramente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ux/fx/ws/uxfxws0f9y1bf-zdpjynmle3xqk.png" width="400"></div><br>  <em>t-SNE en ELMo (30 intenciones principales) Puntaje F1 0.86 (0.85 por tf-idf)</em> <br><br>  Para las 30 principales intenciones, la estructura del cl√∫ster a√∫n se conserva, y tambi√©n hay un aumento de la calidad en un punto. <br><br>  Pero en ese modelo no hay garant√≠a de que las propuestas "Y si abre un dep√≥sito, ¬øcu√°les son los intereses en ellas?"  y "Y quiero abrir una contribuci√≥n al 7 por ciento" estar√°n lejos el uno del otro, aunque se encuentran en diferentes clases.  Con ELMo, simplemente aprendemos el modelo de lenguaje, y si los textos son sem√°nticamente similares, entonces estar√°n cerca.  <strong>ELMo no sabe nada acerca de nuestras clases</strong> , pero puede reunir vectores de texto de la misma intenci√≥n en el espacio utilizando etiquetas de clase. <br><br><h3>  Red siamesa </h3><br>  Tome su arquitectura de red neuronal favorita para la vectorizaci√≥n de texto y dos ejemplos de intenciones.  Para cada uno de los ejemplos obtenemos incrustaciones, y luego calculamos la distancia del coseno entre ellos. <br><br><img src="https://habrastorage.org/webt/ah/sw/ha/ahswhaivfdofbehikcsi7qvhntk.jpeg"><br><br>  La distancia del coseno es igual a uno menos la proximidad del coseno que conocimos anteriormente. <br><br>  Este enfoque se llama la <strong>red siamesa</strong> . <br><br>  Queremos que los textos de la misma clase, por ejemplo, "hacer una transferencia" y "tirar dinero", est√©n cerca en el espacio.  Es decir, la distancia del coseno entre sus vectores debe ser lo m√°s peque√±a posible, idealmente cero.  Y los textos relacionados con diferentes intenciones deben estar lo m√°s separados posible. <br><br>  Pero en la pr√°ctica, este m√©todo de entrenamiento no funciona tan bien, porque los objetos de diferentes clases no se alejan lo suficiente entre s√≠.  La funci√≥n de p√©rdida llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"p√©rdida de triplete"</a> funciona mucho mejor.  Utiliza triples de objetos llamados trillizos. <br><br>  La ilustraci√≥n muestra un triplete: un objeto de anclaje en un c√≠rculo azul, un objeto positivo en verde y un objeto negativo en un c√≠rculo rojo.  El objeto negativo y el ancla est√°n en diferentes clases, y el positivo y el ancla est√°n en uno. <br><br><img src="https://habrastorage.org/webt/qw/38/lz/qw38lz9wpgcphm8w2ic55aqbhea.png"><br><br>  Queremos asegurarnos de que, despu√©s del entrenamiento, el objeto positivo est√© m√°s cerca del ancla que el negativo.  Para hacer esto, consideramos la distancia del coseno entre los pares de objetos e ingresamos el hiperpar√°metro - "margen" - la distancia que esperamos que est√© entre los objetos positivos y negativos. <br><br><img src="https://habrastorage.org/webt/lv/ib/b8/lvibb8qcivpp0evrqxgnyezlo0a.png"><br><br>  La funci√≥n de p√©rdida se ve as√≠: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&amp;#xA0;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo><mo>.</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>$</mo></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="59.526ex" height="2.66ex" viewBox="0 -832 25629.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-74" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-72" x="361" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-69" x="813" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-70" x="1158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6C" x="1662" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-65" x="1960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-74" x="2427" y="0"></use><g transform="translate(2788,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6C" x="353" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6F" x="3349" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-73" x="3835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-73" x="4304" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-3D" x="5051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6D" x="6358" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-61" x="7236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-78" x="7766" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-5B" x="8338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-30" x="8617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2C" x="9117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6D" x="9562" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-61" x="10441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-72" x="10970" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-67" x="11422" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-65" x="11902" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-6E" x="12369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2B" x="13192" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-64" x="14192" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-69" x="14716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-73" x="15061" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-74" x="15531" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-28" x="15892" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-41" x="16282" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2C" x="17032" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-50" x="17477" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-29" x="18229" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2212" x="18841" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-64" x="19841" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-69" x="20365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-73" x="20710" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-74" x="21180" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-28" x="21541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-41" x="21931" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2C" x="22681" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMATHI-4E" x="23127" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-29" x="24015" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-5D" x="24405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-2E" x="24683" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/oleg-bunin/blog/455652/&amp;usg=ALkJrhgd0OgO6SuD6z78eLJN9gPTiKB5vA#MJMAIN-24" x="25128" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>t</mi><mi>r</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>t</mi><msub><mtext>&nbsp;</mtext><mi>l</mi></msub><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mo>+</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>N</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>.</mo><mrow class="MJX-TeXAtom-ORD"><mo>$</mo></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> triplet \ _loss = \ max [0, margen + dist (A, P) - dist (A, N)]. $ </script></p><br>  En otras palabras, durante el entrenamiento, logramos que el objeto positivo est√© m√°s cerca del ancla que el negativo, al menos margen.  Si la funci√≥n de p√©rdida es cero, entonces funciona, y terminamos el entrenamiento, de lo contrario, continuamos minimizando la funci√≥n objetivo. <br><br>  Despu√©s de haber entrenado el modelo, todav√≠a no obtenemos un clasificador, es solo un m√©todo para obtener tales incrustaciones que los objetos que se encuentran en la misma intenci√≥n probablemente tengan vectores cercanos. <br><br>  Cuando obtuvimos el modelo, podemos usar un m√©todo de clasificaci√≥n diferente adem√°s de las incrustaciones.  <strong>KNN es una</strong> buena opci√≥n, ya que hemos logrado que las incrustaciones tengan una estructura de cl√∫ster distinta. <br><br>  Recuerde c√≥mo funciona kNN para los textos: tome un elemento del texto, incr√∫stelo, traduzca al espacio vectorial y luego vea qui√©n es su vecino.  Entre los vecinos, consideramos la clase m√°s frecuente y concluimos que el nuevo objeto pertenece a esta clase. <br><br>  La dimensi√≥n de las incrustaciones que utilizamos es de 300, y en la muestra de entrenamiento hay alrededor de 500,000 objetos.  Los m√©todos est√°ndar para encontrar a los vecinos m√°s cercanos no nos convienen en t√©rminos de rendimiento.  Utilizamos el m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HNSW</a> : <strong>Hierarchical Navigable Small World</strong> . <br><br>  Navigable Small World es un gr√°fico conectado en el que hay pocos bordes entre v√©rtices que est√°n a una gran distancia y muchos bordes entre v√©rtices cercanos.  En nuestro caso, la longitud del borde estar√° determinada por la distancia del coseno, es decir          ,        ,      . <br><br>      ,    Hierarchical.        ,  ,       ,    .            . <br><br>       , ,         ,     ,      . <br><br>     ,     ,       , ,  ,       .   ,    ,         ,     <strong>  ‚Äî  0,95-0,99</strong> ,    . <br><br>  ,       ,     ,          , <strong>   </strong> .              . <br><br>  ,      .    ,      .          . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/db/2v/yx/db2vyx8eoubfaytjge3imauvgra.png" width="400"></div><br> <em>t-SNE  siamese (-10 ), F1 score 0,95 (0,93  ELMo)</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rz/ib/op/rzibopijmd5mqginmihafxfiivy.png" width="400"></div><br> <em>t-SNE  siamese (-30 ), F1 score 0,87 (0,86  ELMo)</em> <br><br>  10             ELMo,  30 ‚Äî  ,       . <br><br><h2>  Resumen </h2><br> <b>      ,     </b> , , 2-5,          .    ,     ,        ,     20-30  .      ,   . <br><br> <b>  ,      ,     ,       tf-idf</b> .    ,      ,    ,       . <br><br> <b>   ,    word2vec  fasttext.</b>   ,   ,         .        ,      ,       ,     . <br><br>   ,  ,   ELMo.       , , ,      ,       ,      . <b>   ELMo,        </b> ,           . <br><br>             ,    -  .       .      ,              .  ,   ,           .  ,     ,     ..    ,      . <br><div class="scrollable-table"><table><tbody><tr><td> F1-score </td><td> ~2-5  <br>   </td><td> ~10  <br>   </td><td> ~30  <br>   </td></tr><tr><td> ,  </td><td>  MVP </td><td>    </td><td>    </td></tr><tr><td> ML + tf-idf </td><td>  </td><td> 0,92 </td><td> 0,85 </td></tr><tr><td> ML + fasttext </td><td> ? </td><td> 0,86 </td><td> 0,82 </td></tr><tr><td> ELMo </td><td> ?? </td><td> 0,93 </td><td> 0,86 </td></tr><tr><td> siamese </td><td> ??? </td><td> 0,95 </td><td> 0,87 </td></tr></tbody></table></div> <b> :</b> <br><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rusvectores.org/ru/models</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">www.mihaileric.com/posts/deep-contextualized-word-representations-elmo</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">omoindrot.github.io/triplet-loss</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">habr.com/ru/company/mailru/blog/338360</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://jalammar.github.io/illustrated-bert</a> </li></ul><br><blockquote>    ‚Äî ¬´Deep Learning vs common sense¬ª ‚Äî       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">UseData Conf</a> .  ,    -   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>  18        ,      ,          . <br><br>        ,        ,    ,         ,   16   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">UseData Conf</a> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455652/">https://habr.com/ru/post/455652/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455642/index.html">La arquitectura del servicio de cola de mensajes distribuidos en Yandex.Cloud</a></li>
<li><a href="../455644/index.html">Usamos datos en la pr√°ctica.</a></li>
<li><a href="../455646/index.html">Semana de la seguridad 24: puertas traseras de f√°brica en tel√©fonos inteligentes Android</a></li>
<li><a href="../455648/index.html">Life Cycle ML</a></li>
<li><a href="../455650/index.html">C√≥mo entrenamos una red neuronal para clasificar tornillos</a></li>
<li><a href="../455658/index.html">El legendario Intel Core i7-2600K: prueba de Sandy Bridge en 2019 (parte 3)</a></li>
<li><a href="../455662/index.html">Gran pantalla mec√°nica con mecanismo de leva como decodificador.</a></li>
<li><a href="../455666/index.html">Creaci√≥n de ventas salientes en una empresa de servicios de TI</a></li>
<li><a href="../455668/index.html">Escribimos bajo FPGA sin HDL. Comparaci√≥n de herramientas de desarrollo de alto nivel.</a></li>
<li><a href="../455670/index.html">C√≥mo las impresoras 3D imprimen huesos, vasos sangu√≠neos y √≥rganos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>