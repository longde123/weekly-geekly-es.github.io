<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòù üê≥ üë∂üèæ Aten√ß√£o para manequins e implementa√ß√£o em Keras üë®üèª‚Äçüé§ üöø üôéüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sobre artigos sobre intelig√™ncia artificial em russo 
 Apesar do mecanismo de aten√ß√£o estar descrito na literatura inglesa, ainda n√£o vi uma descri√ß√£o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aten√ß√£o para manequins e implementa√ß√£o em Keras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2>  Sobre artigos sobre intelig√™ncia artificial em russo </h2><br>  Apesar do mecanismo de aten√ß√£o estar descrito na literatura inglesa, ainda n√£o vi uma descri√ß√£o decente dessa tecnologia no setor de l√≠ngua russa.  Existem muitos artigos sobre Intelig√™ncia Artificial (IA) em nosso idioma.  No entanto, os artigos encontrados revelam apenas os modelos mais simples de IA, por exemplo, redes de convolu√ß√£o, redes generativas.  No entanto, de acordo com os mais recentes desenvolvimentos de ponta no campo da IA, existem muito poucos artigos no setor de l√≠ngua russa. <br><br><a name="habracut"></a>  A falta de artigos em russo sobre os √∫ltimos desenvolvimentos se tornou um problema para mim quando entrei no t√≥pico, estudei o estado atual das coisas no campo da IA.  Eu sei ingl√™s bem, leio artigos em ingl√™s sobre t√≥picos de IA.  No entanto, quando um novo conceito ou um novo princ√≠pio de IA sai, seu entendimento em uma l√≠ngua estrangeira √© doloroso e longo.  Saber ingl√™s, penetrar em um objeto n√£o-nativo em um objeto complexo ainda vale muito mais tempo e esfor√ßo.  Depois de ler a descri√ß√£o, voc√™ se pergunta: quantos por cento voc√™ entende?  Se houvesse um artigo em russo, eu entenderia 100% ap√≥s a primeira leitura.  Isso aconteceu com redes generativas, para as quais h√° uma excelente s√©rie de artigos: depois de ler tudo ficou claro.  Mas no mundo das redes, existem muitas abordagens descritas apenas em ingl√™s e que precisavam ser tratadas por dias. <br><br>  Escreverei periodicamente artigos em meu idioma nativo, trazendo conhecimento para o nosso campo de idioma.  Como voc√™ sabe, a melhor maneira de entender um t√≥pico √© explic√°-lo a algu√©m.  Ent√£o, quem mais, al√©m de eu, deveria come√ßar uma s√©rie de artigos sobre a IA de arquitetura mais moderna, complexa e avan√ßada.  No final do artigo, eu mesmo entenderei uma abordagem 100%, e ser√° √∫til para algu√©m que l√™ e melhora sua compreens√£o (a prop√≥sito, eu amo Gesser, mas melhor ** Blanche de bruxelles **). <br><br>  Quando voc√™ entende o assunto, existem 4 n√≠veis de entendimento: <br><br><ol><li>  voc√™ entende o princ√≠pio e as entradas e sa√≠das do algoritmo / n√≠vel </li><li>  voc√™ entende as sa√≠das de coleta e, em termos gerais, como funciona </li><li>  voc√™ entende tudo o que foi dito acima, bem como o dispositivo de cada n√≠vel de rede (por exemplo, no modelo VAE, voc√™ entendeu o princ√≠pio e tamb√©m a ess√™ncia do truque de reparameteriza√ß√£o) </li><li>  Entendi tudo, incluindo todos os n√≠veis, tamb√©m entendi por que tudo aprende e, ao mesmo tempo, sou capaz de selecionar hiper par√¢metros para minha tarefa, em vez de copiar e colar solu√ß√µes prontas. </li></ol><br>  Para novas arquiteturas, a transi√ß√£o do n√≠vel 1 para o n√≠vel 4 √© muitas vezes dif√≠cil: os autores enfatizam que est√£o mais pr√≥ximos descrevendo superficialmente v√°rios detalhes importantes (eles os entenderam?).  Ou seu c√©rebro n√£o cont√©m nenhuma constru√ß√£o, portanto, mesmo depois de ler a descri√ß√£o, ele n√£o decifrou e n√£o se transformou em habilidades.  Isso acontece se durante os anos de estudante voc√™ dormiu na mesma li√ß√£o de matem√°tica, depois de uma festa noturna, onde deu o tapete certo.  aparelho.  E aqui precisamos de artigos em nossa l√≠ngua nativa que revelem as nuances e sutilezas de cada opera√ß√£o. <br><br><h2>  Aten√ß√£o conceito e aplica√ß√£o </h2><br>  O exposto acima √© um cen√°rio de n√≠veis de entendimento.  Para analisar Aten√ß√£o, vamos come√ßar no n√≠vel um.  Antes de descrever as entradas e sa√≠das, analisaremos a ess√™ncia: em que conceitos b√°sicos, compreens√≠veis at√© para uma crian√ßa, esse conceito se baseia.  No artigo, usaremos o termo em ingl√™s Aten√ß√£o, pois, neste formul√°rio, tamb√©m √© uma chamada para a fun√ß√£o de biblioteca Keras (n√£o √© implementada diretamente nela, √© necess√°rio um m√≥dulo adicional, mas mais sobre isso abaixo).  Para ler mais, voc√™ deve entender as bibliotecas Keras e python, porque o c√≥digo-fonte ser√° fornecido. <br><br><img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right">  Aten√ß√£o traduz do ingl√™s como "aten√ß√£o".  Esse termo descreve corretamente a ess√™ncia da abordagem: se voc√™ √© motorista e o general da pol√≠cia de tr√¢nsito √© mostrado na foto, voc√™ atribui import√¢ncia a ela de forma intuitiva, independentemente do contexto da foto.  √â prov√°vel que voc√™ d√™ uma olhada mais de perto no general.  Voc√™ for√ßa os olhos, olha atentamente as tiras dos ombros: quantas estrelas ele tem l√° especificamente.  Se o general n√£o for muito alto, ignore-o.  Caso contr√°rio, considere-o como um fator-chave na tomada de decis√µes.  √â assim que nosso c√©rebro funciona.  Na cultura russa, fomos treinados por gera√ß√µes para prestar aten√ß√£o a altos escal√µes; nosso c√©rebro automaticamente atribui alta prioridade a esses objetos. <br><br>  Aten√ß√£o √© uma maneira de informar √† rede o que voc√™ deve prestar mais aten√ß√£o, ou seja, relatar a probabilidade de um resultado espec√≠fico, dependendo do estado dos neur√¥nios e dos dados de entrada.  A camada de aten√ß√£o implementada no pr√≥prio Keras identifica fatores com base no conjunto de treinamento, cuja aten√ß√£o reduz o erro de rede.  A identifica√ß√£o de fatores importantes √© realizada atrav√©s do m√©todo de propaga√ß√£o reversa de erros, semelhante √† maneira como isso √© feito para redes de convolu√ß√£o. <br><br>  No treinamento, a Aten√ß√£o demonstra sua natureza probabil√≠stica.  O mecanismo em si forma uma matriz de escalas de import√¢ncia.  Se n√£o tiv√©ssemos treinado a Aten√ß√£o, poder√≠amos ter dado import√¢ncia, por exemplo, empiricamente (o general √© mais importante que o alferes).  Mas quando treinamos uma rede em dados, a import√¢ncia se torna uma fun√ß√£o da probabilidade de um resultado espec√≠fico, dependendo dos dados recebidos na entrada da rede.  Por exemplo, se encontr√°ssemos um morador em geral na R√∫ssia czarista, a probabilidade de conseguir manoplas seria alta.  Tendo verificado isso, seria poss√≠vel atrav√©s de v√°rias reuni√µes pessoais, coletando estat√≠sticas.  Depois disso, nosso c√©rebro colocar√° o peso apropriado no fato de conhecer esse assunto e colocar√° marcadores nas tiras e listras nos ombros.  Deve-se notar que o marcador definido n√£o √© prov√°vel: agora a reuni√£o do general acarretar√° consequ√™ncias completamente diferentes para voc√™ do que ent√£o; al√©m disso, o peso pode ser mais do que um.  Mas, o peso pode ser reduzido √† probabilidade normalizando-o. <br><br>  A natureza probabil√≠stica do mecanismo Aten√ß√£o na aprendizagem se manifesta nas tarefas de tradu√ß√£o autom√°tica.  Por exemplo, informe √† rede que, ao traduzir do russo para o ingl√™s, a palavra Amor √© traduzida em 90% dos casos como Amor, em 9% dos casos como Sexo, em 1% dos casos.  A rede marca imediatamente muitas op√ß√µes, mostrando a melhor qualidade de treinamento.  Ao traduzir, dizemos √† rede: "ao traduzir a palavra amor, preste aten√ß√£o especial √† palavra inglesa Amor, veja tamb√©m se ainda pode ser Sexo". <br><br>  A abordagem Aten√ß√£o √© aplicada ao trabalho com texto, bem como a s√©ries de sons e temporais.  Para o processamento de texto, redes neurais recorrentes (RNN, LSTM, GRU) s√£o amplamente utilizadas.  A aten√ß√£o pode complement√°-los ou substitu√≠-los, movendo a rede para arquiteturas mais simples e r√°pidas. <br><br>  Um dos usos mais famosos do Attention √© us√°-lo para abandonar a rede de recorr√™ncias e mudar para um modelo totalmente conectado.  As redes recorrentes t√™m uma s√©rie de defici√™ncias: a incapacidade de fornecer treinamento sobre a GPU, reciclagem r√°pida.  Usando o mecanismo Aten√ß√£o, podemos construir uma rede capaz de aprender seq√º√™ncias com base em uma rede totalmente conectada, trein√°-la na GPU, usar droput. <br><br>  A aten√ß√£o √© amplamente usada para melhorar o desempenho das redes de recorr√™ncia, por exemplo, no campo da tradu√ß√£o de um idioma para outro.  Ao usar a abordagem de codifica√ß√£o / decodifica√ß√£o, que √© frequentemente usada na IA moderna (por exemplo, auto-codificadores variacionais).  Quando uma camada de aten√ß√£o √© adicionada entre o codificador e o decodificador, o resultado da opera√ß√£o de rede melhora visivelmente. <br><br>  Neste artigo, n√£o cito arquiteturas de rede espec√≠ficas usando Aten√ß√£o; este ser√° o assunto de um trabalho separado.  Uma lista de todos os usos poss√≠veis da aten√ß√£o vale um artigo separado. <br><br><h2>  Implementando Aten√ß√£o em Keras Out of the Box </h2><br>  Quando voc√™ entende que tipo de abordagem, √© muito √∫til aprender o princ√≠pio b√°sico.  Mas, muitas vezes, o entendimento completo ocorre apenas olhando para uma implementa√ß√£o t√©cnica.  Voc√™ v√™ os fluxos de dados que comp√µem a fun√ß√£o da opera√ß√£o, fica claro o que exatamente √© calculado.  Mas primeiro voc√™ precisa execut√°-lo e escrever "Aten√ß√£o, ol√°, palavra". <br><br>  Atualmente, a aten√ß√£o n√£o √© implementada no pr√≥prio Keras.  Mas j√° existem implementa√ß√µes de terceiros, como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kera-aten√ß√£o,</a> que pode ser instalado com o github.  Ent√£o seu c√≥digo se tornar√° extremamente simples: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> attention_keras.layers.attention <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AttentionLayer attn_layer = AttentionLayer(name=<span class="hljs-string"><span class="hljs-string">'attention_layer'</span></span>) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</code> </pre> <br>  Esta implementa√ß√£o suporta a fun√ß√£o de visualiza√ß√£o da escala de aten√ß√£o.  Ap√≥s treinar a Aten√ß√£o, voc√™ pode obter uma sinaliza√ß√£o matricial, que, de acordo com a rede, √© especialmente importante sobre esse tipo (foto do github na p√°gina da biblioteca de aten√ß√£o-keras). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br>  Basicamente, voc√™ n√£o precisa de mais nada: inclua esse c√≥digo na sua rede como um dos n√≠veis e aproveite para aprender sua rede.  Qualquer rede, qualquer algoritmo √© projetado nos primeiros est√°gios em um n√≠vel conceitual (como o banco de dados, a prop√≥sito), ap√≥s o qual a implementa√ß√£o √© especificada em uma representa√ß√£o l√≥gica e f√≠sica antes da implementa√ß√£o.  Esse m√©todo de design ainda n√£o foi desenvolvido para redes neurais (ah, sim, esse ser√° o t√≥pico do meu pr√≥ximo artigo).  Voc√™ n√£o entende como as camadas de convolu√ß√£o funcionam por dentro?  O princ√≠pio √© descrito, voc√™ os usa. <br><br><h2>  Keras implementa√ß√£o de Aten√ß√£o baixa </h2><br>  Para finalmente entender o t√≥pico, analisaremos detalhadamente abaixo a implementa√ß√£o do Attention under the hood.  O conceito √© bom, mas como exatamente funciona e por que o resultado √© obtido exatamente como indicado? <br><br>  A implementa√ß√£o mais simples do mecanismo de aten√ß√£o no Keras leva apenas 3 linhas: <br><br><pre> <code class="python hljs">inputs = Input(shape=(input_dims,)) attention_probs = Dense(input_dims, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_probs'</span></span>)(inputs) attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number"><span class="hljs-number">32</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_mul'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'mul'</span></span></code> </pre><br>  Nesse caso, a camada de entrada √© declarada na primeira linha e, em seguida, vem uma camada totalmente conectada com a fun√ß√£o de ativa√ß√£o softmax com o n√∫mero de neur√¥nios igual ao n√∫mero de elementos na primeira camada.  A terceira camada multiplica o resultado da camada totalmente conectada pelo elemento de dados de entrada por elemento. <br><br>  Abaixo est√° toda a classe Attention, que implementa um mecanismo de auto-aten√ß√£o um pouco mais complexo, que pode ser usado como um n√≠vel completo no modelo; a classe herda a classe da camada Keras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Attention class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim</span></span></code> </pre><br>  Aqui vemos aproximadamente a mesma coisa que foi implementada acima atrav√©s de uma camada Keras totalmente conectada, realizada apenas atrav√©s de uma l√≥gica mais profunda em um n√≠vel inferior.  Um n√≠vel param√©trico (self.W) √© criado na fun√ß√£o, que √© multiplicada escalarmente (K.dot) pelo vetor de entrada.  A l√≥gica com fio dessa variante √© um pouco mais complicada: deslocamento (se o par√¢metro de vi√©s √© divulgado), tangente hiperb√≥lica, exposi√ß√£o, m√°scara (se especificada), normaliza√ß√£o s√£o aplicadas ao vetor de entrada vezes self.W, ent√£o o vetor de entrada √© ponderado novamente por o resultado obtido.  N√£o tenho descri√ß√£o da l√≥gica estabelecida neste exemplo; reproduzo as opera√ß√µes de leitura do c√≥digo.  A prop√≥sito, escreva nos coment√°rios se reconhecer algum tipo de fun√ß√£o matem√°tica de alto n√≠vel nessa l√≥gica. <br><br>  A classe possui um par√¢metro "vi√©s", isto √©,  preconceito.  Se o par√¢metro estiver ativado, depois de aplicar a camada Densa, o vetor final ser√° adicionado ao vetor dos par√¢metros da camada "self.b", o que permitir√° n√£o apenas determinar os "pesos" para a nossa fun√ß√£o de aten√ß√£o, mas tamb√©m mudar o n√≠vel de aten√ß√£o por um n√∫mero.  Exemplo de vida: temos medo de fantasmas, mas nunca os encontramos.  Assim, fazemos uma corre√ß√£o para o medo -100 pontos.  Ou seja, somente se o medo diminuir de 100 pontos, tomaremos decis√µes sobre a prote√ß√£o contra fantasmas, ligando para uma ag√™ncia de captura de fantasmas, comprando dispositivos repelentes etc. <br><br><h2>  Conclus√£o </h2><br>  O mecanismo de aten√ß√£o possui varia√ß√µes.  A op√ß√£o mais simples de aten√ß√£o implementada na classe acima √© chamada de auto-aten√ß√£o.  A aten√ß√£o pessoal √© um mecanismo projetado para processar dados seq√ºenciais, levando em considera√ß√£o o contexto de cada registro de data e hora.  √â usado com mais frequ√™ncia para trabalhar com informa√ß√µes de texto.  A implementa√ß√£o da aten√ß√£o pessoal pode ser retirada da caixa importando a biblioteca de aten√ß√£o pessoal do keras.  Existem outras varia√ß√µes de aten√ß√£o.  Estudando materiais em ingl√™s, foi poss√≠vel contar mais de 5 varia√ß√µes. <br><br>  Ao escrever este artigo relativamente curto, estudei mais de 10 artigos em ingl√™s.  Obviamente, n√£o consegui baixar todos os dados de todos esses artigos em cinco p√°ginas; fiz apenas um aperto para criar um "guia para manequins".  Para entender todas as nuances do mecanismo Aten√ß√£o, voc√™ precisa de um livro das p√°ginas 150-200.  Eu realmente espero que eu tenha sido capaz de revelar a ess√™ncia b√°sica desse mecanismo, para que aqueles que est√£o come√ßando a entender o aprendizado de m√°quina entendam como tudo isso funciona. <br><br><h2>  Fontes </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mecanismo de aten√ß√£o em redes neurais com Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aten√ß√£o em redes profundas com Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sequ√™ncia-sequ√™ncia-baseada na aten√ß√£o em Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Classifica√ß√£o de texto usando o mecanismo de aten√ß√£o em Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aten√ß√£o Pervasiva: Redes Neurais Convolucionais 2D para Previs√£o de Sequ√™ncia a Sequ√™ncia</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Como implementar a camada de aten√ß√£o no Keras?</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aten√ß√£o?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aten√ß√£o!</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tradu√ß√£o autom√°tica neural com aten√ß√£o</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt458992/">https://habr.com/ru/post/pt458992/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt458982/index.html">Nginx Recipes: Convertendo de HTML e URLs para PDF e PS</a></li>
<li><a href="../pt458984/index.html">Como criar o primeiro aplicativo para negocia√ß√£o na bolsa: 3 etapas iniciais</a></li>
<li><a href="../pt458986/index.html">Receitas do PostgreSQL: Convertendo de HTML e URLs para PDF e PS</a></li>
<li><a href="../pt458988/index.html">Texturiza√ß√£o ou o que voc√™ precisa saber para se tornar um Artista de Superf√≠cie. Parte 4. Modelos, normais e varredura</a></li>
<li><a href="../pt458990/index.html">Pare de zeloso com coment√°rios no c√≥digo</a></li>
<li><a href="../pt458994/index.html">Raspberry Pi + CentOS = Ponto de acesso Wi-Fi (ou Raspberry Router em um Red Hat)</a></li>
<li><a href="../pt458996/index.html">Usu√°rio Inyerface - como n√£o atormentar o usu√°rio</a></li>
<li><a href="../pt459000/index.html">Como eu tentei melhorar o Halo 2, mas quase arruinei</a></li>
<li><a href="../pt459002/index.html">Como configurar o HTTPS - SSL Configuration Generator ajudar√°</a></li>
<li><a href="../pt459004/index.html">Algoritmo criptogr√°fico Grasshopper: praticamente o complexo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>