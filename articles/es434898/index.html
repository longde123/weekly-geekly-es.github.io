<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ©Ô∏è üë©üèΩ‚Äçü§ù‚Äçüë®üèæ üêñ Grasp2Vec: aprendizaje de representaci√≥n de objetos de autoaprendizaje üçÖ üôèüèΩ üî¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las personas desde una edad sorprendentemente joven ya pueden reconocer sus objetos favoritos y recogerlos, a pesar de que no se les ense√±a espec√≠fica...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grasp2Vec: aprendizaje de representaci√≥n de objetos de autoaprendizaje</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434898/"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br>  Las personas desde una edad sorprendentemente joven ya pueden reconocer sus objetos favoritos y recogerlos, a pesar de que no se les ense√±a espec√≠ficamente.  Seg√∫n los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estudios sobre el</a> desarrollo de las habilidades cognitivas, la posibilidad de interactuar con objetos del mundo que nos rodea desempe√±a un papel fundamental en el desarrollo de habilidades tales como la detecci√≥n y manipulaci√≥n de objetos, por ejemplo, la captura dirigida.  Al interactuar con el mundo exterior, las personas pueden aprender corrigiendo sus propios errores: sabemos lo que hemos hecho y aprendemos de los resultados.  En rob√≥tica, este tipo de entrenamiento con autocorrecci√≥n de errores se estudia activamente, ya que permite que los sistemas rob√≥ticos aprendan sin una gran cantidad de datos de entrenamiento o ajuste manual. <br><br>  En Google, inspirados en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">concepto de persistencia de objetos</a> , ofrecemos el sistema <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Grasp2Vec</a> , un algoritmo simple pero efectivo para construir la representaci√≥n de objetos.  Grasp2Vec se basa en una comprensi√≥n intuitiva de que un intento de levantar cualquier objeto nos dar√° alguna informaci√≥n: si el robot agarra el objeto y lo levanta, entonces el objeto debe estar en este lugar antes de ser capturado.  Adem√°s, el robot sabe que si el objeto capturado est√° en su captura, significa que el objeto ya no est√° en el lugar donde estaba.  Usando esta forma de autoaprendizaje, el robot puede aprender a reconocer un objeto debido al cambio visual en la escena despu√©s de ser capturado. <br><a name="habracut"></a><br>  Basado en nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">colaboraci√≥n con X Robotics</a> , donde varios robots fueron entrenados simult√°neamente para capturar objetos dom√©sticos usando solo una c√°mara como fuente de datos de entrada, utilizamos la captura rob√≥tica para capturar objetos "inadvertidamente", y esta experiencia nos permite tener una idea rica del objeto.  Esta idea ya se puede utilizar para adquirir la capacidad de "captura intencional", cuando el brazo del robot puede levantar objetos a pedido. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Crear una funci√≥n de recompensa perceptual </h2><br>  En una plataforma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprendizaje de refuerzo, el</a> √©xito de una tarea se mide a trav√©s de una funci√≥n de recompensa.  Al maximizar las recompensas, los robots aprenden varias habilidades de captura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">desde cero</a> .  Crear una funci√≥n de recompensa es f√°cil cuando el √©xito se puede medir con lecturas de sensores simples.  Un ejemplo simple es un bot√≥n que transfiere una recompensa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">directamente a la entrada de un robot</a> haciendo clic en √©l. <br><br>  Sin embargo, crear una funci√≥n de recompensa es mucho m√°s complicado cuando el criterio para el √©xito depende de una comprensi√≥n perceptiva de la tarea.  Considere el problema de captura en un ejemplo en el que el robot recibe una imagen del objeto deseado contenido en la captura.  Despu√©s de que el robot intenta capturar el objeto, examina el contenido de la captura.  La funci√≥n de recompensa para esta tarea depende de la respuesta a la pregunta de reconocimiento de patrones: ¬øcoinciden los objetos? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>A la izquierda, la empu√±adura sujeta el pincel, y varios objetos son visibles en el fondo (una copa amarilla, un bloque de pl√°stico azul).</i>  <i>A la derecha, el agarre sostiene la copa y el pincel est√° en el fondo.</i>  <i>Si la imagen de la izquierda representa el resultado deseado, una buena funci√≥n de recompensa ser√≠a "comprender" que estas dos fotos corresponden a dos objetos diferentes.</i> <br><br>  Para resolver el problema de reconocimiento, necesitamos un sistema de percepci√≥n que extraiga conceptos significativos de objetos de im√°genes no estructuradas (no firmadas por personas) y aprenda a visualizar objetos sin un maestro.  Esencialmente, los algoritmos de aprendizaje sin maestros funcionan creando suposiciones estructurales sobre los datos.  A menudo se supone que las im√°genes se pueden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">comprimir en un espacio con menos dimensiones</a> , y los cuadros de video se pueden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">predecir a partir de los anteriores</a> .  Sin embargo, sin suposiciones adicionales sobre el contenido de los datos, esto generalmente no es suficiente para aprender de representaciones no relacionadas de objetos. <br><br>  ¬øQu√© pasa si usamos un robot para separar f√≠sicamente los objetos durante la recopilaci√≥n de datos?  La rob√≥tica ofrece una excelente oportunidad para aprender a representar objetos, ya que los robots pueden manipularlos, lo que dar√° los factores de variaci√≥n necesarios.  Nuestro m√©todo se basa en la idea de que capturar un objeto lo elimina de la escena.  El resultado es 1) una imagen de la escena antes de la captura, 2) una imagen de la escena despu√©s de la captura y 3) una vista separada del objeto capturado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>Izquierda: objetos para capturar.</i>  <i>En el centro, despu√©s de la captura.</i>  <i>A la derecha est√° el objeto capturado.</i> <br><br>  Si consideramos una funci√≥n integrada que extrae un "conjunto de objetos" de las im√°genes, deber√≠a preservar la siguiente relaci√≥n de resta: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>objetos antes de la captura - objetos despu√©s de la captura = objeto capturado</i> <br><br>  Logramos esta igualdad con una arquitectura convolucional y un algoritmo de aprendizaje m√©trico simple.  Durante el entrenamiento, la arquitectura que se muestra a continuaci√≥n incorpora im√°genes antes y despu√©s de la captura en un denso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mapa de propiedades espaciales</a> .  Estos mapas se convierten en vectores a trav√©s de una uni√≥n promediada, y la diferencia entre los vectores "antes de la captura" y "despu√©s de la captura" representa un conjunto de objetos.  Este vector y la representaci√≥n correspondiente del vector de este objeto percibido se equiparan mediante la funci√≥n de N pares. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br>  Despu√©s del entrenamiento, nuestro modelo tiene naturalmente dos propiedades √∫tiles. <br><br><h2>  1. Similitud de objetos. </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El coeficiente coseno de la</a> distancia entre las incrustaciones de vectores nos permite comparar objetos y determinar si son id√©nticos.  Esto se puede usar para implementar la funci√≥n de recompensa para el aprendizaje reforzado, y permite a los robots aprender a capturar con ejemplos sin marcar datos por humanos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2. Encontrar objetivos </h2><br>  Podemos combinar mapas espaciales de la escena y la incrustaci√≥n de objetos para localizar el "objeto deseado" en el espacio de la imagen.  Llevando a cabo la multiplicaci√≥n por elementos de los mapas de caracter√≠sticas espaciales y la correspondencia vectorial del objeto deseado, podemos encontrar todos los p√≠xeles en el mapa espacial que corresponden al objeto objetivo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>Uso de incrustaciones Grasp2Vec para localizar objetos en la escena.</i>  <i>Arriba a la izquierda hay objetos en la cesta.</i>  <i>Abajo a la izquierda: el objeto que se desea capturar.</i>  <i>El producto escalar del vector del objeto de destino y las caracter√≠sticas espaciales de la imagen nos dan un "mapa de activaci√≥n" por p√≠xel (arriba a la derecha) de la similitud de una secci√≥n dada de la imagen con el objetivo.</i>  <i>Este mapa se puede usar para acercarse al objetivo.</i> <br><br>  Nuestro m√©todo tambi√©n funciona cuando varios objetos corresponden al objetivo, o incluso cuando el objetivo consta de varios objetos (el promedio de dos vectores).  Por ejemplo, en este escenario, el robot identifica varios bloques naranjas en la escena. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>El "mapa de calor" resultante puede usarse para planificar el acercamiento del robot a los objetos objetivo.</i>  <i>Combinamos la localizaci√≥n de Grasp2Vec y el reconocimiento de patrones con nuestra pol√≠tica de "capturar cualquier cosa" y logramos √©xito en el 80% de los casos durante la recopilaci√≥n de datos y en el 59% con nuevos objetos que el robot no ha encontrado previamente.</i> <br><br><h2>  Conclusi√≥n </h2><br>  En nuestro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo,</a> mostramos c√≥mo las habilidades de agarre rob√≥tico pueden crear datos utilizados para ense√±ar representaciones de objetos.  Luego, podemos usar el entrenamiento de presentaci√≥n para adquirir r√°pidamente habilidades m√°s complejas, como capturar con el ejemplo, mientras conservamos todas las propiedades de aprendizaje sin un maestro en nuestro sistema de captura aut√≥nomo. <br><br>  Adem√°s de nuestro trabajo, varios otros trabajos recientes tambi√©n estudiaron c√≥mo la interacci√≥n sin un maestro se puede utilizar para obtener representaciones de objetos, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capturando</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">empujando</a> y otros tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">interacciones</a> con objetos en el entorno.  Estamos anticipando con alegr√≠a no solo lo que el aprendizaje autom√°tico puede dar a la rob√≥tica en t√©rminos de una mejor percepci√≥n y control, sino tambi√©n lo que la rob√≥tica puede dar al aprendizaje autom√°tico en t√©rminos de nuevos paradigmas de autoaprendizaje. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es434898/">https://habr.com/ru/post/es434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es434888/index.html">Regalo de a√±o nuevo del distrito binario</a></li>
<li><a href="../es434890/index.html">Qiwi Bank (JSC) asigna dinero a los usuarios</a></li>
<li><a href="../es434892/index.html">Dibujando c√≥digo en Swift, PaintCode</a></li>
<li><a href="../es434894/index.html">El arte del chamanismo o firmware personalizado para Olinuxino. Parte 1</a></li>
<li><a href="../es434896/index.html">Sal√≥n de la fama de la electr√≥nica de consumo: las historias de los mejores artilugios de los √∫ltimos 50 a√±os, parte 1</a></li>
<li><a href="../es434902/index.html">Creaci√≥n de un generador de consultas personalizado en Spring Data Neo4j (Parte 1)</a></li>
<li><a href="../es434906/index.html">Pruebas en C ++ sin macros y memoria din√°mica.</a></li>
<li><a href="../es434908/index.html">Programador de Educaci√≥n - ¬øQu√©? Donde Cuando</a></li>
<li><a href="../es434912/index.html">El stock anual de Porsche Taycan ya est√° reservado, principalmente por los propietarios de Tesla</a></li>
<li><a href="../es434924/index.html">Qu√© leer sobre organizaci√≥n de lugares de trabajo, coworking y dise√±o de espacios para trabajo remoto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>