<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘ğŸ½ ğŸ‰ ğŸ› Oh metode Newton ini ğŸŒ’ ğŸ¤‘ ğŸ™‰</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Banyak yang telah ditulis tentang metode optimasi numerik. Ini dapat dimengerti, terutama dengan latar belakang keberhasilan yang baru-baru ini ditunj...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Oh metode Newton ini</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469877/">  Banyak yang telah ditulis tentang metode optimasi numerik.  Ini dapat dimengerti, terutama dengan latar belakang keberhasilan yang baru-baru ini ditunjukkan oleh jaringan saraf yang dalam.  Dan sangat memuaskan bahwa setidaknya beberapa penggemar tertarik tidak hanya pada bagaimana membom jaringan saraf mereka pada kerangka kerja yang telah mendapatkan popularitas di Internet ini, tetapi juga bagaimana dan mengapa semuanya bekerja.  Namun, saya baru-baru ini harus mencatat bahwa dalam mengajukan masalah yang berkaitan dengan pelatihan jaringan saraf (dan tidak hanya dengan pelatihan, dan tidak hanya jaringan), termasuk di HabrÃ©, semakin sering sejumlah pernyataan "terkenal" digunakan untuk penerusan, validitasnya secara halus, diragukan.  Di antara pernyataan meragukan tersebut: <br><br><ol><li>  Metode pesanan kedua dan lebih banyak tidak berfungsi dengan baik dalam tugas-tugas pelatihan jaringan saraf.  Karena itu. </li><li>  Metode Newton membutuhkan kepastian positif dari matriks Hessian (turunan kedua) dan karenanya tidak bekerja dengan baik. <br></li><li>  Metode Levenberg-Marquardt adalah kompromi antara gradient descent dan metode Newton dan umumnya heuristik. <br></li></ol><br>  dll.  Daripada melanjutkan daftar ini, lebih baik turun ke bisnis.  Dalam posting ini kita akan mempertimbangkan pernyataan kedua, karena saya hanya bertemu dengannya setidaknya dua kali di HabrÃ©.  Saya akan menyentuh pada pertanyaan pertama hanya pada bagian mengenai metode Newton, karena jauh lebih luas.  Yang ketiga dan sisanya akan dibiarkan sampai waktu yang lebih baik. <br><a name="habracut"></a><br>  Fokus perhatian kami adalah tugas optimasi tanpa syarat <img src="https://habrastorage.org/getpro/habr/post_images/823/e50/c93/823e50c935e4cc19a175f53c17ca79af.gif" title="&quot;f (x) \ rightarrow \ min&quot;">  dimana <img src="https://habrastorage.org/getpro/habr/post_images/bc7/838/e10/bc7838e10e143195c0381efbf0671cce.gif" title="&quot;x = (x_ {1}, x_ {2}, \ dots)&quot;">  - titik ruang vektor, atau hanya - vektor.  Secara alami, tugas ini lebih mudah untuk diselesaikan, semakin banyak yang kita ketahui <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  .  Biasanya diasumsikan dapat dibedakan sehubungan dengan setiap argumen <img src="https://habrastorage.org/getpro/habr/post_images/6d8/d4e/07d/6d8d4e07d259325d5dd652e4b3b97af6.gif" title="&quot;x_ {k}&quot;">  , dan sebanyak yang diperlukan untuk perbuatan kotor kita.  Sudah diketahui bahwa kondisi yang diperlukan untuk itu pada suatu titik <img src="https://habrastorage.org/getpro/habr/post_images/8ae/e32/d7e/8aee32d7e93fb189b268894bf91622b0.gif" title="&quot;x ^ {*}&quot;">  minimum tercapai, adalah kesetaraan gradien fungsi <img src="https://habrastorage.org/getpro/habr/post_images/735/1d5/454/7351d54544ca7acc4b7a9bff7a2c2f6a.gif" title="&quot;\ bigtriangledown f (x ^ {*})&quot;">  pada titik ini nol.  Dari sini kita langsung mendapatkan metode minimalisasi berikut: <br><br>  Pecahkan persamaannya <img src="https://habrastorage.org/getpro/habr/post_images/d59/223/ac1/d59223ac159bfe660ea26a1d60f8f33f.gif" title="&quot;\ bigtriangledown f (x) = 0&quot;">  . <br><br>  Tugasnya, secara sederhana, bukanlah tugas yang mudah.  Jelas tidak lebih mudah dari aslinya.  Namun, pada titik ini, kita dapat segera mencatat hubungan antara masalah minimisasi dan masalah penyelesaian sistem persamaan nonlinier.  Koneksi ini akan kembali kepada kita ketika mempertimbangkan metode Levenberg-Marquardt (ketika kita sampai di sana).  Sementara itu, ingat (atau cari tahu) bahwa salah satu metode yang paling umum digunakan untuk menyelesaikan sistem persamaan nonlinier adalah metode Newton.  Terdiri dari fakta bahwa untuk menyelesaikan persamaan <img src="https://habrastorage.org/getpro/habr/post_images/0a0/ec7/804/0a0ec780406efe57ca6444290ccfde09.gif" title="&quot;F (x) = 0&quot;">  kita mulai dari beberapa perkiraan awal <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  membangun urutan <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a0/23d/4a2/3a023d4a27cdff86f8cf3bc78d5b3a21.gif" title="&quot;x_ {i + 1} = x_ {i} -H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - Metode eksplisit Newton <br><br>  atau <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/6fa/27b/1166fa27b4038fed75d435daaaab53fe.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + p_ {i} \ end {cases}&quot;">  - Metode tersirat Newton <br><br>  dimana <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  - Matriks terdiri dari turunan parsial dari suatu fungsi <img src="https://habrastorage.org/getpro/habr/post_images/01a/a15/8fc/01aa158fc8bc3d7f7f3b2807df8b4a5e.gif" title="&quot;F&quot;">  .  Secara alami, dalam kasus umum, ketika sistem persamaan nonlinier hanya diberikan kepada kita dalam sensasi, memerlukan sesuatu dari matriks <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  kami tidak berhak.  Dalam kasus ketika persamaan adalah kondisi minimum untuk beberapa fungsi, kita dapat menyatakan bahwa matriks <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  simetris.  Tapi tidak lebih. <br><br>  Metode Newton untuk memecahkan sistem persamaan nonlinier telah dipelajari dengan cukup baik.  Dan inilah masalahnya - untuk konvergensi, kepastian positif dari matriks tidak diperlukan <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  .  Ya, dan tidak bisa diminta - kalau tidak, dia tidak akan berharga.  Sebaliknya, ada kondisi lain yang memastikan konvergensi lokal dari metode ini dan yang tidak akan kami pertimbangkan di sini, mengirim orang yang tertarik ke literatur khusus (atau dalam komentar).  Kami mendapatkan bahwa pernyataan 2 salah. <br><br>  Jadi? <br><br>  Ya dan tidak  Penyergapan di sini dalam kata ini adalah konvergensi lokal sebelum kata.  Artinya perkiraan awal <img src="https://habrastorage.org/getpro/habr/post_images/460/82f/7d6/46082f7d6471c3fabb832d8f94075758.gif" title="&quot;x_ {0}&quot;">  harus "cukup dekat" dengan solusi, jika tidak pada setiap langkah kita akan semakin jauh dari itu.  Apa yang harus dilakukan  Saya tidak akan masuk ke rincian tentang bagaimana masalah ini diselesaikan untuk sistem persamaan non-linear dari bentuk umum.  Alih-alih, kembali ke tugas pengoptimalan kami.  Kesalahan pertama dari pernyataan 2 sebenarnya adalah bahwa biasanya berbicara tentang metode Newton dalam masalah optimasi, itu berarti modifikasi - metode Newton teredam, di mana urutan perkiraan dibangun sesuai dengan aturan <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23d/ca8/404/23dca84042b77a1560b8cd2db607e8ae.gif" title="&quot;x_ {i + 1} = x_ {i} - \ alpha_ {i} H ^ {- 1} (x_ {i}) F (x_ {i})&quot;">  - Metode teredam eksplisit Newton <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9d4/67c/c96/9d467cc96266cf1179d3e553718f5bee.gif" title="&quot;\ begin {cases} H (x_ {i}) p_ {i} = - F (x_ {i}) \\ x_ {i + 1} = x_ {i} + \ alpha_ {i} p_ {i} \ end {cases} &quot;">  - Metode teredam implisit Newton <br><br>  Ini urutannya <img src="https://habrastorage.org/getpro/habr/post_images/c70/738/fd1/c70738fd1eb4d9bfff34f20904f41bbf.gif" title="&quot;\ {\ alpha_ {i} \}&quot;">  adalah parameter dari metode dan konstruksinya adalah tugas yang terpisah.  Dalam masalah minimisasi, wajar bila memilih <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  akan ada persyaratan bahwa, pada setiap iterasi, nilai fungsi f menurun, yaitu <img src="https://habrastorage.org/getpro/habr/post_images/3ae/e45/ba0/3aee45ba0097ca8bdc8a23ef6a465f21.gif" title="&quot;f (x_ {i + 1}) &amp; lt; f (x_ {i})&quot;">  .  Muncul pertanyaan logis: apakah ada (positif) <img src="https://habrastorage.org/getpro/habr/post_images/eb2/94d/fe4/eb294dfe4cfca7355f8b030f3d7dade8.gif" title="&quot;\ alpha_ {i}&quot;">  ?  Dan jika jawaban untuk pertanyaan ini positif, maka <img src="https://habrastorage.org/getpro/habr/post_images/cf2/deb/64e/cf2deb64e8b0e4d34902a32a5fd93b7b.gif" title="&quot;p_ {i}&quot;">  disebut arah keturunan.  Maka pertanyaannya dapat diajukan dengan cara ini: <br>  <i>Kapan arah yang dihasilkan oleh metode Newton adalah arah keturunan?</i> <br>  Dan untuk menjawabnya Anda harus melihat masalah minimisasi dari sisi lain. <br><br><h2>  Metode keturunan </h2><br>  Untuk masalah minimisasi, pendekatan ini tampaknya cukup alami: mulai dari titik arbitrer, kami memilih arah p dalam beberapa cara dan mengambil langkah ke arah ini <img src="https://habrastorage.org/getpro/habr/post_images/fbf/01e/b21/fbf01eb21703831c5dd0e196a2efccc2.gif" title="&quot;\ alpha p&quot;">  .  Jika <img src="https://habrastorage.org/getpro/habr/post_images/bd9/6f9/580/bd96f95806b05f65a5766db233a85653.gif" title="&quot;f (x + \ alpha p) &amp; lt; f (x)&quot;">  lalu ambil <img src="https://habrastorage.org/getpro/habr/post_images/b87/e59/538/b87e59538ed10c96ec3db2e7bad8dc85.gif" title="&quot;x + \ alpha p&quot;">  sebagai titik awal baru dan ulangi prosedur.  Jika arah dipilih secara sewenang-wenang, maka metode seperti itu kadang-kadang disebut metode berjalan acak.  Anda dapat menggunakan vektor satuan dasar sebagai arah - yaitu, ambil langkah hanya dalam satu koordinat, metode ini disebut metode penurunan koordinat.  Tak perlu dikatakan, mereka tidak efektif?  Agar pendekatan ini bekerja dengan baik, kami membutuhkan beberapa jaminan tambahan.  Untuk melakukan ini, kami memperkenalkan fungsi bantu <img src="https://habrastorage.org/getpro/habr/post_images/8bf/1d5/4e1/8bf1d54e1f36dd4c9dfd5720437af51c.gif" title="&quot;g (p) = f (x + p)&quot;">  .  Saya pikir itu jelas minimisasi <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  sepenuhnya sama dengan meminimalkan <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  .  Jika <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  dibedakan itu <img src="https://habrastorage.org/getpro/habr/post_images/da7/7c5/b48/da77c5b4891cf3d059f1b04a28b230ef.gif" title="g">  dapat direpresentasikan sebagai <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/615/d31/e47615d310276ab67a9163889a2335a5.gif" title="&quot;g (p) = f (x) + \ bigtriangledown f ^ {T} (x) p + o (\ parallel p \ parallel ^ {2})&quot;"><br><br>  dan jika <img src="https://habrastorage.org/getpro/habr/post_images/2a7/342/acb/2a7342acbe0772f75af6eee281c247d0.gif" title="&quot;\ parallel p \ parallel&quot;">  cukup kecil <img src="https://habrastorage.org/getpro/habr/post_images/2ef/8a9/23f/2ef8a923f49cf84264effb5f3f703c31.gif" title="&quot;g (p) \ approx \ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p&quot;">  .  Kami sekarang dapat mencoba mengganti masalah minimisasi <img src="https://habrastorage.org/getpro/habr/post_images/076/563/484/076563484d4e576c5c48098bfa94d45c.gif" title="&quot;g (p)&quot;">  tugas meminimalkan aproksimasi (atau <i>model</i> ) <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  .  Omong-omong, semua metode didasarkan pada penggunaan model <img src="https://habrastorage.org/getpro/habr/post_images/174/774/888/1747748884846362babfd8fe73857f1e.gif" title="&quot;\ bar {g} (p)&quot;">  disebut gradien.  Tapi masalahnya adalah, <img src="https://habrastorage.org/getpro/habr/post_images/462/957/dda/462957dda265f4fb8be04327f1c12b0f.gif" title="&quot;\ bar {g}&quot;">  Merupakan fungsi linier dan, karenanya, tidak memiliki minimum.  Untuk mengatasi masalah ini, kami menambahkan batasan pada panjang langkah yang ingin kami ambil.  Dalam hal ini, ini adalah persyaratan yang sepenuhnya alami - karena model kami kurang lebih menggambarkan fungsi objektif hanya di lingkungan yang cukup kecil.  Akibatnya, kami memperoleh masalah tambahan tentang pengoptimalan bersyarat: <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/8a2/c43/279/8a2c4327974067619cfad20b7ea1e821.gif" title="\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ parallel p \ parallel_ {2} = \ Delta"></a> <br><br>  Tugas ini memiliki solusi yang jelas: <img src="https://habrastorage.org/getpro/habr/post_images/3ff/1f6/a21/3ff1f6a2117e5d9a99603bcc8fde4f69.gif" title="&quot;p = - \ beta \ bigtriangledown f (x)&quot;">  dimana <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  - Faktor yang menjamin pemenuhan kendala.  Kemudian iterasi metode keturunan mengambil formulir <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/987/c25/966987c257a50df1855a50ea363350dd.gif" title="&quot;x_ {i + 1} = x_ {i} - \ beta \ bigtriangledown f (x_ {i})&quot;">  , <br><br>  di mana kita belajar <b>metode gradient descent yang</b> terkenal.  Parameter <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  , yang biasanya disebut kecepatan turun, kini telah memperoleh makna yang dapat dipahami, dan nilainya ditentukan dari kondisi bahwa titik baru itu terletak pada bidang radius tertentu, dibatasi di sekitar titik lama. <br><br>  Berdasarkan pada sifat-sifat model yang dibangun dari fungsi tujuan, kita dapat berargumen bahwa ada <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  , meskipun sangat kecil, bagaimana jika <img src="https://habrastorage.org/getpro/habr/post_images/84b/5fd/00f/84b5fd00fe1f4ca32b7cd7bd095a1490.gif" title="&quot;\ bar {g} (p) &amp; lt; \ bar {g} (0)&quot;">  lalu <img src="https://habrastorage.org/getpro/habr/post_images/553/80b/dc5/55380bdc5a434366df6d181078d6a8b7.gif" title="&quot;g (p) &amp; lt; g (0)&quot;">  .  Perlu dicatat bahwa dalam hal ini arah di mana kita akan bergerak tidak tergantung pada ukuran jari-jari bola ini.  Maka kita dapat memilih salah satu dari cara berikut: <br><br><ol><li>  Pilih menurut beberapa metode nilainya <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  . </li><li>  Tetapkan tugas memilih nilai yang sesuai <img src="https://habrastorage.org/getpro/habr/post_images/76d/0eb/69b/76d0eb69ba026a58bbe3edd275fee712.gif" title="&quot;\ beta&quot;">  memberikan penurunan nilai fungsi tujuan. </li></ol><br>  Pendekatan pertama adalah tipikal untuk <i>metode wilayah kepercayaan</i> , yang kedua mengarah pada perumusan masalah tambahan yang disebut  <i>pencarian linear (LineSearch)</i> .  Dalam kasus khusus ini, perbedaan antara pendekatan ini kecil dan kami tidak akan mempertimbangkannya.  Sebaliknya, perhatikan hal-hal berikut: <br><br>  <b><i>mengapa, sebenarnya, kita mencari penyeimbang</i></b> <b><i><img src="https://habrastorage.org/getpro/habr/post_images/4b2/8c1/3d5/4b28c13d5f5d658adb7478fbc9efc923.gif" title="&quot;p&quot;"></i></b>  <b><i>berbaring persis di bola?</i></b> <br><br>  Faktanya, kita dapat mengganti batasan ini dengan persyaratan, misalnya, bahwa p termasuk permukaan kubus, mis., <img src="https://habrastorage.org/getpro/habr/post_images/cf1/a35/92e/cf1a3592ebe97c9e262a083ea44c594c.gif" title="&quot;\ parallel p \ parallel _ {\ infty} = \ Delta&quot;">  (dalam hal ini, itu tidak terlalu masuk akal, tetapi mengapa tidak), atau permukaan elips?  Ini sudah tampak cukup logis, jika kita mengingat masalah yang muncul saat meminimalkan fungsi jurang.  Inti dari masalah adalah bahwa di sepanjang garis koordinat fungsi berubah jauh lebih cepat daripada di sepanjang yang lain.  Karena itu, kita dapatkan bahwa jika kenaikan itu harus menjadi milik bola, maka jumlahnya <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  di mana "keturunan" disediakan harus sangat kecil.  Dan ini mengarah pada fakta bahwa mencapai minimum akan membutuhkan sejumlah besar langkah.  Tetapi jika sebaliknya kita mengambil elips yang cocok sebagai lingkungan, maka masalah ini secara ajaib akan sia-sia. <br><br>  Dengan syarat bahwa titik-titik dari permukaan elips termasuk, dapat ditulis <img src="https://habrastorage.org/getpro/habr/post_images/6ff/1b4/930/6ff1b49309ec84aa656d848764359b4e.gif" title="&quot;\ parallel p \ parallel_ {B} = \ sqrt {p ^ {T} Bp} = \ Delta&quot;">  dimana <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  Apakah beberapa matriks pasti positif, juga disebut metrik.  Norma <img src="https://habrastorage.org/getpro/habr/post_images/ab8/b42/711/ab8b42711a932f9129bdb193b6a74360.gif" title="&quot;\ parallel \ cdot \ parallel_ {B}&quot;">  disebut norma elips yang disebabkan oleh matriks <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Matriks macam apa ini dan dari mana mendapatkannya - kita akan mempertimbangkannya nanti, dan sekarang kita sampai pada tugas baru. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/f4c/cc0/757/f4ccc0757e09fb304ff10a9a8c4751b6.gif" title="\\ bar {g} (p) = f (x) + \ bigtriangledown f ^ {T} (x) p \ rightarrow \ min \\ \ dfrac {1} {2} \ parallel p \ parallel_ {B} ^ {2} = \ Delta"></a> <br><br>  Kuadrat norma dan faktor 1/2 ada di sini semata-mata untuk kenyamanan, agar tidak mengacaukan akarnya.  Menerapkan metode pengali Lagrange, kami mendapatkan masalah terikat dari optimasi tanpa syarat <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/605/36d/d5e/60536dd5e2297580940a5b926760a3ce.gif" title="f (x) + \ bigtriangledown f ^ {T} (x) p + \ dfrac {\ lambda} {2} p ^ {T} Bp- \ lambda \ Delta \ rightarrow \ min"></a> <br><br>  Syarat yang diperlukan untuk minimum untuk itu adalah <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/4c5/f7e/921/4c5f7e921637c73fcb992c5d9b9efcd6.gif" title="\ bigtriangledown f (x) + \ lambda Bp = 0"></a>  , atau <img src="https://habrastorage.org/getpro/habr/post_images/a80/6c9/ff7/a806c9ff7ce22ea27c87b6a61a4c8fed.gif" title="&quot;B \ kiri (\ lambda p \ kanan) = - \ bigtriangledown f (x)&quot;">  dari mana <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/b50/032/3a9/b500323a970c7ae821295450627bdad2.gif" title="p = - \ dfrac {1} {\ lambda} B ^ {- 1} \ bigtriangledown f (x) = \ dfrac {1} {\ lambda} \ bar {p}"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/697/906/5d7/6979065d729033e0093ffad8475e80a6.gif" title="\ dfrac {1} {\ lambda ^ {2}} \ kiri (B ^ {- 1} \ bigtriangledown f (x) \ kanan) ^ {T} B \ kiri (B ^ {- 1} \ bigtriangledown f (x) \ kanan) = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} BB ^ {- 1} \ bigtriangledown f (x) = \ \ = \ dfrac {1} {\ lambda ^ {2}} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x) = \ Delta"></a> <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/41f/689/d89/41f689d890b92c84f05fdd0ede8a8114.gif" title="\ lambda = \ sqrt {\ dfrac {1} {\ Delta} \ bigtriangledown f (x) ^ {T} B ^ {- 1} \ bigtriangledown f (x)}> 0"></a> <br><br>  Sekali lagi kita melihat arahnya <img src="https://habrastorage.org/getpro/habr/post_images/45b/686/bb0/45b686bb0219a74b212cfdeaf1998653.gif" title="&quot;\ bar {p} = - B ^ {- 1} \ bigtriangledown f (x)&quot;">  , di mana kami akan pindah, tidak tergantung pada nilai <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  - hanya dari matriks <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  .  Dan lagi, kita bisa mengambilnya <img src="https://habrastorage.org/getpro/habr/post_images/a2b/068/6ad/a2b0686adbc103ad9f96be85cca5d418.gif" title="&quot;\ Delta&quot;">  yang penuh dengan kebutuhan untuk menghitung <img src="https://habrastorage.org/getpro/habr/post_images/99d/394/e7d/99d394e7d0b74248114405067e0ffd51.gif" title="&quot;\ lambda&quot;">  dan inversi matriks eksplisit <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  , atau memecahkan masalah pelengkap dalam menemukan bias yang cocok <img src="https://habrastorage.org/getpro/habr/post_images/b7b/6a7/371/b7b6a73716dc8f4e40a52c1c5ef0e6b4.gif" title="&quot;x_ {i + 1} = x_ {i} + \ beta \ bar {p} _ {i}&quot;">  .  Sejak <img src="https://habrastorage.org/getpro/habr/post_images/0b8/52f/d1b/0b852fd1bbc20f2966bf757a56186312.gif" title="&quot;\ lambda &amp; gt; 0&quot;">  , solusi untuk masalah tambahan ini dijamin ada. <br><br>  Jadi apa yang seharusnya untuk matriks B?  Kami membatasi diri pada ide spekulatif.  Jika fungsi objektif <img src="https://habrastorage.org/getpro/habr/post_images/188/ee6/44e/188ee644e8202aad30eac11166858841.gif" title="&quot;f&quot;">  - kuadrat, yaitu memiliki bentuk <img src="https://habrastorage.org/getpro/habr/post_images/974/f7f/cf6/974f7fcf6345b91ef8466f2cabba6efe.gif" title="&quot;f (x) = a + b ^ {T} x + x ^ {T} Hx&quot;">  dimana <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  positif pasti, jelas bahwa kandidat terbaik untuk peran matriks <img src="https://habrastorage.org/getpro/habr/post_images/dab/ea9/01c/dabea901c4b1a4079aa96d47bcee4e75.gif" title="&quot;B&quot;">  adalah goni <img src="https://habrastorage.org/getpro/habr/post_images/bc8/190/17b/bc819017bab0b9f9d995f262f3f76a42.gif" title="&quot;H&quot;">  , karena dalam hal ini diperlukan satu iterasi dari metode keturunan yang telah kami bangun.  Jika H tidak pasti positif, maka itu tidak bisa menjadi metrik, dan iterasi yang dibangun dengannya adalah iterasi dari metode Newton teredam, tetapi mereka bukan iterasi dari metode keturunan.  Akhirnya, kita bisa memberikan jawaban yang tepat <br><br>  <b>Pertanyaan:</b> <i>Apakah matriks Hessian dalam metode Newton harus positif pasti?</i> <br>  <b>Jawab:</b> <i>tidak, tidak diperlukan dalam metode standar atau metode Newton yang teredam.</i>  <i>Tetapi jika kondisi ini dipenuhi, maka metode Newton yang teredam adalah metode turunan dan memiliki properti <i>global</i> , dan bukan hanya konvergensi lokal.</i> <br><br>  Sebagai ilustrasi, mari kita lihat bagaimana daerah kepercayaan terlihat ketika meminimalkan fungsi Rosenbrock yang terkenal menggunakan gradient descent dan metode Newton, dan bagaimana bentuk daerah mempengaruhi konvergensi proses. <br><br><img src="https://habrastorage.org/webt/_x/30/nx/_x30nxs-eyrixuan0-diyvwixww.gif" width="600"><br><br>  Ini adalah bagaimana metode keturunan berperilaku dengan wilayah kepercayaan bola, itu juga merupakan keturunan gradien.  Semuanya seperti buku teks - kita terjebak di jurang. <br><br><img src="https://habrastorage.org/webt/9x/ik/td/9xiktd4lapdka-uk010evfvlcdm.gif" width="600"><br><br>  Dan ini kita dapatkan jika wilayah kepercayaan memiliki bentuk elips yang didefinisikan oleh matriks Hessian.  Ini tidak lebih dari sebuah iterasi dari metode Newton yang teredam. <br><br>  Hanya pertanyaan apa yang harus dilakukan jika matriks Hessian tidak pasti positif tetap belum terpecahkan.  Ada banyak pilihan.  Yang pertama adalah skor.  Mungkin Anda beruntung dan iterasi Newton akan bertemu tanpa properti ini.  Ini cukup nyata, terutama pada tahap akhir dari proses minimisasi, ketika Anda sudah cukup dekat dengan solusi.  Dalam hal ini, iterasi metode Newton standar dapat digunakan tanpa repot mencari lingkungan yang diizinkan untuk turun.  Atau gunakan iterasi metode Newton teredam dalam kasus <img src="https://habrastorage.org/getpro/habr/post_images/6da/2c0/bc5/6da2c0bc54434a64d7630c142d0c7bf9.gif" title="&quot;\ beta = 0&quot;">  , yaitu, dalam kasus ketika arah yang diperoleh bukan arah keturunan, ubahlah, katakanlah, menjadi anti-gradien.  <i>Hanya saja, tidak perlu secara eksplisit memeriksa apakah Goni itu pasti positif menurut kriteria Sylvester</i> , seperti yang dilakukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini !!!</a>  .  Itu sia-sia dan sia-sia. <br>  Metode yang lebih halus melibatkan membangun matriks, dalam arti dekat dengan matriks Hessian, tetapi memiliki sifat kepastian positif, khususnya, dengan mengoreksi nilai eigen.  Topik terpisah adalah metode kuasi-Newtonian, atau metode metrik variabel, yang menjamin kepastian positif dari matriks B dan tidak memerlukan perhitungan turunan kedua.  Secara umum, diskusi rinci tentang masalah-masalah ini melampaui ruang lingkup artikel ini. <br><br>  Ya, dan omong-omong, ini mengikuti dari apa yang telah dikatakan bahwa <i>metode teredam Newton dengan kepastian positif Hessian adalah metode gradien</i> .  Serta metode kuasi-Newtonian.  Dan banyak lainnya, berdasarkan pilihan arah dan ukuran langkah yang terpisah.  Jadi membandingkan metode Newton dengan terminologi gradien tidak benar. <br><br><h2>  Untuk meringkas </h2><br>  Metode Newton, yang sering diingat ketika membahas metode minimisasi, biasanya bukan metode Newton dalam pengertian klasiknya, tetapi metode keturunan dengan metrik yang ditentukan oleh Hessian tentang fungsi tujuan.  Dan ya, itu konvergen secara global jika Goni di mana-mana pasti positif.  Ini hanya mungkin untuk fungsi-fungsi cembung, yang jauh lebih jarang terjadi dalam praktik daripada yang kita inginkan, jadi dalam kasus umum, tanpa modifikasi yang sesuai, penerapan metode Newton (kami tidak akan melepaskan diri dari kolektif dan terus menyebutnya demikian) tidak menjamin hasil yang benar.  Mempelajari jaringan saraf, bahkan yang dangkal, biasanya mengarah pada masalah optimisasi non-cembung dengan banyak minimum lokal.  Dan inilah serangan baru.  Metode Newton biasanya menyatu (jika menyatu) dengan cepat.  Maksud saya sangat cepat.  Dan anehnya, ini buruk, karena kita mencapai minimum lokal dalam beberapa iterasi.  Dan untuk fungsi dengan medan yang kompleks bisa jauh lebih buruk daripada yang global.  Keturunan gradien dengan pencarian linier konvergen jauh lebih lambat, tetapi lebih cenderung untuk "melewati" punggungan fungsi objektif, yang sangat penting dalam tahap awal minimisasi.  Jika Anda telah mengurangi nilai fungsi objektif dengan baik, dan konvergensi penurunan gradien telah melambat secara signifikan, maka perubahan dalam metrik dapat mempercepat proses, tetapi ini adalah untuk tahap akhir. <br><br>  Tentu saja, argumen ini tidak universal, tidak dapat disangkal, dan dalam beberapa kasus bahkan salah.  Serta pernyataan bahwa metode gradien bekerja paling baik dalam masalah pembelajaran. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id469877/">https://habr.com/ru/post/id469877/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id469859/index.html">Pointer dan Value semantik dalam menentukan penerima suatu metode</a></li>
<li><a href="../id469861/index.html">Struktur data untuk pemrogram game: data massal</a></li>
<li><a href="../id469869/index.html">Mengapa Anda harus overclock RAM (mudah!)</a></li>
<li><a href="../id469871/index.html">Ketika keyboard adalah tabel</a></li>
<li><a href="../id469875/index.html">Cara melindungi kata sandi Anda pada tahun 2019</a></li>
<li><a href="../id469879/index.html">VPN ganda dalam satu klik. Cara mudah membagi alamat IP dari titik masuk dan keluar</a></li>
<li><a href="../id469881/index.html">Tiga hari pertama kehidupan sebuah pos di HabrÃ©</a></li>
<li><a href="../id469883/index.html">Apakah mungkin memprogram keacakan?</a></li>
<li><a href="../id469885/index.html">Nonaktifkan konsol lokal saat menggunakan x11vnc</a></li>
<li><a href="../id469889/index.html">SamsPcbGuide, bagian 12: Teknologi - penutup tipe BGA, plastik, dan ruang II</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>