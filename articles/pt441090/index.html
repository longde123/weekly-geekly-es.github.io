<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñêüèª üö† üíç AI de √°udio: extra√ß√£o de vocais da m√∫sica usando redes neurais convolucionais üõÄüèº üö¥üèº ‚ú¥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hacking de m√∫sica para democratizar o conte√∫do derivado 

 Isen√ß√£o de responsabilidade: Todas as propriedades intelectuais, designs e m√©todos descrito...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI de √°udio: extra√ß√£o de vocais da m√∫sica usando redes neurais convolucionais</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441090/">  <i>Hacking de m√∫sica para democratizar o conte√∫do derivado</i> <br><br><blockquote>  <b>Isen√ß√£o de responsabilidade:</b> Todas as propriedades intelectuais, designs e m√©todos descritos neste artigo s√£o divulgados nos documentos US10014002B2 e US9842609B2. </blockquote><br>  Eu gostaria de poder voltar a 1965, bater na porta da frente do est√∫dio da Abby Road com um passe, entrar e ouvir as vozes reais de Lennon e McCartney ... Bem, vamos tentar.  Entrada: MP3 de qualidade m√©dia dos Beatles, <i>podemos resolver isso</i> .  A faixa superior √© o mix de entrada, a faixa inferior s√£o os vocais isolados que nossa rede neural destacou. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Formalmente, esse problema √© conhecido como <i>separa√ß√£o das fontes de som</i> ou <i>separa√ß√£o do sinal</i> (separa√ß√£o da fonte de √°udio).  Consiste em restaurar ou reconstruir um ou mais dos sinais originais, que s√£o misturados com outros sinais como resultado de um processo <i>linear ou convolucional</i> .  Esse campo de pesquisa tem muitas aplica√ß√µes pr√°ticas, incluindo a melhoria da qualidade do som (fala) e a elimina√ß√£o de ru√≠dos, remixes de m√∫sicas, distribui√ß√£o espacial do som, remasteriza√ß√£o etc. Os engenheiros de som √†s vezes chamam essa t√©cnica de desmistifica√ß√£o.  Existem muitos recursos nesse t√≥pico, desde a separa√ß√£o cega de sinais com an√°lise de componentes independentes (ACI) at√© a fatora√ß√£o semi-controlada de matrizes n√£o-negativas e terminando com abordagens posteriores baseadas em redes neurais.  Voc√™ pode encontrar boas informa√ß√µes sobre os dois primeiros pontos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nesses mini-guias</a> da CCRMA, que ao mesmo tempo foram muito √∫teis para mim. <br><br>  <b>Mas antes de mergulhar no desenvolvimento ... um pouco da filosofia aplicada ao aprendizado de m√°quina ...</b> <br><br>  Eu estava envolvido no processamento de sinais e imagens mesmo antes de o slogan "aprendizado profundo resolver tudo" se espalhar, para que eu possa apresentar uma solu√ß√£o como uma viagem de <i>engenharia de recursos</i> e mostrar <b>por que uma rede neural √© a melhor abordagem para esse problema em particular</b> .  Porque  Muitas vezes, vejo pessoas escreverem algo assim: <br><br>  <i>‚ÄúCom o aprendizado profundo, voc√™ n√£o precisa mais se preocupar em escolher recursos;</i>  <i>far√° isso por voc√™.</i> <br><br>  ou pior ... <br><br>  <i>‚ÄúA diferen√ßa entre aprendizado de m√°quina e aprendizado profundo</i> [ei ... aprendizado profundo ainda √© aprendizado de m√°quina!] √â <i>que no ML voc√™ extrai os atributos e no aprendizado profundo isso acontece automaticamente na rede.‚Äù</i> <br><br>  Essas generaliza√ß√µes provavelmente v√™m do fato de que os DNNs podem ser muito eficazes na explora√ß√£o de bons espa√ßos ocultos.  Mas, portanto, √© imposs√≠vel generalizar.  Fico muito chateado quando rec√©m-formados e profissionais sucumbem aos conceitos errados acima e adotam a abordagem de "aprender profundamente tudo".  Assim, basta lan√ßar um monte de dados brutos (mesmo ap√≥s um pequeno processamento preliminar) - e tudo funcionar√° como deveria.  No mundo real, voc√™ precisa cuidar de coisas como desempenho, execu√ß√£o em tempo real, etc. Por causa de tais equ√≠vocos, voc√™ ficar√° preso no modo de experimento por muito tempo ... <br><br>  <b>A Engenharia de recursos continua sendo uma disciplina muito importante no projeto de redes neurais artificiais.</b>  <b>Como em qualquer outra t√©cnica de ML, na maioria dos casos, √© o que distingue solu√ß√µes efetivas do n√≠vel de produ√ß√£o de experimentos malsucedidos ou ineficazes.</b>  <b>Uma profunda compreens√£o dos seus dados e sua natureza ainda significa muito ...</b> <br><br><h1>  A a Z </h1><br>  Ok, eu terminei o serm√£o.  Agora vamos ver porque estamos aqui!  Como em qualquer problema de processamento de dados, vamos primeiro ver como ele √©.  Veja a pr√≥xima parte dos vocais da grava√ß√£o original do est√∫dio. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Vocais de est√∫dio 'One Last Time', Ariana Grande</font></i> <br><br>  N√£o √© muito interessante, certo?  Bem, isso √© porque visualizamos o sinal <i>no tempo</i> .  Aqui vemos apenas mudan√ßas de amplitude ao longo do tempo.  Mas voc√™ pode extrair todo tipo de outras coisas, como envelopes de amplitude (envelope), valores quadr√°ticos m√©dios da raiz (RMS), a taxa de altera√ß√£o de valores positivos de amplitude para negativos (taxa de cruzamento zero) etc., mas esses <i>sinais s√£o</i> muito <i>primitivos</i> e n√£o s√£o suficientemente distintos, para ajudar no nosso problema.  Se queremos extrair vocais de um sinal de √°udio, primeiro precisamos determinar de alguma forma a estrutura da fala humana.  Felizmente, a Janela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fourier Transform</a> (STFT) vem em socorro. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Espectro de amplitude STFT - tamanho da janela = 2048, sobreposi√ß√£o = 75%, escala de frequ√™ncia logar√≠tmica [Sonic Visualizer]</font></i> <br><br>  Embora eu <i>adore o</i> processamento de fala e definitivamente <i>adore</i> brincar com <i>simula√ß√µes de filtro de entrada, cepstrums, sottotami, LPC, MFCC</i> e assim por diante <i>, pularemos</i> toda essa bobagem e focaremos nos principais elementos relacionados ao nosso problema para que o artigo possa ser entendido pelo maior n√∫mero poss√≠vel de pessoas, n√£o apenas especialistas em processamento de sinais. <br><br>  Ent√£o, o que a estrutura da fala humana nos diz? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Bem, podemos definir tr√™s elementos principais aqui: <br><br><ul><li>  <b>A frequ√™ncia fundamental</b> (f0), que √© determinada pela frequ√™ncia de vibra√ß√£o de nossas cordas vocais.  Nesse caso, Ariana canta na faixa de 300 a 500 Hz. <br></li><li>  Uma s√©rie de <b>harm√¥nicos</b> acima de f0 que seguem uma forma ou padr√£o semelhante.  Esses harm√¥nicos aparecem em frequ√™ncias que s√£o m√∫ltiplos de f0. <br></li><li>  <b>Fala n√£o</b> verbalizada, que inclui consoantes como 't', 'p', 'k', 's' (que n√£o √© produzida pela vibra√ß√£o das cordas vocais), respira√ß√£o etc. Tudo isso se manifesta na forma de rajadas curtas na regi√£o de alta frequ√™ncia. </li></ul><br><h1>  Primeira tentativa com regras </h1><br>  Vamos esquecer por um segundo o que √© chamado de aprendizado de m√°quina.  Um m√©todo de extra√ß√£o vocal pode ser desenvolvido com base em nosso conhecimento do sinal?  Deixe-me tentar ... <br><br>  <b>Isolamento vocal <i>ing√™nuo</i> V1.0:</b> <br><br><ol><li>  Identifique √°reas com vocais.  H√° muitas coisas no sinal original.  Queremos focar nas √°reas que realmente cont√™m conte√∫do vocal e ignorar todo o resto. <br></li><li>  Distinga entre voz e voz.  Como vimos, eles s√£o muito diferentes.  Eles provavelmente precisam ser tratados de maneira diferente. <br></li><li>  Avalie a mudan√ßa na frequ√™ncia fundamental ao longo do tempo. <br></li><li>  Com base no pino 3, aplique algum tipo de m√°scara para capturar harm√¥nicos. <br></li><li>  Fa√ßa algo com fragmentos de fala n√£o verbalizada ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  Se trabalharmos dignamente, o resultado deve ser um <i>soft</i> ou <i>bitmask</i> , cuja aplica√ß√£o √† amplitude do STFT (multiplica√ß√£o por elementos) fornece uma reconstru√ß√£o aproximada da amplitude dos vocais do STFT.  Em seguida, combinamos esse STFT vocal com informa√ß√µes sobre a fase do sinal original, calculamos o STFT inverso e obtemos o sinal de tempo do vocal reconstru√≠do. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Fazer isso do zero j√° √© um grande trabalho.  Mas, para fins de demonstra√ß√£o, a implementa√ß√£o do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">algoritmo pYIN √© aplic√°vel</a> .  Embora se pretenda resolver o passo 3, mas com as configura√ß√µes corretas, ele executa decentemente os passos 1 e 2, rastreando a base vocal mesmo na presen√ßa de m√∫sica.  O exemplo abaixo cont√©m a sa√≠da ap√≥s o processamento desse algoritmo, sem processar a fala n√£o verbalizada. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  E o que ...?  Ele parece ter feito todo o trabalho, mas n√£o h√° boa qualidade nem perto.  Talvez gastando mais tempo, energia e dinheiro, melhoraremos esse m√©todo ... <br><br>  Mas deixe-me perguntar-lhe ... <br><br>  O que acontece se <b>algumas vozes</b> aparecerem na faixa, e ainda assim elas s√£o encontradas em pelo menos 50% das faixas profissionais modernas? <br><br>  O que acontece se os vocais forem processados ‚Äã‚Äãpor <b>reverb, atrasos</b> e outros efeitos?  Vamos dar uma olhada no √∫ltimo refr√£o de Ariana Grande dessa m√∫sica. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Voc√™ j√° sente dor ...?  Eu sou <br><br>  Tais m√©todos em regras estritas muito rapidamente se transformam em um castelo de cartas.  O problema √© muito complicado.  Muitas regras, muitas exce√ß√µes e muitas condi√ß√µes diferentes (efeitos e configura√ß√µes de mixagem).  Uma abordagem em v√°rias etapas tamb√©m implica que os erros em uma etapa estendem os problemas para a pr√≥xima etapa.  Melhorar cada etapa se tornar√° muito caro: ser√° necess√°rio um grande n√∫mero de itera√ß√µes para acertar.  E por √∫ltimo, mas n√£o menos importante, √© prov√°vel que, no final, tenhamos um transportador que consome muitos recursos, o que por si s√≥ pode negar todos os esfor√ßos. <br><br>  <b>Em tal situa√ß√£o, √© hora de come√ßar a pensar em uma abordagem mais <i>abrangente</i> e deixar o ML descobrir parte dos processos e opera√ß√µes b√°sicos necess√°rios para resolver o problema.</b>  <b>Mas ainda precisamos mostrar nossas habilidades e participar da engenharia de recursos, e voc√™ ver√° o porqu√™.</b> <br><br><h1>  Hip√≥tese: use a rede neural como uma fun√ß√£o de transfer√™ncia que traduz misturas em vocais </h1><br>  Observando as realiza√ß√µes das redes neurais convolucionais no processamento de fotos, por que n√£o aplicar a mesma abordagem aqui? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">As redes neurais resolvem com sucesso problemas como coloriza√ß√£o de imagens, nitidez e resolu√ß√£o.</font></i> <br><br>  No final, voc√™ pode imaginar o sinal sonoro "como uma imagem" usando a transformada de Fourier de curto prazo, certo?  Embora essas <i>imagens sonoras</i> n√£o correspondam √† distribui√ß√£o estat√≠stica das imagens naturais, elas ainda possuem padr√µes espaciais (no espa√ßo de tempo e frequ√™ncia) nos quais treinar a rede. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">Esquerda: batida de bateria e linha de base abaixo, v√°rios sons de sintetizador no meio, todos misturados com vocais.</font></i>  <i><font color="gray">Direita: apenas vocais</font></i> <br><br>  A realiza√ß√£o de tal experimento seria uma tarefa cara, pois √© dif√≠cil obter ou gerar os dados de treinamento necess√°rios.  Mas na pesquisa aplicada, sempre tento usar essa abordagem: primeiro, <b>para identificar um problema mais simples que confirme os mesmos princ√≠pios</b> , mas que n√£o exige muito trabalho.  Isso permite avaliar a hip√≥tese, iterar mais rapidamente e corrigir o modelo com perdas m√≠nimas, se n√£o funcionar como deveria. <br><br>  A condi√ß√£o impl√≠cita √© que a <b>rede neural deve entender a estrutura da fala humana</b> .  Um problema mais simples pode ser o seguinte: <i>uma rede neural pode determinar a presen√ßa de fala em um fragmento arbitr√°rio de uma grava√ß√£o de som</i> .  Estamos falando de um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">detector de atividade de voz</a> confi√°vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">(VAD)</a> , implementado na forma de um classificador bin√°rio. <br><br><h3>  N√≥s projetamos o espa√ßo dos sinais </h3><br>  Sabemos que os sinais sonoros, como m√∫sica e fala humana, s√£o baseados em depend√™ncias de tempo.  Simplificando, nada acontece isoladamente em um determinado momento.  Se eu quiser saber se h√° uma voz em uma determinada pe√ßa de grava√ß√£o de som, preciso examinar as regi√µes vizinhas.  Esse <i>contexto de tempo</i> fornece boas informa√ß√µes sobre o que est√° acontecendo na √°rea de interesse.  Ao mesmo tempo, √© desej√°vel executar a classifica√ß√£o com incrementos de tempo muito pequenos para reconhecer uma voz humana com a maior resolu√ß√£o poss√≠vel de tempo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Vamos contar um pouco ... <br><br><ul><li>  Frequ√™ncia de amostragem (fs): 22050 Hz (diminu√≠mos a amostra de 44100 para 22050) <br></li><li>  Projeto STFT: tamanho da janela = 1024, tamanho do salto = 256, interpola√ß√£o da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escala de giz</a> para o filtro de pondera√ß√£o, levando em considera√ß√£o a percep√ß√£o.  Como nossa entrada √© <i>real</i> , voc√™ pode trabalhar com metade do STFT (uma explica√ß√£o est√° al√©m do escopo deste artigo ...) enquanto mant√©m o componente DC (opcional), o que fornece 513 compartimentos de frequ√™ncia. <br></li><li>  Resolu√ß√£o da classifica√ß√£o alvo: um quadro STFT (~ 11,6 ms = 256/22050) <br></li><li>  Contexto do tempo alvo: ~ 300 milissegundos = 25 quadros STFT. <br></li><li>  O n√∫mero alvo de exemplos de treinamento: 500 mil. <br></li><li>  Supondo que usamos uma janela deslizante em incrementos de 1 intervalo de tempo STFT para gerar dados de treinamento, precisamos de cerca de 1,6 horas de som marcado para gerar 500 mil amostras de dados </li></ul><br>  Com os requisitos acima, a entrada e a sa√≠da do nosso classificador bin√°rio s√£o as seguintes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Modelo </h3><br>  Usando Keras, construiremos um pequeno modelo de rede neural para testar nossa hip√≥tese. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  Ao dividir os dados 80/20 em treinamento e teste ap√≥s ~ 50 √©pocas, obtemos a <b>precis√£o ao testar ~ 97%</b> .  Isso √© evid√™ncia suficiente de que nosso modelo √© capaz de distinguir entre vocais em fragmentos de som musical (e fragmentos sem vocais).  Se verificarmos alguns mapas de caracter√≠sticas da quarta camada convolucional, podemos concluir que a rede neural parece ter otimizado seus kernels para executar duas tarefas: filtrar m√∫sicas e filtrar vocais ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">Um exemplo de um mapa de objetos na sa√≠da da 4¬™ camada convolucional.</font></i>  <i><font color="gray">Aparentemente, a sa√≠da √† esquerda √© o resultado de opera√ß√µes do kernel na tentativa de preservar o conte√∫do vocal enquanto ignora a m√∫sica.</font></i>  <i><font color="gray">Valores altos se assemelham √† estrutura harmoniosa da fala humana.</font></i>  <i><font color="gray">O mapa de objetos √† direita parece ser o resultado da tarefa oposta.</font></i> <br><br><h1>  Do detector de voz √† desconex√£o do sinal </h1><br>  Tendo resolvido o problema mais simples de classifica√ß√£o, como podemos avan√ßar para a verdadeira separa√ß√£o dos vocais da m√∫sica?  Bem, olhando o primeiro m√©todo <i>ing√™nuo</i> , ainda queremos obter um espectrograma de amplitude para os vocais.  Agora isso est√° se tornando uma tarefa de regress√£o.  O que queremos fazer √© calcular o espectro de amplitude apropriado para os vocais neste per√≠odo de tempo a partir de um per√≠odo de tempo espec√≠fico do STFT do sinal original, ou seja, o mix (com um contexto de tempo suficiente). <br><br>  <b>E o conjunto de dados de treinamento?</b>  <b>(voc√™ pode me perguntar neste momento)</b> <br><br>  Porra ... por que sim.  Eu ia considerar isso no final do artigo para n√£o me distrair do t√≥pico! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  Se nosso modelo √© bem treinado, para uma conclus√£o l√≥gica, voc√™ s√≥ precisa implementar uma janela deslizante simples no mix STFT.  Ap√≥s cada previs√£o, mova a janela para a direita em 1 per√≠odo, preveja o pr√≥ximo quadro com os vocais e associe-o √† previs√£o anterior.  Quanto ao modelo, vamos pegar o mesmo modelo usado para o detector de voz e fazer pequenas altera√ß√µes: a forma de onda de sa√≠da √© agora (513.1), ativa√ß√£o linear na sa√≠da, MSE em fun√ß√£o de perdas.  Agora come√ßamos o treinamento. <br><br>  <b>N√£o se alegrar ainda ...</b> <br><br>  Embora essa representa√ß√£o de E / S fa√ßa sentido, ap√≥s treinar nosso modelo v√°rias vezes, com v√°rios par√¢metros e normaliza√ß√µes de dados, n√£o h√° resultados.  Parece que estamos pedindo demais ... <br><br>  Passamos de um classificador bin√°rio para <i>regress√£o</i> em um vetor 513-dimensional.  Embora a rede esteja estudando o problema at√© certo ponto, os vocais restaurados ainda t√™m artefatos √≥bvios e interfer√™ncia de outras fontes.  Mesmo ap√≥s adicionar camadas adicionais e aumentar o n√∫mero de par√¢metros do modelo, os resultados n√£o mudam muito.  E ent√£o surge a pergunta: <b>como "simplificar" a tarefa da rede por engano e, ao mesmo tempo, alcan√ßar os resultados desejados?</b> <br><br>  E se, em vez de estimar a amplitude dos vocais do STFT, treinarmos a rede para obter uma m√°scara bin√°ria, que quando aplicada ao mix STFT nos fornecer um espectrograma de amplitude simplificado, mas <b>perceptivamente aceit√°vel</b> dos vocais? <br><br>  Experimentando v√°rias heur√≠sticas, criamos uma maneira muito simples (e, √© claro, pouco ortodoxa em termos de processamento de sinal ...) para extrair vocais de mixagens usando m√°scaras bin√°rias.  Sem entrar em detalhes, a ess√™ncia √© a seguinte.  Imagine a sa√≠da como uma imagem bin√°ria, onde o valor '1' indica a <b>presen√ßa predominante de conte√∫do vocal</b> em uma determinada frequ√™ncia e per√≠odo de tempo, e o valor '0' indica a presen√ßa predominante de m√∫sica em um determinado local.  Podemos cham√°-lo de <i>binariza√ß√£o da percep√ß√£o</i> , apenas para criar um nome.  Visualmente, parece muito feio, para ser honesto, mas os resultados s√£o surpreendentemente bons. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Agora, nosso problema se torna um tipo de classifica√ß√£o de regress√£o h√≠brida (mais ou menos ...).  Pedimos ao modelo para ‚Äúclassificar pixels‚Äù na sa√≠da como vocal ou n√£o vocal, embora conceitualmente (bem como do ponto de vista da fun√ß√£o de perda MSE usada) a tarefa permane√ßa regressiva. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Embora essa distin√ß√£o possa parecer inadequada para alguns, de fato √© de grande import√¢ncia na capacidade do modelo de estudar a tarefa, a segunda das quais √© mais simples e mais limitada.  Ao mesmo tempo, isso nos permite manter nosso modelo relativamente pequeno em termos de n√∫mero de par√¢metros, dada a complexidade da tarefa, algo muito desej√°vel para o trabalho em tempo real, que nesse caso era um requisito de design.  Ap√≥s alguns pequenos ajustes, o modelo final fica assim. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  Como recuperar um sinal no dom√≠nio do tempo? </h3><br>  De fato, como no <i>m√©todo ing√™nuo</i> .  Nesse caso, para cada passagem, prevemos um per√≠odo de tempo da m√°scara de vocais bin√°rios.  Novamente, percebendo uma janela deslizante simples com uma etapa de um per√≠odo, continuamos a avaliar e combinar per√≠odos sucessivos, que comp√µem toda a m√°scara bin√°ria vocal. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Crie um conjunto de treinamento </h3><br>  Como voc√™ sabe, um dos principais problemas ao ensinar com um professor (deixe esses exemplos de brinquedos com conjuntos de dados prontos) s√£o os dados corretos (em quantidade e qualidade) para o problema espec√≠fico que voc√™ est√° tentando resolver.  Com base nas representa√ß√µes descritas de entrada e sa√≠da, para treinar nosso modelo, primeiro voc√™ precisar√° de um n√∫mero significativo de mixagens e suas faixas vocais correspondentes, perfeitamente alinhadas e normalizadas.  Esse conjunto pode ser criado de v√°rias maneiras, e usamos uma combina√ß√£o de estrat√©gias, desde a cria√ß√£o manual de pares [mix &lt;-&gt; vocais] com base em v√°rias cappelas encontradas na Internet, at√© a pesquisa de material de m√∫sica de banda de rock e scrapbooking do Youtube.  S√≥ para voc√™ ter uma id√©ia de como esse processo √© trabalhoso e penoso, parte do projeto foi o desenvolvimento de uma ferramenta para criar automaticamente pares [mix &lt;-&gt; vocais]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  √â necess√°ria uma quantidade realmente grande de dados para a rede neural aprender a fun√ß√£o de transfer√™ncia para a transmiss√£o de mixagens para os vocais.  Nosso conjunto final consistiu em aproximadamente 15 milh√µes de amostras de misturas de 300 ms e suas m√°scaras bin√°rias vocais correspondentes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Arquitetura de Pipeline </h3><br>  Como voc√™ provavelmente sabe, criar um modelo de ML para uma tarefa espec√≠fica √© apenas metade da batalha.  No mundo real, voc√™ precisa pensar na arquitetura de software, especialmente se precisar trabalhar em tempo real ou pr√≥ximo a ela. <br><br>  Nesta implementa√ß√£o espec√≠fica, a reconstru√ß√£o no dom√≠nio do tempo pode ocorrer imediatamente ap√≥s a previs√£o da m√°scara de vocais bin√°rios completos (modo aut√¥nomo) ou, mais interessante, no modo multithread, onde recebemos e processamos dados, restauramos vocais e reproduzimos sons - tudo em pequenos segmentos, pr√≥ximo a streaming e at√© quase em tempo real, processando m√∫sicas gravadas em tempo real com o m√≠nimo atraso.  Na verdade, este √© um t√≥pico separado, e deixarei para outro artigo <b>sobre pipelines ML em tempo real</b> ... <br><br><h1>  Eu acho que disse o suficiente, ent√£o por que n√£o ouvir alguns exemplos!? </h1><br><h3>  Daft Punk - Get Lucky (grava√ß√£o de est√∫dio) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Aqui voc√™ pode ouvir alguma interfer√™ncia m√≠nima da bateria ...</font></i> <br><br><h3>  Adele - Atear fogo √† chuva (grava√ß√£o ao vivo!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Observe como, no in√≠cio, nosso modelo extrai os gritos da multid√£o como conte√∫do vocal :).</font></i>  <i><font color="gray">Nesse caso, h√° alguma interfer√™ncia de outras fontes.</font></i>  <i><font color="gray">Como se trata de uma grava√ß√£o ao vivo, parece aceit√°vel que os vocais extra√≠dos tenham pior qualidade do que os anteriores.</font></i> <br><br><h1>  Sim, e "outra coisa" ... </h1><br><h1>  Se o sistema funciona para vocais, por que n√£o aplic√°-lo a outros instrumentos ...? </h1><br>  O artigo j√° √© bastante amplo, mas, devido ao trabalho realizado, voc√™ merece ouvir a √∫ltima demonstra√ß√£o.  Com exatamente a mesma l√≥gica de quando extra√≠mos vocais, podemos tentar dividir a m√∫sica est√©reo em componentes (bateria, baixo, voz, outros), fazendo algumas mudan√ßas em nosso modelo e, √© claro, tendo o treinamento apropriado :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Obrigado pela leitura.  Como nota final: como voc√™ pode ver, o modelo atual de nossa rede neural convolucional n√£o √© t√£o especial.  O sucesso deste trabalho foi determinado pela <b>Feature Engineering</b> e pelo puro processo de teste de hip√≥teses, sobre o qual escreverei em artigos futuros! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt441090/">https://habr.com/ru/post/pt441090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt441076/index.html">Otimiza√ß√£o de script com o Webpack SplitChunksPlugin</a></li>
<li><a href="../pt441078/index.html">A LG apresentar√° um smartphone com um alto-falante de tela OLED: algumas palavras sobre o novo dispositivo e tecnologia</a></li>
<li><a href="../pt441082/index.html">Ingressos para Mars custam menos de US $ 500.000</a></li>
<li><a href="../pt441084/index.html">Para onde foram os primeiros adotantes?</a></li>
<li><a href="../pt441088/index.html">Desenvolvedor, lembre-se: o tr√°fego do seu aplicativo est√° sendo monitorado</a></li>
<li><a href="../pt441092/index.html">Embedded World 2019 - a maior exposi√ß√£o de eletr√¥nicos embarcados</a></li>
<li><a href="../pt441096/index.html">Artigos de leitura do simulador</a></li>
<li><a href="../pt441098/index.html">Profundidades do SIEM: correla√ß√µes prontas para uso. Parte 4. Modelo do sistema como um contexto de regras de correla√ß√£o</a></li>
<li><a href="../pt441102/index.html">Kaspersky Mobile Talks - uma reuni√£o para desenvolvedores avan√ßados</a></li>
<li><a href="../pt441104/index.html">Obter informa√ß√µes e ignorar a autentica√ß√£o de dois fatores em cart√µes banc√°rios do TOP-10 (Ucr√¢nia)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>