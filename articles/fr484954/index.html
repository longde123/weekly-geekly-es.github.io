<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüéì üèÖ üë©üèª‚Äçüî¨ Guide de d√©pannage visuel pour Kubernetes üì¶ üö∂üèº üëô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : Cet article fait partie du mat√©riel disponible gratuitement du projet learnk8s , qui vous apprend √† travailler avec les soci√©t√©s Kub...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guide de d√©pannage visuel pour Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/484954/">  <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: Cet article fait partie du mat√©riel disponible gratuitement du projet <a href="https://learnk8s.io/">learnk8s</a> , qui vous apprend √† travailler avec les soci√©t√©s Kubernetes et les administrateurs individuels.</i>  <i>Dans ce document, Daniele Polencic, le chef de projet, partage une instruction claire sur les mesures √† prendre en cas de probl√®mes g√©n√©raux pour les applications s'ex√©cutant dans le cluster K8s.</i> <br><br><img src="https://habrastorage.org/webt/ch/5u/xa/ch5uxanj-3ivwqu88swqyoi6bsu.png"><br><br>  TL; DR: voici un diagramme qui vous aidera √† d√©boguer le d√©ploiement dans Kubernetes: <a name="habracut"></a><br><br> <a href=""><img src="https://habrastorage.org/webt/4r/qp/si/4rqpsie8dnplkqxahaqntpi2ssw.png"></a> <br><br>  <i>Organigramme pour rechercher et corriger les erreurs dans un cluster.</i>  <i>Dans l'original (en anglais), il est disponible en <a href="https://learnk8s.io/a/troubleshooting-kubernetes.pdf">PDF</a> et <a href="">en image</a> .</i> <br><br>  Lors du d√©ploiement d'une application sur Kubernetes, vous devez g√©n√©ralement d√©finir trois composants: <br><br><ul><li>  <b>Le d√©ploiement</b> est une recette pour cr√©er des copies d'une application appel√©e pods; </li><li>  <b>Service</b> - un √©quilibreur de charge interne qui r√©partit le trafic entre les pods; </li><li>  <b>Entr√©e</b> - une description de la fa√ßon dont le trafic sera achemin√© du monde ext√©rieur vers le service. </li></ul><br>  Voici un bref r√©sum√© graphique: <br><br>  1) Dans Kubernetes, les applications re√ßoivent le trafic du monde ext√©rieur via deux couches d'√©quilibreurs de charge: interne et externe. <br><br><img src="https://habrastorage.org/webt/3v/cy/z9/3vcyz9a-2ciiqbh9he7idgvo7uy.png"><br><br>  2) L'√©quilibreur interne est appel√© Service, externe - Ingress. <br><br><img src="https://habrastorage.org/webt/23/mn/zc/23mnzcfo_b3niccdivn4bc4vzei.png"><br><br>  3) Le d√©ploiement cr√©e des pods et les surveille (ils ne sont pas cr√©√©s manuellement). <br><br><img src="https://habrastorage.org/webt/4j/c2/h9/4jc2h9pgzbxmkon4ewkbeuf0vhc.png"><br><br>  Supposons que vous souhaitiez d√©ployer une application simple √† la <i>Hello World</i> .  La configuration YAML pour cela ressemblera √† ceci: <br><br><pre><code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment # &lt;&lt;&lt; metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service # &lt;&lt;&lt; metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 selector: name: app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress # &lt;&lt;&lt; metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: app servicePort: 80 path: /</code> </pre> <br>  La d√©finition est assez longue et il est facile de se tromper sur la fa√ßon dont les composants sont li√©s les uns aux autres. <br><br>  Par exemple: <br><br><ul><li>  Quand faut-il utiliser le port 80 et quand - 8080? </li><li>  Dois-je cr√©er un nouveau port pour chaque service afin qu'ils n'entrent pas en conflit? </li><li>  Les noms d'√©tiquette sont-ils importants?  Devraient-ils √™tre les m√™mes partout? </li></ul><br>  Avant de nous concentrer sur le d√©bogage, rappelons comment les trois composants sont li√©s les uns aux autres.  Commen√ßons par le d√©ploiement et le service. <br><br><h2>  D√©ploiement de connexion'a et Service'a </h2><br>  Vous serez surpris, mais les d√©ploiements et les services ne sont en aucun cas connect√©s.  Au lieu de cela, le service pointe directement vers des pods contournant le d√©ploiement. <br><br>  Ainsi, nous sommes int√©ress√©s par la fa√ßon dont les pods et les services sont li√©s les uns aux autres.  Trois choses √† retenir: <br><br><ol><li>  Un <code>selector</code> service doit correspondre √† au moins une √©tiquette de pod. </li><li>  <code>targetPort</code> doit correspondre au <code>containerPort</code> conteneur √† l'int√©rieur du pod. </li><li>  <code>port</code> service <code>port</code> peut √™tre n'importe quoi.  Diff√©rents services peuvent utiliser le m√™me port car ils ont des adresses IP diff√©rentes. </li></ol><br>  Le diagramme suivant repr√©sente tout ce qui pr√©c√®de sous forme graphique: <br><br>  1) Imaginez que le service dirige le trafic vers un certain pod: <br><br><img src="https://habrastorage.org/webt/2a/e5/8f/2ae58fcgoi7aifmcr5rl_0bseym.png"><br><br>  2) Lors de la cr√©ation d'un pod, vous devez sp√©cifier <code>containerPort</code> pour chaque conteneur dans les pods: <br><br><img src="https://habrastorage.org/webt/xc/fa/ow/xcfaowomhbtqhebhodgzhzrkupc.png"><br><br>  3) Lors de la cr√©ation du service, vous devez sp√©cifier le <code>port</code> et <code>targetPort</code> .  <i>Mais lequel se connecte au conteneur?</i> <br><br><img src="https://habrastorage.org/webt/vg/wj/nd/vgwjnde0xyzdblwamomfxjaxb40.png"><br><br>  4) Via <code>targetPort</code> .  Il doit correspondre √† <code>containerPort</code> . <br><br><img src="https://habrastorage.org/webt/q4/yx/qn/q4yxqnkxxilupalikahmqqp09x8.png"><br><br>  5) Supposons que le port 3000 soit ouvert dans le conteneur, puis la valeur <code>targetPort</code> devrait √™tre la m√™me. <br><br><img src="https://habrastorage.org/webt/cq/tj/-s/cqtj-srznih70qh7bxs3w_l7bis.png"><br><br>  Dans le fichier YAML, les √©tiquettes et les <code>ports</code> / <code>targetPort</code> doivent correspondre: <br><br><pre> <code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt; spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 # &lt;&lt;&lt; --- apiVersion: v1 kind: Service metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 # &lt;&lt;&lt; selector: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt;</code> </pre> <br>  <i>Qu'en est-il de la <code>track: canary</code> en haut de la section D√©ploiement?</i>  <i>Doit-elle correspondre?</i> <br><br>  Cette √©tiquette fait r√©f√©rence au d√©ploiement et n'est pas utilis√©e par le service pour acheminer le trafic.  En d'autres termes, il peut √™tre supprim√© ou attribu√© une valeur diff√©rente. <br><br>  <i>Qu'en est-il du s√©lecteur <code>matchLabels</code> ?</i> <br><br>  <b>Il doit toujours correspondre aux √©tiquettes de Pod</b> , car il est utilis√© par Deployment pour suivre les pods. <br><br>  <i>Supposons que vous ayez effectu√© les modifications correctes.</i>  <i>Comment les v√©rifier?</i> <br><br>  Vous pouvez v√©rifier l'√©tiquette du pod √† l'aide de la commande suivante: <br><br><pre> <code class="bash hljs">kubectl get pods --show-labels</code> </pre> <br>  Ou, si les pods appartiennent √† plusieurs applications: <br><br><pre> <code class="bash hljs">kubectl get pods --selector any-name=my-app --show-labels</code> </pre> <br>  O√π <code>any-name=my-app</code> est l'√©tiquette <code>any-name: my-app</code> . <br><br>  <i>Y a-t-il des difficult√©s?</i> <br><br>  Vous pouvez vous connecter au pod!  Pour ce faire, utilisez la commande <code>port-forward</code> dans kubectl.  Il vous permet de vous connecter au service et de v√©rifier la connexion. <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service name&gt; 3000:80</code> </pre> <br>  Ici: <br><br><ul><li>  <code>service/&lt;service name&gt;</code> - nom du service;  dans notre cas, c'est <code>my-service</code> ; </li><li>  3000 - le port que vous souhaitez ouvrir sur l'ordinateur; </li><li>  80 - port sp√©cifi√© dans le champ <code>port</code> du service. </li></ul><br>  Si vous pouvez √©tablir une connexion, les param√®tres sont corrects. <br><br>  Si la connexion n'a pas pu √™tre √©tablie, il y a un probl√®me avec les √©tiquettes ou les ports ne correspondent pas. <br><br><h2>  Connexion du service et de l'entr√©e </h2><br>  L'√©tape suivante pour fournir l'acc√®s √† l'application est li√©e √† la configuration d'Ingress.  Ingress doit savoir comment trouver le service, puis trouver les pods et diriger le trafic vers eux.  Ingress trouve le service souhait√© par son nom et son port ouvert. <br><br>  Dans la description d'Ingress and Service, deux param√®tres doivent correspondre: <br><br><ol><li>  <code>servicePort</code> dans Ingress doit correspondre au param√®tre de <code>port</code> dans Service; </li><li>  <code>serviceName</code> dans Ingress doit correspondre au champ de <code>name</code> dans Service. </li></ol><br>  Le sch√©ma suivant r√©sume la connexion des ports: <br><br>  1) Comme vous le savez d√©j√†, le service √©coute sur un certain <code>port</code> : <br><br><img src="https://habrastorage.org/webt/9q/t0/fz/9qt0fzsyme9mnrd4ki07ezamnkg.png"><br><br>  2) Ingress a un param√®tre appel√© <code>servicePort</code> : <br><br><img src="https://habrastorage.org/webt/rn/du/yw/rnduyw4xvfmpjmhup8fy9ao2d1a.png"><br><br>  3) Ce param√®tre ( <code>servicePort</code> ) doit toujours correspondre au <code>port</code> dans la d√©finition de service: <br><br><img src="https://habrastorage.org/webt/1d/ap/ty/1daptyulxphnbb2dt6uben-lnzk.png"><br><br>  4) Si le port 80 est sp√©cifi√© dans Service, alors <code>servicePort</code> doit √©galement √™tre 80: <br><br><img src="https://habrastorage.org/webt/nc/mi/dl/ncmidlxiegmtmhtozaxxqhznaya.png"><br><br>  En pratique, vous devez faire attention aux lignes suivantes: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: my-service # &lt;&lt;&lt; spec: ports: - port: 80 # &lt;&lt;&lt; targetPort: 8080 selector: any-name: my-app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: my-service # &lt;&lt;&lt; servicePort: 80 # &lt;&lt;&lt; path: /</code> </pre> <br>  <i>Comment v√©rifier si Ingress fonctionne?</i> <br><br>  Vous pouvez utiliser la m√©thode avec <code>kubectl port-forward</code> , mais au lieu du service, vous devez vous connecter au contr√¥leur Ingress. <br><br>  Vous devez d'abord trouver le nom du pod avec le contr√¥leur Ingress: <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Localisez le module Ingress (il peut faire r√©f√©rence √† un autre espace de noms) et ex√©cutez la commande <code>describe</code> pour conna√Ætre les num√©ros de port: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc \ --namespace kube-system \ | grep Ports Ports: 80/TCP, 443/TCP, 18080/TCP</code> </pre> <br>  Enfin, connectez-vous au pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  D√©sormais, chaque fois que vous envoyez une demande au port 3000 sur l'ordinateur, elle sera redirig√©e vers le port 80 du pod avec le contr√¥leur Ingress.  En allant sur <a href="http://localhost:3000/">http: // localhost: 3000</a> , vous devriez voir la page cr√©√©e par l'application. <br><br><h2>  R√©sum√© du port </h2><br>  Rappelons-nous √† nouveau quels ports et √©tiquettes doivent correspondre: <br><br><ol><li>  Le s√©lecteur dans la d√©finition de service doit correspondre √† l'√©tiquette du pod; </li><li>  <code>targetPort</code> dans la d√©finition de service doit correspondre au <code>containerPort</code> conteneur √† l'int√©rieur du pod; </li><li>  <code>port</code> dans la d√©finition de Service peut √™tre n'importe quoi.  Diff√©rents services peuvent utiliser le m√™me port car ils ont des adresses IP diff√©rentes; </li><li>  <code>servicePort</code> Ingress doit correspondre au <code>port</code> dans la d√©finition de service; </li><li>  Le nom du service doit correspondre au champ <code>serviceName</code> dans Ingress. </li></ol><br>  H√©las, il ne suffit pas de savoir structurer correctement votre configuration YAML. <br><br>  <i>Que se passe-t-il en cas de probl√®me?</i> <br><br>  Peut-√™tre que le pod ne d√©marre pas ou qu'il plante. <br><br><h2>  3 √©tapes pour r√©soudre les √©checs d'applications dans Kubernetes </h2><br>  Avant de d√©boguer un d√©ploiement, vous devez avoir une bonne compr√©hension du fonctionnement de Kubernetes. <br><br>  Puisqu'il y a trois composants dans chaque application t√©l√©charg√©e sur K8, ils doivent √™tre d√©bogu√©s dans un certain ordre, en commen√ßant par le bas. <br><br><ol><li>  Vous devez d'abord vous assurer que les modules fonctionnent, puis ... </li><li>  V√©rifiez si le service fournit du trafic aux pods, puis ... </li><li>  V√©rifiez si Ingress est correctement configur√©. </li></ol><br>  Pr√©sentation visuelle: <br><br>  1) Commencez la recherche des probl√®mes par le bas.  V√©rifiez d'abord que les modules ont les √©tats <code>Ready</code> et <code>Running</code> : <br><br><img src="https://habrastorage.org/webt/f-/lc/iz/f-lcizmfav5sb1sc7hvu8samwes.png"><br><br>  2) Si les pods sont <code>Ready</code> , vous devez savoir si le service r√©partit le trafic entre les pods: <br><br><img src="https://habrastorage.org/webt/yg/we/bu/ygwebumu8ga9lmd7krineuw38mq.png"><br><br>  3) Enfin, vous devez analyser la connexion entre le service et Ingress: <br><br><img src="https://habrastorage.org/webt/y7/ze/uz/y7zeuzkhzgsdzcjmng2ei4fjrxg.png"><br><br><h2>  1. Diagnostic des pods </h2><br>  Dans la plupart des cas, le probl√®me vient du pod.  Assurez-vous que les modules sont <code>Ready</code> et en <code>Running</code> .  Vous pouvez le v√©rifier en utilisant la commande: <br><br><pre> <code class="bash hljs">kubectl get pods NAME READY STATUS RESTARTS AGE app1 0/1 ImagePullBackOff 0 47h app2 0/1 Error 0 47h app3-76f9fcd46b-xbv4k 1/1 Running 1 47h</code> </pre> <br>  Dans la sortie de la commande ci-dessus, le dernier module est r√©pertori√© comme en <code>Running</code> d' <code>Running</code> et <code>Ready</code> , mais pas pour les deux autres. <br><br>  <i>Comment comprendre ce qui a mal tourn√©?</i> <br><br>  Il existe quatre commandes utiles pour diagnostiquer les modules: <br><br><ol><li>  <code>kubectl logs &lt; pod'&gt;</code> vous permet d'extraire les journaux des conteneurs du pod; </li><li>  <code>kubectl describe pod &lt; pod'&gt;</code> vous permet d'afficher une liste des √©v√©nements associ√©s au pod; </li><li>  <code>kubectl get pod &lt; pod'&gt;</code> vous permet d'obtenir la configuration YAML du <code>kubectl get pod &lt; pod'&gt;</code> stock√©e dans Kubernetes; </li><li>  <code>kubectl exec -ti &lt; pod'&gt; bash</code> vous permet d'ex√©cuter un shell de commande interactif dans l'un des conteneurs de pod </li></ol><br>  <i>Lequel choisir?</i> <br><br>  Le fait est qu'il n'y a pas d'√©quipe universelle.  Une combinaison de ces √©l√©ments doit √™tre utilis√©e. <br><br><h3>  Probl√®mes courants de pod </h3><br>  Il existe deux principaux types d'erreurs de module: les erreurs de d√©marrage et les erreurs d'ex√©cution. <br><br>  Erreurs de d√©marrage: <br><br><ul><li> <code>ImagePullBackoff</code> </li> <li> <code>ImageInspectError</code> </li> <li> <code>ErrImagePull</code> </li> <li> <code>ErrImageNeverPull</code> </li> <li> <code>RegistryUnavailable</code> </li> <li> <code>InvalidImageName</code> </li> </ul><br>  Erreurs d'ex√©cution: <br><br><ul><li> <code>CrashLoopBackOff</code> </li> <li> <code>RunContainerError</code> </li> <li> <code>KillContainerError</code> </li> <li> <code>VerifyNonRootError</code> </li> <li> <code>RunInitContainerError</code> </li> <li> <code>CreatePodSandboxError</code> </li> <li> <code>ConfigPodSandboxError</code> </li> <li> <code>KillPodSandboxError</code> </li> <li> <code>SetupNetworkError</code> </li> <li> <code>TeardownNetworkError</code> </li> </ul><br>  Certaines erreurs sont plus courantes que d'autres.  Voici quelques erreurs courantes et comment les corriger. <br><br><h4>  ImagePullBackOff </h4><br>  Cette erreur appara√Æt lorsque Kubernetes ne parvient pas √† obtenir une image pour l'un des conteneurs de pod.  Voici les trois raisons les plus courantes √† cela: <br><br><ol><li>  Le nom de l'image n'est pas sp√©cifi√© correctement - par exemple, vous y avez fait une erreur ou l'image n'existe pas; </li><li>  Une balise inexistante pour l'image est sp√©cifi√©e; </li><li>  L'image est stock√©e dans un registre priv√© et Kubernetes n'est pas autoris√© √† y acc√©der. </li></ol><br>  Les deux premi√®res raisons sont faciles √† √©liminer - il suffit de corriger le nom et la balise de l'image.  Dans ce dernier cas, vous devez saisir les informations d'identification du registre priv√© dans Secret et ajouter des liens vers celui-ci dans les pods.  La documentation de Kubernetes <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">a un exemple</a> de comment cela peut √™tre fait. <br><br><h4>  CrashLoopBackOff </h4><br>  Kubenetes g√©n√©rera une erreur CrashLoopBackOff si le conteneur ne peut pas d√©marrer.  Cela se produit g√©n√©ralement lorsque: <br><br><ol><li>  Il y a une erreur dans l'application qui l'emp√™che de d√©marrer; </li><li>  Le conteneur n'est pas <a href="https://stackoverflow.com/questions/41604499/my-kubernetes-pods-keep-crashing-with-crashloopbackoff-but-i-cant-find-any-lo">configur√© correctement</a> ; </li><li>  Le test de vivacit√© a √©chou√© trop de fois. </li></ol><br>  Vous devez essayer d'acc√©der aux journaux du conteneur pour d√©couvrir la raison de son √©chec.  Si l'acc√®s aux journaux est difficile, car le conteneur red√©marre trop rapidement, vous pouvez utiliser la commande suivante: <br><br><pre> <code class="bash hljs">kubectl logs &lt;pod-name&gt; --previous</code> </pre> <br>  Il affiche les messages d'erreur d'une r√©incarnation de conteneur pr√©c√©dente. <br><br><h4>  RunContainerError </h4><br>  Cette erreur se produit lorsque le conteneur ne d√©marre pas.  Il correspond √† l'instant pr√©c√©dant le lancement de l'application.  Habituellement, sa cause est une configuration incorrecte, par exemple: <br><br><ul><li>  Tenter de monter un volume inexistant, tel que ConfigMap ou Secrets; </li><li>  essayez de monter un volume en lecture seule en lecture-√©criture. </li></ul><br>  La commande <code>kubectl describe pod &lt;pod-name&gt;</code> est bien adapt√©e pour analyser de telles erreurs. <br><br><h3>  Gousses en attente </h3><br>  Apr√®s la cr√©ation, le pod reste dans l'√©tat <code>Pending</code> . <br><br>  <i>Pourquoi cela se produit-il?</i> <br><br>  Voici les raisons possibles (je suppose que le planificateur fonctionne bien): <br><br><ol><li>  Le cluster ne dispose pas de suffisamment de ressources, telles que la puissance de traitement et la m√©moire, pour ex√©cuter le module. </li><li>  L'objet <code>ResourceQuota</code> est install√© dans l'espace de noms correspondant et la cr√©ation d'un pod entra√Ænera l'espace de noms au-del√† du quota. </li><li>  Le pod est li√© √† Pending <code>PersistentVolumeClaim</code> . </li></ol><br>  Dans ce cas, il est recommand√© d'utiliser la commande <code>kubectl describe</code> et de v√©rifier la section <code>Events</code> : <br><br><pre> <code class="bash hljs">kubectl describe pod &lt;pod name&gt;</code> </pre> <br>  En cas d'erreurs li√©es √† <code>ResourceQuotas</code> , il est recommand√© d'afficher les journaux de cluster √† l'aide de la commande <br><br><pre> <code class="bash hljs">kubectl get events --sort-by=.metadata.creationTimestamp</code> </pre> <br><h3>  Pods non pr√™ts </h3><br>  Si le module est r√©pertori√© comme <code>Running</code> d' <code>Running</code> , mais n'est pas √† l'√©tat <code>Ready</code> , la <i>sonde de</i> pr√©paration √©choue. <br><br>  Lorsque cela se produit, le pod ne se connecte pas au service et le trafic n'y circule pas.  Le test de pr√©paration a √©chou√© en raison de probl√®mes d'application.  Dans ce cas, pour trouver l'erreur, vous devez analyser la section <code>Events</code> dans la sortie de la commande <code>kubectl describe</code> . <br><br><h2>  2. Diagnostic des services </h2><br>  Si les modules sont r√©pertori√©s comme <code>Running</code> d' <code>Running</code> et <code>Ready</code> , mais qu'il n'y a toujours pas de r√©ponse de l'application, vous devez v√©rifier les param√®tres de service. <br><br>  Les services sont impliqu√©s dans l'acheminement du trafic vers les pods en fonction de leurs √©tiquettes.  Par cons√©quent, la premi√®re chose √† faire est de v√©rifier combien de pods fonctionnent avec le service.  Pour ce faire, vous pouvez v√©rifier les points de terminaison du service: <br><br><pre> <code class="bash hljs">kubectl describe service &lt;service-name&gt; | grep Endpoints</code> </pre> <br>  Le point de terminaison est une paire de valeurs de la forme <code>&lt;IP-:&gt;</code> , et au moins une telle paire doit √™tre pr√©sente dans la sortie (c'est-√†-dire qu'au moins un pod fonctionne avec le service). <br><br>  Si la section <code>Endpoins</code> vide, deux options sont possibles: <br><br><ol><li>  il n'y a pas de pods avec la bonne √©tiquette (astuce: v√©rifiez si l'espace de noms est s√©lectionn√© correctement); </li><li>  Il y a une erreur dans les √©tiquettes de service dans le s√©lecteur. </li></ol><br>  Si vous voyez une liste de points de terminaison, mais ne pouvez toujours pas acc√©der √† l'application, le coupable probable est l'erreur dans <code>targetPort</code> dans la description du service. <br><br>  <i>Comment v√©rifier le bon fonctionnement du service?</i> <br><br>  Quel que soit le type de service, vous pouvez utiliser la commande <code>kubectl port-forward</code> pour vous y connecter: <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service-name&gt; 3000:80</code> </pre> <br>  Ici: <br><br><ul><li>  <code>&lt;service-name&gt;</code> - le nom du service; </li><li>  3000 - le port que vous ouvrez sur l'ordinateur; </li><li>  80 - port c√¥t√© service. </li></ul><br><h2>  3. Diagnostics d'entr√©e </h2><br>  Si vous lisez jusqu'√† cet endroit, alors: <br><br><ul><li>  les pods sont r√©pertori√©s comme en <code>Running</code> d' <code>Running</code> et <code>Ready</code> ; </li><li>  le service distribue avec succ√®s le trafic entre les pods. </li></ul><br>  Cependant, vous ne pouvez toujours pas ¬´tendre la main¬ª √† l'application. <br><br>  Cela signifie que, tr√®s probablement, le contr√¥leur Ingress est mal configur√©.  Le contr√¥leur Ingress √©tant un composant tiers du cluster, il existe diff√©rentes m√©thodes de d√©bogage en fonction de son type. <br><br>  Mais avant de recourir √† des outils sp√©ciaux pour configurer Ingress, vous pouvez faire quelque chose de tr√®s simple.  Ingress utilise <code>serviceName</code> et <code>servicePort</code> pour se connecter au service.  Vous devez v√©rifier qu'ils sont correctement configur√©s.  Vous pouvez le faire en utilisant la commande: <br><br><pre> <code class="bash hljs">kubectl describe ingress &lt;ingress-name&gt;</code> </pre> <br>  Si la colonne <code>Backend</code> est vide, le risque d'erreur de configuration est √©lev√©.  Si les backends sont en place, mais qu'il n'y a toujours pas acc√®s √† l'application, le probl√®me peut √™tre li√© √†: <br><br><ul><li>  Entrer les param√®tres d'accessibilit√© √† partir d'Internet public; </li><li>  param√®tres d'accessibilit√© du cluster √† partir d'Internet public. </li></ul><br>  Vous pouvez identifier les probl√®mes d'infrastructure en vous connectant directement au module Ingress.  Pour ce faire, recherchez d'abord le pod du contr√¥leur Ingress (il peut se trouver dans un espace de noms diff√©rent): <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Utilisez la commande <code>describe</code> pour d√©finir le port: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc --namespace kube-system \ | grep Ports</code> </pre> <br>  Enfin, connectez-vous au pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  D√©sormais, toutes les demandes de port 3000 sur l'ordinateur seront redirig√©es vers le port 80 pod. <br><br>  <i>√áa marche maintenant?</i> <br><br><ul><li>  Si c'est le cas, le probl√®me est li√© √† l'infrastructure.  Il est n√©cessaire de savoir exactement comment le trafic est achemin√© vers le cluster. </li><li>  Sinon, le probl√®me vient du contr√¥leur Ingress. </li></ul><br>  Si vous ne pouvez pas faire fonctionner le contr√¥leur Ingress, vous devrez le d√©boguer. <br><br>  Il existe de nombreuses vari√©t√©s de contr√¥leurs Ingress.  Les plus populaires sont Nginx, HAProxy, Traefik, etc. <i>(pour plus d'informations sur les solutions existantes, consultez <a href="https://habr.com/ru/company/flant/blog/447180/">notre revue</a> - environ la traduction). Vous devez</i> utiliser le guide de d√©pannage dans la documentation du contr√¥leur correspondant.  √âtant donn√© <a href="https://github.com/kubernetes/ingress-nginx">qu'Ingress Nginx</a> est le contr√¥leur Ingress le plus populaire, nous avons inclus quelques conseils sur la r√©solution des probl√®mes connexes dans cet article. <br><br><h3>  D√©bogage d'un contr√¥leur Ingress Nginx </h3><br><br>  Le projet Ingress-nginx a un <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">plugin</a> officiel <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">pour kubectl</a> .  La commande <code>kubectl ingress-nginx</code> peut √™tre utilis√©e pour: <br><br><ul><li>  analyse des journaux, backends, certificats, etc.; </li><li>  connexion √† Ingress; </li><li>  √©tudier la configuration actuelle. </li></ul><br>  Les trois √©quipes suivantes vous y aideront: <br><br><ul><li>  <code>kubectl ingress-nginx lint</code> - v√©rifie <code>nginx.conf</code> ; </li><li>  <code>kubectl ingress-nginx backend</code> - examine le backend (similaire √† <code>kubectl describe ingress &lt;ingress-name&gt;</code> ); </li><li>  <code>kubectl ingress-nginx logs</code> - v√©rifie les journaux. </li></ul><br>  Veuillez noter que dans certains cas, il peut √™tre n√©cessaire de sp√©cifier l'espace de noms correct pour le contr√¥leur Ingress √† l'aide de l' <code>--namespace &lt;name&gt;</code> . <br><br><h2>  R√©sum√© </h2><br>  Le diagnostic de Kubernetes peut √™tre une t√¢che ardue si vous ne savez pas par o√π commencer.  Le probl√®me doit toujours √™tre abord√© selon le principe ascendant: commencer par les pods, puis aller au service et √† Ingress.  Les m√©thodes de d√©bogage d√©crites dans l'article peuvent √™tre appliqu√©es √† d'autres objets, tels que: <br><br><ul><li>  Jobs inactifs et CronJobs; </li><li>  StatefulSets et DaemonSets. </li></ul><br>  Merci √† <a href="https://github.com/errge">Gergely Risko</a> , <a href="https://medium.com/%40weibeld">Daniel Weibel</a> et <a href="https://www.linkedin.com/in/charles-christyraj-0bab8a36/">Charles Christyraj</a> pour leurs pr√©cieux commentaires et ajouts. <br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/436112/">Plugin Kubectl-debug pour le d√©bogage dans les pods Kubernetes</a> ¬ª; </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/443458/">6 bugs syst√®me divertissants dans le fonctionnement de Kubernetes [et leur solution]</a> ¬ª; </li><li>  ¬´ <a href="https://habr.com/ru/company/flant/blog/462707/">Outils pour les d√©veloppeurs d'applications fonctionnant sur Kubernetes</a> ¬ª; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/471892/">6 histoires pratiques de notre vie quotidienne SRE</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr484954/">https://habr.com/ru/post/fr484954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr484936/index.html">Connaissances et comp√©tences dans l'√©quipe: trouver, voir, pomper</a></li>
<li><a href="../fr484944/index.html">Qu'est-ce que je suis dans ACID ou ne nous convient pas</a></li>
<li><a href="../fr484946/index.html">Mod√©lisation GPR</a></li>
<li><a href="../fr484948/index.html">NEC a sorti un c√¢ble sous-marin avec un record de 20 paires de fibres optiques</a></li>
<li><a href="../fr484952/index.html">Remplacement de Redux par des observables et des crochets React</a></li>
<li><a href="../fr484964/index.html">Configuration de l'√©quilibrage de charge sur InfoWatch Traffic Monitor</a></li>
<li><a href="../fr484966/index.html">Mod√®le pr√™t √† l'emploi pour les tests √† l'aide de Spring</a></li>
<li><a href="../fr484968/index.html">WPF DataGrid. Battez-vous pour le mod√®le</a></li>
<li><a href="../fr484972/index.html">Wine 5.0 est sorti</a></li>
<li><a href="../fr484974/index.html">Tuiles Wang pour la simulation de la machine de Turing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>