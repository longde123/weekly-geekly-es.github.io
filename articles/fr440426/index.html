<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìç ü§ó üéí Une nouvelle approche pour comprendre la pens√©e machine üï£ üåí ‚ÜôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les r√©seaux de neurones sont connus pour leur incompr√©hensibilit√© - l'ordinateur peut donner une bonne r√©ponse, mais ne peut pas expliquer ce qui l'a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Une nouvelle approche pour comprendre la pens√©e machine</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440426/"><h3>  Les r√©seaux de neurones sont connus pour leur incompr√©hensibilit√© - l'ordinateur peut donner une bonne r√©ponse, mais ne peut pas expliquer ce qui l'a conduit √† cette conclusion.  Bin Kim d√©veloppe un ¬´traducteur humain¬ª pour que si l'intelligence artificielle tombe en panne, nous pouvons le comprendre. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/069/0a0/2c1/0690a02c197e3af97ec6e5cec9754fd6.jpg"><br>  <i>Bean Kim, chercheuse √† Google Brain, d√©veloppe un moyen de questionner un syst√®me d'apprentissage automatique sur ses d√©cisions.</i> <br><br>  Si le m√©decin vous dit que vous avez besoin d'une intervention chirurgicale, vous voudrez savoir pourquoi - et vous vous attendez √† ce que son explication vous semble significative, m√™me si vous n'avez pas √©t√© form√© en tant que m√©decin.  Been Kim, chercheur √† Google Brain, pense que nous devrions pouvoir attendre la m√™me chose de l'intelligence artificielle (IA).  Sp√©cialiste de l'apprentissage automatique (MO) ¬´interpr√©t√©¬ª, elle souhaite cr√©er une IA capable d'expliquer ses actions √† tous. <br><a name="habracut"></a><br>  Depuis une dizaine d'ann√©es, la technologie des r√©seaux de neurones derri√®re l'IA a commenc√© √† se r√©pandre de plus en plus, elle a pu transformer tous les processus, du tri des e-mails √† la recherche de nouveaux m√©dicaments, gr√¢ce √† sa capacit√© √† apprendre des donn√©es et √† rechercher des mod√®les en elles.  Mais cette capacit√© a un pi√®ge inexplicable: la complexit√© m√™me qui permet aux r√©seaux de neurones modernes avec une formation approfondie d'apprendre avec succ√®s √† conduire une voiture et √† reconna√Ætre la fraude √† l'assurance, il est presque impossible pour les experts de comprendre les principes de leur travail.  Si un r√©seau neuronal est form√© pour rechercher des patients √† risque de cancer du foie ou de schizophr√©nie - et un tel syst√®me appel√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Patient a √©t√©</a> lanc√© au Mount Sinai Hospital √† New York en 2015 - alors il n'y a aucun moyen de d√©terminer lesquels les donn√©es pr√©sentent le r√©seau neuronal ¬´fait attention¬ª.  Cette ¬´connaissance¬ª est r√©partie sur plusieurs couches de neurones artificiels, dont chacun a des connexions avec des centaines ou des milliers d'autres neurones. <br><br>  Alors que de plus en plus d'industries tentent d'automatiser ou d'am√©liorer leurs processus de prise de d√©cision en utilisant l'IA, ce probl√®me de ¬´bo√Æte noire¬ª semble √™tre moins un d√©faut technologique, mais plus un d√©faut fondamental.  Un projet de la DARPA appel√© XAI (abr√©viation de ¬´Explanable AI¬ª, eXplainable AI) explore activement cette question, et l'interpr√©tabilit√© se d√©place des lignes de front de la recherche dans le domaine des MO plus pr√®s de son centre.  ¬´L'IA se trouve √† un moment critique lorsque nous, l'humanit√©, essayons de d√©terminer si cette technologie nous convient¬ª, explique Kim.  ¬´Si nous ne r√©solvons pas le probl√®me de l'interpr√©tabilit√©, je pense que nous ne pourrons pas continuer avec cette technologie, et peut-√™tre que nous la refuserons tout simplement.¬ª <br><br>  Kim et ses coll√®gues de Google Brain ont r√©cemment d√©velopp√© le syst√®me Testing with Concept Activation Vectors (TCAV), qu'elle d√©crit comme un traducteur humain qui permet √† l'utilisateur de poser une question sur la bo√Æte noire de l'IA. dans quelle mesure un certain concept de haut niveau √©tait impliqu√© dans la prise de d√©cision.  Par exemple, si le syst√®me MO est form√© pour trouver des images de z√®bres, une personne pourrait demander √† TCAV de d√©crire la contribution du concept de ¬´rayures¬ª au processus d√©cisionnel. <br><br>  TCAV a √©t√© initialement test√© sur des mod√®les form√©s pour reconna√Ætre les images, mais il fonctionne √©galement avec des mod√®les con√ßus pour le traitement de texte ou certaines t√¢ches de visualisation de donn√©es, par exemple les graphiques EEG.  "Il est g√©n√©ralis√© et simple - il peut √™tre connect√© √† de nombreux mod√®les diff√©rents", explique Kim. <br><br>  Quanta a parl√© √† Kim de ce que signifie l'interpr√©tabilit√©, qui en a besoin et pourquoi c'est important. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4c8/f00/6c4/4c8f006c4ccacab3d668b366f5cbecee.jpg"><br><br>  <b>Dans votre carri√®re, vous vous √™tes concentr√© sur ¬´l'interpr√©tabilit√©¬ª pour le MO.</b>  <b>Mais que signifie exactement ce terme?</b> <br><br>  L'interpr√©tabilit√© a deux branches.  La premi√®re est l'interpr√©tabilit√© pour la science: si vous consid√©rez le r√©seau neuronal comme un objet d'√©tude, vous pouvez y mener des exp√©riences scientifiques pour vraiment comprendre tous les tenants et aboutissants du mod√®le, les raisons de sa r√©action, etc. <br><br>  La deuxi√®me branche, sur laquelle je concentre principalement mes efforts, est l'interpr√©tabilit√© pour cr√©er une IA capable de r√©pondre aux questions.  Vous n'avez pas besoin de comprendre chaque petit d√©tail du mod√®le.  Mais notre objectif est d'en comprendre suffisamment pour que cet outil puisse √™tre utilis√© en toute s√©curit√©. <br><br>  <b>Mais comment croire en un syst√®me, si l'on ne comprend pas parfaitement comment il fonctionne?</b> <br><br>  Je vais vous donner une analogie.  Supposons que dans ma cour il y ait un arbre que je veux couper.  J'ai une tron√ßonneuse pour √ßa.  Je ne comprends pas exactement comment fonctionne une tron√ßonneuse.  Mais les instructions disent: "Quelque chose √† manipuler avec soin pour ne pas se couper."  Ayant des instructions, je ferais mieux d'utiliser une tron√ßonneuse au lieu d'une scie √† main - cette derni√®re est plus facile √† comprendre, mais je devrais la voir pendant cinq heures. <br><br>  <b>Vous comprenez ce que signifie ¬´couper¬ª, m√™me si vous ne savez pas tout sur le m√©canisme qui rend cela possible.</b> <br><br>  Oui  Le but de la deuxi√®me branche de l'interpr√©tabilit√© est le suivant: pouvons-nous comprendre l'outil suffisamment pour √™tre s√ªr √† utiliser?  Et nous pouvons cr√©er cette compr√©hension en confirmant que les connaissances humaines utiles se refl√®tent dans l'instrument. <br><br>  <b>Mais comment la ¬´r√©flexion de la connaissance humaine¬ª rend-elle la bo√Æte noire de l'IA plus compr√©hensible?</b> <br><br>  Voici un autre exemple.  Si le m√©decin utilise le mod√®le MO pour poser un diagnostic de cancer, le m√©decin devra savoir que le mod√®le ne s√©lectionne pas simplement une corr√©lation al√©atoire dans les donn√©es dont nous n'avons pas besoin.  Une fa√ßon de v√©rifier cela est de confirmer que le mod√®le MO fait √† peu pr√®s la m√™me chose que le m√©decin ferait.  Autrement dit, pour montrer que les connaissances diagnostiques du m√©decin se refl√®tent dans le mod√®le. <br><br>  Par exemple, si un m√©decin recherche une instance cellulaire appropri√©e pour le diagnostic du cancer, il cherchera quelque chose appel√© ¬´la glande fusionn√©e¬ª.  Il tiendra √©galement compte d'indicateurs tels que l'√¢ge du patient et s'il a d√©j√† subi une chimioth√©rapie.  Ces facteurs, ou concepts, seront pris en compte par un m√©decin qui tentera de diagnostiquer un cancer.  Si nous pouvons montrer que le mod√®le MO attire √©galement l'attention sur eux, alors le mod√®le sera plus compr√©hensible, car il refl√©tera les connaissances humaines des m√©decins. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/8Bi-EhFPSLk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  <b>C'est ce que TCAV traite - montre quels concepts de haut niveau le mod√®le MO utilise pour la prise de d√©cision?</b> <br><br>  Oui  Avant cela, les m√©thodes d'interpr√©tabilit√© expliquaient uniquement ce que fait le r√©seau neuronal en termes de ¬´caract√©ristiques d'entr√©e¬ª.  Qu'est-ce que cela signifie?  Si vous avez une image, chacun de ses pixels sera une fonction d'entr√©e.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jan Lekun</a> (pionnier du deep learning, directeur de la recherche en IA chez Facebook), a d√©clar√© qu'il consid√©rait ces mod√®les comme super-interpr√©tables, car vous pouvez regarder chaque n≈ìud du r√©seau neuronal et voir les valeurs num√©riques pour chacune des fonctionnalit√©s d'entr√©e.  Pour les ordinateurs, cela peut convenir, mais les gens pensent diff√©remment.  Je ne vous dis pas "Regardez les pixels de 100 √† 200, leurs valeurs RVB sont 0,2 et 0,3".  Je dis: "Ceci est une image d'un chien tr√®s hirsute."  Les gens communiquent de cette fa√ßon - √† travers des concepts. <br><br>  <b>Comment TCAV se traduit-il entre les fonctionnalit√©s d'entr√©e et les concepts?</b> <br><br>  Revenons √† l'exemple d'un m√©decin utilisant le mod√®le MO, qui a d√©j√† √©t√© form√© pour classer les images d'√©chantillons cellulaires en fonction du cancer.  En tant que m√©decin, vous devez savoir √† quel point le concept de ¬´glandes fusionn√©es¬ª √©tait important pour que le mod√®le fasse des pr√©dictions positives pour le cancer.  Tout d'abord, vous collectez, disons, 20 images qui montrent des exemples de glandes fusionn√©es.  Ensuite, vous connectez ces exemples √©tiquet√©s au mod√®le. <br><br>  Ensuite, TCAV r√©alise en soi ce que l'on appelle  "Contr√¥le de sensibilit√©".  Lorsque nous ajoutons ces images √©tiquet√©es des glandes fusionn√©es, dans quelle mesure la probabilit√© d'une pr√©diction positive du cancer augmente-t-elle?  La r√©ponse peut √™tre estim√©e par un nombre de 0 √† 1. Et ce seront vos points dans TCAV.  Si la probabilit√© augmentait, ce concept √©tait important pour le mod√®le.  Sinon, ce concept n'est pas important. <br><br>  <b>¬´Concept¬ª est un terme vague.</b>  <b>Y a-t-il des concepts qui ne fonctionneront pas avec TCAV?</b> <br><br>  Si vous ne pouvez pas d√©crire un concept √† l'aide d'un sous-ensemble de votre ensemble de donn√©es, cela ne fonctionnera pas.  Si votre mod√®le MO est form√© aux images, le concept doit √™tre exprim√© visuellement.  Si, par exemple, je veux exprimer visuellement le concept de l'amour, ce sera assez difficile √† faire. <br><br>  Nous v√©rifions √©galement soigneusement le concept.  Nous avons une proc√©dure de v√©rification statistique qui rejette le vecteur concept s'il a un effet √©quivalent √† al√©atoire sur le mod√®le.  Si votre concept ne passe pas ce test, alors TCAV dira: "Je ne sais pas, ce concept ne ressemble pas √† quelque chose d'important pour le mod√®le." <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e7d/2e4/bb5/e7d2e4bb5cb0093890c6938abdb5acdc.jpg"><br><br>  <b>Le projet TCAV est-il plus ax√© sur le renforcement de la confiance en IA que sur la g√©n√©ralisation de sa compr√©hension?</b> <br><br>  Non, - et je vais vous expliquer pourquoi, car cette diff√©rence est tr√®s subtile. <br><br>  De nombreuses √©tudes dans le domaine des sciences cognitives et de la psychologie, nous savons que les gens sont tr√®s confiants.  Cela signifie qu'il est tr√®s facile de tromper une personne en la for√ßant √† croire en quelque chose.  Le but de l'interpr√©tabilit√© MO est le contraire.  Elle consiste √† informer une personne qu'il n'est pas s√ªr d'utiliser un syst√®me particulier.  Le but est de d√©couvrir la v√©rit√©.  Par cons√©quent, ¬´confiance¬ª n'est pas le bon mot. <br><br>  <b>Le but de l'interpr√©tabilit√© est donc de d√©couvrir des failles potentielles dans le raisonnement de l'IA?</b> <br><br>  Oui, exactement. <br><br>  <b>Comment peut-elle r√©v√©ler les d√©fauts?</b> <br><br>  TCAV peut √™tre utilis√© pour poser au mod√®le une question sur des concepts qui ne sont pas li√©s au domaine de la recherche.  Revenons √† l'exemple des m√©decins utilisant l'IA pour pr√©dire la probabilit√© de cancer.  Les m√©decins peuvent soudain penser: ¬´Apparemment, la machine donne des pr√©dictions positives pour la pr√©sence d'un cancer pour de nombreuses images dans lesquelles la couleur est l√©g√®rement d√©cal√©e vers le bleu.  Nous pensons que ce facteur ne doit pas √™tre pris en consid√©ration. ¬ª  Et s'ils obtiennent un score TCAV √©lev√© pour le bleu, cela signifie qu'ils ont trouv√© un probl√®me dans leur mod√®le MO. <br><br>  <b>TCAV est con√ßu pour √™tre suspendu √† des syst√®mes d'IA existants qui ne peuvent pas √™tre interpr√©t√©s.</b>  <b>Pourquoi ne pas cr√©er imm√©diatement des syst√®mes interpr√©t√©s au lieu de bo√Ætes noires?</b> <br><br>  Il y a une branche de l'√©tude de l'interpr√©tabilit√©, se concentrant sur la cr√©ation de mod√®les initialement interpr√©t√©s qui refl√®tent le raisonnement d'une personne.  Mais je pense que oui: maintenant, nous sommes d√©j√† pleins de mod√®les d'IA pr√™ts √† l'emploi qui sont d√©j√† utilis√©s pour r√©soudre des probl√®mes importants, et lors de leur cr√©ation, nous n'avons pas pens√© au d√©part √† l'interpr√©tabilit√©.  Si facile √† manger.  Beaucoup d'entre eux travaillent sur Google!  Vous pouvez dire: "L'interpr√©tabilit√© est si utile que nous pouvons cr√©er un autre mod√®le pour remplacer celui que vous avez."  Et bonne chance. <br><br>  Et puis que faire?  Nous devons encore traverser ce moment crucial pour d√©cider si cette technologie nous est utile ou non.  Par cons√©quent, je travaille sur des m√©thodes d'interpr√©tabilit√© post-formation.  Si quelqu'un vous a donn√© un mod√®le et que vous ne pouvez pas le changer, comment abordez-vous la t√¢che de g√©n√©rer des explications de son comportement afin de pouvoir l'utiliser en toute s√©curit√©?  C'est exactement ce que fait TCAV. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/05d/9ba/522/05d9ba52265cd534871c8f62f4ce05f7.jpg"><br><br>  <b>TCAV permet aux gens de demander √† l'IA l'importance de certains concepts.</b>  <b>Mais que se passe-t-il si nous ne savons pas quoi demander - et si nous voulons que l'IA explique simplement?</b> <br><br>  En ce moment, nous travaillons sur un projet qui peut automatiquement trouver des concepts pour vous.  Nous l'appelons DTCAV - le TCAV d'ouverture.  Mais je pense que le principal probl√®me d'interpr√©tabilit√© est que les gens participent √† ce processus, et que nous permettons aux gens et aux machines de communiquer. <br><br>  Dans de nombreux cas, lorsqu'ils travaillent avec des applications dont beaucoup d√©pend, les experts dans un domaine particulier ont d√©j√† une liste de concepts qui sont importants pour eux.  Chez Google Brain, nous sommes constamment confront√©s √† cela dans les applications m√©dicales de l'IA.  Ils n'ont pas besoin d'un ensemble de concepts - ils veulent fournir des mod√®les conceptuels qui les int√©ressent.  Nous travaillons avec un m√©decin traitant la r√©tinopathie diab√©tique, les maladies oculaires, et quand nous lui avons parl√© du TCAV, elle √©tait tr√®s heureuse car elle avait d√©j√† tout un tas d'hypoth√®ses sur ce que le mod√®le peut faire, et maintenant elle peut v√©rifier toutes les questions qui se sont pos√©es.  C'est un √©norme avantage et une mani√®re tr√®s centr√©e sur l'utilisateur de mettre en ≈ìuvre l'apprentissage machine collaboratif. <br><br>  <b>Vous pensez que sans interpr√©tabilit√©, l'humanit√© peut simplement abandonner la technologie de l'IA.</b>  <b>Compte tenu de ses opportunit√©s, √©valuez-vous vraiment une telle option comme r√©elle?</b> <br><br>  Oui  C'est exactement ce qui s'est produit avec les syst√®mes experts.  Dans les ann√©es 80, nous avons d√©termin√© qu'ils sont moins chers que les gens pour r√©soudre certains probl√®mes.  Et qui utilise des syst√®mes experts aujourd'hui?  Personne.  Et apr√®s cela, l'hiver IA est arriv√©. <br><br>  Jusqu'√† pr√©sent, cela ne semble pas probable, tant de battage m√©diatique et d'argent sont investis dans l'IA.  Mais √† long terme, je pense que l'humanit√© peut d√©cider - peut-√™tre par peur, peut-√™tre par manque de preuves - que cette technologie ne nous convient pas.  C'est possible. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr440426/">https://habr.com/ru/post/fr440426/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr440414/index.html">√Ä propos de linter, de la qualit√© du code, de la qualit√© en g√©n√©ral et de la gestion de la qualit√©</a></li>
<li><a href="../fr440416/index.html">Colonie. Chapitre 25: Soir√©e</a></li>
<li><a href="../fr440420/index.html">Bienvenue au Devleads Meetup 21 f√©vrier</a></li>
<li><a href="../fr440422/index.html">Lorsque vous √™tes responsable de la qualit√© du cadeau. L'histoire d'une exp√©rience blockchain</a></li>
<li><a href="../fr440424/index.html">Algorithme de pens√©e et de conscience</a></li>
<li><a href="../fr440428/index.html">SMAA: lissage morphologique sous-pixel am√©lior√©</a></li>
<li><a href="../fr440430/index.html">D'o√π vient le slogan "Don't Be Evil"</a></li>
<li><a href="../fr440432/index.html">Vendredi SciFi sur les m√©tiers du futur: ¬´Real Girls¬ª</a></li>
<li><a href="../fr440434/index.html">L'industrie automobile russe: la voie des technologies additives</a></li>
<li><a href="../fr440436/index.html">T√¢ches pratiques Java - pour les cours et autres activit√©s</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>