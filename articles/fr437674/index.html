<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌵 📓 👲🏾 Méthodes de reconnaissance d'objets 3D pour les véhicules sans pilote. Rapport Yandex 🚵🏽 👩🏽‍🤝‍👩🏼 🚪</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Les voitures sans pilote ne peuvent se passer de comprendre ce qui se trouve autour et où exactement. En décembre dernier, le développeur Viktor Otlig...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Méthodes de reconnaissance d'objets 3D pour les véhicules sans pilote. Rapport Yandex</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437674/">  Les voitures sans pilote ne peuvent se passer de comprendre ce qui se trouve autour et où exactement.  En décembre dernier, le développeur Viktor Otliga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">vitonka a</a> fait une présentation sur la détection d'objets 3D au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Data-Christmas tree</a> .  Victor travaille en direction des véhicules sans pilote Yandex, dans le groupe traitant de la situation de la circulation (et enseigne également au ShAD).  Il a expliqué comment nous résolvons le problème de la reconnaissance des autres usagers de la route dans un nuage de points en trois dimensions, en quoi ce problème diffère de la reconnaissance des objets dans une image et comment tirer profit du partage de différents types de capteurs. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Bonjour à tous!  Je m'appelle Victor Otliga, je travaille au bureau Yandex de Minsk et je développe des véhicules sans pilote.  Aujourd'hui, je vais parler d'une tâche assez importante pour les drones - la reconnaissance des objets 3D autour de nous. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/ky/wn/hu/kywnhuprs-iq34tx4rgohnvwqm8.jpeg"><br><br>  Pour rouler, vous devez comprendre ce qui vous entoure.  Je vais vous dire brièvement quels capteurs et capteurs sont utilisés sur les véhicules sans pilote et lesquels nous utilisons.  Je vais vous dire quelle est la tâche de détection des objets 3D et comment mesurer la qualité de la détection.  Ensuite, je vais vous dire sur quoi cette qualité peut être mesurée.  Et puis je ferai une brève revue des bons algorithmes modernes, y compris ceux sur lesquels nos solutions sont basées.  Et au final - petits résultats, une comparaison de ces algorithmes, dont le nôtre. <br><br><img src="https://habrastorage.org/webt/pk/sf/vo/pksfvoxtqwlyf6k-wido8ixektg.jpeg"><br><br>  Voilà à quoi ressemble notre prototype fonctionnel de voiture sans pilote.  Un tel taxi peut être loué par toute personne sans chauffeur dans la ville d'Innopolis en Russie, ainsi qu'à Skolkovo.  Et si vous regardez bien, il y a un gros dé sur le dessus.  Qu'y a-t-il à l'intérieur? <br><br><img src="https://habrastorage.org/webt/ei/3c/mq/ei3cmqe3l4kjfzjju-s1ndavkwi.jpeg"><br><br>  À l'intérieur d'un simple ensemble de capteurs.  Il existe une antenne GNSS et GSM pour déterminer où se trouve la voiture et communiquer avec le monde extérieur.  Où sans un capteur aussi classique qu'un appareil photo.  Mais aujourd'hui, nous nous intéresserons aux lidars. <br><br><img src="https://habrastorage.org/webt/cm/s6/pd/cms6pdxey7d4ykgktlc8lvyd8fc.jpeg"><br><br><img src="https://habrastorage.org/webt/un/ia/bg/uniabgh6yomlr2rourl7ynj3coc.jpeg"><br><br>  Lidar produit environ un tel nuage de points autour de lui, qui ont trois coordonnées.  Et vous devez travailler avec eux.  Je vais vous expliquer comment, à l'aide d'une image de caméra et d'un nuage lidar, reconnaître des objets. <br><br><img src="https://habrastorage.org/webt/yx/rt/si/yxrtsiwsnt_gcdgjbz42pp8lqvc.jpeg"><br><br>  Quel est le défi?  L'image de la caméra entre dans l'entrée, la caméra est synchronisée avec le lidar.  Il serait étrange d'utiliser l'image de l'appareil photo il y a une seconde, de prendre le nuage lidar d'un moment complètement différent et d'essayer de reconnaître des objets dessus. <br><br><img src="https://habrastorage.org/webt/xa/g6/aj/xag6ajjiigpo5m737kklxiwkmb0.jpeg"><br><br>  Nous synchronisons en quelque sorte les caméras et les lidars, c'est une tâche difficile distincte, mais nous y parvenons avec succès.  Ces données entrent en entrée, et à la fin, nous voulons obtenir des boîtes, des boîtes de délimitation qui limitent l'objet: piétons, cyclistes, voitures et autres usagers de la route et pas seulement. <br><br>  La tâche était fixée.  Comment allons-nous l'évaluer? <br><br><img src="https://habrastorage.org/webt/ew/hm/wd/ewhmwdgektceevdbmgu838n5wui.jpeg"><br><br>  Le problème de la reconnaissance 2D d'objets dans une image a été largement étudié. <br><br><img src="https://habrastorage.org/webt/-c/ri/cu/-cricuguziob4idgyr_u-ihfq-e.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Vous pouvez utiliser des métriques standard ou leurs analogues.  Il y a un coefficient Jacquard ou une intersection sur l'union, un merveilleux coefficient qui montre à quel point nous avons détecté un objet.  Nous pouvons prendre une boîte où, comme nous le supposons, l'objet est situé, et une boîte où il se trouve réellement.  Comptez cette métrique.  Il existe des seuils standard - disons que pour les voitures, ils prennent souvent un seuil de 0,7.  Si cette valeur est supérieure à 0,7, nous pensons avoir réussi à détecter l'objet, que l'objet est là.  Nous sommes grands, nous pouvons aller plus loin. <br><br>  De plus, afin de détecter un objet et de comprendre qu'il se trouve quelque part, nous voudrions prendre une sorte de confiance que nous voyons vraiment l'objet là-bas, et le mesurer aussi.  Vous pouvez mesurer simple, considérer la précision moyenne.  Vous pouvez prendre la courbe de rappel de précision et la zone en dessous et dire: plus elle est grande, mieux c'est. <br><br><img src="https://habrastorage.org/webt/p7/jk/z5/p7jkz5xnyyhehx99e73nijtb798.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Habituellement, pour mesurer la qualité de la détection 3D, ils prennent un ensemble de données et le divisent en plusieurs parties, car les objets peuvent être proches ou plus éloignés, ils peuvent être partiellement masqués par autre chose.  Par conséquent, l'échantillon de validation est souvent divisé en trois parties.  Objets faciles à détecter, de complexité moyenne et complexes, distants ou fortement obscurcis.  Et ils mesurent séparément en trois parties.  Et dans les résultats de la comparaison, nous prendrons également une telle partition. <br><br><img src="https://habrastorage.org/webt/6v/d9/tx/6vd9tx38777pdvhxt4iij_rc4xu.jpeg"><br><br>  Vous pouvez mesurer la qualité comme en 3D, un analogue d'intersection sur union, mais pas le rapport des surfaces, mais, par exemple, les volumes.  Mais une voiture sans pilote, en règle générale, ne se soucie pas vraiment de ce qui se passe dans la coordonnée Z. Nous pouvons prendre une vue plongeante d'en haut et prendre une sorte de métrique, comme si nous regardions tout cela en 2D.  L'homme navigue plus ou moins en 2D, et un véhicule sans pilote est le même.  La hauteur de la boîte n'est pas très importante. <br><br><img src="https://habrastorage.org/webt/gs/gg/js/gsggjsnlbhi0qd_ppdpnvqvlncm.jpeg"><br><br>  Que mesurer? <br><br><img src="https://habrastorage.org/webt/-p/vq/qn/-pvqqnyi_yovi_bit9x6lergyva.jpeg"><br><br>  Probablement tous ceux qui ont au moins fait face à la tâche de détecter en 3D par le nuage lidar ont entendu parler d'un ensemble de données tel que KITTI. <br><br><img src="https://habrastorage.org/webt/bp/bz/wn/bpbzwnd-1o69xdoonrm1k81vndo.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Dans certaines villes d'Allemagne, un ensemble de données a été enregistré, une voiture équipée de capteurs est allée, elle avait des capteurs GPS, des caméras et des lidars.  Ensuite, il a été balisé sur environ 8 000 scènes et divisé en deux parties.  Une partie est la formation, sur laquelle tout le monde peut se former, et la seconde est la validation, afin de mesurer les résultats.  L'échantillon de validation KITTI est considéré comme une mesure de qualité.  Tout d'abord, il y a un tableau des leaders sur le site du jeu de données KITTI, vous pouvez y envoyer votre décision, vos résultats sur le jeu de données de validation et comparer avec les décisions d'autres acteurs du marché ou chercheurs.  Mais cet ensemble de données est également disponible publiquement, vous pouvez télécharger, ne le dire à personne, vérifier le vôtre, comparer avec vos concurrents, mais ne pas télécharger publiquement. <br><br><img src="https://habrastorage.org/webt/12/mu/8_/12mu8_z9hlu8-zuvhlm1bofiko4.jpeg"><br><br>  Les ensembles de données externes sont bons, vous n'avez pas à y consacrer votre temps et vos ressources, mais en règle générale, une voiture qui a voyagé en Allemagne peut être équipée de capteurs complètement différents.  Et c'est toujours bien d'avoir votre propre jeu de données interne.  De plus, il est plus difficile d'étendre un jeu de données externe au détriment des autres, mais il est plus facile de gérer le vôtre.  Par conséquent, nous utilisons le merveilleux service Yandex.Tolok. <br><br><img src="https://habrastorage.org/webt/i4/ha/ns/i4hansnkczrkhpeq9_liyn8t9bk.jpeg"><br><br>  Nous avons finalisé notre système de tâches spéciales.  À l'utilisateur qui veut aider avec le balisage et obtenir une récompense pour cela, nous distribuons une image de la caméra, distribuons un nuage lidar que vous pouvez faire pivoter, zoomer, dézoomer et lui demander de mettre des boîtes qui limitent nos zones de délimitation afin qu'une voiture ou un piéton y pénètre ou autre chose.  Ainsi, nous collectons des échantillons internes pour un usage personnel. <br><br>  Supposons que nous ayons décidé quelle tâche nous allons résoudre, comment nous supposerons que nous l'avons fait bien ou mal.  Nous avons pris quelque part les données. <br><br>  Quels sont les algorithmes?  Commençons par 2D.  La tâche de détection 2D est très bien connue et étudiée. <br><br><img src="https://habrastorage.org/webt/6x/vq/ta/6xvqtadjyarjzn9h9v0xz6cqrny.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Certes, beaucoup de gens connaissent l'algorithme SSD, qui est l'une des méthodes de pointe pour détecter les objets 2D, et en principe, nous pouvons supposer que, d'une certaine manière, le problème de la détection d'objets dans l'image est assez bien résolu.  Si quelque chose, nous pouvons utiliser ces résultats comme une sorte d'informations supplémentaires. <br><br>  Mais notre nuage lidar a ses propres caractéristiques qui le distinguent grandement de l'image.  Premièrement, elle est très clairsemée.  Si l'image est une structure dense, les pixels sont proches, tout est dense, alors le nuage est très mince, il n'y a pas tellement de points et il n'a pas de structure régulière.  Purement physiquement, il y a beaucoup plus de points près de là que dans la distance, et plus vous allez loin, moins il y a de points, moins il y a de précision, plus il est difficile de déterminer quelque chose. <br><br>  Eh bien, les points, en principe, du cloud viennent dans un ordre incompréhensible.  Personne ne garantit qu'un point sera toujours antérieur à un autre.  Ils viennent dans un ordre relativement aléatoire.  Vous pouvez en quelque sorte accepter de les trier ou de les réorganiser à l'avance, puis de soumettre les modèles à l'entrée, mais cela sera assez gênant, vous devez prendre le temps de les modifier, etc. <br><br>  Nous aimerions trouver un système qui sera invariant à nos problèmes, résoudra tous ces problèmes.  Heureusement, l'année dernière, CVPR a présenté un tel système.  Il y avait une telle architecture - PointNet.  Comment fonctionne-t-elle? <br><br><img src="https://habrastorage.org/webt/ny/ay/np/nyaynp7lrsoopcumwsbjmm9ynik.jpeg"><br><br>  Un nuage de n points arrive à l'entrée, chacun avec trois coordonnées.  Ensuite, chaque point est en quelque sorte normalisé par une petite transformation spéciale.  De plus, il est conduit à travers un réseau entièrement connecté afin d'enrichir ces points de signes.  Ensuite, la transformation a lieu et, à la fin, elle s'enrichit en plus.  À un moment donné, n points sont obtenus, mais chacun a environ 1024 caractéristiques, ils sont en quelque sorte standardisés.  Mais jusqu'à présent, nous n'avons pas résolu le problème concernant l'invariance des déplacements, des virages, etc.  Ici, il est proposé de faire un maximum de regroupement, de prendre le maximum parmi les points sur chaque canal et d'obtenir un vecteur de 1024 signes, qui sera un descripteur de notre nuage, qui contiendra des informations sur l'ensemble du nuage.  Et puis avec ce descripteur, vous pouvez faire beaucoup de choses différentes. <br><br><img src="https://habrastorage.org/webt/qj/lr/hc/qjlrhclpvd2smfl3q2j28bft0sw.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Par exemple, vous pouvez le coller aux descripteurs de points individuels et résoudre le problème de segmentation, pour chaque point afin de déterminer à quel objet il appartient.  C'est juste une route ou une personne ou une voiture.  Et voici les résultats de l'article. <br><br><img src="https://habrastorage.org/webt/kh/-o/cw/kh-ocwqrkgelcs958zio30nc7ek.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Vous remarquerez peut-être que cet algorithme fait un très bon travail.  En particulier, j'aime beaucoup cette petite table dans laquelle certaines des données sur le comptoir ont été jetées, et il a néanmoins déterminé où sont les pieds et où se trouve le comptoir.  Et cet algorithme, en particulier, peut être utilisé comme une brique pour construire d'autres systèmes. <br><br>  Une approche qui utilise cela est l'approche Frustum PointNets ou l'approche pyramidale tronquée.  L'idée est quelque chose comme ça: reconnaissons les objets en 2D, nous sommes bons à le faire. <br><br><img src="https://habrastorage.org/webt/lh/xb/uv/lhxbuvbemlftwpvufw7hbpmmhog.jpeg"><br><br>  Puis, sachant comment fonctionne la caméra, nous pouvons estimer dans quelle zone l'objet qui nous intéresse, la machine, peut se trouver.  Pour projeter, découpez uniquement cette zone, et déjà sur elle, résolvez le problème de trouver un objet intéressant, par exemple une machine.  C'est beaucoup plus facile que de rechercher n'importe quel nombre de voitures dans le cloud.  La recherche d'une voiture exactement dans le même nuage semble être beaucoup plus claire et plus efficace. <br><br><img src="https://habrastorage.org/webt/lt/t-/0v/ltt-0v8iobuju4yfoj-ips2uka8.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  L'architecture ressemble à ceci.  Tout d'abord, nous sélectionnons en quelque sorte les régions qui nous intéressent, dans chaque région, nous faisons une segmentation, puis nous résolvons le problème de trouver une boîte englobante qui limite l'objet qui nous intéresse. <br><br><img src="https://habrastorage.org/webt/2w/lj/8g/2wlj8gt2m9vyiljomkkz0tjvd_y.jpeg"><br><br>  L'approche a fait ses preuves.  Sur les photos, vous pouvez voir que cela fonctionne assez bien, mais il présente également des inconvénients.  L'approche est à deux niveaux, de ce fait, elle peut être lente.  Nous devons d'abord appliquer des réseaux et reconnaître des objets 2D, puis couper, puis résoudre le problème de segmentation et d'allocation du cadre de délimitation sur un morceau du nuage, afin qu'il puisse fonctionner un peu lentement. <br><br>  Une autre approche.  Pourquoi ne transformons-nous pas notre nuage en une sorte de structure qui ressemble à une image?  L'idée est la suivante: regardons-la d'en haut et échantillonnons notre nuage lidar.  Nous obtenons des cubes d'espaces. <br><br><img src="https://habrastorage.org/webt/sc/7l/3h/sc7l3h0ewwcfmiuejsxnl7ozx5c.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  À l'intérieur de chaque cube, nous avons obtenu quelques points.  Nous pouvons compter certaines fonctionnalités dessus, mais nous pouvons utiliser PointNet, qui pour chaque morceau d'espace comptera une sorte de descripteur.  Nous obtiendrons un voxel, chaque voxel a une description caractéristique, et il ressemblera plus ou moins à une structure dense, comme une image.  Nous pouvons déjà créer différentes architectures, par exemple une architecture de type SSD pour détecter des objets. <br><br><img src="https://habrastorage.org/webt/x_/tb/c_/x_tbc_hsv_yuahyllkmlxijsowe.jpeg"><br><br>  Cette dernière approche, qui a été l'une des toutes premières approches pour combiner les données de plusieurs capteurs.  Ce serait un péché d'utiliser uniquement des données lidar alors que nous avons également des données de caméra.  L'une de ces approches est appelée le réseau de détection d'objets 3D à vues multiples.  Son idée est la suivante: alimenter trois canaux de données d'entrée à l'entrée d'un grand réseau. <br><br><img src="https://habrastorage.org/webt/tz/xv/ty/tzxvty86li7jozjwozhqlun8r9a.jpeg"><br><h5>  <sup><sub><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien depuis la diapositive</a></sub></sup> </h5><br>  Il s'agit d'une image de la caméra et, en deux versions, d'un nuage lidar: d'en haut, avec une vue plongeante et une sorte de vue de face, ce que nous voyons devant nous.  Nous soumettons cela à l'entrée du neurone, et il configurera tout en lui-même, nous donnera le résultat final - l'objet. <br><br>  Je veux comparer ces modèles.  Sur l'ensemble de données KITTI, sur les lecteurs de validation, la qualité est évaluée en pourcentage de précision moyenne. <br><br><img src="https://habrastorage.org/webt/pt/ny/8x/ptny8xzcmojphtluvg5pxm1-mlq.jpeg"><br><br>  Vous remarquerez peut-être que F-PointNet fonctionne assez bien et assez rapidement, bat tout le monde dans différents domaines - du moins selon les auteurs. <br><br>  Notre approche est basée sur plus ou moins toutes les idées que j'ai énumérées.  Si vous comparez, vous obtenez l'image suivante.  Si nous n'occupons pas la première place, alors au moins la seconde.  De plus, sur les objets difficiles à détecter, nous entrons dans les chefs.  Et surtout, notre approche est assez rapide.  Cela signifie qu'il est déjà assez bien applicable pour les systèmes en temps réel, et il est particulièrement important pour un véhicule sans pilote de surveiller ce qui se passe sur la route et de mettre en évidence tous ces objets. <br><br><img src="https://habrastorage.org/webt/ad/dx/bk/addxbko462x7r4doq22akwwkpik.jpeg"><br><br>  En conclusion - un exemple de notre détecteur: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/celzhoWh2TE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  On voit que la situation est compliquée: certains objets sont fermés, certains ne sont pas visibles par la caméra.  Piétons, cyclistes.  Mais le détecteur s'en sort assez bien.  Je vous remercie! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr437674/">https://habr.com/ru/post/fr437674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr437660/index.html">Mise à jour du profil à vie dans Visual Studio 2019 Preview 2</a></li>
<li><a href="../fr437664/index.html">Récupération du composé</a></li>
<li><a href="../fr437666/index.html">Annonce de l'aperçu de F # 4.6</a></li>
<li><a href="../fr437670/index.html">Mises à jour du backend MSVC dans Visual Studio 2019 Preview 2: nouvelles optimisations, améliorations OpenMP et Build Throughput</a></li>
<li><a href="../fr437672/index.html">cyberd: Calculer les connaissances du web3</a></li>
<li><a href="../fr437676/index.html">Les universités et les accélérateurs d'entreprise comme levier pour lancer une start-up B2B aux États-Unis</a></li>
<li><a href="../fr437680/index.html">Ma collection de bricolage sur Youtube</a></li>
<li><a href="../fr437682/index.html">Écriture d'un autre outil de création de modèles Kubernetes</a></li>
<li><a href="../fr437684/index.html">Algorithme suprême - Compendium biaisé</a></li>
<li><a href="../fr437686/index.html">Learning Go: écrire un messager p2p avec un cryptage de bout en bout</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>