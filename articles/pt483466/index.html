<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ∏ üò™ üëÆ Introduzindo o m√©todo de retropropaga√ß√£o üçö üöü üö¥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal! As f√©rias de Ano Novo chegaram ao fim, o que significa que estamos novamente prontos para compartilhar material √∫til com voc√™. Uma tradu√ß...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introduzindo o m√©todo de retropropaga√ß√£o</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>Ol√° pessoal!</i>  <i>As f√©rias de Ano Novo chegaram ao fim, o que significa que estamos novamente prontos para compartilhar material √∫til com voc√™.</i>  <i>Uma tradu√ß√£o deste artigo foi preparada em antecipa√ß√£o ao lan√ßamento de um novo fluxo no curso <a href="https://otus.pw/h0mh/">"Algoritmos para desenvolvedores"</a> .</i> <i><br><br></i>  <i>Vamos l√°!</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br>  O m√©todo de erro de propaga√ß√£o traseira √© provavelmente o componente mais fundamental de uma rede neural.  Foi descrito pela primeira vez na d√©cada de 1960 e quase 30 anos depois foi popularizado por Rumelhart, Hinton e Williams em um artigo intitulado <a href="https://www.nature.com/articles/323533a0">"Aprendendo representa√ß√µes por erros de propaga√ß√£o traseira"</a> . <a name="habracut"></a><br><br>  O m√©todo √© usado para treinar efetivamente uma rede neural usando a chamada regra de cadeia (a regra de diferencia√ß√£o de uma fun√ß√£o complexa).  Simplificando, ap√≥s cada passagem pela rede, a propaga√ß√£o de retorno realiza uma passagem na dire√ß√£o oposta e ajusta os par√¢metros do modelo (pesos e deslocamentos). <br><br>  Neste artigo, gostaria de considerar em detalhes do ponto de vista matem√°tico o processo de aprendizado e otimiza√ß√£o de uma rede neural simples de quatro camadas.  Acredito que isso ajudar√° o leitor a entender como a retropropaga√ß√£o funciona, bem como a perceber seu significado. <br><br><h3>  Definindo um modelo de rede neural </h3><br>  A rede neural de quatro camadas consiste em quatro neur√¥nios na camada de entrada, quatro neur√¥nios nas camadas ocultas e 1 neur√¥nio na camada de sa√≠da. <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>Uma imagem simples de uma rede neural de quatro camadas.</i> <br><br><h3>  Camada de entrada </h3><br>  Na figura, os neur√¥nios roxos representam a entrada.  Podem ser quantidades escalares simples ou mais complexas - vetores ou matrizes multidimensionais. <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>Equa√ß√£o que descreve as entradas xi.</i> <br><br>  O primeiro conjunto de ativa√ß√µes (a) √© igual aos valores de entrada.  "Ativa√ß√£o" √© o valor de um neur√¥nio ap√≥s a aplica√ß√£o da fun√ß√£o de ativa√ß√£o.  Veja abaixo para mais detalhes. <br><br><h3>  Camadas ocultas </h3><br>  Os valores finais nos neur√¥nios ocultos (na figura verde) s√£o calculados usando entradas ponderadas em zl na camada I e em ativa√ß√µes <sup>I</sup> na camada L. Para as camadas 2 e 3, as equa√ß√µes ser√£o as seguintes: <br><br>  Para l = 2: <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br>  Para l = 3: <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup> e W <sup>3</sup> s√£o os pesos nas camadas 2 e 3 eb <sup>2</sup> eb <sup>3</sup> s√£o as compensa√ß√µes nessas camadas. <br><br>  As ativa√ß√µes a <sup>2</sup> e a <sup>3</sup> s√£o calculadas usando a fun√ß√£o de ativa√ß√£o f.  Por exemplo, essa fun√ß√£o f √© n√£o linear (como <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigm√≥ide</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> e <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">tangente hiperb√≥lica</a> ) e permite que a rede estude padr√µes complexos nos dados.  N√£o vamos nos deter sobre como as fun√ß√µes de ativa√ß√£o funcionam, mas se voc√™ estiver interessado, recomendo a leitura deste maravilhoso <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">artigo</a> . <br><br>  Se voc√™ observar atentamente, ver√° que todos os x, z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> , a <sup>3</sup> , W <sup>1</sup> , W <sup>2</sup> , b <sup>1</sup> eb <sup>2</sup> n√£o t√™m os √≠ndices mais baixos mostrados na figura da rede neural de quatro camadas.  O fato √© que combinamos todos os valores de par√¢metros em matrizes agrupadas por camadas.  Essa √© uma maneira padr√£o de trabalhar com redes neurais e √© bastante confort√°vel.  No entanto, examinarei as equa√ß√µes para que n√£o haja confus√£o. <br><br>  Vamos pegar a camada 2 e seus par√¢metros como exemplo.  As mesmas opera√ß√µes podem ser aplicadas a qualquer camada da rede neural. <br>  W <sup>1</sup> √© a matriz de pesos de dimens√£o <i>(n, m)</i> , onde <i>n</i> √© o n√∫mero de neur√¥nios de sa√≠da (neur√¥nios na pr√≥xima camada) e <i>m</i> √© o n√∫mero de neur√¥nios de entrada (neur√¥nios na camada anterior).  No nosso caso, <i>n = 2</i> e <i>m = 4</i> . <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br>  Aqui, o primeiro n√∫mero no subscrito de qualquer um dos pesos corresponde ao √≠ndice de neur√¥nios na pr√≥xima camada (no nosso caso, esta √© a segunda camada oculta) e o segundo n√∫mero corresponde ao √≠ndice de neur√¥nios na camada anterior (no nosso caso, essa √© a camada de entrada). <br><br>  <i>x</i> √© o vetor de entrada da dimens√£o ( <i>m</i> , 1), onde <i>m</i> √© o n√∫mero de neur√¥nios de entrada.  No nosso caso, <i>m</i> = 4. <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup> √© o vetor de deslocamento da dimens√£o ( <i>n</i> , 1), onde <i>n</i> √© o n√∫mero de neur√¥nios na camada atual.  No nosso caso, <i>n</i> = 2. <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br>  Seguindo a equa√ß√£o para z <sup>2,</sup> podemos usar as defini√ß√µes acima de W <sup>1</sup> , x e b <sup>1</sup> para obter a equa√ß√£o z <sup>2</sup> : <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br>  Agora observe atentamente a ilustra√ß√£o da rede neural acima: <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br>  Como voc√™ pode ver, z <sup>2</sup> pode ser expresso em termos de z <sub>1</sub> <sup>2</sup> e z <sub>2</sub> <sup>2</sup> , em que z <sub>1</sub> <sup>2</sup> e z <sub>2</sub> <sup>2</sup> s√£o as somas dos produtos de cada valor de entrada x <sup>i</sup> pelo peso correspondente W <sub>ij</sub> <sup>1</sup> . <br><br>  Isso leva √† mesma equa√ß√£o para z <sup>2</sup> e prova que as representa√ß√µes da matriz z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> e a <sup>3</sup> s√£o verdadeiras. <br><br><h3>  Camada de sa√≠da </h3><br>  A √∫ltima parte da rede neural √© a camada de sa√≠da, que fornece o valor previsto.  Em nosso exemplo simples, ele √© apresentado na forma de um √∫nico neur√¥nio corado em azul e calculado da seguinte forma: <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br>  Novamente, usamos a representa√ß√£o matricial para simplificar a equa√ß√£o.  Voc√™ pode usar os m√©todos acima para entender a l√≥gica subjacente. <br><br><h3>  Distribui√ß√£o e avalia√ß√£o direta </h3><br>  As equa√ß√µes acima formam uma distribui√ß√£o direta atrav√©s da rede neural.  Aqui est√° uma r√°pida vis√£o geral: <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>(1) - camada de entrada</i> <i><br></i>  <i>(2) - o valor do neur√¥nio na primeira camada oculta</i> <i><br></i>  <i>(3) - valor de ativa√ß√£o na primeira camada oculta</i> <i><br></i>  <i>(4) - o valor do neur√¥nio na segunda camada oculta</i> <i><br></i>  <i>(5) - valor de ativa√ß√£o no segundo n√≠vel oculto</i> <i><br></i>  <i>(6) - camada de sa√≠da</i> <br><br>  A etapa final na passagem direta √© avaliar o valor de sa√≠da previsto <i>s em</i> rela√ß√£o ao valor de sa√≠da esperado <i>y</i> . <br><br>  A sa√≠da y faz parte do conjunto de dados de treinamento (x, y), onde <i>x</i> √© a entrada (como recordamos na se√ß√£o anterior). <br><br>  A estimativa entre <i>s</i> e <i>y</i> ocorre atrav√©s da fun√ß√£o de perda.  Pode ser simples como um <a href="https://en.wikipedia.org/wiki/Mean_squared_error">erro padr√£o</a> ou mais complexo como <a href="http://neuralnetworksanddeeplearning.com/chap3.html">entropia cruzada</a> . <br><br>  Chamamos essa fun√ß√£o de perda C e a denotamos da seguinte forma: <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br>  Onde o <i>custo</i> pode ser igual ao erro padr√£o, entropia cruzada ou qualquer outra fun√ß√£o de perda. <br><br>  Com base no valor de C, o modelo ‚Äúsabe‚Äù quanto seus par√¢metros precisam ser ajustados para se aproximar do valor de sa√≠da esperado de <i>y</i> .  Isso acontece usando o m√©todo de retropropaga√ß√£o. <br><br><h3>  Propaga√ß√£o traseira de erro e c√°lculo de gradientes </h3><br>  Com base em um artigo de 1989, o m√©todo de retropropaga√ß√£o: <br><br>  <i>Ajusta constantemente os pesos das conex√µes na rede para minimizar a medida da diferen√ßa entre o vetor de sa√≠da real da rede e o vetor de sa√≠da desejado</i> . <br>  e <br>  <i>... torna poss√≠vel criar novas fun√ß√µes √∫teis que distinguem a retropropaga√ß√£o dos m√©todos anteriores e mais simples ...</i> <br><br>  Em outras palavras, a retropropaga√ß√£o visa minimizar a fun√ß√£o de perda, ajustando os pesos e compensa√ß√µes da rede.  O grau de ajuste √© determinado pelos gradientes da fun√ß√£o de perda em rela√ß√£o a esses par√¢metros. <br><br>  Surge uma pergunta: <i>por que calcular gradientes</i> ? <br><br>  Para responder a essa pergunta, primeiro precisamos revisar alguns conceitos de computa√ß√£o: <br><br>  O gradiente da fun√ß√£o C (x <sup>1</sup> , x <sup>2</sup> , ..., x <sup>m</sup> ) em x √© o <a href="https://en.wikipedia.org/wiki/Partial_derivative">vetor de derivadas parciais de</a> C em <i>rela√ß√£o</i> a <i>x</i> . <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br>  A derivada da fun√ß√£o C reflete a sensibilidade a uma mudan√ßa no valor da fun√ß√£o (valor de sa√≠da) em rela√ß√£o √† mudan√ßa em seu argumento <i>x</i> ( <a href="https://en.wikipedia.org/wiki/Derivative">valor de entrada</a> ).  Em outras palavras, a derivada nos diz em qual dire√ß√£o C. est√° se movendo. <br><br>  O gradiente mostra quanto √© necess√°rio alterar o par√¢metro <i>x</i> (na dire√ß√£o positiva ou negativa) para minimizar C. <br><br>  Esses gradientes s√£o calculados usando um m√©todo chamado <a href="https://en.wikipedia.org/wiki/Chain_rule">regra de</a> cadeia. <br>  Para um peso (w <sup>jk</sup> ) <sub>l, o</sub> gradiente √©: <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>(1) Regra da cadeia</i> <i><br></i>  <i>(2) Por defini√ß√£o, m √© o n√∫mero de neur√¥nios por camada l - 1</i> <i><br></i>  <i>(3) C√°lculo derivativo</i> <i><br></i>  <i>(4) Valor final</i> <i><br></i>  <i>Um conjunto semelhante de equa√ß√µes pode ser aplicado a (b <sup>j</sup> ) <sub>l</sub></i> : <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>(1) Regra da cadeia</i> <i><br></i>  <i>(2) C√°lculo derivativo</i> <i><br></i>  <i>(3) Valor final</i> <br>  A parte comum em ambas as equa√ß√µes √© freq√ºentemente chamada de "gradiente local" e √© expressa da seguinte forma: <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br>  Um "gradiente local" pode ser facilmente determinado usando uma regra de cadeia.  N√£o vou pintar esse processo agora. <br><br>  Os gradientes permitem otimizar os par√¢metros do modelo: <br><br>  At√© que o crit√©rio de parada seja alcan√ßado, o seguinte √© executado: <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>Algoritmo para otimizar pesos e compensa√ß√µes</i> (tamb√©m chamado descida de gradiente) <br><ul><li>  Os valores iniciais de <i>w</i> e <i>b</i> s√£o selecionados aleatoriamente. </li><li>  Epsilon (e) √© a velocidade da aprendizagem.  Determina o efeito do gradiente. </li><li>  <i>w</i> e <i>b</i> s√£o representa√ß√µes matriciais de pesos e compensa√ß√µes. </li><li>  A derivada de C em rela√ß√£o a <i>w</i> ou <i>b</i> pode ser calculada usando derivadas parciais de C em rela√ß√£o a pesos ou compensa√ß√µes individuais. </li><li>  A condi√ß√£o de t√©rmino √© satisfeita assim que a fun√ß√£o de perda √© minimizada. </li></ul><br><br>  Quero dedicar a parte final desta se√ß√£o a um exemplo simples no qual calculamos o gradiente C com rela√ß√£o a um peso (w <sup>22</sup> ) <sub>2</sub> . <br><br>  Vamos dar um zoom na parte inferior da rede neural acima mencionada: <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>Representa√ß√£o visual da retropropaga√ß√£o em uma rede neural</i> <br>  O peso (w <sup>22</sup> ) <sub>2</sub> conecta (a <sup>2</sup> ) <sub>2</sub> e (z <sup>2</sup> ) <sub>2</sub> , portanto, para calcular o gradiente √© necess√°rio aplicar a regra da cadeia em (z <sup>2</sup> ) <sub>3</sub> e (a <sup>2</sup> ) <sub>3</sub> : <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br>  O c√°lculo do valor final da derivada de C a partir de (a <sup>2</sup> ) <sub>3</sub> requer conhecimento da fun√ß√£o C. Como C depende de (a <sup>2</sup> ) <sub>3</sub> , o c√°lculo da derivada deve ser simples. <br><br>  Espero que este exemplo tenha conseguido lan√ßar alguma luz sobre a matem√°tica por tr√°s do c√°lculo de gradientes.  Se voc√™ quiser saber mais, recomendo que voc√™ verifique a s√©rie de artigos da PNL de Stanford, em que Richard Socher fornece 4 √≥timas explica√ß√µes para a propaga√ß√£o posterior. <br><br><h3>  Observa√ß√£o final </h3><br>  Neste artigo, expliquei em detalhes como a propaga√ß√£o retroativa de um erro funciona sob o cap√¥ usando m√©todos matem√°ticos, como c√°lculo de gradientes, regra de cadeia etc.  O conhecimento dos mecanismos desse algoritmo fortalecer√° seu conhecimento de redes neurais e permitir√° que voc√™ se sinta confort√°vel ao trabalhar com modelos mais complexos.  Boa sorte em sua jornada de aprendizado profundo! <br><br>  <b><i>S√≥ isso.</i></b>  <b><i>Convidamos a todos para um webinar gratuito sobre o tema <a href="https://otus.pw/h0mh/">"√Årvore de segmentos: simples e r√°pido"</a> .</i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt483466/">https://habr.com/ru/post/pt483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt483448/index.html">Disney - A maior via dupla da hist√≥ria da humanidade</a></li>
<li><a href="../pt483454/index.html">Mudando de Mercurial para GIT no Atlassian Bitbucket com salvando arquivos em cir√≠lico</a></li>
<li><a href="../pt483458/index.html">Assistente de Banco de Dados GreenPig</a></li>
<li><a href="../pt483460/index.html">HowTo SQL: construindo cadeias usando fun√ß√µes de janela</a></li>
<li><a href="../pt483462/index.html">Cale a boca e pegue meu dinheiro</a></li>
<li><a href="../pt483468/index.html">Testes de integra√ß√£o de vibra√ß√£o - √© f√°cil</a></li>
<li><a href="../pt483470/index.html">Coloque blocos de forma eficiente (Pro CSS, SVG, padr√£o e muito mais)</a></li>
<li><a href="../pt483472/index.html">Exclua tudo: como apagar dados e restaurar o NVMe SSD para as configura√ß√µes de f√°brica</a></li>
<li><a href="../pt483476/index.html">A moral do transporte rob√≥tico: o problema do carrinho, riscos e consequ√™ncias</a></li>
<li><a href="../pt483478/index.html">Sol, vento e √°gua ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>