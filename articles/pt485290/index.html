<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíê ‚ÅâÔ∏è ‚ò£Ô∏è Guia simples de destila√ß√£o BERT üë®‚Äç‚úàÔ∏è üí∏ ü§∂üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se voc√™ est√° interessado em aprendizado de m√°quina, provavelmente j√° ouviu falar sobre BERT e transformadores. 


 O BERT √© um modelo de linguagem do ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guia simples de destila√ß√£o BERT</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/avito/blog/485290/"><p>  Se voc√™ est√° interessado em aprendizado de m√°quina, provavelmente j√° ouviu falar sobre BERT e transformadores. </p><br><p>  O BERT √© um modelo de linguagem do Google, mostrando resultados de ponta por uma ampla margem em v√°rias tarefas.  O BERT, e geralmente os transformadores, tornaram-se uma etapa completamente nova no desenvolvimento de algoritmos de processamento de linguagem natural (PNL).  O artigo sobre eles e as "classifica√ß√µes" para v√°rios benchmarks podem ser encontrados <a href="https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional">no site da Papers With Code</a> . </p><br><p>  H√° um problema com o BERT: √© problem√°tico usar em sistemas industriais.  O BERT-base cont√©m 110M de par√¢metros, BERT-grande - 340M.  Devido a um n√∫mero t√£o grande de par√¢metros, √© dif√≠cil fazer o download desse modelo em dispositivos com recursos limitados, como telefones celulares.  Al√©m disso, o longo tempo de infer√™ncia torna esse modelo inadequado quando a velocidade de resposta √© cr√≠tica.  Portanto, encontrar maneiras de acelerar o BERT √© um t√≥pico muito quente. </p><br><p>  Em Avito, geralmente precisamos resolver problemas de classifica√ß√£o de texto.  Essa √© uma tarefa t√≠pica de aprendizado de m√°quina aplicada que foi bem estudada.  Mas sempre h√° a tenta√ß√£o de tentar algo novo.  Este artigo nasceu de uma tentativa de aplicar o BERT nas tarefas di√°rias de aprendizado de m√°quina.  Nele, mostrarei como voc√™ pode melhorar significativamente a qualidade de um modelo existente usando o BERT sem adicionar novos dados ou complicar o modelo. </p><br><p><img src="https://habrastorage.org/webt/c_/po/z2/c_poz2e3dkggx7ekt3gn_9wva3g.png"></p><a name="habracut"></a><br><h2 id="knowledge-distillation-kak-metod-uskoreniya-neyronnyh-setey">  Destila√ß√£o de conhecimento como m√©todo de acelera√ß√£o de redes neurais </h2><br><p>  Existem v√°rias maneiras de acelerar / clarear redes neurais.  A revis√£o mais detalhada que conheci foi publicada <a href="https://blog.inten.to/speeding-up-bert-5528e18bb4ea">no blog Intento no Medium</a> . </p><br><p>  Os m√©todos podem ser divididos em tr√™s grupos: </p><br><ol><li>  Mudan√ßa na arquitetura de rede. </li><li>  Compress√£o de modelo (quantiza√ß√£o, poda). </li><li>  Destila√ß√£o de conhecimento. </li></ol><br><p>  Se os dois primeiros m√©todos s√£o relativamente bem conhecidos e compreens√≠veis, o terceiro √© menos comum.  Pela primeira vez, a id√©ia de destila√ß√£o foi proposta por Rich Caruana <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">no artigo ‚ÄúModel Compression‚Äù</a> .  Sua ess√™ncia √© simples: voc√™ pode treinar um modelo leve que imite o comportamento de um modelo de professor ou mesmo de um conjunto de modelos.  No nosso caso, o professor ser√° o BERT e o aluno ser√° um modelo leve. </p><br><h2 id="zadacha">  Desafio </h2><br><p>  Vamos analisar a destila√ß√£o usando a classifica√ß√£o bin√°ria como exemplo.  Pegue o conjunto de dados aberto SST-2 do conjunto padr√£o de tarefas que testam modelos para PNL. </p><br><p>  Este conjunto de dados √© uma cole√ß√£o de resenhas de filmes com IMDb divididos por cor emocional - positiva ou negativa.  A m√©trica neste conjunto de dados √© precis√£o. </p><br><h2 id="obuchenie-bert-based-modeli-ili-uchitelya">  Treinar modelos baseados no BERT ou "professores" </h2><br><p>  Primeiro de tudo, voc√™ precisa treinar o modelo ‚Äúgrande‚Äù baseado em BERT, que se tornar√° professor.  A maneira mais f√°cil de fazer isso √© pegar os embeddings do BERT e treinar o classificador sobre eles, adicionando uma camada √† rede. </p><br><p> Gra√ßas √† <a href="https://github.com/huggingface/transformers">biblioteca de tranformers,</a> √© muito f√°cil fazer isso, porque existe uma classe pronta para o modelo BertForSequenceClassification.  Na minha opini√£o, o tutorial mais detalhado e compreens√≠vel para o ensino desse modelo foi <a href="https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca">publicado pela Towards Data Science</a> . </p><br><p>  Vamos imaginar que recebemos um modelo BertForSequenceClassification treinado.  No nosso caso, num_labels = 2, pois temos uma classifica√ß√£o bin√°ria.  Usaremos esse modelo como um "professor". </p><br><h2 id="obuchenie-uchenika">  Aprendendo "aluno" </h2><br><p>  Voc√™ pode ter qualquer arquitetura como estudante: uma rede neural, um modelo linear, uma √°rvore de decis√£o.  Vamos tentar ensinar o BiLSTM para uma melhor visualiza√ß√£o.  Para come√ßar, ensinaremos BiLSTM sem BERT. </p><br><p>  Para enviar texto para a entrada de uma rede neural, voc√™ precisa apresent√°-lo como um vetor.  Uma das maneiras mais f√°ceis √© mapear cada palavra para seu √≠ndice no dicion√°rio.  O dicion√°rio consistir√° nas principais palavras mais populares do conjunto de dados, al√©m de duas palavras de servi√ßo: "pad" - "palavra fict√≠cia" para que todas as seq√º√™ncias tenham o mesmo tamanho e "unk" - para palavras fora do dicion√°rio.  Vamos construir o dicion√°rio usando o conjunto padr√£o de ferramentas do torchtext.  Para simplificar, n√£o usei palavras incorporadas pr√©-treinadas. <br></p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_vocab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] text_field = data.Field() text_field.build_vocab(X_split, max_size=<span class="hljs-number"><span class="hljs-number">10000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text_field <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(seq, max_len)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(seq) &lt; max_len: seq = seq + [<span class="hljs-string"><span class="hljs-string">'&lt;pad&gt;'</span></span>] * (max_len - len(seq)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> seq[<span class="hljs-number"><span class="hljs-number">0</span></span>:max_len] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_indexes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vocab, words)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [vocab.stoi[w] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_dataset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y, y_real)</span></span></span><span class="hljs-function">:</span></span> torch_x = torch.tensor(x, dtype=torch.long) torch_y = torch.tensor(y, dtype=torch.float) torch_real_y = torch.tensor(y_real, dtype=torch.long) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> TensorDataset(torch_x, torch_y, torch_real_y)</code> </pre> <br><h3 id="model-bilstm">  Modelo BiLSTM </h3><br><p>  O c√≥digo para o modelo ficar√° assim: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SimpleLSTM</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, batch_size, device=None)</span></span></span><span class="hljs-function">:</span></span> super(SimpleLSTM, self).__init__() self.batch_size = batch_size self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout) self.fc = nn.Linear(hidden_dim * <span class="hljs-number"><span class="hljs-number">2</span></span>, output_dim) self.dropout = nn.Dropout(dropout) self.device = self.init_device(device) self.hidden = self.init_hidden() @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(device)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> device <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> device <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init_hidden</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)), Variable(torch.zeros(<span class="hljs-number"><span class="hljs-number">2</span></span> * self.n_layers, self.batch_size, self.hidden_dim).to(self.device))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text, text_lengths=None)</span></span></span><span class="hljs-function">:</span></span> self.hidden = self.init_hidden() x = self.embedding(text) x, self.hidden = self.rnn(x, self.hidden) hidden, cell = self.hidden hidden = self.dropout(torch.cat((hidden[<span class="hljs-number"><span class="hljs-number">-2</span></span>, :, :], hidden[<span class="hljs-number"><span class="hljs-number">-1</span></span>, :, :]), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>)) x = self.fc(hidden) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><h3 id="obuchenie">  Treinamento </h3><br><p>  Para este modelo, a dimens√£o do vetor de sa√≠da ser√° (batch_size, output_dim).  No treinamento, usaremos o logloss usual.  PyTorch possui uma classe BCEWithLogitsLoss que combina entrmo sigm√≥ide e entropia cruzada.  O que voc√™ precisa </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> criterion = torch.nn.BCEWithLogitsLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> criterion(output, real_label.float())</code> </pre> <br><p>  C√≥digo para uma era de aprendizado: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_optimizer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model)</span></span></span><span class="hljs-function">:</span></span> optimizer = torch.optim.Adam(model.parameters()) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-number"><span class="hljs-number">2</span></span>, gamma=<span class="hljs-number"><span class="hljs-number">0.9</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> optimizer, scheduler <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_train_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> train_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> train_sampler = RandomSampler(dataset) data_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.train() optimizer, scheduler = get_optimizer(model) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Train'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) model.zero_grad() output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) loss.backward() optimizer.step() train_loss += loss.item() scheduler.step() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_loss / len(data_loader)</code> </pre> <br><p>  C√≥digo para verifica√ß√£o ap√≥s a √©poca: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">epoch_evaluate_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, eval_dataset, loss_func, batch_size)</span></span></span><span class="hljs-function">:</span></span> eval_sampler = SequentialSampler(eval_dataset) data_loader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=batch_size, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) eval_loss = <span class="hljs-number"><span class="hljs-number">0.0</span></span> model.eval() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (text, bert_prob, real_label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(tqdm(data_loader, desc=<span class="hljs-string"><span class="hljs-string">'Val'</span></span>)): text, bert_prob, real_label = to_device(text, bert_prob, real_label) output = model(text.t(), <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>).squeeze(<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = loss_func(output, bert_prob, real_label) eval_loss += loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> eval_loss / len(data_loader)</code> </pre> <br><p>  Se tudo isso estiver organizado, obteremos o seguinte c√≥digo para treinar o modelo: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.utils.data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> (TensorDataset, random_split, RandomSampler, DataLoader, SequentialSampler) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torchtext <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"cpu"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">to_device</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> text = text.to(device()) bert_prob = bert_prob.to(device()) real_label = real_label.to(device()) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text, bert_prob, real_label <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMBaseline</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'simple_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> self.settings = settings self.criterion = torch.nn.BCEWithLogitsLoss().to(device()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.criterion(output, real_label.float()) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X, y, y_real, output_dir)</span></span></span><span class="hljs-function">:</span></span> max_len = self.settings[<span class="hljs-string"><span class="hljs-string">'max_seq_length'</span></span>] text_field = get_vocab(X) X_split = [t.split() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X] X_pad = [pad(s, max_len) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_split, desc=<span class="hljs-string"><span class="hljs-string">'pad'</span></span>)] X_index = [to_indexes(text_field.vocab, s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(X_pad, desc=<span class="hljs-string"><span class="hljs-string">'to index'</span></span>)] dataset = to_dataset(X_index, y, y_real) val_len = int(len(dataset) * <span class="hljs-number"><span class="hljs-number">0.1</span></span>) train_dataset, val_dataset = random_split(dataset, (len(dataset) - val_len, val_len)) model = self.model(text_field) model.to(device()) self.full_train(model, train_dataset, val_dataset, output_dir) torch.save(text_field, os.path.join(output_dir, self.vocab_name)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">full_train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, model, train_dataset, val_dataset, output_dir)</span></span></span><span class="hljs-function">:</span></span> train_settings = self.settings num_train_epochs = train_settings[<span class="hljs-string"><span class="hljs-string">'num_train_epochs'</span></span>] best_eval_loss = <span class="hljs-number"><span class="hljs-number">100000</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_train_epochs): train_loss = epoch_train_func(model, train_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) eval_loss = epoch_evaluate_func(model, val_dataset, self.loss, self.settings[<span class="hljs-string"><span class="hljs-string">'eval_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> eval_loss &lt; best_eval_loss: best_eval_loss = eval_loss torch.save(model.state_dict(), os.path.join(output_dir, self.weights_name))</code> </pre> <br><h3 id="distillyaciya">  Destila√ß√£o </h3><br><p>  A id√©ia desse m√©todo de destila√ß√£o √© retirada <a href="https://arxiv.org/abs/1903.12136">de um artigo de pesquisadores da Universidade de Waterloo</a> .  Como eu disse acima, o "aluno" deve aprender a imitar o comportamento do "professor".  Qual √© exatamente o comportamento?  No nosso caso, essas s√£o as previs√µes do modelo do professor no conjunto de treinamento.  E a id√©ia principal √© usar a sa√≠da de rede antes de aplicar a fun√ß√£o de ativa√ß√£o.  Sup√µe-se que, dessa maneira, o modelo possa aprender melhor a representa√ß√£o interna do que no caso das probabilidades finais. </p><br><p>  O artigo original prop√µe adicionar um termo √† fun√ß√£o de perda, que ser√° respons√°vel pelo erro "imita√ß√£o" - MSE entre os logs do modelo. </p><br><p><img src="https://habrastorage.org/webt/hx/_d/w9/hx_dw9ypkcwc_fhgui_jcappoo4.png"></p><br><p>  Para esses fins, fazemos duas pequenas altera√ß√µes: altere o n√∫mero de sa√≠das de rede de 1 para 2 e corrija a fun√ß√£o de perda. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> criterion_mse = torch.nn.MSELoss() criterion_ce = torch.nn.CrossEntropyLoss() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a*criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span>-a)*criterion_mse(output, bert_prob)</code> </pre> <br><p>  Voc√™ pode reutilizar todo o c√≥digo que escrevemos redefinindo apenas o modelo e a perda: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LSTMDistilled</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(LSTMBaseline)</span></span></span><span class="hljs-class">:</span></span> vocab_name = <span class="hljs-string"><span class="hljs-string">'distil_text_vocab.pt'</span></span> weights_name = <span class="hljs-string"><span class="hljs-string">'distil_lstm.pt'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, settings)</span></span></span><span class="hljs-function">:</span></span> super(LSTMDistilled, self).__init__(settings) self.criterion_mse = torch.nn.MSELoss() self.criterion_ce = torch.nn.CrossEntropyLoss() self.a = <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output, bert_prob, real_label)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.a * self.criterion_ce(output, real_label) + (<span class="hljs-number"><span class="hljs-number">1</span></span> - self.a) * self.criterion_mse(output, bert_prob) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, text_field)</span></span></span><span class="hljs-function">:</span></span> model = SimpleLSTM( input_dim=len(text_field.vocab), embedding_dim=<span class="hljs-number"><span class="hljs-number">64</span></span>, hidden_dim=<span class="hljs-number"><span class="hljs-number">128</span></span>, output_dim=<span class="hljs-number"><span class="hljs-number">2</span></span>, n_layers=<span class="hljs-number"><span class="hljs-number">1</span></span>, bidirectional=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, dropout=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, batch_size=self.settings[<span class="hljs-string"><span class="hljs-string">'train_batch_size'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><p>  Isso √© tudo, agora nosso modelo est√° aprendendo a "imitar". </p><br><h3 id="sravnenie-modeley">  Compara√ß√£o de modelos </h3><br><p>  No artigo original, os melhores resultados de classifica√ß√£o para o SST-2 s√£o obtidos em a = 0, quando o modelo aprende apenas a imitar, sem levar em considera√ß√£o os r√≥tulos reais.  A precis√£o ainda √© menor que o BERT, mas significativamente melhor que o BiLSTM comum. </p><br><p><img src="https://habrastorage.org/webt/0m/y6/g7/0my6g7eahypj6eq3o52arxxldxq.png"></p><br><p>  Tentei repetir os resultados do artigo, mas em meus experimentos o melhor resultado foi obtido em = 0,5. </p><br><p>  √â assim que os gr√°ficos de perda e precis√£o s√£o exibidos ao aprender LSTM da maneira usual.  A julgar pelo comportamento da perda, o modelo aprendeu rapidamente e, em algum momento ap√≥s a sexta era, o treinamento come√ßou. </p><br><p><img src="https://habrastorage.org/webt/lk/bg/rn/lkbgrnank6obtu7pewsoalba9yk.png"></p><br><p>  Gr√°ficos de destila√ß√£o: </p><br><p><img src="https://habrastorage.org/webt/gl/u5/7g/glu57giappdlhkc31wlpdulsdkc.png"></p><br><p>  O BiLSTM destilado √© consistentemente melhor que o normal.  √â importante que eles sejam absolutamente id√™nticos na arquitetura, a √∫nica diferen√ßa est√° na maneira de ensinar.  <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">Publiquei o</a> c√≥digo de treinamento completo <a href="https://github.com/pvgladkov/knowledge-distillation/tree/master/experiments/sst2">no GitHub</a> . </p><br><h2 id="zaklyuchenie">  Conclus√£o </h2><br><p>  Neste guia, tentei explicar a id√©ia b√°sica de uma abordagem de destila√ß√£o.  A arquitetura espec√≠fica do aluno depender√° da tarefa em quest√£o.  Mas, em geral, essa abordagem √© aplic√°vel em qualquer tarefa pr√°tica.  Devido √† complexidade no est√°gio de treinamento do modelo, √© poss√≠vel obter um aumento significativo em sua qualidade, mantendo a simplicidade original da arquitetura. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt485290/">https://habr.com/ru/post/pt485290/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt485278/index.html">A m√£o de Deus. Ajuda do cupom</a></li>
<li><a href="../pt485280/index.html">Go Fakedb Emula√ß√£o de banco de dados em testes</a></li>
<li><a href="../pt485284/index.html">Recursos do SPIKE ‚Ñ¢ Prime LEGO¬Æ Education</a></li>
<li><a href="../pt485286/index.html">Como pesamos mercadorias ou um pequeno ode de automa√ß√£o</a></li>
<li><a href="../pt485288/index.html">Amor para odiar gamedev'a indie</a></li>
<li><a href="../pt485294/index.html">Curso moderno sobre Node.js em 2020</a></li>
<li><a href="../pt485298/index.html">O misterioso programa LyX. Parte 4</a></li>
<li><a href="../pt485300/index.html">Novo surto de worms H2Miner que explora Redis RCE foi descoberto</a></li>
<li><a href="../pt485304/index.html">Alguns truques do elemento iframe</a></li>
<li><a href="../pt485312/index.html">DevOps para aplicativos m√≥veis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>