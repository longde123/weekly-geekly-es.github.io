<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🔱 👼🏼 💶 Ya veo, significa que existo: revisión de Deep Learning en Computer Vision (parte 2) 🚮 🦌 👢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuamos comprendiendo la magia moderna (visión artificial). La Parte 2 no significa que primero deba leer la Parte 1. La Parte 2 significa que aho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ya veo, significa que existo: revisión de Deep Learning en Computer Vision (parte 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/458190/"> Continuamos comprendiendo la magia moderna (visión artificial).  La Parte 2 no significa que primero deba leer la Parte 1. La Parte 2 significa que ahora todo es serio: queremos entender todo el poder de las redes neuronales en la visión.  Detección, seguimiento, segmentación, evaluación de la postura, reconocimiento de acciones ... ¡Las arquitecturas más modernas y modernas, cientos de capas y docenas de ideas brillantes ya te están esperando! <br><br><img src="https://habrastorage.org/webt/yt/nk/uu/ytnkuundiudek47rjvlmlujrrm4.jpeg"><br><a name="habracut"></a><br><h2>  En la última serie </h2><br>  Permítanme recordarles que en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera parte</a> nos familiarizamos con las redes neuronales convolucionales y su visualización, así como con las tareas de clasificar imágenes y construir sus representaciones efectivas (incrustaciones).  Incluso discutimos las tareas de reconocimiento facial y reidentificación de personas. <br><br>  Incluso en el artículo anterior hablamos sobre diferentes tipos de arquitecturas (sí, las mismas tabletas <s>que hice un mes</s> ), y aquí Google no perdió el tiempo: lanzaron otra arquitectura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">EfficientNet</a> extremadamente rápida y precisa.  Lo crearon utilizando el <abbr title="Búsqueda de arquitectura neuronal">NAS</abbr> y el procedimiento especial de escalado compuesto.  Mira el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">artículo</a> , vale la pena. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9m/_h/5c/9m_h5cc1tsxs7bfkainm5zom-wg.jpeg" width="500"></div><br>  Mientras tanto, algunos investigadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">animan caras</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buscan besos en películas</a> , nos ocuparemos de problemas más acuciantes. <br><br>  Aquí la gente dice: "reconocimiento de imagen".  Pero, ¿qué es el "reconocimiento"?  ¿Qué es "comprensión (escena)"?  En mi opinión, las respuestas a estas preguntas dependen de qué es exactamente lo que queremos "reconocer" y qué es exactamente lo que queremos "entender".  Si estamos construyendo Inteligencia Artificial, que extraerá información sobre el mundo del flujo visual de manera tan eficiente (o incluso mejor) como las personas, entonces debemos pasar de las tareas a las necesidades.  Históricamente, el moderno "reconocimiento" y "comprensión de la escena" se puede dividir en varias tareas específicas: clasificación, detección, seguimiento, evaluación de posturas y puntos faciales, segmentación, reconocimiento de acciones en el video y descripción de la imagen en el texto.  Este artículo se centrará en las dos primeras tareas de la lista (ups, spoiler de la tercera parte), por lo que el plan actual es este: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Encuéntrame si puedes: detección de objetos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Detección de rostro: no atrapado - no es un ladrón</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Muchas letras: detección de texto (y reconocimiento)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Video y seguimiento: en una sola transmisión</a> </li></ol><br>  ¡A rockear, superestrellas! <br><br><a name="1"></a><h2>  Encuéntrame si puedes: detección de objetos </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-q/9e/on/-q9eonan6thdv5jivk8kq0h7gm0.jpeg" width="700"></div><br>  Entonces, la tarea parece simple: se proporciona una imagen, debe encontrar objetos de clases predefinidas (persona, libro, manzana, basset-griffon artesiano-normando, etc.).  Para resolver este problema con la ayuda de redes neuronales, lo planteamos en términos de tensores y aprendizaje automático. <br><br>  Recordamos que una imagen en color es un tensor (H, W, 3) (si no lo recordamos, es decir, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">parte 1</a> ).  Anteriormente, solo sabíamos cómo clasificar la imagen completa, pero ahora nuestro objetivo es predecir las posiciones de los objetos de interés (coordenadas de píxeles) en la imagen y sus clases. <br><br>  La idea clave aquí es resolver dos problemas a la vez: clasificación y regresión.  Usamos una red neuronal para hacer retroceder las coordenadas y clasificar los objetos dentro de ellas. <br><br><div class="spoiler">  <b class="spoiler_title">Clasificación?</b>  <b class="spoiler_title">Regresión?</b> <div class="spoiler_text">  Permíteme recordarte que estamos hablando de las tareas del aprendizaje automático.  En el problema de <b>clasificación</b> , <b>las</b> etiquetas de <b>clase</b> actúan como la calidad de las etiquetas verdaderas para los objetos, y predecimos la clase del objeto.  En el problema de <b>regresión</b> , <b>los números reales</b> actúan como <b>números reales</b> , y predecimos el número (por ejemplo: peso, altura, salario, número de personajes que mueren en la próxima serie del Juego de Tronos ...).  En más detalle: usted es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bienvenido a la tercera conferencia de DLSchool (FPMI MIPT)</a> . <br></div></div><br>  Pero las coordenadas del objeto, en general, pueden formalizarse de diferentes maneras, en DL hay tres formas principales: <i>detección</i> ( <abbr title="rectángulos que delimitan objetos">cajas de</abbr> objetos), <i>evaluación de la postura</i> (puntos clave de los objetos) y <i>segmentación</i> ("máscaras" de los objetos).  Ahora <abbr title="rectángulos que delimitan objetos"><b>hablemos de</b></abbr> predecir con precisión los <abbr title="rectángulos que delimitan objetos"><b>cuadros delimitadores</b></abbr> , los puntos y la segmentación estarán más adelante en el texto. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ki/wt/xm/kiwtxmvvhmwsvqn3_5dovlmp8w8.jpeg" width="500"></div><br>  Básicamente, los conjuntos de datos de detección están marcados con cuadros en el formato: "coordenadas de las esquinas superior izquierda e inferior derecha para cada objeto en cada imagen" (este formato también se denomina <abbr title="&quot;Tlbr&quot;">arriba a la izquierda, abajo a la derecha</abbr> ), y la mayoría de los enfoques de redes neuronales predicen estas coordenadas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uo/zs/tj/uozstjspdifxpslvqyuurauxs2g.png" width="500"></div><br><div class="spoiler">  <b class="spoiler_title">Sobre conjuntos de datos y métricas en el problema de detección</b> <div class="spoiler_text">  Después de configurar la tarea, es mejor ver qué datos están disponibles para la capacitación y qué métricas se utilizan para medir la calidad.  De esto es de lo que hablo lentamente en la primera mitad de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">decimotercera conferencia de la Escuela de Aprendizaje Profundo</a> (en x2.0 es lo máximo). <br></div></div><br>  Antes de sumergirnos en los tipos de redes neuronales para la detección, pensemos juntos cómo resolver el problema de detectar cualquier cosa en las imágenes.  Probablemente, si queremos encontrar un determinado objeto en la imagen, entonces sabemos aproximadamente cómo se ve y qué área debería ocupar en la imagen (aunque puede cambiar). <br><br><div class="spoiler">  <b class="spoiler_title">Inventar la detección desde cero</b> <div class="spoiler_text">  El enfoque más simple e ingenuo sería simplemente hacer un algoritmo de "búsqueda de plantilla": dejar que la imagen sea de 100x100 píxeles, y estamos buscando un balón de fútbol.  Que haya un patrón de bolas de 20x20 píxeles.  Tome esta plantilla y la veremos como una convolución a lo largo de la imagen, contando la diferencia píxel por píxel.  Así es como funciona la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">coincidencia de plantillas</a> (a menudo se usa algún tipo de correlación en lugar de la diferencia de píxel por píxel). <br><br>  Si no hay una plantilla, pero hay un clasificador de red neuronal, entonces podemos hacer esto: iremos por una ventana de un tamaño fijo en la imagen y predeciremos la clase del área actual de la imagen.  Luego, solo decimos que las regiones más probables de los objetos son aquellas en las que el clasificador respondió con confianza.  Por lo tanto, podemos resolver el problema del hecho de que el objeto se ve diferente en apariencia diferente (ya que fue entrenado para clasificar en una muestra muy diversa). <br><br>  Pero luego aparece un problema: los objetos en las imágenes tienen diferentes tamaños.  El mismo balón de fútbol puede estar en toda la altura / anchura de la imagen, o puede estar lejos de la meta, tomando solo 10-20 píxeles de 1000. Me gustaría escribir el algoritmo de Fuerza Bruta: simplemente recorremos los tamaños de las ventanas.  Supongamos que tenemos 100x200 píxeles, luego iremos a una ventana de 2x2, 2X3, 3x2, 2x4, 4x2, 3x3 ..., 3x4, 4x3 ... Creo que entiendes que el número de ventanas posibles será de 100 * 200, y cada una de las cuales pasaremos por la imagen , realizar operaciones de clasificación (100-W_window) * (200 - H_window), lo que lleva mucho tiempo.  Me temo que no esperaremos hasta que dicho algoritmo funcione. <br><br>  Por supuesto, puede elegir las ventanas más características según el objeto, pero esto también funcionará durante mucho tiempo, y si es rápido, es poco probable que sea exacto: en aplicaciones reales habrá una cantidad increíble de variaciones en el tamaño de los objetos en las imágenes. <br></div></div><br>  Además, a veces me basaré en una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nueva revisión del área de detección a partir de enero de 2019</a> (las fotos también serán de ella).  Esta es solo una lectura obligatoria si desea obtener rápidamente la visión más amplia posible de DL en la detección. <br><br>  Uno de los primeros artículos sobre detección y localización usando CNN fue <a href="">Overfeat</a> .  Los autores afirman que primero utilizaron una red neuronal para la detección en ImageNet, reformulando el problema y cambiando la pérdida.  El enfoque, por cierto, fue casi de extremo a extremo (a continuación se muestra el esquema de sobrecompresión). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8o/5v/tv/8o5vtvhgukkn7frba0nx8yltfis.png" width="700"></div><br>  La siguiente arquitectura importante fue la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Red neuronal convolucional basada en</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">región</a> ( <b>RCNN</b> ), inventada por investigadores de <abbr>FAIR</abbr> en 2014.  Su esencia es que primero predice muchas de las llamadas "regiones de interés" (RoI), dentro de las cuales potencialmente puede haber objetos (usando el algoritmo de búsqueda selectiva), y los clasifica y refina las coordenadas de los cuadros usando CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2r/2p/kp/2r2pkpcoysglv4z_v-ll_y14mqw.png" width="700"></div><br>  Es cierto que tal canalización hizo que todo el sistema fuera lento, porque ejecutamos todas las regiones a través de la red neuronal (lo adelantamos miles de veces).  Un año después, el mismo FAIR Ross Girshick actualizó RCNN a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fast-RCNN</a> .  Aquí la idea era intercambiar la Búsqueda selectiva y la predicción de la red: primero, pasamos la imagen completa a través de una red neuronal pre-entrenada, y luego predecimos regiones de interés sobre el mapa de características emitido por la red troncal (por ejemplo, usando la misma Búsqueda selectiva, pero Hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otros algoritmos</a> ).  Todavía era bastante lento, mucho más lento que el tiempo real (por ahora, suponemos que el tiempo real es inferior a 40 milisegundos por imagen). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tn/uc/d-/tnucd-y6i7tr4edgjeudtrsj16u.png" width="700"></div><br>  La velocidad se vio afectada, sobre todo, no por CNN, sino por el algoritmo de generación de cajas en sí, por lo que se decidió reemplazarlo con una segunda red neuronal: la Red de Propuesta de Región ( <b>RPN</b> ), que se entrenará para predecir las regiones de interés de los objetos.  Así es como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">apareció Faster-RCNN</a> (sí, obviamente no pensaron en el nombre durante mucho tiempo).  Esquema: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6v/h_/ye/6vh_yee2zrsflyhh8jdvtm_bbgy.png" width="700"></div><br>  Luego hubo otra mejora en la forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">R-FCN</a> , no hablaremos de ello en detalle, pero quiero mencionar a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mask-RCNN</a> .  Mask-RCNN es único, la primera red neuronal que resuelve el <b>problema de la detección y la segmentación de instancias al mismo tiempo</b> : predice las máscaras (siluetas) exactas de los objetos dentro de los cuadros delimitadores.  Su idea es bastante simple: hay dos ramas: para la detección y la segmentación, y necesita capacitar a la red para ambas tareas a la vez.  Lo principal es tener datos etiquetados.  Mask-RCNN en sí es muy similar a Faster-RCNN: la columna vertebral es la misma, pero al final hay dos <b>"cabezas"</b> (como se llaman a menudo las <b>últimas capas de la</b> red neuronal) para dos tareas diferentes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7w/ig/hq/7wighq6ox7tptik5f_7d7cez2hg.png" width="700"></div><br>  Estos fueron los llamados enfoques de <b>dos etapas</b> (o <b>basados ​​en la región</b> ).  Paralelamente a ellos, se desarrollaron análogos en la detección de DL: enfoques de una <b>etapa</b> .  Estos incluyen redes neuronales como: Detector de disparo único (SSD), Solo mira una vez (YOLO), Detector de objetos supervisados ​​(DSOD), Red de bloqueo de campo receptivo (RFBNet) y muchos otros (consulte el mapa a continuación, desde <a href="">este repositorio</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/lc/-y/uelc-yeav4_avjdkycglf9uwrmm.png" width="750"></div><br>  Los enfoques de una etapa, a diferencia de las dos etapas, no utilizan un algoritmo separado para generar cuadros, sino que simplemente predicen varias coordenadas de cuadro para cada mapa de características producido por una red neuronal convolucional.  YOLO actúa de manera similar, SSD es ligeramente diferente, pero solo hay una idea: una convolución 1x1 predice muchos números de los mapas de características recibidos en profundidad, sin embargo, acordamos de antemano qué número significa. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/ml/w_/qjmlw_ympdcib6jkpfdjdvcirdy.png" width="600"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jr/i3/oy/jri3oymb48sv5vq9dwwdxbndszg.png" width="600"></div><br>  Por ejemplo, predecimos a partir de un mapa de características el tamaño de 13x13x256 es un mapa de características de 13x13x (4 * (5 + 80)) números, donde en profundidad predecimos 85 números para 4 cajas: los primeros 4 números en la secuencia son siempre las coordenadas de la caja, el 5to - confianza en el boxeo, y 80 números - las probabilidades de cada una de las clases (clasificación).  Esto es necesario para luego enviar los números necesarios a las pérdidas necesarias y entrenar adecuadamente la red neuronal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/xi/o2/aoxio2rty3hgpu2f8mduomvs9nu.png" width="800"></div><br>  Quiero llamar la atención sobre el hecho de que la calidad del trabajo del detector depende de la calidad de la red neuronal para extraer características (es decir, una <b>red neuronal troncal</b> ).  Por lo general, este papel lo desempeña una de las arquitecturas, de la que hablé en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">artículo anterior</a> (ResNet, SENet, etc.), pero a veces los autores presentan sus propias arquitecturas más óptimas (por ejemplo, Darknet-53 en YOLOv3) o modificaciones (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Feature Pyramid Pooling</a> (FPN)). <br><br>  Una vez más, noto que entrenamos la red para la clasificación y la regresión al mismo tiempo.  En la comunidad, esto se llama pérdida de tareas múltiples: la suma de las pérdidas para varias tareas (con algunos coeficientes) aparece en una pérdida. <br><br><div class="spoiler">  <b class="spoiler_title">Noticias con pérdida multitarea líder</b> <div class="spoiler_text">  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Machines Can See 2019,</a> uno de los oradores usó la pérdida de tareas múltiples para 7 tareas simultáneamente <s>, Carl</s> .  Resultó que algunas tareas se establecieron inicialmente como un contrapeso entre sí y se obtuvo un "conflicto", que impidió que la red aprendiera mejor que si estuviera entrenada para cada tarea por separado.  Conclusión: si está utilizando la pérdida de tareas múltiples, asegúrese de que estas mismas tareas múltiples no entren en conflicto con la declaración (por ejemplo, predecir los límites de los objetos y su segmentación interna puede interferir entre sí, porque estas cosas pueden depender de diferentes signos dentro de la red).  El autor evitó esto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">agregando bloques separados de Exprimir y Excitar para cada tarea</a> . <br></div></div><br>  Recientemente, aparecieron artículos de 2019 en los que los autores declaran una relación velocidad / precisión aún mejor en la tarea de detección utilizando la <b>predicción de recuadro basada en puntos</b> .  Estoy hablando de los artículos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Objetos como puntos"</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"CornerNet-Lite"</a> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ExtremeNet</a> es una modificación de CornerNet.  Parece que ahora se les puede llamar SOTA en la detección utilizando redes neuronales (pero esto no es exacto). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/qf/mr/wkqfmrenwm3u5f6c6bzinjcffga.png" width="900"></div><br>  Si de repente mi explicación de los detectores todavía parecía caótica e incomprensible, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro video</a> lo discuto lentamente.  Quizás deberías verlo primero. <br><br>  A continuación, he presentado tablas de redes neuronales en detección con enlaces al código y una breve descripción de los chips de cada red.  Intenté recopilar solo aquellas redes que son realmente importantes para conocer (al menos sus ideas) para tener una buena idea sobre la detección de objetos hoy: <br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (dos etapas)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Año </th><th>  Artículo </th><th>  Idea clave </th><th>  Código </th></tr><tr><td>  2013-2014 </td><td>  <a href="">RCNN</a> </td><td>  Generación de regiones de interés y predicción de redes neuronales de clases dentro de ellas. </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fast-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primero pase la imagen a través de la red y luego genere regiones de interés</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Rcnn más rápido</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">usar RPN para generar regiones de interés</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">R-FCN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque totalmente convolucional en lugar de generar regiones de interés</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mask-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dos "cabezas" para resolver dos tareas a la vez, RoI-Align</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras, TF</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Razonamiento-RCNN</a> </td><td>  Mejorar la calidad de RCNN mediante la construcción de un gráfico de las relaciones semánticas de los objetos. </td><td>  --- </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (una etapa)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Año </th><th>  Artículo </th><th>  Idea clave </th><th>  Código </th></tr><tr><td>  2013-2014 </td><td>  <a href="">Sobrepeso</a> </td><td>  uno de los primeros detectores de redes neuronales </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C ++ (con envoltorios para otros idiomas)</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SSD</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque de una etapa muy flexible utilizado ahora en muchas aplicaciones</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Yolo</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una idea similar a SSD, se está desarrollando en paralelo y no menos popular (hay nuevas versiones)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C ++</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YOLOv2 (también conocido como YOLO9000)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una serie de mejoras para YOLO</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YOLOv3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una serie de mejoras para YOLOv2</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DSOD</a> </td><td>  Idea de supervisión profunda e ideas de DenseNet </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RFBNet</a> </td><td>  Los filtros de convolución se seleccionan cuidadosamente en función de la estructura del sistema visual humano (bloque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RF</a> ) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (varios)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Año </th><th>  Artículo </th><th>  Idea clave </th><th>  Código </th></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RetinaNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pérdida focal especial para resolver el problema del desequilibrio de clase</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras</a> </td></tr><tr><td>  2014-2015 </td><td>  <a href="">SPP</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">módulo que le permite trabajar eficazmente con imágenes de diferentes tamaños</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras</a> </td></tr><tr><td>  2016-2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FPN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuentan con pirámides para una mejor detección de objetos de diferentes tamaños</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NAS-FPN</a> </td><td>  Encontrar el mejor FPN con la búsqueda de arquitectura neuronal </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dédalo</a> </td><td>  Cómo romper el detector con un ataque de confrontación </td><td>  --- </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (basados ​​en puntos)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Año </th><th>  Artículo </th><th>  Idea clave </th><th>  Código </th></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Centernet</a> </td><td>  Un nuevo enfoque para la detección, que permite resolver de manera rápida y eficiente el problema de encontrar puntos, cajas y cajas 3D al mismo tiempo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cornernet</a> </td><td>  predicción de cajas basada en pares de puntos de esquina </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CornerNet-Lite</a> </td><td>  esquina acelerada </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ExtremeNet</a> </td><td>  predicción de puntos "extremos" de objetos (límites geométricamente precisos) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Para comprender cómo se correlaciona la velocidad / calidad de cada arquitectura, puede consultar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta revisión</a> o su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versión más popular</a> . <br><br>  La arquitectura está bien, pero la detección es principalmente una tarea práctica.  "No tengo cien redes, pero tengo al menos 1 en funcionamiento" - este es mi mensaje.  Hay enlaces al código en la tabla anterior, pero personalmente, rara vez encuentro detectores de lanzamiento directamente desde los repositorios (al menos con el objetivo de un mayor despliegue en producción).  Muy a menudo, se usa una biblioteca para esto, por ejemplo, la API de detección de objetos TensorFlow (consulte la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">parte práctica de mi lección</a> ) o una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">biblioteca de investigadores de CUHK</a> .  Les traigo a su atención otra súper mesa (les gustan, ¿verdad?): <br><br><div class="spoiler">  <b class="spoiler_title">Bibliotecas para ejecutar modelos de detección</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Titulo </th><th>  Los autores </th><th>  Descripción </th><th>  Redes neuronales implementadas </th><th>  Marco </th></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Detectron</a> </td><td>  Facebook AI Research </td><td>  Repositorio de Facebook con varios códigos modelo para detectar y evaluar posturas </td><td>  Todo basado en la región </td><td>  Caffe2 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">API de detección de objetos TF</a> </td><td>  Equipo TensorFlow </td><td>  Muchos modelos listos para usar (se dan pesos) </td><td>  Todos los SSD y basados ​​en regiones (con diferentes esquemas) </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Flujo oscuro</a> </td><td>  trio </td><td>  Implementaciones de YOLO y YOLOv2 listas para usar </td><td>  Todos los tipos de YOLO (con modificaciones) excepto YOLOv3 </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mmdetection</a> </td><td>  Abrir MMLab (CUHK) </td><td>  Una gran cantidad de detectores en PyTorch, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vea su artículo</a> </td><td>  Casi todos los modelos excepto la familia YOLO </td><td>  Pytorch </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Darknet (modificado)</a> </td><td>  AlexAB </td><td>  Implementación conveniente de YOLOv3 con muchas mejoras en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio original</a> </td><td>  YOLOv3 </td><td>  C ++ </td></tr></tbody></table></div><br></div></div><br>  A menudo necesita detectar un objeto de una sola clase, pero específico y altamente variable.  Por ejemplo, para detectar todas las caras en la foto (para mayor verificación / conteo de personas), para detectar personas enteras (para reidentificación / conteo / seguimiento) o para detectar texto en la escena (para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OCR</a> / traducción de palabras en la foto).  En general, el enfoque de detección "ordinario" aquí funcionará hasta cierto punto, pero cada una de estas subtareas tiene sus propios trucos para mejorar la calidad. <br><br><a name="2"></a><h2>  Detección de rostro: no atrapado - no es un ladrón </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/ul/rp/iculrpbc7niyrdxg1yk_8r82nsw.jpeg" width="700"></div><br>  Aquí aparece cierta especificidad, ya que las caras a menudo ocupan una parte bastante pequeña de la imagen.  Además, las personas no siempre miran a la cámara, a menudo la cara solo es visible desde un lado.  Uno de los primeros enfoques para el reconocimiento facial fue el famoso detector Viola-Jones basado en cascadas Haar, inventado en 2001. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hc/tf/zn/hctfzn0xudbedi_aymhmlcwkamu.png" width="400"></div><br>  Las redes neuronales <s>no estaban de moda entonces,</s> todavía no tenían una visión tan fuerte, sin embargo, el buen enfoque hecho a mano hizo su trabajo.  Se usaron activamente varios tipos de máscaras de filtro especiales, que ayudaron a extraer regiones faciales de la imagen y sus signos, y luego estos signos se enviaron al clasificador AdaBoost.  Por cierto, este método realmente funciona bien y ahora, es lo suficientemente rápido y comienza <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">con OpenCV</a> .  La desventaja de este detector es que solo ve caras desplegadas frontalmente a la cámara.  Uno solo tiene que darse la vuelta un poco y se viola la estabilidad de la detección. <br><br>  Para casos tan complejos, puede usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dlib</a> .  Esto es C ++, una biblioteca en la que se implementan muchos algoritmos de visión, incluso para la detección de rostros. <br><br>  De los enfoques de redes neuronales en la detección de rostros, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CNN en cascada multitarea (MTCNN)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MatLab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow</a> ) es especialmente significativa.  En general, ahora se usa activamente (en la misma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/80/wn/vu/80wnvuf59poodmodzswcgjlyjt4.jpeg" width="400"></div><br>  La idea de MTCNN es utilizar tres redes neuronales secuencialmente (por lo tanto, una <b>"cascada"</b> ) para predecir la posición de una cara y sus puntos singulares.  En este caso, hay exactamente 5 puntos especiales en la cara: el ojo izquierdo, el ojo derecho, el borde izquierdo de los labios, el borde derecho de los labios y la nariz.  La primera red neuronal de la cascada ( <abbr title="Proposal Net">P-Net</abbr> ) se utiliza para generar regiones potenciales de la cara.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El segundo ( </font></font><abbr title="Refine net"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R-Net</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - para mejorar las coordenadas de los cuadros recibidos. </font><font style="vertical-align: inherit;">La tercera </font></font><abbr title="Red de puntos de referencia faciales"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">red</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> neuronal </font><font style="vertical-align: inherit;">( </font><abbr title="Red de puntos de referencia faciales"><font style="vertical-align: inherit;">O-Net</font></abbr><font style="vertical-align: inherit;"> ) vuelve a retroceder las coordenadas de las cajas y, además, predice 5 puntos clave de la cara. </font><font style="vertical-align: inherit;">Esta red es una tarea múltiple porque se resuelven tres tareas: regresión de puntos de cuadro, clasificación de cara / no cara para cada cuadro y regresión de puntos de cara. </font><font style="vertical-align: inherit;">Además, MTCNN lo hace todo en tiempo real, es decir, requiere menos de 40 ms por imagen.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/iz/d5/5eizd5lwag9umfo1ccypep42eik.jpeg" width="800"></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¿Cómo, usted todavía no lee artículos con ArXiv?</font></font></b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En este caso, te recomiendo que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">intentes</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> leer el </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">artículo original sobre MTCNN</font></a><font style="vertical-align: inherit;"> , si ya tienes experiencia en redes de convolución. </font><font style="vertical-align: inherit;">Este artículo solo toma </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5 páginas</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pero establece toda la información que necesita para comprender el enfoque. </font><font style="vertical-align: inherit;">Pruébalo, se apretará :)</font></font><br></div></div><br>   State-of-the-Art   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dual Shot Face Detector (DSFD)</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FaceBoxes</a> . FaceBoxes      CPU (!),  DSFD    (   2019 ). DSFD  ,  MTCNN,          ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dilated convolutions</a> ),        . ,  dilated convolutions            .    DSFD (,   ?). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xv/or/it/xvoritceua0_ocjteyvm4xvisbq.jpeg"></div><br>     <b></b> ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> ,      . <br><br><a name="3"></a><h2>  :  ( )  </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qn/v9/wo/qnv9woeqru3dioeennrkvalkjqg.png" width="500"></div><br>     .  , ,   bounding box',    (   ),    .     ,   , ,       recognition-,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> . <br><br>       bounding box',        ,    ( ).     , , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">EAST-</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j3/hd/bj/j3hdbj-xozs4voec8ekiz4k1eo0.png" width="500"></div><br><br>  EAST-  ,      ,    : <br><br><ol><li> Text Score Map' (     ) </li><li>     </li><li>        </li></ol><br>  ,      (  ),  .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arxiv-</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dj/vt/uu/djvtuu36dzinoratlumncsgb5t8.png" width="700"></div><br><br>    (    )  ,    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TextBoxes++</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Caffe</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SegLinks</a> ,  EAST,   ,    . <br><br>         ,  <b></b>     .       —    .     ,      ,   ,          . , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MORAN</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  PyTorch</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ASTER</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  TensorFlow</a> )     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vt/ot/nc/vtotncz1d1zhhsiytuzmarta9ta.png" width="700"></div><br><br>    - ,          : CNN  RNN.       ,     .    MORAN':     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hu/qx/_2/huqx_2jakjcggsgacvhj9a8vce4.png" width="300"></div><br>       EAST', -       ,           .  ,           ,     . <br><br>   <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a></b>   ,  / .      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spatial Transformet Network (STN)</a> ,              (,       ,    ).   / STN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6y/z1/p9/6yz1p942ensgepwirqrtzpcnb7q.jpeg" width="700"></div><br>   STN    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> (  ,  )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  PyTorch</a> . <br><br>  MORAN (     )    —      ,        <b> </b>  x   y,     ,      .    <i><abbr title="corrección, corrección">rectification</abbr></i> ,         ( <i>rectifier'</i> ).         : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n4/az/du/n4azduih5p7wd9sx-tqhpgy-p20.png" width="300"></div><br>  Sin embargo, además de los enfoques para el reconocimiento de texto "modularmente" (red de detección -&gt; red de reconocimiento), existe una arquitectura de extremo a extremo: la entrada es una imagen, y la salida es una detección y el texto se reconoce dentro de ellos.  Y todo esto es una sola tubería que aprende ambas tareas a la vez.  En esta dirección, existe el impresionante trabajo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spotting de texto orientado rápidamente con una red unificada ( <b>FOTS</b> ) (</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">código</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>PyTorch</b></a> ), donde los autores también señalan que el enfoque de extremo a extremo es dos veces más rápido que "detección + reconocimiento".  A continuación se muestra el diagrama de la red neuronal FOTS, el bloque RoiRotate desempeña un papel especial, debido al cual es posible "emitir gradientes" desde la red para su reconocimiento en la red neuronal para su detección (esto es realmente más complicado de lo que parece) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/a5/xh/ch/a5xhchryrwf-zpfudielmab9pqu.png" width="800"></div><br>  Por cierto, cada año se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">celebra la</a> conferencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICDAR</a> , a la que se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">realizan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">varios concursos</a> para el reconocimiento de texto en una variedad de imágenes. <br><br><h3>  Problemas actuales en la detección </h3><br>  En mi opinión, el principal problema en la detección ahora no es la calidad del modelo de detector, sino los datos: generalmente son largos y caros de marcar, especialmente si hay muchas clases que deben detectarse (pero por cierto, hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo de una solución</a> para 500 clases).  Por lo tanto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muchos trabajos</a> ahora se dedican a la generación de los datos más plausibles "sintéticamente" y a obtener un marcado "gratis".  A continuación se muestra una imagen de <s>mi diploma de un</s> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">artículo de Nvidia</a> , que trata específicamente con la generación de datos sintéticos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6k/mn/wh/6kmnwhvc1pgahzwtjloa4dy9sbm.png" width="800"></div><br>  Pero aún así es genial que ahora podamos decir con certeza en qué parte de la imagen estar.  Y si queremos, por ejemplo, calcular la cantidad de algo en el marco, entonces es suficiente para detectar esto y dar el número de cajas.  En la detección de personas, YOLO ordinario también funciona bien, solo que lo principal es enviar una gran cantidad de datos.  El mismo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Darkflow es</a> adecuado, y la clase "humana" se encuentra en casi todos los principales conjuntos de datos de detección.  Entonces, si queremos usar la cámara para contar la cantidad de personas que pasaron, por ejemplo, en un día, o la cantidad de productos que una persona tomó en una tienda, simplemente detectaremos y daremos la cantidad ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m2/ts/pl/m2tsplbgrnmoauvlghy8ivq2jdm.jpeg" width="700"></div><br>  Para  Pero si vamos a detectar personas en cada imagen de la cámara, entonces podemos calcular su número en un cuadro y en dos, ya no, porque no podemos decir exactamente dónde está esa persona.  Necesitamos un algoritmo que nos permita contar exactamente personas únicas en la transmisión de video.  Puede ser un algoritmo de reidentificación, pero cuando se trata de video y detección, es un pecado no utilizar algoritmos de seguimiento. <br><br><a name="4"></a><h3>  Video y seguimiento: en una sola transmisión </h3><br>  Hasta ahora, solo hemos hablado de tareas en imágenes, pero lo más interesante sucede en el video.  Para resolver el mismo reconocimiento de acciones, necesitamos usar no solo el llamado componente <i>espacial</i> , sino también el <i>temporal</i> , ya que el video es una secuencia de imágenes en el tiempo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qm/da/pa/qmdapao0kcytnxhorj7htm6susy.jpeg" width="700"></div><br>  El seguimiento es un análogo de la detección de imágenes, pero para video.  Es decir, queremos enseñarle a la red a predecir no el boxeo en la imagen, sino un tracklet en el tiempo (que es esencialmente una secuencia de cuadros).  A continuación se muestra un ejemplo de una imagen que muestra las "colas": las pistas de estas personas en el video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xz/yv/c_/xzyvc_oumhrnf_-bmf8gz5l_hli.png" width="600"></div><br>  Pensemos cómo resolver el problema de seguimiento.  Que haya un video, y sus marcos # 1 y # 2.  Consideremos hasta ahora solo un objeto: rastreamos una bola.  En el cuadro # 1, podemos usar un detector para detectarlo.  En el segundo también podemos detectar una pelota, y si está allí sola, entonces todo está bien: decimos que el boxeo en el cuadro anterior es el boxeo de la misma bola que en el cuadro # 2.  También puede continuar con los cuadros restantes, debajo del gif del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">curso de</a> visión <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pyimagesearch</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_w/et/ak/_wetakdpybemefhert6cxslqaju.gif" width="600"></div><br>  Por cierto, para ahorrar tiempo, no podemos iniciar la red neuronal en el segundo cuadro, sino simplemente "cortar" la caja de la pelota desde el primer cuadro y buscar exactamente la misma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">correlación</a> en el segundo cuadro o píxel por píxel.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los rastreadores de correlación</a> utilizan este enfoque, se consideran simples y más o menos confiables si se trata de casos simples como "rastrear una pelota frente a la cámara en una habitación vacía".  Esta tarea también se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Visual Object Tracking</a> .  A continuación se muestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo del trabajo del</a> rastreador de correlación utilizando el ejemplo de una persona. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cv/fp/qd/cvfpqdd5ghelxfpucxrejxq6vpm.gif" width="600"></div><br>  Sin embargo, si hay varias detecciones / personas, entonces debe poder hacer coincidir los cuadros del cuadro 1 y del cuadro 2.  La primera idea que se me ocurre es tratar de hacer coincidir el cuadro con el que tiene el área de intersección más grande ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">IoU</a> ).  Es cierto que en el caso de varias detecciones superpuestas, dicho rastreador será inestable, por lo que debe utilizar aún más información. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qx/i2/t1/qxi2t1ejpnir0h52ip-23zpf4ti.png" width="600"></div><br>  El enfoque con IoU se basa solo en los <i>signos de</i> detección <i>"geométricos"</i> , es decir, simplemente trata de compararlos por proximidad en los marcos.  Pero tenemos a nuestra disposición una imagen completa (incluso dos en este caso), y podemos usar el hecho de que dentro de estas detecciones hay <i>signos "visuales"</i> .  Además, tenemos un historial de detecciones para cada persona, lo que nos permite predecir con mayor precisión su próxima posición en función de la velocidad y la dirección del movimiento, esto puede llamarse condicionalmente <i>signos "físicos"</i> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ay/se/mcaysee7xq25-j6hnvjalhfy5z0.gif" width="700"></div><br>  Uno de los primeros rastreadores en tiempo real, que era completamente confiable y capaz de hacer frente a situaciones difíciles, se publicó en 2016 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Simple Online y Realtime Traker (SORT)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">código Python</a> ).  SORT no usó ningún signo visual y redes neuronales, sino que solo estimó una serie de parámetros de cada cuadro en cada cuadro: la velocidad actual (x e y por separado) y el tamaño (altura y ancho).  La relación de aspecto de un cuadro siempre se toma desde la primera detección de ese cuadro.  Además, las velocidades se predicen utilizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">filtros de Kalman</a> (generalmente son buenas y ligeras en el mundo del procesamiento de señales), se construye la matriz de intersección de las cajas por IoU, y el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algoritmo húngaro</a> asigna las detecciones. <br><br>  Si le parece que las matemáticas ya se han vuelto un poco demasiado, entonces en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este artículo</a> todo se explica de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">manera</a> accesible (esto es medio :). <br><br>  Ya en 2017, se lanzó una modificación de SORT en forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepSORT</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">código para TensorFlow</a> ).  DeepSORT ya ha comenzado a usar la red neuronal para extraer signos visuales, usándolos para resolver colisiones.  La calidad del seguimiento ha crecido: no es por nada que se considera uno de los mejores rastreadores en línea en la actualidad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/it/ny/15itnyht32oes3n-keaqk36jnpq.gif" width="800"></div><br>  El campo de rastreo se está desarrollando activamente: hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rastreadores con redes neuronales siamesas</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rastreadores con RNN</a> .  Mantenga el dedo en el pulso, porque en cualquier día puede salir una arquitectura aún más precisa y rápida (o ya ha salido).  Por cierto, es muy conveniente seguir tales cosas en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PapersWithCode</a> , siempre hay enlaces a artículos y códigos para ellos (si los hay). <br><br><h3>  Epílogo </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/yh/bf/jbyhbf2ovu_ynurp_t_d9_hi4vg.jpeg" width="600"></div><br><br>  Realmente hemos experimentado mucho y aprendido mucho.  Pero la visión por computadora es un área extremadamente vasta, y yo soy una persona extremadamente terca.  Es por eso que lo veremos en el tercer artículo de este ciclo (¿será el último? Quién sabe ...), donde discutiremos con más detalle la segmentación, la evaluación de la postura, el reconocimiento de acciones en un video y la generación de una descripción de una imagen usando redes neuronales. <br><br>  PD: Quiero expresar un agradecimiento especial a Vadim Gorbachev por sus valiosos consejos y comentarios en la preparación de este y el artículo anterior. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458190/">https://habr.com/ru/post/458190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458172/index.html">Métodos para emparejar conexiones eléctricas al rastrear pares diferenciales en placas de circuito impreso</a></li>
<li><a href="../458176/index.html">La barrera de exaflops se superará en 2021</a></li>
<li><a href="../458180/index.html">Servidor DHCP de conmutación por error basado en Kea</a></li>
<li><a href="../458182/index.html">Leemos VKontakte a través de RSS</a></li>
<li><a href="../458188/index.html">Como hice una red social en 2019</a></li>
<li><a href="../458202/index.html">Solo eche un vistazo a SObjectizer si desea usar Actores o CSP en su proyecto C ++</a></li>
<li><a href="../458204/index.html">Cómo evaluar el rendimiento del almacenamiento en Linux: evaluación comparativa utilizando herramientas abiertas</a></li>
<li><a href="../458218/index.html">¿Cirugía maxilofacial o no? Esa es la pregunta</a></li>
<li><a href="../458220/index.html">El resumen de materiales interesantes para el desarrollador móvil # 304 (del 24 al 30 de junio)</a></li>
<li><a href="../458222/index.html">Romper un juego de memoria: toda una historia de detectives</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>