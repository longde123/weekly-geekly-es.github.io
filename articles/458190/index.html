<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî± üëºüèº üí∂ Ya veo, significa que existo: revisi√≥n de Deep Learning en Computer Vision (parte 2) üöÆ ü¶å üë¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuamos comprendiendo la magia moderna (visi√≥n artificial). La Parte 2 no significa que primero deba leer la Parte 1. La Parte 2 significa que aho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ya veo, significa que existo: revisi√≥n de Deep Learning en Computer Vision (parte 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/458190/"> Continuamos comprendiendo la magia moderna (visi√≥n artificial).  La Parte 2 no significa que primero deba leer la Parte 1. La Parte 2 significa que ahora todo es serio: queremos entender todo el poder de las redes neuronales en la visi√≥n.  Detecci√≥n, seguimiento, segmentaci√≥n, evaluaci√≥n de la postura, reconocimiento de acciones ... ¬°Las arquitecturas m√°s modernas y modernas, cientos de capas y docenas de ideas brillantes ya te est√°n esperando! <br><br><img src="https://habrastorage.org/webt/yt/nk/uu/ytnkuundiudek47rjvlmlujrrm4.jpeg"><br><a name="habracut"></a><br><h2>  En la √∫ltima serie </h2><br>  Perm√≠tanme recordarles que en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera parte</a> nos familiarizamos con las redes neuronales convolucionales y su visualizaci√≥n, as√≠ como con las tareas de clasificar im√°genes y construir sus representaciones efectivas (incrustaciones).  Incluso discutimos las tareas de reconocimiento facial y reidentificaci√≥n de personas. <br><br>  Incluso en el art√≠culo anterior hablamos sobre diferentes tipos de arquitecturas (s√≠, las mismas tabletas <s>que hice un mes</s> ), y aqu√≠ Google no perdi√≥ el tiempo: lanzaron otra arquitectura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">EfficientNet</a> extremadamente r√°pida y precisa.  Lo crearon utilizando el <abbr title="B√∫squeda de arquitectura neuronal">NAS</abbr> y el procedimiento especial de escalado compuesto.  Mira el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> , vale la pena. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9m/_h/5c/9m_h5cc1tsxs7bfkainm5zom-wg.jpeg" width="500"></div><br>  Mientras tanto, algunos investigadores <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">animan caras</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buscan besos en pel√≠culas</a> , nos ocuparemos de problemas m√°s acuciantes. <br><br>  Aqu√≠ la gente dice: "reconocimiento de imagen".  Pero, ¬øqu√© es el "reconocimiento"?  ¬øQu√© es "comprensi√≥n (escena)"?  En mi opini√≥n, las respuestas a estas preguntas dependen de qu√© es exactamente lo que queremos "reconocer" y qu√© es exactamente lo que queremos "entender".  Si estamos construyendo Inteligencia Artificial, que extraer√° informaci√≥n sobre el mundo del flujo visual de manera tan eficiente (o incluso mejor) como las personas, entonces debemos pasar de las tareas a las necesidades.  Hist√≥ricamente, el moderno "reconocimiento" y "comprensi√≥n de la escena" se puede dividir en varias tareas espec√≠ficas: clasificaci√≥n, detecci√≥n, seguimiento, evaluaci√≥n de posturas y puntos faciales, segmentaci√≥n, reconocimiento de acciones en el video y descripci√≥n de la imagen en el texto.  Este art√≠culo se centrar√° en las dos primeras tareas de la lista (ups, spoiler de la tercera parte), por lo que el plan actual es este: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Encu√©ntrame si puedes: detecci√≥n de objetos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Detecci√≥n de rostro: no atrapado - no es un ladr√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Muchas letras: detecci√≥n de texto (y reconocimiento)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Video y seguimiento: en una sola transmisi√≥n</a> </li></ol><br>  ¬°A rockear, superestrellas! <br><br><a name="1"></a><h2>  Encu√©ntrame si puedes: detecci√≥n de objetos </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-q/9e/on/-q9eonan6thdv5jivk8kq0h7gm0.jpeg" width="700"></div><br>  Entonces, la tarea parece simple: se proporciona una imagen, debe encontrar objetos de clases predefinidas (persona, libro, manzana, basset-griffon artesiano-normando, etc.).  Para resolver este problema con la ayuda de redes neuronales, lo planteamos en t√©rminos de tensores y aprendizaje autom√°tico. <br><br>  Recordamos que una imagen en color es un tensor (H, W, 3) (si no lo recordamos, es decir, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">parte 1</a> ).  Anteriormente, solo sab√≠amos c√≥mo clasificar la imagen completa, pero ahora nuestro objetivo es predecir las posiciones de los objetos de inter√©s (coordenadas de p√≠xeles) en la imagen y sus clases. <br><br>  La idea clave aqu√≠ es resolver dos problemas a la vez: clasificaci√≥n y regresi√≥n.  Usamos una red neuronal para hacer retroceder las coordenadas y clasificar los objetos dentro de ellas. <br><br><div class="spoiler">  <b class="spoiler_title">Clasificaci√≥n?</b>  <b class="spoiler_title">Regresi√≥n?</b> <div class="spoiler_text">  Perm√≠teme recordarte que estamos hablando de las tareas del aprendizaje autom√°tico.  En el problema de <b>clasificaci√≥n</b> , <b>las</b> etiquetas de <b>clase</b> act√∫an como la calidad de las etiquetas verdaderas para los objetos, y predecimos la clase del objeto.  En el problema de <b>regresi√≥n</b> , <b>los n√∫meros reales</b> act√∫an como <b>n√∫meros reales</b> , y predecimos el n√∫mero (por ejemplo: peso, altura, salario, n√∫mero de personajes que mueren en la pr√≥xima serie del Juego de Tronos ...).  En m√°s detalle: usted es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bienvenido a la tercera conferencia de DLSchool (FPMI MIPT)</a> . <br></div></div><br>  Pero las coordenadas del objeto, en general, pueden formalizarse de diferentes maneras, en DL hay tres formas principales: <i>detecci√≥n</i> ( <abbr title="rect√°ngulos que delimitan objetos">cajas de</abbr> objetos), <i>evaluaci√≥n de la postura</i> (puntos clave de los objetos) y <i>segmentaci√≥n</i> ("m√°scaras" de los objetos).  Ahora <abbr title="rect√°ngulos que delimitan objetos"><b>hablemos de</b></abbr> predecir con precisi√≥n los <abbr title="rect√°ngulos que delimitan objetos"><b>cuadros delimitadores</b></abbr> , los puntos y la segmentaci√≥n estar√°n m√°s adelante en el texto. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ki/wt/xm/kiwtxmvvhmwsvqn3_5dovlmp8w8.jpeg" width="500"></div><br>  B√°sicamente, los conjuntos de datos de detecci√≥n est√°n marcados con cuadros en el formato: "coordenadas de las esquinas superior izquierda e inferior derecha para cada objeto en cada imagen" (este formato tambi√©n se denomina <abbr title="&quot;Tlbr&quot;">arriba a la izquierda, abajo a la derecha</abbr> ), y la mayor√≠a de los enfoques de redes neuronales predicen estas coordenadas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uo/zs/tj/uozstjspdifxpslvqyuurauxs2g.png" width="500"></div><br><div class="spoiler">  <b class="spoiler_title">Sobre conjuntos de datos y m√©tricas en el problema de detecci√≥n</b> <div class="spoiler_text">  Despu√©s de configurar la tarea, es mejor ver qu√© datos est√°n disponibles para la capacitaci√≥n y qu√© m√©tricas se utilizan para medir la calidad.  De esto es de lo que hablo lentamente en la primera mitad de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">decimotercera conferencia de la Escuela de Aprendizaje Profundo</a> (en x2.0 es lo m√°ximo). <br></div></div><br>  Antes de sumergirnos en los tipos de redes neuronales para la detecci√≥n, pensemos juntos c√≥mo resolver el problema de detectar cualquier cosa en las im√°genes.  Probablemente, si queremos encontrar un determinado objeto en la imagen, entonces sabemos aproximadamente c√≥mo se ve y qu√© √°rea deber√≠a ocupar en la imagen (aunque puede cambiar). <br><br><div class="spoiler">  <b class="spoiler_title">Inventar la detecci√≥n desde cero</b> <div class="spoiler_text">  El enfoque m√°s simple e ingenuo ser√≠a simplemente hacer un algoritmo de "b√∫squeda de plantilla": dejar que la imagen sea de 100x100 p√≠xeles, y estamos buscando un bal√≥n de f√∫tbol.  Que haya un patr√≥n de bolas de 20x20 p√≠xeles.  Tome esta plantilla y la veremos como una convoluci√≥n a lo largo de la imagen, contando la diferencia p√≠xel por p√≠xel.  As√≠ es como funciona la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">coincidencia de plantillas</a> (a menudo se usa alg√∫n tipo de correlaci√≥n en lugar de la diferencia de p√≠xel por p√≠xel). <br><br>  Si no hay una plantilla, pero hay un clasificador de red neuronal, entonces podemos hacer esto: iremos por una ventana de un tama√±o fijo en la imagen y predeciremos la clase del √°rea actual de la imagen.  Luego, solo decimos que las regiones m√°s probables de los objetos son aquellas en las que el clasificador respondi√≥ con confianza.  Por lo tanto, podemos resolver el problema del hecho de que el objeto se ve diferente en apariencia diferente (ya que fue entrenado para clasificar en una muestra muy diversa). <br><br>  Pero luego aparece un problema: los objetos en las im√°genes tienen diferentes tama√±os.  El mismo bal√≥n de f√∫tbol puede estar en toda la altura / anchura de la imagen, o puede estar lejos de la meta, tomando solo 10-20 p√≠xeles de 1000. Me gustar√≠a escribir el algoritmo de Fuerza Bruta: simplemente recorremos los tama√±os de las ventanas.  Supongamos que tenemos 100x200 p√≠xeles, luego iremos a una ventana de 2x2, 2X3, 3x2, 2x4, 4x2, 3x3 ..., 3x4, 4x3 ... Creo que entiendes que el n√∫mero de ventanas posibles ser√° de 100 * 200, y cada una de las cuales pasaremos por la imagen , realizar operaciones de clasificaci√≥n (100-W_window) * (200 - H_window), lo que lleva mucho tiempo.  Me temo que no esperaremos hasta que dicho algoritmo funcione. <br><br>  Por supuesto, puede elegir las ventanas m√°s caracter√≠sticas seg√∫n el objeto, pero esto tambi√©n funcionar√° durante mucho tiempo, y si es r√°pido, es poco probable que sea exacto: en aplicaciones reales habr√° una cantidad incre√≠ble de variaciones en el tama√±o de los objetos en las im√°genes. <br></div></div><br>  Adem√°s, a veces me basar√© en una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nueva revisi√≥n del √°rea de detecci√≥n a partir de enero de 2019</a> (las fotos tambi√©n ser√°n de ella).  Esta es solo una lectura obligatoria si desea obtener r√°pidamente la visi√≥n m√°s amplia posible de DL en la detecci√≥n. <br><br>  Uno de los primeros art√≠culos sobre detecci√≥n y localizaci√≥n usando CNN fue <a href="">Overfeat</a> .  Los autores afirman que primero utilizaron una red neuronal para la detecci√≥n en ImageNet, reformulando el problema y cambiando la p√©rdida.  El enfoque, por cierto, fue casi de extremo a extremo (a continuaci√≥n se muestra el esquema de sobrecompresi√≥n). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8o/5v/tv/8o5vtvhgukkn7frba0nx8yltfis.png" width="700"></div><br>  La siguiente arquitectura importante fue la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Red neuronal convolucional basada en</a> la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">regi√≥n</a> ( <b>RCNN</b> ), inventada por investigadores de <abbr>FAIR</abbr> en 2014.  Su esencia es que primero predice muchas de las llamadas "regiones de inter√©s" (RoI), dentro de las cuales potencialmente puede haber objetos (usando el algoritmo de b√∫squeda selectiva), y los clasifica y refina las coordenadas de los cuadros usando CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2r/2p/kp/2r2pkpcoysglv4z_v-ll_y14mqw.png" width="700"></div><br>  Es cierto que tal canalizaci√≥n hizo que todo el sistema fuera lento, porque ejecutamos todas las regiones a trav√©s de la red neuronal (lo adelantamos miles de veces).  Un a√±o despu√©s, el mismo FAIR Ross Girshick actualiz√≥ RCNN a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fast-RCNN</a> .  Aqu√≠ la idea era intercambiar la B√∫squeda selectiva y la predicci√≥n de la red: primero, pasamos la imagen completa a trav√©s de una red neuronal pre-entrenada, y luego predecimos regiones de inter√©s sobre el mapa de caracter√≠sticas emitido por la red troncal (por ejemplo, usando la misma B√∫squeda selectiva, pero Hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otros algoritmos</a> ).  Todav√≠a era bastante lento, mucho m√°s lento que el tiempo real (por ahora, suponemos que el tiempo real es inferior a 40 milisegundos por imagen). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tn/uc/d-/tnucd-y6i7tr4edgjeudtrsj16u.png" width="700"></div><br>  La velocidad se vio afectada, sobre todo, no por CNN, sino por el algoritmo de generaci√≥n de cajas en s√≠, por lo que se decidi√≥ reemplazarlo con una segunda red neuronal: la Red de Propuesta de Regi√≥n ( <b>RPN</b> ), que se entrenar√° para predecir las regiones de inter√©s de los objetos.  As√≠ es como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">apareci√≥ Faster-RCNN</a> (s√≠, obviamente no pensaron en el nombre durante mucho tiempo).  Esquema: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6v/h_/ye/6vh_yee2zrsflyhh8jdvtm_bbgy.png" width="700"></div><br>  Luego hubo otra mejora en la forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">R-FCN</a> , no hablaremos de ello en detalle, pero quiero mencionar a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mask-RCNN</a> .  Mask-RCNN es √∫nico, la primera red neuronal que resuelve el <b>problema de la detecci√≥n y la segmentaci√≥n de instancias al mismo tiempo</b> : predice las m√°scaras (siluetas) exactas de los objetos dentro de los cuadros delimitadores.  Su idea es bastante simple: hay dos ramas: para la detecci√≥n y la segmentaci√≥n, y necesita capacitar a la red para ambas tareas a la vez.  Lo principal es tener datos etiquetados.  Mask-RCNN en s√≠ es muy similar a Faster-RCNN: la columna vertebral es la misma, pero al final hay dos <b>"cabezas"</b> (como se llaman a menudo las <b>√∫ltimas capas de la</b> red neuronal) para dos tareas diferentes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7w/ig/hq/7wighq6ox7tptik5f_7d7cez2hg.png" width="700"></div><br>  Estos fueron los llamados enfoques de <b>dos etapas</b> (o <b>basados ‚Äã‚Äãen la regi√≥n</b> ).  Paralelamente a ellos, se desarrollaron an√°logos en la detecci√≥n de DL: enfoques de una <b>etapa</b> .  Estos incluyen redes neuronales como: Detector de disparo √∫nico (SSD), Solo mira una vez (YOLO), Detector de objetos supervisados ‚Äã‚Äã(DSOD), Red de bloqueo de campo receptivo (RFBNet) y muchos otros (consulte el mapa a continuaci√≥n, desde <a href="">este repositorio</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/lc/-y/uelc-yeav4_avjdkycglf9uwrmm.png" width="750"></div><br>  Los enfoques de una etapa, a diferencia de las dos etapas, no utilizan un algoritmo separado para generar cuadros, sino que simplemente predicen varias coordenadas de cuadro para cada mapa de caracter√≠sticas producido por una red neuronal convolucional.  YOLO act√∫a de manera similar, SSD es ligeramente diferente, pero solo hay una idea: una convoluci√≥n 1x1 predice muchos n√∫meros de los mapas de caracter√≠sticas recibidos en profundidad, sin embargo, acordamos de antemano qu√© n√∫mero significa. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qj/ml/w_/qjmlw_ympdcib6jkpfdjdvcirdy.png" width="600"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jr/i3/oy/jri3oymb48sv5vq9dwwdxbndszg.png" width="600"></div><br>  Por ejemplo, predecimos a partir de un mapa de caracter√≠sticas el tama√±o de 13x13x256 es un mapa de caracter√≠sticas de 13x13x (4 * (5 + 80)) n√∫meros, donde en profundidad predecimos 85 n√∫meros para 4 cajas: los primeros 4 n√∫meros en la secuencia son siempre las coordenadas de la caja, el 5to - confianza en el boxeo, y 80 n√∫meros - las probabilidades de cada una de las clases (clasificaci√≥n).  Esto es necesario para luego enviar los n√∫meros necesarios a las p√©rdidas necesarias y entrenar adecuadamente la red neuronal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/xi/o2/aoxio2rty3hgpu2f8mduomvs9nu.png" width="800"></div><br>  Quiero llamar la atenci√≥n sobre el hecho de que la calidad del trabajo del detector depende de la calidad de la red neuronal para extraer caracter√≠sticas (es decir, una <b>red neuronal troncal</b> ).  Por lo general, este papel lo desempe√±a una de las arquitecturas, de la que habl√© en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior</a> (ResNet, SENet, etc.), pero a veces los autores presentan sus propias arquitecturas m√°s √≥ptimas (por ejemplo, Darknet-53 en YOLOv3) o modificaciones (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Feature Pyramid Pooling</a> (FPN)). <br><br>  Una vez m√°s, noto que entrenamos la red para la clasificaci√≥n y la regresi√≥n al mismo tiempo.  En la comunidad, esto se llama p√©rdida de tareas m√∫ltiples: la suma de las p√©rdidas para varias tareas (con algunos coeficientes) aparece en una p√©rdida. <br><br><div class="spoiler">  <b class="spoiler_title">Noticias con p√©rdida multitarea l√≠der</b> <div class="spoiler_text">  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Machines Can See 2019,</a> uno de los oradores us√≥ la p√©rdida de tareas m√∫ltiples para 7 tareas simult√°neamente <s>, Carl</s> .  Result√≥ que algunas tareas se establecieron inicialmente como un contrapeso entre s√≠ y se obtuvo un "conflicto", que impidi√≥ que la red aprendiera mejor que si estuviera entrenada para cada tarea por separado.  Conclusi√≥n: si est√° utilizando la p√©rdida de tareas m√∫ltiples, aseg√∫rese de que estas mismas tareas m√∫ltiples no entren en conflicto con la declaraci√≥n (por ejemplo, predecir los l√≠mites de los objetos y su segmentaci√≥n interna puede interferir entre s√≠, porque estas cosas pueden depender de diferentes signos dentro de la red).  El autor evit√≥ esto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">agregando bloques separados de Exprimir y Excitar para cada tarea</a> . <br></div></div><br>  Recientemente, aparecieron art√≠culos de 2019 en los que los autores declaran una relaci√≥n velocidad / precisi√≥n a√∫n mejor en la tarea de detecci√≥n utilizando la <b>predicci√≥n de recuadro basada en puntos</b> .  Estoy hablando de los art√≠culos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Objetos como puntos"</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"CornerNet-Lite"</a> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ExtremeNet</a> es una modificaci√≥n de CornerNet.  Parece que ahora se les puede llamar SOTA en la detecci√≥n utilizando redes neuronales (pero esto no es exacto). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/qf/mr/wkqfmrenwm3u5f6c6bzinjcffga.png" width="900"></div><br>  Si de repente mi explicaci√≥n de los detectores todav√≠a parec√≠a ca√≥tica e incomprensible, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro video</a> lo discuto lentamente.  Quiz√°s deber√≠as verlo primero. <br><br>  A continuaci√≥n, he presentado tablas de redes neuronales en detecci√≥n con enlaces al c√≥digo y una breve descripci√≥n de los chips de cada red.  Intent√© recopilar solo aquellas redes que son realmente importantes para conocer (al menos sus ideas) para tener una buena idea sobre la detecci√≥n de objetos hoy: <br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (dos etapas)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  C√≥digo </th></tr><tr><td>  2013-2014 </td><td>  <a href="">RCNN</a> </td><td>  Generaci√≥n de regiones de inter√©s y predicci√≥n de redes neuronales de clases dentro de ellas. </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Fast-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primero pase la imagen a trav√©s de la red y luego genere regiones de inter√©s</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Rcnn m√°s r√°pido</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">usar RPN para generar regiones de inter√©s</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">R-FCN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque totalmente convolucional en lugar de generar regiones de inter√©s</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mask-rcnn</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dos "cabezas" para resolver dos tareas a la vez, RoI-Align</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras, TF</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Razonamiento-RCNN</a> </td><td>  Mejorar la calidad de RCNN mediante la construcci√≥n de un gr√°fico de las relaciones sem√°nticas de los objetos. </td><td>  --- </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (una etapa)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  C√≥digo </th></tr><tr><td>  2013-2014 </td><td>  <a href="">Sobrepeso</a> </td><td>  uno de los primeros detectores de redes neuronales </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C ++ (con envoltorios para otros idiomas)</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SSD</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enfoque de una etapa muy flexible utilizado ahora en muchas aplicaciones</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Yolo</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una idea similar a SSD, se est√° desarrollando en paralelo y no menos popular (hay nuevas versiones)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C ++</a> </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YOLOv2 (tambi√©n conocido como YOLO9000)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una serie de mejoras para YOLO</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YOLOv3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una serie de mejoras para YOLOv2</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DSOD</a> </td><td>  Idea de supervisi√≥n profunda e ideas de DenseNet </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cafe</a> </td></tr><tr><td>  2017-2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RFBNet</a> </td><td>  Los filtros de convoluci√≥n se seleccionan cuidadosamente en funci√≥n de la estructura del sistema visual humano (bloque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RF</a> ) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (varios)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  C√≥digo </th></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RetinaNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">P√©rdida focal especial para resolver el problema del desequilibrio de clase</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras</a> </td></tr><tr><td>  2014-2015 </td><td>  <a href="">SPP</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√≥dulo que le permite trabajar eficazmente con im√°genes de diferentes tama√±os</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras</a> </td></tr><tr><td>  2016-2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FPN</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuentan con pir√°mides para una mejor detecci√≥n de objetos de diferentes tama√±os</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NAS-FPN</a> </td><td>  Encontrar el mejor FPN con la b√∫squeda de arquitectura neuronal </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">D√©dalo</a> </td><td>  C√≥mo romper el detector con un ataque de confrontaci√≥n </td><td>  --- </td></tr></tbody></table></div><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Detectores de redes neuronales (basados ‚Äã‚Äãen puntos)</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  A√±o </th><th>  Art√≠culo </th><th>  Idea clave </th><th>  C√≥digo </th></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Centernet</a> </td><td>  Un nuevo enfoque para la detecci√≥n, que permite resolver de manera r√°pida y eficiente el problema de encontrar puntos, cajas y cajas 3D al mismo tiempo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cornernet</a> </td><td>  predicci√≥n de cajas basada en pares de puntos de esquina </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CornerNet-Lite</a> </td><td>  esquina acelerada </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ExtremeNet</a> </td><td>  predicci√≥n de puntos "extremos" de objetos (l√≠mites geom√©tricamente precisos) </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Para comprender c√≥mo se correlaciona la velocidad / calidad de cada arquitectura, puede consultar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta revisi√≥n</a> o su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versi√≥n m√°s popular</a> . <br><br>  La arquitectura est√° bien, pero la detecci√≥n es principalmente una tarea pr√°ctica.  "No tengo cien redes, pero tengo al menos 1 en funcionamiento" - este es mi mensaje.  Hay enlaces al c√≥digo en la tabla anterior, pero personalmente, rara vez encuentro detectores de lanzamiento directamente desde los repositorios (al menos con el objetivo de un mayor despliegue en producci√≥n).  Muy a menudo, se usa una biblioteca para esto, por ejemplo, la API de detecci√≥n de objetos TensorFlow (consulte la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">parte pr√°ctica de mi lecci√≥n</a> ) o una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">biblioteca de investigadores de CUHK</a> .  Les traigo a su atenci√≥n otra s√∫per mesa (les gustan, ¬øverdad?): <br><br><div class="spoiler">  <b class="spoiler_title">Bibliotecas para ejecutar modelos de detecci√≥n</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Titulo </th><th>  Los autores </th><th>  Descripci√≥n </th><th>  Redes neuronales implementadas </th><th>  Marco </th></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Detectron</a> </td><td>  Facebook AI Research </td><td>  Repositorio de Facebook con varios c√≥digos modelo para detectar y evaluar posturas </td><td>  Todo basado en la regi√≥n </td><td>  Caffe2 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">API de detecci√≥n de objetos TF</a> </td><td>  Equipo TensorFlow </td><td>  Muchos modelos listos para usar (se dan pesos) </td><td>  Todos los SSD y basados ‚Äã‚Äãen regiones (con diferentes esquemas) </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Flujo oscuro</a> </td><td>  trio </td><td>  Implementaciones de YOLO y YOLOv2 listas para usar </td><td>  Todos los tipos de YOLO (con modificaciones) excepto YOLOv3 </td><td>  Tensorflow </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mmdetection</a> </td><td>  Abrir MMLab (CUHK) </td><td>  Una gran cantidad de detectores en PyTorch, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vea su art√≠culo</a> </td><td>  Casi todos los modelos excepto la familia YOLO </td><td>  Pytorch </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Darknet (modificado)</a> </td><td>  AlexAB </td><td>  Implementaci√≥n conveniente de YOLOv3 con muchas mejoras en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio original</a> </td><td>  YOLOv3 </td><td>  C ++ </td></tr></tbody></table></div><br></div></div><br>  A menudo necesita detectar un objeto de una sola clase, pero espec√≠fico y altamente variable.  Por ejemplo, para detectar todas las caras en la foto (para mayor verificaci√≥n / conteo de personas), para detectar personas enteras (para reidentificaci√≥n / conteo / seguimiento) o para detectar texto en la escena (para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OCR</a> / traducci√≥n de palabras en la foto).  En general, el enfoque de detecci√≥n "ordinario" aqu√≠ funcionar√° hasta cierto punto, pero cada una de estas subtareas tiene sus propios trucos para mejorar la calidad. <br><br><a name="2"></a><h2>  Detecci√≥n de rostro: no atrapado - no es un ladr√≥n </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/ul/rp/iculrpbc7niyrdxg1yk_8r82nsw.jpeg" width="700"></div><br>  Aqu√≠ aparece cierta especificidad, ya que las caras a menudo ocupan una parte bastante peque√±a de la imagen.  Adem√°s, las personas no siempre miran a la c√°mara, a menudo la cara solo es visible desde un lado.  Uno de los primeros enfoques para el reconocimiento facial fue el famoso detector Viola-Jones basado en cascadas Haar, inventado en 2001. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hc/tf/zn/hctfzn0xudbedi_aymhmlcwkamu.png" width="400"></div><br>  Las redes neuronales <s>no estaban de moda entonces,</s> todav√≠a no ten√≠an una visi√≥n tan fuerte, sin embargo, el buen enfoque hecho a mano hizo su trabajo.  Se usaron activamente varios tipos de m√°scaras de filtro especiales, que ayudaron a extraer regiones faciales de la imagen y sus signos, y luego estos signos se enviaron al clasificador AdaBoost.  Por cierto, este m√©todo realmente funciona bien y ahora, es lo suficientemente r√°pido y comienza <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">con OpenCV</a> .  La desventaja de este detector es que solo ve caras desplegadas frontalmente a la c√°mara.  Uno solo tiene que darse la vuelta un poco y se viola la estabilidad de la detecci√≥n. <br><br>  Para casos tan complejos, puede usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dlib</a> .  Esto es C ++, una biblioteca en la que se implementan muchos algoritmos de visi√≥n, incluso para la detecci√≥n de rostros. <br><br>  De los enfoques de redes neuronales en la detecci√≥n de rostros, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CNN en cascada multitarea (MTCNN)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MatLab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TensorFlow</a> ) es especialmente significativa.  En general, ahora se usa activamente (en la misma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/80/wn/vu/80wnvuf59poodmodzswcgjlyjt4.jpeg" width="400"></div><br>  La idea de MTCNN es utilizar tres redes neuronales secuencialmente (por lo tanto, una <b>"cascada"</b> ) para predecir la posici√≥n de una cara y sus puntos singulares.  En este caso, hay exactamente 5 puntos especiales en la cara: el ojo izquierdo, el ojo derecho, el borde izquierdo de los labios, el borde derecho de los labios y la nariz.  La primera red neuronal de la cascada ( <abbr title="Proposal Net">P-Net</abbr> ) se utiliza para generar regiones potenciales de la cara.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El segundo ( </font></font><abbr title="Refine net"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R-Net</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) - para mejorar las coordenadas de los cuadros recibidos. </font><font style="vertical-align: inherit;">La tercera </font></font><abbr title="Red de puntos de referencia faciales"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">red</font></font></abbr><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> neuronal </font><font style="vertical-align: inherit;">( </font><abbr title="Red de puntos de referencia faciales"><font style="vertical-align: inherit;">O-Net</font></abbr><font style="vertical-align: inherit;"> ) vuelve a retroceder las coordenadas de las cajas y, adem√°s, predice 5 puntos clave de la cara. </font><font style="vertical-align: inherit;">Esta red es una tarea m√∫ltiple porque se resuelven tres tareas: regresi√≥n de puntos de cuadro, clasificaci√≥n de cara / no cara para cada cuadro y regresi√≥n de puntos de cara. </font><font style="vertical-align: inherit;">Adem√°s, MTCNN lo hace todo en tiempo real, es decir, requiere menos de 40 ms por imagen.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/iz/d5/5eizd5lwag9umfo1ccypep42eik.jpeg" width="800"></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬øC√≥mo, usted todav√≠a no lee art√≠culos con ArXiv?</font></font></b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En este caso, te recomiendo que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">intentes</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> leer el </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">art√≠culo original sobre MTCNN</font></a><font style="vertical-align: inherit;"> , si ya tienes experiencia en redes de convoluci√≥n. </font><font style="vertical-align: inherit;">Este art√≠culo solo toma </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5 p√°ginas</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , pero establece toda la informaci√≥n que necesita para comprender el enfoque. </font><font style="vertical-align: inherit;">Pru√©balo, se apretar√° :)</font></font><br></div></div><br>   State-of-the-Art   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dual Shot Face Detector (DSFD)</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FaceBoxes</a> . FaceBoxes      CPU (!),  DSFD    (   2019 ). DSFD  ,  MTCNN,          ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dilated convolutions</a> ),        . ,  dilated convolutions            .    DSFD (,   ?). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xv/or/it/xvoritceua0_ocjteyvm4xvisbq.jpeg"></div><br>     <b></b> ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> ,      . <br><br><a name="3"></a><h2>  :  ( )  </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qn/v9/wo/qnv9woeqru3dioeennrkvalkjqg.png" width="500"></div><br>     .  , ,   bounding box',    (   ),    .     ,   , ,       recognition-,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> . <br><br>       bounding box',        ,    ( ).     , , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">EAST-</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j3/hd/bj/j3hdbj-xozs4voec8ekiz4k1eo0.png" width="500"></div><br><br>  EAST-  ,      ,    : <br><br><ol><li> Text Score Map' (     ) </li><li>     </li><li>        </li></ol><br>  ,      (  ),  .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arxiv-</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dj/vt/uu/djvtuu36dzinoratlumncsgb5t8.png" width="700"></div><br><br>    (    )  ,    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">TextBoxes++</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Caffe</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SegLinks</a> ,  EAST,   ,    . <br><br>         ,  <b></b>     .       ‚Äî    .     ,      ,   ,          . , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MORAN</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  PyTorch</a> )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ASTER</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  TensorFlow</a> )     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vt/ot/nc/vtotncz1d1zhhsiytuzmarta9ta.png" width="700"></div><br><br>    - ,          : CNN  RNN.       ,     .    MORAN':     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hu/qx/_2/huqx_2jakjcggsgacvhj9a8vce4.png" width="300"></div><br>       EAST', -       ,           .  ,           ,     . <br><br>   <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a></b>   ,  / .      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spatial Transformet Network (STN)</a> ,              (,       ,    ).   / STN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6y/z1/p9/6yz1p942ensgepwirqrtzpcnb7q.jpeg" width="700"></div><br>   STN    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  </a> (  ,  )  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">  PyTorch</a> . <br><br>  MORAN (     )    ‚Äî      ,        <b> </b>  x   y,     ,      .    <i><abbr title="correcci√≥n, correcci√≥n">rectification</abbr></i> ,         ( <i>rectifier'</i> ).         : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n4/az/du/n4azduih5p7wd9sx-tqhpgy-p20.png" width="300"></div><br>  Sin embargo, adem√°s de los enfoques para el reconocimiento de texto "modularmente" (red de detecci√≥n -&gt; red de reconocimiento), existe una arquitectura de extremo a extremo: la entrada es una imagen, y la salida es una detecci√≥n y el texto se reconoce dentro de ellos.  Y todo esto es una sola tuber√≠a que aprende ambas tareas a la vez.  En esta direcci√≥n, existe el impresionante trabajo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spotting de texto orientado r√°pidamente con una red unificada ( <b>FOTS</b> ) (</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>PyTorch</b></a> ), donde los autores tambi√©n se√±alan que el enfoque de extremo a extremo es dos veces m√°s r√°pido que "detecci√≥n + reconocimiento".  A continuaci√≥n se muestra el diagrama de la red neuronal FOTS, el bloque RoiRotate desempe√±a un papel especial, debido al cual es posible "emitir gradientes" desde la red para su reconocimiento en la red neuronal para su detecci√≥n (esto es realmente m√°s complicado de lo que parece) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/a5/xh/ch/a5xhchryrwf-zpfudielmab9pqu.png" width="800"></div><br>  Por cierto, cada a√±o se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">celebra la</a> conferencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICDAR</a> , a la que se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">realizan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">varios concursos</a> para el reconocimiento de texto en una variedad de im√°genes. <br><br><h3>  Problemas actuales en la detecci√≥n </h3><br>  En mi opini√≥n, el principal problema en la detecci√≥n ahora no es la calidad del modelo de detector, sino los datos: generalmente son largos y caros de marcar, especialmente si hay muchas clases que deben detectarse (pero por cierto, hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo de una soluci√≥n</a> para 500 clases).  Por lo tanto, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muchos trabajos</a> ahora se dedican a la generaci√≥n de los datos m√°s plausibles "sint√©ticamente" y a obtener un marcado "gratis".  A continuaci√≥n se muestra una imagen de <s>mi diploma de un</s> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo de Nvidia</a> , que trata espec√≠ficamente con la generaci√≥n de datos sint√©ticos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6k/mn/wh/6kmnwhvc1pgahzwtjloa4dy9sbm.png" width="800"></div><br>  Pero a√∫n as√≠ es genial que ahora podamos decir con certeza en qu√© parte de la imagen estar.  Y si queremos, por ejemplo, calcular la cantidad de algo en el marco, entonces es suficiente para detectar esto y dar el n√∫mero de cajas.  En la detecci√≥n de personas, YOLO ordinario tambi√©n funciona bien, solo que lo principal es enviar una gran cantidad de datos.  El mismo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Darkflow es</a> adecuado, y la clase "humana" se encuentra en casi todos los principales conjuntos de datos de detecci√≥n.  Entonces, si queremos usar la c√°mara para contar la cantidad de personas que pasaron, por ejemplo, en un d√≠a, o la cantidad de productos que una persona tom√≥ en una tienda, simplemente detectaremos y daremos la cantidad ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/m2/ts/pl/m2tsplbgrnmoauvlghy8ivq2jdm.jpeg" width="700"></div><br>  Para  Pero si vamos a detectar personas en cada imagen de la c√°mara, entonces podemos calcular su n√∫mero en un cuadro y en dos, ya no, porque no podemos decir exactamente d√≥nde est√° esa persona.  Necesitamos un algoritmo que nos permita contar exactamente personas √∫nicas en la transmisi√≥n de video.  Puede ser un algoritmo de reidentificaci√≥n, pero cuando se trata de video y detecci√≥n, es un pecado no utilizar algoritmos de seguimiento. <br><br><a name="4"></a><h3>  Video y seguimiento: en una sola transmisi√≥n </h3><br>  Hasta ahora, solo hemos hablado de tareas en im√°genes, pero lo m√°s interesante sucede en el video.  Para resolver el mismo reconocimiento de acciones, necesitamos usar no solo el llamado componente <i>espacial</i> , sino tambi√©n el <i>temporal</i> , ya que el video es una secuencia de im√°genes en el tiempo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qm/da/pa/qmdapao0kcytnxhorj7htm6susy.jpeg" width="700"></div><br>  El seguimiento es un an√°logo de la detecci√≥n de im√°genes, pero para video.  Es decir, queremos ense√±arle a la red a predecir no el boxeo en la imagen, sino un tracklet en el tiempo (que es esencialmente una secuencia de cuadros).  A continuaci√≥n se muestra un ejemplo de una imagen que muestra las "colas": las pistas de estas personas en el video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xz/yv/c_/xzyvc_oumhrnf_-bmf8gz5l_hli.png" width="600"></div><br>  Pensemos c√≥mo resolver el problema de seguimiento.  Que haya un video, y sus marcos # 1 y # 2.  Consideremos hasta ahora solo un objeto: rastreamos una bola.  En el cuadro # 1, podemos usar un detector para detectarlo.  En el segundo tambi√©n podemos detectar una pelota, y si est√° all√≠ sola, entonces todo est√° bien: decimos que el boxeo en el cuadro anterior es el boxeo de la misma bola que en el cuadro # 2.  Tambi√©n puede continuar con los cuadros restantes, debajo del gif del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">curso de</a> visi√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pyimagesearch</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_w/et/ak/_wetakdpybemefhert6cxslqaju.gif" width="600"></div><br>  Por cierto, para ahorrar tiempo, no podemos iniciar la red neuronal en el segundo cuadro, sino simplemente "cortar" la caja de la pelota desde el primer cuadro y buscar exactamente la misma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">correlaci√≥n</a> en el segundo cuadro o p√≠xel por p√≠xel.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los rastreadores de correlaci√≥n</a> utilizan este enfoque, se consideran simples y m√°s o menos confiables si se trata de casos simples como "rastrear una pelota frente a la c√°mara en una habitaci√≥n vac√≠a".  Esta tarea tambi√©n se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Visual Object Tracking</a> .  A continuaci√≥n se muestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un ejemplo del trabajo del</a> rastreador de correlaci√≥n utilizando el ejemplo de una persona. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cv/fp/qd/cvfpqdd5ghelxfpucxrejxq6vpm.gif" width="600"></div><br>  Sin embargo, si hay varias detecciones / personas, entonces debe poder hacer coincidir los cuadros del cuadro 1 y del cuadro 2.  La primera idea que se me ocurre es tratar de hacer coincidir el cuadro con el que tiene el √°rea de intersecci√≥n m√°s grande ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">IoU</a> ).  Es cierto que en el caso de varias detecciones superpuestas, dicho rastreador ser√° inestable, por lo que debe utilizar a√∫n m√°s informaci√≥n. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qx/i2/t1/qxi2t1ejpnir0h52ip-23zpf4ti.png" width="600"></div><br>  El enfoque con IoU se basa solo en los <i>signos de</i> detecci√≥n <i>"geom√©tricos"</i> , es decir, simplemente trata de compararlos por proximidad en los marcos.  Pero tenemos a nuestra disposici√≥n una imagen completa (incluso dos en este caso), y podemos usar el hecho de que dentro de estas detecciones hay <i>signos "visuales"</i> .  Adem√°s, tenemos un historial de detecciones para cada persona, lo que nos permite predecir con mayor precisi√≥n su pr√≥xima posici√≥n en funci√≥n de la velocidad y la direcci√≥n del movimiento, esto puede llamarse condicionalmente <i>signos "f√≠sicos"</i> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ay/se/mcaysee7xq25-j6hnvjalhfy5z0.gif" width="700"></div><br>  Uno de los primeros rastreadores en tiempo real, que era completamente confiable y capaz de hacer frente a situaciones dif√≠ciles, se public√≥ en 2016 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Simple Online y Realtime Traker (SORT)</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo Python</a> ).  SORT no us√≥ ning√∫n signo visual y redes neuronales, sino que solo estim√≥ una serie de par√°metros de cada cuadro en cada cuadro: la velocidad actual (x e y por separado) y el tama√±o (altura y ancho).  La relaci√≥n de aspecto de un cuadro siempre se toma desde la primera detecci√≥n de ese cuadro.  Adem√°s, las velocidades se predicen utilizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">filtros de Kalman</a> (generalmente son buenas y ligeras en el mundo del procesamiento de se√±ales), se construye la matriz de intersecci√≥n de las cajas por IoU, y el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algoritmo h√∫ngaro</a> asigna las detecciones. <br><br>  Si le parece que las matem√°ticas ya se han vuelto un poco demasiado, entonces en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> todo se explica de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">manera</a> accesible (esto es medio :). <br><br>  Ya en 2017, se lanz√≥ una modificaci√≥n de SORT en forma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepSORT</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digo para TensorFlow</a> ).  DeepSORT ya ha comenzado a usar la red neuronal para extraer signos visuales, us√°ndolos para resolver colisiones.  La calidad del seguimiento ha crecido: no es por nada que se considera uno de los mejores rastreadores en l√≠nea en la actualidad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/it/ny/15itnyht32oes3n-keaqk36jnpq.gif" width="800"></div><br>  El campo de rastreo se est√° desarrollando activamente: hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rastreadores con redes neuronales siamesas</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rastreadores con RNN</a> .  Mantenga el dedo en el pulso, porque en cualquier d√≠a puede salir una arquitectura a√∫n m√°s precisa y r√°pida (o ya ha salido).  Por cierto, es muy conveniente seguir tales cosas en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PapersWithCode</a> , siempre hay enlaces a art√≠culos y c√≥digos para ellos (si los hay). <br><br><h3>  Ep√≠logo </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/yh/bf/jbyhbf2ovu_ynurp_t_d9_hi4vg.jpeg" width="600"></div><br><br>  Realmente hemos experimentado mucho y aprendido mucho.  Pero la visi√≥n por computadora es un √°rea extremadamente vasta, y yo soy una persona extremadamente terca.  Es por eso que lo veremos en el tercer art√≠culo de este ciclo (¬øser√° el √∫ltimo? Qui√©n sabe ...), donde discutiremos con m√°s detalle la segmentaci√≥n, la evaluaci√≥n de la postura, el reconocimiento de acciones en un video y la generaci√≥n de una descripci√≥n de una imagen usando redes neuronales. <br><br>  PD: Quiero expresar un agradecimiento especial a Vadim Gorbachev por sus valiosos consejos y comentarios en la preparaci√≥n de este y el art√≠culo anterior. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458190/">https://habr.com/ru/post/458190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458172/index.html">M√©todos para emparejar conexiones el√©ctricas al rastrear pares diferenciales en placas de circuito impreso</a></li>
<li><a href="../458176/index.html">La barrera de exaflops se superar√° en 2021</a></li>
<li><a href="../458180/index.html">Servidor DHCP de conmutaci√≥n por error basado en Kea</a></li>
<li><a href="../458182/index.html">Leemos VKontakte a trav√©s de RSS</a></li>
<li><a href="../458188/index.html">Como hice una red social en 2019</a></li>
<li><a href="../458202/index.html">Solo eche un vistazo a SObjectizer si desea usar Actores o CSP en su proyecto C ++</a></li>
<li><a href="../458204/index.html">C√≥mo evaluar el rendimiento del almacenamiento en Linux: evaluaci√≥n comparativa utilizando herramientas abiertas</a></li>
<li><a href="../458218/index.html">¬øCirug√≠a maxilofacial o no? Esa es la pregunta</a></li>
<li><a href="../458220/index.html">El resumen de materiales interesantes para el desarrollador m√≥vil # 304 (del 24 al 30 de junio)</a></li>
<li><a href="../458222/index.html">Romper un juego de memoria: toda una historia de detectives</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>