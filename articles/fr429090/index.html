<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🆚 👨🏽‍💻 🦂 Introduction à l'apprentissage renforcé 👳🏿 🛏️ 💨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour à tous! 

 Nous avons ouvert un nouveau volet pour le cours d' apprentissage automatique , alors attendez dans un proche avenir les articles l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introduction à l'apprentissage renforcé</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Bonjour à tous! <br><br>  Nous avons ouvert un nouveau volet pour le cours d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage automatique</a> , alors attendez dans un proche avenir les articles liés à cette discipline, pour ainsi dire.  Eh bien, bien sûr, des séminaires ouverts.  Voyons maintenant ce qu'est l'apprentissage par renforcement. <br><br>  L'apprentissage renforcé est une forme importante d'apprentissage automatique, où un agent apprend à se comporter dans un environnement en effectuant des actions et en voyant les résultats. <br><br>  Ces dernières années, nous avons connu de nombreux succès dans ce domaine de recherche fascinant.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind et Deep Q Learning Architecture</a> en 2014, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">victoire sur go champion avec AlphaGo</a> en 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI et PPO</a> en 2017, entre autres. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  Dans cette série d'articles, nous nous concentrerons sur l'étude des différentes architectures utilisées aujourd'hui pour résoudre le problème de l'apprentissage renforcé.  Il s'agit notamment de Q-learning, Deep Q-learning, Policy Gradients, Actor Critic et PPO. <br><br>  Dans cet article, vous apprendrez: <br><br><ul><li>  Qu'est-ce que l'apprentissage par renforcement et pourquoi les récompenses sont une idée centrale </li><li>  Trois approches d'apprentissage par renforcement </li><li>  Que signifie «profond» dans l'apprentissage par renforcement profond </li></ul><br>  Il est très important de maîtriser ces aspects avant de plonger dans la mise en place d'agents d'apprentissage par renforcement. <br><br>  L'idée de la formation de renforcement est que l'agent apprendra de l'environnement en interagissant avec lui et en recevant des récompenses pour effectuer des actions. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  L'apprentissage par l'interaction avec l'environnement provient de notre expérience naturelle.  Imaginez que vous êtes un enfant dans le salon.  Vous voyez la cheminée et allez-y. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  A proximité chaleureux, vous vous sentez bien (récompense positive +1).  Vous comprenez que le feu est une chose positive. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Mais ensuite, vous essayez de toucher le feu.  Aïe!  Il s'est brûlé la main (récompense négative -1).  Vous venez de réaliser que le feu est positif lorsque vous êtes à une distance suffisante car il produit de la chaleur.  Mais si vous vous approchez de lui, vous serez brûlé. <br><br>  C'est ainsi que les gens apprennent par l'interaction.  L'apprentissage renforcé est simplement une approche informatique de l'apprentissage par l'action. <br><br>  <b>Processus d'apprentissage par renforcement</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Par exemple, imaginez un agent apprenant à jouer à Super Mario Bros.  Le processus d'apprentissage par renforcement (RL) peut être modélisé comme un cycle qui fonctionne comme suit: <br><br><ul><li>  L'agent reçoit l'état S0 de l'environnement (dans notre cas, nous obtenons la première image du jeu (état) de Super Mario Bros (environnement)) </li><li>  Sur la base de cet état S0, l'agent entreprend l'action A0 (l'agent se déplace vers la droite) </li><li>  L'environnement passe à un nouvel état S1 (nouvelle trame) </li><li>  L'environnement donne une récompense à l'agent R1 (pas mort: +1) </li></ul><br>  Ce cycle RL produit une séquence d' <b>états, d'actions et de récompenses.</b> <br>  L'objectif de l'agent est de maximiser les récompenses cumulées attendues. <br><br>  <b>Hypothèses de récompense d'idée centrale</b> <br><br>  Pourquoi l'objectif d'un agent est-il de maximiser les récompenses cumulées attendues?  Eh bien, l'apprentissage par renforcement est basé sur l'idée d'une hypothèse de récompense.  Tous les objectifs peuvent être décrits en maximisant les récompenses cumulées attendues. <br><br>  <b>Par conséquent, dans la formation de renforcement, afin d'obtenir le meilleur comportement, nous devons maximiser les récompenses accumulées attendues.</b> <br><br>  La récompense accumulée à chaque pas de temps t peut s'écrire: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Cela équivaut à: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  Cependant, en réalité, nous ne pouvons pas simplement ajouter de telles récompenses.  Les récompenses qui arrivent plus tôt (au début du jeu) sont plus probables, car elles sont plus prévisibles que les récompenses à l'avenir. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Supposons que votre agent soit une petite souris et que votre adversaire soit un chat.  Votre objectif est de manger le maximum de fromage avant que le chat ne vous mange.  Comme nous le voyons dans le diagramme, une souris est plus susceptible de manger du fromage à côté d'elle-même que du fromage près d'un chat (plus on est près de lui, plus il est dangereux). <br><br>  En conséquence, la récompense d'un chat, même si elle est supérieure (plus de fromage), sera réduite.  Nous ne sommes pas sûrs de pouvoir en manger.  Pour réduire la rémunération, nous procédons comme suit: <br><br><ul><li>  Nous déterminons le taux d'actualisation appelé gamma.  Il doit être compris entre 0 et 1. </li><li>  Plus le gamma est grand, plus la remise est faible.  Cela signifie que l'agent d'apprentissage se préoccupe davantage des récompenses à long terme. </li><li>  En revanche, plus le gamma est petit, plus la remise est importante.  Cela signifie que la priorité est donnée aux récompenses à court terme (fromage le plus proche). </li></ul><br>  La contrepartie attendue cumulée, compte tenu de l'actualisation, est la suivante: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  En gros, chaque récompense sera réduite en utilisant le gamma de l'indicateur de temps.  À mesure que le pas de temps augmente, le chat se rapproche de nous, de sorte que la récompense future devient de moins en moins probable. <br><br>  <b>Tâches occasionnelles ou continues</b> <br><br>  Une tâche est une instance du problème d'apprentissage avec renforcement.  Nous pouvons avoir deux types de tâches: épisodiques et continues. <br><br>  <b>Tâche épisodique</b> <br><br>  Dans ce cas, nous avons un point de départ et un point d'arrivée <b>(état terminal).</b>  <b>Cela crée un épisode</b> : une liste d'états, d'actions, de récompenses et de nouveaux états. <br>  Prenez Super Mario Bros par exemple: l'épisode commence avec le lancement du nouveau Mario et se termine lorsque vous êtes tué ou atteignez la fin du niveau. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>Le début d'un nouvel épisode</i> <br><br>  <b>Tâches continues</b> <br><br>  <b>Ce sont des tâches qui durent indéfiniment (sans état terminal)</b> .  Dans ce cas, l'agent doit apprendre à choisir les meilleures actions et en même temps interagir avec l'environnement. <br><br>  Par exemple, un agent qui effectue des transactions boursières automatisées.  Il n'y a pas de point de départ et d'état terminal pour cette tâche.  <b>L'agent continue de travailler jusqu'à ce que nous décidions de l'arrêter.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Monte Carlo vs méthode de décalage horaire</b> <br><br>  Il y a deux façons d'apprendre: <br><br><ul><li>  Récolter des récompenses à la fin de l'épisode puis calculer le maximum de récompenses futures attendues - approche Monte Carlo </li><li>  Évaluation des récompenses à chaque étape - une différence temporaire </li></ul><br>  <b>Monte Carlo</b> <br><br>  À la fin de l'épisode (l'agent atteint un «état terminal»), l'agent examine la récompense totale accumulée pour voir dans quelle mesure il a réussi.  Dans l'approche Monte Carlo, les récompenses ne sont reçues qu'à la fin de la partie. <br><br>  Ensuite, nous commençons un nouveau jeu avec des connaissances augmentées.  <b>L'agent prend les meilleures décisions à chaque itération.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Voici un exemple: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Si nous prenons le labyrinthe comme un environnement: <br><br><ul><li>  Nous partons toujours du même point de départ. </li><li>  Nous arrêtons l'épisode si le chat nous mange ou si nous bougeons&gt; 20 pas. </li><li>  À la fin de l'épisode, nous avons une liste d'états, d'actions, de récompenses et de nouveaux états. </li><li>  L'agent résume la récompense totale de Gt (pour voir comment il l'a fait). </li><li>  Ensuite, il met à jour V (st) selon la formule ci-dessus. </li><li>  Ensuite, un nouveau jeu commence avec de nouvelles connaissances. </li></ul><br>  En exécutant de plus en plus d'épisodes, l' <b>agent apprendra à jouer de mieux en mieux.</b> <br><br>  <b>Différences de temps: apprendre à chaque pas de temps</b> <br><br>  La méthode d'apprentissage par différence temporelle (TD) n'attendra pas la fin de l'épisode pour mettre à jour la récompense la plus élevée possible.  Il mettra à jour V en fonction de l'expérience acquise. <br><br>  Cette méthode est appelée TD (0) ou <b>TD par étapes (met à jour la fonction utilitaire après une seule étape).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  Les méthodes TD n'attendent que le prochain <b>pas de temps pour mettre à jour les valeurs.</b>  Au temps t + 1 <b>, une cible TD est formée en utilisant la récompense Rt + 1 et la note actuelle V (St + 1).</b> <br><br>  La cible TD est une estimation de la valeur attendue: en fait, vous mettez à jour la note V (St) précédente vers la cible en une seule étape. <br><br>  <b>Exploration / exploitation de compromis</b> <br><br>  Avant d'envisager différentes stratégies pour résoudre les problèmes d'entraînement par renforcement, nous devons considérer un autre sujet très important: le compromis entre l'exploration et l'exploitation. <br><br><ul><li>  L'intelligence trouve plus d'informations sur l'environnement. </li><li>  L'exploitation utilise des informations connues pour maximiser les récompenses. </li></ul><br>  N'oubliez pas que l'objectif de notre agent RL est de maximiser les récompenses cumulées attendues.  Cependant, nous pouvons tomber dans un piège commun. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  Dans ce jeu, notre souris peut avoir un nombre infini de petits morceaux de fromage (+1 chacun).  Mais au sommet du labyrinthe, il y a un morceau de fromage géant (+1000).  Cependant, si nous nous concentrons uniquement sur les récompenses, notre agent n'atteindra jamais un gigantesque morceau.  Au lieu de cela, il n'utilisera que la source de récompenses la plus proche, même si cette source est petite (exploitation).  Mais si notre agent reconnait un peu, il pourra trouver une grosse récompense. <br><br>  C'est ce que nous appelons un compromis entre exploration et exploitation.  Nous devons définir une règle qui aidera à faire face à ce compromis.  Dans les prochains articles, vous apprendrez différentes façons de procéder. <br><br>  <b>Trois approches d'apprentissage par renforcement</b> <br><br>  Maintenant que nous avons identifié les principaux éléments de l'apprentissage par renforcement, passons à trois approches pour résoudre l'apprentissage renforcé: basé sur les coûts, basé sur les politiques et basé sur le modèle. <br><br>  <b>Basé sur le coût</b> <br><br>  Dans le RL basé sur les coûts, l'objectif est d'optimiser la fonction d'utilité V (s). <br>  Une fonction d'utilité est une fonction qui nous informe de la récompense maximale attendue qu'un agent recevra dans chaque état. <br><br>  La valeur de chaque état est le montant total de la récompense que l'agent peut s'attendre à accumuler à l'avenir, à partir de cet état. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  L'agent utilisera cette fonction utilitaire pour décider de l'état à choisir à chaque étape.  L'agent sélectionne l'état avec la valeur la plus élevée. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Dans l'exemple du labyrinthe, à chaque étape, nous prendrons la valeur la plus élevée: -7, puis -6, puis -5 (etc.) pour atteindre l'objectif. <br><br>  <b>Basé sur la politique</b> <br><br>  Dans RL basé sur des politiques, nous voulons optimiser directement la fonction de politique π (s) sans utiliser la fonction d'utilité.  Une politique est ce qui détermine le comportement d'un agent à un moment donné. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>action = politique (état)</i> <br>  Nous étudions la fonction de la politique.  Cela nous permet de corréler chaque état avec la meilleure action appropriée. <br><br>  Il existe deux types de politiques: <br><br><ul><li>  Déterministe: la politique dans un État donné renverra toujours la même action. </li><li>  Stochastique: affiche la probabilité de distribution par action. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Comme vous pouvez le voir, la politique indique directement la meilleure action pour chaque étape. <br><br>  <b>Basé sur le modèle</b> <br><br>  Dans RL basé sur un modèle, nous modélisons l'environnement.  Cela signifie que nous créons un modèle de comportement environnemental.  Le problème est que chaque environnement aura besoin d'une vue différente du modèle.  C'est pourquoi nous ne nous concentrerons pas beaucoup sur ce type de formation dans les articles suivants. <br><br>  <b>Présentation de l'apprentissage par renforcement profond</b> <br><br>  L'apprentissage par renforcement profond introduit des réseaux de neurones profonds pour résoudre les problèmes d'apprentissage renforcé - d'où le nom «profond». <br>  Par exemple, dans le prochain article, nous travaillerons sur Q-Learning (apprentissage par renforcement classique) et Deep Q-Learning. <br><br>  Vous verrez la différence dans le fait que dans la première approche, nous utilisons l'algorithme traditionnel pour créer la table Q, ce qui nous aide à trouver l'action à entreprendre pour chaque état. <br><br>  Dans la deuxième approche, nous utiliserons un réseau de neurones (pour approximer les récompenses basées sur l'état: valeur q). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Tableau de conception inspiré par Udacity Q</i> <i><br></i> <br><br>  C’est tout.  Comme toujours, nous attendons vos commentaires ou questions ici, ou vous pouvez les poser au professeur de cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Arthur Kadurin</a> dans sa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">leçon ouverte</a> sur le réseautage. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr429090/">https://habr.com/ru/post/fr429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr429078/index.html">Comment créer de l'art procédural en moins de 100 lignes de code</a></li>
<li><a href="../fr429082/index.html">La Thaïlande sans stéréotypes</a></li>
<li><a href="../fr429084/index.html">La deuxième vie du four électrique "Kharkov"</a></li>
<li><a href="../fr429086/index.html">Fête de la bière de sauvegarde</a></li>
<li><a href="../fr429088/index.html">Exécution de requêtes GraphQL avec OdataToEntity</a></li>
<li><a href="../fr429092/index.html">Pourquoi la furtivité dans l'espace est-elle toujours là</a></li>
<li><a href="../fr429094/index.html">Son directionnel: une technologie qui peut remplacer les écouteurs - comment ça marche</a></li>
<li><a href="../fr429096/index.html">Antiquités: ZX Spectrum, programmes de cassettes et haute définition</a></li>
<li><a href="../fr429102/index.html">Ventes de véhicules électriques au Canada</a></li>
<li><a href="../fr429104/index.html">Données élevées</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>