<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÜö üë®üèΩ‚Äçüíª ü¶Ç Introduction √† l'apprentissage renforc√© üë≥üèø üõèÔ∏è üí®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour √† tous! 

 Nous avons ouvert un nouveau volet pour le cours d' apprentissage automatique , alors attendez dans un proche avenir les articles l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introduction √† l'apprentissage renforc√©</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/">  Bonjour √† tous! <br><br>  Nous avons ouvert un nouveau volet pour le cours d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">apprentissage automatique</a> , alors attendez dans un proche avenir les articles li√©s √† cette discipline, pour ainsi dire.  Eh bien, bien s√ªr, des s√©minaires ouverts.  Voyons maintenant ce qu'est l'apprentissage par renforcement. <br><br>  L'apprentissage renforc√© est une forme importante d'apprentissage automatique, o√π un agent apprend √† se comporter dans un environnement en effectuant des actions et en voyant les r√©sultats. <br><br>  Ces derni√®res ann√©es, nous avons connu de nombreux succ√®s dans ce domaine de recherche fascinant.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepMind et Deep Q Learning Architecture</a> en 2014, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">victoire sur go champion avec AlphaGo</a> en 2016, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI et PPO</a> en 2017, entre autres. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  Dans cette s√©rie d'articles, nous nous concentrerons sur l'√©tude des diff√©rentes architectures utilis√©es aujourd'hui pour r√©soudre le probl√®me de l'apprentissage renforc√©.  Il s'agit notamment de Q-learning, Deep Q-learning, Policy Gradients, Actor Critic et PPO. <br><br>  Dans cet article, vous apprendrez: <br><br><ul><li>  Qu'est-ce que l'apprentissage par renforcement et pourquoi les r√©compenses sont une id√©e centrale </li><li>  Trois approches d'apprentissage par renforcement </li><li>  Que signifie ¬´profond¬ª dans l'apprentissage par renforcement profond </li></ul><br>  Il est tr√®s important de ma√Ætriser ces aspects avant de plonger dans la mise en place d'agents d'apprentissage par renforcement. <br><br>  L'id√©e de la formation de renforcement est que l'agent apprendra de l'environnement en interagissant avec lui et en recevant des r√©compenses pour effectuer des actions. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  L'apprentissage par l'interaction avec l'environnement provient de notre exp√©rience naturelle.  Imaginez que vous √™tes un enfant dans le salon.  Vous voyez la chemin√©e et allez-y. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  A proximit√© chaleureux, vous vous sentez bien (r√©compense positive +1).  Vous comprenez que le feu est une chose positive. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  Mais ensuite, vous essayez de toucher le feu.  A√Øe!  Il s'est br√ªl√© la main (r√©compense n√©gative -1).  Vous venez de r√©aliser que le feu est positif lorsque vous √™tes √† une distance suffisante car il produit de la chaleur.  Mais si vous vous approchez de lui, vous serez br√ªl√©. <br><br>  C'est ainsi que les gens apprennent par l'interaction.  L'apprentissage renforc√© est simplement une approche informatique de l'apprentissage par l'action. <br><br>  <b>Processus d'apprentissage par renforcement</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  Par exemple, imaginez un agent apprenant √† jouer √† Super Mario Bros.  Le processus d'apprentissage par renforcement (RL) peut √™tre mod√©lis√© comme un cycle qui fonctionne comme suit: <br><br><ul><li>  L'agent re√ßoit l'√©tat S0 de l'environnement (dans notre cas, nous obtenons la premi√®re image du jeu (√©tat) de Super Mario Bros (environnement)) </li><li>  Sur la base de cet √©tat S0, l'agent entreprend l'action A0 (l'agent se d√©place vers la droite) </li><li>  L'environnement passe √† un nouvel √©tat S1 (nouvelle trame) </li><li>  L'environnement donne une r√©compense √† l'agent R1 (pas mort: +1) </li></ul><br>  Ce cycle RL produit une s√©quence d' <b>√©tats, d'actions et de r√©compenses.</b> <br>  L'objectif de l'agent est de maximiser les r√©compenses cumul√©es attendues. <br><br>  <b>Hypoth√®ses de r√©compense d'id√©e centrale</b> <br><br>  Pourquoi l'objectif d'un agent est-il de maximiser les r√©compenses cumul√©es attendues?  Eh bien, l'apprentissage par renforcement est bas√© sur l'id√©e d'une hypoth√®se de r√©compense.  Tous les objectifs peuvent √™tre d√©crits en maximisant les r√©compenses cumul√©es attendues. <br><br>  <b>Par cons√©quent, dans la formation de renforcement, afin d'obtenir le meilleur comportement, nous devons maximiser les r√©compenses accumul√©es attendues.</b> <br><br>  La r√©compense accumul√©e √† chaque pas de temps t peut s'√©crire: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  Cela √©quivaut √†: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  Cependant, en r√©alit√©, nous ne pouvons pas simplement ajouter de telles r√©compenses.  Les r√©compenses qui arrivent plus t√¥t (au d√©but du jeu) sont plus probables, car elles sont plus pr√©visibles que les r√©compenses √† l'avenir. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Supposons que votre agent soit une petite souris et que votre adversaire soit un chat.  Votre objectif est de manger le maximum de fromage avant que le chat ne vous mange.  Comme nous le voyons dans le diagramme, une souris est plus susceptible de manger du fromage √† c√¥t√© d'elle-m√™me que du fromage pr√®s d'un chat (plus on est pr√®s de lui, plus il est dangereux). <br><br>  En cons√©quence, la r√©compense d'un chat, m√™me si elle est sup√©rieure (plus de fromage), sera r√©duite.  Nous ne sommes pas s√ªrs de pouvoir en manger.  Pour r√©duire la r√©mun√©ration, nous proc√©dons comme suit: <br><br><ul><li>  Nous d√©terminons le taux d'actualisation appel√© gamma.  Il doit √™tre compris entre 0 et 1. </li><li>  Plus le gamma est grand, plus la remise est faible.  Cela signifie que l'agent d'apprentissage se pr√©occupe davantage des r√©compenses √† long terme. </li><li>  En revanche, plus le gamma est petit, plus la remise est importante.  Cela signifie que la priorit√© est donn√©e aux r√©compenses √† court terme (fromage le plus proche). </li></ul><br>  La contrepartie attendue cumul√©e, compte tenu de l'actualisation, est la suivante: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  En gros, chaque r√©compense sera r√©duite en utilisant le gamma de l'indicateur de temps.  √Ä mesure que le pas de temps augmente, le chat se rapproche de nous, de sorte que la r√©compense future devient de moins en moins probable. <br><br>  <b>T√¢ches occasionnelles ou continues</b> <br><br>  Une t√¢che est une instance du probl√®me d'apprentissage avec renforcement.  Nous pouvons avoir deux types de t√¢ches: √©pisodiques et continues. <br><br>  <b>T√¢che √©pisodique</b> <br><br>  Dans ce cas, nous avons un point de d√©part et un point d'arriv√©e <b>(√©tat terminal).</b>  <b>Cela cr√©e un √©pisode</b> : une liste d'√©tats, d'actions, de r√©compenses et de nouveaux √©tats. <br>  Prenez Super Mario Bros par exemple: l'√©pisode commence avec le lancement du nouveau Mario et se termine lorsque vous √™tes tu√© ou atteignez la fin du niveau. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>Le d√©but d'un nouvel √©pisode</i> <br><br>  <b>T√¢ches continues</b> <br><br>  <b>Ce sont des t√¢ches qui durent ind√©finiment (sans √©tat terminal)</b> .  Dans ce cas, l'agent doit apprendre √† choisir les meilleures actions et en m√™me temps interagir avec l'environnement. <br><br>  Par exemple, un agent qui effectue des transactions boursi√®res automatis√©es.  Il n'y a pas de point de d√©part et d'√©tat terminal pour cette t√¢che.  <b>L'agent continue de travailler jusqu'√† ce que nous d√©cidions de l'arr√™ter.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Monte Carlo vs m√©thode de d√©calage horaire</b> <br><br>  Il y a deux fa√ßons d'apprendre: <br><br><ul><li>  R√©colter des r√©compenses √† la fin de l'√©pisode puis calculer le maximum de r√©compenses futures attendues - approche Monte Carlo </li><li>  √âvaluation des r√©compenses √† chaque √©tape - une diff√©rence temporaire </li></ul><br>  <b>Monte Carlo</b> <br><br>  √Ä la fin de l'√©pisode (l'agent atteint un ¬´√©tat terminal¬ª), l'agent examine la r√©compense totale accumul√©e pour voir dans quelle mesure il a r√©ussi.  Dans l'approche Monte Carlo, les r√©compenses ne sont re√ßues qu'√† la fin de la partie. <br><br>  Ensuite, nous commen√ßons un nouveau jeu avec des connaissances augment√©es.  <b>L'agent prend les meilleures d√©cisions √† chaque it√©ration.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Voici un exemple: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Si nous prenons le labyrinthe comme un environnement: <br><br><ul><li>  Nous partons toujours du m√™me point de d√©part. </li><li>  Nous arr√™tons l'√©pisode si le chat nous mange ou si nous bougeons&gt; 20 pas. </li><li>  √Ä la fin de l'√©pisode, nous avons une liste d'√©tats, d'actions, de r√©compenses et de nouveaux √©tats. </li><li>  L'agent r√©sume la r√©compense totale de Gt (pour voir comment il l'a fait). </li><li>  Ensuite, il met √† jour V (st) selon la formule ci-dessus. </li><li>  Ensuite, un nouveau jeu commence avec de nouvelles connaissances. </li></ul><br>  En ex√©cutant de plus en plus d'√©pisodes, l' <b>agent apprendra √† jouer de mieux en mieux.</b> <br><br>  <b>Diff√©rences de temps: apprendre √† chaque pas de temps</b> <br><br>  La m√©thode d'apprentissage par diff√©rence temporelle (TD) n'attendra pas la fin de l'√©pisode pour mettre √† jour la r√©compense la plus √©lev√©e possible.  Il mettra √† jour V en fonction de l'exp√©rience acquise. <br><br>  Cette m√©thode est appel√©e TD (0) ou <b>TD par √©tapes (met √† jour la fonction utilitaire apr√®s une seule √©tape).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  Les m√©thodes TD n'attendent que le prochain <b>pas de temps pour mettre √† jour les valeurs.</b>  Au temps t + 1 <b>, une cible TD est form√©e en utilisant la r√©compense Rt + 1 et la note actuelle V (St + 1).</b> <br><br>  La cible TD est une estimation de la valeur attendue: en fait, vous mettez √† jour la note V (St) pr√©c√©dente vers la cible en une seule √©tape. <br><br>  <b>Exploration / exploitation de compromis</b> <br><br>  Avant d'envisager diff√©rentes strat√©gies pour r√©soudre les probl√®mes d'entra√Ænement par renforcement, nous devons consid√©rer un autre sujet tr√®s important: le compromis entre l'exploration et l'exploitation. <br><br><ul><li>  L'intelligence trouve plus d'informations sur l'environnement. </li><li>  L'exploitation utilise des informations connues pour maximiser les r√©compenses. </li></ul><br>  N'oubliez pas que l'objectif de notre agent RL est de maximiser les r√©compenses cumul√©es attendues.  Cependant, nous pouvons tomber dans un pi√®ge commun. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  Dans ce jeu, notre souris peut avoir un nombre infini de petits morceaux de fromage (+1 chacun).  Mais au sommet du labyrinthe, il y a un morceau de fromage g√©ant (+1000).  Cependant, si nous nous concentrons uniquement sur les r√©compenses, notre agent n'atteindra jamais un gigantesque morceau.  Au lieu de cela, il n'utilisera que la source de r√©compenses la plus proche, m√™me si cette source est petite (exploitation).  Mais si notre agent reconnait un peu, il pourra trouver une grosse r√©compense. <br><br>  C'est ce que nous appelons un compromis entre exploration et exploitation.  Nous devons d√©finir une r√®gle qui aidera √† faire face √† ce compromis.  Dans les prochains articles, vous apprendrez diff√©rentes fa√ßons de proc√©der. <br><br>  <b>Trois approches d'apprentissage par renforcement</b> <br><br>  Maintenant que nous avons identifi√© les principaux √©l√©ments de l'apprentissage par renforcement, passons √† trois approches pour r√©soudre l'apprentissage renforc√©: bas√© sur les co√ªts, bas√© sur les politiques et bas√© sur le mod√®le. <br><br>  <b>Bas√© sur le co√ªt</b> <br><br>  Dans le RL bas√© sur les co√ªts, l'objectif est d'optimiser la fonction d'utilit√© V (s). <br>  Une fonction d'utilit√© est une fonction qui nous informe de la r√©compense maximale attendue qu'un agent recevra dans chaque √©tat. <br><br>  La valeur de chaque √©tat est le montant total de la r√©compense que l'agent peut s'attendre √† accumuler √† l'avenir, √† partir de cet √©tat. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  L'agent utilisera cette fonction utilitaire pour d√©cider de l'√©tat √† choisir √† chaque √©tape.  L'agent s√©lectionne l'√©tat avec la valeur la plus √©lev√©e. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  Dans l'exemple du labyrinthe, √† chaque √©tape, nous prendrons la valeur la plus √©lev√©e: -7, puis -6, puis -5 (etc.) pour atteindre l'objectif. <br><br>  <b>Bas√© sur la politique</b> <br><br>  Dans RL bas√© sur des politiques, nous voulons optimiser directement la fonction de politique œÄ (s) sans utiliser la fonction d'utilit√©.  Une politique est ce qui d√©termine le comportement d'un agent √† un moment donn√©. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>action = politique (√©tat)</i> <br>  Nous √©tudions la fonction de la politique.  Cela nous permet de corr√©ler chaque √©tat avec la meilleure action appropri√©e. <br><br>  Il existe deux types de politiques: <br><br><ul><li>  D√©terministe: la politique dans un √âtat donn√© renverra toujours la m√™me action. </li><li>  Stochastique: affiche la probabilit√© de distribution par action. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  Comme vous pouvez le voir, la politique indique directement la meilleure action pour chaque √©tape. <br><br>  <b>Bas√© sur le mod√®le</b> <br><br>  Dans RL bas√© sur un mod√®le, nous mod√©lisons l'environnement.  Cela signifie que nous cr√©ons un mod√®le de comportement environnemental.  Le probl√®me est que chaque environnement aura besoin d'une vue diff√©rente du mod√®le.  C'est pourquoi nous ne nous concentrerons pas beaucoup sur ce type de formation dans les articles suivants. <br><br>  <b>Pr√©sentation de l'apprentissage par renforcement profond</b> <br><br>  L'apprentissage par renforcement profond introduit des r√©seaux de neurones profonds pour r√©soudre les probl√®mes d'apprentissage renforc√© - d'o√π le nom ¬´profond¬ª. <br>  Par exemple, dans le prochain article, nous travaillerons sur Q-Learning (apprentissage par renforcement classique) et Deep Q-Learning. <br><br>  Vous verrez la diff√©rence dans le fait que dans la premi√®re approche, nous utilisons l'algorithme traditionnel pour cr√©er la table Q, ce qui nous aide √† trouver l'action √† entreprendre pour chaque √©tat. <br><br>  Dans la deuxi√®me approche, nous utiliserons un r√©seau de neurones (pour approximer les r√©compenses bas√©es sur l'√©tat: valeur q). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Tableau de conception inspir√© par Udacity Q</i> <i><br></i> <br><br>  C‚Äôest tout.  Comme toujours, nous attendons vos commentaires ou questions ici, ou vous pouvez les poser au professeur de cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Arthur Kadurin</a> dans sa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le√ßon ouverte</a> sur le r√©seautage. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr429090/">https://habr.com/ru/post/fr429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr429078/index.html">Comment cr√©er de l'art proc√©dural en moins de 100 lignes de code</a></li>
<li><a href="../fr429082/index.html">La Tha√Ølande sans st√©r√©otypes</a></li>
<li><a href="../fr429084/index.html">La deuxi√®me vie du four √©lectrique "Kharkov"</a></li>
<li><a href="../fr429086/index.html">F√™te de la bi√®re de sauvegarde</a></li>
<li><a href="../fr429088/index.html">Ex√©cution de requ√™tes GraphQL avec OdataToEntity</a></li>
<li><a href="../fr429092/index.html">Pourquoi la furtivit√© dans l'espace est-elle toujours l√†</a></li>
<li><a href="../fr429094/index.html">Son directionnel: une technologie qui peut remplacer les √©couteurs - comment √ßa marche</a></li>
<li><a href="../fr429096/index.html">Antiquit√©s: ZX Spectrum, programmes de cassettes et haute d√©finition</a></li>
<li><a href="../fr429102/index.html">Ventes de v√©hicules √©lectriques au Canada</a></li>
<li><a href="../fr429104/index.html">Donn√©es √©lev√©es</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>