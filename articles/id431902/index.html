<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸµï¸ â¬œï¸ ğŸ™‚ Bagaimana kami membangun repositori tampilan iklan yang cepat dan andal ğŸ¯ ğŸ‘´ğŸ½ ğŸ•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salah satu fungsi situs iklan kami yang tidak mencolok namun penting adalah untuk menyimpan dan menampilkan jumlah penayangannya. Situs kami telah men...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana kami membangun repositori tampilan iklan yang cepat dan andal</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/kolesa/blog/431902/"> Salah satu fungsi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs iklan kami yang</a> tidak mencolok namun penting adalah untuk menyimpan dan menampilkan jumlah penayangannya.  Situs kami telah menonton penayangan iklan selama lebih dari 10 tahun.  Implementasi teknis fungsi berhasil berubah beberapa kali selama waktu ini, dan sekarang ini adalah layanan (mikro) di Go, bekerja dengan Redis sebagai antrian cache dan tugas, dan dengan MongoDB sebagai penyimpanan persisten.  Beberapa tahun yang lalu, ia belajar bekerja tidak hanya dengan jumlah penayangan iklan, tetapi juga dengan statistik untuk setiap hari.  Tetapi dia belajar untuk melakukan semua ini dengan sangat cepat dan andal baru-baru ini. <br><br><img src="https://habrastorage.org/webt/ee/nn/jp/eennjposwyrztis3a7s6cbkhq2e.png" alt="gambar"><br><br>  Secara total, layanan memproses ~ 300 ribu permintaan baca dan ~ 9 ribu permintaan tulis per menit, 99% di antaranya dieksekusi hingga 5ms.  Ini, tentu saja, bukan indikator astronomi dan bukan peluncuran roket di Mars - tetapi juga bukan tugas sepele seperti penyimpanan angka yang sederhana.  Ternyata melakukan semua ini, memastikan penyimpanan data lossless dan membaca nilai-nilai yang konsisten dan relevan, memerlukan beberapa upaya, yang akan kita bahas di bawah ini. <br><a name="habracut"></a><br><h3>  Tugas dan Gambaran Umum Proyek </h3><br>  Meskipun penghitung tampilan tidak begitu penting untuk bisnis seperti, katakanlah, memproses pembayaran atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">permintaan pinjaman</a> , mereka pertama-tama penting bagi pengguna kami.  Orang-orang terpesona dengan melacak popularitas iklan mereka: beberapa bahkan memanggil dukungan ketika mereka melihat informasi tampilan yang tidak akurat (ini terjadi dengan salah satu implementasi layanan sebelumnya).  Selain itu, kami menyimpan dan menampilkan statistik terperinci di akun pribadi pengguna (misalnya, untuk menilai efektivitas penggunaan layanan berbayar).  Semua ini membuat kami berhati-hati dalam menyimpan setiap acara tontonan dan menampilkan nilai yang paling relevan. <br><br>  Secara umum, fungsi dan prinsip proyek terlihat seperti ini: <br><br><ul><li>  Halaman web atau layar aplikasi membuat permintaan di belakang penghitung tampilan iklan (permintaan biasanya tidak sinkron untuk memprioritaskan keluaran informasi dasar).  Dan jika halaman iklan itu sendiri ditampilkan, klien akan meminta Anda untuk menambah dan mengembalikan jumlah tampilan yang diperbarui. </li><li>  Dengan memproses permintaan baca, layanan mencoba untuk mendapatkan informasi dari cache Redis, dan melengkapi yang tidak diketahui dengan menyelesaikan permintaan ke MongoDB. </li><li>  Permintaan tulis dikirim ke 2 struktur di lobak: antrian pembaruan tambahan (diproses di latar belakang, secara tidak serempak) dan cache dari jumlah total tampilan. </li><li>  Proses latar belakang dalam layanan yang sama membaca elemen dari antrian, menumpuknya di buffer lokal, dan secara berkala menulisnya ke MongoDB. </li></ul><br><h3>  Rekam Tampilan Penghitung: Perangkap </h3><br>  Meskipun langkah-langkah yang dijelaskan di atas terlihat cukup sederhana, masalahnya di sini adalah organisasi interaksi antara database dan contoh layanan mikro sehingga data tidak hilang, tidak terduplikasi, dan tidak tertinggal. <br><br>  Menggunakan hanya satu repositori (misalnya, hanya MongoDB) akan menyelesaikan beberapa masalah ini.  Bahkan, layanan digunakan untuk bekerja sebelumnya, sampai kami menemukan masalah penskalaan, stabilitas dan kecepatan. <br><br>  Implementasi naif data bergerak antara penyimpanan dapat menyebabkan, misalnya, ke anomali seperti: <br><br><ul><li>  Kehilangan data saat kompetitif menulis ke cache: <br><ol><li>  Proses <b>A</b> meningkatkan jumlah tampilan dalam cache Redis, tetapi menemukan bahwa masih belum ada data untuk entitas ini (bisa berupa deklarasi baru atau yang lama yang telah diekstrusi dari cache), jadi proses tersebut pertama-tama harus mendapatkan nilai ini dari MongoDB. <br></li><li>  Proses <b>A</b> mendapatkan jumlah tampilan dari MongoDB - misalnya, angka 5;  kemudian menambahkan 1 ke dalamnya dan akan menulis ke Redis <b>6</b> . </li><li>  Proses <b>B</b> (dimulai, misalnya, oleh pengguna situs lain yang juga memasukkan iklan yang sama) secara bersamaan melakukan hal yang sama. </li><li>  Proses <b>A</b> menulis nilai <b>6</b> ke Redis. </li><li>  Proses <b>B</b> menulis nilai <b>6</b> ke Redis. </li><li>  Akibatnya, satu tampilan hilang karena balapan saat merekam data. <br>  <i>Skenarionya tidak begitu mustahil: misalnya, kami memiliki layanan berbayar yang menempatkan iklan di halaman utama situs.</i>  <i>Untuk pengumuman baru, rangkaian peristiwa semacam itu dapat menyebabkan hilangnya banyak pandangan sekaligus karena arus masuknya yang tiba-tiba.</i> </li></ol></li><li>  Contoh skenario lain adalah kehilangan data saat memindahkan tampilan dari Redis ke MongoDb: <br><br><ol><li>  Proses mengambil nilai yang tertunda dari Redis dan menyimpannya dalam memori untuk kemudian menulis ke MongoDB. </li><li>  Permintaan penulisan gagal (atau proses macet sebelum dijalankan). </li><li>  Data hilang lagi, yang akan menjadi jelas pada saat nilai yang di-cache didorong keluar dan diganti dengan nilai dari database. </li></ol><br></li></ul><br>  Kesalahan lain dapat terjadi, alasan yang juga terletak pada sifat non-atom operasi antara database, misalnya, konflik saat menghapus dan meningkatkan pandangan dari entitas yang sama. <br><br><h3>  Merekam Jumlah Tampilan: Solusi </h3><br>  Pendekatan kami untuk menyimpan dan memproses data dalam proyek ini didasarkan pada harapan bahwa pada suatu saat MongoDB mungkin gagal lebih mungkin daripada Redis.  Ini, tentu saja, bukan <i>aturan</i> mutlak - setidaknya tidak untuk setiap proyek - tetapi di lingkungan kami, kami benar-benar terbiasa mengamati batas waktu berkala untuk kueri di MongoDB yang disebabkan oleh kinerja operasi disk, yang sebelumnya merupakan salah satu alasan hilangnya beberapa peristiwa. <br><br>  Untuk menghindari banyak masalah yang disebutkan di atas, kami menggunakan antrian tugas untuk penyimpanan yang ditangguhkan dan skrip lua, yang memungkinkan untuk mengubah data secara atomis dalam beberapa struktur lobak sekaligus.  Dengan mengingat hal ini, detail untuk menyimpan tampilan adalah sebagai berikut: <br><br><ol><li>  Ketika permintaan tulis jatuh ke dalam microservice, ia menjalankan skrip lua <b>IncrementIfExists</b> untuk menambah penghitung hanya jika sudah ada dalam cache.  Script segera mengembalikan <b>-1</b> jika tidak ada data untuk entitas yang dilihat di lobak;  jika tidak, itu meningkatkan nilai tampilan dalam cache melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">HINCRBY</a> , menambahkan acara ke antrian untuk penyimpanan selanjutnya di MongoDB (disebut <i>antrian tertunda oleh</i> kami) melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">LPUSH</a> , dan mengembalikan jumlah tampilan yang diperbarui. <br></li><li>  Jika IncrementIfExists mengembalikan angka positif, nilai ini dikembalikan ke klien dan permintaan berakhir. <br><br>  Jika tidak, microservice mengambil penghitung tampilan dari MongoDb, menambahnya dengan 1 dan mengirimkannya ke lobak. <br></li><li>  Menulis ke lobak dilakukan melalui lua-script lain - <b>Upsert</b> - yang menyimpan jumlah tampilan ke cache jika masih kosong, atau meningkatkannya dengan 1 jika orang lain berhasil mengisi cache antara langkah 1 dan 3. <br></li><li>  Upsert juga menambahkan acara tampilan ke antrian yang tertunda, dan mengembalikan jumlah yang diperbarui, yang kemudian dikirim ke klien. <br></li></ol><br>  Karena fakta bahwa skrip lua <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dieksekusi secara atom</a> , kami menghindari banyak masalah potensial yang dapat disebabkan oleh penulisan yang kompetitif. <br><br>  Detail penting lainnya adalah memastikan transfer pembaruan yang aman dari antrian yang tertunda ke MongoDB.  Untuk melakukan ini, kami menggunakan templat "antrian yang dapat diandalkan" yang dijelaskan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi Redis</a> , yang secara signifikan mengurangi kemungkinan kehilangan data dengan membuat salinan elemen yang diproses dalam antrian yang terpisah, yang lain hingga akhirnya disimpan dalam penyimpanan persisten. <br><br>  Untuk lebih memahami seluruh langkah proses, kami telah menyiapkan visualisasi kecil.  Pertama, mari kita lihat skenario normal dan sukses (langkah-langkahnya diberi nomor di sudut kanan atas dan dijelaskan secara rinci di bawah): <br><br><img src="https://habrastorage.org/webt/0v/al/bq/0valbqz3kj6du62z0foxfcb6bay.gif" alt="gambar"><br><br><ol><li>  Layanan mikro menerima permintaan tulis </li><li>  Penangan permintaan meneruskannya ke skrip lua yang menulis pencarian ke cache (segera membuatnya dapat dibaca) dan ke antrian untuk diproses lebih lanjut. </li><li>  Goroutine latar belakang (secara berkala) melakukan operasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BRPopLPush</a> , yang secara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">atomis</a> memindahkan elemen dari satu antrian ke antrian (kami menyebutnya "antrian pemrosesan" - antrian dengan elemen yang sedang diproses).  Elemen yang sama kemudian disimpan dalam buffer di memori proses. <br></li><li>  Permintaan tulis lain tiba dan sedang diproses, yang membuat kami memiliki 2 elemen dalam buffer dan 2 elemen dalam antrian pemrosesan. </li><li>  Setelah beberapa waktu habis, proses latar belakang memutuskan untuk menyiram buffer di MongoDB.  Menulis beberapa nilai dari buffer dilakukan oleh satu permintaan, yang secara positif memengaruhi throughput.  Selain itu, sebelum merekam, proses mencoba menggabungkan beberapa tampilan menjadi satu, merangkum nilainya untuk iklan yang sama. <br>  <i>Pada masing-masing proyek kami, 3 instance microservice digunakan, masing-masing dengan buffernya sendiri, yang disimpan ke database setiap 2 detik.</i>  <i>Selama waktu ini, sekitar 100 elemen terakumulasi dalam satu buffer.</i> <i><br></i> </li><li>  Setelah penulisan berhasil, proses menghapus item dari antrian pemrosesan, menandakan bahwa pemrosesan telah selesai dengan sukses. <br></li></ol><br>  Ketika semua subsistem dalam urutan, beberapa langkah ini mungkin tampak berlebihan.  Dan pembaca yang penuh perhatian mungkin juga memiliki pertanyaan tentang apa yang dilakukan gopher yang tidur di sudut kiri bawah. <br>  Semuanya dijelaskan ketika mempertimbangkan skenario ketika MongoDB tidak tersedia: <br><br><img src="https://habrastorage.org/webt/hl/jd/bs/hljdbsmwzxn6i2k5xfgj2mm02em.gif" alt="Contoh layanan ketika MongoDB mogok"><br><br><ol><li>  Langkah pertama identik dengan peristiwa dari skenario sebelumnya: layanan menerima 2 permintaan untuk merekam pandangan dan memprosesnya. <br></li><li>  Proses kehilangan koneksi dengan MongoDB (proses itu sendiri, tentu saja, belum tahu tentang ini). <br>  Pawang Gorutin, seperti sebelumnya, sedang mencoba memasukkan buffer ke dalam basis data - tetapi kali ini tidak berhasil.  Dia kembali menunggu iterasi berikutnya. <br></li><li>  Goroutine latar belakang lain bangun dan memeriksa antrian pemrosesan.  Dia menemukan bahwa unsur-unsur telah ditambahkan padanya sejak lama;  menyimpulkan bahwa pemrosesan mereka gagal, ia memindahkan mereka kembali ke antrian yang tertunda. <br></li><li>  Setelah beberapa saat, koneksi dengan MongoDB dipulihkan. <br></li><li>  Goroutine latar belakang pertama lagi mencoba melakukan operasi penulisan - kali ini berhasil - dan pada akhirnya menghapus item dari antrian pemrosesan secara permanen. <br></li></ol><br>  Dalam skema ini, ada beberapa batas waktu penting dan heuristik yang diperoleh melalui pengujian dan akal sehat: misalnya, elemen dipindahkan kembali dari antrian pemrosesan ke antrian tertunda setelah 15 menit tidak aktif.  Selain itu, goroutine yang bertanggung jawab untuk tugas ini melakukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kunci</a> sebelum eksekusi sehingga beberapa instance dari layanan microser tidak mencoba untuk mengembalikan tampilan "beku" pada saat yang sama. <br><br>  Sebenarnya, bahkan langkah-langkah ini tidak memberikan jaminan yang terbukti secara teoritis (misalnya, kami mengabaikan skenario seperti proses membeku selama 15 menit) - tetapi dalam praktiknya itu bekerja dengan cukup andal. <br><br>  Juga dalam skema ini, setidaknya ada 2 kerentanan yang diketahui oleh kita yang penting untuk diperhatikan: <br><br><ul><li>  Jika microservice mogok segera setelah berhasil menyimpan ke MongoDb, tetapi sebelum membersihkan daftar antrian pemrosesan, maka data ini akan dianggap tidak disimpan - dan setelah 15 menit akan disimpan lagi. <br>  <i>Untuk mengurangi kemungkinan skenario seperti itu, kami telah melakukan upaya berulang kali untuk menghapus dari antrian pemrosesan jika terjadi kesalahan.</i>  <i>Pada kenyataannya, kami belum mengamati kasus-kasus seperti ini dalam produksi.</i> <br></li><li>  Saat me-reboot, lobak dapat kehilangan tidak hanya cache, tetapi juga beberapa tampilan yang belum disimpan dari antrian, karena dikonfigurasi untuk secara berkala menyimpan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">snapshot RDB</a> setiap beberapa menit. <br>  <i>Meskipun secara teori ini bisa menjadi masalah serius (terutama jika proyek berurusan dengan data yang sangat kritis), dalam praktiknya node sangat jarang dimulai kembali.</i>  <i>Pada saat yang sama, menurut pemantauan, elemen menghabiskan antrian kurang dari 3 detik, yaitu, jumlah kemungkinan kerugian sangat terbatas.</i> <br></li></ul><br>  Tampaknya ada lebih banyak masalah daripada yang kita inginkan.  Namun, pada kenyataannya, ternyata skenario yang kami pertahankan awalnya - kegagalan MongoDB - memang merupakan ancaman yang jauh lebih nyata, dan skema pemrosesan data baru berhasil memastikan ketersediaan layanan dan mencegah kerugian. <br><br>  Salah satu contoh nyata dari hal ini adalah ketika instance MongoDB di salah satu proyek bukan kepalang tersedia sepanjang malam.  Selama ini, jumlah penghitungan terakumulasi dan diputar dalam lobak dari satu antrian ke antrian yang lain, sampai akhirnya disimpan dalam database setelah menyelesaikan insiden;  sebagian besar pengguna bahkan tidak melihat kegagalan. <br><br><h3>  Jumlah tampilan baca </h3><br>  Permintaan baca jauh lebih sederhana daripada permintaan tulis: microservice terlebih dahulu memeriksa cache di lobak;  segala sesuatu yang tidak ditemukan dalam cache diisi dengan data dari MongoDb dan dikembalikan ke klien. <br><br>  Tidak ada penulisan end-to-end ke cache selama operasi baca untuk menghindari biaya perlindungan terhadap penulisan kompetitif.  Hitrate cache tetap bagus, karena lebih sering daripada tidak, itu akan menjadi hangat berkat permintaan tulis lainnya. <br><br>  Statistik tampilan harian dibaca langsung dari MongoDB, seperti yang diminta lebih jarang, dan caching lebih sulit.  Ini juga berarti bahwa ketika database tidak tersedia, membaca statistik berhenti berfungsi;  tetapi hanya mempengaruhi sebagian kecil pengguna. <br><br><h3>  Skema penyimpanan data MongoDB </h3><br>  Skema pengumpulan MongoDB untuk proyek didasarkan pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">rekomendasi ini dari pengembang basis data itu sendiri</a> , dan terlihat seperti ini: <br><br><ul><li>  Tampilan disimpan dalam 2 koleksi: di satu ada jumlah totalnya, di yang lain - statistik per hari. <br></li><li>  Data dalam pengumpulan statistik disusun berdasarkan <b>satu dokumen per iklan per bulan</b> .  Untuk pengumuman baru, dokumen yang diisi dengan tiga puluh satu nol untuk bulan berjalan dimasukkan ke dalam koleksi;  Menurut artikel yang disebutkan di atas, ini memungkinkan Anda untuk segera mengalokasikan cukup ruang untuk dokumen pada disk sehingga database tidak harus memindahkannya saat menambahkan data. <br>  <i>Item ini membuat proses membaca statistik sedikit canggung (permintaan harus dibuat berbulan-bulan di sisi layanan mikro), tetapi secara keseluruhan skema ini tetap cukup intuitif.</i> <br></li><li>  Operasi <b>upsert</b> digunakan untuk merekam, untuk memperbarui dan, jika perlu, membuat dokumen untuk entitas yang diinginkan dalam permintaan yang sama. <br></li></ul><br>  Kami tidak menggunakan kemampuan transaksional MongoDb untuk memperbarui beberapa koleksi secara bersamaan, yang berarti bahwa kami berisiko bahwa data dapat ditulis hanya untuk satu koleksi.  Untuk saat ini, kami cukup login dalam kasus seperti itu;  jumlahnya sedikit, dan sejauh ini tidak ada masalah signifikan yang sama dengan skenario lainnya. <br><br><h3>  Pengujian </h3><br>  Saya tidak akan percaya kata-kata saya sendiri bahwa skenario yang dijelaskan benar-benar berfungsi jika mereka tidak dicakup oleh tes. <br><br>  Karena sebagian besar kode proyek bekerja erat dengan lobak dan MongoDb, sebagian besar tes di dalamnya adalah tes integrasi.  Lingkungan pengujian didukung melalui komposisi buruh pelabuhan, yang berarti dapat digunakan dengan cepat, memberikan reproduksibilitas dengan mengatur ulang dan memulihkan keadaan pada setiap awal, dan memungkinkan untuk bereksperimen tanpa memengaruhi basis data orang lain. <br><br>  Dalam proyek ini, ada 3 bidang utama pengujian: <br><br><ol><li>  Validasi logika bisnis dalam skenario tipikal, yang disebut  jalan bahagia  Tes-tes ini menjawab pertanyaan - ketika semua subsistem dalam urutan, apakah layanan bekerja sesuai dengan persyaratan fungsional? </li><li>  Memeriksa skenario negatif di mana layanan diharapkan untuk melanjutkan pekerjaannya.  Misalnya, apakah layanan benar-benar tidak kehilangan data saat MongoDb lumpuh? <br>  Apakah kami yakin bahwa informasi tersebut tetap konsisten dengan batas waktu berkala, macet, dan operasi perekaman kompetitif? </li><li>  Memeriksa skenario negatif di mana kami tidak mengharapkan layanan untuk melanjutkan, tetapi tingkat fungsionalitas minimum masih harus disediakan.  Misalnya, tidak ada kemungkinan bahwa layanan akan terus menyimpan dan mengembalikan data ketika lobak atau mongo tidak tersedia - tetapi kami ingin memastikan bahwa dalam kasus seperti itu tidak macet, tetapi mengharapkan pemulihan sistem dan kemudian kembali bekerja. </li></ol><br>  Untuk memeriksa skenario yang gagal, kode logika bisnis layanan bekerja dengan antarmuka klien basis data, yang dalam pengujian yang diperlukan diganti dengan implementasi yang mengembalikan kesalahan dan / atau mensimulasikan penundaan jaringan.  Kami juga mensimulasikan operasi paralel beberapa instance layanan menggunakan pola " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">objek lingkungan</a> ".  Ini adalah varian dari pendekatan "inversi kontrol" yang terkenal, di mana fungsi tidak mengakses dependensi itu sendiri, tetapi menerimanya melalui objek lingkungan yang diteruskan dalam argumen.  Di antara kelebihan lainnya, pendekatan ini memungkinkan Anda untuk mensimulasikan beberapa salinan independen dari layanan dalam satu tes, yang masing-masing memiliki kumpulan koneksi ke database dan kurang lebih secara efisien mereproduksi lingkungan produksi.  Beberapa tes menjalankan setiap instance secara paralel dan memastikan bahwa mereka semua melihat data yang sama, dan tidak ada kondisi balapan. <br><br>  Kami juga melakukan tes stres yang belum sempurna, tetapi masih cukup berguna berdasarkan <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pengepungan</a> , yang secara kasar membantu memperkirakan beban yang diizinkan dan kecepatan respons dari layanan. <br><br><h3>  Tentang kinerja </h3><br>  Untuk 90% permintaan, waktu pemrosesan sangat kecil, dan yang paling penting - stabil;  Berikut adalah contoh pengukuran pada salah satu proyek selama beberapa hari: <br><br><img src="https://habrastorage.org/webt/ln/bk/zy/lnbkzy7-wnykbaelv4vzsd8b53q.png" alt="gambar"><br><br>  Menariknya, catatan (yang sebenarnya merupakan operasi tulis + baca, karena mengembalikan nilai yang diperbarui) sedikit lebih cepat daripada membaca (tetapi hanya dari sudut pandang klien yang tidak mengamati penulisan yang tertunda sebenarnya). <br>  Peningkatan penundaan reguler di pagi hari adalah efek samping dari pekerjaan tim analitik kami, yang mengumpulkan statistiknya sendiri setiap hari berdasarkan data layanan, menciptakan "muatan berlebihan artifisial" bagi kami. <br><br>      :           (          â€”          MongoDB),       (      ),     : <br><br><img src="https://habrastorage.org/webt/6f/pz/v9/6fpzv9b8lmswdsjhmrn4gk_qugw.png" alt="gambar"><br><br><h3>  Kesimpulan </h3><br> ,  -  , ,   Redis                . <br><br>       , 95%    ,     .      ,                .           5. <br><br>        Go, Redis  MongoDB             .                 ,       .         ,      â€”      . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id431902/">https://habr.com/ru/post/id431902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id431888/index.html">â€œSaya pikir ide tim adalah yang paling penting ketika mengembangkan suatu produk.â€</a></li>
<li><a href="../id431890/index.html">Cara melakukan pemesanan di bursa lepas</a></li>
<li><a href="../id431892/index.html">Kami menggunakan Veeam Backup & Replication untuk menguji sistem dan aplikasi baru sebelum memutakhirkan</a></li>
<li><a href="../id431894/index.html">Pada bulan Desember, mereka akan memutuskan pendaftaran wajib stasiun pangkalan LPWAN</a></li>
<li><a href="../id431898/index.html">Ini Semua Tentang Agile - 2: Fitur Implementasi Agile</a></li>
<li><a href="../id431904/index.html">Bagaimana kami menurunkan spesialis HR: info untuk menerbitkan lembar pembayaran</a></li>
<li><a href="../id431906/index.html">PIFR - metode menghasilkan topeng 3D, terlepas dari sudut rotasi wajah</a></li>
<li><a href="../id431908/index.html">Menyiapkan API Tinkoff Bank. Bagaimana intuisi Anda ....? Atau lagu tentang Oauth 2.0</a></li>
<li><a href="../id431910/index.html">PSEFABRIC - pendekatan baru untuk manajemen jaringan dan otomatisasi. Langkah menuju yang ideal</a></li>
<li><a href="../id431912/index.html">Bot-no terbesar telah ditangkap di AS: apa artinya ini bagi komunitas digital?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>