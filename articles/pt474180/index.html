<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê£ ü¶Ö üë®üèæ‚Äç‚úàÔ∏è Como a arquitetura da Web tolerante a falhas √© implementada na plataforma Mail.ru Cloud Solutions üó°Ô∏è üë©‚Äçüé§ ‚¨ÖÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Sou Artyom Karamyshev, chefe da equipe de administra√ß√£o de sistemas da Mail.Ru Cloud Solutions (MCS) . No ano passado, tivemos muitos lan√ßam...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como a arquitetura da Web tolerante a falhas √© implementada na plataforma Mail.ru Cloud Solutions</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/474180/"><img src="https://habrastorage.org/webt/gd/wp/de/gdwpdevye3ploqkbmd4rwjqkvva.jpeg"><br><br>  Ol√° Habr!  Sou Artyom Karamyshev, chefe da equipe de administra√ß√£o de sistemas da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Mail.Ru Cloud Solutions (MCS)</a> .  No ano passado, tivemos muitos lan√ßamentos de novos produtos.  Quer√≠amos que os servi√ßos da API fossem escalonados com facilidade, tolerantes a falhas e prontos para um r√°pido aumento na carga do usu√°rio.  Nossa plataforma √© implementada no OpenStack, e quero dizer quais problemas de toler√¢ncia a falhas de componentes tivemos que fechar para obter um sistema tolerante a falhas.  Eu acho que isso ser√° interessante para quem tamb√©m desenvolve produtos no OpenStack. <br><br>  A toler√¢ncia geral a falhas da plataforma consiste na estabilidade de seus componentes.  Ent√£o, passaremos gradualmente a todos os n√≠veis em que descobrimos os riscos e os fechamos. <br><br>  Uma vers√£o em v√≠deo dessa hist√≥ria, cuja fonte foi um relat√≥rio na confer√™ncia Uptime day 4, organizada pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ITSumma</a> , pode ser visualizada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no canal do YouTube da Comunidade Uptime</a> . <br><a name="habracut"></a><br>
<h2>  Toler√¢ncia a falhas da arquitetura f√≠sica </h2><br>  A parte p√∫blica da nuvem MCS agora est√° baseada em dois data centers de n√≠vel III, entre eles existe uma fibra escura pr√≥pria, reservada na camada f√≠sica por diferentes rotas, com uma taxa de transfer√™ncia de 200 Gb / s.  O n√≠vel de camada III fornece o n√≠vel necess√°rio de resili√™ncia da infraestrutura f√≠sica. <br><br>  A fibra escura √© reservada nos n√≠veis f√≠sico e l√≥gico.  O processo de reserva de canais foi iterativo, surgiram problemas e estamos constantemente aprimorando a comunica√ß√£o entre os data centers. <br><br><blockquote>  Por exemplo, n√£o muito tempo atr√°s, ao trabalhar em um po√ßo pr√≥ximo a um dos data centers, uma escavadeira perfurou um cano; dentro desse cano havia um cabo √≥ptico principal e um de backup.  Nosso canal de comunica√ß√£o tolerante a falhas com o data center ficou vulner√°vel em um ponto do po√ßo.  Por conseguinte, perdemos parte da infraestrutura.  Tiramos conclus√µes, realizamos v√°rias a√ß√µes, incluindo a coloca√ß√£o de √≥pticas adicionais ao longo de um po√ßo vizinho. </blockquote><br>  Nos datacenters, existem pontos de presen√ßa de provedores de comunica√ß√£o para os quais transmitimos nossos prefixos via BGP.  Para cada dire√ß√£o da rede, a melhor m√©trica √© selecionada, o que permite que diferentes clientes forne√ßam a melhor qualidade de conex√£o.  Se a comunica√ß√£o atrav√©s de um provedor for desconectada, reconstruiremos nosso roteamento atrav√©s dos provedores dispon√≠veis. <br><br>  No caso de uma falha do provedor, passamos automaticamente para o pr√≥ximo.  Em caso de falha de um dos datacenters, temos uma c√≥pia espelhada de nossos servi√ßos no segundo datacenter, que assumem todo o √¥nus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d0/m8/ly/d0m8lykvifmc-gum-h9mbppyn3g.jpeg"></div><br>  <i>Resili√™ncia da infraestrutura f√≠sica</i> <br><br><h2>  O que usamos para toler√¢ncia a falhas no n√≠vel do aplicativo </h2><br>  Nosso servi√ßo √© constru√≠do em v√°rios componentes de c√≥digo aberto. <br><br>  <b>O ExaBGP</b> √© um servi√ßo que implementa v√°rias fun√ß√µes usando o protocolo de roteamento din√¢mico baseado no BGP.  N√≥s o usamos ativamente para anunciar nossos endere√ßos IP brancos por meio dos quais os usu√°rios obt√™m acesso √† API. <br><br>  <b>O HAProxy</b> √© um balanceador altamente carregado que permite configurar regras muito flex√≠veis para equilibrar o tr√°fego em diferentes n√≠veis do modelo OSI.  Usamos para equilibrar todos os servi√ßos: bancos de dados, intermedi√°rios de mensagens, servi√ßos de API, servi√ßos da Web, nossos projetos internos - tudo est√° por tr√°s do HAProxy. <br><br>  <b>Aplicativo de API</b> - um <b>aplicativo da</b> Web escrito em python, com o qual o usu√°rio controla sua infraestrutura, seu servi√ßo. <br><br>  <b>Aplicativo Worker</b> (a seguir denominado simplesmente worker) - nos servi√ßos OpenStack, √© um daemon de infraestrutura que permite converter comandos API para a infraestrutura.  Por exemplo, um disco √© criado no trabalho e uma solicita√ß√£o de cria√ß√£o est√° na API do aplicativo. <br><br><h2>  Arquitetura padr√£o de aplicativos OpenStack </h2><br>  A maioria dos servi√ßos desenvolvidos para o OpenStack tenta seguir um √∫nico paradigma.  Um servi√ßo geralmente consiste em 2 partes: API e trabalhadores (executores de back-end).  Normalmente, uma API √© um aplicativo WSGI python que √© executado como um processo independente (daemon) ou usando um servidor Web Nginx pronto, Apache.  A API processa a solicita√ß√£o do usu√°rio e passa mais instru√ß√µes ao aplicativo de trabalho.  A transmiss√£o ocorre usando um intermedi√°rio de mensagens, geralmente o RabbitMQ, o restante √© pouco suportado.  Quando as mensagens chegam ao intermedi√°rio, elas s√£o processadas pelos trabalhadores e, se necess√°rio, retornam uma resposta. <br><br>  Este paradigma implica pontos comuns isolados de falha: RabbitMQ e o banco de dados.  Mas o RabbitMQ √© isolado em um servi√ßo e, em teoria, pode ser individual para cada servi√ßo.  Ent√£o, no MCS, compartilhamos esses servi√ßos o m√°ximo poss√≠vel, para cada projeto individual criamos um banco de dados separado, um RabbitMQ separado.  Essa abordagem √© boa porque, no caso de um acidente em alguns pontos vulner√°veis, nem todos os servi√ßos s√£o interrompidos, mas apenas parte dele. <br><br>  O n√∫mero de aplicativos de trabalho √© ilimitado, portanto a API pode ser dimensionada facilmente horizontalmente atr√°s dos balanceadores, a fim de aumentar a produtividade e a toler√¢ncia a falhas. <br><br><blockquote>  Alguns servi√ßos exigem coordena√ß√£o dentro do servi√ßo - quando opera√ß√µes seq√ºenciais complexas ocorrem entre APIs e trabalhadores.  Nesse caso, um √∫nico centro de coordena√ß√£o √© usado, um sistema de cluster como Redis, Memcache, etcd, que permite que um trabalhador diga ao outro que esta tarefa est√° atribu√≠da a ele ("por favor, n√£o a execute").  N√≥s usamos o etcd.  Como regra, os trabalhadores se comunicam ativamente com o banco de dados, escrevem e leem informa√ß√µes a partir da√≠.  Como banco de dados, usamos o mariadb, que temos no cluster multimaster. <br></blockquote><br>  Esse servi√ßo cl√°ssico de usu√°rio √∫nico √© organizado de uma maneira geralmente aceita pelo OpenStack.  Pode ser considerado como um sistema fechado, para o qual os m√©todos de escala e toler√¢ncia a falhas s√£o bastante √≥bvios.  Por exemplo, para toler√¢ncia a falhas da API, basta colocar um balanceador na frente deles.  A escala de trabalhadores √© alcan√ßada aumentando seu n√∫mero. <br><br>  Os pontos fracos em todo o esquema s√£o RabbitMQ e MariaDB.  Sua arquitetura merece um artigo separado.Neste artigo, quero focar na toler√¢ncia a falhas da API. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1a/ab/i1/1aabi1ew0ctxnlrcm2j78nbhefk.jpeg"></div><br>  <i>Arquitetura de aplicativo Openstack</i>  <i>Balanceamento e resili√™ncia da plataforma em nuvem</i> <br><br><h2>  Tornando o HAProxy Balancer resiliente com o ExaBGP </h2><br>  Para tornar nossas APIs escal√°veis, r√°pidas e tolerantes a falhas, definimos um balanceador √† sua frente.  Escolhemos o HAProxy.  Na minha opini√£o, possui todas as caracter√≠sticas necess√°rias para nossa tarefa: balanceamento em v√°rios n√≠veis OSI, interface de gerenciamento, flexibilidade e escalabilidade, um grande n√∫mero de m√©todos de balanceamento, suporte para tabelas de sess√µes. <br><br>  O primeiro problema que precisou ser resolvido foi a toler√¢ncia a falhas do pr√≥prio balanceador.  A instala√ß√£o do balanceador tamb√©m cria um ponto de falha: o balanceador √© interrompido - o servi√ßo cai.  Para evitar isso, usamos o HAProxy junto com o ExaBGP. <br><br>  O ExaBGP permite implementar um mecanismo para verificar o status de um servi√ßo.  Usamos esse mecanismo para verificar a funcionalidade do HAProxy e, em caso de problemas, desativar o servi√ßo HAProxy do BGP. <br><br>  <b>Esquema ExaBGP + HAProxy</b> <br><br><ol><li>  Instalamos o software necess√°rio em tr√™s servidores, ExaBGP e HAProxy. </li><li>  Em cada um dos servidores, criamos uma interface de loopback. </li><li>  Nos tr√™s servidores, atribu√≠mos o mesmo endere√ßo IP branco a essa interface. </li><li>  Um endere√ßo IP branco √© anunciado na Internet atrav√©s do ExaBGP. </li></ol><br>  A toler√¢ncia a falhas √© alcan√ßada anunciando o mesmo endere√ßo IP dos tr√™s servidores.  Do ponto de vista da rede, o mesmo endere√ßo √© acess√≠vel a partir de tr√™s pr√≥ximas esperan√ßas diferentes.  O roteador v√™ tr√™s rotas id√™nticas, seleciona a maior prioridade delas de acordo com sua pr√≥pria m√©trica (geralmente √© a mesma op√ß√£o) e o tr√°fego √© direcionado apenas a um dos servidores. <br><br>  Em caso de problemas com a opera√ß√£o do HAProxy ou falha do servidor, o ExaBGP para de anunciar a rota e o tr√°fego muda suavemente para outro servidor. <br><br>  Assim, alcan√ßamos a toler√¢ncia a falhas do balanceador. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ij/c_/cz/ijc_cz1jvhlug0axiwiiqjqpcww.jpeg"></div><br>  <i>Toler√¢ncia a falhas de balanceadores de HAProxy</i> <br><br>  O esquema acabou sendo imperfeito: aprendemos a reservar o HAProxy, mas n√£o aprendemos a distribuir a carga dentro dos servi√ßos.  Portanto, expandimos um pouco esse esquema: passamos ao equil√≠brio entre v√°rios endere√ßos IP brancos. <br><br><h2>  Balanceamento baseado em DNS Plus BGP </h2><br>  A quest√£o do balanceamento de carga antes do nosso HAProxy permaneceu sem solu√ß√£o.  No entanto, isso pode ser resolvido de maneira bastante simples, como fizemos em casa. <br><br>  Para equilibrar os tr√™s servidores, voc√™ precisar√° de tr√™s endere√ßos IP brancos e um bom e velho DNS.  Cada um desses endere√ßos √© definido na interface de loopback de cada HAProxy e √© anunciado na Internet. <br><br>  O OpenStack usa um cat√°logo de servi√ßos para gerenciar recursos, que define a API do terminal de um servi√ßo.  Neste diret√≥rio, prescrevemos um nome de dom√≠nio - public.infra.mail.ru, que resolve atrav√©s do DNS com tr√™s endere√ßos IP diferentes.  Como resultado, obtemos o balanceamento de carga entre os tr√™s endere√ßos por meio do DNS. <br><br>  Por√©m, como ao anunciar endere√ßos IP brancos, n√£o controlamos as prioridades de sele√ß√£o do servidor, at√© agora isso n√£o est√° equilibrado.  Como regra, apenas um servidor ser√° selecionado pela preced√™ncia do endere√ßo IP e os outros dois ficar√£o ociosos, pois nenhuma m√©trica √© especificada no BGP. <br><br>  Come√ßamos a fornecer rotas atrav√©s do ExaBGP com diferentes m√©tricas.  Cada balanceador anuncia todos os tr√™s endere√ßos IP brancos, mas um deles, o principal desse balanceador, √© anunciado com uma m√©trica m√≠nima.  Portanto, enquanto os tr√™s balanceadores est√£o em opera√ß√£o, as chamadas para o primeiro endere√ßo IP caem no primeiro, as chamadas para o segundo para o segundo, para o terceiro e para o terceiro. <br><br>  O que acontece quando um dos balanceadores cai?  Em caso de falha de qualquer balanceador por sua base, o endere√ßo ainda √© anunciado pelos outros dois, o tr√°fego entre eles √© redistribu√≠do.  Assim, damos ao usu√°rio atrav√©s do DNS v√°rios endere√ßos IP de uma s√≥ vez.  Ao balancear o DNS e diferentes m√©tricas, obtemos uma distribui√ß√£o de carga uniforme nos tr√™s balanceadores.  E, ao mesmo tempo, n√£o perdemos a toler√¢ncia a falhas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ek/xc/3z/ekxc3zsz5oazwdiqziwp_idfk7a.jpeg"></div><br>  <i>Balanceamento HAProxy Baseado em DNS + BGP</i> <br><br><h2>  Intera√ß√£o entre ExaBGP e HAProxy </h2><br>  Portanto, implementamos a toler√¢ncia a falhas caso o servidor saia, com base no t√©rmino do an√∫ncio de rotas.  Mas o HAProxy tamb√©m pode ser desconectado por outros motivos que n√£o a falha do servidor: erros de administra√ß√£o, falhas de servi√ßo.  Queremos remover o balanceador quebrado de baixo da carga e, nesses casos, e precisamos de outro mecanismo. <br><br>  Portanto, expandindo o esquema anterior, implementamos uma pulsa√ß√£o entre ExaBGP e HAProxy.  Esta √© uma implementa√ß√£o de software da intera√ß√£o entre o ExaBGP e o HAProxy, quando o ExaBGP usa scripts personalizados para verificar o status dos aplicativos. <br><br>  Para fazer isso, na configura√ß√£o do ExaBGP, voc√™ deve configurar um verificador de integridade que pode verificar o status do HAProxy.  No nosso caso, configuramos o back-end de integridade no HAProxy e, no lado do ExaBGP, verificamos com uma simples solicita√ß√£o GET.  Se o an√∫ncio deixar de ocorrer, √© prov√°vel que o HAProxy n√£o funcione e n√£o √© necess√°rio anunci√°-lo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w0/ac/5c/w0ac5cvqsvjtki2cqzcgtgyk4x4.jpeg"></div><br>  <i>Verifica√ß√£o de integridade HAProxy</i> <br><br><h2>  HAProxy Peers: sincroniza√ß√£o de sess√£o </h2><br>  A pr√≥xima coisa a fazer era sincronizar as sess√µes.  Ao trabalhar com balanceadores distribu√≠dos, √© dif√≠cil organizar o armazenamento de informa√ß√µes sobre as sess√µes do cliente.  Mas o HAProxy √© um dos poucos balanceadores que podem fazer isso devido √† funcionalidade Peers - a capacidade de transferir tabelas de sess√µes entre diferentes processos HAProxy. <br><br>  Existem diferentes m√©todos de balanceamento: simples, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">round-robin</a> e avan√ßados, quando uma sess√£o do cliente √© lembrada e cada vez que ela chega ao mesmo servidor como antes.  Quer√≠amos implementar a segunda op√ß√£o. <br><br>  O HAProxy usa tabelas de stick para salvar sess√µes do cliente para esse mecanismo.  Eles salvam o endere√ßo IP de origem do cliente, o endere√ßo de destino selecionado (back-end) e algumas informa√ß√µes de servi√ßo.  Normalmente, as tabelas stick s√£o usadas para salvar o par IP de origem + IP de destino, o que √© especialmente √∫til para aplicativos que n√£o podem transmitir o contexto de sess√£o do usu√°rio ao alternar para outro balanceador, por exemplo, no modo de balanceamento RoundRobin. <br><br>  Se a tabela de palitos for ensinada a se mover entre diferentes processos HAProxy (entre os quais ocorre o balanceamento), nossos balanceadores poder√£o trabalhar com um conjunto de tabelas de palitos.  Isso tornar√° poss√≠vel alternar perfeitamente a rede do cliente quando um dos balanceadores cair; o trabalho com sess√µes do cliente continuar√° nos mesmos back-end selecionados anteriormente. <br><br>  Para uma opera√ß√£o adequada, o endere√ßo IP de origem do balanceador a partir do qual a sess√£o √© estabelecida deve ser resolvido.  No nosso caso, este √© um endere√ßo din√¢mico na interface de loopback. <br><br>  A opera√ß√£o correta dos pares √© alcan√ßada apenas em determinadas condi√ß√µes.  Ou seja, os tempos limite do TCP devem ser grandes o suficiente ou o comutador deve ser r√°pido o suficiente para que a sess√£o TCP n√£o tenha tempo para interromper.  No entanto, isso permite alternar sem interrup√ß√µes. <br><br>  Na IaaS, temos um servi√ßo desenvolvido com a mesma tecnologia.  Este √© um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Balanceador de Carga como um servi√ßo para o OpenStack</a> chamado Octavia.  Baseia-se em dois processos HAProxy, originalmente incluindo suporte de pares.  Eles se provaram neste servi√ßo. <br><br>  A imagem mostra esquematicamente o movimento de tabelas de pares entre tr√™s inst√¢ncias HAProxy, √© sugerida uma configura√ß√£o, como isso pode ser configurado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ov/ol/wr/ovolwrmp-gzrvyybotjjagtb-re.jpeg"></div><br>  <i>HAProxy Peers (sincroniza√ß√£o de sess√£o)</i> <br><br>  Se voc√™ implementar o mesmo esquema, seu trabalho dever√° ser cuidadosamente testado.  N√£o √© o fato de que isso funcionar√° da mesma maneira em 100% dos casos.  Mas, pelo menos, voc√™ n√£o perder√° tabelas de stick quando precisar lembrar o IP de origem do cliente. <br><br><h2>  Limitando o n√∫mero de solicita√ß√µes simult√¢neas do mesmo cliente </h2><br>  Quaisquer servi√ßos que sejam de dom√≠nio p√∫blico, incluindo nossas APIs, podem estar sujeitos a uma avalanche de solicita√ß√µes.  Os motivos para eles podem ser completamente diferentes, desde erros do usu√°rio, at√© ataques direcionados.  Periodicamente, somos DDoS em endere√ßos IP.  Os clientes geralmente cometem erros em seus scripts; eles nos tornam mini-DDoSs. <br><br>  De uma forma ou de outra, prote√ß√£o adicional deve ser fornecida.  A solu√ß√£o √≥bvia √© limitar o n√∫mero de solicita√ß√µes de API e n√£o perder tempo na CPU processando solicita√ß√µes maliciosas. <br><br>  Para implementar essas restri√ß√µes, usamos limites de taxa, organizados com base no HAProxy, usando as mesmas tabelas de stick.  Os limites s√£o configurados de maneira bastante simples e permitem limitar o usu√°rio pelo n√∫mero de solicita√ß√µes √† API.  O algoritmo lembra o IP de origem do qual as solicita√ß√µes s√£o feitas e limita o n√∫mero de solicita√ß√µes simult√¢neas de um usu√°rio.  Obviamente, calculamos o perfil m√©dio de carregamento da API para cada servi√ßo e definimos o limite ‚âà 10 vezes esse valor.  At√© agora, continuamos a acompanhar de perto a situa√ß√£o, mantemos o dedo no pulso. <br><br>  Como √© na pr√°tica?  Temos clientes que usam constantemente nossas APIs de dimensionamento autom√°tico.  Eles criam aproximadamente duzentas ou trezentas m√°quinas virtuais mais perto da manh√£ e as excluem mais perto da noite.  Para o OpenStack, crie uma m√°quina virtual, tamb√©m com servi√ßos PaaS, pelo menos 1000 solicita√ß√µes de API, pois a intera√ß√£o entre os servi√ßos tamb√©m ocorre por meio da API. <br><br>  Esse lan√ßamento de tarefa causa uma carga bastante grande.  N√≥s estimamos essa carga, coletamos picos di√°rios, aumentamos dez vezes e isso se tornou nosso limite de taxa.  Mantemos o dedo no pulso.  Muitas vezes vemos bots, scanners, que est√£o tentando nos olhar, temos algum script CGA que possa ser executado, n√≥s os cortamos ativamente. <br><br><h2>  Como atualizar a base de c√≥digo discretamente para os usu√°rios </h2><br>  Tamb√©m implementamos toler√¢ncia a falhas no n√≠vel dos processos de implanta√ß√£o de c√≥digo.  H√° falhas durante as implementa√ß√µes, mas seu impacto na disponibilidade do servi√ßo pode ser minimizado. <br><br>  Estamos constantemente atualizando nossos servi√ßos e devemos garantir o processo de atualiza√ß√£o da base de c√≥digo sem afetar os usu√°rios.  Conseguimos resolver esse problema usando os recursos de gerenciamento HAProxy e a implementa√ß√£o do Graceful Shutdown em nossos servi√ßos. <br><br>  Para resolver esse problema, era necess√°rio fornecer controle do balanceador e o desligamento "correto" dos servi√ßos: <br><br><ul><li>  No caso do HAProxy, o controle √© feito atrav√©s do arquivo de estat√≠sticas, que √© essencialmente um soquete e √© definido na configura√ß√£o do HAProxy.  Voc√™ pode enviar comandos para ele atrav√©s do stdio.  Mas nossa principal ferramenta de controle de configura√ß√£o √© ansible, por isso possui um m√≥dulo interno para gerenciar o HAProxy.  Que estamos usando ativamente. </li><li>  A maioria dos nossos servi√ßos de API e mecanismo oferece suporte a tecnologias de desligamento simples: ap√≥s o desligamento, eles aguardam a conclus√£o da tarefa atual, seja uma solicita√ß√£o http ou algum tipo de tarefa de utilit√°rio.  O mesmo acontece com o trabalhador.  Ele conhece todas as tarefas que realiza e termina quando conclui tudo com √™xito. </li></ul><br>  Gra√ßas a esses dois pontos, o algoritmo seguro de nossa implanta√ß√£o √© o seguinte. <br><br><ol><li>  O desenvolvedor cria um novo pacote de c√≥digo (temos RPM), testa no ambiente de desenvolvimento, testa no est√°gio e o deixa no reposit√≥rio do est√°gio. </li><li>  O desenvolvedor coloca a tarefa na implanta√ß√£o com a descri√ß√£o mais detalhada dos "artefatos": a vers√£o do novo pacote, uma descri√ß√£o da nova funcionalidade e outros detalhes sobre a implanta√ß√£o, se necess√°rio. </li><li>  O administrador do sistema inicia a atualiza√ß√£o.  Lan√ßa o manual do Ansible, que, por sua vez, faz o seguinte: <br><ul><li>  Ele pega um pacote do reposit√≥rio do est√°gio, atualiza a vers√£o do pacote no reposit√≥rio do produto. </li><li>  Faz uma lista de back-end do servi√ßo atualizado. </li><li>  Desativa o primeiro servi√ßo atualizado no HAProxy e aguarda o final de seus processos.  Gra√ßas ao desligamento normal, estamos confiantes de que todas as solicita√ß√µes atuais do cliente ser√£o conclu√≠das com √™xito. </li><li>  Depois que a API, os trabalhadores e o HAProxy s√£o completamente parados, o c√≥digo √© atualizado. </li><li>  Ansible lan√ßa servi√ßos. </li><li>  Para cada servi√ßo, ele puxa certas "canetas" que fazem testes de unidade para v√°rios testes de teclas predefinidos.  Uma verifica√ß√£o b√°sica do novo c√≥digo ocorre. </li><li>  Se nenhum erro foi encontrado na etapa anterior, o back-end √© ativado. </li><li>  V√° para o pr√≥ximo back-end. </li></ul></li><li>  Ap√≥s a atualiza√ß√£o de todos os back-ends, os testes funcionais s√£o iniciados.  Se n√£o forem suficientes, o desenvolvedor analisar√° qualquer nova funcionalidade que ele fez. </li></ol><br>  Nesta implanta√ß√£o est√° conclu√≠da. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-b/km/dt/-bkmdt98ituj53jetxiaay4uf4c.jpeg"></div><br>  <i>Ciclo de atualiza√ß√£o de servi√ßo</i> <br><br>  Esse esquema n√£o funcionaria se n√£o tiv√©ssemos uma regra.  Apoiamos as vers√µes antiga e nova na batalha.  De antem√£o, no est√°gio de desenvolvimento do software, √© estabelecido que, mesmo que haja altera√ß√µes no banco de dados do servi√ßo, eles n√£o quebrar√£o o c√≥digo anterior.  Como resultado, a base de c√≥digo √© gradualmente atualizada. <br><br><h2>  Conclus√£o </h2><br>  Compartilhando meus pr√≥prios pensamentos sobre a arquitetura WEB tolerante a falhas, desejo mais uma vez observar seus pontos principais: <br><br><ul><li>  toler√¢ncia a falhas f√≠sicas; </li><li>  toler√¢ncia a falhas de rede (balanceadores, BGP); </li><li>     . </li></ul><br>   uptime! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt474180/">https://habr.com/ru/post/pt474180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt474166/index.html">√çndices de capas para GiST</a></li>
<li><a href="../pt474170/index.html">Confiss√£o de Design - 15 de novembro, Moscou, DI Telegraph</a></li>
<li><a href="../pt474172/index.html">Multa de 30 mil euros pelo uso ilegal de cookies</a></li>
<li><a href="../pt474176/index.html">11 v√≠deos do primeiro dia do DevFest 2019 em Kaliningrado</a></li>
<li><a href="../pt474178/index.html">URA no Webhook</a></li>
<li><a href="../pt474184/index.html">Passamos o desafio de Callum Macrae 100%</a></li>
<li><a href="../pt474186/index.html">Refutando mitos: pr√°ticas reais de TI na Arm√™nia</a></li>
<li><a href="../pt474192/index.html">Por que mudei do UX para o PM e depois para o Lead PM e o que mudou?</a></li>
<li><a href="../pt474194/index.html">Equipe b√∫ssola</a></li>
<li><a href="../pt474196/index.html">Os 10 marcos mais importantes no desenvolvimento da IA ‚Äã‚Äãhoje</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>