<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🦇 🤺 📘 Wie man einen Computer irreführt: die heimtückische Wissenschaft, künstliche Intelligenz zu täuschen 🐣 🥢 🤷🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zu Beginn des 20. Jahrhunderts gab der deutsche Pferdetrainer und Mathematiker Wilhelm von Austin der Welt bekannt, dass er einem Pferd das Zählen bei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man einen Computer irreführt: die heimtückische Wissenschaft, künstliche Intelligenz zu täuschen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/405773/"><img src="https://habrastorage.org/web/c38/08f/4a7/c3808f4a70d1426ca51c7b7f9abb7627.jpg"><br><br>  Zu Beginn des 20. Jahrhunderts gab der deutsche Pferdetrainer und Mathematiker Wilhelm von Austin der Welt bekannt, dass er einem Pferd das Zählen beigebracht habe.  Von Austin reiste jahrelang durch Deutschland, um dieses Phänomen zu demonstrieren.  Er bat sein Pferd mit dem Spitznamen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Clever Hans</a> (Rasse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Orlov Traber</a> ), die Ergebnisse einfacher Gleichungen zu berechnen.  Antwortete Hans und stampfte mit dem Huf.  Zwei plus zwei?  Vier Treffer. <br><br>  Aber Wissenschaftler glaubten nicht, dass Hans so schlau war, wie von Austin behauptete.  Der Psychologe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Karl Stumpf</a> führte eine gründliche Untersuchung durch, die als "Hans-Komitee" bezeichnet wurde.  Er entdeckte, dass Smart Hans keine Gleichungen löst, sondern auf visuelle Signale reagiert.  Hans tippte auf seinen Huf, bis er die richtige Antwort bekam, woraufhin sein Trainer und eine begeisterte Menge in Schreie ausbrachen.  Und dann hörte er einfach auf.  Als er diese Reaktionen nicht sah, klopfte er weiter. <br><a name="habracut"></a><br>  Die Informatik kann viel von Hans lernen.  Das beschleunigte Entwicklungstempo in diesem Bereich lässt darauf schließen, dass der größte Teil der von uns erstellten KI ausreichend trainiert wurde, um die richtigen Antworten zu liefern, die Informationen jedoch nicht wirklich versteht.  Und es ist leicht zu täuschen. <br><br>  Algorithmen für maschinelles Lernen wurden schnell zu allsehenden Hirten der menschlichen Herde.  Die Software verbindet uns mit dem Internet, überwacht Spam und schädliche Inhalte in unseren E-Mails und wird bald unsere Autos fahren.  Ihre Täuschung verschiebt das tektonische Fundament des Internets und bedroht unsere Sicherheit in der Zukunft. <br><br>  Kleine Forschungsgruppen - von der Pennsylvania State University, von Google, vom US-Militär - entwickeln Pläne zum Schutz vor möglichen Angriffen auf KI.  In der Studie vorgebrachte Theorien besagen, dass ein Angreifer ändern kann, was ein Robomobil „sieht“.  Oder aktivieren Sie die Spracherkennung auf dem Telefon und zwingen Sie es, eine schädliche Website mit Sounds zu betreten, die nur für eine Person Lärm sind.  Oder lassen Sie den Virus durch die Netzwerk-Firewall lecken. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/0e0/75b/0eb/0e075b0eb595f3aaeafaa8233e4df1b2.jpg" alt="Bild"><br>  <i>Links ist das Bild des Gebäudes, rechts das modifizierte Bild, das das tiefe neuronale Netzwerk auf Strauße bezieht.</i>  <i>In der Mitte werden alle Änderungen angezeigt, die auf das Primärbild angewendet wurden.</i> <br><br>  Anstatt die Kontrolle über ein Robomobil zu übernehmen, zeigt ihm diese Methode so etwas wie eine Halluzination - ein Bild, das es eigentlich nicht gibt. <br><br>  Solche Angriffe verwenden Bilder mit einem Trick [gegnerische Beispiele - es gibt keinen etablierten russischen Begriff, wörtlich stellt sich heraus, dass es sich um „Beispiele mit Kontrast“ oder „rivalisierende Beispiele“ handelt - ca.  transl.]: Bilder, Töne, Text, der für Menschen normal aussieht, aber von einer völlig anderen Maschine wahrgenommen wird.  Die kleinen Änderungen, die von den Angreifern vorgenommen werden, können dazu führen, dass das tiefe neuronale Netzwerk die falschen Schlussfolgerungen darüber zieht, was es zeigt. <br><br>  "Jedes System, das maschinelles Lernen verwendet, um sicherheitskritische Entscheidungen zu treffen, ist potenziell anfällig für diese Art von Angriff", sagte Alex Kanchelyan, ein Forscher an der Universität von Berkeley, der Angriffe auf maschinelles Lernen mit gefälschten Bildern untersucht. <br><br>  Wenn die Forscher diese Nuancen in den frühen Stadien der KI-Entwicklung kennen, können sie verstehen, wie diese Mängel behoben werden können.  Einige haben dies bereits aufgegriffen und sagen, dass ihre Algorithmen dadurch immer effizienter geworden sind. <br><br>  Der größte Teil der KI-Forschung basiert auf tiefen neuronalen Netzen, die wiederum auf einem breiteren Feld des maschinellen Lernens basieren.  MoD-Technologien verwenden Differential- und Integralrechnung und Statistiken, um Software zu erstellen, die von den meisten von uns verwendet wird, z. B. Spam-Filter in der E-Mail oder im Internet.  In den letzten 20 Jahren haben Forscher begonnen, diese Techniken auf eine neue Idee anzuwenden, neuronale Netze - Softwarestrukturen, die die Gehirnfunktion nachahmen.  Die Idee ist, die Berechnungen über Tausende kleiner Gleichungen („Neuronen“), die Daten empfangen, dezentralisieren, verarbeiten und weiterleiten, auf die nächste Schicht von Tausenden kleiner Gleichungen zu dezentralisieren. <br><br>  Diese KI-Algorithmen werden auf die gleiche Weise trainiert wie im Fall von MO, das wiederum den Lernprozess einer Person kopiert.  Ihnen werden Beispiele für verschiedene Dinge und die zugehörigen Tags gezeigt.  Zeigen Sie dem Computer (oder Kind) das Bild einer Katze, sagen Sie, dass die Katze so aussieht, und der Algorithmus lernt, Katzen zu erkennen.  Dafür muss der Computer Tausende und Abermillionen Bilder von Katzen und Katzen anzeigen. <br><br>  Forscher haben herausgefunden, dass diese Systeme mit speziell ausgewählten irreführenden Daten angegriffen werden können, die sie als „kontroverse Beispiele“ bezeichnen. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/862/7d4/14c/8627d414c9dbe2c4d35b06ca93e1fe0e.jpg" alt="Bild"><br>  <i>In einem Artikel aus dem Jahr 2015 haben Forscher von Google gezeigt, dass tiefe neuronale Netze gezwungen sein können, dieses Bild eines Pandas Gibbons zuzuschreiben.</i> <br><br>  "Wir zeigen Ihnen ein Foto, das den Schulbus deutlich zeigt und Sie glauben lässt, es sei ein Strauß", sagte Ian Goodfellow, ein Google-Forscher, der aktiv an solchen Angriffen auf neuronale Netze arbeitet. <br><br>  Die Forscher änderten die für neuronale Netze bereitgestellten Bilder nur um 4% und konnten sie in 97% der Fälle dazu verleiten, Fehler bei der Klassifizierung zu machen.  Selbst wenn sie nicht genau wüssten, wie das neuronale Netzwerk Bilder verarbeitet, könnten sie es in 85% der Fälle täuschen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die letzte Variante des</a> Betrugs ohne Daten in der Netzwerkarchitektur wird als „Black-Box-Angriff“ bezeichnet.  Dies ist der erste dokumentierte Fall eines funktionellen Angriffs dieser Art auf ein tiefes neuronales Netzwerk, und seine Bedeutung ist, dass ungefähr in diesem Szenario Angriffe in der realen Welt stattfinden können. <br><br>  In der Studie griffen Forscher der Pennsylvania State University, von Google und des US Navy Research Laboratory ein neuronales Netzwerk an, das vom MetaMind-Projekt unterstützte Bilder klassifiziert und als Online-Tool für Entwickler dient.  Das Team baute das angegriffene Netzwerk auf und trainierte es, aber sein Angriffsalgorithmus funktionierte unabhängig von der Architektur.  Mit einem solchen Algorithmus konnten sie das neuronale Black-Box-Netzwerk mit einer Genauigkeit von 84,24% austricksen. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/7b1/dc3/1f4/7b1dc31f4d7eb5712d1b230f7fdabf64.jpg" alt="Bild"><br>  <i>Die oberste Reihe von Fotos und Zeichen - korrekte Zeichenerkennung.</i> <i><br></i>  <i>Untere Reihe - Das Netzwerk musste Zeichen völlig falsch erkennen.</i> <br><br>  Das Einspeisen ungenauer Daten in Maschinen ist keine neue Idee, aber Doug Tygar, Professor an der Universität von Berkeley, der im Gegensatz dazu seit 10 Jahren maschinelles Lernen studiert, sagt, diese Angriffstechnologie habe sich von einem einfachen MO zu komplexen tiefen neuronalen Netzen entwickelt.  Böswillige Hacker verwenden diese Technik seit Jahren für Spam-Filter. <br><br>  Tigers Forschung stammt aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">seiner Arbeit</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2006</a> über Angriffe dieser Art in einem Netzwerk mit dem Verteidigungsministerium, die er 2011 mit Hilfe von Forschern von UC Berkeley und Microsoft Research erweiterte.  Das Google-Team, das als erstes tiefe neuronale Netze verwendet, veröffentlichte 2014 seine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste Arbeit</a> , zwei Jahre nachdem es die Möglichkeit solcher Angriffe entdeckt hatte.  Sie wollten sicherstellen, dass dies keine Anomalie war, sondern eine echte Möglichkeit.  2015 veröffentlichten sie eine weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit,</a> in der sie einen Weg zum Schutz von Netzwerken und zur Steigerung ihrer Effizienz beschrieben haben. Ian Goodfellow hat seitdem Ratschläge zu anderen wissenschaftlichen Arbeiten in diesem Bereich gegeben, einschließlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">des Black-Box-Angriffs</a> . <br><br>  Forscher nennen die allgemeinere Idee unzuverlässiger Informationen „byzantinische Daten“ und sind dank des Fortschritts der Forschung zu tiefem Lernen gekommen.  Der Begriff stammt von der bekannten „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufgabe der byzantinischen Generäle</a> “, einem Gedankenexperiment auf dem Gebiet der Informatik, bei dem eine Gruppe von Generälen ihre Handlungen mit Hilfe von Boten koordinieren muss, ohne das Vertrauen zu haben, dass einer von ihnen ein Verräter ist.  Sie können den Informationen ihrer Kollegen nicht vertrauen. <br><br>  „Diese Algorithmen sind für zufälliges Rauschen ausgelegt, nicht jedoch für byzantinische Daten“, sagt Taigar.  Um zu verstehen, wie solche Angriffe funktionieren, schlägt Goodfello vor, sich ein neuronales Netzwerk in Form eines Dispersionsdiagramms vorzustellen. <br><br>  Jeder Punkt im Diagramm repräsentiert ein Pixel des vom neuronalen Netzwerk verarbeiteten Bildes.  In der Regel versucht das Netzwerk, eine Linie durch die Daten zu ziehen, die am besten zur Menge aller Punkte passt.  In der Praxis ist dies etwas komplizierter, da unterschiedliche Pixel unterschiedliche Werte für das Netzwerk haben.  In Wirklichkeit ist dies ein komplexer mehrdimensionaler Graph, der von einem Computer verarbeitet wird. <br><br>  In unserer einfachen Analogie eines Streudiagramms bestimmt die Form der durch die Daten gezogenen Linie, was das Netzwerk zu sehen glaubt.  Für einen erfolgreichen Angriff auf solche Systeme müssen Forscher nur einen kleinen Teil dieser Punkte ändern und das Netzwerk eine Entscheidung treffen lassen, die tatsächlich nicht existiert.  In dem Beispiel eines Busses, der wie ein Strauß aussieht, ist das Foto des Schulbusses mit Pixeln gepunktet, die gemäß dem Muster angeordnet sind, das mit den einzigartigen Eigenschaften von Straußenfotos verbunden ist, die dem Netzwerk vertraut sind.  Dies ist eine unsichtbare Kontur für das Auge, aber wenn der Algorithmus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Daten verarbeitet und vereinfacht</a> , scheinen ihm die extremen Datenpunkte für den Strauß eine geeignete Klassifizierungsoption zu sein.  In der Black-Box-Version testeten die Forscher die Arbeit mit verschiedenen Eingabedaten, um festzustellen, wie der Algorithmus bestimmte Objekte sieht. <br><br>  Indem die Forscher dem Objektklassifizierer eine gefälschte Eingabe gaben und die von der Maschine getroffenen Entscheidungen untersuchten, konnten sie den Algorithmus wiederherstellen, um das Bilderkennungssystem zu täuschen.  Möglicherweise kann ein solches System in Robomobilen in diesem Fall das "Nachgeben" -Schild anstelle des Stoppschilds sehen.  Als sie verstanden, wie das Netzwerk funktionierte, konnten sie die Maschine alles sehen lassen. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/d91/d4a/f16/d91d4af16d2c1644444f58a67dd42e8e.jpg" alt="Bild"><br>  <i>Ein Beispiel dafür, wie der Bildklassifizierer abhängig von den verschiedenen Objekten im Bild unterschiedliche Linien zeichnet.</i>  <i>Gefälschte Beispiele können als Extremwerte in der Grafik betrachtet werden.</i> <br><br>  Forscher sagen, dass ein solcher Angriff direkt in das Bildverarbeitungssystem eingegeben werden kann, indem die Kamera umgangen wird, oder dass diese Manipulationen mit einem echten Zeichen ausgeführt werden können. <br><br>  Die Sicherheitsspezialistin der Columbia University, Alison Bishop, sagte jedoch, dass eine solche Prognose unrealistisch sei und von dem im Robomobile verwendeten System abhänge.  Wenn die Angreifer bereits Zugriff auf den Datenstrom von der Kamera haben, können sie ihm bereits Eingaben geben. <br><br>  "Wenn sie zum Eingang der Kamera gelangen können, sind solche Schwierigkeiten nicht erforderlich", sagt sie.  "Du kannst ihr nur das Stoppschild zeigen." <br><br>  Andere Angriffsmethoden, neben der Umgehung der Kamera - zum Beispiel das Zeichnen visueller Markierungen auf einem echten Schild - scheinen Bishop unwahrscheinlich.  Sie bezweifelt, dass die bei Robomobilen verwendeten Kameras mit niedriger Auflösung im Allgemeinen zwischen kleinen Änderungen des Vorzeichens unterscheiden können. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/9ef/d47/a6e/9efd47a6ee9f74748cdb79c976da84bf.jpg" alt="Bild"><br>  <i>Das makellose Bild links ist als Schulbus klassifiziert.</i>  <i>Rechts korrigiert - wie ein Strauß.</i>  <i>In der Mitte ändert sich das Bild.</i> <br><br>  Zwei Gruppen, eine an der University of Berkeley und eine an der Georgetown University, haben erfolgreich Algorithmen entwickelt, mit denen digitale Assistenten wie Siri und Google Now Sprachbefehle erhalten können, die wie unhörbares Rauschen klingen.  Für eine Person erscheinen solche Befehle wie zufälliges Rauschen, aber gleichzeitig können sie Geräten wie Alexa Befehle erteilen, die von ihrem Besitzer nicht vorgesehen sind. <br><br>  Nicholas Carlini, einer der Forscher für byzantinische Audioangriffe, sagt, dass sie in ihren Tests Open-Source-Audioerkennungsprogramme, Siri und Google Now, mit einer Genauigkeit von mehr als 90% aktivieren konnten. <br><br>  Das Geräusch ist wie eine Art Science-Fiction-Alien-Verhandlung.  Dies ist eine Mischung aus weißem Rauschen und einer menschlichen Stimme, aber es ist überhaupt nicht wie ein Sprachbefehl. <br><br>  Laut Carlini kann bei einem solchen Angriff jeder, der ein Telefongeräusch gehört hat (es ist erforderlich, Angriffe auf iOS und Android separat zu planen), gezwungen werden, eine Webseite aufzurufen, auf der auch Geräusche wiedergegeben werden, wodurch in der Nähe befindliche Telefone infiziert werden.  Oder diese Seite lädt leise ein Malware-Programm herunter.  Es ist auch möglich, dass solche Geräusche im Radio verloren gehen und im weißen Rauschen oder parallel zu anderen Audioinformationen versteckt werden. <br><br>  Solche Angriffe können auftreten, weil die Maschine darauf trainiert ist, sicherzustellen, dass fast alle Daten wichtige Daten enthalten und dass eines häufiger vorkommt als das andere, wie von Goodfello erläutert. <br><br>  Das Netzwerk zu täuschen und es zu zwingen zu glauben, dass es ein gemeinsames Objekt sieht, ist einfacher, weil es glaubt, dass es solche Objekte häufiger sehen sollte.  Daher konnten Goodfellow und eine andere Gruppe von der University of Wyoming das Netzwerk dazu bringen, Bilder zu klassifizieren, die überhaupt nicht existierten - es identifizierte Objekte in weißem Rauschen, zufällig erzeugte Schwarz-Weiß-Pixel. <br><br>  In einer Goodfellow-Studie wurde zufälliges weißes Rauschen, das durch ein Netzwerk geht, von ihr als Pferd eingestuft.  Zufälligerweise bringt uns dies zurück zur Geschichte von Clever Hans, einem nicht sehr mathematisch begabten Pferd. <br><br>  Goodfellow sagt, dass neuronale Netze wie Smart Hans keine Ideen lernen, sondern nur herausfinden, wann sie die richtige Idee finden.  Der Unterschied ist klein aber wichtig.  Der Mangel an grundlegendem Wissen erleichtert böswillige Versuche, den Anschein zu erwecken, die „richtigen“ Algorithmusergebnisse zu finden, die sich tatsächlich als falsch herausstellen.  Um zu verstehen, was etwas ist, muss eine Maschine auch verstehen, was es nicht ist. <br><br>  Goodfello, der die Netzwerksortierung von Bildern sowohl auf natürlichen Bildern als auch auf verarbeiteten (gefälschten) Bildern trainiert hatte, stellte fest, dass er nicht nur die Wirksamkeit solcher Angriffe um 90% reduzieren, sondern auch das Netzwerk besser für die anfängliche Aufgabe einsetzen konnte. <br><br>  „Indem Sie es ermöglichen, wirklich ungewöhnliche gefälschte Bilder zu erklären, können Sie die zugrunde liegenden Konzepte noch zuverlässiger erklären“, sagt Goodfellow. <br><br>  Zwei Gruppen von Audioforschern verwendeten einen ähnlichen Ansatz wie das Google-Team und schützten ihre neuronalen Netze durch Übertraining vor ihren eigenen Angriffen.  Sie erzielten ähnliche Erfolge und reduzierten ihre Angriffseffizienz um mehr als 90%. <br><br>  Es ist nicht verwunderlich, dass dieses Forschungsgebiet das US-Militär interessierte.  Das Army Research Laboratory hat sogar zwei der neuesten Arbeiten zu diesem Thema gesponsert, darunter den Black-Box-Angriff.  Und obwohl die Agentur Forschung finanziert, bedeutet dies nicht, dass Technologie im Krieg eingesetzt wird.  Laut dem Vertreter der Abteilung können bis zu 10 Jahre von der Forschung bis zu Technologien vergehen, die für den Einsatz durch einen Soldaten geeignet sind. <br><br>  Ananthram Swami, ein Forscher am US Army Laboratory, war kürzlich an mehreren Arbeiten zur KI-Täuschung beteiligt.  Die Armee ist an der Aufdeckung und Beendigung betrügerischer Daten in unserer Welt interessiert, in der nicht alle Informationsquellen sorgfältig geprüft werden können.  Swami verweist auf eine Reihe von Daten, die von öffentlichen Sensoren an Universitäten stammen und in Open-Source-Projekten arbeiten. <br><br>  „Wir kontrollieren nicht immer alle Daten.  Für unseren Gegner ist es ziemlich einfach, uns auszutricksen “, sagt Swami.  "In einigen Fällen können die Folgen eines solchen Betrugs leichtfertig sein, in einigen Fällen das Gegenteil." <br><br>  Er sagt auch, dass die Armee an autonomen Robotern, Panzern und anderen Fahrzeugen interessiert ist, so dass das Ziel dieser Forschung offensichtlich ist.  Durch die Untersuchung dieser Themen kann die Armee einen Vorsprung bei der Entwicklung von Systemen gewinnen, die keinen derartigen Angriffen ausgesetzt sind. <br><br>  Jede Gruppe, die ein neuronales Netzwerk verwendet, sollte jedoch Bedenken hinsichtlich der Möglichkeit von Angriffen mit KI-Spoofing haben.  Maschinelles Lernen und KI stecken noch in den Kinderschuhen, und Sicherheitslücken können derzeit schwerwiegende Folgen haben.  Viele Unternehmen vertrauen hochsensiblen Informationen auf KI-Systeme, die den Test der Zeit nicht bestanden haben.  Unsere neuronalen Netze sind noch zu jung, um alles zu wissen, was wir über sie brauchen. <br><br>  Ein ähnliches Versehen führte dazu, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Microsofts Twitter-Bot Tay</a> schnell zu einem Rassisten mit einer Vorliebe für Völkermord wurde.  Der Fluss bösartiger Daten und die Funktion „Nach mir wiederholen“ führten dazu, dass Tay stark vom beabsichtigten Pfad abwich.  Der Bot wurde durch minderwertige Eingaben ausgetrickst, und dies ist ein praktisches Beispiel für eine schlechte Implementierung des maschinellen Lernens. <br><br>  Kanchelyan glaubt nicht, dass die Möglichkeiten für solche Angriffe nach erfolgreicher Recherche durch das Google-Team ausgeschöpft sind. <br><br>  "Im Bereich der Computersicherheit sind Angreifer immer vor uns", sagt Kanchelyan.  "Es wird ziemlich gefährlich sein zu behaupten, dass wir alle Probleme mit der Täuschung neuronaler Netze durch ihr wiederholtes Training gelöst haben." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de405773/">https://habr.com/ru/post/de405773/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de405763/index.html">Kingston Duo 3C - Lebensretter für microSD-Speicherkarten</a></li>
<li><a href="../de405765/index.html">Maximieren Sie Ihre Einsparungen mit Canon MAXIFY: Kompakte Tintenstrahldrucker für mittelgroße Arbeitsgruppen und Home Offices</a></li>
<li><a href="../de405767/index.html">Bitcoin Cash: Genie aus der Flasche entlassen</a></li>
<li><a href="../de405769/index.html">Auf der Suche nach verlorenem Geld: BTC-E-Administratoren kündigten die Rückkehr der Kontrolle über die Börsenbasis an. Das ist aber nicht sicher</a></li>
<li><a href="../de405771/index.html">Fünf Jahre auf dem Mars</a></li>
<li><a href="../de405775/index.html">Nur Kaspersky Anti-Virus blockiert das CIA-Dienstprogramm</a></li>
<li><a href="../de405777/index.html">Der Klimawandel könnte einen Teil Südasiens unbewohnt machen</a></li>
<li><a href="../de405779/index.html">Fortgeschrittene Zivilisationen können das galaktische Internet mithilfe planetarischer Komplettlösungen aufbauen</a></li>
<li><a href="../de405781/index.html">Leben mit einem Stern - Teil 2: Weltraumwetter</a></li>
<li><a href="../de405783/index.html">Die dunkle Zukunft des Internets: Ungleichheit und mangelnde Freiheit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>