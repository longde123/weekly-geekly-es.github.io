<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÄ üë° üéä NeurIPS: C√≥mo conquistar la mejor conferencia de ML ü§∂üèΩ üö∫ üëºüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS ‚Äì‚Äì una conferencia que actualmente se considera el evento m√°s importante en el mundo del aprendizaje autom√°tico. Hoy les contar√© sobre mi expe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: C√≥mo conquistar la mejor conferencia de ML</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NeurIPS</a> ‚Äì‚Äì una conferencia que actualmente se considera el evento m√°s importante en el mundo del aprendizaje autom√°tico.  Hoy les contar√© sobre mi experiencia en participar en concursos de NeurIPS: c√≥mo competir con los mejores acad√©micos del mundo, obtener un premio y publicar un art√≠culo. </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  ¬øCu√°l es la esencia de la conferencia? </h1><br><p>  NeurIPS admite la introducci√≥n de m√©todos de aprendizaje autom√°tico en diversas disciplinas cient√≠ficas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Se</a> lanzan alrededor de 10 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pistas</a> anualmente para resolver problemas apremiantes del mundo acad√©mico.  Seg√∫n los resultados de la competencia, los ganadores hablan en la conferencia con informes, nuevos desarrollos y algoritmos.  Sobre todo, me apasiona el aprendizaje reforzado (Reinforcement Learning o RL), por eso he estado participando en concursos de RL dedicados a NeurIPS por segundo a√±o. </p><br><h1 id="pochemu-neurips">  Por qu√© NeurIPS </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS se centra principalmente en la ciencia, no en el dinero.  Al participar en concursos, est√° haciendo algo realmente importante, lidiando con problemas urgentes. <br><p>  En segundo lugar, esta conferencia es un evento global, los cient√≠ficos de diferentes pa√≠ses se re√∫nen en un solo lugar, con cada uno de los cuales puedes hablar. </p><br><p>  Adem√°s, toda la conferencia est√° llena de los √∫ltimos logros cient√≠ficos y resultados de vanguardia, es extremadamente importante que las personas del campo de la ciencia de datos los conozcan y supervisen. </p><br><h1 id="kak-nachat">  Como empezar </h1><br><p>  Comenzar a participar en tales competiciones es bastante simple.  Si comprende DL tanto que puede <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrenar a ResNet</a> , esto es suficiente: reg√≠strese y listo.  Siempre hay una tabla de clasificaci√≥n p√∫blica en la que puede evaluar sobriamente su nivel en comparaci√≥n con otros participantes.  Y si algo no est√° claro, siempre hay canales en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">slack</a> / discord / gitter / etc para discutir todos los temas emergentes.  Si el tema es realmente "suyo", entonces nada le impedir√° recibir el preciado resultado ‚Äì‚Äì en todas las competiciones en las que particip√©, todos los enfoques y soluciones fueron estudiados e implementados en el transcurso de la competencia. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  Estudio de caso de NeurIPS: aprender a correr </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Problema </h3><br><p>  La marcha de una persona es el resultado de la interacci√≥n de m√∫sculos, huesos, √≥rganos de la visi√≥n y el o√≠do interno.  En caso de interrupci√≥n del sistema nervioso central, pueden ocurrir ciertos trastornos motores, incluyendo trastornos de la marcha, abasia. <br>  Investigadores del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Laboratorio de Biomec√°nica Neuromuscular de Stanford</a> decidieron conectar el aprendizaje autom√°tico con el problema del tratamiento para poder experimentar y probar sus teor√≠as en un modelo virtual del esqueleto, y no en personas vivas. </p><br><h3 id="postanovka-zadachi">  Declaraci√≥n del problema. </h3><br><p>  Los participantes recibieron un esqueleto humano virtual (en el simulador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenSim</a> ), que ten√≠a una pr√≥tesis en lugar de una pierna.  La tarea era ense√±arle al esqueleto a moverse en cierta direcci√≥n a una velocidad dada.  Durante la simulaci√≥n, tanto la direcci√≥n como la velocidad podr√≠an cambiar. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Para obtener un modelo de control de esqueleto virtual, se propuso utilizar el aprendizaje por refuerzo.  El simulador nos dio alg√∫n estado del esqueleto S (un vector de ~ 400 n√∫meros).  Era necesario predecir qu√© acci√≥n A debe realizarse (las fuerzas de activaci√≥n de los m√∫sculos de las piernas son un vector de 19 n√∫meros).  Durante la simulaci√≥n, el esqueleto recibi√≥ un premio R, como una constante constante menos la penalizaci√≥n por desviarse de una velocidad y direcci√≥n determinadas. <br><div class="spoiler">  <b class="spoiler_title">Sobre entrenamiento de refuerzo</b> <div class="spoiler_text"><p>  El aprendizaje por refuerzo (RL) es un √°rea que se ocupa de la teor√≠a de la decisi√≥n y la b√∫squeda de pol√≠ticas de comportamiento √≥ptimas. </p><br><p>  Recordemos c√≥mo ense√±an <del>  gato </del>  perrito nuevos trucos.  Repite alguna acci√≥n, da un delicioso para realizar un truco, y no des por incumplimiento.  El perro debe entender todo esto y encontrar una estrategia de comportamiento ("pol√≠tica" o "pol√≠tica" en t√©rminos de RL), que maximiza la cantidad de dulces recibidos. </p><br><p>  Formalmente, tenemos un agente (perro) que est√° capacitado en el historial de interacciones con el entorno (persona).  Al mismo tiempo, el entorno, al evaluar las acciones del agente, le proporciona una recompensa (deliciosa): cuanto mejor sea el comportamiento del agente, mayor ser√° la recompensa.  En consecuencia, la tarea del agente es encontrar una pol√≠tica que maximice bien la recompensa por todo el tiempo de interacci√≥n con el medio ambiente. </p><br><p>  Desarrollando m√°s este tema, soluciones basadas en reglas (software 1.0, cuando todas las reglas fueron establecidas por el desarrollador, aprendizaje supervisado), el mismo software 2.0, cuando el sistema aprende a s√≠ mismo utilizando los ejemplos disponibles y encuentra dependencias de datos, el aprendizaje de refuerzo est√° un paso m√°s all√° cuando el sistema mismo aprende a investigar, experimentar y encontrar las dependencias requeridas en sus decisiones.  Cuanto m√°s avanzamos, mejor intentamos repetir c√≥mo aprende una persona. </p></div></div><br><h3 id="osobennosti-zadachi">  Caracter√≠sticas de la tarea </h3><br><p>  La tarea parece un representante t√≠pico del aprendizaje reforzado para tareas con espacio de acci√≥n continua (RL para espacio de acci√≥n continua).  Se diferencia del RL ordinario en que, en lugar de elegir una acci√≥n espec√≠fica (presionar el bot√≥n del joystick), esta acci√≥n es necesaria para predecir con precisi√≥n (y hay infinitas posibilidades). </p><br><p>  El enfoque b√°sico de la soluci√≥n ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Deterministic Policy Gradient</a> ) se invent√≥ en 2015, que durante mucho tiempo seg√∫n los est√°ndares de DL, la regi√≥n contin√∫a desarroll√°ndose activamente en aplicaciones de rob√≥tica y aplicaciones RL del mundo real.  Hay algo que mejorar: enfoques robustos (para no romper un robot real), eficiencia de muestreo (para no recopilar datos de robots reales durante meses) y otros problemas de RL (compensaci√≥n de exploraci√≥n vs explotaci√≥n, etc.).  En esta competencia, no nos dieron un robot real, solo una simulaci√≥n, pero el simulador en s√≠ mismo fue 2.000 veces m√°s lento que los de c√≥digo abierto (en el que todos verifican sus algoritmos RL), y por lo tanto llev√≥ el problema de la eficiencia de la muestra a un nuevo nivel. </p><br><h3 id="etapy-sorevnovaniya">  Etapas de competencia </h3><br><p>  La competencia en s√≠ tuvo lugar en tres etapas, durante las cuales la tarea y las condiciones cambiaron un poco. </p><br><ul><li>  Etapa 1: el esqueleto aprendi√≥ a caminar derecho a una velocidad de 3 metros por segundo.  La tarea se consider√≥ completada si el agente realiz√≥ 300 pasos. </li><li>  Etapa 2: la velocidad y la direcci√≥n cambiaron con una frecuencia regular.  La longitud de la distancia aument√≥ a 1000 pasos. </li><li>  Etapa 3: la soluci√≥n final tuvo que empaquetarse en una imagen acoplable y enviarse para su verificaci√≥n.  En total, se pueden hacer 10 paquetes. </li></ul><br><p>  La m√©trica de calidad principal se consider√≥ la recompensa total por la simulaci√≥n, que mostr√≥ cu√°n bien se adhiri√≥ el esqueleto a una direcci√≥n y velocidad determinadas a lo largo de la distancia. </p><br><p>  Durante la primera y segunda etapa, el progreso de cada participante se mostr√≥ en la tabla de clasificaci√≥n.  La soluci√≥n final deb√≠a enviarse como una imagen acoplable.  Proporcion√≥ restricciones sobre las horas de trabajo y los recursos. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: tabla de clasificaci√≥n p√∫blica y RL</b> <div class="spoiler_text"><p>  Debido a la disponibilidad de la tabla de clasificaci√≥n, nadie muestra su mejor modelo para dar "un poco m√°s de lo habitual" en la ronda final y sorprender a sus rivales. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  ¬øPor qu√© las im√°genes de Docker son tan importantes? </h6><br><p>  El a√±o pasado, ocurri√≥ un peque√±o incidente al evaluar las decisiones en la primera ronda.  En ese momento, la verificaci√≥n se realiz√≥ a trav√©s de la interacci√≥n http con la plataforma, y ‚Äã‚Äãse encontr√≥ una cara de las condiciones para la prueba.  Uno podr√≠a descubrir en qu√© situaciones particulares se evalu√≥ al agente y volver a capacitarlo solo en estas condiciones.  Lo cual, por supuesto, no resolvi√≥ el problema real.  Es por eso que decidieron transferir el sistema de env√≠os a im√°genes acopladas y lanzarlo en los servidores remotos de los organizadores.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dbrain</a> usa el mismo sistema para calcular el resultado de las competiciones precisamente por las mismas razones. </p><br><h1 id="klyuchevye-momenty">  Puntos clave </h1><br><h3 id="komanda">  El equipo </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  Lo primero que es importante para el √©xito de toda la empresa es el equipo.  No importa cu√°n bueno sea (y cu√°n poderosas sean sus patas): la participaci√≥n en el equipo aumenta enormemente las posibilidades de √©xito.  La raz√≥n es simple: una variedad de opiniones y enfoques, volver a comprobar las hip√≥tesis, la capacidad de paralelizar el trabajo y realizar m√°s experimentos.  Todo esto es extremadamente importante cuando se resuelven nuevos problemas que tiene que enfrentar. <br><p>  Idealmente, sus conocimientos y habilidades deben estar en el mismo nivel y complementarse entre s√≠.  Entonces, por ejemplo, este a√±o plant√© nuestro equipo en PyTorch y obtuve algunas ideas iniciales sobre la implementaci√≥n de un sistema de capacitaci√≥n de agentes distribuidos. </p><br><p>  ¬øC√≥mo encontrar un equipo?  En primer lugar, puede unirse a las filas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ods</a> y buscar personas con ideas afines all√≠.  En segundo lugar, para los becarios de RL hay una sala de chat separada en un telegrama: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RL club</a> .  En tercer lugar, puede tomar un curso maravilloso de ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Practical RL</a> , despu√©s del cual seguramente obtendr√° un par de conocidos. </p><br><p>  Sin embargo, vale la pena recordar la pol√≠tica de "sumisi√≥n, o no lo fue".  Si quieres unirte, primero toma tu decisi√≥n, env√≠a, aparece en la tabla de clasificaci√≥n y muestra tu nivel.  Como muestra la pr√°ctica, estos equipos est√°n mucho m√°s equilibrados. </p><br><h3 id="motivaciya">  Motivaci√≥n </h3><br><p>  Como ya escrib√≠, si el tema es "tuyo", nada te detendr√°.  Esto significa que la regi√≥n no solo te quiere, sino que te inspira: la quemas, quieres convertirte en el mejor. <br>  Conoc√≠ a RL hace 4 a√±os, durante el paso del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Berkeley 188x - Introducci√≥n a la IA</a> , y todav√≠a no puedo dejar de preguntarme por el progreso en esta √°rea. </p><br><h3 id="sistematichnost">  Sistem√°tica </h3><br><p>  Tercero, pero igualmente importante: debe ser capaz de hacer lo que prometi√≥, invertir en la competencia todos los d√≠as y simplemente ... resolverlo.  Todos los dias  Ning√∫n talento innato se puede comparar con la capacidad de hacer algo, aunque sea un poco, pero todos los d√≠as.  Es por esto que se requerir√° motivaci√≥n.  Para tener √©xito, recomiendo leer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DeepWork</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AMA ternaus</a> . </p><br><h3 id="time-management">  Gesti√≥n del tiempo </h3><br><p>  Otra habilidad extremadamente importante es la capacidad de distribuir la fuerza de uno y usar correctamente el tiempo libre.  Combinar el trabajo a tiempo completo y la participaci√≥n en competiciones es una tarea no trivial.  Lo m√°s importante en estas condiciones es no quemarse y soportar toda la carga.  Para hacer esto, debe administrar adecuadamente su tiempo, evaluar sobriamente su fuerza y ‚Äã‚Äãno olvidar relajarse a tiempo. </p><br><h3 id="overwork">  Exceso de trabajo </h3><br><p>  En la etapa final de la competencia, generalmente surge una situaci√≥n en la que, literalmente, en una semana debes hacer no solo mucho, sino MUCHO.  Para obtener el mejor resultado, debe poder obligarse a sentarse y dar el √∫ltimo impulso al codiciado premio. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: plazo tras plazo</b> <div class="spoiler_text"><p>  ¬øPor qu√©, en general, es posible que deba reciclar en beneficio de la competencia?  La respuesta es bastante simple: transferencia de fecha l√≠mite.  En tales competiciones, los organizadores a menudo no pueden predecir todo, por lo que la forma m√°s f√°cil es dar a los participantes m√°s tiempo.  Este a√±o, la competencia se extendi√≥ 3 veces: primero por un mes, luego por una semana y en el √∫ltimo momento (24 horas antes de la fecha l√≠mite), por otros 2 d√≠as.  Y si durante las dos primeras transferencias solo necesitabas organizar el tiempo extra correctamente, en los √∫ltimos dos d√≠as solo ten√≠as que arar. </p></div></div><br><h3 id="theory">  Teor√≠a </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Entre otras cosas, no se olvide de la teor√≠a: estar al tanto de lo que est√° sucediendo en el campo y ser capaz de anotar lo relevante.  Entonces, por ejemplo, para resolver el a√±o pasado, nuestro equipo despeg√≥ de los siguientes art√≠culos: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El control continuo con aprendizaje de refuerzo profundo</a> es un art√≠culo b√°sico sobre el aprendizaje de refuerzo profundo para tareas con espacio de acci√≥n continua. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ruido espacial de par√°metros para exploraci√≥n</a> : un estudio sobre la adici√≥n de ruido a los pesos de los agentes para un mejor estudio del medio ambiente.  Por experiencia, una de las mejores t√©cnicas para la exploraci√≥n en RL. </li></ul><br><p>  Este a√±o, se les agregaron un par m√°s: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una perspectiva distributiva sobre el aprendizaje por refuerzo</a> : una nueva mirada a las predicciones de una posible recompensa.  En lugar de simplemente predecir el promedio, se calcula la distribuci√≥n de recompensas futuras. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El aprendizaje de refuerzo distributivo con regresi√≥n cuantil</a> es una continuaci√≥n del trabajo anterior, pero con la "cuantizaci√≥n" de la distribuci√≥n. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repetici√≥n de experiencia priorizada distribuida</a> : trabaje desde la direcci√≥n del aprendizaje de refuerzo profundo a escala.  Acerca de c√≥mo organizar adecuadamente la arquitectura del experimento para maximizar el uso de los recursos disponibles y aumentar la velocidad de los agentes de capacitaci√≥n. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gradientes de pol√≠tica deterministas distribuidos distribuidos</a> : una combinaci√≥n de los tres art√≠culos anteriores para tareas con un espacio continuo de acci√≥n. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Abordar el error de aproximaci√≥n de la funci√≥n en los m√©todos cr√≠ticos para el actor</a> : excelente trabajo para aumentar la robustez de los agentes de RL.  Recomiendo leerlo. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El aprendizaje de refuerzo jer√°rquico eficiente en datos</a> es un desarrollo de un art√≠culo anterior en el campo del aprendizaje de refuerzo jer√°rquico (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Lectura adicional</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Soft-Actor-Critic: Aprendizaje de refuerzo profundo de entrop√≠a m√°xima fuera de pol√≠tica con un actor estoc√°stico</a> : los autores propusieron un m√©todo para capacitar pol√≠ticas estoc√°sticas con aprendizaje de refuerzo fuera de pol√≠tica.  Gracias a este art√≠culo, se hizo posible capacitar a pol√≠ticos no deterministas incluso en tareas con un espacio continuo de acci√≥n. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pol√≠ticas de espacio latente para el aprendizaje jer√°rquico de refuerzo</a> es una continuaci√≥n de un art√≠culo anterior de HRL con pol√≠ticas estoc√°sticas de varios niveles. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La diversidad es todo lo que necesita: habilidades de aprendizaje sin una funci√≥n de recompensa</a> : este art√≠culo contiene un enfoque con el aprendizaje de muchas pol√≠ticas estoc√°sticas aleatorias de bajo nivel sin ninguna recompensa del medio ambiente.  Posteriormente, cuando hemos establecido la funci√≥n de recompensa, lo m√°s relacionado con el premio se puede utilizar para ense√±ar pol√≠ticas de alto nivel en la parte superior. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje y control de refuerzo como inferencia probabil√≠stica: Tutorial y revisi√≥n</a> : una descripci√≥n general de todo tipo de m√©todos de aprendizaje de refuerzo de entrop√≠a m√°xima de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Tambi√©n aconsejo a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI una selecci√≥n de art√≠culos</a> sobre aprendizaje por refuerzo y su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versi√≥n para mendeley</a> .  Y si est√° interesado en el tema de la capacitaci√≥n de refuerzo, √∫nase al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">club</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RL</a> y los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentos RL</a> . </p></div></div><br><h3 id="practice">  Practica </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Conocer la teor√≠a por s√≠ sola no es suficiente: es importante poder poner en pr√°ctica todos estos enfoques y establecer el sistema de validaci√≥n correcto para evaluar las decisiones.  Por ejemplo, este a√±o nos enteramos de que nuestro agente maneja mal algunos casos regionales solo 2 d√≠as antes del final de la competencia.  Debido a esto, no tuvimos tiempo para arreglar completamente nuestro modelo y literalmente no obtuvimos algunos puntos para el codiciado segundo lugar.  Si encontramos esto incluso en una semana, el resultado podr√≠a ser mejor. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: episodio III</b> <div class="spoiler_text"><p>  El premio promedio por 10 episodios de prueba sirvi√≥ como la evaluaci√≥n final de la soluci√≥n. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  El gr√°fico muestra los resultados de probar a nuestro agente: 9 de cada 10 episodios, nuestro esqueleto sali√≥ bien (promedio - 9955.66), pero un episodio ... El episodio 3 no se le dio (recompensa 9870).  Fue este error lo que llev√≥ a la ca√≠da de la velocidad final a 9947 (-8 puntos). </p></div></div><br><h3 id="udacha">  Buena suerte </h3><br><p>  Y finalmente, no te olvides de la suerte banal.  No piense que este es un punto controvertido.  Por el contrario, un poco de suerte contribuye en gran medida al trabajo constante sobre uno mismo: incluso si la probabilidad de suerte es solo del 10%, una persona que intent√≥ participar en la competencia 100 veces tendr√° mucho m√°s √©xito que alguien que lo intent√≥ solo 1 vez y abandon√≥ la idea. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Ida y vuelta: decisi√≥n del a√±o pasado - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tercer lugar</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  El a√±o pasado, nuestro equipo, Mikhail Pavlov y yo, participamos en las competencias de NeurIPS por primera vez y la motivaci√≥n principal era simplemente participar en la primera competencia de NeurIPS en el aprendizaje por refuerzo.  Luego, acabo de terminar el curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Practical RL</a> en el SHAD y quer√≠a probar las habilidades adquiridas.  Como resultado, tomamos un honorable tercer lugar, perdiendo solo ante el nnaisene (Schmidhuber) y el equipo universitario de China.  En ese momento, nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">soluci√≥n</a> era "bastante simple" y se basaba en DDPG distribuido con ruido de par√°metros ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicaci√≥n</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">presentaci√≥n en ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Capacitaciones</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  La decisi√≥n de este a√±o es el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tercer lugar</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  Ha habido un par de cambios este a√±o.  En primer lugar, no hab√≠a ganas de participar en esta competencia, quer√≠a ganarla.  En segundo lugar, la composici√≥n del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">equipo</a> tambi√©n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ha</a> cambiado: Alexey Grinchuk, Anton Pechenko y yo.  Take and win - no funcion√≥, pero nuevamente tomamos el 3er lugar. <br>  Nuestra soluci√≥n se presentar√° oficialmente en NeurIPS, y ahora nos limitaremos a una peque√±a cantidad de detalles.  Con base en la decisi√≥n del a√±o pasado y el √©xito del aprendizaje de refuerzo fuera de la pol√≠tica de este a√±o (art√≠culos anteriores), agregamos una serie de nuestros propios desarrollos, de los que hablaremos en NeurIPS, y obtuvimos la Cr√≠tica del conjunto de cuantiles distribuidos, con la que obtuvimos el tercer lugar. </p><br><p>  Todas nuestras mejores pr√°cticas: un sistema de aprendizaje distribuido, algoritmos, etc. se publicar√°n y estar√°n disponibles en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Catalyst.RL</a> despu√©s de NeurIPS. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: chicos grandes - armas grandes</b> <div class="spoiler_text"><p>  Nuestro equipo fue con confianza al 1er lugar durante toda la competencia.  Sin embargo, los grandes ten√≠an otros planes: 2 grandes jugadores ingresaron a la competencia 2 semanas antes del final de la competencia: FireWork (Baidu) y nnaisense (Schmidhuber).  Y si no se pod√≠a hacer nada con el Google chino, entonces con el equipo de Schmidhuber por un tiempo pudimos luchar honestamente por el segundo lugar, perdiendo solo con un margen m√≠nimo.  Me parece bastante bueno para los amantes. </p></div></div><br><h1 id="zachem-eto-vse">  ¬øPor qu√© es todo esto? </h1><br><ul><li>  Comunicaci√≥n.  Los mejores investigadores acuden a la conferencia con los que puedes chatear en vivo, lo que no dar√° correspondencia por correo electr√≥nico. </li><li>  Publicaci√≥n  Si la soluci√≥n se lleva el premio, se invita al equipo a la conferencia (o tal vez m√°s de uno) para presentar su decisi√≥n y publicar el art√≠culo. </li><li>  Oferta de trabajo y doctorado.  La publicaci√≥n y un premio en dicha conferencia aumentan significativamente sus posibilidades de obtener un puesto en compa√±√≠as l√≠deres como OpenAI, DeepMind, Google, Facebook, Microsoft. </li><li>  Valor del mundo real.  NeurIPS se lleva a cabo para resolver problemas apremiantes del mundo acad√©mico y real.  Puede estar seguro de que los resultados no ir√°n a la mesa, sino que tendr√°n una gran demanda y ayudar√°n a mejorar el mundo. </li><li>  Conducir  Resolver tales concursos ... simplemente interesante.  En una competencia, puedes encontrar muchas ideas nuevas, probar diferentes enfoques, solo para ser el mejor.  Y seamos honestos, ¬øcu√°ndo m√°s puedes conducir esqueletos, jugar juegos y todo esto con una mirada seria y por el bien de la ciencia? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: visa y RL</b> <div class="spoiler_text"><p>  No recomiendo tratar de explicarle al estadounidense que lo verifica que va a asistir a la conferencia, mientras entrena esqueletos virtuales para ejecutar simulaciones.  Solo ve a la conferencia con una charla. </p></div></div><br><h1 id="itogi">  Resumen </h1><br><p>  Participar en NeurIPS es una experiencia dif√≠cil de sobreestimar.  No tengas miedo a los titulares de alto perfil: solo necesitas unirte y comenzar a decidir. </p><br><p>  Y ve a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Catalyst.RL</a> , entonces qu√©. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es430712/">https://habr.com/ru/post/es430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es430702/index.html">Entrenamiento muy extra√±o</a></li>
<li><a href="../es430704/index.html">C√≥mo las tecnolog√≠as de inteligencia artificial ayudan a Aviasales a crecer: siete ejemplos</a></li>
<li><a href="../es430706/index.html">Nueva teor√≠a de la evoluci√≥n</a></li>
<li><a href="../es430708/index.html">Tic Tac Toe "Sin Fronteras"</a></li>
<li><a href="../es430710/index.html">Qu√© hacer si Black Friday es ma√±ana y sus servidores no est√°n listos</a></li>
<li><a href="../es430714/index.html">VMware compra Heptio: ¬øqu√© significa para Kubernetes?</a></li>
<li><a href="../es430718/index.html">¬øPara qu√© objetos vale la pena usar la videovigilancia en la nube?</a></li>
<li><a href="../es430720/index.html">Intel RealSense D435i: peque√±a actualizaci√≥n y breve digresi√≥n hist√≥rica</a></li>
<li><a href="../es430722/index.html">Rendimiento de PHP: planificaci√≥n, creaci√≥n de perfiles, optimizaci√≥n</a></li>
<li><a href="../es430724/index.html">DEFCON 21. La conferencia DNS puede ser peligrosa para su salud. Parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>