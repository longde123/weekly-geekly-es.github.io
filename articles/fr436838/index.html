<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèà üë∞üèª üññüèª Comprendre les r√©seaux de neurones convolutifs gr√¢ce √† des visualisations dans PyTorch üñáÔ∏è üì≠ üïπÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä notre √©poque, les machines ont r√©ussi √† atteindre 99% de pr√©cision dans la compr√©hension et la d√©finition des caract√©ristiques et des objets dans le...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comprendre les r√©seaux de neurones convolutifs gr√¢ce √† des visualisations dans PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/">  √Ä notre √©poque, les machines ont r√©ussi √† atteindre 99% de pr√©cision dans la compr√©hension et la d√©finition des caract√©ristiques et des objets dans les images.  Nous sommes confront√©s √† cela tous les jours, par exemple: la reconnaissance faciale dans l'appareil photo du smartphone, la possibilit√© de rechercher des photos sur google, de scanner du texte √† partir d'un code √† barres ou de livres √† une bonne vitesse, etc. le r√©seau.  Si vous √™tes un passionn√© d'apprentissage en profondeur, vous en avez probablement entendu parler et vous pourriez d√©velopper plusieurs classificateurs d'images.  Les frameworks modernes d'apprentissage en profondeur tels que Tensorflow et PyTorch simplifient l'apprentissage automatique d'images.  Cependant, la question demeure: comment les donn√©es passent-elles √† travers les couches du r√©seau neuronal et comment l'ordinateur en tire-t-il des le√ßons?  Pour obtenir une vue claire √† partir de z√©ro, nous plongons dans une convolution, en visualisant l'image de chaque couche. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="image"><br><a name="habracut"></a><br><h2>  R√©seaux de neurones convolutifs </h2><br>  Avant de commencer √† √©tudier les r√©seaux de neurones convolutifs (SNA), vous devez apprendre √† travailler avec les r√©seaux de neurones.  Les r√©seaux de neurones imitent le cerveau humain pour r√©soudre des probl√®mes complexes et rechercher des mod√®les dans les donn√©es.  Au cours des derni√®res ann√©es, ils ont remplac√© de nombreux algorithmes d'apprentissage automatique et de vision par ordinateur.  Le mod√®le de base d'un r√©seau neuronal est constitu√© de neurones organis√©s en couches.  Chaque r√©seau de neurones a une couche d'entr√©e et de sortie et plusieurs couches cach√©es qui lui sont ajout√©es en fonction de la complexit√© du probl√®me.  Lors de la transmission de donn√©es √† travers des couches, les neurones sont entra√Æn√©s et reconnaissent les signes.  Cette repr√©sentation d'un r√©seau neuronal s'appelle un mod√®le.  Une fois le mod√®le form√©, nous demandons au r√©seau de faire des pr√©visions sur la base des donn√©es de test. <br><br>  Le SNS est un type sp√©cial de r√©seau de neurones qui fonctionne bien avec les images.  Ian Lekun les a propos√©s en 1998, o√π ils ont reconnu le nombre pr√©sent dans l'image d'entr√©e.  Le SNA est √©galement utilis√© pour la reconnaissance vocale, la segmentation d'images et le traitement de texte.  Avant la cr√©ation de r√©seaux de neurones convolutifs, des perceptrons multicouches √©taient utilis√©s dans la construction de classificateurs d'images.  La classification des images fait r√©f√©rence √† la t√¢che d'extraction des classes d'une image raster multicanal (couleur, noir et blanc).  Les perceptrons multicouches mettent beaucoup de temps √† rechercher des informations dans les images, car chaque entr√©e doit √™tre associ√©e √† chaque neurone de la couche suivante.  Le SCN les a contourn√©s en utilisant un concept appel√© connectivit√© locale.  Cela signifie que nous connecterons chaque neurone uniquement √† la r√©gion d'entr√©e locale.  Cela minimise le nombre de param√®tres, permettant √† diverses parties du r√©seau de se sp√©cialiser dans des attributs de haut niveau tels que la texture ou le motif r√©p√©titif.  Confus?  Comparons la fa√ßon dont les images sont transmises √† travers les perceptrons multicouches (MP) et les r√©seaux de neurones convolutifs. <br><br><h2>  Comparaison de MP et SNA </h2><br>  Le nombre total d'entr√©es dans la couche d'entr√©e pour le perceptron multicouche sera de 784, car l'image d'entr√©e a une taille de 28x28 = 784 (l'ensemble de donn√©es MNIST est pris en compte).  Le r√©seau doit √™tre en mesure de pr√©dire le nombre dans l'image d'entr√©e, ce qui signifie que la sortie peut appartenir √† l'une des classes suivantes dans la plage de 0 √† 9. Dans la couche de sortie, nous renvoyons des estimations de classe, par exemple, si cette entr√©e est l'image avec le num√©ro ¬´3¬ª, puis dans la couche de sortie, le neurone "3" correspondant a une valeur plus √©lev√©e que les autres neurones.  Encore une fois, la question se pose: "De combien de couches cach√©es avons-nous besoin et combien de neurones doivent √™tre dans chacune?"  Par exemple, prenez le code MP suivant: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="image"><br><br>  Le code ci-dessus est impl√©ment√© √† l'aide d'un framework appel√© Keras.  La premi√®re couche cach√©e a 512 neurones qui sont connect√©s √† la couche d'entr√©e de 784 neurones.  La prochaine couche cach√©e: la couche d'exclusion, qui r√©sout le probl√®me du recyclage.  0,2 signifie qu'il y a 20% de chances de ne pas prendre en compte les neurones de la couche cach√©e pr√©c√©dente.  Nous avons √† nouveau ajout√© une deuxi√®me couche cach√©e avec le m√™me nombre de neurones que dans la premi√®re couche cach√©e (512), puis une autre couche exclusive.  Enfin, terminer cet ensemble de couches avec une couche de sortie compos√©e de 10 classes.  La classe qui compte le plus sera le nombre pr√©vu par le mod√®le.  Voici √† quoi ressemble un r√©seau multicouche apr√®s avoir identifi√© toutes les couches.  L'un des inconv√©nients du perceptron √† plusieurs niveaux est qu'il est enti√®rement connect√©, ce qui prend beaucoup de temps et d'espace. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="image"><br><br>  Les convoltes n'utilisent pas de couches enti√®rement connect√©es.  Ils utilisent des couches clairsem√©es, qui prennent des matrices en entr√©e, ce qui donne un avantage sur MP.  Dans MP, chaque n≈ìud est responsable de la compr√©hension de l'ensemble de l'image.  Dans le SCN, nous divisons l'image en zones (petites zones locales de pixels).  La couche de sortie combine les donn√©es re√ßues de chaque n≈ìud cach√© pour trouver des mod√®les.  Voici une image de la fa√ßon dont les couches sont connect√©es. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="image"><br><br>  Voyons maintenant comment le SCN trouve des informations sur les photos.  Avant cela, nous devons comprendre comment les signes sont extraits.  Dans le SCN, nous utilisons diff√©rentes couches, chaque couche pr√©serve les signes de l'image, par exemple, elle prend en compte l'image du chien, lorsque le r√©seau a besoin de classer le chien, elle doit identifier tous les signes, tels que les yeux, les oreilles, la langue, les jambes, etc.  Ces signes sont bris√©s et reconnus au niveau du r√©seau local √† l'aide de filtres et de c≈ìurs. <br><br><h2>  Comment les ordinateurs regardent-ils une image? </h2><br>  Une personne qui regarde une image et en comprend le sens semble tr√®s raisonnable.  Disons que vous marchez et remarquez les nombreux paysages qui vous entourent.  Comment comprenons-nous la nature dans ce cas?  Nous prenons des photos de l'environnement en utilisant notre principal organe sensoriel - l'≈ìil, puis nous l'envoyons √† la r√©tine.  Tout cela semble assez int√©ressant, non?  Imaginons maintenant qu'un ordinateur fasse de m√™me.  Dans les ordinateurs, les images sont interpr√©t√©es √† l'aide d'un ensemble de valeurs de pixels allant de 0 √† 255. L'ordinateur examine ces valeurs de pixels et les comprend.  √Ä premi√®re vue, il ne conna√Æt pas les objets et les couleurs.  Il reconna√Æt simplement les valeurs de pixels et l'image est √©quivalente √† un ensemble de valeurs de pixels pour l'ordinateur.  Plus tard, en analysant les valeurs des pixels, il apprend progressivement si l'image est grise ou couleur.  Les images en niveaux de gris n'ont qu'un seul canal, car chaque pixel repr√©sente l'intensit√© d'une couleur.  0 signifie noir et 255 signifie blanc, les autres variantes du noir et blanc, c'est-√†-dire du gris, sont entre elles. <br><br>  Les images en couleur ont trois canaux, rouge, vert et bleu.  Ils repr√©sentent l'intensit√© de 3 couleurs (matrice tridimensionnelle), et lorsque les valeurs changent simultan√©ment, cela donne un large √©ventail de couleurs, vraiment une palette de couleurs!  Apr√®s cela, l'ordinateur reconna√Æt les courbes et les contours des objets dans l'image.  Tout cela peut √™tre √©tudi√© dans le r√©seau neuronal convolutif.  Pour cela, nous utiliserons PyTorch pour charger un ensemble de donn√©es et appliquer des filtres aux images.  Voici un extrait de code. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="image"><br><br>  Voyons maintenant comment une seule image est introduite dans un r√©seau neuronal. <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="image"><br><br>  C'est ainsi que le nombre ¬´3¬ª est divis√© en pixels.  √Ä partir de l'ensemble des chiffres manuscrits, ¬´3¬ª est s√©lectionn√© au hasard, dans lequel les valeurs des pixels sont affich√©es.  Ici, ToTensor () normalise les valeurs r√©elles des pixels (0‚Äì255) et les limite √† une plage de 0 √† 1. Pourquoi est-ce?  Parce qu'il facilite les calculs dans les sections suivantes, soit pour interpr√©ter des images, soit pour trouver des mod√®les communs qui y existent. <br><br><h2>  Cr√©ez votre propre filtre </h2><br>  Filtre, comme son nom l'indique, filtre les informations.  Dans le cas des r√©seaux de neurones convolutifs, lorsque vous travaillez avec des images, les informations sur les pixels sont filtr√©es.  Pourquoi devrions-nous filtrer?  N'oubliez pas qu'un ordinateur doit passer par un processus d'apprentissage pour comprendre les images, tr√®s similaire √† la fa√ßon dont un enfant le fait.  Dans ce cas, cependant, nous n'aurons pas besoin de plusieurs ann√©es!  Bref, il apprend √† partir de z√©ro puis progresse vers l'ensemble. <br><br>  Par cons√©quent, le r√©seau doit initialement conna√Ætre toutes les parties grossi√®res de l'image, √† savoir les bords, les contours et autres √©l√©ments de bas niveau.  Une fois d√©couverts, le chemin des sympt√¥mes complexes est ouvert.  Pour y acc√©der, nous devons d'abord extraire les attributs de bas niveau, puis ceux du milieu, puis ceux de niveau sup√©rieur.  Les filtres sont un moyen d'extraire les informations dont l'utilisateur a besoin, et pas seulement un transfert de donn√©es aveugle, √† cause duquel l'ordinateur ne comprend pas la structuration des images.  Au d√©but, les fonctions de bas niveau peuvent √™tre extraites en fonction d'un filtre sp√©cifique.  Le filtre ici est √©galement un ensemble de valeurs de pixels, semblable √† une image.  Il peut √™tre compris comme les poids qui relient les couches du r√©seau neuronal convolutif.  Ces poids ou filtres sont multipli√©s par des valeurs d'entr√©e pour produire des images interm√©diaires qui repr√©sentent la compr√©hension informatique de l'image.  Ensuite, ils sont multipli√©s par quelques filtres suppl√©mentaires pour √©tendre la vue.  Ensuite, il d√©tecte les organes visibles d'une personne (√† condition qu'une personne soit pr√©sente dans l'image).  Plus tard, avec l'inclusion de plusieurs filtres et plusieurs couches, l'ordinateur s'exclame: ¬´Oh, oui!  Ceci est un homme. " <br><br>  Si nous parlons de filtres, nous avons de nombreuses options.  Vous pouvez vouloir flouter l'image, puis appliquer un filtre de flou, si vous devez ajouter de la nettet√©, un filtre de nettet√© viendra √† la rescousse, etc. <br><br>  Examinons quelques extraits de code pour comprendre la fonctionnalit√© des filtres. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="image"><br><br>  Voici √† quoi ressemble l'image apr√®s avoir appliqu√© le filtre, dans ce cas, nous avons utilis√© le filtre Sobel. <br><br><h2>  R√©seaux de neurones convolutifs </h2><br>  Jusqu'√† pr√©sent, nous avons vu comment les filtres sont utilis√©s pour extraire les fonctionnalit√©s des images.  Maintenant, pour compl√©ter l'ensemble du r√©seau de neurones convolutionnels, nous devons conna√Ætre toutes les couches que nous utilisons pour le concevoir.  Les couches utilis√©es dans le SCN, <br><br><ol><li>  Couche convolutionnelle </li><li>  Couche de mise en commun </li><li>  Couche enti√®rement coll√©e </li></ol><br>  Avec les trois couches, le classificateur d'images convolutionnel ressemble √† ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="image"><br><br>  Voyons maintenant ce que fait chaque couche. <br><br>  <b>La couche convolutionnelle (CONV)</b> utilise des filtres qui effectuent des op√©rations de convolution en balayant l'image d'entr√©e.  Ses hyperparam√®tres incluent une taille de filtre, qui peut √™tre 2x2, 3x3, 4x4, 5x5 (mais sans s'y limiter) et l'√©tape S. Le r√©sultat O est appel√© une carte d'entit√©s ou une carte d'activation dans laquelle toutes les entit√©s sont calcul√©es √† l'aide de couches d'entr√©e et de filtres.  Vous trouverez ci-dessous une image de la g√©n√©ration de cartes d'entit√©s lors de l'application de la convolution, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="image"><br><br>  <b>La couche de fusion (POOL) est</b> utilis√©e pour compacter les entit√©s g√©n√©ralement utilis√©es apr√®s la couche de convolution.  Il existe deux types d'op√©rations syndicales - il s'agit de l'union maximale et moyenne, o√π les valeurs maximales et moyennes des caract√©ristiques sont prises, respectivement.  Ce qui suit est le fonctionnement des op√©rations de fusion, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="image"><br><br>  <b>Les couches enti√®rement connect√©es (FC)</b> fonctionnent avec une entr√©e plate, o√π chaque entr√©e est connect√©e √† tous les neurones.  Ils sont g√©n√©ralement utilis√©s √† la fin du r√©seau pour connecter les couches cach√©es √† la couche de sortie, ce qui permet d'optimiser les scores de classe. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="image"><br><br><h3>  Visualisation SNA dans PyTorch </h3><br>  Maintenant que nous avons l'id√©ologie compl√®te de la construction du SNA, impl√©mentons le SNA en utilisant le cadre PyTorch de Facebook. <br><br>  <b>√âtape 1</b> : T√©l√©chargez l'image d'entr√©e √† envoyer sur le r√©seau.  (Ici, nous le faisons avec Numpy et OpenCV) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="image"><br><br>  <b>√âtape 2</b> : Filtres de rendu <br><br>  Visualisons les filtres pour mieux comprendre ceux que nous utiliserons, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="image"><br><br>  <b>√âtape 3</b> : d√©terminer le SCN <br><br>  Ce SCN a une couche convolutionnelle et une couche de mise en commun avec une fonction maximale, et les poids sont initialis√©s √† l'aide des filtres montr√©s ci-dessus, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>√âtape 4</b> : Filtres de rendu <br>  Un regard rapide sur les filtres utilis√©s, <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Filtres: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="image"><br><br>  <b>√âtape 5</b> : r√©sultats filtr√©s par couche <br><br>  Les images qui apparaissent dans les couches CONV et POOL sont pr√©sent√©es ci-dessous. <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br>  Couches convolutives <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="image"><br><br>  Regroupement des couches <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="image"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436838/">https://habr.com/ru/post/fr436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436802/index.html">Widget de communication 3CX pour votre site Wordpress</a></li>
<li><a href="../fr436826/index.html">Android Robotics jusqu'en 2019: la vraie histoire; en 5 parties; partie 4</a></li>
<li><a href="../fr436828/index.html">La transition vers Boost-1.65.1 et les bogues qui ont fait surface</a></li>
<li><a href="../fr436830/index.html">Android Robotics jusqu'en 2019: la vraie histoire; en 5 parties; partie 5</a></li>
<li><a href="../fr436836/index.html">Avantages de l'analyse des applications de niveau 7 dans les pare-feu. Partie 2. S√©curit√©</a></li>
<li><a href="../fr436840/index.html">Le chemin du gloss aux neurosciences: un podcast th√©matique sur les carri√®res dans les m√©dias et le marketing de contenu</a></li>
<li><a href="../fr436842/index.html">Solution Veeam pour la sauvegarde et la restauration de machines virtuelles sur la plateforme Nutanix AHV. 2e partie</a></li>
<li><a href="../fr436846/index.html">Le condens√© de mati√®res fra√Æches du monde du front-end de la derni√®re semaine n ¬∞ 348 (14-20 janvier 2019)</a></li>
<li><a href="../fr436848/index.html">La NSA annonce la sortie d'un outil interne de r√©tro-ing√©nierie</a></li>
<li><a href="../fr436850/index.html">Erreurs courantes lors de l'√©criture des tests unitaires. Conf√©rence Yandex</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>