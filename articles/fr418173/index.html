<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë£ üßëüèø‚Äçü§ù‚Äçüßëüèø üíù Aller-retour pour les r√©seaux de neurones, ou un examen de l'utilisation des auto-encodeurs dans l'analyse de texte üåµ üñåÔ∏è üóª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous avons d√©j√† √©crit dans le tout premier article de notre blog d'entreprise sur le fonctionnement de l'algorithme de d√©tection des emprunts transf√©r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aller-retour pour les r√©seaux de neurones, ou un examen de l'utilisation des auto-encodeurs dans l'analyse de texte</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/antiplagiat/blog/418173/"> Nous avons d√©j√† √©crit dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tout premier article de notre blog d'entreprise</a> sur le fonctionnement de l'algorithme de d√©tection des emprunts transf√©rables.  Seuls quelques paragraphes de cet article sont consacr√©s au sujet de la comparaison des textes, bien que l'id√©e m√©rite une description beaucoup plus d√©taill√©e.  Cependant, comme vous le savez, on ne peut pas tout dire imm√©diatement, m√™me si on le veut vraiment.  Dans une tentative de rendre hommage √† ce sujet et √† l'architecture du r√©seau appel√© " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">auto-encoder</a> ", auquel nous avons des sentiments tr√®s chaleureux, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">Oleg_Bakhteev</a> et moi-m√™me avons r√©dig√© cette critique. <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning pour PNL (sans Magic)</a> <br><br>  Comme nous l'avons mentionn√© dans cet article, la comparaison des textes √©tait ¬´s√©mantique¬ª - nous n'avons pas compar√© les fragments de texte eux-m√™mes, mais les vecteurs qui leur correspondent.  Ces vecteurs ont √©t√© obtenus √† la suite de la formation d'un r√©seau de neurones, qui affichait un fragment de texte d'une longueur arbitraire en un vecteur de grande dimension mais fixe.  Comment obtenir une telle cartographie et comment enseigner au r√©seau √† produire les r√©sultats souhait√©s est une question distincte, qui sera discut√©e ci-dessous. <br><a name="habracut"></a><br><h1>  Qu'est-ce qu'un encodeur automatique? </h1><br>  Formellement, un r√©seau de neurones est appel√© un auto-encodeur (ou auto-encoder), qui s'entra√Æne √† restaurer les objets re√ßus √† l'entr√©e du r√©seau. <br><img src="https://habrastorage.org/webt/jy/jw/ip/jyjwipnzwzlyidenzeovey3jba4.png"><br>  L'auto-encodeur se compose de deux parties: un encodeur <b>f</b> , qui encode l'√©chantillon <b>X</b> dans sa repr√©sentation interne <b>H</b> , et un d√©codeur <b>g</b> , qui restaure l'√©chantillon d'origine.  Ainsi, l'autocodeur essaie de combiner la version restaur√©e de chaque objet exemple avec l'objet d'origine. <br><br>  Lors de la formation d'un encodeur automatique, la fonction suivante est minimis√©e: <br><img src="https://habrastorage.org/webt/9f/ay/cm/9faycmmbldgcxvehefjrurcusyq.png"><br><br>  O√π <b>r</b> repr√©sente la version restaur√©e de l'objet d'origine: <br><img src="https://habrastorage.org/webt/zf/oe/oi/zfoeoiwfxnrscvv0n5cv4keku5k.png"><br><br>  Prenons l'exemple fourni dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">blog.keras.io</a> : <br><img src="https://habrastorage.org/webt/lw/wp/i_/lwwpi_kn0wyrduexhkqyrg9jttk.png"><br>  Le r√©seau re√ßoit un objet <b>x</b> en entr√©e (dans notre cas, le num√©ro 2). <br><br>  Notre r√©seau code cet objet dans un √©tat cach√©.  Ensuite, selon l'√©tat latent, la reconstruction de l'objet <b>r est</b> restaur√©e, ce qui devrait √™tre similaire √† x.  Comme on le voit, l'image restaur√©e (√† droite) est devenue plus floue.  Cela s'explique par le fait que nous essayons de ne garder dans une vue cach√©e que les signes les plus importants de l'objet, de sorte que l'objet est restaur√© avec des pertes. <br><br>  Le mod√®le d'auto-encodeur est form√© sur le principe d'un t√©l√©phone endommag√©, o√π une personne (encodeur) transmet des informations <b>(x</b> ) √† la deuxi√®me personne (d√©codeur <b>)</b> , et lui, √† son tour, le dit √† la troisi√®me <b>(r (x))</b> . <br><br>  L'un des principaux objectifs de ces auto-encodeurs est de r√©duire la dimension de l'espace source.  Lorsque nous avons affaire √† des auto-encodeurs, la proc√©dure d'apprentissage du r√©seau de neurones elle-m√™me permet √† l'auto-encodeur de se souvenir des principales caract√©ristiques des objets √† partir desquelles il sera plus facile de restaurer les exemples d'√©chantillons d'origine. <br><br>  Nous pouvons ici faire une analogie avec la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√©thode des composantes principales</a> : il s'agit d'une m√©thode de r√©duction de la dimension, dont le r√©sultat est la projection de l'√©chantillon sur un sous-espace dans lequel la variance de cet √©chantillon est maximale. <br><br>  En effet, l'auto-encodeur est une g√©n√©ralisation de la m√©thode du composant principal: dans le cas o√π l'on se limite √† la consid√©ration de mod√®les lin√©aires, l'auto-encodeur et la m√©thode du composant principal donnent les m√™mes repr√©sentations vectorielles.  La diff√©rence survient lorsque nous consid√©rons des mod√®les plus complexes, par exemple, des r√©seaux de neurones multicouches enti√®rement connect√©s, comme codeur et d√©codeur. <br><br>  Un exemple de comparaison de la m√©thode des composants principaux et de l'auto-encodeur est pr√©sent√© dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©duire la dimensionnalit√© des donn√©es avec les r√©seaux de neurones</a> : <br><img src="https://habrastorage.org/webt/bj/ap/hq/bjaphq8tpjla39pbn2djsrm84y4.png"><br><br>  Ici, les r√©sultats de la formation de l'auto-encodeur et de la m√©thode des composants principaux pour l'√©chantillonnage d'images de visages humains sont d√©montr√©s.  La premi√®re ligne montre les visages des personnes de l'√©chantillon t√©moin, c'est-√†-dire  √† partir d'une partie sp√©cialement diff√©r√©e de l'√©chantillon qui n'a pas √©t√© utilis√©e par les algorithmes dans le processus d'apprentissage.  Sur les deuxi√®me et troisi√®me lignes se trouvent les images restaur√©es des √©tats cach√©s de l'auto-encodeur et de la m√©thode du composant principal, respectivement, de la m√™me dimension.  Ici, vous pouvez voir clairement √† quel point l‚Äôencodeur automatique a fonctionn√©. <br><br>  Dans le m√™me article, un autre exemple illustratif: comparer les r√©sultats de l'auto-encodeur et de la m√©thode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LSA</a> pour la t√¢che de r√©cup√©ration d'informations.  La m√©thode LSA, comme la m√©thode des composants principaux, est une m√©thode d'apprentissage automatique classique et est souvent utilis√©e dans les t√¢ches li√©es au traitement du langage naturel. <br><img src="https://habrastorage.org/webt/di/h5/j3/dih5j3wsflfohomgzzrjo9e6n7k.png"><br>  La figure montre une projection 2D de plusieurs documents obtenus √† l'aide de l'auto-encodeur et de la m√©thode LSA.  Les couleurs indiquent le th√®me du document.  On peut voir que la projection de l'auto-encodeur d√©compose bien les documents par sujet, tandis que le LSA produit un r√©sultat beaucoup plus bruyant. <br><br>  Une autre application importante des auto- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">encodeurs est la pr√©-formation r√©seau</a> .  La formation pr√©alable au r√©seau est utilis√©e lorsque le r√©seau optimis√© est suffisamment profond.  Dans ce cas, la formation du r√©seau ¬´√† partir de z√©ro¬ª peut √™tre assez difficile, par cons√©quent, tout le r√©seau est d'abord repr√©sent√© comme une cha√Æne d'encodeurs. <br><br>  L'algorithme de pr√©-formation est assez simple: pour chaque couche, nous formons notre propre codeur automatique, puis nous d√©finissons que la sortie du codeur suivant est simultan√©ment l'entr√©e pour la couche r√©seau suivante.  Le mod√®le r√©sultant se compose d'une cha√Æne d'encodeurs form√©s pour pr√©server avec empressement les caract√©ristiques les plus importantes des objets, chacune sur sa propre couche.  Le programme de pr√©-formation est pr√©sent√© ci-dessous: <br><img src="https://habrastorage.org/webt/yy/mc/ui/yymcuigpqgegoa_7gwndzfxulia.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">psyyz10.github.io</a> <br><br>  Cette structure est appel√©e Stacked Autoencoder et est souvent utilis√©e comme ¬´overclocking¬ª pour former davantage le mod√®le de r√©seau profond complet.  La motivation pour une telle formation d'un r√©seau de neurones est qu'un r√©seau de neurones profond est une fonction non convexe: dans le processus de formation d'un r√©seau, l'optimisation des param√®tres peut ¬´se coincer¬ª au minimum local.  Une pr√©-formation gourmande des param√®tres r√©seau vous permet de trouver un bon point de d√©part pour la formation finale et ainsi d'√©viter de tels minima locaux. <br><br>  Bien s√ªr, nous n'avons pas consid√©r√© toutes les structures possibles, car il existe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">encodeurs</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">automatiques clairsem√©s</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">automatiques de d√©bruitage</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs automatiques contractifs</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs automatiques</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconstructifs</a> .  Ils diff√®rent entre eux en utilisant diverses fonctions d'erreur et termes de p√©nalit√©.  Toutes ces architectures, √† notre avis, m√©ritent un examen s√©par√©.  Dans notre article, nous montrons tout d'abord le concept g√©n√©ral des auto-encodeurs et les t√¢ches sp√©cifiques d'analyse de texte qui sont r√©solues en l'utilisant. <br><br><h2>  Comment √ßa marche dans les textes? </h2><br>  Nous passons maintenant √† des exemples sp√©cifiques de l'utilisation d'autocodeurs pour les t√¢ches d'analyse de texte.  Nous nous int√©ressons aux deux c√¥t√©s de l'application - les deux mod√®les pour obtenir des repr√©sentations internes, et l'utilisation de ces repr√©sentations internes comme attributs, par exemple, dans le probl√®me de classification suppl√©mentaire.  Les articles sur ce sujet abordent le plus souvent des t√¢ches telles que l'analyse des sentiments ou la d√©tection de reformulation, mais il existe √©galement des travaux d√©crivant l'utilisation d'auto-encodeurs pour comparer des textes dans diff√©rentes langues ou pour la traduction automatique. <br><br>  Dans les t√¢ches d'analyse de texte, le plus souvent l'objet est la phrase, c'est-√†-dire  s√©quence ordonn√©e de mots.  Ainsi, l'auto-encodeur re√ßoit exactement cette s√©quence de mots, ou plut√¥t des repr√©sentations vectorielles de ces mots tir√©es d'un mod√®le pr√©alablement form√©.  Que sont les repr√©sentations vectorielles des mots, elle a √©t√© consid√©r√©e sur Habr√© de mani√®re suffisamment d√©taill√©e, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Ainsi, l'auto-encodeur, en prenant une s√©quence de mots comme entr√©e, doit former une repr√©sentation interne de la phrase enti√®re qui r√©ponde aux caract√©ristiques qui sont importantes pour nous, en fonction de la t√¢che.  Dans les probl√®mes d'analyse de texte, nous devons mapper des phrases √† des vecteurs afin qu'ils soient proches dans le sens d'une fonction de distance, le plus souvent une mesure de cosinus: <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning pour PNL (sans Magic)</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Richard Socher a</a> √©t√© l'un des premiers auteurs √† montrer l'utilisation r√©ussie des auto-encodeurs dans l'analyse de texte. <br><br>  Dans son article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection,</a> il d√©crit une nouvelle structure d'autocodage - Unfolding Recursive Autoencoder (Unfolding RAE) (voir la figure ci-dessous). <br><img src="https://habrastorage.org/webt/zn/va/o9/znvao90juxs6ywwmm5ywwe6nrdm.png"><br>  D√©pliage RAE <br><br>  On suppose que la structure de la phrase est d√©finie par un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">analyseur syntaxique</a> .  La structure la plus simple est consid√©r√©e - la structure d'un arbre binaire.  Un tel arbre se compose de feuilles - mots d'un fragment, de n≈ìuds internes (n≈ìuds de branche) - de phrases et d'un sommet terminal.  En prenant la s√©quence de mots (x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> ) en entr√©e (trois repr√©sentations vectorielles des mots dans cet exemple), l'auto-encodeur code s√©quentiellement, dans ce cas, de droite √† gauche, les repr√©sentations vectorielles des mots de phrase en repr√©sentations vectorielles des collocations, puis en vecteur Pr√©sentation de l'ensemble de l'offre.  Plus pr√©cis√©ment dans cet exemple, nous concat√©nons d'abord les vecteurs x <sub>2</sub> et x <sub>3</sub> , puis les multiplions par la matrice <i>W <sub>e</sub></i> ayant la dimension <i>cach√©e √ó 2visible</i> , o√π <i>cach√©</i> est l'endroit o√π la taille de la repr√©sentation interne cach√©e, <i>visible</i> est la dimension du mot vecteur.  Ainsi, nous r√©duisons la dimension, puis ajoutons la non-lin√©arit√© en utilisant la fonction tanh.  √Ä la premi√®re √©tape, nous obtenons une repr√©sentation vectorielle cach√©e pour la phrase deux mots <i>x <sub>2</sub></i> et <i>x <sub>3</sub></i> : <i>h <sub>1</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [x <sub>2</sub> , x <sub>3</sub> ] + b <sub>e</sub> )</i> .  Sur le second, nous le combinons avec le mot restant <i>h <sub>2</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [h <sub>1</sub> , x <sub>1</sub> ] + b <sub>e</sub> )</i> et obtenons une repr√©sentation vectorielle pour la phrase enti√®re - <i>h <sub>2</sub></i> .  Comme mentionn√© ci-dessus, dans la d√©finition d'un encodeur automatique, nous devons minimiser l'erreur entre les objets et leurs versions restaur√©es.  Dans notre cas, ce sont des mots.  Par cons√©quent, apr√®s avoir re√ßu la repr√©sentation vectorielle finale de la phrase enti√®re <i>h <sub>2</sub></i> , nous allons d√©coder ses versions restaur√©es (x <sub>1</sub> ', x <sub>2</sub> ', x <sub>3</sub> ').  Le d√©codeur fonctionne ici sur le m√™me principe que le codeur, seuls la matrice des param√®tres et le vecteur de d√©calage sont diff√©rents ici: <i>W <sub>d</sub></i> et <i>b <sub>d</sub></i> . <br><br>  En utilisant la structure d'un arbre binaire, vous pouvez encoder des phrases de n'importe quelle longueur en un vecteur de dimension fixe - nous combinons toujours une paire de vecteurs de la m√™me dimension, en utilisant la m√™me matrice de param√®tres <i>W <sub>e</sub></i> .  Dans le cas d'un arbre non binaire, il suffit d'initialiser les matrices √† l'avance si l'on veut combiner plus de deux mots - 3, 4, ... n, dans ce cas la matrice aura juste la dimension <i>cach√©e √ó invisible</i> . <br><br>  Il est √† noter que dans cet article, des repr√©sentations vectorielles entra√Æn√©es de phrases sont utilis√©es non seulement pour r√©soudre le probl√®me de classification - quelques phrases sont reformul√©es ou non.  Les donn√©es d'une exp√©rience sur la recherche des voisins les plus proches sont √©galement pr√©sent√©es - sur la base uniquement du vecteur d'offre re√ßu, les vecteurs les plus proches de l'√©chantillon sont recherch√©s qui lui sont proches en termes de: <br><br><img src="https://habrastorage.org/webt/d6/pv/ae/d6pvaevnd18k2ouizgs3t9k0xbk.png"><br><br>  Cependant, personne ne nous d√©range d'utiliser d'autres architectures de r√©seau pour le codage et le d√©codage pour combiner s√©quentiellement des mots en phrases. <br><br>  Voici un exemple d'un article de NIPS 2017 - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©convolutional Paragraph Representation Learning</a> : <br><img src="https://habrastorage.org/webt/g9/u7/0m/g9u70mec8rpyrbnxuqtnflyxcfo.png"><br><br>  Nous voyons que le codage de l'√©chantillon <b>X</b> dans la repr√©sentation cach√©e <b>h</b> se produit √† l'aide d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seau neuronal convolutionnel</a> , et le d√©codeur fonctionne sur le m√™me principe. <br><br>  Ou voici un exemple d'utilisation de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GRU-GRU</a> dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Skip-Thought Vectors</a> . <br><br>  Une caract√©ristique int√©ressante ici est que le mod√®le fonctionne avec des triplets de phrases: ( <i>s <sub>i-1</sub> , s <sub>i</sub> , s <sub>i + 1</sub></i> ).  La phrase <i>s <sub>i</sub></i> est cod√©e en utilisant des formules GRU standard, et le d√©codeur, en utilisant les informations de repr√©sentation interne <i>s <sub>i</sub></i> , essaie de d√©coder <i>s <sub>i-1</sub></i> et <i>s <sub>i + 1</sub></i> , √©galement en utilisant GRU. <br><br><img src="https://habrastorage.org/webt/ed/od/-r/edod-radj66y43mbsgu31zxo7nu.png"><br><br>  Le principe de fonctionnement dans ce cas ressemble au mod√®le standard de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traduction automatique de r√©seau</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">neurones</a> , qui fonctionne selon le sch√©ma codeur-d√©codeur.  Cependant, ici nous n'avons pas deux langues, nous soumettons une phrase dans une langue √† l'entr√©e de notre unit√© de codage et essayons de la restaurer.  Dans le processus d'apprentissage, il y a une minimisation de certaines fonctions de qualit√© interne (ce n'est pas toujours une erreur de reconstruction), puis, si n√©cessaire, des vecteurs pr√©-form√©s sont utilis√©s comme caract√©ristiques d'un autre probl√®me. <br><br>  Un autre article, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation</a> , pr√©sente une architecture qui jette un regard neuf sur la traduction automatique.  Premi√®rement, pour deux langues, les autocodeurs r√©cursifs sont form√©s s√©par√©ment (selon le principe d√©crit ci-dessus - o√π le d√©ploiement du RAE a √©t√© introduit).  Ensuite, entre eux, un troisi√®me auto-encodeur est form√© - une cartographie entre deux langues.  Une telle architecture pr√©sente un avantage √©vident: lors de l'affichage de textes dans diff√©rentes langues dans un espace cach√© commun, nous pouvons les comparer sans utiliser la traduction automatique comme √©tape interm√©diaire. <br><br><img src="https://habrastorage.org/webt/tj/rq/un/tjrqunxjtnbgsivzz7iohnm8ibs.png"><br><br>  La formation des auto-encodeurs sur les fragments de texte se retrouve souvent dans les articles sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation au classement</a> .  L√† encore, le fait que nous formions la fonction finale de la qualit√© du classement est important, nous pr√©-formons d'abord l'auto-encodeur pour mieux initialiser les vecteurs de requ√™tes et r√©ponses soumises √† l'entr√©e du r√©seau. <br><br><img src="https://habrastorage.org/webt/5t/a1/qn/5ta1qnx9gqzl9dypb0t4nhgcjtg.jpeg"><br><br>  Et, bien s√ªr, nous ne pouvons pas ne pas mentionner les <a href="">encodeurs automatiques variationnels</a> , ou <a href="">VAE</a> , comme mod√®les g√©n√©ratifs.  Bien s√ªr, il est pr√©f√©rable de simplement regarder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cette entr√©e de conf√©rence de Yandex</a> .  Il nous suffit de dire ce qui suit: si nous voulons <i>g√©n√©rer des</i> objets √† partir de l'espace cach√© d'un encodeur automatique conventionnel, alors la qualit√© de cette g√©n√©ration sera faible, car nous ne savons rien de la distribution de la variable cach√©e.  Mais vous pouvez imm√©diatement entra√Æner l'auto-encodeur √† g√©n√©rer, en introduisant une hypoth√®se de distribution. <br><br>  Et puis, √† l'aide de VAE, vous pouvez g√©n√©rer des textes √† partir de cet espace cach√©, par exemple, comme le font les auteurs de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">G√©n√©ration de phrases √† partir d'un espace continu</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un autoencodeur variationnel convolutionnel hybride pour la g√©n√©ration de texte</a> . <br><br>  Les propri√©t√©s g√©n√©ratives de la VAE fonctionnent √©galement bien dans les t√¢ches de comparaison de textes dans diff√©rentes langues - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Une approche de codage automatique variationnel pour induire des incorporations de mots multilingues en est</a> un excellent exemple. <br><br>  En conclusion, nous voulons faire une petite pr√©vision.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage par la repr√©sentation</a> - la formation aux repr√©sentations internes utilisant exactement la VAE, en particulier en conjonction avec les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seaux adversaires g√©n√©ratifs</a> , est l'une des approches les plus d√©velopp√©es ces derni√®res ann√©es - cela peut √™tre jug√© par au moins les sujets d'articles les plus courants lors des derni√®res <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conf√©rences d'</a> apprentissage machine de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICLR 2018</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICML 2018</a> .  C'est tout √† fait logique - car son utilisation a contribu√© √† am√©liorer la qualit√© dans un certain nombre de t√¢ches, et pas seulement en rapport avec les textes.  Mais c'est le sujet d'une revue compl√®tement diff√©rente ... </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr418173/">https://habr.com/ru/post/fr418173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr418163/index.html">Tests de production: Netflix Chaos Automation Platform</a></li>
<li><a href="../fr418165/index.html">Quasar, Sobaken et Vermin: r√©v√©ler les d√©tails de la campagne de cyberespionnage en cours</a></li>
<li><a href="../fr418167/index.html">ScadaPy: ajouter le protocole IEC 60870-5-104</a></li>
<li><a href="../fr418169/index.html">Quoi de neuf dans Veeam Availability Console 2.0 Update 1?</a></li>
<li><a href="../fr418171/index.html">Sur quelles mesures s'appuyer si les utilisateurs effectuent peu de conversions sur le site?</a></li>
<li><a href="../fr418177/index.html">√âdition d'images .heic sans perte de couleur</a></li>
<li><a href="../fr418183/index.html">Application de l'analyse de la parole en entreprise</a></li>
<li><a href="../fr418185/index.html">Une histoire d'autopsie: comment nous avons invers√© Hancitor</a></li>
<li><a href="../fr418187/index.html">En Am√©rique, ils ont sugg√©r√© de remplacer toutes les biblioth√®ques par des hubs Amazon. Le public s'indigne</a></li>
<li><a href="../fr418189/index.html">H√©ritier de Zeus: pourquoi le cheval de Troie IcedID est dangereux pour les clients des banques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>