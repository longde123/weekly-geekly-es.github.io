<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👣 🧑🏿‍🤝‍🧑🏿 💝 Aller-retour pour les réseaux de neurones, ou un examen de l'utilisation des auto-encodeurs dans l'analyse de texte 🌵 🖌️ 🗻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous avons déjà écrit dans le tout premier article de notre blog d'entreprise sur le fonctionnement de l'algorithme de détection des emprunts transfér...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aller-retour pour les réseaux de neurones, ou un examen de l'utilisation des auto-encodeurs dans l'analyse de texte</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/antiplagiat/blog/418173/"> Nous avons déjà écrit dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tout premier article de notre blog d'entreprise</a> sur le fonctionnement de l'algorithme de détection des emprunts transférables.  Seuls quelques paragraphes de cet article sont consacrés au sujet de la comparaison des textes, bien que l'idée mérite une description beaucoup plus détaillée.  Cependant, comme vous le savez, on ne peut pas tout dire immédiatement, même si on le veut vraiment.  Dans une tentative de rendre hommage à ce sujet et à l'architecture du réseau appelé " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">auto-encoder</a> ", auquel nous avons des sentiments très chaleureux, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">Oleg_Bakhteev</a> et moi-même avons rédigé cette critique. <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning pour PNL (sans Magic)</a> <br><br>  Comme nous l'avons mentionné dans cet article, la comparaison des textes était «sémantique» - nous n'avons pas comparé les fragments de texte eux-mêmes, mais les vecteurs qui leur correspondent.  Ces vecteurs ont été obtenus à la suite de la formation d'un réseau de neurones, qui affichait un fragment de texte d'une longueur arbitraire en un vecteur de grande dimension mais fixe.  Comment obtenir une telle cartographie et comment enseigner au réseau à produire les résultats souhaités est une question distincte, qui sera discutée ci-dessous. <br><a name="habracut"></a><br><h1>  Qu'est-ce qu'un encodeur automatique? </h1><br>  Formellement, un réseau de neurones est appelé un auto-encodeur (ou auto-encoder), qui s'entraîne à restaurer les objets reçus à l'entrée du réseau. <br><img src="https://habrastorage.org/webt/jy/jw/ip/jyjwipnzwzlyidenzeovey3jba4.png"><br>  L'auto-encodeur se compose de deux parties: un encodeur <b>f</b> , qui encode l'échantillon <b>X</b> dans sa représentation interne <b>H</b> , et un décodeur <b>g</b> , qui restaure l'échantillon d'origine.  Ainsi, l'autocodeur essaie de combiner la version restaurée de chaque objet exemple avec l'objet d'origine. <br><br>  Lors de la formation d'un encodeur automatique, la fonction suivante est minimisée: <br><img src="https://habrastorage.org/webt/9f/ay/cm/9faycmmbldgcxvehefjrurcusyq.png"><br><br>  Où <b>r</b> représente la version restaurée de l'objet d'origine: <br><img src="https://habrastorage.org/webt/zf/oe/oi/zfoeoiwfxnrscvv0n5cv4keku5k.png"><br><br>  Prenons l'exemple fourni dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">blog.keras.io</a> : <br><img src="https://habrastorage.org/webt/lw/wp/i_/lwwpi_kn0wyrduexhkqyrg9jttk.png"><br>  Le réseau reçoit un objet <b>x</b> en entrée (dans notre cas, le numéro 2). <br><br>  Notre réseau code cet objet dans un état caché.  Ensuite, selon l'état latent, la reconstruction de l'objet <b>r est</b> restaurée, ce qui devrait être similaire à x.  Comme on le voit, l'image restaurée (à droite) est devenue plus floue.  Cela s'explique par le fait que nous essayons de ne garder dans une vue cachée que les signes les plus importants de l'objet, de sorte que l'objet est restauré avec des pertes. <br><br>  Le modèle d'auto-encodeur est formé sur le principe d'un téléphone endommagé, où une personne (encodeur) transmet des informations <b>(x</b> ) à la deuxième personne (décodeur <b>)</b> , et lui, à son tour, le dit à la troisième <b>(r (x))</b> . <br><br>  L'un des principaux objectifs de ces auto-encodeurs est de réduire la dimension de l'espace source.  Lorsque nous avons affaire à des auto-encodeurs, la procédure d'apprentissage du réseau de neurones elle-même permet à l'auto-encodeur de se souvenir des principales caractéristiques des objets à partir desquelles il sera plus facile de restaurer les exemples d'échantillons d'origine. <br><br>  Nous pouvons ici faire une analogie avec la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">méthode des composantes principales</a> : il s'agit d'une méthode de réduction de la dimension, dont le résultat est la projection de l'échantillon sur un sous-espace dans lequel la variance de cet échantillon est maximale. <br><br>  En effet, l'auto-encodeur est une généralisation de la méthode du composant principal: dans le cas où l'on se limite à la considération de modèles linéaires, l'auto-encodeur et la méthode du composant principal donnent les mêmes représentations vectorielles.  La différence survient lorsque nous considérons des modèles plus complexes, par exemple, des réseaux de neurones multicouches entièrement connectés, comme codeur et décodeur. <br><br>  Un exemple de comparaison de la méthode des composants principaux et de l'auto-encodeur est présenté dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Réduire la dimensionnalité des données avec les réseaux de neurones</a> : <br><img src="https://habrastorage.org/webt/bj/ap/hq/bjaphq8tpjla39pbn2djsrm84y4.png"><br><br>  Ici, les résultats de la formation de l'auto-encodeur et de la méthode des composants principaux pour l'échantillonnage d'images de visages humains sont démontrés.  La première ligne montre les visages des personnes de l'échantillon témoin, c'est-à-dire  à partir d'une partie spécialement différée de l'échantillon qui n'a pas été utilisée par les algorithmes dans le processus d'apprentissage.  Sur les deuxième et troisième lignes se trouvent les images restaurées des états cachés de l'auto-encodeur et de la méthode du composant principal, respectivement, de la même dimension.  Ici, vous pouvez voir clairement à quel point l’encodeur automatique a fonctionné. <br><br>  Dans le même article, un autre exemple illustratif: comparer les résultats de l'auto-encodeur et de la méthode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LSA</a> pour la tâche de récupération d'informations.  La méthode LSA, comme la méthode des composants principaux, est une méthode d'apprentissage automatique classique et est souvent utilisée dans les tâches liées au traitement du langage naturel. <br><img src="https://habrastorage.org/webt/di/h5/j3/dih5j3wsflfohomgzzrjo9e6n7k.png"><br>  La figure montre une projection 2D de plusieurs documents obtenus à l'aide de l'auto-encodeur et de la méthode LSA.  Les couleurs indiquent le thème du document.  On peut voir que la projection de l'auto-encodeur décompose bien les documents par sujet, tandis que le LSA produit un résultat beaucoup plus bruyant. <br><br>  Une autre application importante des auto- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">encodeurs est la pré-formation réseau</a> .  La formation préalable au réseau est utilisée lorsque le réseau optimisé est suffisamment profond.  Dans ce cas, la formation du réseau «à partir de zéro» peut être assez difficile, par conséquent, tout le réseau est d'abord représenté comme une chaîne d'encodeurs. <br><br>  L'algorithme de pré-formation est assez simple: pour chaque couche, nous formons notre propre codeur automatique, puis nous définissons que la sortie du codeur suivant est simultanément l'entrée pour la couche réseau suivante.  Le modèle résultant se compose d'une chaîne d'encodeurs formés pour préserver avec empressement les caractéristiques les plus importantes des objets, chacune sur sa propre couche.  Le programme de pré-formation est présenté ci-dessous: <br><img src="https://habrastorage.org/webt/yy/mc/ui/yymcuigpqgegoa_7gwndzfxulia.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">psyyz10.github.io</a> <br><br>  Cette structure est appelée Stacked Autoencoder et est souvent utilisée comme «overclocking» pour former davantage le modèle de réseau profond complet.  La motivation pour une telle formation d'un réseau de neurones est qu'un réseau de neurones profond est une fonction non convexe: dans le processus de formation d'un réseau, l'optimisation des paramètres peut «se coincer» au minimum local.  Une pré-formation gourmande des paramètres réseau vous permet de trouver un bon point de départ pour la formation finale et ainsi d'éviter de tels minima locaux. <br><br>  Bien sûr, nous n'avons pas considéré toutes les structures possibles, car il existe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">encodeurs</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">automatiques clairsemés</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">automatiques de débruitage</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs automatiques contractifs</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des encodeurs automatiques</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconstructifs</a> .  Ils diffèrent entre eux en utilisant diverses fonctions d'erreur et termes de pénalité.  Toutes ces architectures, à notre avis, méritent un examen séparé.  Dans notre article, nous montrons tout d'abord le concept général des auto-encodeurs et les tâches spécifiques d'analyse de texte qui sont résolues en l'utilisant. <br><br><h2>  Comment ça marche dans les textes? </h2><br>  Nous passons maintenant à des exemples spécifiques de l'utilisation d'autocodeurs pour les tâches d'analyse de texte.  Nous nous intéressons aux deux côtés de l'application - les deux modèles pour obtenir des représentations internes, et l'utilisation de ces représentations internes comme attributs, par exemple, dans le problème de classification supplémentaire.  Les articles sur ce sujet abordent le plus souvent des tâches telles que l'analyse des sentiments ou la détection de reformulation, mais il existe également des travaux décrivant l'utilisation d'auto-encodeurs pour comparer des textes dans différentes langues ou pour la traduction automatique. <br><br>  Dans les tâches d'analyse de texte, le plus souvent l'objet est la phrase, c'est-à-dire  séquence ordonnée de mots.  Ainsi, l'auto-encodeur reçoit exactement cette séquence de mots, ou plutôt des représentations vectorielles de ces mots tirées d'un modèle préalablement formé.  Que sont les représentations vectorielles des mots, elle a été considérée sur Habré de manière suffisamment détaillée, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Ainsi, l'auto-encodeur, en prenant une séquence de mots comme entrée, doit former une représentation interne de la phrase entière qui réponde aux caractéristiques qui sont importantes pour nous, en fonction de la tâche.  Dans les problèmes d'analyse de texte, nous devons mapper des phrases à des vecteurs afin qu'ils soient proches dans le sens d'une fonction de distance, le plus souvent une mesure de cosinus: <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning pour PNL (sans Magic)</a> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Richard Socher a</a> été l'un des premiers auteurs à montrer l'utilisation réussie des auto-encodeurs dans l'analyse de texte. <br><br>  Dans son article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection,</a> il décrit une nouvelle structure d'autocodage - Unfolding Recursive Autoencoder (Unfolding RAE) (voir la figure ci-dessous). <br><img src="https://habrastorage.org/webt/zn/va/o9/znvao90juxs6ywwmm5ywwe6nrdm.png"><br>  Dépliage RAE <br><br>  On suppose que la structure de la phrase est définie par un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">analyseur syntaxique</a> .  La structure la plus simple est considérée - la structure d'un arbre binaire.  Un tel arbre se compose de feuilles - mots d'un fragment, de nœuds internes (nœuds de branche) - de phrases et d'un sommet terminal.  En prenant la séquence de mots (x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> ) en entrée (trois représentations vectorielles des mots dans cet exemple), l'auto-encodeur code séquentiellement, dans ce cas, de droite à gauche, les représentations vectorielles des mots de phrase en représentations vectorielles des collocations, puis en vecteur Présentation de l'ensemble de l'offre.  Plus précisément dans cet exemple, nous concaténons d'abord les vecteurs x <sub>2</sub> et x <sub>3</sub> , puis les multiplions par la matrice <i>W <sub>e</sub></i> ayant la dimension <i>cachée × 2visible</i> , où <i>caché</i> est l'endroit où la taille de la représentation interne cachée, <i>visible</i> est la dimension du mot vecteur.  Ainsi, nous réduisons la dimension, puis ajoutons la non-linéarité en utilisant la fonction tanh.  À la première étape, nous obtenons une représentation vectorielle cachée pour la phrase deux mots <i>x <sub>2</sub></i> et <i>x <sub>3</sub></i> : <i>h <sub>1</sub></i> = <i>tanh⁡ (W <sub>e</sub> [x <sub>2</sub> , x <sub>3</sub> ] + b <sub>e</sub> )</i> .  Sur le second, nous le combinons avec le mot restant <i>h <sub>2</sub></i> = <i>tanh⁡ (W <sub>e</sub> [h <sub>1</sub> , x <sub>1</sub> ] + b <sub>e</sub> )</i> et obtenons une représentation vectorielle pour la phrase entière - <i>h <sub>2</sub></i> .  Comme mentionné ci-dessus, dans la définition d'un encodeur automatique, nous devons minimiser l'erreur entre les objets et leurs versions restaurées.  Dans notre cas, ce sont des mots.  Par conséquent, après avoir reçu la représentation vectorielle finale de la phrase entière <i>h <sub>2</sub></i> , nous allons décoder ses versions restaurées (x <sub>1</sub> ', x <sub>2</sub> ', x <sub>3</sub> ').  Le décodeur fonctionne ici sur le même principe que le codeur, seuls la matrice des paramètres et le vecteur de décalage sont différents ici: <i>W <sub>d</sub></i> et <i>b <sub>d</sub></i> . <br><br>  En utilisant la structure d'un arbre binaire, vous pouvez encoder des phrases de n'importe quelle longueur en un vecteur de dimension fixe - nous combinons toujours une paire de vecteurs de la même dimension, en utilisant la même matrice de paramètres <i>W <sub>e</sub></i> .  Dans le cas d'un arbre non binaire, il suffit d'initialiser les matrices à l'avance si l'on veut combiner plus de deux mots - 3, 4, ... n, dans ce cas la matrice aura juste la dimension <i>cachée × invisible</i> . <br><br>  Il est à noter que dans cet article, des représentations vectorielles entraînées de phrases sont utilisées non seulement pour résoudre le problème de classification - quelques phrases sont reformulées ou non.  Les données d'une expérience sur la recherche des voisins les plus proches sont également présentées - sur la base uniquement du vecteur d'offre reçu, les vecteurs les plus proches de l'échantillon sont recherchés qui lui sont proches en termes de: <br><br><img src="https://habrastorage.org/webt/d6/pv/ae/d6pvaevnd18k2ouizgs3t9k0xbk.png"><br><br>  Cependant, personne ne nous dérange d'utiliser d'autres architectures de réseau pour le codage et le décodage pour combiner séquentiellement des mots en phrases. <br><br>  Voici un exemple d'un article de NIPS 2017 - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Déconvolutional Paragraph Representation Learning</a> : <br><img src="https://habrastorage.org/webt/g9/u7/0m/g9u70mec8rpyrbnxuqtnflyxcfo.png"><br><br>  Nous voyons que le codage de l'échantillon <b>X</b> dans la représentation cachée <b>h</b> se produit à l'aide d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">réseau neuronal convolutionnel</a> , et le décodeur fonctionne sur le même principe. <br><br>  Ou voici un exemple d'utilisation de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GRU-GRU</a> dans l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Skip-Thought Vectors</a> . <br><br>  Une caractéristique intéressante ici est que le modèle fonctionne avec des triplets de phrases: ( <i>s <sub>i-1</sub> , s <sub>i</sub> , s <sub>i + 1</sub></i> ).  La phrase <i>s <sub>i</sub></i> est codée en utilisant des formules GRU standard, et le décodeur, en utilisant les informations de représentation interne <i>s <sub>i</sub></i> , essaie de décoder <i>s <sub>i-1</sub></i> et <i>s <sub>i + 1</sub></i> , également en utilisant GRU. <br><br><img src="https://habrastorage.org/webt/ed/od/-r/edod-radj66y43mbsgu31zxo7nu.png"><br><br>  Le principe de fonctionnement dans ce cas ressemble au modèle standard de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traduction automatique de réseau</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">neurones</a> , qui fonctionne selon le schéma codeur-décodeur.  Cependant, ici nous n'avons pas deux langues, nous soumettons une phrase dans une langue à l'entrée de notre unité de codage et essayons de la restaurer.  Dans le processus d'apprentissage, il y a une minimisation de certaines fonctions de qualité interne (ce n'est pas toujours une erreur de reconstruction), puis, si nécessaire, des vecteurs pré-formés sont utilisés comme caractéristiques d'un autre problème. <br><br>  Un autre article, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation</a> , présente une architecture qui jette un regard neuf sur la traduction automatique.  Premièrement, pour deux langues, les autocodeurs récursifs sont formés séparément (selon le principe décrit ci-dessus - où le déploiement du RAE a été introduit).  Ensuite, entre eux, un troisième auto-encodeur est formé - une cartographie entre deux langues.  Une telle architecture présente un avantage évident: lors de l'affichage de textes dans différentes langues dans un espace caché commun, nous pouvons les comparer sans utiliser la traduction automatique comme étape intermédiaire. <br><br><img src="https://habrastorage.org/webt/tj/rq/un/tjrqunxjtnbgsivzz7iohnm8ibs.png"><br><br>  La formation des auto-encodeurs sur les fragments de texte se retrouve souvent dans les articles sur la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation au classement</a> .  Là encore, le fait que nous formions la fonction finale de la qualité du classement est important, nous pré-formons d'abord l'auto-encodeur pour mieux initialiser les vecteurs de requêtes et réponses soumises à l'entrée du réseau. <br><br><img src="https://habrastorage.org/webt/5t/a1/qn/5ta1qnx9gqzl9dypb0t4nhgcjtg.jpeg"><br><br>  Et, bien sûr, nous ne pouvons pas ne pas mentionner les <a href="">encodeurs automatiques variationnels</a> , ou <a href="">VAE</a> , comme modèles génératifs.  Bien sûr, il est préférable de simplement regarder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cette entrée de conférence de Yandex</a> .  Il nous suffit de dire ce qui suit: si nous voulons <i>générer des</i> objets à partir de l'espace caché d'un encodeur automatique conventionnel, alors la qualité de cette génération sera faible, car nous ne savons rien de la distribution de la variable cachée.  Mais vous pouvez immédiatement entraîner l'auto-encodeur à générer, en introduisant une hypothèse de distribution. <br><br>  Et puis, à l'aide de VAE, vous pouvez générer des textes à partir de cet espace caché, par exemple, comme le font les auteurs de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Génération de phrases à partir d'un espace continu</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un autoencodeur variationnel convolutionnel hybride pour la génération de texte</a> . <br><br>  Les propriétés génératives de la VAE fonctionnent également bien dans les tâches de comparaison de textes dans différentes langues - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Une approche de codage automatique variationnel pour induire des incorporations de mots multilingues en est</a> un excellent exemple. <br><br>  En conclusion, nous voulons faire une petite prévision.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage par la représentation</a> - la formation aux représentations internes utilisant exactement la VAE, en particulier en conjonction avec les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">réseaux adversaires génératifs</a> , est l'une des approches les plus développées ces dernières années - cela peut être jugé par au moins les sujets d'articles les plus courants lors des dernières <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conférences d'</a> apprentissage machine de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICLR 2018</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ICML 2018</a> .  C'est tout à fait logique - car son utilisation a contribué à améliorer la qualité dans un certain nombre de tâches, et pas seulement en rapport avec les textes.  Mais c'est le sujet d'une revue complètement différente ... </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr418173/">https://habr.com/ru/post/fr418173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr418163/index.html">Tests de production: Netflix Chaos Automation Platform</a></li>
<li><a href="../fr418165/index.html">Quasar, Sobaken et Vermin: révéler les détails de la campagne de cyberespionnage en cours</a></li>
<li><a href="../fr418167/index.html">ScadaPy: ajouter le protocole IEC 60870-5-104</a></li>
<li><a href="../fr418169/index.html">Quoi de neuf dans Veeam Availability Console 2.0 Update 1?</a></li>
<li><a href="../fr418171/index.html">Sur quelles mesures s'appuyer si les utilisateurs effectuent peu de conversions sur le site?</a></li>
<li><a href="../fr418177/index.html">Édition d'images .heic sans perte de couleur</a></li>
<li><a href="../fr418183/index.html">Application de l'analyse de la parole en entreprise</a></li>
<li><a href="../fr418185/index.html">Une histoire d'autopsie: comment nous avons inversé Hancitor</a></li>
<li><a href="../fr418187/index.html">En Amérique, ils ont suggéré de remplacer toutes les bibliothèques par des hubs Amazon. Le public s'indigne</a></li>
<li><a href="../fr418189/index.html">Héritier de Zeus: pourquoi le cheval de Troie IcedID est dangereux pour les clients des banques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>