<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì† üëÅÔ∏è üë©üèø‚Äçüöí Como os servi√ßos de banco de dados gerenciados s√£o organizados no Yandex. üöÖ üéå ‚≠êÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quando voc√™ confia em algu√©m a coisa mais preciosa que voc√™ tem - os dados do seu aplicativo ou servi√ßo - voc√™ quer imaginar como esse algu√©m lidar√° c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como os servi√ßos de banco de dados gerenciados s√£o organizados no Yandex.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/477860/">  Quando voc√™ confia em algu√©m a coisa mais preciosa que voc√™ tem - os dados do seu aplicativo ou servi√ßo - voc√™ quer imaginar como esse algu√©m lidar√° com o seu maior valor. <br><br>  Meu nome √© Vladimir Borodin, sou o chefe da plataforma de dados Yandex.Cloud.  Hoje, quero contar como tudo est√° organizado e funciona nos servi√ßos do Yandex Managed Databases, por que tudo √© feito dessa maneira e quais s√£o as vantagens - do ponto de vista dos usu√°rios - de nossas v√°rias solu√ß√µes.  E, √© claro, voc√™ definitivamente descobrir√° o que planejamos finalizar no futuro pr√≥ximo, para que o servi√ßo se torne melhor e mais conveniente para todos que precisam. <br><br>  Bem, vamos l√°! <br><br><img src="https://habrastorage.org/webt/xw/qq/id/xwqqid27hmguyyigukvb2z3o8da.png" alt="imagem"><br><a name="habracut"></a><br>  Bancos de dados gerenciados (bancos de dados gerenciados Yandex) √© um dos servi√ßos Yandex.Cloud mais populares.  Mais precisamente, este √© um grupo inteiro de servi√ßos, que agora perde apenas para as m√°quinas virtuais Yandex Compute Cloud em popularidade. <br><br>  O Yandex Managed Databases permite obter rapidamente um banco de dados funcional e executa essas tarefas: <br><br><ul><li>  Dimensionamento - da capacidade elementar de adicionar recursos de computa√ß√£o ou espa√ßo em disco a um aumento no n√∫mero de r√©plicas e shards. </li><li>  Instale atualiza√ß√µes, menores e maiores. </li><li>  Backup e restaura√ß√£o. </li><li>  Fornecendo toler√¢ncia a falhas. </li><li>  Monitoramento </li><li>  Fornecendo ferramentas convenientes de configura√ß√£o e gerenciamento. </li></ul><br><h2>  Como os servi√ßos de banco de dados gerenciados s√£o organizados: vista superior </h2><br>  O servi√ßo consiste em duas partes principais: plano de controle e plano de dados.  O Control Plane √©, simplesmente, uma API de gerenciamento de banco de dados que permite criar, modificar ou excluir bancos de dados.  Data Plane √© o n√≠vel de armazenamento direto de dados. <br><br><img src="https://habrastorage.org/webt/vo/4c/o-/vo4co-q4zc3r6xwe0is2pl9hrri.png" alt="imagem"><br><br>  Os usu√°rios do servi√ßo t√™m, de fato, dois pontos de entrada: <br><br><ul><li>  No plano de controle.  De fato, existem muitas entradas - o console da Web, o utilit√°rio CLI e a API do gateway que fornece a API p√∫blica (gRPC e REST).  Mas todos eles acabam indo para o que chamamos de API interna e, portanto, consideraremos esse ponto de entrada no Plano de controle.  De fato, esse √© o ponto em que a √°rea de responsabilidade de servi√ßo do Managed Databases (MDB) come√ßa. </li><li> No plano de dados.  Esta √© uma conex√£o direta com um banco de dados em execu√ß√£o atrav√©s de protocolos de acesso ao DBMS.  Se for, por exemplo, PostgreSQL, ser√° <a href="https://www.postgresql.org/docs/current/libpq.html">a interface libpq</a> . </li></ul><br><img src="https://habrastorage.org/webt/cr/l5/pq/crl5pqrbr2mqqlun6ivgwlvn0u8.png" alt="imagem"><br>  Abaixo, descreveremos com mais detalhes tudo o que acontece no Plano de Dados e analisaremos cada um dos componentes do Plano de Controle. <br><br><h2>  Plano de dados </h2><br>  Antes de analisar os componentes do plano de controle, vamos dar uma olhada no que est√° acontecendo no plano de dados. <br><br><h3>  Dentro de uma m√°quina virtual </h3><br>  O MDB executa bancos de dados nas mesmas m√°quinas virtuais fornecidas no <a href="https://cloud.yandex.ru/services/compute">Yandex Compute Cloud</a> . <br><br><img src="https://habrastorage.org/webt/-w/gm/qj/-wgmqjlxi30m4jz7dgp4sct86cw.png" alt="imagem"><br><br>  Primeiro, um mecanismo de banco de dados, por exemplo, PostgreSQL, √© implantado l√°.  Paralelamente, v√°rios programas auxiliares podem ser lan√ßados.  Para o PostgreSQL, ser√° o <a href="https://github.com/yandex/odyssey">Odyssey</a> , o extrator de conex√£o com o banco de dados. <br><br>  Tamb√©m dentro da m√°quina virtual, um determinado conjunto de servi√ßos padr√£o √© iniciado, pr√≥prio para cada DBMS: <br><br><ul><li>  Servi√ßo para criar backups.  Para o PostgreSQL, √© uma ferramenta <a href="https://github.com/wal-g/wal-g">WAL-G de</a> c√≥digo aberto <a href="https://github.com/wal-g/wal-g">.</a>  Ele cria backups e os armazena no <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> . </li><li>  O Salt Minion √© um componente do sistema <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> para gerenciamento de opera√ß√µes e configura√ß√µes.  Mais informa√ß√µes sobre isso s√£o fornecidas abaixo na descri√ß√£o da infraestrutura Deploy. </li><li>  M√©tricas do MDB, respons√°veis ‚Äã‚Äãpor transmitir as m√©tricas do banco de dados ao <a href="https://cloud.yandex.ru/services/monitoring">Yandex Monitoring</a> e ao nosso microsservi√ßo para monitorar o status dos clusters e hosts do MDB Health. </li><li>  O cliente push, que envia logs de DBMS e de cobran√ßa ao servi√ßo Logbroker, √© uma solu√ß√£o especial para coletar e entregar dados. </li><li>  MDB cron - nossa bicicleta, que difere do cron habitual na capacidade de executar tarefas peri√≥dicas com precis√£o de um segundo. </li></ul><br><h3>  Topologia de rede </h3><br><img src="https://habrastorage.org/webt/x8/_6/ge/x8_6ge0uluxqju22unryyrxxhxw.png" alt="imagem"><br><br>  Cada host do Data Plane possui duas interfaces de rede: <br><br><ul><li>  Um deles adere √† rede do usu√°rio.  Em geral, √© necess√°rio atender √† carga do produto.  Por meio dele, a replica√ß√£o est√° perseguindo. </li><li>  O segundo fica em uma de nossas redes gerenciadas atrav√©s da qual os hosts acessam o Control Plane. </li></ul><br>  Sim, hosts de clientes diferentes est√£o presos em uma dessas redes gerenciadas, mas isso n√£o √© assustador, porque na interface gerenciada (quase) nada est√° escutando, as conex√µes de rede de sa√≠da no Control Plane s√£o abertas apenas a partir dela.  Quase ningu√©m, porque h√° portas abertas (por exemplo, SSH), mas elas s√£o fechadas por um firewall local que permite apenas conex√µes de hosts espec√≠ficos.  Assim, se um invasor obt√©m acesso a uma m√°quina virtual com um banco de dados, ele n√£o pode acessar os bancos de dados de outras pessoas. <br><br><h3>  Seguran√ßa do plano de dados </h3><br>  Como estamos falando de seguran√ßa, deve-se dizer que inicialmente projetamos o servi√ßo com base no invasor obtendo root na m√°quina virtual do cluster. <br><br>  No final, nos esfor√ßamos muito para fazer o seguinte: <br><br><ul><li>  Firewall local e grande; </li><li>  Criptografia de todas as conex√µes e backups; </li><li>  Tudo com autentica√ß√£o e autoriza√ß√£o; </li><li>  AppArmor </li><li>  IDS auto-escrito. </li></ul><br>  Agora considere os componentes do plano de controle. <br><br><h2>  Plano de controle </h2><br><h3>  API interna </h3><br>  A API interna √© o primeiro ponto de entrada no plano de controle.  Vamos ver como tudo funciona aqui. <br><br><img src="https://habrastorage.org/webt/vb/ru/j7/vbruj7qxxu2tmlaplki2dt_74cy.png" alt="imagem"><br><br>  Suponha que a API interna receba uma solicita√ß√£o para criar um cluster de banco de dados. <br><br>  Primeiro, a API interna acessa o servi√ßo de acesso √† nuvem, que √© respons√°vel por verificar a autentica√ß√£o e autoriza√ß√£o do usu√°rio.  Se o usu√°rio passar na verifica√ß√£o, a API interna verificar√° a validade da solicita√ß√£o em si.  Por exemplo, uma solicita√ß√£o para criar um cluster sem especificar seu nome ou com um nome j√° utilizado falhar√° no teste. <br><blockquote>  E a API interna pode enviar solicita√ß√µes para a API de outros servi√ßos.  Se voc√™ deseja criar um cluster em uma determinada rede A e um host espec√≠fico em uma sub-rede espec√≠fica B, a API interna deve garantir que voc√™ tenha direitos para a rede A e a sub-rede especificada B. Ao mesmo tempo, verificar√° se a sub-rede B pertence √† rede A Isso requer acesso √† API de infraestrutura. </blockquote><br>  Se a solicita√ß√£o for v√°lida, as informa√ß√µes sobre o cluster criado ser√£o salvas na metabase.  N√≥s o chamamos de MetaDB, √© implantado no PostgreSQL.  O MetaDB possui uma tabela com uma fila de opera√ß√µes.  A API interna salva informa√ß√µes sobre a opera√ß√£o e define a tarefa de maneira transacional.  Depois disso, as informa√ß√µes sobre a opera√ß√£o s√£o retornadas ao usu√°rio. <br><br>  Em geral, para processar a maioria das solicita√ß√µes da API Interna, basta usar o MetaDB e a API de servi√ßos relacionados.  Mas h√° mais dois componentes que a API interna utiliza para responder a algumas consultas - LogsDB, onde est√£o localizados os logs do cluster de usu√°rios, e MDB Health.  Sobre cada um deles ser√° descrito em mais detalhes abaixo. <br><br><h3>  Trabalhador </h3><br>  Trabalhadores s√£o simplesmente um conjunto de processos que consultam a fila de opera√ß√µes no MetaDB, as agarram e as executam. <br><br><img src="https://habrastorage.org/webt/ez/u7/q8/ezu7q86g82hjjt2nqrdssqhxpn0.png" alt="imagem"><br><br>  O que exatamente um trabalhador faz quando um cluster √© criado?  Primeiro, ele recorre √† API de infraestrutura para criar m√°quinas virtuais a partir de nossas imagens (elas j√° possuem todos os pacotes necess√°rios instalados e quase tudo est√° configurado, as imagens s√£o atualizadas uma vez por dia).  Quando as m√°quinas virtuais s√£o criadas e a rede decola nelas, o trabalhador recorre √† infra-estrutura Deploy (falaremos mais sobre isso mais adiante) para implantar o que o usu√°rio precisa nas m√°quinas virtuais. <br><br>  Al√©m disso, o trabalhador acessa outros servi√ßos em nuvem.  Por exemplo, no <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> para criar um bucket no qual os backups de cluster ser√£o salvos.  Para o servi√ßo de <a href="https://cloud.yandex.ru/services/monitoring">Monitoramento Yandex</a> , que coletar√° e visualizar√° as m√©tricas do banco de dados.  O trabalhador deve criar l√° as meta-informa√ß√µes do cluster.  Para a API DNS, se o usu√°rio desejar atribuir endere√ßos IP p√∫blicos aos hosts do cluster. <br><br>  Em geral, o trabalhador trabalha de maneira muito simples.  Ele recebe a tarefa da fila da metabase e acessa o servi√ßo desejado.  Ap√≥s concluir cada etapa, o trabalhador armazena informa√ß√µes sobre o andamento da opera√ß√£o na metabase.  Se ocorrer uma falha, a tarefa simplesmente reiniciar√° e ser√° executada de onde parou.  Mas nem mesmo reinici√°-lo desde o in√≠cio n√£o √© um problema, porque quase todos os tipos de tarefas para trabalhadores s√£o gravados de maneira idempotente.  Isso ocorre porque o trabalhador pode executar uma ou outra etapa da opera√ß√£o, mas n√£o h√° informa√ß√µes sobre isso no MetaDB. <br><br><h3>  Implantar infraestrutura </h3><br>  No fundo, est√° o <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> , um sistema de gerenciamento de configura√ß√£o de c√≥digo aberto bastante comum, escrito em Python.  O sistema √© muito <a href="https://docs.saltstack.com/en/latest/ref/index.html">expans√≠vel</a> , pelo qual n√≥s o amamos. <br><br>  Os principais componentes do salt s√£o salt master, que armazena informa√ß√µes sobre o que deve ser aplicado e onde e o salt minion, um agente que est√° instalado em cada host, interage com o mestre e pode aplicar diretamente o sal do mestre de sal ao host.  Para os fins deste artigo, possu√≠mos conhecimento suficiente e voc√™ pode ler mais na <a href="https://docs.saltstack.com/en/getstarted/overview.html">documenta√ß√£o</a> do <a href="https://docs.saltstack.com/en/getstarted/overview.html">SaltStack</a> . <br><br>  Um mestre de sal n√£o √© tolerante a falhas e n√£o escala para milhares de lacaios, s√£o necess√°rios v√°rios mestres.  Interagir com isso diretamente do trabalhador √© inconveniente, e escrevemos nossas liga√ß√µes no Salt, que chamamos de estrutura Deploy. <br><br><img src="https://habrastorage.org/webt/ir/x6/8o/irx68o0wo493tgj6o8xh0xj0x3o.png" alt="imagem"><br><br>  Para o trabalhador, o √∫nico ponto de entrada √© a API Deploy, que implementa m√©todos como "Aplicar o estado inteiro ou suas partes individuais a esses subordinados" e "Informar o status de tal e qual implementa√ß√£o".  A API de implanta√ß√£o armazena informa√ß√µes sobre todos os lan√ßamentos e suas etapas espec√≠ficas no DeployDB, onde tamb√©m usamos o PostgreSQL.  Informa√ß√µes sobre todos os subordinados e mestres e sobre a perten√ßa do primeiro ao segundo tamb√©m s√£o armazenadas l√°. <br><br>  Dois componentes adicionais s√£o instalados nos mestres de sal: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/develop/ref/netapi/all/salt.netapi.rest_cherrypy.html">API REST Salt</a> , com a qual a API de Implanta√ß√£o interage para iniciar lan√ßamentos.  A API REST vai para o salt-master local e ele j√° se comunica com minions usando o ZeroMQ. </li><li>  A ess√™ncia √© que ele vai para a API Deploy e recebe as chaves p√∫blicas de todos os minions que devem estar conectados a este salt-master.  Sem uma chave p√∫blica no mestre, o lacaio simplesmente n√£o pode se conectar ao mestre. </li></ul><br>  Al√©m do salt minion, dois componentes tamb√©m s√£o instalados no Data Plane: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/latest/ref/returners/">Returner</a> - um m√≥dulo (uma das partes extens√≠veis do salt), que traz o resultado da implanta√ß√£o n√£o apenas para o salt master, mas tamb√©m na API Deploy.  A API de implanta√ß√£o inicia a implanta√ß√£o acessando a API REST no assistente e recebe o resultado atrav√©s do retornador do minion. </li><li>  Pinger mestre, que pesquisa periodicamente a API de Implanta√ß√£o √† qual os subordinados principais devem estar conectados.  Se a API de Implementa√ß√£o retornar um novo endere√ßo do assistente (por exemplo, porque o antigo est√° morto ou sobrecarregado), o pinger reconfigura o lacaio. </li></ul><br>  Outro local em que usamos a extensibilidade do SaltStack √© <a href="https://docs.saltstack.com/en/latest/ref/pillar/all/index.html">ext_pillar</a> - a capacidade de obter <a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html">pilares</a> de algum lugar externo (algumas informa√ß√µes est√°ticas, por exemplo, a configura√ß√£o do PostgreSQL, usu√°rios, bancos de dados, extens√µes etc.).  Vamos para a API interna do nosso m√≥dulo para obter configura√ß√µes espec√≠ficas do cluster, pois elas s√£o armazenadas no MetaDB. <br><br>  Separadamente, observe que o pilar tamb√©m cont√©m informa√ß√µes confidenciais (senhas de usu√°rios, certificados TLS, chaves GPG para criptografar backups) e, portanto, primeiro, toda a intera√ß√£o entre todos os componentes √© criptografada (n√£o em nenhum de nossos bancos de dados sem TLS, HTTPS em todos os lugares, o minion e o mestre tamb√©m criptografam todo o tr√°fego).  Em segundo lugar, todos esses segredos s√£o criptografados no MetaDB, e usamos a separa√ß√£o de segredos - nas m√°quinas da API interna, h√° uma chave p√∫blica que criptografa todos os segredos antes de serem armazenados no MetaDB, e a parte privada fica em mestres de sal e somente eles podem obter segredos abertos para a transfer√™ncia como pilar para um lacaio (novamente por meio de um canal criptografado). <br><br><h3>  MDB Health </h3><br>  Ao trabalhar com bancos de dados, √© √∫til conhecer seu status.  Para isso, temos o microsservi√ßo MDB Health.  Ele recebe informa√ß√µes de status do host do componente interno dos MDBs da m√°quina virtual MDB e as armazena em seu pr√≥prio banco de dados (nesse caso, Redis).  E quando chega uma solicita√ß√£o sobre o status de um cluster espec√≠fico na API interna, a API interna usa dados do MetaDB e MDB Health. <br><br><img width="500" src="https://habrastorage.org/webt/ee/su/qu/eesuquvflpn5k_frgbcqlakkfkc.png" alt="imagem"><br><br>  As informa√ß√µes sobre todos os hosts s√£o processadas e apresentadas de forma compreens√≠vel na API.  Al√©m do estado de hosts e clusters para alguns DBMSs, o MDB Health tamb√©m retorna se um host espec√≠fico √© um mestre ou uma r√©plica. <br><br><h3>  DNS MDB </h3><br>  O microsservi√ßo DNS do MDB √© necess√°rio para gerenciar registros CNAME.  Se o driver para conectar-se ao banco de dados n√£o permitir a transfer√™ncia de v√°rios hosts na cadeia de conex√£o, voc√™ poder√° conectar-se a um <a href="https://cloud.yandex.ru/docs/managed-mysql/operations/connect">CNAME</a> especial, que sempre indica o mestre atual no cluster.  Se o mestre mudar, o CNAME ser√° alterado. <br><br><img width="500" src="https://habrastorage.org/webt/2y/g2/sd/2yg2sd8z2cgvuy9spzxo00w3eoy.png" alt="imagem"><br><br>  Como est√° indo isso?  Como dissemos acima, dentro da m√°quina virtual h√° um cron MDB, que envia periodicamente uma pulsa√ß√£o do seguinte conte√∫do para o DNS do MDB: "Nesse cluster, o registro CNAME deve apontar para mim".  O DNS do MDB aceita essas mensagens de todas as m√°quinas virtuais e decide se deseja alterar os registros CNAME.  Se necess√°rio, ele altera o registro pela API DNS. <br><br>  Por que fizemos um servi√ßo separado para isso?  Porque a API DNS tem controle de acesso apenas no n√≠vel da zona.  Um invasor em potencial, obtendo acesso a uma m√°quina virtual separada, pode alterar os registros CNAME de outros usu√°rios.  O DNS do MDB exclui esse cen√°rio porque verifica a autoriza√ß√£o. <br><br><h3>  Entrega e exibi√ß√£o de logs do banco de dados </h3><br>  Quando o banco de dados na m√°quina virtual grava no log, o componente cliente push especial l√™ esse registro e envia a linha que acabou de aparecer para o Logbroker ( <a href="https://habr.com/ru/company/yandex/blog/239823/">eles j√° escreveram</a> sobre ele no Habr√©).  A intera√ß√£o do cliente push com o LogBroker √© constru√≠da com a sem√¢ntica exata: n√≥s definitivamente o enviaremos e garantiremos estritamente uma vez. <br><br>  Um conjunto separado de m√°quinas - LogConsumers - pega os logs da fila do LogBroker e os armazena no banco de dados do LogsDB.  O DBMS do ClickHouse √© usado para o banco de dados de log. <br><br><img width="500" src="https://habrastorage.org/webt/w-/zc/mr/w-zcmrza7z9pg6gacszmpm0kojo.png" alt="imagem"><br><br>  Quando uma solicita√ß√£o √© enviada √† API Interna para exibir logs por um intervalo de tempo espec√≠fico para um cluster espec√≠fico, a API Interna verifica a autoriza√ß√£o e envia a solicita√ß√£o ao LogsDB.  Portanto, o loop de entrega de log √© completamente independente do loop de exibi√ß√£o de log. <br><br><h3>  Faturamento </h3><br>  O esquema de cobran√ßa √© constru√≠do de maneira semelhante.  Dentro da m√°quina virtual, h√° um componente que verifica com certa periodicidade que tudo est√° em ordem com o banco de dados.  Se tudo estiver bem, voc√™ poder√° executar o faturamento para esse intervalo de tempo a partir do momento do √∫ltimo lan√ßamento.  Nesse caso, um registro √© feito no log de cobran√ßa e, em seguida, o cliente push envia o registro para o LogBroker.  Os dados do Logbroker s√£o transferidos para o sistema de faturamento e os c√°lculos s√£o feitos l√°.  Este √© um esquema de cobran√ßa para a execu√ß√£o de clusters. <br><br>  Se o cluster estiver desativado, o uso de recursos de computa√ß√£o deixa de ser cobrado; no entanto, o espa√ßo em disco √© cobrado.  Nesse caso, o faturamento da m√°quina virtual √© imposs√≠vel e o segundo circuito est√° envolvido - o circuito de faturamento offline.  H√° um conjunto de m√°quinas separado que executa a varredura da lista de clusters de desligamento do MetaDB e grava um log no mesmo formato no Logbroker. <br><br>  O faturamento off-line pode ser usado para faturamento e tamb√©m inclui clusters, mas, em seguida, os hosts de faturamento, mesmo que estejam em execu√ß√£o, mas n√£o funcionam.  Por exemplo, quando voc√™ adiciona um host a um cluster, ele √© implantado a partir do backup e √© acompanhado pela replica√ß√£o.  √â errado cobrar o usu√°rio por isso, porque o host est√° inativo por esse per√≠odo. <br><br><img width="500" src="https://habrastorage.org/webt/fa/gx/hw/fagxhwoaqe0fb5q60chfthefkv8.png" alt="imagem"><br><br><h3>  Backup </h3><br>  O esquema de backup pode diferir ligeiramente para diferentes DBMSs, mas o princ√≠pio geral √© sempre o mesmo. <br><br>  Cada mecanismo de banco de dados usa sua pr√≥pria ferramenta de backup.  Para PostgreSQL e MySQL, este √© o <a href="https://github.com/wal-g/wal-g">WAL-G</a> .  Ele cria backups, os compacta, criptografa e os coloca no <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> .  Ao mesmo tempo, cada cluster √© colocado em um bucket separado (primeiro, para isolamento e, em segundo lugar, para facilitar a economia de espa√ßo para backups) e √© criptografado com sua pr√≥pria chave de criptografia. <br><br><img width="500" src="https://habrastorage.org/webt/mf/dh/w0/mfdhw0bgyo59pytnhaqkaqghbou.png" alt="imagem"><br><br>  √â assim que o plano de controle e o plano de dados funcionam.  De tudo isso, o servi√ßo de banco de dados gerenciado Yandex.Cloud √© formado. <br><br><h2>  Por que tudo est√° organizado dessa maneira </h2><br>  Obviamente, no n√≠vel global, algo poderia ser implementado de acordo com esquemas mais simples.  Mas t√≠nhamos nossas pr√≥prias raz√µes para n√£o seguir o caminho de menor resist√™ncia. <br><br>  Antes de tudo, quer√≠amos ter um plano de controle comum para todos os tipos de DBMS.  N√£o importa qual voc√™ escolher, no final, sua solicita√ß√£o chega √† mesma API interna e todos os componentes nela tamb√©m s√£o comuns a todos os DBMSs.  Isso torna nossa vida um pouco mais complicada em termos de tecnologia.  Por outro lado, √© muito mais f√°cil introduzir novos recursos e recursos que afetam todos os DBMSs.  Isso √© feito uma vez, n√£o seis. <br><br>  O segundo momento importante para n√≥s - quer√≠amos garantir o m√°ximo poss√≠vel a independ√™ncia do plano de dados do plano de controle.  E hoje, mesmo que o Control Plane esteja completamente indispon√≠vel, todos os bancos de dados continuar√£o funcionando.  O servi√ßo garantir√° sua confiabilidade e disponibilidade. <br><br>  Em terceiro lugar, o desenvolvimento de praticamente qualquer servi√ßo √© sempre um compromisso.  Em um sentido geral, grosso modo, em algum lugar mais importante √© a velocidade de lan√ßamento de lan√ßamentos e em alguma confiabilidade adicional.  Ao mesmo tempo, agora ningu√©m pode se dar ao luxo de fazer um ou dois lan√ßamentos por ano, isso √© √≥bvio.  Se voc√™ olhar para o Control Plane, aqui vamos nos concentrar na velocidade do desenvolvimento, na r√°pida introdu√ß√£o de novos recursos, lan√ßando atualiza√ß√µes v√°rias vezes por semana.  E o Data Plane √© respons√°vel pela seguran√ßa de seus bancos de dados, pela toler√¢ncia a falhas, ent√£o aqui est√° um ciclo de lan√ßamento completamente diferente, medido em semanas.  E essa flexibilidade em termos de desenvolvimento tamb√©m nos proporciona sua independ√™ncia m√∫tua. <br><br>  Outro exemplo: os servi√ßos de banco de dados gerenciados geralmente fornecem aos usu√°rios apenas unidades de rede.  O Yandex.Cloud tamb√©m oferece unidades locais.  O motivo √© simples: a velocidade deles √© muito maior.  Com unidades de rede, por exemplo, √© mais f√°cil escalar a m√°quina virtual para cima e para baixo.  √â mais f√°cil fazer backups na forma de instant√¢neos do armazenamento em rede.  Como muitos usu√°rios precisam de alta velocidade, aumentamos o n√≠vel das ferramentas de backup. <br><br><h2>  Planos futuros </h2><br>  E algumas palavras sobre planos para melhorar o servi√ßo a m√©dio prazo.  Esses s√£o os planos que afetam os bancos de dados gerenciados Yandex como um todo, em vez dos DBMSs individuais. <br><br>  Antes de tudo, queremos oferecer mais flexibilidade na defini√ß√£o da frequ√™ncia de cria√ß√£o de backup.  Existem cen√°rios em que √© necess√°rio que durante o dia os backups sejam feitos uma vez a cada poucas horas, durante a semana - uma vez por dia, durante o m√™s - uma vez por semana, durante o ano - uma vez por m√™s.  Para fazer isso, estamos desenvolvendo um componente separado entre a API interna e o <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> . <br><br>  Outro ponto importante, importante para n√≥s e usu√°rios, √© a velocidade das opera√ß√µes.  Recentemente, fizemos grandes altera√ß√µes na infraestrutura Deploy e reduzimos o tempo de execu√ß√£o de quase todas as opera√ß√µes para alguns segundos.  N√£o foram cobertas apenas as opera√ß√µes de cria√ß√£o de um cluster e inclus√£o de um host no cluster.  O tempo de execu√ß√£o da segunda opera√ß√£o depende da quantidade de dados.  Mas o primeiro, aceleraremos em um futuro pr√≥ximo, porque os usu√°rios geralmente desejam criar e excluir clusters em seus pipelines de CI / CD. <br><br>  Nossa lista de casos importantes inclui a adi√ß√£o da fun√ß√£o de aumentar automaticamente o tamanho do disco.  Agora isso √© feito manualmente, o que n√£o √© muito conveniente e nem muito bom. <br><br>  Por fim, oferecemos aos usu√°rios um grande n√∫mero de gr√°ficos mostrando o que est√° acontecendo com o banco de dados.  Damos acesso aos logs.  Ao mesmo tempo, vemos que os dados √†s vezes s√£o insuficientes.  Precisa de outros gr√°ficos, outras fatias.  Aqui tamb√©m planejamos melhorias. <br><br>  Nossa hist√≥ria sobre o servi√ßo de banco de dados gerenciado acabou sendo longa e provavelmente bastante entediante.  Melhor do que quaisquer palavras e descri√ß√µes, apenas pr√°tica real.  Portanto, se voc√™ quiser, poder√° avaliar independentemente os recursos de nossos servi√ßos: <br><br><ul><li>  <a href="https://cloud.yandex.ru/services/managed-postgresql">Servi√ßo gerenciado Yandex para PostgreSQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mysql">Servi√ßo Gerenciado Yandex para MySQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mongodb">Servi√ßo Gerenciado Yandex para MongoDB</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-clickhouse">Servi√ßo gerenciado Yandex para ClickHouse</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-redis">Servi√ßo Gerenciado Yandex para Redis</a> </li><li>  <a href="https://cloud.yandex.ru/services/data-proc">Yandex Data Proc</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt477860/">https://habr.com/ru/post/pt477860/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt477850/index.html">Reagir nativo - uma bala de prata para todos os problemas? Como escolhemos uma ferramenta de plataforma cruzada para o Profi.ru</a></li>
<li><a href="../pt477852/index.html">Hipocrisia n√£o t√≥xica</a></li>
<li><a href="../pt477854/index.html">O que acontece ao conectar dentro e fora de um t√∫nel VPN</a></li>
<li><a href="../pt477856/index.html">Aceleradores de flash PCI-E de 800 GB a 6,4 TB: do amanhecer √† vida em um PC / servidor comum</a></li>
<li><a href="../pt477858/index.html">Trabalho fora da mesa: que projetos realmente surgiram ap√≥s a pr√©-acelera√ß√£o?</a></li>
<li><a href="../pt477862/index.html">E assim foi poss√≠vel? Ci√™ncia e TI em uma confer√™ncia</a></li>
<li><a href="../pt477864/index.html">TabPy para trabalhar com dados no ClickHouse do Tableau</a></li>
<li><a href="../pt477866/index.html">Semin√°rio: Solu√ß√µes de TI h√≠bridas para neg√≥cios. 5 de dezembro, S√£o Petersburgo</a></li>
<li><a href="../pt477872/index.html">c.tech: Data Sense n¬∫ 4, lan√ßamento de ano novo</a></li>
<li><a href="../pt477874/index.html">CDN din√¢mico para streaming WebRTC com baixa lat√™ncia e transcodifica√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>