<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìØ üîì ‚è∞ Die Moral des Robotertransports: Das Problem des Wagens, Risiken und Konsequenzen üëßüèª „äóÔ∏è üçû</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die moralische Seite der Entwicklung von Roboterfahrzeugen ist sehr komplex. Die Entwickler sind der festen √úberzeugung, dass Tests auf √∂ffentlichen S...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Moral des Robotertransports: Das Problem des Wagens, Risiken und Konsequenzen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itelma/blog/483476/"><img src="https://habrastorage.org/getpro/habr/post_images/9da/572/3d5/9da5723d574c33ceb952a3ebf5334b57.jpg" alt="Bild"><br><br>  Die moralische Seite der Entwicklung von Roboterfahrzeugen ist sehr komplex.  Die Entwickler sind der festen √úberzeugung, dass Tests auf √∂ffentlichen Stra√üen durchgef√ºhrt werden sollten, obwohl sie wissen, dass dies f√ºr ahnungslose Verkehrsteilnehmer nur ein geringes Risiko darstellt.  Autos, die nach den Tests hergestellt wurden, werden mit Sicherheit (auch t√∂dliche) Unf√§lle erleiden, und dieser Zustand ist be√§ngstigend und entmutigend.  Gleichzeitig verspricht uns der Erfolg enorme Verbesserungen bei der Verkehrssicherheit und die Rettung der Leben einer gro√üen Anzahl von Menschen, die sterben w√ºrden, wenn es keine Gelegenheit g√§be, gef√§hrlicheres menschliches Fahren durch eine Drohne zu ersetzen. <br><br>  Wir werden solche Aspekte ber√ºcksichtigen wie: <br><br><ol><li>  Verst√§ndnis f√ºr unterschiedliche Herangehensweisen an das moralische Denken von Menschen in unterschiedlichen (oder gleichen) Situationen </li><li>  Wie Recht, Gesellschaft und Versicherung mit riskantem Fahren, Unf√§llen und Unf√§llen zusammenh√§ngen. </li><li>  Risiken und Verluste w√§hrend des Fahrens (oder Fahrtrainings), die wir offenbar f√ºr kleine Vorteile akzeptieren. </li><li>  Wie √§ndert sich unsere Ansicht, dass ‚Äûder Zweck das Mittel rechtfertigt‚Äú, je nachdem, worum es geht: absichtliche Gr√§ueltaten oder kleine absichtliche Risiken. </li><li>  Die gro√üen Vorteile, die sich ergeben, wenn eine kleine Flotte autonomer Autos sicherer fahren lernt. Danach wird ihre Software auf Millionen anderer Autos kopiert - eine Situation, die f√ºr menschliche Fahrer unm√∂glich ist. </li><li>  Risiken und Prinzipien moderner Ans√§tze zum Testen und Entwickeln autonomer Autos und wie Uber sie verletzt hat. </li><li>  Ein gro√üer Vorteil, wenn wir den richtigen Ansatz finden. </li></ol><br><a name="habracut"></a><br><br>  Die Tatsache, dass k√ºrzlich in Tempe, Arizona, ein t√∂dlicher Unfall mit einem Uber-Fahrzeug entdeckt wurde, erh√∂ht die Notwendigkeit, dieses Problem zu verstehen.  Es wurden viele Texte verfasst und viele Meinungen ge√§u√üert, wie die Risiken und Grunds√§tze der Moral in unbemannten Fahrzeugen er√∂rtert werden k√∂nnen.  Ich hoffe, dass ich in diesem Artikel sowohl eine klare Sicht auf dieses Problem als auch einen Leitfaden f√ºr die Diskussion dar√ºber pr√§sentieren kann. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ea/921/5cf/7ea9215cf82e51ed5037034af65299a6.jpg" alt="Bild"><br><br>  <i>Die meisten Abst√ºrze sind Einzelmaschinen und k√∂nnen verhindert werden.</i>  <i>Trotzdem haben wir dieses schreckliche Risiko ohne nachzudenken gemeistert.</i> <br><br>  Es stellt sich heraus, dass eine Kombination der Lehren, die wir aus dem Wagenproblem ziehen (n√§mlich aus dem Wagen, nicht aus dem unbemannten Fahrzeug) und der Messung der Ma√ünahmen, die zu Risiken anstelle von Trag√∂dien f√ºhren, uns dabei helfen kann, ein besseres Verst√§ndnis und Rechtssystem f√ºr die L√∂sung komplexer Fragen zu Robotern zu entwickeln, die und k√∂nnen Menschenleben retten und bedrohen.  Wir k√∂nnen Millionen von Menschenleben retten, wenn wir bereit sind, die gleichen Risiken einzugehen wie beim Unterrichten von Teenagern, bevor diese mit dem Autofahren beginnen (anstelle von Teenagern k√∂nnen Sie sich vorstellen, Pizzaboten zu beeilen).  Fast jeder, der an Robotern arbeitet, m√∂chte, dass sie Leben retten und dies so bald wie m√∂glich tun. Daf√ºr muss die √ñffentlichkeit jedoch die Arbeitsmethoden in diesem Bereich verstehen und sogar anwenden.  Dazu m√ºssen wir sowohl unsere moralischen Instinkte als auch unsere interne Mathematik und den Unterschied zwischen Risiko und Trag√∂die verstehen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/57a/d2f/ccb/57ad2fccb3456ac9538215f4505d85fe.png" alt="Bild"><br><br>  Ich erkannte die Schwierigkeit, diese Probleme vor einem Jahr zu verstehen, als ich zu Hause mit meinem Freund beim Abendessen sprach.  Ein Freund war sehr besorgt √ºber die Risiken, die fr√ºhe Prototypen bergen.  Ich fragte ihn: W√§re es vern√ºnftig, 100 Menschen beim Testen sterben zu lassen, aber wenn Sie das Endprodukt verwenden, werden Millionen von Menschenleben gerettet?  Es schien ihm offensichtlich, dass dies falsch war, und er war mit dieser Meinung nicht allein. <br><br>  Die menschliche Moral ist komplex und listig und kann je nach Situation auf unterschiedliche Weise funktionieren.  Nur wenige von uns sind in ihren Grunds√§tzen glasklar, und diese Grunds√§tze, die wir in unseren subjektiven Entscheidungen anwenden, k√∂nnen sich von denen unterscheiden, die in kollektiven Entscheidungen in der Gesellschaft oder vor Gericht verwendet werden.  Um dieses Problem zu verstehen und zu einer besseren L√∂sung zu gelangen, m√ºssen wir verstehen, wie die Menschen √ºber eine solche Moral denken, und m√∂glicherweise dieses Denken √§ndern, um ein Ergebnis zu erzielen, das in der Mehrheitsmeinung besser ist. <br><br>  Die Antwort k√∂nnte in der Tatsache liegen, dass wir, obwohl wir nicht der Meinung sind, dass gro√üe Ziele unmoralische Mittel rechtfertigen k√∂nnen, zugeben m√∂chten, dass auch bescheidene profitable Ziele Mittel mit geringen unmoralischen Risiken rechtfertigen k√∂nnen. <br><br><h3>  Arten von Moral </h3><br>  Grob gesagt unterscheiden Philosophen zwei breite Klassen von Moralsystemen.  Der erste setzt einen Kodex von Regeln und Prinzipien voraus und definiert einen Fehler als Versto√ü gegen dieselben Regeln und Prinzipien, unabh√§ngig vom Ergebnis.  Der komplexe Name solcher Systeme ist "deontologisch", aber wir werden sie als regelbasiert bezeichnen.  Auf der anderen Seite haben wir Systeme, die auf Ergebnissen basieren, die richtig und falsch mit dem messen, womit wir enden.  Sie sind auch als Konsequentialisten bekannt.  Eine bestimmte Untergruppe solcher Systeme ist utilitaristisch und folgt der Regel, das gr√∂√üte Wohl f√ºr die gr√∂√üte Anzahl von Menschen zu erzielen oder der geringsten Anzahl von Menschen den geringsten Schaden zuzuf√ºgen. <br><br>  Manchmal lieben wir utilitaristische Moralsysteme, besonders als Gesellschaft.  Als Individuen neigen wir jedoch dazu, ihnen zu misstrauen, weil sie mit der gef√§hrlichen Idee verbunden sind, dass ‚Äûder Zweck die Mittel rechtfertigt‚Äú, und diese Idee hat im Laufe der Geschichte zu einer Menge moralischer Schrecken gef√ºhrt.  Die meisten von uns geh√∂ren keiner der Schulen an.  Wie bereits erw√§hnt, √§ndert sich unser Ansatz in Abh√§ngigkeit davon, ob wir uns als Einzelperson oder als Gesellschaft betrachten, und wir √§ndern unseren Denkansatz auf der Grundlage unserer pers√∂nlichen Ansichten mehr, als wir zugeben m√∂chten. <br><br>  Ein reiner Anh√§nger des utilitaristischen Ansatzes wird leicht zustimmen, dass Autos 100 Menschen t√∂ten werden, um eine Million zu retten - im Falle der utilitaristischen Moral gibt es nicht einmal eine Frage.  Gleichzeitig wird es f√ºr viele nicht leicht sein, das zu akzeptieren.  Ich vermute, die Antwort ist zu verstehen, wie wir als Individuen Vorf√§llen und Trag√∂dien besondere Aufmerksamkeit schenken, w√§hrend wir in der Rolle der Gesellschaft auf Risikobewertung und √∂ffentliche G√ºter achten. <br><br>  Diejenigen, die meine Texte lesen, wissen, dass <a href="https://ideas.4brad.com/barack-obama-wants-solve-robocar-trolley-problems-now">ich die Standardanwendung des ‚ÄûTrolley-Problems‚Äú auf unbemannte Fahrzeuge hasse</a> .  In dieser Anwendung stellen sich die Leute die Software im Auto vor, die entscheiden soll, welche von zwei verschiedenen Personengruppen bei einem Unfall get√∂tet werden soll.  Die Vorstellung, dass Maschinen jetzt entscheiden, wer sterben soll, erf√ºllt uns schmerzlich, obwohl dies zuvor ein T√§tigkeitsfeld der G√∂tter war.  In der Tat ist dies eine √§u√üerst seltene Situation, und ihre L√∂sung ist in keiner Priorit√§tsliste enthalten.  Programmierer und Unternehmen m√∂chten auch keine Algorithmen schreiben, die sich auf moralische Entscheidungen beziehen. Sie w√ºrden es vorziehen, dass Politiker solche Fragen beantworten und Gesetze schreiben, denen sie gerne folgen. <br><br>  Das urspr√ºngliche Wagenproblem hatte jedoch eine echte Funktion, die in dieser Situation n√ºtzlich sein k√∂nnte.  Es wurde geschaffen, um uns dabei zu helfen, unser eigenes Denken zu verstehen und den Unterschied zwischen regelbasierten und ergebnisorientierten Moralsystemen zu verstehen, das hei√üt, wir sollten mit Hilfe des Trolley-Problems ein besseres Verst√§ndnis der Philosophie erlangen.  Und sie kann uns helfen, diese Situation besser zu verstehen. <br><br>  Wie Sie sich vielleicht erinnern, rast der Wagen in der urspr√ºnglichen Aufgabe auf Schienen mit kaputten Bremsen.  Jemand (in dieser Situation wirklich unmoralisch) band 5 Personen an die Hauptstra√üe und eine Person an den Zweig.  Sie k√∂nnen den Schalter ziehen, um eine Person zu t√∂ten, aber f√ºnf sparen.  Bis zu 90% der Menschen w√§hlen einen zweckm√§√üigen (ergebnisorientierten) Ansatz und ziehen es vor, den Hebel zu ziehen, einige lehnen jedoch ab.  (In der Tat ist es wahrscheinlicher, dass sich diese Personen in der Unterrichts√ºbung so verhalten. In der <a href="https://www.youtube.com/watch%3Fv%3D1sl5KJ69qiA">Mind Field-</a> Show auf YouTube wurde ein Experiment durchgef√ºhrt, bei dem die Menschen zu der Annahme gezwungen wurden, dass sie sich wirklich in einer Situation mit einem rauschenden Wagen und angeh√§ngten Personen befinden. Achtung, Spoiler : die meisten Themen erstarrten einfach vor Entsetzen) <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1sl5KJ69qiA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Ich m√∂chte sagen, dass die technische L√∂sung f√ºr dieses Problem einfacher ist - <a href="https://www.forbes.com/sites/bradtempleton/2019/02/21/robocar-engineers-prefer-to-solve-the-runaway-trolley-problem-by-fixing-the-brakes-on-the-trolley/">Sie m√ºssen die Wagenbremsen reparieren</a> .  Unbemannte Fahrzeugingenieure werden in erster Linie daf√ºr sorgen, dass ihre Autos niemals in solche Situationen geraten, auch wenn sie sehr selten vorkommen. <br><br>  Variationen des von Philosophen erfundenen Trolley-Problems sind viel interessanter als der urspr√ºngliche Wortlaut.  Bei einer der Optionen m√ºssen Sie nicht den Schalter ziehen, sondern den dicken Mann auf die Schienen dr√ºcken, um den Wagen anzuhalten - nur wenige werden es tun, da es sich um einen vors√§tzlichen Mord handelt.  In der extremsten Version haben Sie keinen Wagen, aber es gibt 5 Patienten, die dringend eine Organtransplantation ben√∂tigen.  Sie k√∂nnen sich einen Mann schnappen, der die Stra√üe entlanggeht, ihn schneiden und f√ºnf Menschen retten - fast niemand ist damit einverstanden, obwohl dies das gleiche Problem unter dem Gesichtspunkt eines utilitaristischen Ansatzes ist. <br><br>  Fast niemand wird dies tun, denn unter uns gibt es nur wenige, die einer bestimmten moralischen Schule streng angeh√∂ren.  Es ist schwieriger f√ºr uns, unser Denken zu √§ndern.  Um Probleme auf dem Gebiet des Testens und der Herstellung unbemannter Fahrzeuge zu verstehen und zu l√∂sen, m√ºssen wir den nat√ºrlichen und individuellen Instinkt √ºberwinden, verschiedene F√§lle als ‚Äûnur‚Äú einzelne Trag√∂dien wahrnehmen, die sie darstellen, und dar√ºber nachdenken, welche Risiken akzeptabel sind und sein k√∂nnen als Gesellschaft erlaubt. <br><br>  Dar√ºber hinaus werden wir von den Trag√∂dien, die externen Passanten passieren, viel mehr abgelehnt, obwohl es uns weniger ausmacht, diese Menschen geringeren Risiken auszusetzen.  Schlie√ülich haben wir mehr Angst, dass uns unabh√§ngige Maschinen und nicht Menschen Schaden zuf√ºgen. <br>  Die Leute sind komisch, dass wir, obwohl die meisten Angst haben, an Robotern zu sterben, eher von einem betrunkenen Verbrecher get√∂tet werden. <br><br>  Grunds√§tzlich denken die Leute, dass T√∂ten, um Leben zu retten, falsch ist, es ist unbestreitbar.  Die eigentliche Frage ist, was es eigentlich hei√üt, unbemannte Fahrzeuge auf der Stra√üe zu testen und freizugeben - t√∂ten, um Leben zu retten, oder Risiken f√ºr denselben Zweck einzugehen.  Tats√§chlich sind wir uns in den meisten F√§llen einig, das Risiko einzugehen, um Leben zu retten, sofern ein gutes Ergebnis erzielt wird. <br><br><h3>  Moralische Aspekte des Fahrens </h3><br>  Wie ist also die Beziehung zwischen Moralphilosophie und Autos?  Um dies herauszufinden, m√ºssen wir √ºber den moralischen Aspekt von Verkehrsunf√§llen nachdenken.  Individuen und die Gesellschaft insgesamt stehen diesem Problem unterschiedlich gegen√ºber.  Gesellschaften im Allgemeinen und das Gesetz im Besonderen ziehen es vor, nichts als unmoralisch oder b√∂se zu bezeichnen, es sei denn, es gab eine b√∂swillige Absicht, die in der Rechtsprechung als Mens Rea bezeichnet wird.  Fast alle Straftaten beinhalten b√∂swillige Absichten, und wenn dies nicht der Fall ist, wird die Bestrafung (in den meisten F√§llen) gemildert.  Aus diesem Grund sind Situationen keine Seltenheit, in denen jemand eine Person in einem Auto zu Tode st√∂√üt und daf√ºr nicht strafrechtlich haftbar gemacht wird.  Der Fahrer wird finanziell bestraft, ist aber versichert.  Der T√§ter wird aus dieser Situation nur mit einem Gef√ºhl der Schuld f√ºr das, was passiert ist, herauskommen.  Dies alles ist jedoch nur dann der Fall, wenn das T√∂ten einer Person v√∂llig unbeabsichtigt war und tats√§chlich den Absichten des Fahrers v√∂llig zuwiderlief.  <a href="https://www.lawyers.com/legal-info/criminal/traffic-violations/when-a-drivers-actions-amount-to-manslaughter.html">Es kann keinen (auch nicht unbeabsichtigten) Mord geben, ohne dass eine klare Absicht oder grobe und vors√§tzliche Fahrl√§ssigkeit vorliegt</a> . <br><br>  Gleichzeitig arbeiten wir hart daran, festzustellen, ob Vorsatz oder Fahrl√§ssigkeit vorliegt, da uns die Tatsache sehr beunruhigt, dass etwas so Tragisches wie der Tod m√∂glicherweise unbeantwortet bleibt.  Aber wenn wir √ºber die Situation sprechen, in der der Unfall tats√§chlich passiert ist (womit ich meine, dass er sich im Rahmen der √ºblichen Risiken eines vorsichtigen Fahrens ereignet hat), gibt es keine rechtlichen Konsequenzen. Das Schlimmste, was passieren kann, sind die finanziellen Konsequenzen, die vollst√§ndig von der Versicherung bezahlt werden. <br><br>  Auf der anderen Seite sollten Sie √ºberlegen, zu beschleunigen.  Geschwindigkeits√ºberschreitungen sind illegal, auch wenn sie h√§ufig begangen werden, werden jedoch an einigen Stellen nur selten bestraft.  Wenn Sie beschleunigen, wissen Sie (oder sollten Sie wissen), ob Sie √ºberschreiten oder nicht.  Auf diese Weise setzen Sie andere Personen einem zus√§tzlichen Risiko aus, obwohl gl√ºcklicherweise in der Regel nichts Schlimmes passiert.  Trotz der Tatsache, dass Sie mit Sicherheit eine Geldstrafe f√ºr Geschwindigkeits√ºberschreitungen erhalten, werden die meisten Geldstrafen f√ºr √úberschreitungen verh√§ngt, die niemandem geschadet haben.  Fast alle von uns √ºberschreiten regelm√§√üig die Geschwindigkeit, und dies aus dem offensichtlichen Grund - wir m√∂chten etwas schneller ankommen.  Dies mag der Intuition widersprechen, aber absichtliches Beschleunigen in unserem System ist unmoralischer und illegaler als zuf√§lliges T√∂ten, obwohl wir als Individuen viel toleranter gegen√ºber Geschwindigkeits√ºberschreitungen sind. <br><br>  Ich glaube, dass das eigentliche moralische Problem beim Fahren darin besteht, dass wir andere Menschen absichtlich in Gefahr bringen.  Oder genauer gesagt ein unannehmbar hohes Risiko.  Jedes Fahren bedeutet, dass andere Personen gef√§hrdet sind.  In der Tat ist es wahrscheinlich, dass wir beim Fahren am h√§ufigsten andere Menschen dem gr√∂√üten Risiko aussetzen.  Wir alle kennen eine erstaunliche Anzahl von Todesf√§llen, Verletzungen und Sachsch√§den durch das Fahren - mehr Menschen starben bei Unf√§llen als in allen Kriegen und infolge von Terroranschl√§gen in der Geschichte der Vereinigten Staaten seit dem Unabh√§ngigkeitskrieg.  All dies ist erschreckend riskant, aber f√ºr uns ist es so wichtig, dass wir beschlossen haben, dieses relativ hohe Risiko f√ºr uns und die Menschen um uns herum in Kauf zu nehmen.  Wir tun dies mit Absicht, obwohl wir es oft vergessen, und wir erinnern uns sicherlich nicht gut an das mathematische Wesen dieser Ph√§nomene.  Wir betrachten dieses Grundrisiko als ‚Äûakzeptabel‚Äú in unseren Gesetzen und in unserem Leben. <br><br>  <b>Die Gesellschaft entschied, dass Unrecht und Illegalit√§t nicht bei bestimmten Opfern, sondern bei vors√§tzlicher und unachtsamer Gef√§hrdung anderer Menschen zu suchen sind.</b> <br><br>  Wir haben Gesetze erlassen, die es Ihnen untersagen, andere Personen einem au√üergew√∂hnlichen Risiko auszusetzen, und Sie erhalten Geldstrafen f√ºr Geschwindigkeits√ºberschreitungen, falsche Spurwechsel und unachtsames Fahren.  Wir halten solche Handlungen f√ºr unmoralisch, da sie vors√§tzlich oder fahrl√§ssig begangen werden, obwohl wir einen nicht wirklich unbeabsichtigten Tod nicht f√ºr unmoralisch halten.  Dies h√§lt diejenigen nicht auf, die dies als gro√üe Trag√∂die ansehen, und dies ist eine absolut nat√ºrliche Reaktion.  Aber als Gesellschaft, die Gesetze schreibt und durchsetzt, nehmen wir alles anders wahr. <br><br>  Wir finden eine √ºberraschende Anzahl von Fahrrisiken akzeptabel: <br><br><ol><li>  Schweres Fahren </li><li>  Fahren bei Regen, Schnee und Nacht </li><li>  Fahren an Orten mit vielen Fu√üg√§ngern und Radfahrern </li><li>  Autofahren mit leichten mechanischen Problemen </li><li>  Fahren ohne automatische Notbremsung und andere fortschrittliche Technologien </li><li>  Fahren in einem schl√§frigen Zustand, auch wenn wir einschlafen (zwei Staaten haben Gesetze, die schl√§friges Fahren verbieten) </li><li>  Unbeschwertes Fahren f√ºr Jugendliche, die erst k√ºrzlich eine Lizenz erhalten haben </li><li>  Erwachsene, die Studenten fahren </li><li>  Fahren √§lterer Menschen mit reduzierter Wahrnehmung und l√§ngeren Reaktionszeiten </li><li>  Fahren mit knapp unter dem zul√§ssigen Blutalkoholspiegel </li><li>  Fahren in einem kranken Zustand (obwohl es illegal ist) </li><li>  Verwenden von Ger√§ten und Schreiben von Nachrichten w√§hrend der Fahrt (obwohl dies illegal ist) </li><li>  In den USA ist das Schnellfahren sehr verbreitet, oft mit einem gro√üen Abstand zu anderen Autos </li></ol><br>  Schauen wir uns einen unbemannten Fahrzeug-Prototyp an.  Wenn ein Entwicklerteam ein solches Auto auf die Stra√üe bringt, gef√§hrdet es bewusst andere Verkehrsteilnehmer.  Nat√ºrlich wollen sie keinen Unfall verursachen, sondern ihn verhindern. <br><br>  Momentan arbeiten Testdrohnen bis auf wenige Ausnahmen immer unter der Kontrolle eines menschlichen Fahrers, der bereit ist, bei Problemen zur Arbeit zu kommen.  Fast immer gibt es eine zweite Person, die die Systeme √ºberwacht und gelegentlich einen Blick auf die Stra√üe wirft.  Dies kann mit der Situation verglichen werden, in der ein jugendlicher Fahrer mit einem Studentenausweis von einem Ausbilder begleitet wird.  Der Ausbilder hat sein eigenes Bremspedal und kann das Rad abfangen, wie der Fahrer in einer Drohne.  Jugendliche Fahrer, begleitet von Ausbildern, haben tats√§chlich ziemlich gute Sicherheitswerte, wie fast alle unbemannten Fahrzeugteams - mit Ausnahme von Uber, √ºber das ich sp√§ter sprechen werde. <br><br>  Diese jugendlichen Fahrer, die nach bestandener Mindestpr√ºfung einen F√ºhrerschein erhalten haben, werden zu den gef√§hrlichsten Fahrern auf der Stra√üe.  Wir lassen sie auf der Stra√üe frei, um ihnen Mobilit√§t zu erm√∂glichen, auch weil dies der einzige Weg ist, sie vorsichtiger zu machen, was sie schlie√ülich werden.  Wir gehen die Risiken ein, die mit dem Fahren eines Fahrers im Teenageralter verbunden sind, in der Hoffnung, in Zukunft einen vorsichtigen Fahrer zu finden.  Von jedem riskanten Teenager auf der Stra√üe w√§chst ein Erwachsener auf, ein vorsichtigerer Erwachsener (tats√§chlich etwas weniger als einer, weil nicht alle Teenager dieses sicherste Alter erreichen). <br><br><img src="https://habrastorage.org/webt/vr/dq/bq/vrdqbqrlnxepkvveqq-do1xcoko.jpeg" alt="Bild"><br><br>  Wie bereits erw√§hnt, setzt das Drohnen-Entwicklungsteam die Menschen in ihrer Umgebung einem gewissen Risiko aus, doch der Nutzen dieses Risikos ist enorm.  Eine kurze Studienreise wird die Sicherheit aller nachfolgenden Autos verbessern, die sp√§ter erstellt werden, was letztendlich Millionen von verbesserten Autos bedeutet.  Indem wir die Hauptrisiken des Testens und Entwickelns akzeptieren, profitieren wir von einer signifikanten Reduzierung der Risiken in der Zukunft.  Risikominderung wird eintreten, wenn Autos sicherer als Menschen zu fahren beginnen, und Menschen aufh√∂ren, andere in Gefahr zu bringen, indem sie sich f√ºr eine Fahrt mit einem unbemannten Auto entscheiden, anstatt selbstst√§ndig zu fahren.  Dies gilt insbesondere f√ºr Personen, die einen der oben aufgef√ºhrten Artikel getrunken haben oder mit diesen in Verbindung stehen. <br><br>  Einige argumentieren, dass wir nicht nur au√üergew√∂hnliche, sondern auch notwendige Risiken ber√ºcksichtigen m√ºssen.  Weil es falsch sein kann, Menschen einem normalen Risiko auszusetzen, wenn dies nicht notwendig ist.  Insbesondere argumentieren einige, dass derzeitige Teams mehr Tests als n√∂tig durchf√ºhren und dass dies falsch ist.  Vielleicht ist dies der richtige Ansatz (obwohl nat√ºrlich die erforderliche Anzahl von Tests umstritten bleibt).  Ber√ºcksichtigt man jedoch die Gr√ºnde, warum Menschen auf der Stra√üe riskante Entscheidungen treffen - von der Lieferung von Lebensmitteln bis zur R√ºckkehr eine Minute fr√ºher nach Hause -, erf√ºllen sie kaum das notwendige Ma√ü, obwohl wir diese Risiken tolerieren. <br><br>  Hier m√ºssen wir ausgehend von den Ergebnissen argumentieren.  ,    ,      ,         .      - ,         .     -,                   .    ‚Äì ,   ‚Äì .   . <br><br><h3>   </h3><br>  ,   .         ,      0.1     .  ‚Äì  ,  1  .       ‚Äì  0.013   . ,      ,       .           ‚Äì ,  ,     ,   1/250   .         ,    ,       .     ,                      .     ,    .  ,        ,        <b></b> (  )    ,     .       1.5   ,     .     (    1.7    ),          . <br><br>   ,     ¬´ ¬ª,      ?       ‚Äì         ,       ,           .   ,       ,            . <br><br>       ,     .    ,      (    ),      100 000      .    1.5        .    . 100 000     ‚Äì    ,    10         2-3  . <br><br>   ,      .        ,    ,       ,   ,              ,  .      . ,      ,   ,  ¬´¬ª  ,     .   ,        -,        . , ,            . <br><br><h3>    ? </h3><br>   ?       Waymo.      7  ,    Google.     10          .       ,   ,     ,             ‚Äì     ,   ,   ,     .  ,                Waymo,   ,      ‚Äì   ,     ,      .     ,          . <br><br> Tesla    ,      Tesla      .     4.3        2.7     . Tesla      ,        ,       ,   ,         .  ,          ,   (       )       ,  ,          .   , ,     - ,      ,    ,          . , Tesla            ,       ,    .          ,      Uber. <br><br><h3> Uber </h3><br>   2018    Uber    ,  , ,        .       ,            .    <a href="https://ideas.4brad.com/search/node/uber%2520fatality"> </a> ,        ,  ,      ,     99.9%  . Uber         ,   ,          -  ,       ,      .   ,   ,    ,  ,     ,       Uber,    ,    ,    ,  -          ,       .      ,             .    ,    .      ,     ‚Äî       ,   ,     ‚Äî      . Uber      18        ,      . <br><br>   ,            ,   ,   ,          .   ,      .  ,   Uber           ,   .    ,       ,     .     - ,    ,       ,    Uber  -  ,        .  Uber        . <br><br>            ,      ,       .  ,          .        ,       ,         .   ,   ,      ,          ,     ,             .               0,07%        ,           ,   ,    . <br><br>           . ,            .   ,  Uber      ( ),  ,          , ,      .        ,   .   ,      ‚Äì  .     ,             ,       .        ,         ,       .          3  . <br><br><h3>  Fazit </h3><br>        ,    ‚Äì      ,      .     ,    ,   ,        ,   .     ,    ,            ,       . <br><br> <b>  ,     ,   100      ,    .    ,   ,     ,      ,        .</b> <br><br>       ,    .       ,    ,         ,       ‚Äî ,     ‚Äî     .  ,   ,     ,     ,   - ,     ,       .   ,     ,       . <br><br>      ,   ,     .    .               ,       ,       . <br><br><hr><br><img src="https://habrastorage.org/webt/4m/5z/_p/4m5z_pc9zhjja8pmtwxvaihckfe.png" alt="Bild"><br><br><div class="spoiler">  <b class="spoiler_title">√úber ITELMA</b> <div class="spoiler_text">  Wir sind ein gro√ües <a href="https://en.wikipedia.org/wiki/Automotive_industry">Automobilzulieferunternehmen</a> .  Das Unternehmen besch√§ftigt rund 2.500 Mitarbeiter, darunter 650 Ingenieure. <br><br>  Wir sind vielleicht das leistungsst√§rkste Kompetenzzentrum in Russland f√ºr die Entwicklung der Automobilelektronik in Russland.  Jetzt sind wir aktiv am Wachsen und haben viele offene Stellen (etwa 30, einschlie√ülich in den Regionen) ge√∂ffnet, wie z. B. einen Software-Ingenieur, einen Konstrukteur, einen leitenden Entwicklungsingenieur (DSP-Programmierer) usw. <br><br>  Wir haben viele interessante Herausforderungen von Autoherstellern und Sorgen, die die Branche antreiben.  Wenn Sie als Spezialist wachsen und von den Besten lernen m√∂chten, freuen wir uns, Sie in unserem Team zu sehen.  Wir sind auch bereit, Know-how zu teilen, das wichtigste, was in der Automobilindustrie passiert.  Stellen Sie uns Fragen, wir werden antworten, wir werden diskutieren. </div></div><br>  <b>Lesen Sie weitere n√ºtzliche Artikel:</b> <br><br><ul><li>  <a href="https://habr.com/ru/company/itelma/blog/479736/">Kameras oder Laser</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/478640/">Autonome Autos auf Open Source</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/476824/">McKinsey: Software- und Elektronikarchitektur in der Automobilindustrie neu denken</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/476054/">Ein weiterer OS-Krieg ist bereits unter der Motorhaube von Autos</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/475576/">Programmcode im Auto</a> </li><li>  <a href="https://habr.com/ru/company/itelma/blog/475448/">In einem modernen Auto gibt es mehr Codezeilen als ...</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de483476/">https://habr.com/ru/post/de483476/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de483462/index.html">Halt die Klappe und nimm mein Geld</a></li>
<li><a href="../de483466/index.html">Einf√ºhrung in die Backpropagation-Methode</a></li>
<li><a href="../de483468/index.html">Flatterintegrationstests - ganz einfach</a></li>
<li><a href="../de483470/index.html">Fliesen effizient verlegen (Pro CSS, SVG, Muster und mehr)</a></li>
<li><a href="../de483472/index.html">Alles l√∂schen: So l√∂schen Sie Daten und setzen die NVMe-SSD auf die Werkseinstellungen zur√ºck</a></li>
<li><a href="../de483478/index.html">Sonne, Wind und Wasser ver 0.1</a></li>
<li><a href="../de483480/index.html">Kreuzwortr√§tsel "F√ºhlen Sie sich wie ein SOC-Analyst"</a></li>
<li><a href="../de483482/index.html">US Federal Communications Commission f√ºr V2V, V2I und V2X</a></li>
<li><a href="../de483484/index.html">"Pretend it": Wie unbemannte Fahrzeuge "nach rechts kapitulieren"</a></li>
<li><a href="../de483492/index.html">L√∂sen typischer Probleme mit json_encode (PHP)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>