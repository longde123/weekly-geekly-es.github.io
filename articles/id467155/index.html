<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☢️ ⛈️ 💲 Praktik Terbaik untuk Kontainer Kubernet: Pemeriksaan Kesehatan 👐🏻 🈹 🆗</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL; DR 


- Untuk mencapai observabilitas tinggi terhadap wadah dan layanan mikro, majalah dan metrik primer tidak cukup. 
- Untuk pemulihan yang lebi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Praktik Terbaik untuk Kontainer Kubernet: Pemeriksaan Kesehatan</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/467155/"><p><img src="https://habrastorage.org/webt/et/w4/sa/etw4sacz-ttuiwcyyvxx1utukq4.jpeg"></p><br><p>  <strong>TL; DR</strong> </p><br><ul><li>  Untuk mencapai observabilitas tinggi terhadap wadah dan layanan mikro, majalah dan metrik primer tidak cukup. </li><li>  Untuk pemulihan yang lebih cepat dan peningkatan toleransi kesalahan, aplikasi harus menerapkan Prinsip Observabilitas Tinggi (HOP). </li><li>  Di tingkat aplikasi, NRA membutuhkan: pencatatan yang benar, pemantauan yang cermat, pemeriksaan kesehatan, dan pelacakan kinerja / transisi. </li><li>  Gunakan pemeriksaan <em>readinessProbe</em> dan <em>livenessProbe</em> Kubernetes <em>sebagai</em> elemen <em>HOP</em> . <a name="habracut"></a></li></ul><br><h3 id="chto-takoe-shablon-proverki-rabotosposobnosti">  Apa itu Template Pemeriksaan Kesehatan? </h3><br><p>  Saat merancang aplikasi yang sangat penting dan sangat penting untuk misi, sangat penting untuk memikirkan hal seperti toleransi kesalahan.  Suatu aplikasi dianggap toleran terhadap kesalahan jika dengan cepat dikembalikan setelah kegagalan.  Aplikasi cloud yang khas menggunakan arsitektur microservice - ketika setiap komponen ditempatkan dalam wadah yang terpisah.  Dan untuk memastikan bahwa aplikasi pada k8s sangat mudah diakses, ketika Anda mendesain sebuah cluster, Anda harus mengikuti pola-pola tertentu.  Di antara mereka adalah Template Cek Kesehatan.  Ini menentukan bagaimana aplikasi melaporkan k8 tentang kinerjanya.  Ini bukan hanya informasi tentang apakah pod berfungsi, tetapi juga tentang bagaimana pod menerima permintaan dan meresponsnya.  Semakin banyak Kubernet tahu tentang kinerja pod, semakin banyak keputusan cerdas yang dibuat tentang perutean lalu lintas dan penyeimbangan muatan.  Dengan demikian, prinsip aplikasi yang dapat diamati tinggi pada waktu yang tepat untuk menanggapi permintaan. </p><br><h3 id="princip-vysokoy-nablyudaemosti-nor">  Prinsip observability tinggi (NRA) </h3><br><p> Prinsip observabilitas tinggi adalah salah satu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">prinsip merancang aplikasi kemas</a> .  Dalam arsitektur microservice, layanan tidak peduli bagaimana permintaan mereka diproses (dan memang demikian), tetapi penting bagaimana mendapatkan jawaban dari menerima layanan.  Misalnya, untuk mengautentikasi pengguna, satu wadah mengirim permintaan HTTP lain, menunggu tanggapan dalam format tertentu - itu saja.  PythonJS juga dapat menangani permintaan, dan Python Flask dapat merespons.  Wadah untuk satu sama lain seperti kotak hitam dengan konten tersembunyi.  Namun, prinsip NRA mensyaratkan bahwa setiap layanan mengungkapkan beberapa titik akhir API yang menunjukkan seberapa efisiennya, serta tingkat ketersediaan dan toleransi kesalahannya.  Kubernetes meminta metrik ini untuk memikirkan langkah selanjutnya untuk merutekan dan memuat keseimbangan. </p><br><p>  Aplikasi cloud yang dirancang dengan baik mencatat peristiwa-peristiwa utamanya menggunakan stream I / O STDERR dan STDOUT standar.  Layanan tambahan, misalnya, filebeat, logstash, atau fluentd, yang mengirimkan log ke sistem pemantauan terpusat (seperti Prometheus) dan sistem pengumpulan log (rangkaian perangkat lunak ELK), mengikuti.  Diagram di bawah ini menunjukkan bagaimana aplikasi cloud bekerja sesuai dengan Template Pemeriksaan Kesehatan dan Prinsip Observabilitas Tinggi. </p><br><p><img src="https://habrastorage.org/webt/tz/v1/oi/tzv1oixhe3wj4xdwlc08w9roo0k.jpeg"></p><br><h3 id="kak-primenit-shablon-proverki-rabotosposobnosti-v-kubernetes">  Bagaimana cara menerapkan Pola Pemeriksaan Kesehatan di Kubernetes? </h3><br><p>  Di luar kotak, k8 memantau status pod menggunakan salah satu pengontrol ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Penyebaran</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Replika</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DaemonSet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">StatefulSets</a> , dll., Dll.).  Setelah mengetahui bahwa pod telah jatuh karena suatu alasan, controller mencoba untuk me-restart atau memindahkannya ke node lain.  Namun, pod dapat melaporkan bahwa itu sudah aktif dan berjalan, sementara itu sendiri tidak berfungsi.  Berikut ini sebuah contoh: aplikasi Anda menggunakan Apache sebagai server web, Anda menginstal komponen pada beberapa pod cluster.  Karena perpustakaan tidak dikonfigurasi dengan benar, semua permintaan ke aplikasi merespons dengan kode 500 (kesalahan server internal).  Saat memeriksa pengiriman, memeriksa status polong memberikan hasil yang sukses, namun, pelanggan berpikir berbeda.  Kami menggambarkan situasi yang tidak diinginkan ini sebagai berikut: </p><br><p><img src="https://habrastorage.org/webt/zw/w-/fi/zww-fiwvktahptnfb2izazwzs6u.png"></p><br><p>  Dalam contoh kami, k8 melakukan <em>pemeriksaan kesehatan</em> .  Dalam jenis cek ini, kubelet terus-menerus memeriksa status proses dalam wadah.  Begitu dia mengerti bahwa proses telah meningkat, dia akan memulai kembali.  Jika kesalahan dihilangkan dengan hanya me-restart aplikasi, dan program ini dirancang untuk mematikan ketika ada kesalahan, maka untuk mengikuti NRA dan Template Pemeriksaan Kesehatan, proses pemeriksaan kesehatan sudah cukup.  Sangat disayangkan bahwa tidak semua kesalahan dihilangkan dengan memulai kembali.  Untuk kasus ini, k8s menawarkan 2 cara yang lebih dalam untuk memecahkan masalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pod</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">livenessProbe</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">readinessProbe</a> . </p><br><h3 id="livenessprobe">  LiveProbe </h3><br><p>  Selama liveProbe <strong>,</strong> kubelet melakukan 3 jenis pemeriksaan: ia tidak hanya mengetahui apakah pod berfungsi, tetapi apakah siap menerima dan merespons permintaan: </p><br><ul><li>  Setel permintaan HTTP ke pod.  Respons harus berisi kode respons HTTP dalam kisaran 200 hingga 399. Dengan demikian, kode 5xx dan 4xx menunjukkan bahwa pod memiliki masalah, bahkan jika proses sedang berjalan. </li><li>  Untuk memeriksa pod dengan layanan non-HTTP (misalnya, server surat Postfix), Anda perlu membuat koneksi TCP. </li><li>  Eksekusi perintah arbitrer untuk pod (secara internal).  Verifikasi dianggap berhasil jika kode keluar perintah adalah 0. </li></ul><br><p>  Contoh cara kerjanya.  Definisi pod berikut ini berisi aplikasi NodeJS yang memberikan kesalahan 500 untuk permintaan HTTP. Untuk memastikan bahwa kontainer dimulai ulang setelah menerima kesalahan seperti itu, kami menggunakan parameter livenessProbe: </p><br><pre><code class="plaintext hljs">apiVersion: v1 kind: Pod metadata: name: node500 spec: containers: - image: magalix/node500 name: node500 ports: - containerPort: 3000 protocol: TCP livenessProbe: httpGet: path: / port: 3000 initialDelaySeconds: 5</code> </pre> <br><p>  Ini tidak berbeda dari definisi <code>.spec.containers.livenessProbe</code> lainnya, tetapi kami menambahkan objek <code>.spec.containers.livenessProbe</code> .  Parameter <code>httpGet</code> menerima jalur di mana permintaan HTTP GET dikirim (dalam contoh kami, ini adalah <code>/</code> , tetapi dalam skenario pertempuran mungkin juga ada sesuatu seperti <code>/api/v1/status</code> ).  Masih livenessProbe menerima parameter <code>initialDelaySeconds</code> , yang menginstruksikan operasi validasi untuk menunggu sejumlah detik tertentu.  Penundaan diperlukan karena wadah perlu waktu untuk memulai, dan ketika dinyalakan kembali tidak akan tersedia untuk sementara waktu. </p><br><p>  Untuk menerapkan pengaturan ini ke sebuah cluster, gunakan: </p><br><pre> <code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Setelah beberapa detik, Anda dapat memeriksa konten pod dengan perintah berikut: </p><br><pre> <code class="plaintext hljs">kubectl describe pods node500</code> </pre> <br><p>  Temukan yang berikut di akhir output. </p><br><p>  Seperti yang Anda lihat, livenessProbe memprakarsai permintaan GET HTTP, wadah menghasilkan kesalahan 500 (yang diprogram untuk), kubelet memulainya kembali. </p><br><p>  Jika Anda tertarik dengan bagaimana aplikasi NideJS diprogram, berikut adalah app.js dan Dockerfile yang digunakan: </p><br><p>  app.js </p><br><pre> <code class="plaintext hljs">var http = require('http'); var server = http.createServer(function(req, res) { res.writeHead(500, { "Content-type": "text/plain" }); res.end("We have run into an error\n"); }); server.listen(3000, function() { console.log('Server is running at 3000') })</code> </pre> <br><p>  Dockerfile </p><br><pre> <code class="plaintext hljs">FROM node COPY app.js / EXPOSE 3000 ENTRYPOINT [ "node","/app.js" ]</code> </pre> <br><p>  Penting untuk memperhatikan hal ini: livenessProbe akan memulai kembali wadah hanya jika terjadi kegagalan.  Jika restart tidak memperbaiki kesalahan yang mengganggu pengoperasian wadah, kubelet tidak akan dapat mengambil langkah-langkah untuk menghilangkan kerusakan. </p><br><h3 id="readinessprobe">  kesiapanProbe </h3><br><p>  readinessProbe bekerja mirip dengan livenessProbe (MENDAPATKAN permintaan, komunikasi TCP dan eksekusi perintah), dengan pengecualian tindakan pemecahan masalah.  Wadah tempat kegagalan dicatat tidak dimulai kembali, tetapi diisolasi dari lalu lintas masuk.  Bayangkan salah satu wadah melakukan banyak perhitungan atau berada di bawah beban berat, yang meningkatkan waktu respons untuk permintaan.  Dalam kasus livenessProbe, pemeriksaan ketersediaan respons dipicu (melalui parameter periksa timeoutSeconds), setelah itu kubelet memulai kembali wadah.  Ketika diluncurkan, wadah mulai melakukan tugas-tugas yang intensif sumber daya dan dimulai kembali.  Ini bisa sangat penting untuk aplikasi yang peduli dengan kecepatan respons.  Misalnya, sebuah mobil tepat di jalan menunggu respons dari server, responsnya tertunda - dan mobil mogok. </p><br><p>  Mari kita menulis definisi readinessProbe yang menetapkan waktu respons untuk permintaan GET menjadi tidak lebih dari dua detik, dan aplikasi akan menanggapi permintaan GET dalam 5 detik.  File pod.yaml akan terlihat seperti ini: </p><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod metadata: name: nodedelayed spec: containers: - image: afakharany/node_delayed name: nodedelayed ports: - containerPort: 3000 protocol: TCP readinessProbe: httpGet: path: / port: 3000 timeoutSeconds: 2</code> </pre> <br><p>  Perluas pod dengan kubectl: </p><br><pre> <code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Tunggu beberapa detik, dan kemudian lihat bagaimana kesiapanProbe bekerja: </p><br><pre> <code class="plaintext hljs">kubectl describe pods nodedelayed</code> </pre> <br><p>  Di akhir kesimpulan, Anda dapat melihat bahwa beberapa peristiwa mirip <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dengan ini</a> . </p><br><p>  Seperti yang Anda lihat, kubectl tidak memulai ulang pod ketika waktu pemindaian melebihi 2 detik.  Sebaliknya, ia membatalkan permintaan itu.  Koneksi masuk dialihkan ke pod lain yang berfungsi. </p><br><p>  Catatan: sekarang setelah beban tambahan telah dihapus dari pod, kubectl mengirimkan permintaan lagi: respons terhadap permintaan GET tidak lagi tertunda. </p><br><p>  Sebagai perbandingan: berikut ini adalah file app.js yang dimodifikasi: </p><br><pre> <code class="plaintext hljs">var http = require('http'); var server = http.createServer(function(req, res) { const sleep = (milliseconds) =&gt; { return new Promise(resolve =&gt; setTimeout(resolve, milliseconds)) } sleep(5000).then(() =&gt; { res.writeHead(200, { "Content-type": "text/plain" }); res.end("Hello\n"); }) }); server.listen(3000, function() { console.log('Server is running at 3000') })</code> </pre> <br><p>  <strong>TL; DR</strong> <br>  Sebelum munculnya aplikasi berbasis cloud, log adalah sarana utama untuk memantau dan memeriksa status aplikasi.  Namun, tidak ada cara untuk mengambil langkah pemecahan masalah.  Log berguna hari ini, mereka harus dikumpulkan dan dikirim ke sistem perakitan log untuk analisis situasi darurat dan pengambilan keputusan.  [ <em>semua ini bisa dilakukan tanpa aplikasi cloud menggunakan monit, misalnya, tetapi dengan k8 menjadi lebih mudah :) - Ed.</em>  ] </p><br><p>  Saat ini, koreksi harus dilakukan hampir secara real time, jadi aplikasi seharusnya tidak lagi menjadi kotak hitam.  Tidak, mereka harus menunjukkan titik akhir yang memungkinkan sistem pemantauan untuk meminta dan mengumpulkan data berharga tentang status proses sehingga mereka dapat merespons secara instan jika perlu.  Ini disebut Template Desain Pemeriksaan Kesehatan, yang mengikuti Prinsip Observabilitas Tinggi (NRA). </p><br><p>  Kubernetes secara default menawarkan 2 jenis pemeriksaan kesehatan: readinessProbe dan livenessProbe.  Keduanya menggunakan jenis pemeriksaan yang sama (permintaan HTTP GET, komunikasi TCP, dan eksekusi perintah).  Mereka berbeda dalam keputusan apa yang dibuat dalam menanggapi masalah dalam polong.  livenessProbe me-restart wadah dengan harapan bahwa kesalahan tidak akan terulang, dan readinessProbe mengisolasi pod dari lalu lintas masuk sampai penyebab masalah teratasi. </p><br><p>  Desain aplikasi yang tepat harus mencakup kedua jenis validasi dan bahwa mereka mengumpulkan data yang cukup, terutama ketika pengecualian dibuat.  Ini juga harus menunjukkan titik akhir API yang diperlukan yang mengirimkan metrik status kesehatan penting ke sistem pemantauan (juga disebut Prometheus). </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id467155/">https://habr.com/ru/post/id467155/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id467145/index.html">Tidak hanya berdagang: cara menggunakan pertukaran sebagai alternatif simpanan bank dan mendapatkan penghasilan</a></li>
<li><a href="../id467147/index.html">Serangan massal: fitur serangan balik pada pengalaman beberapa tahun terakhir</a></li>
<li><a href="../id467149/index.html">Antiquities: 1992 di pers komputer</a></li>
<li><a href="../id467151/index.html">Menangani Keberatan: Analisis Statis Akan Menghabiskan Waktu Kerja</a></li>
<li><a href="../id467153/index.html">Bekerja dengan keberatan: analisis statis akan mengambil bagian dari waktu kerja</a></li>
<li><a href="../id467161/index.html">Aplikasi web di Kotlin + Spring Boot + Vue.js</a></li>
<li><a href="../id467163/index.html">Cara bermigrasi ke cloud dalam dua jam berkat Kubernetes dan otomatisasi</a></li>
<li><a href="../id467165/index.html">Mengikuti jejak gerakan Scala Rusia. Bagian 2</a></li>
<li><a href="../id467169/index.html">Pelajaran yang diperoleh dari pengujian Lebih dari 200.000 baris Kode Infrastruktur</a></li>
<li><a href="../id467171/index.html">Apa yang saya pelajari dengan menguji 200.000 baris kode infrastruktur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>