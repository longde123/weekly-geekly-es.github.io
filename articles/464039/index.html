<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüë¶ üêπ ü§òüèø Redes neuronales y aprendizaje profundo: un tutorial en l√≠nea, cap√≠tulo 6, parte 2: progreso reciente en el reconocimiento de im√°genes üï∫üèæ üë´ üë©üèº‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Contenido 

- Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano 
- Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y aprendizaje profundo: un tutorial en l√≠nea, cap√≠tulo 6, parte 2: progreso reciente en el reconocimiento de im√°genes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/464039/"><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: mejorar el m√©todo de entrenamiento de redes neuronales</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: ¬øPor qu√© la regularizaci√≥n ayuda a reducir el reciclaje?</a> <br></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3: ¬øc√≥mo elegir hiperpar√°metros de red neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 4: prueba visual de que las redes neuronales son capaces de calcular cualquier funci√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 5: ¬øpor qu√© las redes neuronales profundas son tan dif√≠ciles de entrenar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: progreso reciente en el reconocimiento de im√°genes</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ep√≠logo: ¬øexiste un algoritmo simple para crear inteligencia?</a> </li></ul></div></div><br>  En 1998, cuando apareci√≥ la base de datos MNIST, tom√≥ semanas entrenar las computadoras m√°s avanzadas, lo que logr√≥ resultados mucho peores que las computadoras de hoy, que tardan menos de una hora en llegar a la GPU.  Por lo tanto, MNIST ya no es una tarea que empuja los l√≠mites de la tecnolog√≠a;  La velocidad del entrenamiento sugiere que esta tarea es adecuada para estudiar esta tecnolog√≠a.  Mientras tanto, la investigaci√≥n va m√°s all√° y el trabajo moderno estudia problemas mucho m√°s complejos.  En esta secci√≥n, describir√© brevemente algunos ejemplos de trabajos en curso relacionados con el reconocimiento de im√°genes utilizando redes neuronales. <br><br>  Esta secci√≥n es diferente del resto del libro.  En el libro, me concentr√© en ideas presumiblemente de larga vida: propagaci√≥n hacia atr√°s, regularizaci√≥n, redes convolucionales.  Trat√© de evitar los resultados considerados de moda al momento de escribir, cuyo valor a largo plazo parec√≠a dudoso.  En la ciencia, estos resultados suelen ser ef√≠meros, desaparecen r√°pidamente y no tienen un efecto a largo plazo.  Ante esto, el esc√©ptico dir√≠a: ‚ÄúPor supuesto, el progreso reciente en el reconocimiento de im√°genes puede considerarse un ejemplo de tal viaje de un d√≠a?  En dos o tres a√±os, todo cambiar√°.  Entonces, ¬øes probable que estos resultados sean de inter√©s para un peque√±o n√∫mero de profesionales que compiten en primer plano?  ¬øPor qu√© discutirlos en absoluto? <br><a name="habracut"></a><br>  Tal esc√©ptico tendr√° raz√≥n en que los peque√±os detalles de trabajos recientes est√°n perdiendo gradualmente importancia percibida.  Sin embargo, en los √∫ltimos a√±os ha habido mejoras incre√≠bles en la resoluci√≥n de problemas particularmente complejos de reconocimiento de im√°genes utilizando redes neuronales profundas (GNS).  Imagine un historiador de la ciencia escribiendo material sobre visi√≥n por computadora en 2100.  Definir√°n 2011-2015 (y probablemente varios a√±os despu√©s de eso) como un per√≠odo de avances significativos impulsados ‚Äã‚Äãpor redes de convoluci√≥n profunda (GSS).  Esto no significa que el GOS se seguir√° utilizando en 2100, sin mencionar detalles como la excepci√≥n, ReLU y m√°s.  Pero todo esto significa que hay una transici√≥n importante en la historia de las ideas en el momento actual.  Esto es similar a observar el descubrimiento del √°tomo, la invenci√≥n de los antibi√≥ticos: la invenci√≥n y el descubrimiento de proporciones hist√≥ricas.  Por lo tanto, sin entrar en detalles, vale la pena hacerse una idea de los interesantes descubrimientos que se est√°n haciendo hoy. <br><br><h3>  Trabajo 2012 LRMD </h3><br>  Perm√≠tanme comenzar con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo de 2012,</a> escrito por un grupo de investigadores de Stanford y Google.  La llamar√© LRMD, por las primeras letras de los nombres de los primeros cuatro autores.  LRMD us√≥ NS para clasificar im√°genes de la base de datos ImageNet, que es una tarea muy dif√≠cil de reconocimiento de patrones.  Los datos que utilizaron en ImageNet 2011 incluyeron 16 millones de im√°genes a todo color, divididas en 20,000 categor√≠as.  Las im√°genes fueron descargadas de Internet y clasificadas por Mechanical Turk de Amazon.  Aqu√≠ hay algunos de ellos: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/323/b71/89a/323b7189a538cc4532b293b48f587cca.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bed/423/c27/bed423c2789cb88297bf8946f1e92e82.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/855/3e5/38d/8553e538d4514742d4ebf554eae7c490.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/326/bc0/f49/326bc0f4920430fb06490cab8bb8eeeb.jpg"><br><br>  Pertenecen a las categor√≠as, respectivamente: paperas, hongo de ra√≠z marr√≥n, leche pasteurizada, lombrices intestinales.  Si desea hacer ejercicio, le recomiendo que visite la lista de herramientas manuales de ImagNet, donde se hacen diferencias entre mont√≠culos, cepilladores, cepilladores para biselar y docenas de otros tipos de cepilladores, sin mencionar otras categor√≠as.  No s√© sobre usted, pero no puedo distinguir con certeza todas estas herramientas.  ¬°Esto es obviamente mucho m√°s desafiante que MNIST!  La red LRMD obtuvo un resultado decente con una precisi√≥n de reconocimiento de imagen del 15.8% de ImageNet.  Esto puede no parecer un resultado tan impresionante, pero fue una gran mejora sobre el resultado anterior de 9.3%.  Tal salto sugiere que los NS pueden ofrecer un enfoque efectivo para tareas de reconocimiento de im√°genes muy complejas, como ImageNet. <br><br><h3>  Trabajo 2012 KSH </h3><br>  El trabajo de LRMD en 2012 fue seguido por el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo de</a> Krizhevsky, Sutskever y Hinton (KSH).  KSH entren√≥ y prob√≥ GSS utilizando un subconjunto limitado de datos ImagNet.  Este subconjunto est√° definido por la popular competencia de aprendizaje autom√°tico: el desaf√≠o de reconocimiento visual a gran escala ImageNet (ILSVRC).  El uso de este subconjunto les dio una manera conveniente de comparar su enfoque con otras t√©cnicas l√≠deres.  El conjunto ILSVRC 2012 contiene aproximadamente 1.2 millones de im√°genes de 1000 categor√≠as.  Los conjuntos de verificaci√≥n y confirmaci√≥n contienen 150,000 y 50,000 im√°genes, respectivamente, de las mismas 1000 categor√≠as. <br><br>  Uno de los desaf√≠os de la competencia ILSVRC es que muchas im√°genes de ImageNet contienen m√∫ltiples objetos.  Por ejemplo, en la imagen, el Labrador Retriever corre tras un bal√≥n de f√∫tbol.  T.N.  La clasificaci√≥n "correcta" de ILSVRC puede corresponder a la etiqueta del Labrador Retriever.  ¬øEs necesario seleccionar puntos del algoritmo si marca la imagen como un bal√≥n de f√∫tbol?  Debido a tal ambig√ºedad, el algoritmo se consider√≥ correcto si la clasificaci√≥n de ImageNet se encontraba entre las 5 conjeturas m√°s probables del algoritmo con respecto al contenido de la imagen.  Seg√∫n este criterio, de los 5 primeros, el GSS de KSH logr√≥ una precisi√≥n del 84.7%, mucho mejor que el oponente anterior, que logr√≥ una precisi√≥n del 73.8%.  Usando una m√©trica m√°s rigurosa, cuando la etiqueta debe coincidir exactamente con la prescrita, la precisi√≥n de KSH alcanz√≥ el 63.3%. <br><br>  Vale la pena describir brevemente la red KSH, ya que inspir√≥ tantos trabajos que siguieron.  Tambi√©n, como veremos, est√° estrechamente relacionado con las redes que hemos entrenado en este cap√≠tulo, aunque es m√°s complejo.  KSH utiliz√≥ GSS entrenado en dos GPU.  Usaron dos GPU porque su tarjeta particular (NVIDIA GeForce GTX 580) no ten√≠a suficiente memoria para almacenar toda la red.  Por lo tanto, dividen la red en dos partes. <br><br>  La red KSH tiene 7 capas de neuronas ocultas.  Las primeras cinco capas ocultas son convolucionales (algunas usan la agrupaci√≥n m√°xima), y las siguientes 2 est√°n completamente conectadas.  La capa softmax de salida consta de 1000 neuronas correspondientes a 1000 clases de im√°genes.  Aqu√≠ hay un boceto de la red, tomado del trabajo de KSH.  Los detalles se describen a continuaci√≥n.  Tenga en cuenta que muchas capas se dividen en 2 partes correspondientes a dos GPU. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/986/143/0bb/9861430bb7c8c79540298a2db5122ec2.jpg"><br><br>  En la capa de entrada hay una neurona 3x224x224 que indica los valores RGB para una imagen de tama√±o 224x224.  Recuerde que ImageNet contiene im√°genes de varias resoluciones.  Esto presenta un problema, ya que la capa de red de entrada suele ser de un tama√±o fijo.  KSH se ocup√≥ de esto escalando cada imagen para que su lado corto tuviera una longitud de 256 p√≠xeles.  Luego cortaron un √°rea de 256x256 p√≠xeles desde el centro de la imagen redimensionada.  Finalmente, KSH recupera piezas aleatorias de im√°genes de 224x224 (y sus reflejos horizontales) de im√°genes de 256x256.  Este corte aleatorio es una forma de expandir los datos de entrenamiento para reducir el reciclaje.  Esto ayuda especialmente a entrenar una red tan grande como KSH.  Y finalmente, estas im√°genes de 224x224 se utilizan como entrada a la red.  En la mayor√≠a de los casos, la imagen recortada contiene el objeto principal de la imagen original. <br><br>  Pasamos a las capas ocultas de la red KSH.  La primera capa oculta es convolucional, con un paso de extracci√≥n m√°xima.  Utiliza campos receptivos locales de tama√±o 11x11 y un paso de 4 p√≠xeles.  En total, se obtienen 96 tarjetas de caracter√≠sticas.  Las cartas de personaje se dividen en dos grupos de 48 piezas, con las primeras 48 cartas en una GPU y la segunda en la otra.  La agrupaci√≥n m√°xima en esta y las capas subsiguientes se realiza mediante secciones de 3x3, pero las secciones de agrupaci√≥n pueden solaparse y est√°n ubicadas a una distancia de solo 2 p√≠xeles entre s√≠. <br><br>  La segunda capa oculta tambi√©n es convolucional, con agrupaci√≥n m√°xima.  Utiliza campos receptivos locales de 5x5 y tiene 256 tarjetas de funciones, divididas en 128 piezas para cada GPU.  Los mapas de funciones usan solo 48 canales entrantes, y no todas las 96 salidas de la capa anterior, como de costumbre.  Esto se debe a que cualquier tarjeta de funci√≥n recibe informaci√≥n de la GPU en la que est√° almacenada.  En este sentido, la red se est√° alejando de la arquitectura convolucional que describimos anteriormente en este cap√≠tulo, aunque, obviamente, la idea b√°sica sigue siendo la misma. <br><br>  Las capas tercera, cuarta y quinta son convolucionales, pero sin agrupaci√≥n m√°xima.  Sus par√°metros: (3) 384 mapas de caracter√≠sticas, campos receptivos locales 3x3, 256 canales entrantes;  (4) 384 mapas de caracter√≠sticas, campos receptivos locales 3x3, 192 canales entrantes;  (5) 256 tarjetas de funciones, campos receptivos locales 3x3, 192 canales entrantes.  En la tercera capa, los datos se intercambian entre las GPU (como se muestra en la imagen) para que los mapas de caracter√≠sticas puedan usar los 256 canales entrantes. <br><br>  Las capas ocultas sexta y s√©ptima est√°n completamente conectadas, 4096 neuronas cada una. <br><br>  La capa de salida es softmax, consta de 1000 unidades. <br><br>  La red KSH aprovecha muchas t√©cnicas.  En lugar de utilizar la tangente sigmoidea o hiperb√≥lica como funci√≥n de activaci√≥n, utiliza ReLU, que acelera enormemente el aprendizaje.  La red KSH contiene alrededor de 60 millones de par√°metros de entrenamiento y, por lo tanto, incluso con un gran conjunto de datos de entrenamiento, est√° sujeta a reciclaje.  Para hacer frente a esto, los autores ampliaron el conjunto de capacitaci√≥n recortando im√°genes al azar, como se describi√≥ anteriormente.  Luego usaron la variante de regularizaci√≥n L2 y la excepci√≥n.  La red fue entrenada usando el descenso de gradiente estoc√°stico basado en el impulso y con mini paquetes. <br><br>  Esta es una breve descripci√≥n de muchas de las ideas clave de KSH.  Omit√≠ algunos detalles; b√∫scalos en el art√≠culo t√∫ mismo.  Tambi√©n puede ver el proyecto de Alex Krizhevsky <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuda-convnet</a> (y sus seguidores), que contiene c√≥digo que implementa muchas de las ideas descritas.  Tambi√©n se ha <a href="">desarrollado una</a> versi√≥n de esta red <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">basada en Theano</a> .  Puede reconocer ideas en el c√≥digo que son similares a las que desarrollamos en este cap√≠tulo, aunque el uso de m√∫ltiples GPU complica las cosas.  El marco Caffe tiene su propia versi√≥n de la red KSH, consulte sus " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">modelos de zool√≥gico</a> " para m√°s detalles. <br><br><h3>  Concurso ILSVRC 2014 </h3><br>  Desde 2012, el progreso ha sido bastante r√°pido.  Asista a la competencia ILSVRC 2014.  Como en 2012, los participantes tuvieron que entrenar redes para 1.2 millones de im√°genes de 1000 categor√≠as, y una de las 5 predicciones probables en la categor√≠a correcta era un criterio de calidad.  <a href="">El equipo ganador</a> , compuesto principalmente por empleados de Google, utiliz√≥ el GSS con 22 capas de neuronas.  Llamaron a su red GoogLeNet, despu√©s de LeNet-5.  GoogLeNet logr√≥ una precisi√≥n del 93,33% seg√∫n los criterios de las cinco mejores opciones, lo que mejor√≥ seriamente los resultados del ganador de 2013 (Clarifai, del 88,3%) y el ganador de 2012 (KSH, del 84,7%). <br><br>  ¬øQu√© tan buena es la precisi√≥n de GoogLeNet 93.33%?  En 2014, un equipo de investigaci√≥n escribi√≥ una <a href="">revisi√≥n de</a> la competencia ILSVRC.  Una de las cuestiones abordadas fue qu√© tan bien las personas podr√≠an hacer frente a la tarea.  Para el experimento, crearon un sistema que permite a las personas clasificar im√°genes con ILSVRC.  Como explica uno de los autores del trabajo, Andrei Karpaty, en una entrada informativa en su blog, fue muy dif√≠cil llevar la efectividad de las personas a los indicadores de GoogLeNet: <br><blockquote>  La tarea de marcar im√°genes con cinco categor√≠as de 1000 posibles r√°pidamente se volvi√≥ extremadamente dif√≠cil, incluso para aquellos de mis amigos en el laboratorio que hab√≠an estado trabajando con ILSVRC y sus categor√≠as durante alg√∫n tiempo.  Primero, quer√≠amos enviar la tarea a Amazon Mechanical Turk.  Luego decidimos intentar contratar estudiantes por dinero.  Por lo tanto, organic√© una fiesta de marcado entre expertos en mi laboratorio.  Despu√©s de eso, desarroll√© una interfaz modificada que utilizaba las predicciones de GoogLeNet para reducir el n√∫mero de categor√≠as de 1000 a 100. Sin embargo, la tarea fue dif√≠cil: la gente omiti√≥ categor√≠as, dando errores del orden del 13-15%.  Al final, me di cuenta de que para acercarme a√∫n m√°s al resultado de GoogLeNet, el enfoque m√°s efectivo ser√≠a que me sentara y pasara por un proceso de aprendizaje imposiblemente largo y el posterior proceso de marcado completo.  Al principio, el marcado fue a una velocidad del orden de 1 pieza por minuto, pero se aceler√≥ con el tiempo.  Algunas im√°genes fueron f√°ciles de reconocer, mientras que otras (por ejemplo, ciertas razas de perros, especies de p√°jaros o monos) requirieron varios minutos de concentraci√≥n.  Me hice muy bueno para distinguir entre razas de perros.  En base a mi muestra de im√°genes, se obtuvieron los siguientes resultados: GoogLeNet se equivoc√≥ en el 6,8% de los casos;  mi tasa de error fue de 5.1%, que fue aproximadamente 1.7% mejor. </blockquote><br><br>  En otras palabras, el experto, que trabaj√≥ con mucho cuidado, solo haciendo esfuerzos serios, pudo adelantarse un poco al STS.  Karpaty informa que el segundo experto, capacitado en menos im√°genes, logr√≥ reducir el error en solo un 12% al elegir hasta 5 etiquetas por imagen, que es mucho menos que GoogLeNet. <br><br>  Resultados impresionantes  Y desde la llegada de este trabajo, varios equipos han informado sobre el desarrollo de sistemas cuya tasa de error al elegir las 5 mejores etiquetas fue incluso inferior al 5,1%.  A veces, estos logros fueron cubiertos en los medios como la aparici√≥n de sistemas capaces de reconocer im√°genes mejor que las personas.  Aunque los resultados son generalmente sorprendentes, hay muchos matices que no se pueden considerar que la visi√≥n por computadora funciona mejor en estos sistemas que en los humanos.  En muchos sentidos, la competencia ILSVRC es una tarea muy limitada: los resultados de una b√∫squeda de im√°genes en una red abierta no se corresponder√°n necesariamente con lo que el programa encontrar√° en una tarea pr√°ctica.  Y, por supuesto, el criterio "una de las cinco mejores calificaciones" es bastante artificial.  Todav√≠a tenemos un largo camino por recorrer para resolver el problema del reconocimiento de im√°genes, sin mencionar la tarea m√°s general de la visi√≥n por computadora.  Pero a√∫n as√≠ es genial ver cu√°nto progreso se ha logrado en la resoluci√≥n de una tarea tan dif√≠cil en solo unos pocos a√±os. <br><br><h3>  Otras tareas </h3><br>  Me concentr√© en ImageNet, sin embargo, hay muchos otros proyectos que usan NS para el reconocimiento de im√°genes.  Perm√≠tanme describir brevemente algunos resultados interesantes obtenidos recientemente, solo para tener una idea del trabajo moderno. <br><br>  Un equipo de Google obtuvo un <a href="">conjunto</a> pr√°ctico inspirador <a href="">de resultados</a> , que aplic√≥ GSS a la tarea de reconocimiento de placa de direcci√≥n en Google Street View.  En su trabajo, informan c√≥mo descubrieron y reconocieron autom√°ticamente casi 100 millones de placas de direcciones con una precisi√≥n comparable al trabajo humano.  Y su sistema es r√°pido: ¬°fue capaz de descifrar datos de todas las im√°genes de Google Street View en Francia en menos de una hora!  Escriben: "Obtener este nuevo conjunto de datos ha aumentado significativamente la calidad de la geocodificaci√≥n de Google Maps en varios pa√≠ses, especialmente donde no hab√≠a otras fuentes de geocodificaci√≥n".  Luego hacen una declaraci√≥n m√°s general: "Creemos que, gracias a este modelo, resolvimos el problema del reconocimiento √≥ptico de secuencias cortas de una manera que es aplicable en muchas aplicaciones pr√°cticas". <br><br>  Quiz√°s cre√© la impresi√≥n de un desfile de resultados victoriosos e inspiradores.  Por supuesto, los informes m√°s interesantes se refieren a cosas fundamentales que a√∫n no nos quedan claras.  Por ejemplo, en el <a href="">trabajo de 2013 se</a> demostr√≥ que la Asamblea Nacional tiene, de hecho, puntos ciegos.  Echa un vistazo a las im√°genes a continuaci√≥n.  A la izquierda est√° la imagen de ImageNet, que la red de investigadores clasific√≥ correctamente.  A la derecha hay una imagen ligeramente modificada (en el medio se muestran las diferencias), que la red ya no pudo reconocer correctamente.  Y los autores descubrieron que dichos cambios "adversarios" se pueden seleccionar para cualquier imagen de la base de datos, y no solo para la √©lite. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ddf/c77/f29/ddfc77f2996c1fb482fa448f1384d08a.jpg"><br><br>  Resultado desagradable.  Usamos una red basada en el mismo c√≥digo que la red KSH, es decir, es una red que se usa cada vez m√°s.  Y aunque tales NS calculan, en principio, funciones continuas, resultados similares sugieren que probablemente calculen funciones casi discretas.  Peor a√∫n, resultan ser discretos de una manera que viola nuestra noci√≥n intuitiva de comportamiento inteligente.  Esto es un problema  Adem√°s, no est√° muy claro qu√© conduce exactamente a la discreci√≥n, cu√°l es el problema: ¬øen la funci√≥n de p√©rdida?  ¬øQu√© funciones de activaci√≥n usar?  ¬øEn arquitectura de red?  En otra cosa?  No lo sabemos <br><br>  Pero estos resultados no son tan malos como parecen.  Aunque tales cambios adversos son bastante comunes, es poco probable que se encuentren en la pr√°ctica.  Como se indica en el trabajo: <br><blockquote>  La existencia de negativos adversos contradice la capacidad de la red para lograr una alta generalizaci√≥n.  De hecho, si la red pudiera generalizarse bien, ¬øc√≥mo podr√≠a ser enga√±ada por tales negativas adversarias que no se pueden distinguir de los ejemplos ordinarios?  La explicaci√≥n es que un conjunto de negativos competitivos tiene una probabilidad extremadamente baja y, por lo tanto, no se observa (o casi no se observa) en el conjunto de datos de entrenamiento, sin embargo, tiene una alta densidad (aproximadamente como n√∫meros racionales) y, por lo tanto, se puede encontrar en casi cualquier caso . </blockquote><br><br>  Sin embargo, es desagradable que comprendamos tan mal el trabajo de la Asamblea Nacional que este resultado se descubri√≥ recientemente.  Por supuesto, la principal ventaja de tales resultados ser√° que estimularon la aparici√≥n de trabajos posteriores sobre este tema.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabajo reciente en 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mostr√≥ que es posible que una red capacitada cree im√°genes que parezcan ruido blanco para una persona, y la red las clasificar√° en categor√≠as conocidas con un alto grado de confianza. Esta es otra demostraci√≥n que todav√≠a tenemos mucho que entender en el trabajo del NS y en su uso para el reconocimiento de im√°genes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero, a pesar de la presencia de resultados similares, el panorama general es inspirador. </font><font style="vertical-align: inherit;">Estamos viendo un r√°pido progreso en la realizaci√≥n de pruebas extremadamente complejas como ImageNet. </font><font style="vertical-align: inherit;">Tambi√©n estamos viendo un r√°pido progreso en la resoluci√≥n de problemas del mundo real, como el reconocimiento de placas de direcciones en StreetView. </font><font style="vertical-align: inherit;">Pero, a pesar de la inspiraci√≥n, no basta con observar mejoras en el rendimiento de las pruebas de velocidad o incluso en las tareas del mundo real. </font><font style="vertical-align: inherit;">Hay fen√≥menos fundamentales, cuya esencia todav√≠a no entendemos bien, por ejemplo, la existencia de im√°genes competitivas. </font><font style="vertical-align: inherit;">Y si bien estos problemas fundamentales a√∫n se abren (sin mencionar la soluci√≥n de ellos), ser√≠a prematuro hablar sobre abordar la soluci√≥n del problema de reconocimiento de im√°genes. </font><font style="vertical-align: inherit;">Pero al mismo tiempo, estos problemas son excelentes incentivos para seguir trabajando.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Otros enfoques para redes neuronales profundas </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En este libro, nos centramos en una tarea: la clasificaci√≥n de n√∫meros MNIST. Una tarea excelente que nos hizo comprender muchas ideas efectivas: descenso de gradiente estoc√°stico, propagaci√≥n hacia atr√°s, redes convolucionales, regularizaci√≥n, etc. Sin embargo, esta tambi√©n es una tarea bastante limitada. Despu√©s de leer la literatura sobre redes neuronales, se encontrar√° con muchas ideas que no discutimos: NS recurrentes, m√°quinas de Boltzmann, modelos generativos, transferencia de entrenamiento, aprendizaje reforzado, y as√≠ sucesivamente. Las redes neuronales son un √°rea extensa. Sin embargo, muchas ideas importantes son variaciones de esas ideas que ya hemos discutido, y son bastante f√°ciles de entender. En esta secci√≥n, abrir√© ligeramente la cortina sobre estas vastas extensiones. Su discusi√≥n no ser√≠a detallada y exhaustiva; esto inflar√≠a extremadamente el libro. Ser√° impresionistaun intento de mostrar la riqueza conceptual de esta √°rea y conectar algunos conceptos con los que ya hemos visto. En el texto dar√© varias referencias a otras fuentes, en cuanto a materiales para capacitaci√≥n adicional. Por supuesto, muchos de ellos pronto ser√°n reemplazados por otros, y es posible que desee buscar literatura m√°s reciente. Sin embargo, creo que muchas ideas b√°sicas seguir√°n siendo interesantes durante mucho tiempo.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Redes neuronales recurrentes (RNS) </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En las redes de propagaci√≥n directa que utilizamos, hay una entrada que determina completamente la activaci√≥n de todas las neuronas en las capas posteriores. </font><font style="vertical-align: inherit;">Esta es una imagen muy est√°tica: todo en la red es fijo y tiene un car√°cter cristalino congelado. </font><font style="vertical-align: inherit;">Pero supongamos que permitimos que los elementos de la red cambien din√°micamente. </font><font style="vertical-align: inherit;">Por ejemplo, el comportamiento de las neuronas ocultas puede determinarse no solo por activaciones en capas anteriores, sino tambi√©n por activaciones que ocurrieron antes en el tiempo. </font><font style="vertical-align: inherit;">La activaci√≥n de una neurona puede determinarse parcialmente por su activaci√≥n anterior. </font><font style="vertical-align: inherit;">En redes con distribuci√≥n directa, esto claramente no sucede. </font><font style="vertical-align: inherit;">O, tal vez, la activaci√≥n de las neuronas ocultas y de salida estar√° determinada no solo por la entrada actual a la red, sino tambi√©n por las anteriores.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Las redes neuronales con este tipo de comportamiento variable en el tiempo se conocen como redes neuronales recurrentes o RNS. Hay muchas formas de formalizar matem√°ticamente la descripci√≥n informal del p√°rrafo anterior. Puedes hacerte una idea al leer el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo de Wikipedia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Al momento de escribir este art√≠culo, en la versi√≥n en ingl√©s del art√≠culo, se describen al menos 13 modelos diferentes [en el momento de la traducci√≥n en 2019, ya 18 / aprox.</font></font> transl.].<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero, si dejamos de lado los detalles matem√°ticos, entonces la idea general del RNS es la presencia de cambios din√°micos en la red que ocurren con el tiempo. Y, como era de esperar, son especialmente √∫tiles para analizar datos o procesos que cambian con el tiempo. Tales datos y procesos aparecen naturalmente en tareas como el an√°lisis del habla o el lenguaje natural. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una de las formas actuales de usar RNS es integrar mejor las redes neuronales con los m√©todos tradicionales de representaci√≥n de algoritmos, con conceptos como una m√°quina de Turing y lenguajes de programaci√≥n comunes. En el </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">trabajo desde 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNS fue desarrollado, capaz de aceptar una descripci√≥n letra por letra de un programa python muy simple, y predecir el resultado de su trabajo. Hablando informalmente, la red est√° aprendiendo a "comprender" ciertos programas de Python. </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El segundo trabajo de 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> utiliz√≥ el RNS como punto de partida para el desarrollo de la neurom√°quina de Turing (BDC). Esta es una computadora universal, cuya estructura completa puede ser entrenada usando el gradiente de descenso. Entrenaron a su BDC para construir algoritmos para varias tareas simples, como ordenar o copiar.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estos, por supuesto, son modelos de juguete muy simples. ¬°Aprender a ejecutar un programa en python como print (398345 + 42598) no convierte a una red neuronal en un int√©rprete completo del lenguaje! No est√° claro qu√© tan fuertes ser√°n estas ideas. Sin embargo, los resultados son bastante interesantes. Hist√≥ricamente, las redes neuronales hicieron un buen trabajo al reconocer patrones que tropezaron con los enfoques algor√≠tmicos convencionales. Y viceversa, los enfoques algor√≠tmicos convencionales hacen un buen trabajo resolviendo problemas que son complejos para NS. ¬°Hoy, nadie est√° tratando de implementar un servidor web o una base de datos basada en NS! Ser√≠a genial desarrollar modelos integrados que integren las fortalezas de los enfoques algor√≠tmicos tradicionales y NS. RNS, y las ideas inspiradas por ellos, pueden ayudarnos a hacer esto.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En los √∫ltimos a√±os, RNS se ha utilizado para resolver muchos otros problemas. Eran especialmente √∫tiles en el reconocimiento de voz. Los enfoques basados ‚Äã‚Äãen RNS </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">establecen r√©cords</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> de calidad de reconocimiento de fonemas. Tambi√©n se </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">utilizaron para desarrollar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> modelos mejorados del lenguaje utilizado por las personas. Los modelos de lenguaje mejorados ayudan a reconocer ambig√ºedades en el habla que suenan similares. Un buen modelo de lenguaje puede decirnos que la frase "hacia adelante hasta el infinito" es mucho m√°s probable que la frase "hacia adelante sin extremidades", aunque suenen similares. RNS se utiliz√≥ para obtener logros r√©cord en ciertas pruebas de idiomas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este trabajo es parte del uso m√°s amplio de NS de todo tipo, no solo de RNS, para resolver el problema del reconocimiento de voz. Por ejemplo, un enfoque basado en GNS ha mostrado </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">excelentes resultados</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> al reconocer el habla continua con un vocabulario amplio. Otro </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sistema</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> basado en GNS </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">se </font></a></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementa</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en el sistema operativo Android de Google.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Habl√© un poco sobre lo que los RNC son capaces de hacer, pero no expliqu√© c√≥mo funcionan. </font><font style="vertical-align: inherit;">Es posible que no se sorprenda al saber que muchas de las ideas del mundo de las redes de distribuci√≥n directa tambi√©n se pueden utilizar en RNS. </font><font style="vertical-align: inherit;">En particular, podemos entrenar el RNS modificando el descenso del gradiente y la propagaci√≥n hacia atr√°s en la frente. </font><font style="vertical-align: inherit;">Muchas otras ideas utilizadas en redes de distribuci√≥n directa, desde t√©cnicas de regularizaci√≥n hasta convoluci√≥n y activaci√≥n y funciones de costo, tambi√©n ser√°n √∫tiles. </font><font style="vertical-align: inherit;">Adem√°s, muchas de las ideas que desarrollamos como parte del libro se pueden adaptar para su uso en el RNS.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> M√≥dulos de memoria a corto plazo (DCT) a largo plazo </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uno de los problemas de RNS es que los primeros modelos eran muy dif√≠ciles de entrenar, m√°s complicados que incluso GNS. La raz√≥n fueron los problemas del gradiente inestable, que discutimos en el Cap√≠tulo 5. Recuerde que la manifestaci√≥n habitual de este problema es que el gradiente disminuye todo el tiempo cuando se propaga a trav√©s de las capas en la direcci√≥n opuesta. Esto ralentiza extremadamente el aprendizaje de las primeras capas. En RNS, este problema empeora a√∫n m√°s, ya que los gradientes se propagan no solo en la direcci√≥n opuesta a lo largo de las capas, sino tambi√©n en la direcci√≥n opuesta en el tiempo. Si la red funciona durante un tiempo bastante largo, el gradiente puede volverse extremadamente inestable y, en consecuencia, ser√° muy dif√≠cil de aprender. Afortunadamente, una idea conocida como m√≥dulos de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">memoria a corto plazo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (DCT) a </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">largo plazo</font></a><font style="vertical-align: inherit;"> se puede incluir en el RNS </font><font style="vertical-align: inherit;">. Por primera vez, los m√≥dulos introducidos</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hochreiter y Schmidguber en 1997</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , espec√≠ficamente para ayudar a resolver el problema de un gradiente inestable. </font><font style="vertical-align: inherit;">Las DCT hacen que sea m√°s f√°cil obtener buenos resultados en el aprendizaje de RNS, y muchos trabajos recientes (incluidos los que ya he mencionado) usan DCT o ideas similares.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Redes de confianza profunda, modelos generativos y m√°quinas Boltzmann </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hoy en d√≠a, el inter√©s en el aprendizaje profundo ha ganado un segundo impulso en 2006, despu√©s de la publicaci√≥n de trabajos ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) que explican c√≥mo ense√±ar un tipo especial de NS llamado red de confianza profunda (GDS). </font><font style="vertical-align: inherit;">GDS durante varios a√±os influy√≥ en el campo de la investigaci√≥n, pero luego su popularidad comenz√≥ a disminuir, y las redes de distribuci√≥n directa y los NS recurrentes se pusieron de moda. </font><font style="vertical-align: inherit;">A pesar de esto, algunas de las propiedades de GDS los hacen muy interesantes.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Primero, los GDS son un ejemplo de modelo generativo. En una red de distribuci√≥n directa, especificamos activaciones de entrada y determinan la activaci√≥n de neuronas caracter√≠sticas m√°s abajo en la red. El modelo generativo puede usarse de manera similar, pero puede establecer los valores de las neuronas en √©l y luego ejecutar la red "en la direcci√≥n opuesta", generando los valores de las activaciones de entrada. M√°s espec√≠ficamente, un GDS capacitado en im√°genes de d√≠gitos escritos a mano puede generar im√°genes similares a los d√≠gitos escritos a mano (potencialmente, y despu√©s de ciertas acciones). En otras palabras, GDM en cierto sentido puede aprender a escribir. En este sentido, los modelos generativos son similares al cerebro humano: no solo pueden leer n√∫meros, sino tambi√©n escribirlos. </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">El</font></a><font style="vertical-align: inherit;"> famoso </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dicho de </font></font></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jeffrey Hinton</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">establece que para el reconocimiento de patrones, primero debe aprender a generar im√°genes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En segundo lugar, son capaces de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aprender sin un maestro</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y casi sin un maestro. Por ejemplo, cuando se entrena en im√°genes, los GDS pueden aprender signos que son √∫tiles para comprender otras im√°genes, incluso si no hay marcas en las im√°genes de entrenamiento. La capacidad de aprender sin un maestro es extremadamente interesante tanto desde un punto de vista cient√≠fico fundamental como desde un punto de vista pr√°ctico, si se puede hacer que funcione lo suficientemente bien.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dados todos estos puntos atractivos del GDS como modelos para el aprendizaje profundo, ¬øpor qu√© su popularidad disminuy√≥? En parte debido al hecho de que otros modelos, como la distribuci√≥n directa y las redes recurrentes, han logrado resultados sorprendentes, en particular, avances en las √°reas de reconocimiento de imagen y habla. No es sorprendente que estos modelos hayan recibido tanta atenci√≥n y sean muy merecidos. Sin embargo, una conclusi√≥n desagradable se sigue de esto. El mercado de ideas a menudo funciona de acuerdo con el esquema "el ganador lo consigue todo", y casi toda la atenci√≥n se dirige a lo que est√° m√°s de moda en esta √°rea ahora. Puede ser extremadamente dif√≠cil para las personas trabajar en ideas actualmente impopulares, incluso si es obvio que pueden ser de inter√©s a largo plazo. Mi opini√≥n personal es que el GDS y otros modelos generativos merecen m√°s atenci√≥n de la que reciben.No me sorprender√° si el GDM o modelo similar supera alguna vez a los modelos populares de hoy. Leer</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este art√≠culo es</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> para una introducci√≥n al campo de GDM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este art√≠culo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tambi√©n puede ser √∫til </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">No se trata completamente de GDM, pero tiene muchas cosas √∫tiles sobre las m√°quinas de Boltzmann limitadas, un componente clave de GDM.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Otras ideas </font></font></h3><br>  ¬øQu√© m√°s est√° sucediendo en el campo de la Asamblea Nacional y la Defensa Civil?  Una gran cantidad de trabajo interesante.  Entre las √°reas activas de investigaci√≥n se encuentra el uso de NS para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">procesar</a> <a href="">lenguaje</a> natural, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducci√≥n autom√°tica</a> y aplicaciones m√°s inesperadas, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">inform√°tica musical</a> .  Hay muchas otras √°reas.  En muchos casos, despu√©s de leer este libro, podr√° comprender el trabajo reciente, aunque, por supuesto, es posible que necesite llenar algunos vac√≠os de conocimiento. <br><br>  Terminar√© esta secci√≥n con una menci√≥n de un trabajo particularmente interesante.  Ella combina redes convolucionales profundas con una t√©cnica llamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprendizaje de</a> refuerzo para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprender a jugar videojuegos</a> (y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">otro art√≠culo</a> sobre esto).  La idea es utilizar una red convolucional para simplificar los datos de p√≠xeles de la pantalla del juego, convertirlos en un conjunto m√°s simple de atributos que luego se puedan usar para tomar decisiones sobre acciones adicionales: "ir a la izquierda", "ir a la derecha", "disparar" y etc.  Particularmente interesante es que una red aprendi√≥ bastante bien a jugar siete videojuegos cl√°sicos diferentes, por delante de los expertos en tres de ellos.  Esto, por supuesto, parece un truco, y el trabajo se promocion√≥ activamente bajo el t√≠tulo "Jugar juegos de Atari con aprendizaje de refuerzo".  Sin embargo, detr√°s de un brillo superficial, vale la pena considerar el hecho de que el sistema toma datos de p√≠xeles en bruto, ni siquiera conoce las reglas del juego, y sobre su base est√° capacitado para tomar decisiones de buena calidad en varias situaciones muy diferentes y muy competitivas, cada una de las cuales tiene su propio conjunto complejo de reglas.  Bastante bien <br><br><h2>  El futuro de las redes neuronales. </h2><br><h3>  Interfaces de intenci√≥n de usuario </h3><br>  En una vieja broma, un profesor impaciente le dice a un estudiante confundido: "No escuches mis palabras, escucha lo que quiero decir".  Hist√≥ricamente, las computadoras a menudo no entend√≠an, como un estudiante confundido, lo que un usuario quiere decir.  Sin embargo, la situaci√≥n est√° cambiando.  Todav√≠a recuerdo la primera vez que me sorprend√≠ cuando escrib√≠ una solicitud por error a Google, y el motor de b√∫squeda me dijo: "¬øQuiso decir [solicitud correcta]?"  El Director de Google, Larry Page, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una vez describi√≥ el</a> motor de b√∫squeda perfecto como un sistema que comprende exactamente lo que significan sus consultas y le brinda exactamente lo que desea. <br><br>  Esta es la idea de una interfaz basada en la intenci√≥n del usuario.  En √©l, en lugar de responder a las solicitudes literales de los usuarios, el motor de b√∫squeda utilizar√° el MO para tomar una solicitud vaga del usuario, comprender exactamente lo que significa y actuar sobre esta base. <br><br>  La idea de una interfaz basada en la intenci√≥n del usuario puede aplicarse m√°s ampliamente que solo en la b√∫squeda.  Durante las pr√≥ximas d√©cadas, miles de compa√±√≠as crear√°n productos en los que MO se utilizar√° para interfaces de usuario, refiri√©ndose con calma a acciones inexactas del usuario y adivinando sus verdaderas intenciones.  Ya vemos ejemplos tempranos de tales interfaces basadas en la intenci√≥n: Apple Siri;  Wolfram Alpha;  IBM Watson  sistemas que etiquetan autom√°ticamente fotos y videos, y m√°s. <br><br>  La mayor√≠a de ellos fallar√°n.  El desarrollo de interfaces es algo complicado, y sospecho que en lugar de inspirar interfaces, muchas compa√±√≠as crear√°n interfaces sin vida sobre la base de MO.  El mejor MO del mundo no te ayudar√° si tu interfaz apesta.  Sin embargo, algunos productos tendr√°n √©xito.  Con el tiempo, esto conducir√° a un cambio serio en nuestra relaci√≥n con las computadoras.  No hace mucho tiempo, por ejemplo, en 2005, los usuarios daban por sentado que interactuar con las computadoras requiere una alta precisi√≥n.  La naturaleza literal de la computadora sirvi√≥ para difundir la idea de que las computadoras son muy literales;  El √∫nico punto y coma olvidado podr√≠a cambiar completamente la naturaleza de la interacci√≥n con la computadora.  Pero creo que en las pr√≥ximas d√©cadas desarrollaremos varias interfaces exitosas basadas en la intenci√≥n del usuario, y esto cambiar√° radicalmente nuestras expectativas al trabajar con computadoras. <br><br><h3>  Aprendizaje autom√°tico, ciencia de datos y el c√≠rculo inmaculado de innovaci√≥n </h3><br>  Por supuesto, MO no solo se usa para crear interfaces basadas en la intenci√≥n del usuario.  Otra aplicaci√≥n interesante de MO es la ciencia de datos, donde se utiliza para buscar "inc√≥gnitas conocidas" ocultas en los datos obtenidos.  Este ya es un tema de moda, sobre el cual se han escrito muchos art√≠culos, por lo que no lo extender√© por mucho tiempo.  Quiero mencionar una consecuencia de esta moda, que no se menciona a menudo: a la larga, es posible que el mayor avance en la regi√≥n de Mosc√∫ no sea solo un avance conceptual.  El mayor avance ser√° que la investigaci√≥n en el campo de MO ser√° rentable mediante el uso de datos en ciencias y otras √°reas.  Si una empresa puede invertir un d√≥lar en investigaci√≥n de MO y obtener un d√≥lar y diez centavos de ingresos con bastante rapidez, entonces se invertir√° mucho dinero en la regi√≥n MO.  En otras palabras, MO es el motor que nos impulsa a la aparici√≥n de varios grandes mercados y √°reas de crecimiento tecnol√≥gico.  Como resultado, aparecer√°n grandes equipos de personas expertas en este campo que tendr√°n acceso a recursos incre√≠bles.  Esto mover√° el MO a√∫n m√°s lejos, crear√° a√∫n m√°s mercados y oportunidades, que ser√°n el c√≠rculo inmaculado de la innovaci√≥n. <br><br><h3>  El papel de las redes neuronales y el aprendizaje profundo. </h3><br>  Describ√≠ MO en t√©rminos generales como una forma de crear nuevas oportunidades para el desarrollo tecnol√≥gico.  ¬øCu√°l ser√° el papel espec√≠fico de la Asamblea Nacional y la Sociedad Civil en todo esto? <br><br>  Para responder a la pregunta, es √∫til pasar a la historia.  En la d√©cada de 1980, hubo un avivamiento alegre y optimista asociado con las redes neuronales, especialmente despu√©s de la popularizaci√≥n de la propagaci√≥n hacia atr√°s.  Pero la recuperaci√≥n disminuy√≥, y en la d√©cada de 1990, el bast√≥n MO se transfiri√≥ a otras tecnolog√≠as, por ejemplo, el m√©todo del vector de soporte.  Hoy, la Asamblea Nacional est√° nuevamente en el caballo, estableciendo todo tipo de r√©cords y superando a muchos rivales en varios problemas.  Pero, ¬øqui√©n garantiza que ma√±ana no se desarrollar√° un nuevo enfoque que eclipsar√° nuevamente a NA?  ¬øO, tal vez, el progreso en el campo de la Asamblea Nacional comenzar√° a detenerse y nada los reemplazar√°? <br><br>  Por lo tanto, es mucho m√°s f√°cil pensar en el futuro del Ministerio de Defensa en su conjunto que espec√≠ficamente en la Asamblea Nacional.  Parte del problema es que entendemos muy mal la Asamblea Nacional.  ¬øPor qu√© NS es tan bueno compilando informaci√≥n?  ¬øC√≥mo evitan el reentrenamiento tan bien, dada la gran cantidad de opciones?  ¬øPor qu√© el descenso de gradiente estoc√°stico funciona tan bien?  ¬øQu√© tan bien funcionar√° NS al escalar conjuntos de datos?  Por ejemplo, si ampliamos la base de ImageNet 10 veces, ¬øel rendimiento del NS mejorar√° m√°s o menos que la efectividad de otras tecnolog√≠as de MO?  Todas estas son preguntas simples y fundamentales.  Y hasta ahora tenemos una comprensi√≥n muy pobre de las respuestas a estas preguntas.  En este sentido, es dif√≠cil decir qu√© papel jugar√° la Asamblea Nacional en el futuro de la Regi√≥n de Mosc√∫. <br><br>  Har√© una predicci√≥n: creo que GO no ir√° a ninguna parte.  La capacidad de estudiar jerarqu√≠as de conceptos, de construir diferentes capas de abstracciones, aparentemente, es fundamental para el conocimiento del mundo.  Esto no significa que las redes GO del ma√±ana no diferir√°n radicalmente de las de hoy.  Podemos encontrar cambios importantes en sus componentes, arquitecturas o algoritmos de aprendizaje.  Estos cambios pueden llegar a ser lo suficientemente dram√°ticos como para que dejemos de considerar los sistemas resultantes como redes neuronales.  Sin embargo, seguir√°n participando en la defensa civil. <br><br><h3>  ¬øNS y GO conducir√°n pronto a la aparici√≥n de inteligencia artificial? </h3><br>  En este libro, nos centramos en el uso de NS para resolver problemas espec√≠ficos, por ejemplo, la clasificaci√≥n de im√°genes.  Expandamos nuestras consultas: ¬øqu√© pasa con las computadoras de pensamiento de prop√≥sito general?  ¬øPueden la Asamblea Nacional y la Sociedad Civil ayudarnos a resolver el problema de crear una IA de prop√≥sito general?  Y si es as√≠, dada la alta velocidad de progreso en el campo de la defensa civil, ¬øveremos el surgimiento de la IA en el futuro cercano? <br><br>  Una respuesta detallada a tal pregunta requerir√≠a un libro separado.  En cambio, perm√≠teme ofrecerte una observaci√≥n basada en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la ley de Conway</a> : <br><blockquote>  Las organizaciones que dise√±an sistemas se limitan a un dise√±o que copia la estructura de comunicaciones de esta organizaci√≥n. </blockquote><br><br>  Es decir, por ejemplo, la ley de Conway establece que el dise√±o del avi√≥n Boeing 747 reflejar√° la estructura ampliada de Boeing y sus contratistas en el momento en que se desarroll√≥ el modelo 747. U otro ejemplo simple y concreto: considere una compa√±√≠a que est√° desarrollando un software complejo.  Si el panel de control del software debe estar conectado con el algoritmo MO, entonces el dise√±ador del panel debe comunicarse con el experto en MO de la compa√±√≠a.  La ley de Conway simplemente formaliza esta observaci√≥n. <br><br>  Por primera vez cuando escucharon la ley de Conway, muchas personas dicen "¬øNo es una evidencia com√∫n?" O "¬øEs as√≠?"  Comenzar√© con un comentario sobre su infidelidad.  Pensemos: ¬øc√≥mo se refleja la contabilidad de Boeing en el modelo 747?  ¬øQu√© pasa con el departamento de limpieza?  ¬øUn personal de alimentaci√≥n?  La respuesta es que estas partes de la organizaci√≥n probablemente no aparecen expl√≠citamente en ning√∫n otro lugar del Esquema 747.  Por lo tanto, debe comprender que la ley de Conway se aplica solo a aquellas partes de la organizaci√≥n que est√°n directamente involucradas en el dise√±o y la ingenier√≠a. <br><br>  ¬øQu√© pasa con el comentario sobre banalidad y evidencia?  Quiz√°s sea as√≠, pero no lo creo, porque las organizaciones a menudo trabajan para rechazar la ley de Conway.  Los equipos que desarrollan nuevos productos a menudo se inflan debido al n√∫mero excesivo de empleados o, por el contrario, carecen de una persona con conocimientos cr√≠ticos.  Piense en todos los productos con caracter√≠sticas in√∫tiles y complicadas.  O piense en productos con defectos obvios, por ejemplo, con una interfaz de usuario terrible.  En ambas clases de programas, a menudo surgen problemas debido a un desajuste entre el equipo necesario para lanzar un buen producto y el equipo que realmente se reuni√≥.  La ley de Conway puede ser obvia, pero eso no significa que la gente no pueda ignorarla regularmente. <br><br>  La ley de Conway es aplicable al dise√±o y la creaci√≥n de sistemas en los casos en que desde el principio imaginamos en qu√© partes constituyentes estar√° compuesto el producto y c√≥mo fabricarlos.  No se puede aplicar directamente al desarrollo de la IA, ya que la IA no es (todav√≠a) una tarea como esta: no sabemos en qu√© partes consiste.  Ni siquiera estamos seguros de qu√© preguntas b√°sicas puede hacer.  En otras palabras, en este momento, la IA es m√°s un problema de la ciencia que los ingenieros.  Imagine que necesita comenzar a desarrollar el 747 sin saber nada sobre los motores a reacci√≥n o los principios de la aerodin√°mica.  No sabr√≠a qu√© expertos contratar en su organizaci√≥n.  Como escribi√≥ Werner von Braun, "la investigaci√≥n b√°sica es lo que estoy haciendo cuando no s√© lo que estoy haciendo".  ¬øExiste una versi√≥n de la ley de Conway que se aplique a tareas que est√°n m√°s relacionadas con la ciencia que los ingenieros? <br><br>  Para encontrar la respuesta a esta pregunta, recordemos la historia de la medicina.  En los primeros d√≠as, la medicina era dominio de practicantes, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Galeno</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hip√≥crates</a> , que estudiaban todo el cuerpo humano.  Pero con el crecimiento en el volumen de nuestro conocimiento, tuve que especializarme.  Hemos descubierto muchas ideas profundas: recordar la teor√≠a microbiana de las enfermedades o comprender el principio del funcionamiento de los anticuerpos o el hecho de que el coraz√≥n, los pulmones, las venas y las arterias forman el sistema cardiovascular.  Estas ideas profundas formaron la base de disciplinas m√°s estrechas, como la epidemiolog√≠a, la inmunolog√≠a y la acumulaci√≥n de √°reas superpuestas relacionadas con el sistema cardiovascular.  As√≠ es como la estructura de nuestro conocimiento form√≥ la estructura social de la medicina.  Esto es especialmente notable en el caso de la inmunolog√≠a: la idea de la existencia de un sistema inmune digno de un estudio separado era muy poco trivial.  Por lo tanto, tenemos todo un campo de la medicina, con especialistas, conferencias, premios, etc., organizados en torno a algo que no solo es invisible, sino que tal vez ni siquiera est√° separado. <br><br>  Tal desarrollo de eventos a menudo se repiti√≥ en muchas disciplinas cient√≠ficas establecidas: no solo en medicina, sino tambi√©n en f√≠sica, matem√°ticas, qu√≠mica y otras.  Las regiones nacen monol√≠ticas, con pocas ideas profundas en existencia.  Los primeros expertos pueden cubrirlos a todos.  Pero con el tiempo, la solidez cambia.  Descubrimos muchas nuevas ideas profundas, y hay demasiadas para que alguien pueda realmente dominarlas todas.  Como resultado, la estructura social de la regi√≥n se est√° reorganizando y dividiendo, concentr√°ndose en torno a estas ideas.  En lugar de un monolito, tenemos campos divididos por campos divididos por campos: una estructura social compleja y recursiva que se refiere a s√≠ misma, cuya organizaci√≥n refleja las conexiones entre las ideas m√°s profundas.  As√≠ es como la estructura de nuestro conocimiento forma la organizaci√≥n social de la ciencia.  Sin embargo, esta forma social a su vez limita y ayuda a determinar lo que podemos detectar.  Este es el an√°logo cient√≠fico de la ley de Conway. <br><br>  Pero, ¬øqu√© tiene que ver todo esto con el aprendizaje profundo o la IA? <br><br>  Bueno, desde los primeros d√≠as del desarrollo de la IA <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">, se ha debatido</a> que todo ir√° "no demasiado complicado, gracias a nuestra super arma", o "la super arma no ser√° suficiente".  El aprendizaje profundo es el √∫ltimo ejemplo de una super arma que se ha utilizado en las disputas que he visto.  En las primeras versiones de tales disputas, se usaba la l√≥gica, o Prolog, o sistemas expertos, o alguna otra tecnolog√≠a, que entonces era la m√°s poderosa.  El problema con tales disputas es que no le dan la oportunidad de decir exactamente cu√°n poderosos ser√°n cualquiera de los candidatos a las super armas.  Por supuesto, acabamos de pasar un cap√≠tulo entero revisando evidencia de que la defensa civil puede resolver problemas extremadamente complejos.  Definitivamente se ve muy interesante y prometedor.  Pero este fue el caso con sistemas como Prolog o Eurisko, o con sistemas expertos.  Por lo tanto, solo el hecho de que un conjunto de ideas parezca prometedor no significa nada especial.  ¬øC√≥mo sabemos que GO es realmente diferente de estas ideas iniciales?  ¬øHay alguna manera de medir cu√°n poderoso y prometedor es un conjunto de ideas?  De la ley de Conway se deduce que podemos utilizar la complejidad de la estructura social asociada con estas ideas como una m√©trica cruda y heur√≠stica. <br><br>  Por lo tanto, tenemos dos preguntas.  Primero, ¬øqu√© tan poderoso es el conjunto de ideas relacionadas con la sociedad civil de acuerdo con esta m√©trica de complejidad social?  En segundo lugar, ¬øqu√© tan poderosa es la teor√≠a que necesitamos para crear una IA de prop√≥sito general? <br><br>  Sobre la primera pregunta: cuando miramos la defensa civil hoy, este campo se ve interesante y en r√°pido desarrollo, pero relativamente monol√≠tico.  Tiene varias ideas profundas y se llevan a cabo varias conferencias importantes, algunas de las cuales se superponen mucho.  El trabajo en el trabajo utiliza el mismo conjunto de ideas: descenso de gradiente estoc√°stico (o su equivalente cercano) para optimizar la funci√≥n de costo.  Es genial que estas ideas tengan tanto √©xito.  Lo que no estamos observando hasta ahora es una gran cantidad de √°reas m√°s peque√±as bien desarrolladas, cada una de las cuales explorar√≠a su propio conjunto de ideas profundas, que mover√≠an a la sociedad civil en muchas direcciones.  Por lo tanto, de acuerdo con la m√©trica de la complejidad social, el aprendizaje profundo, perd√≥n por el juego de palabras, mientras que sigue siendo un √°rea de investigaci√≥n muy superficial.  Una persona a√∫n puede dominar la mayor√≠a de las ideas profundas de esta √°rea. <br><br>  Sobre la segunda pregunta: ¬øcu√°nto se necesitar√° un conjunto complejo y poderoso de ideas para crear IA?  Naturalmente, la respuesta ser√°: nadie lo sabe con certeza.  Pero en el ep√≠logo del libro, estudi√© algunas de las pruebas existentes sobre este tema.  Llegu√© a la conclusi√≥n de que, incluso de acuerdo con estimaciones optimistas, la creaci√≥n de IA requerir√° muchas, muchas ideas profundas.  Seg√∫n la ley de Conway, para llegar a este punto, debemos ver el surgimiento de muchas disciplinas interrelacionadas, con una estructura compleja e inesperada que refleja la estructura de nuestras ideas m√°s profundas.  Todav√≠a no observamos una estructura social tan compleja cuando usamos NS y defensa civil.  Por lo tanto, creo que, al menos, estamos a varias d√©cadas de usar GO para desarrollar IA de uso general. <br><br>  Dediqu√© mucho esfuerzo a crear un argumento especulativo, que, tal vez, parece bastante obvio y no lleva a una cierta conclusi√≥n.  Esto seguramente decepcionar√° a las personas amantes de la certeza.  Conozco a muchas personas en l√≠nea que anuncian p√∫blicamente sus opiniones muy definidas y confiadas sobre la IA, a menudo basadas en argumentos inestables y evidencia inexistente.  Puedo decir honestamente: creo que es demasiado pronto para juzgar.  Como en el viejo chiste: si le preguntas a un cient√≠fico cu√°nto m√°s tenemos que esperar para cualquier descubrimiento, y √©l dice "10 a√±os" (o m√°s), entonces, de hecho, quiere decir "no tengo idea".  Antes del advenimiento de la IA, como en el caso de la fusi√≥n nuclear controlada y algunas otras tecnolog√≠as, "10 a√±os" han permanecido durante m√°s de 60 a√±os.  Por otro lado, lo que definitivamente tenemos en el campo de la defensa civil es una tecnolog√≠a efectiva, cuyos l√≠mites a√∫n no hemos descubierto, y muchas tareas fundamentales abiertas.  Y abre incre√≠bles oportunidades creativas. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/464039/">https://habr.com/ru/post/464039/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../464021/index.html">¬øQu√© es la alternancia de funciones o c√≥mo deshacerse de las morsas insoportables y las ramas de larga vida?</a></li>
<li><a href="../464023/index.html">"Fundamentos de programaci√≥n" establecido para un curso gratuito con ejemplos en JavaScript</a></li>
<li><a href="../464027/index.html">C√≥mo sobrevivir al contenido en la era de la explosi√≥n de informaci√≥n</a></li>
<li><a href="../464031/index.html">"Finds of an Audiomaniac": tarjetas de sonido como una forma de sumergirse en la atm√≥sfera de una ciudad desconocida</a></li>
<li><a href="../464037/index.html">Noticias del mundo de OpenStreetMap No. 472 (30/07/2019 - 05.08.2019)</a></li>
<li><a href="../464041/index.html">¬øPor qu√© los mejores pilotos de combate suelen meterse en grandes problemas?</a></li>
<li><a href="../464043/index.html">Historia del convertidor Ethernet-CAN</a></li>
<li><a href="../464045/index.html">C√≥mo casi recorr√≠ carreras en tiempo real en 1997</a></li>
<li><a href="../464053/index.html">Nota: Selecci√≥n de pista y algoritmo de rotaci√≥n</a></li>
<li><a href="../464055/index.html">Estudiamos los datos recopilados por Xiaomi Mi Band para el a√±o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>