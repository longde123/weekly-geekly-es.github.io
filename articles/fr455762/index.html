<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐏 🧓🏾 🤴🏼 Une brève introduction aux chaînes de Markov 🔏 🤷🏾 🏴󠁧󠁢󠁥󠁮󠁧󠁿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En 1998, Lawrence Page, Sergey Brin, Rajiv Motwani et Terry Vinograd ont publié l'article «The PageRank Citation Ranking: Bringing Order to the Web», ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Une brève introduction aux chaînes de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455762/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif" alt="image"></div><br>  En 1998, Lawrence Page, Sergey Brin, Rajiv Motwani et Terry Vinograd ont publié l'article «The PageRank Citation Ranking: Bringing Order to the Web», qui décrivait le désormais célèbre algorithme PageRank, qui est devenu le fondement de Google.  Après un peu moins de deux décennies, Google est devenu un géant, et même si son algorithme a beaucoup évolué, le PageRank est toujours le «symbole» des algorithmes de classement de Google (bien que seules quelques personnes puissent vraiment dire combien de poids il prend dans l'algorithme aujourd'hui) . <br><br>  D'un point de vue théorique, il est intéressant de noter que l'une des interprétations standard de l'algorithme PageRank est basée sur un concept simple mais fondamental des chaînes de Markov.  À partir de l'article, nous verrons que les chaînes de Markov sont de puissants outils de modélisation stochastique qui peuvent être utiles à tout scientifique des données.  En particulier, nous répondrons à ces questions fondamentales: quelles sont les chaînes de Markov, quelles bonnes propriétés possèdent-elles et que peut-on faire avec leur aide? <br><a name="habracut"></a><br><h4>  Brève revue </h4><br>  Dans la première section, nous donnons les définitions de base nécessaires à la compréhension des chaînes de Markov.  Dans la deuxième section, nous considérons le cas particulier des chaînes de Markov dans un espace à états finis.  Dans la troisième section, nous considérons certaines des propriétés élémentaires des chaînes de Markov et illustrons ces propriétés avec de nombreux petits exemples.  Enfin, dans la quatrième section, nous associons les chaînes de Markov à l'algorithme PageRank et voyons avec un exemple artificiel comment les chaînes de Markov peuvent être utilisées pour classer les nœuds d'un graphique. <br><br><blockquote>  <strong>Remarque</strong>  La compréhension de ce poste nécessite une connaissance des bases de la probabilité et de l'algèbre linéaire.  En particulier, les concepts suivants seront utilisés: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener">probabilité conditionnelle</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener">vecteur propre</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" rel="noopener">formule de probabilité complète</a> . </blockquote><br><hr><br><h3>  Quelles sont les chaînes de Markov? </h3><br><h4>  Variables aléatoires et processus aléatoires </h4><br>  Avant d'introduire le concept de chaînes de Markov, rappelons brièvement les concepts fondamentaux mais importants de la théorie des probabilités. <br><br>  Premièrement, en dehors du langage des mathématiques, une <strong>variable aléatoire</strong> X est une quantité qui est déterminée par le résultat d'un phénomène aléatoire.  Son résultat peut être un nombre (ou "similitude d'un nombre", par exemple, des vecteurs) ou autre chose.  Par exemple, nous pouvons définir une variable aléatoire comme le résultat d'un jet de dé (nombre) ou comme le résultat d'un tirage au sort (pas un nombre, sauf si nous désignons, par exemple, "aigle" comme 0, mais "queues" comme 1).  Nous mentionnons également que l'espace des résultats possibles d'une variable aléatoire peut être discret ou continu: par exemple, une variable aléatoire normale est continue, et une variable aléatoire de Poisson est discrète. <br><br>  De plus, nous pouvons définir un <strong>processus aléatoire</strong> (également appelé stochastique) comme un ensemble de variables aléatoires indexées par l'ensemble T, qui dénote souvent différents moments dans le temps (dans ce qui suit, nous supposerons cela).  Les deux cas les plus courants: T peut être soit un ensemble de nombres naturels (processus aléatoire à temps discret), soit un ensemble de nombres réels (processus aléatoire à temps continu).  Par exemple, si nous jetons une pièce tous les jours, nous définirons un processus aléatoire avec une heure discrète, et la valeur toujours changeante d'une option sur l'échange définira un processus aléatoire avec une heure continue.  Les variables aléatoires à différents moments peuvent être indépendantes les unes des autres (un exemple avec un tirage au sort), ou avoir une sorte de dépendance (un exemple avec la valeur de l'option);  de plus, ils peuvent avoir un espace d'état continu ou discret (l'espace des résultats possibles à chaque instant). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31e/ada/2f8/31eada2f80d66f0df4c007ec8da11579.jpg"></div><br>  <i>Différents types de processus aléatoires (discrets / continus dans l'espace / temps).</i> <br><br><h4>  Propriété Markov et chaîne Markov </h4><br>  Il existe des familles bien connues de processus aléatoires: processus gaussiens, processus de Poisson, modèles autorégressifs, modèles à moyenne mobile, chaînes de Markov et autres.  Chacun de ces cas individuels possède certaines propriétés qui nous permettent de mieux les explorer et les comprendre. <br><br>  L'une des propriétés qui simplifie considérablement l'étude d'un processus aléatoire est la propriété Markov.  Si nous l'expliquons dans un langage très informel, la propriété Markov nous dit que si nous connaissons la valeur obtenue par un processus aléatoire à un moment donné, nous ne recevrons aucune information supplémentaire sur le comportement futur du processus, en collectant d'autres informations sur son passé.  Dans un langage plus mathématique: à tout moment, la distribution conditionnelle des états futurs d'un processus avec des états actuels et passés donnés ne dépend que de l'état actuel, et non des états passés (la <strong>propriété du manque de mémoire</strong> ).  Un processus aléatoire avec une propriété Markov est appelé <strong>processus Markov</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44f/1ba/f48/44f1baf48ceb669e0416489eea2bae35.png"></div><br>  <i>La propriété Markov signifie que si nous connaissons l'état actuel à un moment donné, nous n'avons pas besoin d'informations supplémentaires sur l'avenir, collectées sur le passé.</i> <br><br>  Sur la base de cette définition, nous pouvons formuler la définition de «chaînes de Markov homogènes à temps discret» (ci-après pour des raisons de simplicité, nous les appellerons «chaînes de Markov»).  <strong>La chaîne de Markov</strong> est un processus de Markov avec un temps discret et un espace d'état discret.  Ainsi, une chaîne de Markov est une séquence d'états discrets, dont chacun est extrait d'un espace d'états discrets (fini ou infini), satisfaisant la propriété de Markov. <br><br>  Mathématiquement, nous pouvons désigner la chaîne de Markov comme suit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/114/3b7/fc3/1143b7fc371f3142534c2b886bf3e69c.png"></div><br>  où à chaque instant le processus prend ses valeurs dans un ensemble discret E, tel que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/886/a22/d76/886a22d7671798102ee3d94fe9868b81.png"></div><br>  Ensuite, la propriété Markov implique que nous avons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/edc/8bf/384/edc8bf38422705e72c9dd7d094b249db.png"></div><br>  Notez à nouveau que cette dernière formule reflète le fait que pour la chronologie (où je suis maintenant et où j'étais auparavant) la distribution de probabilité de l'état suivant (où je serai le prochain) dépend de l'état actuel, mais pas des états passés. <br><br><blockquote>  <strong>Remarque</strong>  Dans ce billet introductif, nous avons décidé de ne parler que de simples chaînes de Markov homogènes à temps discret.  Cependant, il existe également des chaînes de Markov inhomogènes (dépendantes du temps) et / ou des chaînes à temps continu.  Dans cet article, nous ne considérerons pas de telles variations du modèle.  Il convient également de noter que la définition ci-dessus d'une propriété de Markov est extrêmement simplifiée: la véritable définition mathématique utilise le concept de filtrage, qui va bien au-delà de notre connaissance introductive du modèle. </blockquote><br><h4>  Nous caractérisons la dynamique d'aléatoire d'une chaîne de Markov </h4><br>  Dans la sous-section précédente, nous nous sommes familiarisés avec la structure générale correspondant à toute chaîne de Markov.  Voyons ce dont nous avons besoin pour définir une "instance" spécifique d'un tel processus aléatoire. <br><br>  Tout d'abord, nous notons que la détermination complète des caractéristiques d'un processus aléatoire à temps discret qui ne satisfait pas la propriété de Markov peut être difficile: la distribution de probabilité à un moment donné peut dépendre d'un ou de plusieurs moments du passé et / ou du futur.  Toutes ces dépendances temporelles possibles peuvent potentiellement compliquer la création d'une définition de processus. <br><br>  Cependant, en raison de la propriété Markov, la dynamique de la chaîne de Markov est assez simple à déterminer.  Et en effet.  nous devons déterminer seulement deux aspects: la <strong>distribution de probabilité initiale</strong> (c'est-à-dire la distribution de probabilité au temps n = 0), notée par <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/595/90e/140/59590e140cdb348c943d9dcab0ea011d.png"></div><br>  et <strong>la matrice de probabilité de transition</strong> (qui nous donne les probabilités que l'état au temps n + 1 soit le suivant pour un autre état au temps n pour n'importe quelle paire d'états), noté par <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/011/aee/574/011aee5747fe7e42fa09bf044c421e26.png"></div><br>  Si ces deux aspects sont connus, alors la dynamique complète (probabiliste) du processus est clairement définie.  Et en fait, la probabilité de tout résultat du processus peut alors être calculée de manière cyclique. <br><br>  Exemple: supposons que nous voulons connaître la probabilité que les 3 premiers états du processus aient des valeurs (s0, s1, s2).  Autrement dit, nous voulons calculer la probabilité <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c5/991/81b/6c599181b1fd3892391711f311878b72.png"></div><br>  Ici, nous appliquons la formule de la probabilité totale, qui stipule que la probabilité d'obtenir (s0, s1, s2) est égale à la probabilité d'obtenir les premiers s0 fois la probabilité d'obtenir s1, étant donné que nous avons précédemment obtenu s0 fois la probabilité d'obtenir s2 en tenant compte du fait que nous avons obtenu plus tôt dans l'ordre s0 et s1.  Mathématiquement, cela peut être écrit comme <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a46/da3/64f/a46da364f28d5b69055700759db02663.png"></div><br>  Et puis la simplification est révélée, déterminée par l'hypothèse de Markov.  Et en fait, dans le cas des chaînes longues, on obtient des probabilités fortement conditionnelles pour ces derniers états.  Cependant, dans le cas des chaînes de Markov, nous pouvons simplifier cette expression en profitant du fait que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/7f6/568/0ab7f6568e28bb562ebb287252422d51.png"></div><br>  obtenir de cette façon <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b6c/700/30a/b6c70030a8627d87c39dab96c147513a.png"></div><br>  Puisqu'ils caractérisent pleinement la dynamique probabiliste du processus, de nombreux événements complexes ne peuvent être calculés que sur la base de la distribution de probabilité initiale q0 et de la matrice de probabilité de transition p.  Il convient également de mentionner un autre lien de base: l'expression de la distribution de probabilité au temps n + 1, exprimée par rapport à la distribution de probabilité au temps n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/f31/a8a/5aef31a8ae120a25e5b43d6534dc20ff.png"></div><br><h3>  Chaînes de Markov dans les espaces d'états finis </h3><br><h4>  Représentation matricielle et graphique </h4><br>  Ici, nous supposons que l'ensemble E a un nombre fini d'états possibles N: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d57/788/f81/d57788f81d0a92aa1d94ef572bdf25e3.png"></div><br>  Ensuite, la distribution de probabilité initiale peut être décrite comme <strong>un vecteur ligne</strong> q0 de taille N, et les probabilités de transition peuvent être décrites comme une matrice p de taille N par N, de telle sorte que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/100/6f8/6ae/1006f86aeff6699711058bd890190917.png"></div><br>  L'avantage de cette notation est que si nous notons la distribution de probabilité à l'étape n par le vecteur ligne qn tel que ses composantes sont spécifiées <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/19b/4ae/00619b4ae1601eacdb8fd7ce248f1738.png"></div><br>  alors les relations matricielles simples sont préservées <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a12/cdc/a7b/a12cdca7b75ef09ceaacef33a3667549.png"></div><br>  (ici nous ne considérerons pas la preuve, mais il est très simple de la reproduire). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23c/3c6/a10/23c3c6a102bd7aa079c36a75f60a5e42.png"></div><br>  <i>Si nous multiplions le vecteur ligne à droite, qui décrit la distribution de probabilité à un moment donné, par la matrice de probabilité de transition, alors nous obtenons la distribution de probabilité au stade suivant.</i> <br><br>  Ainsi, comme nous le voyons, la transition de la distribution de probabilité d'un stade donné au suivant est simplement définie comme la multiplication correcte du vecteur ligne de probabilités de l'étape initiale par la matrice p.  De plus, cela implique que nous avons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/35b/072/e0c/35b072e0c74397094e5bf6a9dab11417.png"></div><br>  La dynamique aléatoire d'une chaîne de Markov dans un espace d'états finis peut facilement être représentée comme un graphe orienté normalisé de telle sorte que chaque nœud du graphe est un état, et pour chaque paire d'états (ei, ej) il existe un bord allant de ei à ej si p (ei, ej )&gt; 0.  La valeur de bord sera alors la même probabilité p (ei, ej). <br><br><h4>  Exemple: un lecteur de notre site </h4><br>  Illustrons tout cela avec un exemple simple.  Considérez le comportement quotidien d'un visiteur fictif sur un site.  Chaque jour, il a 3 conditions possibles: le lecteur ne visite pas le site ce jour-là (N), le lecteur visite le site, mais ne lit pas l'intégralité du post (V), et le lecteur visite le site et lit un post entier (R).  Nous avons donc l'espace d'état suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/242/b1c/627/242b1c62745a4a13f2a24b8f919c6820.png"></div><br>  Supposons que le premier jour, ce lecteur ait 50% de chances d'accéder uniquement au site et 50% de chances de visiter le site et de lire au moins un article.  Le vecteur décrivant la distribution de probabilité initiale (n = 0) ressemble alors à ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/362/a5a/045/362a5a0455c939f9d855d65605945c90.png"></div><br>  Imaginez également que les probabilités suivantes sont observées: <br><br><ul><li>  lorsque le lecteur ne visite pas un jour, il y a une probabilité de 25% de ne pas lui rendre visite le lendemain, une probabilité de 50% seulement de lui rendre visite et de 25% de visiter et de lire l'article </li><li>  lorsque le lecteur visite le site un jour, mais ne lit pas, il a 50% de chances de le visiter à nouveau le lendemain sans lire l'article, et 50% de chances de visiter et de lire </li><li>  lorsqu'un lecteur visite et lit un article le même jour, il a 33% de chances de ne pas se connecter le lendemain <em>(j'espère que ce message n'aura pas un tel effet!)</em> , 33% de chances de se connecter uniquement au site et 34% de visiter et de relire l'article </li></ul><br>  Ensuite, nous avons la matrice de transition suivante: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cf5/7e6/ba5/cf57e6ba5303d4e13cbb736e6115306d.png"></div><br>  De la sous-section précédente, nous savons comment calculer pour ce lecteur la probabilité de chaque état le lendemain (n = 1) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c64/a77/76c/c64a7776cfc56a5af1a0ccf495469ef7.png"></div><br>  La dynamique probabiliste de cette chaîne de Markov peut être représentée graphiquement comme suit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/797/9a1/8327979a1aa6edd9d462a0b40a4c072d.png"></div><br>  <i>Présentation sous forme de graphique de la chaîne de Markov, modélisant le comportement de notre visiteur inventé sur le site.</i> <br><br><h3>  Propriétés des chaînes de Markov </h3><br>  Dans cette section, nous ne parlerons que de certaines des propriétés ou caractéristiques les plus fondamentales des chaînes de Markov.  Nous n'entrerons pas dans les détails mathématiques, mais fournirons un bref aperçu des points intéressants qui doivent être étudiés pour utiliser les chaînes de Markov.  Comme nous l'avons vu, dans le cas d'un espace à états finis, la chaîne de Markov peut être représentée sous forme de graphe.  À l'avenir, nous utiliserons la représentation graphique pour expliquer certaines propriétés.  Cependant, n'oubliez pas que ces propriétés ne sont pas nécessairement limitées au cas d'un espace d'états fini. <br><br><h4>  Décomposabilité, périodicité, irrévocabilité et récupérabilité </h4><br>  Dans cette sous-section, commençons par plusieurs façons classiques de caractériser un état ou une chaîne de Markov entière. <br><br>  Tout d'abord, nous mentionnons que la chaîne de Markov est <strong>indécomposable</strong> s'il est possible d'atteindre n'importe quel état à partir de n'importe quel autre état (ce n'est pas nécessaire qu'en une seule étape de temps).  Si l'espace d'état est fini et que la chaîne peut être représentée sous forme de graphe, alors nous pouvons dire que le graphe d'une chaîne de Markov indécomposable est fortement connecté (théorie des graphes). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cef/a39/05c/cefa3905cced27b4e27a4e9547fbe846.png"></div><br>  <i>Illustration de la propriété de l'indécomposabilité (irréductibilité).</i>  <i>La chaîne de gauche ne peut pas être raccourcie: à partir de 3 ou 4, nous ne pouvons pas entrer dans 1 ou 2. La chaîne de droite (un bord est ajouté) peut être raccourcie: chaque état peut être atteint à partir de n'importe quel autre.</i> <br><br>  Un état a une période k si, à sa sortie, pour tout retour à cet état, le nombre de pas de temps est un multiple de k (k est le plus grand diviseur commun de toutes les longueurs possibles de chemins de retour).  Si k = 1, alors ils disent que l'état est apériodique, et toute la chaîne de Markov est <strong>apériodique</strong> si tous ses états sont apériodiques.  Dans le cas d'une chaîne de Markov irréductible, on peut également mentionner que si un état est apériodique, alors tous les autres sont également apériodiques. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbe/65f/a07/cbe65fa07e7b816d385408824ba0ff39.png"></div><br>  <i>Illustration de la propriété de périodicité.</i>  <i>La chaîne de gauche est périodique avec k = 2: lorsque vous quittez un état, y revenir nécessite toujours le nombre de pas multiple de 2. La chaîne de droite a une période de 3.</i> <br><br>  Un état est <strong>irrévocable</strong> si, à sa sortie, il y a une probabilité non nulle que nous n'y revenions jamais.  Inversement, un état est considéré comme <strong>retournable</strong> si nous savons qu'après avoir quitté l'état, nous pouvons y revenir à l'avenir avec la probabilité 1 (s'il n'est pas irrévocable). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6a0/1ad/7b0/6a01ad7b05e96f5a6606a8a48d127233.png"></div><br>  <i>Illustration de la propriété de retour / irrévocabilité.</i>  <i>La chaîne de gauche a les propriétés suivantes: 1, 2 et 3 sont irrévocables (en quittant ces points, nous ne pouvons pas être absolument sûrs que nous y reviendrons) et ont une période de 3, et 4 et 5 sont retournables (en quittant ces points, nous sommes absolument sûrs un jour, nous y reviendrons) et avons une période de 2. La chaîne de droite a une autre nervure, ce qui rend la chaîne entière retournable et apériodique.</i> <br><br>  Pour l'état de retour, nous pouvons calculer le temps de retour moyen, qui est le <strong>temps de retour attendu</strong> en quittant l'état.  Notez que même la probabilité d'un retour est 1, cela ne signifie pas que le temps de retour attendu est fini.  Par conséquent, parmi tous les états de retour, nous pouvons distinguer <strong>les états de retour positifs</strong> (avec un temps de retour attendu fini) et les <strong>états de retour zéro</strong> (avec un temps de retour attendu infini). <br><br><h4>  Distribution stationnaire, comportement marginal et ergodicité </h4><br>  Dans cette sous-section, nous considérons les propriétés qui caractérisent certains aspects de la dynamique (aléatoire) décrite par la chaîne de Markov. <br><br>  La distribution de probabilité π sur l'espace d'état E est appelée <strong>distribution stationnaire</strong> si elle satisfait l'expression <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/736/d70/e5d/736d70e5ddfc5aeb788d29ebfa79f9ec.png"></div><br>  Puisque nous avons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b98/c4e/e6c/b98c4ee6c3b8032b074c7543db816c7e.png"></div><br>  Alors la distribution stationnaire satisfait l'expression <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b8/df4/0a8/2b8df40a8a9a2ae90eb49a7c60dafb55.png"></div><br>  Par définition, la distribution de probabilité stationnaire ne change pas avec le temps.  Autrement dit, si la distribution initiale q est stationnaire, elle sera la même à toutes les étapes ultérieures du temps.  Si l'espace d'état est fini, alors p peut être représenté comme une matrice, et π comme un vecteur ligne, puis nous obtenons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee0/565/9fc/ee05659fc4ed268ee3189342a76d9311.png"></div><br>  Cela exprime à nouveau le fait que la distribution de probabilité stationnaire ne change pas avec le temps (comme nous le voyons, multiplier la distribution de probabilité à droite par p nous permet de calculer la distribution de probabilité à l'étape suivante du temps).  Gardez à l'esprit qu'une chaîne de Markov indécomposable a une distribution de probabilité stationnaire si et seulement si l'un de ses états est un retour positif. <br><br>  Une autre propriété intéressante liée à la distribution de probabilité stationnaire est la suivante.  Si la chaîne est un retour positif (c'est-à-dire qu'il y a une distribution stationnaire) et apériodique, alors, quelles que soient les probabilités initiales, la distribution de probabilité de la chaîne converge alors que les intervalles de temps tendent vers l'infini: ils disent que la chaîne a une <strong>distribution limite</strong> , qui n'est rien d'autre, comme une distribution stationnaire.  En général, cela peut être écrit comme ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95b/fb9/1c6/95bfb91c67d024e2df40b0e6dcdaf747.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous soulignons une fois de plus que nous ne faisons aucune hypothèse sur la distribution de probabilité initiale: la distribution de probabilité de la chaîne se réduit à une distribution stationnaire (distribution d'équilibre de la chaîne) quels que soient les paramètres initiaux. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Enfin, l' </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ergodicité</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> est une autre propriété intéressante liée au comportement de la chaîne de Markov. Si la chaîne de Markov est indécomposable, alors on dit aussi qu'elle est «ergodique» car elle satisfait au théorème ergodique suivant. Supposons que nous ayons une fonction f (.) Qui va de l'espace d'état E à l'axe (cela peut être, par exemple, le prix d'être dans chaque état). Nous pouvons déterminer la valeur moyenne qui déplace cette fonction le long d'une trajectoire donnée (moyenne temporelle). Pour les nèmes premiers termes, cela est noté comme</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af1/8c5/4a3/af18c54a3d7dc1e1ad4a4015ab7ad64c.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> On peut également calculer la valeur moyenne de la fonction f sur l'ensemble E, pondérée par la distribution stationnaire (moyenne spatiale), qui est notée </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04a/352/c77/04a352c77b0687ef3cc89f3b7e0edf38.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le théorème ergodique nous dit alors que lorsque la trajectoire devient infiniment longue, la moyenne temporelle est égale à la moyenne spatiale (pondérée par la distribution stationnaire). </font><font style="vertical-align: inherit;">La propriété ergodicity peut s'écrire comme suit:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/37d/c4d/6cb37dc4dcf0a3e53cc8e6baec8f4b1a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En d'autres termes, cela signifie que dans la limite antérieure, le comportement de la trajectoire devient insignifiant et seul le comportement stationnaire à long terme est important lors du calcul de la moyenne temporelle. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Revenons à l'exemple avec le lecteur de site </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Encore une fois, considérons l'exemple du lecteur de site. </font><font style="vertical-align: inherit;">Dans cet exemple simple, il est évident que la chaîne est indécomposable, apériodique et que tous ses états sont positivement retournables. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour montrer quels résultats intéressants peuvent être calculés à l'aide des chaînes de Markov, nous considérons le temps moyen de retour à l'état R (l'état «visite le site et lit l'article»). </font><font style="vertical-align: inherit;">En d'autres termes, nous voulons répondre à la question suivante: si notre lecteur visite un jour le site et lit un article, alors combien de jours devra-t-il attendre en moyenne pour qu'il revienne lire l'article? </font><font style="vertical-align: inherit;">Essayons d'obtenir un concept intuitif de la façon dont cette valeur est calculée. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On note d'abord</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/57c/c74/2a257cc74db27e5ac89ffc1e06bd9ed9.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nous voulons donc calculer m (R, R). </font><font style="vertical-align: inherit;">En parlant du premier intervalle atteint après avoir quitté R, on obtient</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f9/e1a/a6e/4f9e1aa6e04e736fde182693398a4dca.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, cette expression nécessite que pour le calcul de m (R, R) nous connaissions m (N, R) et m (V, R). </font><font style="vertical-align: inherit;">Ces deux quantités peuvent être exprimées de manière similaire:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e23/7cb/f2d/e237cbf2d81597544f800d38b5a59e91.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ainsi, nous avons obtenu 3 équations avec 3 inconnues et après les avoir résolues, nous obtenons m (N, R) = 2,67, m (V, R) = 2,00 et m (R, R) = 2,54. </font><font style="vertical-align: inherit;">Le temps moyen pour revenir à l'état R est alors de 2,54. </font><font style="vertical-align: inherit;">Autrement dit, en utilisant l'algèbre linéaire, nous avons pu calculer le temps moyen de retour à l'état R (ainsi que le temps de transition moyen de N à R et le temps de transition moyen de V à R). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour terminer avec cet exemple, voyons quelle sera la distribution stationnaire de la chaîne de Markov. </font><font style="vertical-align: inherit;">Pour déterminer la distribution stationnaire, nous devons résoudre l'équation d'algèbre linéaire suivante:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb3/73d/068/bb373d068a04d681c0501d8276731c0a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Autrement dit, nous devons trouver le vecteur propre gauche p associé au vecteur propre 1. En résolvant ce problème, nous obtenons la distribution stationnaire suivante: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0c6/abe/19e/0c6abe19e37b67af5f380eb3e5c0beb9.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distribution stationnaire dans l'exemple avec le lecteur de site. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez également remarquer que π (R) = 1 / m (R, R), et si un peu de réflexion, alors cette identité est tout à fait logique (mais nous n'en parlerons pas en détail).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La chaîne étant indécomposable et apériodique, cela signifie qu'à long terme la distribution de probabilité converge vers une distribution stationnaire (pour tous les paramètres initiaux). En d'autres termes, quel que soit l'état initial du lecteur du site, si nous attendons assez longtemps et sélectionnons un jour au hasard, nous aurons la probabilité π (N) que le lecteur ne visitera pas le site ce jour-là, la probabilité π (V) que le lecteur s'arrêtera mais ne lira pas l'article, et la probabilité est π® que le lecteur s'arrête et lise l'article. Pour mieux comprendre la propriété de la convergence, regardons le graphique suivant montrant l'évolution des distributions de probabilité à partir de différents points de départ et convergeant (rapidement) vers une distribution stationnaire:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/128/58e/a88/12858ea88a0e3bd05950b9d30096b776.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualisation de la convergence de 3 distributions de probabilité avec différents paramètres initiaux (bleu, orange et vert) vers la distribution stationnaire (rouge).</font></font></i> <br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exemple classique: algorithme de PageRank </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il est temps de revenir au PageRank! </font><font style="vertical-align: inherit;">Mais avant de poursuivre, il convient de mentionner que l'interprétation du PageRank donnée dans cet article n'est pas la seule possible, et les auteurs de l'article d'origine ne se sont pas nécessairement appuyés sur l'utilisation de chaînes de Markov pour développer la méthodologie. </font><font style="vertical-align: inherit;">Cependant, notre interprétation est bonne car elle est très claire.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Utilisateur Web arbitraire </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PageRank essaie de résoudre le problème suivant: comment pouvons-nous classer un ensemble existant (nous pouvons supposer que cet ensemble a déjà été filtré, par exemple, par une requête) en utilisant des liens déjà existants entre les pages? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour résoudre ce problème et pouvoir classer les pages, PageRank effectue approximativement le processus suivant. Nous pensons qu'un internaute arbitraire à la première fois est sur l'une des pages. Ensuite, cet utilisateur commence à se déplacer au hasard, en cliquant sur chaque page sur l'un des liens qui mènent à une autre page de l'ensemble en question (il est supposé que tous les liens menant en dehors de ces pages sont interdits). Sur n'importe quelle page, tous les liens valides ont la même probabilité de cliquer.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C'est ainsi que nous définissons la chaîne de Markov: les pages sont des états possibles, les probabilités de transition sont définies par des liens de page en page (pondérés de telle sorte que sur chaque page toutes les pages liées ont la même probabilité de sélection), et les propriétés du manque de mémoire sont clairement déterminées par le comportement de l'utilisateur. Si nous supposons également que la chaîne donnée est retournable positivement et apériodique (de petites astuces sont utilisées pour satisfaire ces exigences), alors à long terme, la distribution de probabilité de la «page actuelle» converge vers une distribution stationnaire. Autrement dit, quelle que soit la page initiale, après une longue période, chaque page a une probabilité (presque fixe) de devenir actuelle si nous choisissons un moment aléatoire dans le temps.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le PageRank est basé sur l'hypothèse suivante: les pages les plus probables d'une distribution stationnaire devraient également être les plus importantes (nous visitons souvent ces pages parce qu'elles obtiennent des liens de pages qui sont également fréquemment visitées pendant les transitions). </font><font style="vertical-align: inherit;">Ensuite, la distribution de probabilité stationnaire détermine la valeur du PageRank pour chaque état.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exemple artificiel </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pour rendre cela beaucoup plus clair, regardons un exemple artificiel. </font><font style="vertical-align: inherit;">Supposons que nous ayons un petit site Web de 7 pages, libellé de 1 à 7, et que les liens entre ces pages correspondent à la colonne suivante.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Par souci de clarté, les probabilités de chaque transition dans l'animation ci-dessus ne sont pas affichées. </font><font style="vertical-align: inherit;">Cependant, comme on suppose que la «navigation» doit être exclusivement aléatoire (c'est ce qu'on appelle «marche aléatoire»), les valeurs peuvent être facilement reproduites à partir de la règle simple suivante: pour un site avec K liens sortants (page avec K liens vers d'autres pages), la probabilité de chaque lien sortant égal à 1 / K. </font><font style="vertical-align: inherit;">Autrement dit, la matrice de probabilité de transition a la forme:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b30/3ec/a66/b303eca66763a4187d027842214ff529.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">où les valeurs de 0,0 sont remplacées par commodité par ".". </font><font style="vertical-align: inherit;">Avant d'effectuer d'autres calculs, nous pouvons remarquer que cette chaîne de Markov est indécomposable et apériodique, c'est-à-dire qu'à long terme le système converge vers une distribution stationnaire. </font><font style="vertical-align: inherit;">Comme nous l'avons vu, cette distribution stationnaire peut être calculée en résolvant le problème de vecteur propre gauche suivant</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2f/8da/687/e2f8da6879f6f19fdc921803c8c7e371.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ce faisant, nous obtenons les valeurs de PageRank suivantes (valeurs de distribution stationnaires) pour chaque page </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a8/b69/a5b/0a8b69a5b2916bca1f5fa45955af1b4b.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Valeurs de PageRank calculées pour notre exemple artificiel de 7 pages. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ensuite, le classement PageRank de ce petit site Web est 1&gt; 7&gt; 4&gt; 2&gt; 5 = 6&gt; 3.</font></font><br><br><hr><br><h3>  Conclusions </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Principales conclusions de cet article: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> les processus aléatoires sont des ensembles de variables aléatoires qui sont souvent indexées par le temps (les indices indiquent souvent un temps discret ou continu) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour un processus aléatoire, la propriété de Markov signifie que pour un courant donné, la probabilité de l'avenir ne dépend pas du passé (cette propriété est aussi appelée «manque de mémoire») </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> chaîne de Markov à temps discret est des processus aléatoires avec des indices à temps discret satisfaisant la propriété de Markov </font></font></li><li>                 (  ,  …) </li><li>     PageRank ( )    -,       ;          ( ,             ,  ,      ) </li></ul><br>     ,         ,    .         , ,    (   ,             ,     ),   (   -            ),   (   ),   (           ),     . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bien sûr, les énormes opportunités offertes par les chaînes de Markov du point de vue de la modélisation et du calcul sont beaucoup plus larges que celles considérées dans cette modeste revue. </font><font style="vertical-align: inherit;">Par conséquent, nous espérons avoir pu susciter l'intérêt du lecteur à poursuivre l'étude de ces outils, qui occupent une place importante dans l'arsenal d'un scientifique et expert en données.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr455762/">https://habr.com/ru/post/fr455762/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr455746/index.html">Celesta 7.x: ORM, migration et test «dans un seul paquet»</a></li>
<li><a href="../fr455754/index.html">Tests d'un stratostat dérivant. Lancement de Rogozin et LoRa dans la stratosphère</a></li>
<li><a href="../fr455756/index.html">Est [favor] e</a></li>
<li><a href="../fr455758/index.html">Hacking de croissance chez Retail Rocket: de la recherche d'hypothèses aux techniques de test</a></li>
<li><a href="../fr455760/index.html">La magie de SwiftUI ou sur les constructeurs de fonctions</a></li>
<li><a href="../fr455764/index.html">Recherche de codes à barres extrêmement précise, rapide et légère grâce à la segmentation sémantique</a></li>
<li><a href="../fr455768/index.html">Facteurs SEO essentiels sur site</a></li>
<li><a href="../fr455770/index.html">AERODISK: attente vs réalité</a></li>
<li><a href="../fr455774/index.html">Moteurs de turbines à gaz pour aéronefs</a></li>
<li><a href="../fr455784/index.html">En raison de ce que le gris foncé est plus clair que le gris en CSS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>