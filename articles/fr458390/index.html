<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçü§ù‚Äçüë®üèº ü•© üö¥üèª Ceph - de ¬´√† genoux¬ª √† ¬´production¬ª partie 2 üëàüèº ‚õπüèæ ü§öüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(premi√®re partie ici: https://habr.com/en/post/456446/ ) 
 Ceph 
 Pr√©sentation 


 Le r√©seau √©tant l'un des √©l√©ments cl√©s de Ceph, et un peu sp√©cifiqu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de ¬´√† genoux¬ª √† ¬´production¬ª partie 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (premi√®re partie ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Pr√©sentation </h3><br><p>  Le r√©seau √©tant l'un des √©l√©ments cl√©s de Ceph, et un peu sp√©cifique √† notre entreprise, nous allons d'abord vous en parler un peu. <br>  Il y aura beaucoup moins de descriptions de Ceph lui-m√™me, principalement une infrastructure de r√©seau.  Seuls les serveurs Ceph et certaines fonctionnalit√©s des serveurs de virtualisation Proxmox seront d√©crits. </p><a name="habracut"></a><br><p>  Donc: La topologie du r√©seau elle-m√™me est con√ßue comme <strong>Leaf-Spine.</strong>  L'architecture classique √† trois niveaux est un r√©seau o√π se trouvent le <strong>noyau</strong> (routeurs principaux), l' <strong>agr√©gation</strong> (routeurs d'agr√©gation) et directement connect√© aux clients <strong>Access</strong> (routeurs d'acc√®s): </p><br><p>  <strong>Sch√©ma √† trois niveaux</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  La topologie Leaf-Spine se compose de deux niveaux: <strong>Spine</strong> (grosso modo le routeur principal) et <strong>Leaf</strong> (branches). </p><br><p>  <strong>Sch√©ma √† deux niveaux</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Tout le routage interne et externe est construit sur BGP.  Le syst√®me principal qui traite du contr√¥le d'acc√®s, des annonces et plus est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>XCloud.</strong></a> <br>  Les serveurs, pour la r√©servation de canal (et aussi pour son expansion) sont connect√©s √† deux commutateurs L3 (la plupart des serveurs sont connect√©s aux commutateurs Leaf, mais certains serveurs avec une charge r√©seau accrue sont connect√©s directement √† la colonne vert√©brale du commutateur), et via BGP, annoncer leur adresse de monodiffusion, ainsi que l'adresse anycast pour le service si plusieurs serveurs desservent le trafic de service et que l'√©quilibrage ECMP leur suffit.  Une caract√©ristique distincte de ce sch√©ma, qui nous a permis d'√©conomiser sur les adresses, mais a √©galement oblig√© les ing√©nieurs √† se familiariser avec le monde IPv6, √©tait l'utilisation de la norme BGP non num√©rot√©e bas√©e sur la RFC 5549. Pendant un certain temps, Quagga a √©t√© utilis√© pour les serveurs en BGP pour ce sch√©ma pour les serveurs et p√©riodiquement. il y avait des probl√®mes avec la perte des f√™tes et de la connectivit√©.  Mais apr√®s le passage √† FRRouting (dont les contributeurs actifs sont nos fournisseurs d'√©quipements r√©seau: Cumulus et XCloudNetworks), nous n'avons plus observ√© de tels probl√®mes. </p><br><p>  Pour plus de commodit√©, nous appelons tout ce sch√©ma g√©n√©ral une "usine". </p><br><h2 id="poisk-puti">  Recherche de chemin </h2><br><p>  Options de configuration du r√©seau de cluster: </p><br><p>  1) Deuxi√®me r√©seau sur BGP </p><br><p>  2) Le deuxi√®me r√©seau sur deux commutateurs empil√©s s√©par√©s avec LACP </p><br><p>  3) Deuxi√®me r√©seau sur deux commutateurs isol√©s s√©par√©s avec OSPF </p><br><h3 id="testy">  Les tests </h3><br><p>  Les tests ont √©t√© effectu√©s en deux types: </p><br><p>  a) r√©seau utilisant les utilitaires iperf, qperf, nuttcp </p><br><p>  b) tests internes Ceph ceph-gobench, banc rados, cr√©√© rbd et test√© sur eux en utilisant dd dans un ou plusieurs threads, en utilisant fio </p><br><p>  Tous les tests ont √©t√© effectu√©s sur des machines de test avec des disques SAS.  Les chiffres de la performance rbd n'ont pas √©t√© beaucoup examin√©s, ils ont √©t√© utilis√©s uniquement √† des fins de comparaison.  Int√©ress√© par des changements en fonction du type de connexion. </p><br><h3 id="pervyy-variant">  Premi√®re option </h3><br><p>  <strong>Les cartes r√©seau sont connect√©es au BGP configur√© en usine.</strong> </p><br><p>  L'utilisation de ce sch√©ma pour le r√©seau interne n'a pas √©t√© consid√©r√©e comme le meilleur choix: </p><br><p>  Tout d'abord, le nombre excessif d'√©l√©ments interm√©diaires sous forme de commutateurs donnant une latence suppl√©mentaire (c'√©tait la raison principale). <br>  Deuxi√®mement, au d√©part, pour diffuser des donn√©es statiques via s3, ils ont utilis√© une adresse anycast √©lev√©e sur plusieurs machines avec radosgateway.  Cela a entra√Æn√© le fait que le trafic des machines frontales vers RGW n'√©tait pas r√©parti uniform√©ment, mais passait par le chemin le plus court - c'est-√†-dire que Nginx frontal se tournait toujours vers le m√™me n≈ìud avec RGW qui √©tait connect√© √† la feuille partag√©e avec lui (cela, bien s√ªr, √©tait pas l'argument principal - nous avons simplement refus√© par la suite √† partir des adresses anycast de retourner statique).  Mais pour la puret√© de l'exp√©rience, ils ont d√©cid√© de mener des tests sur un tel sch√©ma afin d'avoir des donn√©es √† comparer. </p><br><p>  Nous avions peur d'ex√©cuter des tests pour toute la bande passante, car l'usine est utilis√©e par les serveurs prod, et si nous bloquions les liens entre les feuilles et la colonne vert√©brale, cela nuirait √† certaines ventes. <br>  En fait, c'√©tait une autre raison de rejeter un tel r√©gime. <br>  Des tests Iperf avec une limite BW de 3Gbps de 1, 10 et 100 flux ont √©t√© utilis√©s pour la comparaison avec d'autres sch√©mas. <br>  Les tests ont montr√© les r√©sultats suivants: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  en <strong>1</strong> flux, environ <strong>9,30 - 9,43 Gbits / sec</strong> (dans ce cas, le nombre de retransmissions cro√Æt fortement, √† <strong>39148</strong> ).  Le chiffre av√©r√© proche du maximum d'une interface sugg√®re que l'une des deux est utilis√©e.  Le nombre de retransmissions est d'environ <strong>500</strong> √† <strong>600.</strong> <br>  <strong>10</strong> flux de <strong>9,63 Gbits / s</strong> par interface, tandis que le nombre de retransmissions est pass√© √† une moyenne de <strong>17045.</strong> <br>  en <strong>100</strong> threads, le r√©sultat √©tait pire qu'en <strong>10</strong> , alors que le nombre de retransmissions est moindre: la valeur moyenne est de <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Deuxi√®me option </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Il y avait deux commutateurs Juniper EX4500.  Ils les ont rassembl√©s sur la pile, ont connect√© le serveur avec les premiers liens √† un commutateur, le second au second. <br>  La configuration de liaison initiale √©tait la suivante: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Les tests iperf et qperf ont montr√© Bw jusqu'√† <strong>16 Gbits / sec.</strong>  Nous avons d√©cid√© de comparer diff√©rents types de mod: <br>  <strong>rr, balance-xor et 802.3ad.</strong>  Nous avons √©galement compar√© diff√©rents types de hachage <strong>layer2 + 3 et layer3 + 4</strong> (en esp√©rant gagner un avantage sur le hash computing). <br>  Nous avons √©galement compar√© les r√©sultats pour diff√©rentes valeurs sysctl de la variable <strong>net.ipv4.fib_multipath_hash_policy,</strong> (enfin, nous avons jou√© un peu avec <strong>net.ipv4.tcp_congestion_control</strong> , bien que cela n'ait rien √† voir avec la <strong>liaison</strong> . Il y a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bon article ValdikSS</a> sur cette variable)). </p><br><p>  Mais dans tous les tests, cela n'a pas fonctionn√© pour d√©passer le seuil de <strong>18 Gbits / s</strong> (ce chiffre a √©t√© obtenu en utilisant <strong>balance-xor et 802.3ad</strong> , il n'y avait pas beaucoup de diff√©rence entre les r√©sultats du test) et cette valeur a √©t√© atteinte "en saut" par les rafales. </p><br><h3 id="tretiy-variant">  Troisi√®me option </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Pour configurer cette option, LACP a √©t√© supprim√© des commutateurs (l'empilement a √©t√© laiss√©, mais il a √©t√© utilis√© uniquement pour la gestion).  Sur chaque commutateur, nous avons collect√© un vlan distinct pour un groupe de ports (en pensant √† l'avenir que les serveurs QA et PROD seront coinc√©s dans les m√™mes commutateurs). </p><br><p>  Configur√© deux r√©seaux priv√©s plats pour chaque vlan (une interface par commutateur).  En plus de ces adresses se trouve l'annonce d'une autre adresse du troisi√®me r√©seau priv√©, qui est le r√©seau de clusters du CEPH. </p><br><p>  Comme le <em>r√©seau public</em> (via lequel nous utilisons SSH) fonctionne sur BGP, nous avons utilis√© frr pour configurer OSPF, qui est d√©j√† sur le syst√®me. </p><br><p>  <strong>10.10.10.0/24 et 20.20.20.0/24</strong> - deux r√©seaux plats sur les commutateurs </p><br><p>  <strong>172.16.1.0/24</strong> - r√©seau pour annonce </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Configuration de la machine: <br>  interfaces <strong>ens1f0 ens1f1</strong> regarder un r√©seau priv√© <br>  interfaces <strong>ens4f0 ens4f1</strong> regarder le r√©seau public </p><br><p>  La configuration r√©seau sur la machine ressemble √† ceci: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Les configurations de Frr ressemblent √† ceci: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  Sur ces param√®tres, le r√©seau teste iperf, qperf, etc.  a montr√© une utilisation maximale des deux canaux √† <strong>19,8 Gbit / s,</strong> tandis que la latence a chut√© √† <strong>20us</strong> </p><br><p>  <em><strong>Champ Bgp router-id:</strong> utilis√© pour identifier le n≈ìud lors du traitement des informations de routage et de la cr√©ation de routes.</em>  <em>Si elle n'est pas sp√©cifi√©e dans la configuration, l'une des adresses IP de l'h√¥te est s√©lectionn√©e.</em>  <em>Diff√©rents fabricants de mat√©riel et de logiciels peuvent avoir des algorithmes diff√©rents, dans notre cas, FRR a utilis√© la plus grande adresse IP de bouclage.</em>  <em>Cela a conduit √† deux probl√®mes:</em> <em><br></em>  <em>1) Si nous avons essay√© de raccrocher une autre adresse (par exemple, priv√©e du r√©seau 172.16.0.0) plus que celle actuelle, cela a conduit √† un changement d' <strong>ID de routeur</strong> et, par cons√©quent, √† r√©installer les sessions en cours.</em>  <em>Cela signifie une courte interruption et une perte de connectivit√© r√©seau.</em> <em><br></em>  <em>2) Si nous avons essay√© de raccrocher une adresse anycast partag√©e par plusieurs machines et qu'elle a √©t√© s√©lectionn√©e en tant <strong>qu'ID de routeur</strong> , deux n≈ìuds avec le m√™me <strong>ID de routeur</strong> sont apparus sur le r√©seau <strong>.</strong></em> </p><br><h2 id="chast-2">  2e partie </h2><br><p>  Apr√®s avoir test√© QA, nous avons commenc√© √† mettre √† niveau le combat Ceph. </p><br><h3 id="network">  R√âSEAU </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Passer d'un r√©seau √† deux </h3><br><p>  Le param√®tre r√©seau du cluster est l'un de ceux qui ne peuvent pas √™tre modifi√©s √† la vol√©e en sp√©cifiant l'OSD via <strong>ceph tell osd. * Injectargs.</strong>  Le changer dans la configuration et red√©marrer le cluster entier est une solution tol√©rable, mais je ne voulais vraiment pas avoir m√™me un petit temps d'arr√™t.  Il est √©galement impossible de red√©marrer un OSD avec un nouveau param√®tre r√©seau - √† un moment donn√©, nous aurions eu deux demi-clusters - d'anciens OSD sur l'ancien r√©seau, de nouveaux sur le nouveau.  Heureusement, le param√®tre r√©seau du cluster (ainsi que public_network, soit dit en passant) est une liste, c'est-√†-dire que vous pouvez sp√©cifier plusieurs valeurs.  Nous avons d√©cid√© de nous d√©placer progressivement - ajoutez d'abord un nouveau r√©seau aux configurations, puis supprimez l'ancien.  Ceph passe en revue la liste des r√©seaux de mani√®re s√©quentielle - OSD commence √† travailler en premier avec le r√©seau qui est r√©pertori√© en premier. </p><br><p>  La difficult√© √©tait que le premier r√©seau fonctionnait via bgp et √©tait connect√© √† un commutateur, et le second - √† ospf et connect√© √† d'autres qui n'√©taient pas physiquement connect√©s au premier.  Au moment de la transition, il √©tait n√©cessaire d'avoir temporairement acc√®s au r√©seau entre les deux r√©seaux.  La particularit√© de la configuration de notre usine √©tait que les ACL ne peuvent pas √™tre configur√©es sur le r√©seau si elles ne figurent pas dans la liste des publicit√©s (dans ce cas, elles sont ¬´externes¬ª et les ACL car elles ne peuvent √™tre cr√©√©es qu'en externe. Elle a √©t√© cr√©√©e en Espagne, mais n'est pas arriv√©e sur feuilles). </p><br><p>  La solution √©tait une b√©quille, compliqu√©e, mais cela a fonctionn√©: pour annoncer le r√©seau interne via bgp, simultan√©ment avec ospf. </p><br><p>  La s√©quence de transition est la suivante: </p><br><p>  1) Configurer le r√©seau de cluster pour ceph sur deux r√©seaux: via bgp et via ospf <br>  Dans les configurations FRR, il n'√©tait pas n√©cessaire de changer quoi que ce soit, une ligne </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  cela ne nous limite pas dans les adresses annonc√©es, l'adresse du r√©seau interne lui-m√™me a √©t√© relev√©e sur l'interface de bouclage, il suffisait de configurer la r√©ception de l'annonce de cette adresse sur les routeurs. </p><br><p>  2) Ajoutez un nouveau r√©seau √† la configuration <strong>ceph.conf</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  et commencez √† red√©marrer l'OSD un par un jusqu'√† ce que tout le monde <strong>passe au</strong> r√©seau <strong>172.16.1.0/24.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Ensuite, nous supprimons le r√©seau en exc√®s de la configuration </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  et r√©p√©tez la proc√©dure. </p><br><p>  C'est tout, nous avons d√©m√©nag√© en douceur vers un nouveau r√©seau. </p><br><p>  R√©f√©rences: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458390/">https://habr.com/ru/post/fr458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458376/index.html">Pas un autre langage de programmation. Partie 1: Logique de domaine</a></li>
<li><a href="../fr458378/index.html">Utilisation d'Avocode pour la mise en page du site. Revue pour les d√©butants. Bonus - enregistrez une p√©riode d'essai de 30 jours</a></li>
<li><a href="../fr458382/index.html">Pourquoi enseignons-nous cela?</a></li>
<li><a href="../fr458384/index.html">Examen et test du scanner 3D √† lumi√®re structur√©e Pro S3 HP</a></li>
<li><a href="../fr458388/index.html">Analyse approfondie des for√™ts et des articles (apprentissage + al√©atoire)</a></li>
<li><a href="../fr458394/index.html">S√©curisation des protocoles sans fil en utilisant LoRaWAN comme exemple</a></li>
<li><a href="../fr458396/index.html">Comment j'ai rendu le d√©veloppement sur Vue.js pratique avec le rendu c√¥t√© serveur</a></li>
<li><a href="../fr458398/index.html">L'hygi√®ne du travail √† distance ou les avantages de la t√©l√©pathie</a></li>
<li><a href="../fr458400/index.html">Architecture et impl√©mentation des microservices √âtape par √©tape, partie 1</a></li>
<li><a href="../fr458404/index.html">Transition du monolithe au microservices: histoire et pratique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>