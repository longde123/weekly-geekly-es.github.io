<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🤝‍👨🏼 🥩 🚴🏻 Ceph - de «à genoux» à «production» partie 2 👈🏼 ⛹🏾 🤚🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(première partie ici: https://habr.com/en/post/456446/ ) 
 Ceph 
 Présentation 


 Le réseau étant l'un des éléments clés de Ceph, et un peu spécifiqu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ceph - de «à genoux» à «production» partie 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458390/"><p>  (première partie ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://habr.com/en/post/456446/</a> ) </p><br><h1 id="ceph">  Ceph </h1><br><h3 id="vvedenie">  Présentation </h3><br><p>  Le réseau étant l'un des éléments clés de Ceph, et un peu spécifique à notre entreprise, nous allons d'abord vous en parler un peu. <br>  Il y aura beaucoup moins de descriptions de Ceph lui-même, principalement une infrastructure de réseau.  Seuls les serveurs Ceph et certaines fonctionnalités des serveurs de virtualisation Proxmox seront décrits. </p><a name="habracut"></a><br><p>  Donc: La topologie du réseau elle-même est conçue comme <strong>Leaf-Spine.</strong>  L'architecture classique à trois niveaux est un réseau où se trouvent le <strong>noyau</strong> (routeurs principaux), l' <strong>agrégation</strong> (routeurs d'agrégation) et directement connecté aux clients <strong>Access</strong> (routeurs d'accès): </p><br><p>  <strong>Schéma à trois niveaux</strong> </p><br><p><img src="https://habrastorage.org/webt/yf/e8/cm/yfe8cmp5qspkply3yniplpk53oo.jpeg"></p><br><p>  La topologie Leaf-Spine se compose de deux niveaux: <strong>Spine</strong> (grosso modo le routeur principal) et <strong>Leaf</strong> (branches). </p><br><p>  <strong>Schéma à deux niveaux</strong> </p><br><p><img src="https://habrastorage.org/webt/dw/ka/qo/dwkaqo4_ru7urikqyvmv3mqe8ik.jpeg"></p><br><p>  Tout le routage interne et externe est construit sur BGP.  Le système principal qui traite du contrôle d'accès, des annonces et plus est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>XCloud.</strong></a> <br>  Les serveurs, pour la réservation de canal (et aussi pour son expansion) sont connectés à deux commutateurs L3 (la plupart des serveurs sont connectés aux commutateurs Leaf, mais certains serveurs avec une charge réseau accrue sont connectés directement à la colonne vertébrale du commutateur), et via BGP, annoncer leur adresse de monodiffusion, ainsi que l'adresse anycast pour le service si plusieurs serveurs desservent le trafic de service et que l'équilibrage ECMP leur suffit.  Une caractéristique distincte de ce schéma, qui nous a permis d'économiser sur les adresses, mais a également obligé les ingénieurs à se familiariser avec le monde IPv6, était l'utilisation de la norme BGP non numérotée basée sur la RFC 5549. Pendant un certain temps, Quagga a été utilisé pour les serveurs en BGP pour ce schéma pour les serveurs et périodiquement. il y avait des problèmes avec la perte des fêtes et de la connectivité.  Mais après le passage à FRRouting (dont les contributeurs actifs sont nos fournisseurs d'équipements réseau: Cumulus et XCloudNetworks), nous n'avons plus observé de tels problèmes. </p><br><p>  Pour plus de commodité, nous appelons tout ce schéma général une "usine". </p><br><h2 id="poisk-puti">  Recherche de chemin </h2><br><p>  Options de configuration du réseau de cluster: </p><br><p>  1) Deuxième réseau sur BGP </p><br><p>  2) Le deuxième réseau sur deux commutateurs empilés séparés avec LACP </p><br><p>  3) Deuxième réseau sur deux commutateurs isolés séparés avec OSPF </p><br><h3 id="testy">  Les tests </h3><br><p>  Les tests ont été effectués en deux types: </p><br><p>  a) réseau utilisant les utilitaires iperf, qperf, nuttcp </p><br><p>  b) tests internes Ceph ceph-gobench, banc rados, créé rbd et testé sur eux en utilisant dd dans un ou plusieurs threads, en utilisant fio </p><br><p>  Tous les tests ont été effectués sur des machines de test avec des disques SAS.  Les chiffres de la performance rbd n'ont pas été beaucoup examinés, ils ont été utilisés uniquement à des fins de comparaison.  Intéressé par des changements en fonction du type de connexion. </p><br><h3 id="pervyy-variant">  Première option </h3><br><p>  <strong>Les cartes réseau sont connectées au BGP configuré en usine.</strong> </p><br><p>  L'utilisation de ce schéma pour le réseau interne n'a pas été considérée comme le meilleur choix: </p><br><p>  Tout d'abord, le nombre excessif d'éléments intermédiaires sous forme de commutateurs donnant une latence supplémentaire (c'était la raison principale). <br>  Deuxièmement, au départ, pour diffuser des données statiques via s3, ils ont utilisé une adresse anycast élevée sur plusieurs machines avec radosgateway.  Cela a entraîné le fait que le trafic des machines frontales vers RGW n'était pas réparti uniformément, mais passait par le chemin le plus court - c'est-à-dire que Nginx frontal se tournait toujours vers le même nœud avec RGW qui était connecté à la feuille partagée avec lui (cela, bien sûr, était pas l'argument principal - nous avons simplement refusé par la suite à partir des adresses anycast de retourner statique).  Mais pour la pureté de l'expérience, ils ont décidé de mener des tests sur un tel schéma afin d'avoir des données à comparer. </p><br><p>  Nous avions peur d'exécuter des tests pour toute la bande passante, car l'usine est utilisée par les serveurs prod, et si nous bloquions les liens entre les feuilles et la colonne vertébrale, cela nuirait à certaines ventes. <br>  En fait, c'était une autre raison de rejeter un tel régime. <br>  Des tests Iperf avec une limite BW de 3Gbps de 1, 10 et 100 flux ont été utilisés pour la comparaison avec d'autres schémas. <br>  Les tests ont montré les résultats suivants: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0a5/257/66b/0a525766bf7e61ffc4ba1129db0d17fd.png"></p><br><p>  en <strong>1</strong> flux, environ <strong>9,30 - 9,43 Gbits / sec</strong> (dans ce cas, le nombre de retransmissions croît fortement, à <strong>39148</strong> ).  Le chiffre avéré proche du maximum d'une interface suggère que l'une des deux est utilisée.  Le nombre de retransmissions est d'environ <strong>500</strong> à <strong>600.</strong> <br>  <strong>10</strong> flux de <strong>9,63 Gbits / s</strong> par interface, tandis que le nombre de retransmissions est passé à une moyenne de <strong>17045.</strong> <br>  en <strong>100</strong> threads, le résultat était pire qu'en <strong>10</strong> , alors que le nombre de retransmissions est moindre: la valeur moyenne est de <strong>3354</strong> </p><br><h3 id="vtoroy-variant">  Deuxième option </h3><br><p>  <strong>Lacp</strong> </p><br><p>  Il y avait deux commutateurs Juniper EX4500.  Ils les ont rassemblés sur la pile, ont connecté le serveur avec les premiers liens à un commutateur, le second au second. <br>  La configuration de liaison initiale était la suivante: </p><br><pre><code class="plaintext hljs">root@ceph01-test:~# cat /etc/network/interfaces auto ens3f0 iface ens3f0 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f0 rx 8192 post-up /sbin/ethtool -G ens3f0 tx 8192 post-up /sbin/ethtool -L ens3f0 combined 32 post-up /sbin/ip link set ens3f0 txqueuelen 10000 mtu 9000 auto ens3f1 iface ens3f1 inet manual bond-master bond0 post-up /sbin/ethtool -G ens3f1 rx 8192 post-up /sbin/ethtool -G ens3f1 tx 8192 post-up /sbin/ethtool -L ens3f1 combined 32 post-up /sbin/ip link set ens3f1 txqueuelen 10000 mtu 9000 auto bond0 iface bond0 inet static address 10.10.10.1 netmask 255.255.255.0 slaves none bond_mode 802.3ad bond_miimon 100 bond_downdelay 200 bond_xmit_hash_policy 3 #(layer3+4 ) mtu 9000</code> </pre> <br><p>  Les tests iperf et qperf ont montré Bw jusqu'à <strong>16 Gbits / sec.</strong>  Nous avons décidé de comparer différents types de mod: <br>  <strong>rr, balance-xor et 802.3ad.</strong>  Nous avons également comparé différents types de hachage <strong>layer2 + 3 et layer3 + 4</strong> (en espérant gagner un avantage sur le hash computing). <br>  Nous avons également comparé les résultats pour différentes valeurs sysctl de la variable <strong>net.ipv4.fib_multipath_hash_policy,</strong> (enfin, nous avons joué un peu avec <strong>net.ipv4.tcp_congestion_control</strong> , bien que cela n'ait rien à voir avec la <strong>liaison</strong> . Il y a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bon article ValdikSS</a> sur cette variable)). </p><br><p>  Mais dans tous les tests, cela n'a pas fonctionné pour dépasser le seuil de <strong>18 Gbits / s</strong> (ce chiffre a été obtenu en utilisant <strong>balance-xor et 802.3ad</strong> , il n'y avait pas beaucoup de différence entre les résultats du test) et cette valeur a été atteinte "en saut" par les rafales. </p><br><h3 id="tretiy-variant">  Troisième option </h3><br><p>  <strong>OSPF</strong> </p><br><p>  Pour configurer cette option, LACP a été supprimé des commutateurs (l'empilement a été laissé, mais il a été utilisé uniquement pour la gestion).  Sur chaque commutateur, nous avons collecté un vlan distinct pour un groupe de ports (en pensant à l'avenir que les serveurs QA et PROD seront coincés dans les mêmes commutateurs). </p><br><p>  Configuré deux réseaux privés plats pour chaque vlan (une interface par commutateur).  En plus de ces adresses se trouve l'annonce d'une autre adresse du troisième réseau privé, qui est le réseau de clusters du CEPH. </p><br><p>  Comme le <em>réseau public</em> (via lequel nous utilisons SSH) fonctionne sur BGP, nous avons utilisé frr pour configurer OSPF, qui est déjà sur le système. </p><br><p>  <strong>10.10.10.0/24 et 20.20.20.0/24</strong> - deux réseaux plats sur les commutateurs </p><br><p>  <strong>172.16.1.0/24</strong> - réseau pour annonce </p><br><p><img src="https://habrastorage.org/webt/t5/c5/fp/t5c5fpxxwqv7u82ywsvkuumcsag.jpeg"></p><br><p>  Configuration de la machine: <br>  interfaces <strong>ens1f0 ens1f1</strong> regarder un réseau privé <br>  interfaces <strong>ens4f0 ens4f1</strong> regarder le réseau public </p><br><p>  La configuration réseau sur la machine ressemble à ceci: </p><br><pre> <code class="plaintext hljs">oot@ceph01-test:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback auto ens1f0 iface ens1f0 inet static post-up /sbin/ethtool -G ens1f0 rx 8192 post-up /sbin/ethtool -G ens1f0 tx 8192 post-up /sbin/ethtool -L ens1f0 combined 32 post-up /sbin/ip link set ens1f0 txqueuelen 10000 mtu 9000 address 10.10.10.1/24 auto ens1f1 iface ens1f1 inet static post-up /sbin/ethtool -G ens1f1 rx 8192 post-up /sbin/ethtool -G ens1f1 tx 8192 post-up /sbin/ethtool -L ens1f1 combined 32 post-up /sbin/ip link set ens1f1 txqueuelen 10000 mtu 9000 address 20.20.20.1/24 auto ens4f0 iface ens4f0 inet manual post-up /sbin/ethtool -G ens4f0 rx 8192 post-up /sbin/ethtool -G ens4f0 tx 8192 post-up /sbin/ethtool -L ens4f0 combined 32 post-up /sbin/ip link set ens4f0 txqueuelen 10000 mtu 9000 auto ens4f1 iface ens4f1 inet manual post-up /sbin/ethtool -G ens4f1 rx 8192 post-up /sbin/ethtool -G ens4f1 tx 8192 post-up /sbin/ethtool -L ens4f1 combined 32 post-up /sbin/ip link set ens4f1 txqueuelen 10000 mtu 9000 #     loopback-: auto lo:0 iface lo:0 inet static address 55.66.77.88/32 dns-nameservers 55.66.77.88 auto lo:1 iface lo:1 inet static address 172.16.1.1/32</code> </pre> <br><p>  Les configurations de Frr ressemblent à ceci: </p><br><pre> <code class="plaintext hljs">root@ceph01-test:~# cat /etc/frr/frr.conf frr version 6.0 frr defaults traditional hostname ceph01-prod log file /var/log/frr/bgpd.log log timestamp precision 6 no ipv6 forwarding service integrated-vtysh-config username cumulus nopassword ! interface ens4f0 ipv6 nd ra-interval 10 ! interface ens4f1 ipv6 nd ra-interval 10 ! router bgp 65500 bgp router-id 55.66.77.88 # ,       timers bgp 10 30 neighbor ens4f0 interface remote-as 65001 neighbor ens4f0 bfd neighbor ens4f1 interface remote-as 65001 neighbor ens4f1 bfd ! address-family ipv4 unicast redistribute connected route-map redis-default exit-address-family ! router ospf ospf router-id 172.16.0.1 redistribute connected route-map ceph-loopbacks network 10.10.10.0/24 area 0.0.0.0 network 20.20.20.0/24 area 0.0.0.0 ! ip prefix-list ceph-loopbacks seq 10 permit 172.16.1.0/24 ge 32 ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32 ! route-map ceph-loopbacks permit 10 match ip address prefix-list ceph-loopbacks ! route-map redis-default permit 10 match ip address prefix-list default-out ! line vty !</code> </pre> <br><p>  Sur ces paramètres, le réseau teste iperf, qperf, etc.  a montré une utilisation maximale des deux canaux à <strong>19,8 Gbit / s,</strong> tandis que la latence a chuté à <strong>20us</strong> </p><br><p>  <em><strong>Champ Bgp router-id:</strong> utilisé pour identifier le nœud lors du traitement des informations de routage et de la création de routes.</em>  <em>Si elle n'est pas spécifiée dans la configuration, l'une des adresses IP de l'hôte est sélectionnée.</em>  <em>Différents fabricants de matériel et de logiciels peuvent avoir des algorithmes différents, dans notre cas, FRR a utilisé la plus grande adresse IP de bouclage.</em>  <em>Cela a conduit à deux problèmes:</em> <em><br></em>  <em>1) Si nous avons essayé de raccrocher une autre adresse (par exemple, privée du réseau 172.16.0.0) plus que celle actuelle, cela a conduit à un changement d' <strong>ID de routeur</strong> et, par conséquent, à réinstaller les sessions en cours.</em>  <em>Cela signifie une courte interruption et une perte de connectivité réseau.</em> <em><br></em>  <em>2) Si nous avons essayé de raccrocher une adresse anycast partagée par plusieurs machines et qu'elle a été sélectionnée en tant <strong>qu'ID de routeur</strong> , deux nœuds avec le même <strong>ID de routeur</strong> sont apparus sur le réseau <strong>.</strong></em> </p><br><h2 id="chast-2">  2e partie </h2><br><p>  Après avoir testé QA, nous avons commencé à mettre à niveau le combat Ceph. </p><br><h3 id="network">  RÉSEAU </h3><br><h3 id="pereezd-s-odnoy-seti-na-dve">  Passer d'un réseau à deux </h3><br><p>  Le paramètre réseau du cluster est l'un de ceux qui ne peuvent pas être modifiés à la volée en spécifiant l'OSD via <strong>ceph tell osd. * Injectargs.</strong>  Le changer dans la configuration et redémarrer le cluster entier est une solution tolérable, mais je ne voulais vraiment pas avoir même un petit temps d'arrêt.  Il est également impossible de redémarrer un OSD avec un nouveau paramètre réseau - à un moment donné, nous aurions eu deux demi-clusters - d'anciens OSD sur l'ancien réseau, de nouveaux sur le nouveau.  Heureusement, le paramètre réseau du cluster (ainsi que public_network, soit dit en passant) est une liste, c'est-à-dire que vous pouvez spécifier plusieurs valeurs.  Nous avons décidé de nous déplacer progressivement - ajoutez d'abord un nouveau réseau aux configurations, puis supprimez l'ancien.  Ceph passe en revue la liste des réseaux de manière séquentielle - OSD commence à travailler en premier avec le réseau qui est répertorié en premier. </p><br><p>  La difficulté était que le premier réseau fonctionnait via bgp et était connecté à un commutateur, et le second - à ospf et connecté à d'autres qui n'étaient pas physiquement connectés au premier.  Au moment de la transition, il était nécessaire d'avoir temporairement accès au réseau entre les deux réseaux.  La particularité de la configuration de notre usine était que les ACL ne peuvent pas être configurées sur le réseau si elles ne figurent pas dans la liste des publicités (dans ce cas, elles sont «externes» et les ACL car elles ne peuvent être créées qu'en externe. Elle a été créée en Espagne, mais n'est pas arrivée sur feuilles). </p><br><p>  La solution était une béquille, compliquée, mais cela a fonctionné: pour annoncer le réseau interne via bgp, simultanément avec ospf. </p><br><p>  La séquence de transition est la suivante: </p><br><p>  1) Configurer le réseau de cluster pour ceph sur deux réseaux: via bgp et via ospf <br>  Dans les configurations FRR, il n'était pas nécessaire de changer quoi que ce soit, une ligne </p><br><pre> <code class="plaintext hljs">ip prefix-list default-out seq 5 permit 0.0.0.0/0 ge 32</code> </pre> <br><p>  cela ne nous limite pas dans les adresses annoncées, l'adresse du réseau interne lui-même a été relevée sur l'interface de bouclage, il suffisait de configurer la réception de l'annonce de cette adresse sur les routeurs. </p><br><p>  2) Ajoutez un nouveau réseau à la configuration <strong>ceph.conf</strong> </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24, 55.66.77.88/27</code> </pre> <br><p>  et commencez à redémarrer l'OSD un par un jusqu'à ce que tout le monde <strong>passe au</strong> réseau <strong>172.16.1.0/24.</strong> </p><br><pre> <code class="plaintext hljs">root@ceph01-prod:~#ceph osd set noout # -          OSD #     .  ,     #  , OSD      30 . root@ceph01-prod:~#for i in $(ps ax | grep osd | grep -v grep| awk '{ print $10}'); \ root@ceph01-prod:~# do systemctl restart ceph-osd@$i; sleep 30; done</code> </pre> <br><p>  3) Ensuite, nous supprimons le réseau en excès de la configuration </p><br><pre> <code class="plaintext hljs">cluster network = 172.16.1.0/24</code> </pre> <br><p>  et répétez la procédure. </p><br><p>  C'est tout, nous avons déménagé en douceur vers un nouveau réseau. </p><br><p>  Références: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://shalaginov.com/2016/03/26/network-topology-leaf-spine/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.xcloudnetworks.com/case-studies/innova-case-study/</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/rumanzo/ceph-gobench</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458390/">https://habr.com/ru/post/fr458390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458376/index.html">Pas un autre langage de programmation. Partie 1: Logique de domaine</a></li>
<li><a href="../fr458378/index.html">Utilisation d'Avocode pour la mise en page du site. Revue pour les débutants. Bonus - enregistrez une période d'essai de 30 jours</a></li>
<li><a href="../fr458382/index.html">Pourquoi enseignons-nous cela?</a></li>
<li><a href="../fr458384/index.html">Examen et test du scanner 3D à lumière structurée Pro S3 HP</a></li>
<li><a href="../fr458388/index.html">Analyse approfondie des forêts et des articles (apprentissage + aléatoire)</a></li>
<li><a href="../fr458394/index.html">Sécurisation des protocoles sans fil en utilisant LoRaWAN comme exemple</a></li>
<li><a href="../fr458396/index.html">Comment j'ai rendu le développement sur Vue.js pratique avec le rendu côté serveur</a></li>
<li><a href="../fr458398/index.html">L'hygiène du travail à distance ou les avantages de la télépathie</a></li>
<li><a href="../fr458400/index.html">Architecture et implémentation des microservices Étape par étape, partie 1</a></li>
<li><a href="../fr458404/index.html">Transition du monolithe au microservices: histoire et pratique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>