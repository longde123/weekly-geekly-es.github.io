<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ÅâÔ∏è ü•Å üê± Neugier und Aufschub beim maschinellen Lernen üö© üë®‚Äçüë©‚Äçüë¶‚Äçüë¶ ü§∂üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforced Learning (RL) ist eine der vielversprechendsten Techniken des maschinellen Lernens, die aktiv entwickelt werden. Hier erh√§lt der KI-Agent e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neugier und Aufschub beim maschinellen Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427847/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reinforced Learning</a> (RL) ist eine der vielversprechendsten Techniken des maschinellen Lernens, die aktiv entwickelt werden.  Hier erh√§lt der KI-Agent eine positive Belohnung f√ºr die richtigen Aktionen und eine negative Belohnung f√ºr die falschen.  Diese Methode von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zuckerbrot und Peitsche</a> ist einfach und universell.  Damit brachte DeepMind dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DQN-</a> Algorithmus das Spielen alter Atari-Videospiele und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AlphaGoZero</a> das Spielen des alten Go-Spiels bei.  So brachte OpenAI dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI-Five-</a> Algorithmus das Spielen des modernen Dota-Videospiels bei, und Google brachte Roboterh√§nden das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erfassen neuer Objekte bei</a> .  Trotz des Erfolgs von RL gibt es immer noch viele Probleme, die die Wirksamkeit dieser Technik verringern. <br><br>  RL-Algorithmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">finden es schwierig,</a> in einer Umgebung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu arbeiten,</a> in der der Agent selten Feedback erh√§lt.  Dies ist jedoch typisch f√ºr die reale Welt.  Stellen Sie sich zum Beispiel vor, Sie suchen Ihren Lieblingsk√§se in einem gro√üen Labyrinth wie einem Supermarkt.  Sie suchen und suchen eine Abteilung mit K√§se, k√∂nnen diese aber nicht finden.  Wenn Sie bei jedem Schritt weder eine "Peitsche" noch eine "Karotte" bekommen, ist es unm√∂glich zu sagen, ob Sie sich in die richtige Richtung bewegen.  Was hindert Sie daran, f√ºr immer herumzuwandern, wenn Sie keine Belohnung erhalten?  Nichts als vielleicht deine Neugier.  Es motiviert zum Umzug in die Lebensmittelabteilung, die ungewohnt aussieht. <br><a name="habracut"></a><br>  Die wissenschaftliche Arbeit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûEpisodische Neugier durch Erreichbarkeit‚Äú</a> ist das Ergebnis einer Zusammenarbeit zwischen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem Google Brain-Team</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMind</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Swiss Higher Technical School in Z√ºrich</a> .  Wir bieten ein neues episodisches, auf dem Ged√§chtnis basierendes RL-Belohnungsmodell an.  Sie sieht aus wie eine Neugier, mit der Sie die Umgebung erkunden k√∂nnen.  Da der Agent nicht nur die Umgebung untersuchen, sondern auch das anf√§ngliche Problem l√∂sen muss, f√ºgt unser Modell der anf√§nglich sp√§rlichen Belohnung einen Bonus hinzu.  Die kombinierte Belohnung ist nicht mehr sp√§rlich, sodass die Standard-RL-Algorithmen daraus lernen k√∂nnen.  Unsere Neugiermethode erweitert somit das Spektrum der Aufgaben, die mit RL gel√∂st werden k√∂nnen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">Gelegentliche Neugier durch Erreichbarkeit: Beobachtungsdaten werden dem Speicher hinzugef√ºgt, die Belohnung wird basierend darauf berechnet, wie weit die aktuelle Beobachtung von √§hnlichen Beobachtungen im Speicher entfernt ist.</font></i>  <i><font color="gray">Der Agent erh√§lt eine gr√∂√üere Belohnung f√ºr Beobachtungen, die noch nicht im Ged√§chtnis vorhanden sind.</font></i> <br><br>  Die Hauptidee der Methode besteht darin, die Beobachtungen des Agenten √ºber die Umgebung im episodischen Ged√§chtnis zu speichern und den Agenten f√ºr das Anzeigen der Beobachtungen zu belohnen, die noch nicht im Ged√§chtnis vorhanden sind.  "Mangel an Ged√§chtnis" ist die Definition von Neuheit in unserer Methode.  Die Suche nach solchen Beobachtungen bedeutet die Suche nach einem Fremden.  Ein solches Verlangen, nach einem Fremden zu suchen, f√ºhrt den KI-Agenten an neue Orte, verhindert so das Wandern im Kreis und hilft ihm letztendlich, √ºber das Ziel zu stolpern.  Wie wir sp√§ter diskutieren, kann unser Wortlaut den Agenten von dem unerw√ºnschten Verhalten abhalten, dem einige andere Formulierungen unterliegen.  Zu unserer gro√üen √úberraschung hat dieses Verhalten einige √Ñhnlichkeiten mit dem, was ein Laie "Aufschub" nennen w√ºrde. <br><br><h4>  Vorherige Neugier </h4><br>  Obwohl in der Vergangenheit viele Versuche unternommen wurden, Neugierde zu formulieren <sup>[1] [2] [3] [4]</sup> , konzentrieren wir uns in diesem Artikel auf einen nat√ºrlichen und sehr beliebten Ansatz: Neugierde durch √úberraschung basierend auf Prognosen.  Diese Technik wird in einem k√ºrzlich erschienenen Artikel beschrieben: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûUntersuchen einer Umgebung mithilfe von Neugierde durch Vorhersagen unter eigener Kontrolle‚Äú</a> (normalerweise als ICM bezeichnet).  Um den Zusammenhang zwischen √úberraschung und Neugier zu veranschaulichen, verwenden wir wieder die Analogie, K√§se in einem Supermarkt zu finden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Illustration von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Indira Pasko</a> , lizenziert unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CC BY-NC-ND 4.0</a></font></i> <br><br>  Wenn Sie durch den Laden schlendern, versuchen Sie, die Zukunft vorherzusagen ( <i>"Jetzt bin ich in der Fleischabteilung, also denke ich, dass die Abteilung um die Ecke die Abteilung f√ºr Fisch ist, sie sind normalerweise in der N√§he dieser Supermarktkette"</i> ).  Wenn die Prognose falsch ist, sind Sie √ºberrascht ( <i>"Eigentlich gibt es eine Abteilung f√ºr Gem√ºse. Ich habe das nicht erwartet!"</i> ) - und auf diese Weise erhalten Sie eine Belohnung.  Dies erh√∂ht die Motivation in der Zukunft, wieder um die Ecke zu schauen und neue Orte zu erkunden, um zu √ºberpr√ºfen, ob Ihre Erwartungen wahr sind (und m√∂glicherweise √ºber K√§se zu stolpern). <br><br>  In √§hnlicher Weise erstellt die ICM-Methode ein Vorhersagemodell f√ºr die Dynamik der Welt und gibt dem Agenten eine Belohnung, wenn das Modell keine guten Vorhersagen trifft - ein Marker f√ºr √úberraschung oder Neuheit.  Bitte beachten Sie, dass das Erkunden neuer Orte nicht direkt in der Neugier des ICM zum Ausdruck kommt.  F√ºr die ICM-Methode ist die Teilnahme nur eine M√∂glichkeit, mehr ‚Äû√úberraschungen‚Äú zu erzielen und so Ihre Gesamtbelohnung zu maximieren.  Wie sich herausstellt, gibt es in einigen Umgebungen andere M√∂glichkeiten, sich selbst zu √ºberraschen, was zu unerwarteten Ergebnissen f√ºhrt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">Ein Agent mit einem auf √úberraschung basierenden Neugier-System friert ein, wenn er sich mit einem Fernseher trifft.</font></i>  <i><font color="gray">Animation aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deepak Pataks</a> Video, lizenziert unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CC BY 2.0</a></font></i> <br><br><h4>  Die Gefahr des Aufschubs </h4><br>  In dem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûEine gro√ü angelegte Studie zum neugierigen Lernen‚Äú zeigen die</a> Autoren der ICM-Methode zusammen mit OpenAI-Forschern eine versteckte Gefahr der Maximierung der √úberraschung: Agenten k√∂nnen lernen, sich dem Aufschieben hinzugeben, anstatt etwas N√ºtzliches f√ºr die Aufgabe zu tun.  Um zu verstehen, warum dies geschieht, betrachten Sie ein Gedankenexperiment, das die Autoren als ‚ÄûProblem des Fernsehrauschens‚Äú bezeichnen.  Hier wird der Agent in ein Labyrinth gebracht, um einen sehr n√ºtzlichen Gegenstand zu finden (wie in unserem Beispiel ‚ÄûK√§se‚Äú).  Die Umgebung verf√ºgt √ºber einen Fernseher und der Agent √ºber eine Fernbedienung.  Es gibt eine begrenzte Anzahl von Kan√§len (jeder Kanal hat eine separate √úbertragung), und jedes Dr√ºcken auf der Fernbedienung schaltet das Fernsehger√§t auf einen zuf√§lligen Kanal um.  Wie wird sich ein Agent in einer solchen Umgebung verhalten? <br><br>  Wenn Neugierde auf der Grundlage von √úberraschungen entsteht, wird ein Kanalwechsel mehr Belohnungen bringen, da jeder Wechsel unvorhersehbar und unerwartet ist.  Es ist wichtig zu beachten, dass selbst nach einem zyklischen Scan aller verf√ºgbaren Kan√§le eine zuf√§llige Auswahl eines Kanals sicherstellt, dass jede neue √Ñnderung immer noch unerwartet ist. Der Agent sagt voraus, dass nach dem Umschalten des Kanals TV angezeigt wird, und die Prognose wird h√∂chstwahrscheinlich falsch sein, was zu √úberraschungen f√ºhren wird.  Es ist wichtig zu beachten, dass die √Ñnderung immer noch unvorhersehbar ist, selbst wenn der Agent bereits jede √úbertragung auf jedem Kanal gesehen hat.  Aus diesem Grund bleibt der Agent, anstatt nach einem sehr n√ºtzlichen Gegenstand zu suchen, letztendlich vor dem Fernseher - √§hnlich wie beim Aufschieben.  Wie kann man den Wortlaut der Neugier √§ndern, um dieses Verhalten zu verhindern? <br><br><h4>  Episodische Neugier </h4><br>  In dem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûEpisodische Neugier durch Erreichbarkeit‚Äú</a> untersuchen wir ein auf episodischem Ged√§chtnis basierendes Neugiermodell, das weniger anf√§llig f√ºr sofortiges Vergn√ºgen ist.  Warum so?  Wenn wir das obige Beispiel nehmen, werden nach einiger Zeit beim Umschalten der Kan√§le alle √úbertragungen schlie√ülich im Speicher landen.  Dadurch verliert der Fernseher seine Attraktivit√§t: Auch wenn die Reihenfolge, in der die Programme auf dem Bildschirm angezeigt werden, zuf√§llig und unvorhersehbar ist, sind sie alle im Speicher!  Dies ist der Hauptunterschied zu der Methode, die auf √úberraschung basiert: Unsere Methode versucht nicht einmal, die Zukunft vorherzusagen, sie ist schwer vorherzusagen (oder sogar unm√∂glich).  Stattdessen untersucht der Agent die Vergangenheit und pr√ºft, ob im Speicher Beobachtungen <i>wie die</i> aktuelle vorhanden sind.  Daher ist unser Agent nicht anf√§llig f√ºr sofortige Freuden, die ein "Fernsehger√§usch" verursachen.  Der Agent muss die Welt au√üerhalb des Fernsehers erkunden, um mehr Belohnungen zu erhalten. <br><br>  Aber wie entscheiden wir, ob ein Agent dasselbe sieht, was im Speicher gespeichert ist?  Eine genaue √úbereinstimmungspr√ºfung ist sinnlos: In einer realen Umgebung sieht ein Agent selten zweimal dasselbe.  Selbst wenn der Agent beispielsweise in denselben Raum zur√ºckkehrt, sieht er diesen Raum immer noch aus einem anderen Blickwinkel. <br><br>  Anstatt nach genauen √úbereinstimmungen zu suchen, verwenden wir ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tiefes neuronales Netzwerk</a> , das darauf trainiert ist, zu messen, wie √§hnlich zwei Erfahrungen sind.  Um dieses Netzwerk zu trainieren, m√ºssen wir erraten, wie genau die Beobachtungen rechtzeitig stattgefunden haben.  Die zeitliche N√§he ist ein guter Indikator daf√ºr, ob zwei Beobachtungen als Teil derselben betrachtet werden sollten.  Ein solches Lernen f√ºhrt zu einem allgemeinen Konzept der Neuheit durch Erreichbarkeit, das unten dargestellt wird. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">Das Erreichbarkeitsdiagramm definiert die Neuheit.</font></i>  <i><font color="gray">In der Praxis ist dieses Diagramm nicht verf√ºgbar. Daher trainieren wir den Approximator f√ºr neuronale Netze, um die Anzahl der Schritte zwischen den Beobachtungen abzusch√§tzen</font></i> <br><br><h4>  Experimentelle Ergebnisse </h4><br>  Um die Leistung verschiedener Ans√§tze zur Beschreibung von Neugier zu vergleichen, haben wir sie in zwei visuell reichhaltigen 3D-Umgebungen getestet: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ViZDoom</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DMLab</a> .  Unter diesen Bedingungen erhielt der Agent verschiedene Aufgaben, z. B. ein Ziel im Labyrinth zu finden, gute Objekte zu sammeln und schlechte zu vermeiden.  In der DMLab-Umgebung ist der Agent standardm√§√üig mit einem fantastischen Gadget wie einem Laser ausgestattet. Wenn das Gadget jedoch f√ºr eine bestimmte Aufgabe nicht ben√∂tigt wird, kann der Agent es nicht frei verwenden.  Interessanterweise hat der ICM-Agent den Laser √ºberraschenderweise sehr oft verwendet, auch wenn es nutzlos war, die Aufgabe zu erledigen!  Wie im Fall des Fernsehers zog er es vor, Zeit damit zu verbringen, an den W√§nden zu schie√üen, anstatt nach einem wertvollen Gegenstand im Labyrinth zu suchen, da dies viele Belohnungen in Form von √úberraschungen brachte.  Theoretisch sollte das Ergebnis eines Wandschusses vorhersehbar sein, aber in der Praxis ist es zu schwierig vorherzusagen.  Dies erfordert wahrscheinlich tiefere Kenntnisse der Physik, als sie dem Standard-KI-Agenten zur Verf√ºgung stehen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">Der √ºberraschte ICM-Agent schie√üt st√§ndig in die Wand, anstatt das Labyrinth zu erkunden</font></i> <br><br>  Im Gegensatz zu ihm hat unser Agent vern√ºnftiges Verhalten beherrscht, um die Umwelt zu untersuchen.  Dies geschah, weil er nicht versucht, das Ergebnis seiner Handlungen vorherzusagen, sondern nach Beobachtungen sucht, die ‚Äûweiter‚Äú von denen entfernt sind, die sich im episodischen Ged√§chtnis befinden.  Mit anderen Worten, der Agent verfolgt implizit Ziele, die mehr Aufwand erfordern als ein einfacher Schuss auf die Wand. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Unsere Methode demonstriert intelligentes Umwelterkundungsverhalten.</font></i> <br><br>  Es ist interessant zu beobachten, wie unser Ansatz zur Belohnung einen in einem Kreis laufenden Agenten bestraft, da der Agent nach Abschluss des ersten Kreises keine neuen Beobachtungen findet und daher keine Belohnung erh√§lt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">Belohnungsvisualisierung: Rot entspricht einer negativen Belohnung, Gr√ºn einer positiven.</font></i>  <i><font color="gray">Von links nach rechts: Pr√§mienkarte, Karte mit Speicherorten, Ansicht aus der ersten Person</font></i> <br><br>  Gleichzeitig tr√§gt unsere Methode zu einer guten Untersuchung der Umwelt bei: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">Belohnungsvisualisierung: Rot entspricht einer negativen Belohnung, Gr√ºn einer positiven.</font></i>  <i><font color="gray">Von links nach rechts: Pr√§mienkarte, Karte mit Speicherorten, Ansicht aus der ersten Person</font></i> <br><br>  Wir hoffen, dass unsere Arbeit zu einer neuen Forschungswelle beitr√§gt, die √ºber den Rahmen der √úberraschungstechnik hinausgeht, um Agenten √ºber intelligenteres Verhalten aufzukl√§ren.  F√ºr eine eingehende Analyse unserer Methode werfen Sie bitte einen Blick auf den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorabdruck der wissenschaftlichen Arbeit</a> . <br><br><h4>  Danksagung: </h4><br>  Dieses Projekt ist das Ergebnis einer Zusammenarbeit zwischen dem Google Brain-Team DeepMind und der Swiss Higher Technical School in Z√ºrich.  Hauptforschungsgruppe: Nikolay Savinov, Anton Raichuk, Rafael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillirap und Sylvain Zheli.  Wir m√∂chten Olivier Pietkin, Carlos Riquelme, Charles Blundell und Sergey Levine f√ºr die Er√∂rterung dieses Dokuments danken.  Wir danken Indira Pasco f√ºr die Hilfe bei den Abbildungen. <br><br><h4>  Literaturhinweise: </h4><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Die Untersuchung der Umwelt basierend auf dem Z√§hlen mit Modellen der neuronalen Dichte"</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remy Munoz <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Äû</a> Z√§hlbasierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lernumgebungen f√ºr tiefes Lernen mit Verst√§rkung‚Äú</a> , Khaoran Tan, Rain Huthuft, Davis Foot, Adam Knock, Xi Chen, Yan Duan, John Schulman, Philip de Turk und Peter Abbel <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûLernen ohne Lehrer, um Ziele f√ºr intern motivierte Forschung zu finden‚Äú,</a> Alexander Pere, Sebastien Forestier, Olivier Sigot und Pierre-Yves Udaye <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"VIME: Intelligenz zur Maximierung von Informations√§nderungen",</a> Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de427847/">https://habr.com/ru/post/de427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de427837/index.html">Die ersten Tage im Entwicklungsteam - so wie es bei uns passiert</a></li>
<li><a href="../de427839/index.html">Benutzerautorisierung in Django durch GSSAPI und Delegierung von Benutzerrechten an den Server</a></li>
<li><a href="../de427841/index.html">Magic Leap Scam</a></li>
<li><a href="../de427843/index.html">Wie man richtig und falsch schl√§ft</a></li>
<li><a href="../de427845/index.html">Wie man eine Million Sterne in ein iPhone passt</a></li>
<li><a href="../de427849/index.html">Gerade mit TM. v3.0</a></li>
<li><a href="../de427853/index.html">Reflexionen √ºber TDD. Warum diese Methode nicht allgemein anerkannt ist</a></li>
<li><a href="../de427855/index.html">MOSDROID-Mitaps in FunCorp</a></li>
<li><a href="../de427857/index.html">Steuerliche und rechtliche Fragen f√ºr Anf√§nger</a></li>
<li><a href="../de427859/index.html">Warum technische F√§higkeiten dem Projektmanager: Erkl√§ren Sie die F√§lle</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>