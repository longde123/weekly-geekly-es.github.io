<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游 游뱟游 游눢 Aprendizaje autom치tico reforzado de redes neuronales profundas en tensorflow.js: trucos 游끧 游녝游낕 游뱊游낕</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entrenar redes neuronales profundas desde cero no es una tarea f치cil. 

 Se necesitan muchos datos y tiempo para aprender, pero algunos trucos pueden ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje autom치tico reforzado de redes neuronales profundas en tensorflow.js: trucos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/452612/">  Entrenar redes neuronales profundas desde cero no es una tarea f치cil. <br><br>  Se necesitan muchos datos y tiempo para aprender, pero algunos trucos pueden ayudar a acelerar el proceso, del que hablar칠 m치s adelante. <br><br>  Demostraci칩n del paso de un laberinto simple utilizando trucos.  Duraci칩n del entrenamiento de red: 1 hora 06 minutos.  Grabaci칩n acelerada 8 veces. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/KbuNjZKidpw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Para cada tarea, debe desarrollar su propio conjunto de trucos para acelerar el aprendizaje en red.  Compartir칠 algunos trucos que me ayudaron a entrenar la red mucho m치s r치pido. <br><br>  Para conocimientos te칩ricos, recomiendo cambiar al canal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sim0nsays</a> . <br>  Y contar칠 sobre mis modestos 칠xitos en el entrenamiento de redes neuronales. <br><br><h2>  Declaraci칩n del problema. </h2><br>  <i>Aproximar la funci칩n de convergencia minimizando la funci칩n de p칠rdida cuadr치tica mediante la propagaci칩n hacia atr치s del error por redes neuronales profundas.</i> <br><br>  Ten칤a una opci칩n de estrategia sobre c칩mo entrenar una red neuronal. <br>  Fomente la finalizaci칩n exitosa de la tarea o aliente a medida que se acerca a la finalizaci칩n de la tarea. <br><br>  Eleg칤 el segundo m칠todo, por dos razones: <br><br><ul><li>  La probabilidad de que la red llegue a la l칤nea de meta por s칤 sola es muy peque침a, por lo que estar치 condenada a recibir mucho refuerzo negativo.  Esto restablecer치 los pesos de todas las neuronas y la red no ser치 capaz de seguir entrenando. <br></li><li>  Las redes neuronales profundas son poderosas.  No excluyo que el primer m칠todo hubiera sido exitoso si tuviera una gran potencia inform치tica y mucho tiempo para la capacitaci칩n.  Tom칠 el camino de menor costo desarrollando trucos. <br></li></ul><br><h2>  Arquitectura de red neuronal </h2><br>  La arquitectura se est치 desarrollando experimentalmente, basada en la experiencia del arquitecto y la buena suerte. <br><br>  Arquitectura para resolver el problema: <br><br><ul><li>  3 neuronas de entrada: las coordenadas del agente y el valor de la c칠lula pasada (normalizamos en el rango de 0 a 1). <br></li><li>  2 capas ocultas de 256 y 128 neuronas (reducimos la dimensi칩n de las capas hacia la salida de la red). <br></li><li>  1 capa de ca칤da de neuronas aleatorias para la red de aprendizaje de sostenibilidad. <br></li><li>  4 neuronas de salida: la probabilidad de decidir qu칠 lado elegir para el siguiente paso. <br></li><li>  Funci칩n de activaci칩n neuronal: sigmoidea.  Optimizador: Adam. <br></li></ul><br>  sigmoid da 4 probabilidades en la salida en el rango de 0 a 1, eligiendo el m치ximo, obtenemos el lado para el siguiente paso: [jumpTop, jumpRight, jumpBottom, jumpLeft]. <br><br><h2>  Desarrollo de la arquitectura </h2><br>  La reentrenamiento ocurre cuando se usan modelos demasiado complejos. <br><br>  Esto es cuando la red record칩 los datos de entrenamiento y para los nuevos datos que la red a칰n no ha visto, funcionar치 mal porque la red no necesitaba buscar generalizaciones, ya que ten칤a suficiente memoria para memorizar. <br><br>  Falta de educaci칩n - con modelos insuficientemente complejos.  Esto es cuando la red ten칤a pocos datos de entrenamiento para encontrar generalizaciones. <br><br>  <b>Conclusi칩n: cuantas</b> m치s capas y neuronas contengan, m치s datos se necesitan para el entrenamiento. <br><br><h2>  Campo de juego </h2><br><img src="https://habrastorage.org/webt/3n/we/ck/3nweckh5jsx0-pfebojf_yq3n3k.png"><br><br><h3>  Reglas del juego </h3><br>  0: al ingresar a esta celda, el agente se destruye. <br>  1..44 - Celdas cuyos valores aumentan con cada paso. <br>  Cuanto m치s lejos vaya el agente, m치s recompensa recibir치. <br>  45 - Acabado.  Al mismo tiempo, el entrenamiento no ocurre, solo cuando todos los agentes son destruidos, y el final es una excepci칩n que simplemente usa la red ya entrenada para el pr칩ximo pron칩stico desde el comienzo del laberinto. <br><br><h2>  Descripci칩n de los par치metros. </h2><br>  El agente tiene una "antena" en cuatro direcciones: desempe침a el papel de inteligencia ambiental y es una descripci칩n de las coordenadas del agente y el valor de la celda en la que se encuentra. <br><br>  La descripci칩n juega el papel de predecir la siguiente direcci칩n para el movimiento del agente.  Es decir, el agente escanea de antemano lo que sigue y, en consecuencia, con el tiempo, la red aprende a moverse en la direcci칩n de aumentar el valor de la celda y no ir m치s all치 de los l칤mites del movimiento permisible. <br><br>  <b>El prop칩sito de la red neuronal:</b> obtener m치s recompensas. <br>  <b>Prop칩sito de aprendizaje: para</b> alentar las acciones correctas, cuanto m치s cerca est칠 el agente de resolver la tarea, mayor ser치 la recompensa para la red neuronal. <br><br><h2>  Trucos </h2><br>  Los primeros intentos de aprender sin trucos tomaron varias horas de entrenamiento y el resultado estuvo lejos de ser completo.  Aplicando ciertas t칠cnicas, 춰el resultado se logr칩 en solo una hora y seis minutos! <br><br><h3>  Agente en bucle </h3><br>  Durante el entrenamiento, la red comenz칩 a tomar decisiones, hacer movimientos de ida y vuelta, el problema del "uso".  Ambos movimientos dan a la red una recompensa positiva, que detuvo el proceso de exploraci칩n del laberinto y no permiti칩 salir del m칤nimo local. <br><br>  El primer intento de soluci칩n fue limitar el n칰mero de movimientos del agente, pero esto no fue 칩ptimo, ya que el agente pas칩 mucho tiempo en un ciclo antes de la autodestrucci칩n.  La mejor soluci칩n era destruir al agente si iba a la celda con un valor m치s bajo que el que estaba parado: la prohibici칩n de ir en la direcci칩n opuesta. <br><br><h3>  Investigar o usar </h3><br>  Se us칩 un truco simple para explorar los caminos alrededor de la posici칩n actual del agente: en cada paso, 5 agentes ser치n investigadores "voluntarios".  El curso de estos agentes se elegir치 al azar, y no por el pron칩stico de la red neuronal. <br><br>  Por lo tanto, tenemos una mayor probabilidad de que uno de los cinco agentes avance m치s que los dem치s y ayude a capacitar a la red con mejores resultados. <br><br><h3>  Algoritmo gen칠tico </h3><br>  Cada era, 500 agentes participan en el campo de juego.  Las predicciones para todos los agentes se realizan en modo as칤ncrono para todos los agentes a la vez, adem치s, los c치lculos se delegan a gpu.  Por lo tanto, obtenemos un uso m치s eficiente de la potencia inform치tica de la computadora, lo que conduce a una reducci칩n en el tiempo para predecir una red neuronal para 500 agentes al mismo tiempo. <br><br>  La predicci칩n funciona m치s r치pido que el entrenamiento, por lo que la red tiene m치s posibilidades de avanzar por el laberinto con la menor cantidad de tiempo y el mejor resultado. <br><br><h3>  Aprendiendo lo mejor de la generaci칩n </h3><br>  A lo largo de la era, para 500 agentes, se conservan los resultados de su avance a trav칠s del laberinto.  Cuando se destruye el 칰ltimo agente, se seleccionan los 5 mejores agentes de cada 500, que llegaron m치s lejos al laberinto. <br><br>  Con base en los mejores resultados de la era, se capacitar치 una red neuronal. <br><br>  Por lo tanto, reduciremos la cantidad de memoria utilizada al no guardar y no capacitar a la red en agentes que no avanzan la red. <br><br><h2>  Finalizaci칩n </h2><br>  Al no ser un especialista en este campo, logr칠 cierto 칠xito en el entrenamiento de la red neuronal, y usted tendr치 칠xito, 춰adelante! <br><br>  Esforzarse por aprender m치s r치pido que las computadoras, mientras lo hacemos mejor. <br><br><h3>  Materiales </h3><br>  <a href="">Repositorio con c칩digo</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Iniciar la capacitaci칩n del navegador</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La documentaci칩n de tensorflow.js</a> , donde tambi칠n puede encontrar recursos adicionales para el aprendizaje. <br><br><h3>  Libros </h3><br><ul><li>  Aprendizaje profundo  Inmersi칩n en el mundo de las redes neuronales. <br>  S. Nikolenko, A. Kadurin, E. Arkhangelskaya <br></li><li>  Aprendizaje autom치tico y TensorFlow <br>  N. Shakla <br></li><li>  Sistemas de autoaprendizaje <br>  S. I. Nikolenko, A. L. Tulupyev <br></li><li>  Entrenamiento de refuerzo <br>  R.S. Sutton, E.G. Barto <br></li><li>  Tarjetas autoorganizadas <br>  T. Kohonen <br></li></ul><br><h2>  Gracias por su atencion! </h2></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/452612/">https://habr.com/ru/post/452612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../452598/index.html">Gesti칩n de un equipo de programadores: 쯖칩mo y c칩mo motivarlos correctamente? Primera parte</a></li>
<li><a href="../452602/index.html">Cisco Hyperflex para sistemas de administraci칩n de bases de datos de alta carga</a></li>
<li><a href="../452606/index.html">UDB Que es esto Parte 8. Abordar UDB</a></li>
<li><a href="../452608/index.html">Parte 1. QInst: es mejor perder un d칤a, luego volar en cinco minutos (escribir instrumentos es trivial)</a></li>
<li><a href="../452610/index.html">Ayuda y solicitud por ella. Art칤culo sobre seguridad de la informaci칩n para usuarios comunes.</a></li>
<li><a href="../452614/index.html">C칩mo comenzar a programar en Adobe Illustrator. Parte dos</a></li>
<li><a href="../452618/index.html">Lo que se dijo en Google I / O 2019: Android 10, aplicaciones AR y mucho m치s</a></li>
<li><a href="../452620/index.html">Derivar un tipo de acci칩n usando el mecanografiado</a></li>
<li><a href="../452622/index.html">Introducci칩n a la gen칩mica para programadores</a></li>
<li><a href="../452624/index.html">Introducci칩n al actuador Spring Boot</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>