<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥔 🌘 👉 Cassandra Sink für Spark Structured Streaming 👈🏾 🧡 🤶🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor ein paar Monaten habe ich angefangen, Spark zu studieren, und irgendwann hatte ich das Problem, strukturierte Streaming-Berechnungen in der Cassan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink für Spark Structured Streaming</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Vor ein paar Monaten habe ich angefangen, Spark zu studieren, und irgendwann hatte ich das Problem, strukturierte Streaming-Berechnungen in der Cassandra-Datenbank zu speichern. <br><br>  In diesem Beitrag gebe ich ein einfaches Beispiel für das Erstellen und Verwenden von Cassandra Sink für Spark Structured Streaming.  Ich hoffe, dass der Beitrag für diejenigen nützlich sein wird, die kürzlich mit Spark Structured Streaming begonnen haben und sich fragen, wie sie die Berechnungsergebnisse in die Datenbank hochladen können. <br><br>  Die Idee der Anwendung ist sehr einfach: Nachrichten von Kafka zu empfangen und zu analysieren, einfache Transformationen in einem Paar durchzuführen und die Ergebnisse in Cassandra zu speichern. <br><a name="habracut"></a><br><h3>  Vorteile von strukturiertem Streaming </h3><br>  Weitere Informationen zu Structured Streaming finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> .  Kurz gesagt, Structured Streaming ist eine gut skalierbare Streaming-Informationsverarbeitungs-Engine, die auf der Spark SQL-Engine basiert.  Mit Dataset / DataFrame können Sie Daten aggregieren, Fensterfunktionen, Verbindungen usw. berechnen. Mit Structured Streaming können Sie das gute alte SQL für die Arbeit mit Datenströmen verwenden. <br><br><h3>  Was ist das Problem? </h3><br>  Die stabile Version von Spark Structured Streaming wurde 2017 veröffentlicht.  Das heißt, dies ist eine ziemlich neue API, die die Grundfunktionalität implementiert, aber einige Dinge müssen von uns selbst erledigt werden.  Beispielsweise verfügt Structured Streaming über Standardfunktionen zum Schreiben von Ausgaben in eine Datei, eine Kachel, eine Konsole oder einen Speicher. Um jedoch Daten in der Datenbank zu speichern, müssen Sie den in Structured Streaming verfügbaren <i>foreach-</i> Empfänger verwenden und die <i>ForeachWriter-</i> Schnittstelle implementieren.  <b>Ab Spark 2.3.1 kann diese Funktionalität nur in Scala und Java implementiert werden</b> . <br><br>  Ich gehe davon aus, dass der Leser bereits weiß, wie strukturiertes Streaming allgemein funktioniert, wie er die erforderlichen Transformationen implementiert und nun bereit ist, die Ergebnisse in die Datenbank hochzuladen.  Wenn einige der oben genannten Schritte unklar sind, kann die offizielle Dokumentation als guter Ausgangspunkt für das Erlernen von strukturiertem Streaming dienen.  In diesem Artikel möchte ich mich auf den letzten Schritt konzentrieren, wenn Sie die Ergebnisse in einer Datenbank speichern müssen. <br><br>  Im Folgenden werde ich eine Beispielimplementierung der Cassandra-Senke für strukturiertes Streaming beschreiben und erläutern, wie sie in einem Cluster ausgeführt wird.  Den vollständigen Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  Als ich zum ersten Mal auf das oben genannte Problem stieß, erwies sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses Projekt</a> als sehr nützlich.  Es mag jedoch etwas kompliziert erscheinen, wenn der Leser gerade mit Structured Streaming begonnen hat und nach einem einfachen Beispiel für das Hochladen von Daten auf Cassandra sucht.  Darüber hinaus ist das Projekt für die Arbeit im lokalen Modus geschrieben und erfordert einige Änderungen, um im Cluster ausgeführt zu werden. <br><br>  Ich möchte auch Beispiele geben, wie Daten mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JDBC</a> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MongoDB</a> und jeder anderen Datenbank gespeichert werden. <br><br><h3>  Einfache Lösung </h3><br>  Um Daten auf ein externes System hochzuladen, müssen Sie den <i>foreach-</i> Empfänger verwenden.  Lesen Sie hier mehr darüber.  Kurz gesagt, die <i>ForeachWriter-</i> Schnittstelle muss implementiert sein.  Das heißt, es muss festgelegt werden, wie die Verbindung geöffnet werden soll, wie jedes Datenelement verarbeitet werden soll und wie die Verbindung am Ende der Verarbeitung geschlossen werden soll.  Der Quellcode lautet wie folgt: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  Ich werde die Definition von <i>CassandraDriver</i> und die Struktur der Ausgabetabelle später beschreiben, aber jetzt schauen wir uns genauer an, wie der obige Code funktioniert.  Um von Spark aus eine Verbindung zu Kasandra herzustellen, erstelle ich ein <i>CassandraDriver-</i> Objekt, das den Zugriff auf den von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DataStax</a> entwickelten <i>CassandraConnector</i> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ermöglicht</a> .  Der CassandraConnector ist für das Öffnen und Schließen der Verbindung zur Datenbank verantwortlich. Daher zeige ich nur Debugging-Meldungen in den Methoden <i>open</i> und <i>close</i> der <i>CassandraSinkForeach-</i> Klasse an. <br><br>  Der obige Code wird von der Hauptanwendung wie folgt aufgerufen: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> wird für jede <i>Datenzeile</i> erstellt, sodass jeder Arbeitsknoten seinen Teil der Zeilen in die Datenbank einfügt.  Das heißt, jeder Arbeitsknoten führt <i>val cassandraDriver = new CassandraDriver () aus;</i>  So sieht CassandraDriver aus: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Schauen wir uns das Funkenobjekt genauer an.  Der Code für <i>SparkSessionBuilder lautet</i> wie folgt: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Auf jedem Arbeitsknoten bietet <i>SparkSessionBuilder</i> Zugriff auf die <i>SparkSession</i> , die auf dem Treiber erstellt wurde.  Um einen solchen Zugriff zu ermöglichen, muss <i>SparkSessionBuilder</i> serialisiert und ein <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">vorübergehender</a> Lazy Val verwendet werden</i> , wodurch das Serialisierungssystem <i>conf-</i> und <i>spark-</i> Objekte ignorieren <i>kann</i> , wenn das Programm initialisiert wird und bis auf die Objekte zugegriffen wird.  Wenn das <i>buildSparkSession-</i> Programm <i>gestartet</i> wird <i>, wird es</i> serialisiert und an jeden Arbeitsknoten gesendet. <i>Conf-</i> und <i>Spark-</i> Objekte sind jedoch nur zulässig, wenn der Arbeitsknoten auf sie zugreift. <br><br>  Schauen wir uns nun den Hauptanwendungscode an: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Wenn die Anwendung zur Ausführung <i>gesendet wird</i> , wird <i>buildSparkSession</i> serialisiert und an die Arbeitsknoten gesendet. <i>Conf-</i> und <i>Spark-</i> Objekte bleiben jedoch ungelöst.  Anschließend erstellt der Treiber ein <i>Funkenobjekt</i> in <i>KafkaToCassandra</i> und verteilt die Arbeit auf die Arbeitsknoten.  Jeder Arbeitsknoten liest Daten aus Kafka, führt einfache Transformationen für den empfangenen Teil der Datensätze durch. Wenn der Arbeitsknoten bereit ist, die Ergebnisse in die Datenbank zu schreiben, ermöglicht er <i>Conf-</i> und <i>Spark-</i> Objekte und erhält so Zugriff auf die auf dem Treiber erstellte <i>SparkSession</i> . <br><br><h3>  Wie erstelle ich die Anwendung und führe sie aus? </h3><br>  Als ich von PySpark zu Scala wechselte, brauchte ich eine Weile, um herauszufinden, wie die Anwendung erstellt werden sollte.  Deshalb habe ich Maven <i>pom.xml</i> in mein Projekt aufgenommen.  Der Reader kann die Anwendung mit Maven erstellen, indem er den Befehl <i>mvn package ausführt</i> .  Nachdem die Anwendung zur Ausführung gesendet werden kann mit <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Um die Anwendung zu erstellen und auszuführen, müssen Sie die Namen meiner AWS-Computer durch Ihre eigenen ersetzen (d. H. Alles ersetzen, was wie ec2-xx-xxx-xx-xx.compute-1.amazonaws.com aussieht). <br><br>  Insbesondere Spark und Structured Streaming ist für mich ein neues Thema, daher bin ich den Lesern für Kommentare, Diskussionen und Korrekturen sehr dankbar. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de425503/">https://habr.com/ru/post/de425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de425489/index.html">Automatisierung: Übertriebene Roboterbedrohung</a></li>
<li><a href="../de425493/index.html">MikroTik hAP mini für IPTV Beeline konfigurieren</a></li>
<li><a href="../de425497/index.html">Tutu PHP Meetup # 2: Live-Übertragung von Ereignissen</a></li>
<li><a href="../de425499/index.html">HyperX Impact DDR4 - SO-DIMM, das könnte! Oder warum in einem Laptop 64 GB Speicher mit einer Frequenz von 3200 MHz?</a></li>
<li><a href="../de425501/index.html">A / B-Tests auf Android von A bis Z.</a></li>
<li><a href="../de425505/index.html">Analyse des Linux-Kernel-Boot-Prozesses</a></li>
<li><a href="../de425507/index.html">Parsim Wikipedia für NLP-Aufgaben in 4 Teams</a></li>
<li><a href="../de425511/index.html">Nicht offensichtliche Funktionen der Rotativa-Anwendung zum Generieren von PDF in der ASP.NET MVC-Anwendung</a></li>
<li><a href="../de425515/index.html">Apple blockiert die unabhängige Reparatur neuer MacBook-Modelle</a></li>
<li><a href="../de425517/index.html">Wie Yandex mithilfe von Radar und Satelliten eine globale Niederschlagsvorhersage erstellte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>