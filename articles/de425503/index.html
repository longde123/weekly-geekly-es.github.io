<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•î üåò üëâ Cassandra Sink f√ºr Spark Structured Streaming üëàüèæ üß° ü§∂üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor ein paar Monaten habe ich angefangen, Spark zu studieren, und irgendwann hatte ich das Problem, strukturierte Streaming-Berechnungen in der Cassan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink f√ºr Spark Structured Streaming</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Vor ein paar Monaten habe ich angefangen, Spark zu studieren, und irgendwann hatte ich das Problem, strukturierte Streaming-Berechnungen in der Cassandra-Datenbank zu speichern. <br><br>  In diesem Beitrag gebe ich ein einfaches Beispiel f√ºr das Erstellen und Verwenden von Cassandra Sink f√ºr Spark Structured Streaming.  Ich hoffe, dass der Beitrag f√ºr diejenigen n√ºtzlich sein wird, die k√ºrzlich mit Spark Structured Streaming begonnen haben und sich fragen, wie sie die Berechnungsergebnisse in die Datenbank hochladen k√∂nnen. <br><br>  Die Idee der Anwendung ist sehr einfach: Nachrichten von Kafka zu empfangen und zu analysieren, einfache Transformationen in einem Paar durchzuf√ºhren und die Ergebnisse in Cassandra zu speichern. <br><a name="habracut"></a><br><h3>  Vorteile von strukturiertem Streaming </h3><br>  Weitere Informationen zu Structured Streaming finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> .  Kurz gesagt, Structured Streaming ist eine gut skalierbare Streaming-Informationsverarbeitungs-Engine, die auf der Spark SQL-Engine basiert.  Mit Dataset / DataFrame k√∂nnen Sie Daten aggregieren, Fensterfunktionen, Verbindungen usw. berechnen. Mit Structured Streaming k√∂nnen Sie das gute alte SQL f√ºr die Arbeit mit Datenstr√∂men verwenden. <br><br><h3>  Was ist das Problem? </h3><br>  Die stabile Version von Spark Structured Streaming wurde 2017 ver√∂ffentlicht.  Das hei√üt, dies ist eine ziemlich neue API, die die Grundfunktionalit√§t implementiert, aber einige Dinge m√ºssen von uns selbst erledigt werden.  Beispielsweise verf√ºgt Structured Streaming √ºber Standardfunktionen zum Schreiben von Ausgaben in eine Datei, eine Kachel, eine Konsole oder einen Speicher. Um jedoch Daten in der Datenbank zu speichern, m√ºssen Sie den in Structured Streaming verf√ºgbaren <i>foreach-</i> Empf√§nger verwenden und die <i>ForeachWriter-</i> Schnittstelle implementieren.  <b>Ab Spark 2.3.1 kann diese Funktionalit√§t nur in Scala und Java implementiert werden</b> . <br><br>  Ich gehe davon aus, dass der Leser bereits wei√ü, wie strukturiertes Streaming allgemein funktioniert, wie er die erforderlichen Transformationen implementiert und nun bereit ist, die Ergebnisse in die Datenbank hochzuladen.  Wenn einige der oben genannten Schritte unklar sind, kann die offizielle Dokumentation als guter Ausgangspunkt f√ºr das Erlernen von strukturiertem Streaming dienen.  In diesem Artikel m√∂chte ich mich auf den letzten Schritt konzentrieren, wenn Sie die Ergebnisse in einer Datenbank speichern m√ºssen. <br><br>  Im Folgenden werde ich eine Beispielimplementierung der Cassandra-Senke f√ºr strukturiertes Streaming beschreiben und erl√§utern, wie sie in einem Cluster ausgef√ºhrt wird.  Den vollst√§ndigen Code finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  Als ich zum ersten Mal auf das oben genannte Problem stie√ü, erwies sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses Projekt</a> als sehr n√ºtzlich.  Es mag jedoch etwas kompliziert erscheinen, wenn der Leser gerade mit Structured Streaming begonnen hat und nach einem einfachen Beispiel f√ºr das Hochladen von Daten auf Cassandra sucht.  Dar√ºber hinaus ist das Projekt f√ºr die Arbeit im lokalen Modus geschrieben und erfordert einige √Ñnderungen, um im Cluster ausgef√ºhrt zu werden. <br><br>  Ich m√∂chte auch Beispiele geben, wie Daten mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JDBC</a> in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MongoDB</a> und jeder anderen Datenbank gespeichert werden. <br><br><h3>  Einfache L√∂sung </h3><br>  Um Daten auf ein externes System hochzuladen, m√ºssen Sie den <i>foreach-</i> Empf√§nger verwenden.  Lesen Sie hier mehr dar√ºber.  Kurz gesagt, die <i>ForeachWriter-</i> Schnittstelle muss implementiert sein.  Das hei√üt, es muss festgelegt werden, wie die Verbindung ge√∂ffnet werden soll, wie jedes Datenelement verarbeitet werden soll und wie die Verbindung am Ende der Verarbeitung geschlossen werden soll.  Der Quellcode lautet wie folgt: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  Ich werde die Definition von <i>CassandraDriver</i> und die Struktur der Ausgabetabelle sp√§ter beschreiben, aber jetzt schauen wir uns genauer an, wie der obige Code funktioniert.  Um von Spark aus eine Verbindung zu Kasandra herzustellen, erstelle ich ein <i>CassandraDriver-</i> Objekt, das den Zugriff auf den von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DataStax</a> entwickelten <i>CassandraConnector</i> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erm√∂glicht</a> .  Der CassandraConnector ist f√ºr das √ñffnen und Schlie√üen der Verbindung zur Datenbank verantwortlich. Daher zeige ich nur Debugging-Meldungen in den Methoden <i>open</i> und <i>close</i> der <i>CassandraSinkForeach-</i> Klasse an. <br><br>  Der obige Code wird von der Hauptanwendung wie folgt aufgerufen: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> wird f√ºr jede <i>Datenzeile</i> erstellt, sodass jeder Arbeitsknoten seinen Teil der Zeilen in die Datenbank einf√ºgt.  Das hei√üt, jeder Arbeitsknoten f√ºhrt <i>val cassandraDriver = new CassandraDriver () aus;</i>  So sieht CassandraDriver aus: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Schauen wir uns das Funkenobjekt genauer an.  Der Code f√ºr <i>SparkSessionBuilder lautet</i> wie folgt: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  Auf jedem Arbeitsknoten bietet <i>SparkSessionBuilder</i> Zugriff auf die <i>SparkSession</i> , die auf dem Treiber erstellt wurde.  Um einen solchen Zugriff zu erm√∂glichen, muss <i>SparkSessionBuilder</i> serialisiert und ein <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">vor√ºbergehender</a> Lazy Val verwendet werden</i> , wodurch das Serialisierungssystem <i>conf-</i> und <i>spark-</i> Objekte ignorieren <i>kann</i> , wenn das Programm initialisiert wird und bis auf die Objekte zugegriffen wird.  Wenn das <i>buildSparkSession-</i> Programm <i>gestartet</i> wird <i>, wird es</i> serialisiert und an jeden Arbeitsknoten gesendet. <i>Conf-</i> und <i>Spark-</i> Objekte sind jedoch nur zul√§ssig, wenn der Arbeitsknoten auf sie zugreift. <br><br>  Schauen wir uns nun den Hauptanwendungscode an: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Wenn die Anwendung zur Ausf√ºhrung <i>gesendet wird</i> , wird <i>buildSparkSession</i> serialisiert und an die Arbeitsknoten gesendet. <i>Conf-</i> und <i>Spark-</i> Objekte bleiben jedoch ungel√∂st.  Anschlie√üend erstellt der Treiber ein <i>Funkenobjekt</i> in <i>KafkaToCassandra</i> und verteilt die Arbeit auf die Arbeitsknoten.  Jeder Arbeitsknoten liest Daten aus Kafka, f√ºhrt einfache Transformationen f√ºr den empfangenen Teil der Datens√§tze durch. Wenn der Arbeitsknoten bereit ist, die Ergebnisse in die Datenbank zu schreiben, erm√∂glicht er <i>Conf-</i> und <i>Spark-</i> Objekte und erh√§lt so Zugriff auf die auf dem Treiber erstellte <i>SparkSession</i> . <br><br><h3>  Wie erstelle ich die Anwendung und f√ºhre sie aus? </h3><br>  Als ich von PySpark zu Scala wechselte, brauchte ich eine Weile, um herauszufinden, wie die Anwendung erstellt werden sollte.  Deshalb habe ich Maven <i>pom.xml</i> in mein Projekt aufgenommen.  Der Reader kann die Anwendung mit Maven erstellen, indem er den Befehl <i>mvn package ausf√ºhrt</i> .  Nachdem die Anwendung zur Ausf√ºhrung gesendet werden kann mit <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Um die Anwendung zu erstellen und auszuf√ºhren, m√ºssen Sie die Namen meiner AWS-Computer durch Ihre eigenen ersetzen (d. H. Alles ersetzen, was wie ec2-xx-xxx-xx-xx.compute-1.amazonaws.com aussieht). <br><br>  Insbesondere Spark und Structured Streaming ist f√ºr mich ein neues Thema, daher bin ich den Lesern f√ºr Kommentare, Diskussionen und Korrekturen sehr dankbar. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de425503/">https://habr.com/ru/post/de425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de425489/index.html">Automatisierung: √úbertriebene Roboterbedrohung</a></li>
<li><a href="../de425493/index.html">MikroTik hAP mini f√ºr IPTV Beeline konfigurieren</a></li>
<li><a href="../de425497/index.html">Tutu PHP Meetup # 2: Live-√úbertragung von Ereignissen</a></li>
<li><a href="../de425499/index.html">HyperX Impact DDR4 - SO-DIMM, das k√∂nnte! Oder warum in einem Laptop 64 GB Speicher mit einer Frequenz von 3200 MHz?</a></li>
<li><a href="../de425501/index.html">A / B-Tests auf Android von A bis Z.</a></li>
<li><a href="../de425505/index.html">Analyse des Linux-Kernel-Boot-Prozesses</a></li>
<li><a href="../de425507/index.html">Parsim Wikipedia f√ºr NLP-Aufgaben in 4 Teams</a></li>
<li><a href="../de425511/index.html">Nicht offensichtliche Funktionen der Rotativa-Anwendung zum Generieren von PDF in der ASP.NET MVC-Anwendung</a></li>
<li><a href="../de425515/index.html">Apple blockiert die unabh√§ngige Reparatur neuer MacBook-Modelle</a></li>
<li><a href="../de425517/index.html">Wie Yandex mithilfe von Radar und Satelliten eine globale Niederschlagsvorhersage erstellte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>