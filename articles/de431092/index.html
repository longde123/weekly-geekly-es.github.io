<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÑ ü•® ü§≤üèæ ROS: Tiefenkarte auf dem Raspberry Pi "low blood" üßñüèª ü§±üèº üè¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Sie beim Erstellen von Robotern ROS verwenden, wissen Sie wahrscheinlich, dass es die Arbeit mit Stereokameras unterst√ºtzt. Sie k√∂nnen beispielsw...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>ROS: Tiefenkarte auf dem Raspberry Pi "low blood"</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/431092/"><img src="https://habrastorage.org/webt/jl/ks/vn/jlksvndgnvhxzgvr_jmo19ni580.jpeg" alt="Bild"><br><br>  Wenn Sie beim Erstellen von Robotern ROS verwenden, wissen Sie wahrscheinlich, dass es die Arbeit mit Stereokameras unterst√ºtzt.  Sie k√∂nnen beispielsweise eine Tiefenkarte des sichtbaren Teils des Raums oder eine Punktwolke erstellen.  Und ich fragte mich, wie einfach es sein w√ºrde, eine Himbeer-basierte StereoPi-Stereokamera in ROS zu verwenden.  Fr√ºher war ich bereits davon √ºberzeugt, dass die Tiefenkarte perfekt von OpenCV erstellt wurde, aber ich habe mich nie mit ROS befasst.  Und ich habe beschlossen, es zu versuchen.  Ich m√∂chte √ºber meine Abenteuer bei der Suche nach einer L√∂sung sprechen. <br><a name="habracut"></a><br><h3>  1. Gibt es ROS auf dem Raspberry Pi? </h3><br>  Zuerst habe ich mich entschlossen herauszufinden, ob es m√∂glich ist, ROS f√ºr den Raspberry Pi zu bauen.  Das erste, was Google mir sagte, war eine Liste mit Anweisungen zum Installieren verschiedener Versionen von ROS auf dem Raspberry Pi, n√§mlich diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ROS-Wiki-</a> Seite <br><br>  Nun, es gibt schon etwas, von dem aus man anfangen kann!  Ich erinnerte mich gut daran, wie lange es gedauert hatte, OpenCV auf Raspberry zu erstellen (ungef√§hr acht Stunden), und entschied mich daher, nach vorgefertigten Bildern von MicroSD-Karten zu suchen, um Zeit zu sparen. <br><br><h3>  2. Gibt es fertige microSD-Kartenbilder mit ROS f√ºr Himbeere? </h3><br>  Es stellte sich heraus, dass dieses Problem bereits von mehreren Entwicklungsteams behoben wurde.  Wenn Sie keine einmaligen Builds von Enthusiasten erstellen, fallen einige Bilder auf, die mit der Ver√∂ffentlichung neuer Versionen von OS und ROS st√§ndig aktualisiert werden. <br><br>  Die erste Option ist ROS, das vom ROSbots-Team auf dem nativen Raspbian-Betriebssystem installiert wurde. Hier ist eine Seite mit einem aktualisierten Image-Link: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ready-to-Use-Image-Raspbian-Stretch-Ros-OpenCV</a> <br><br>  Das zweite sind die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bilder von Ubiquiti Robotics auf Ubuntu</a> . <br><br>  Nun, die zweite Frage war auch schnell genug geschlossen.  Es ist Zeit, tiefer zu tauchen. <br><br><h3>  3. Wie funktioniert ROS mit der Raspberry Pi-Kamera? </h3><br>  Und welche Stereokameras werden in ROS generell unterst√ºtzt?  Ich habe mir die Seite mit Stereokameras angesehen, f√ºr die die Verf√ºgbarkeit von vorgefertigten Treibern f√ºr ROS deklariert wurde: <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wiki.ros.org/Sensors</a> <br><br>  Es gab zwei Abschnitte: <br>  <i><b>2.3 3D-Sensoren (Entfernungsmesser und RGB-D-Kameras)</b></i> <i><br></i>  <i><b>2.5 Kameras</b></i> <br>  Es stellte sich heraus, dass im ersten Abschnitt nicht nur Stereokameras, sondern auch TOF-Sensoren und Scan-Lidars aufgelistet sind - im Allgemeinen alles, was sofort Informationen in 3D liefern kann.  Und im zweiten gibt es schon Stereokameras.  Der Versuch, Treiber f√ºr mehrere Stereokameras zu sehen, trug nicht zu meiner Freude bei, da dies auf ein ernsthaftes Eintauchen in den Code hindeutete. <br><br>  Okay, tritt einen Schritt zur√ºck.  Wie funktioniert es mit einer einzelnen Raspberry Pi-Kamera in ROS? <br><br>  Drei angenehme √úberraschungen erwarteten mich hier: <br><br><ul><li>  Es stellt sich heraus, dass es f√ºr ROS einen speziellen <b><i>raspicam_node-</i></b> Knoten gibt, der nur f√ºr die Arbeit mit der Raspberry Pi-Kamera vorgesehen ist </li><li>  Arten des Knotens liegen auf dem Github, der Code wird aktiv gepflegt und gut dokumentiert: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/UbiquityRobotics/raspicam_node</a> </li><li>  Der Autor des Rohan Agrawal-Knotens ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@Rohbotics</a> ) arbeitet f√ºr ein Unternehmen, das eines der vorgefertigten Bilder f√ºr den Raspberry Pi aktiv unterst√ºtzt </li></ul><br>  Ich habe mir das Github-Repository raspicam_node angesehen und mir Probleme angesehen.  Dort fand ich vor fast sieben Monaten ein offenes Problem mit dem ger√§umigen Namen "Stereomodus", ohne Antworten und Kommentare.  Tats√§chlich haben sich darin alle Ereignisse weiterentwickelt. <br><br><h3>  4. Hardcore oder nicht? </h3><br>  Um den Autoren keine Kinderfragen zu stellen, habe ich mich entschlossen, den Quellcode zu betrachten und die Gefahr des Hinzuf√ºgens des Stereomodus zu bewerten.  Der Systemteil hier interessierte mich mehr: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/UbiquityRobotics/raspicam_node/tree/kinetic/src</a> <br>  Nun, die Jungs haben geschrieben, dass der Fahrer in das MMAL-Level eingestiegen ist.  Ich erinnerte mich auch daran, dass der Quellcode f√ºr Himbeeren im Stereomodus ebenfalls offen ist (die Entwicklung kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier im Himbeerforum verfolgt werden</a> ) und die Aufgabe, einen vollwertigen Stereotreiber zu schreiben, l√∂sbar, aber umfangreich ist.  Als ich mir die Beschreibung der Treiber anderer Kameras ansah, wurde mir klar, dass es notwendig war, nicht nur das linke und das rechte Bild zu ver√∂ffentlichen, sondern auch die Parameter beider Kameras anzugeben, Kalibrierungsergebnisse auf jede Kamera anzuwenden und viele andere Dinge zu tun.  Dies zog Experimente ein oder zwei Monate lang an.  Aus diesem Grund habe ich mich entschlossen, den Ansatz zu parallelisieren: Schreiben Sie dem Autor eine Frage zur Stereo-Unterst√ºtzung und suchen Sie selbst nach einer einfacheren, aber funktionierenden L√∂sung. <br><br><h3>  5. Dialoge mit dem Autor </h3><br>  Im Thread √ºber den Stereomodus auf dem Github stellte ich dem Autor eine Frage, in der ich erw√§hnte, dass die Stereoanlage seit 2014 von Himbeeren unterst√ºtzt wird, und schlug vor, ihm bei Bedarf ein Debugboard f√ºr Experimente zu senden.  Ich m√∂chte Sie daran erinnern, dass ich immer noch daran gezweifelt habe, dass die Stereoanlage in dieser Distribution wie in der einheimischen Raspbian funktioniert. <br><br>  Rohan antwortete √ºberraschend schnell und sagte, dass ihre Distribution einen Himbeerkern verwendet und alles funktionieren sollte.  Und bat darum, es auf einer ihrer Baugruppen zu √ºberpr√ºfen. <br><br>  Himbeerkern!  Hurra!  Theoretisch sollte ein Stereobild aufgenommen werden, ohne mit einem Tamburin zu tanzen! <br><br>  Ich habe das neueste Image <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√ºber einen Link von Rohan</a> heruntergeladen und bereitgestellt und ein einfaches Python-Skript ausgef√ºhrt, um ein Stereopaar zu erfassen.  Es hat funktioniert! <br><br><img src="https://habrastorage.org/webt/vh/6i/fg/vh6ifg37hbuzr0khcyxnuxpq5fa.jpeg" alt="Bild"><br><br>  Danach schrieb Rohan, dass er sich den Treibercode f√ºr den Stereomodus ansehen w√ºrde, und schrieb ein paar Fragen.  Zum Beispiel erzeugt unser Stereomodus ein geklebtes Bild, und wir m√ºssten es in zwei Teile schneiden - links und rechts.  Und die zweite Frage zu den Kalibrierungsparametern jeder Kamera ist, wie man damit umgeht. <br><br>  Ich sagte, dass Sie als ersten Schritt unabh√§ngig voneinander Bilder von Kameras aufnehmen k√∂nnen.  Ja, sie werden in Bezug auf Aufnahmezeit und Einstellungen (z. B. Helligkeitskontrast-Wei√üabgleich) nicht synchronisiert. In einem ersten Schritt kann dies jedoch zu Problemen f√ºhren. <br><br>  Rohan hat umgehend <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Patch ver√∂ffentlicht</a> , mit dem Sie direkt von ROS aus festlegen k√∂nnen, von welcher Kamera Bilder aufgenommen werden sollen.  Ich habe es √ºberpr√ºft - die Auswahl einer Kamera funktioniert, es ist bereits ein hervorragendes Ergebnis. <br><br><h3>  6. Unerwartete Hilfe </h3><br>  Und dann erscheint ein Kommentar des Wezzoid-Benutzers im Thread.  Er sagte, dass er ein Projekt basierend auf einer Stereokamera auf einem Pi Compute 3 mit Himbeer-Devboards mache.  Sein vierbeiniger Laufroboter verfolgte die Position eines Objekts im Weltraum, √§nderte die Position der Kameras und hielt einen festgelegten Abstand dazu (das Projekt ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> auf hackaday.io ver√∂ffentlicht). <br><br><img src="https://habrastorage.org/webt/og/sp/iy/ogspiywmjvs67yhodxpbes0pd-8.jpeg" alt="Bild"><br><br>  Und er teilte den Code, in dem er das Bild aufgenommen hatte, schnitt es mit Python in zwei H√§lften und teilte es wie Knoten der linken und rechten Kamera. <br>  Python ist in diesen Angelegenheiten kein sehr schneller Freund, daher verwendete er eine niedrige Aufl√∂sung von 320x240 und einen guten Life-Hack.  Wenn wir ein Side-by-Sibe-Stereobild aufnehmen (eine Kamera links vom Stereobild, die zweite rechts), sollte die Python jede der 240 Zeilen halbieren.  Wenn Sie jedoch ein Bild von oben nach unten erstellen (die linke Kamera ist die obere H√§lfte des Rahmens, die rechte die untere), schneidet die Python das Array in einem Arbeitsgang in zwei H√§lften.  Was vom Benutzer Wezzoid erfolgreich durchgef√ºhrt wurde. <br>  Au√üerdem hat er seinen Python-Code auf Pastebin gepostet, der diese Operation ausgef√ºhrt hat.  Da ist er: <br><br><div class="spoiler">  <b class="spoiler_title">Wezzoid-Code zum Ver√∂ffentlichen von Knoten zweier Kameras aus einem Stereopaar</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python # picamera stereo ROS node using dual CSI Pi CS3 board # Wes Freeman 2018 # modified from code by Adrian Rosebrock, pyimagesearch.com # and jensenb, https://gist.github.com/jensenb/7303362 from picamera.array import PiRGBArray from picamera import PiCamera import time import rospy from sensor_msgs.msg import CameraInfo, Image import yaml import io import signal # for ctrl-C handling import sys def parse_calibration_yaml(calib_file): with file(calib_file, 'r') as f: params = yaml.load(f) cam_info = CameraInfo() cam_info.height = params['image_height'] cam_info.width = params['image_width'] cam_info.distortion_model = params['distortion_model'] cam_info.K = params['camera_matrix']['data'] cam_info.D = params['distortion_coefficients']['data'] cam_info.R = params['rectification_matrix']['data'] cam_info.P = params['projection_matrix']['data'] return cam_info # cam resolution res_x = 320 #320 # per camera res_y = 240 #240 target_FPS = 15 # initialize the camera print "Init camera..." camera = PiCamera(stereo_mode = 'top-bottom',stereo_decimate=False) camera.resolution = (res_x, res_y*2) # top-bottom stereo camera.framerate = target_FPS # using several camera options can cause instability, hangs after a while camera.exposure_mode = 'antishake' #camera.video_stabilization = True # fussy about res? stream = io.BytesIO() # ---------------------------------------------------------- #setup the publishers print "init publishers" # queue_size should be roughly equal to FPS or that causes lag? left_img_pub = rospy.Publisher('left/image_raw', Image, queue_size=1) right_img_pub = rospy.Publisher('right/image_raw', Image, queue_size=1) left_cam_pub = rospy.Publisher('left/camera_info', CameraInfo, queue_size=1) right_cam_pub = rospy.Publisher('right/camera_info', CameraInfo, queue_size=1) rospy.init_node('stereo_pub') # init messages left_img_msg = Image() left_img_msg.height = res_y left_img_msg.width = res_x left_img_msg.step = res_x*3 # bytes per row: pixels * channels * bytes per channel (1 normally) left_img_msg.encoding = 'rgb8' left_img_msg.header.frame_id = 'stereo_camera' # TF frame right_img_msg = Image() right_img_msg.height = res_y right_img_msg.width = res_x right_img_msg.step = res_x*3 right_img_msg.encoding = 'rgb8' right_img_msg.header.frame_id = 'stereo_camera' imageBytes = res_x*res_y*3 # parse the left and right camera calibration yaml files left_cam_info = parse_calibration_yaml('/home/pi/catkin_ws/src/mmstereocam/camera_info/left.yaml') right_cam_info = parse_calibration_yaml('/home/pi/catkin_ws/src/mmstereocam/camera_info/right.yaml') # --------------------------------------------------------------- # this is supposed to shut down gracefully on CTRL-C but doesn't quite work: def signal_handler(signal, frame): print 'CTRL-C caught' print 'closing camera' camera.close() time.sleep(1) print 'camera closed' sys.exit(0) signal.signal(signal.SIGINT, signal_handler) #----------------------------------------------------------- print "Setup done, entering main loop" framecount=0 frametimer=time.time() toggle = True # capture frames from the camera for frame in camera.capture_continuous(stream, format="rgb", use_video_port=True): framecount +=1 stamp = rospy.Time.now() left_img_msg.header.stamp = stamp right_img_msg.header.stamp = stamp left_cam_info.header.stamp = stamp right_cam_info.header.stamp = stamp left_cam_pub.publish(left_cam_info) right_cam_pub.publish(right_cam_info) frameBytes = stream.getvalue() left_img_msg.data = frameBytes[:imageBytes] right_img_msg.data = frameBytes[imageBytes:] #publish the image pair left_img_pub.publish(left_img_msg) right_img_pub.publish(right_img_msg) # console info if time.time() &gt; frametimer +1.0: if toggle: indicator = ' o' # just so it's obviously alive if values aren't changing else: indicator = ' -' toggle = not toggle print 'approx publish rate:', framecount, 'target FPS:', target_FPS, indicator frametimer=time.time() framecount=0 # clear the stream ready for next frame stream.truncate(0) stream.seek(0)</span></span></code> </pre> <br></div></div><br><h3>  7. Ver√∂ffentlichen Sie die Knoten der linken und rechten Kamera </h3><br>  Beim ersten Start verfluchte der Code, dass es keinen Zugriff auf YML-Dateien mit Kameraparametern gab.  Ich habe himbeerfarbene V2-Kameras verwendet und mich daran erinnert, dass fertige Dateien mit Kalibrierungsergebnissen f√ºr verschiedene Kameramodelle zum <i><b>raspicam_node</b></i> auf dem Github kamen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/UbiquityRobotics/raspicam_node/tree/kinetic/camera_info</a> <br>  Ich nahm eine davon, machte zwei Kopien und speicherte sie mit den Namen left.yml und right.yml, wobei ich die Kameraaufl√∂sung aus dem Skript des Autors schrieb.  Folgendes ist f√ºr die linke Kamera passiert: <br><br><div class="spoiler">  <b class="spoiler_title">left.yml</b> <div class="spoiler_text"><pre> <code class="xml hljs">image_width: 320 image_height: 240 camera_name: left camera_matrix: rows: 3 cols: 3 data: [1276.704618338571, 0, 634.8876509199106, 0, 1274.342831275509, 379.8318028940378, 0, 0, 1] distortion_model: plumb_bob distortion_coefficients: rows: 1 cols: 5 data: [0.1465167016954302, -0.2847343180128725, 0.00134017721235817, -0.004309553450829512, 0] rectification_matrix: rows: 3 cols: 3 data: [1, 0, 0, 0, 1, 0, 0, 0, 1] projection_matrix: rows: 3 cols: 4 data: [1300.127197265625, 0, 630.215390285608, 0, 0, 1300.670166015625, 380.1702884455881, 0, 0, 0, 1, 0]</code> </pre> <br></div></div><br>  Rechts wird der Kameraname durch rechts ersetzt und die Datei selbst hei√üt right.yml.  Der Rest der Datei ist identisch. <br><br>  Da ich nicht vorhatte, ein komplexes Projekt durchzuf√ºhren, habe ich die langen Pfade des Autors nicht mit Unterordnern wiederholt und die Dateien einfach im Stammverzeichnis des Basisordners neben dem Python-Skript abgelegt.  Der Code wurde erfolgreich gestartet und zeigt Statusmeldungen in der Konsole an. <br><br><img src="https://habrastorage.org/webt/sz/oi/my/szoimymcugjmfggyfdez98l3kku.jpeg" alt="Bild"><br><br>  Es blieb nur zu sehen, was schlie√ülich von unserer linken und rechten Kamera ver√∂ffentlicht wurde.  Dazu habe ich rqt_image_view gestartet.  Die Elemente / left / image_raw und / right / image_raw wurden im Dropdown-Men√º angezeigt. Als ich sie ausw√§hlte, sah ich Bilder von der linken und rechten Kamera. <br><br><img src="https://habrastorage.org/webt/og/1i/du/og1iduqsqdfjq_j2ijp-fkzfxhm.jpeg" alt="Bild"><br><br>  Nun, das Ding hat verdient!  Nun der lustige Teil. <br><br><h3>  8. Wir betrachten die Karte der Tiefen. </h3><br>  Um die Tiefenkarte anzuzeigen, habe ich mir keinen eigenen Ansatz ausgedacht und das klassische <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ROS-Handbuch zum Einstellen der Stereoparameter durchgesehen</a> . <br>  Von dort fand ich heraus, dass es sch√∂n w√§re, beide Knoten in einem bestimmten Namespace zu ver√∂ffentlichen und nicht wie Wezzoid im Stammverzeichnis.  Infolgedessen sind die alten Ver√∂ffentlichungszeilen des Formulars <br><br><pre> <code class="python hljs">left_img_pub = rospy.Publisher(<span class="hljs-string"><span class="hljs-string">'left/image_raw'</span></span>, Image, queue_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  begann so auszusehen: <br><br><pre> <code class="python hljs">left_img_pub = rospy.Publisher(<span class="hljs-string"><span class="hljs-string">'stereo/right/image_raw'</span></span>, Image, queue_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Danach starten wir den Stereomodus-Verarbeitungsknoten stereo_image_proc: <br><br><pre> <code class="bash hljs">ROS_NAMESPACE=stereo rosrun stereo_image_proc stereo_ige_proc</code> </pre> <br>  Nun, wir wollen uns auch das Ergebnis ansehen, also starten wir den Beobachter: <br><br><pre> <code class="bash hljs">rosrun image_view stereo_view stereo:=/stereo image:=image_rect_color</code> </pre> <br>  F√ºhren Sie das Konfigurationsdienstprogramm aus, um die Parameter der Tiefenkarte zu konfigurieren: <br><br><pre> <code class="bash hljs">rosrun rqt_reconfigure rqt_reconfigure</code> </pre> <br>  Infolgedessen sehen wir das Bild ganz am Anfang des Artikels.  Hier ist etwas gr√∂√üer: <br><br><img src="https://habrastorage.org/webt/qc/oy/s8/qcoys8o4-yrwfxgc7kynjrxhd9m.jpeg" alt="Bild"><br><br>  Alle Dateien, die ich auf dem Github gepostet habe: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/realizator/StereoPi-ROS-depth-map-test</a> <br><br><h3>  9. Sofortige Pl√§ne </h3><br>  Nach meiner Ver√∂ffentlichung des Ergebnisses in einer Diskussion √ºber den Github schrieb Rohan: ‚ÄûCool!  Ich muss mein StereoPi abholen. ‚Äú  Wir haben ihm per Post geschrieben, ich habe ihm eine Geb√ºhr geschickt.  Ich hoffe, dass es ihm mit der funktionierenden Hardware in seinen H√§nden leichter f√§llt, einen vollwertigen Stereotreiber f√ºr ROS und Raspberry fertigzustellen und zu debuggen. <br><br><h3>  10. Zusammenfassung </h3><br>  Eine Tiefenkarte aus einem Stereobild auf Himbeeren in ROS kann auf verschiedene Arten erhalten werden.  Der f√ºr die schnelle √úberpr√ºfung gew√§hlte Pfad ist hinsichtlich der Leistung nicht der optimalste, kann jedoch f√ºr Anwendungszwecke verwendet werden.  Die Sch√∂nheit seiner Einfachheit und die F√§higkeit, sofort mit Experimenten zu beginnen. <br><br>  Nun, aus dem Witzigen: Nachdem ich die Ergebnisse erhalten hatte, bemerkte ich, dass Wezzoid, der seine L√∂sung vorschlug, der Autor der Frage nach der Ver√∂ffentlichung von zwei Stereobildern war.  Er fragte sich, entschied er. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de431092/">https://habr.com/ru/post/de431092/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de431082/index.html">Kotlin: auf der Suche nach Marketingleiter</a></li>
<li><a href="../de431084/index.html">In jeder unverst√§ndlichen Situation - schreiben Sie Skripte</a></li>
<li><a href="../de431086/index.html">Alles, was Sie √ºber PVS-Studio wissen wollten und ohne zu z√∂gern zu fragen</a></li>
<li><a href="../de431088/index.html">Dateiverwaltung falsch gemacht - Teil 1: Urspr√ºnglich aus den 90ern</a></li>
<li><a href="../de431090/index.html">Ein VK-Bot, ein C # und eine Orange</a></li>
<li><a href="../de431094/index.html">Solitaire Sort</a></li>
<li><a href="../de431096/index.html">So erstellen Sie ein Chat-Bot-Produkt</a></li>
<li><a href="../de431098/index.html">Selbst ein Feuer ist kein Hindernis oder eine Wiederherstellung der Zimbra-Geschwindigkeit nach einer Katastrophe</a></li>
<li><a href="../de431102/index.html">Wie die physikalische Adresse in DRAM-Strings und -Banken angezeigt wird</a></li>
<li><a href="../de431104/index.html">Wie wir bei Neoflex DevOps-Know-how entwickeln</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>