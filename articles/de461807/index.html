<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌇 🤘🏽 💼 Wie Pod-Prioritäten bei Kubernetes zu Ausfallzeiten bei Grafana Labs führten 🔨 👱 👩🏼‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hinweis perev. : Wir präsentieren Ihnen technische Details zu den Gründen für den jüngsten Ausfall des Cloud-Dienstes, der von den Entwicklern von Gra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie Pod-Prioritäten bei Kubernetes zu Ausfallzeiten bei Grafana Labs führten</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/461807/">  <i><b>Hinweis</b></i>  <i><b>perev.</b></i>  <i>: Wir präsentieren Ihnen technische Details zu den Gründen für den jüngsten Ausfall des Cloud-Dienstes, der von den Entwicklern von Grafana bereitgestellt wird.</i>  <i>Dies ist ein klassisches Beispiel dafür, wie eine neue und scheinbar äußerst nützliche Funktion zur Verbesserung der Qualität der Infrastruktur viel Schaden anrichten kann, wenn man die zahlreichen Nuancen ihrer Anwendung in der Realität der Produktion nicht voraussieht.</i>  <i>Es ist wunderbar, wenn solche Materialien erscheinen, mit denen Sie nicht nur aus Ihren Fehlern lernen können.</i>  <i>Details finden Sie in der Übersetzung dieses Textes vom Vice President of Product von Grafana Labs.</i> <br><br><img src="https://habrastorage.org/webt/yb/jj/1h/ybjj1hh4m7ro1eym14eiercw7po.jpeg"><br><br>  Am Freitag, dem 19. Juli, funktionierte der Hosted Prometheus-Dienst in Grafana Cloud für etwa 30 Minuten nicht mehr.  Ich entschuldige mich bei allen Kunden, die unter dem Ausfall gelitten haben.  Unsere Aufgabe ist es, die notwendigen Werkzeuge für die Überwachung bereitzustellen, und wir verstehen, dass ihre Unzugänglichkeit Ihr Leben kompliziert.  Wir nehmen diesen Vorfall sehr ernst.  Dieser Hinweis erklärt, was passiert ist, wie wir darauf reagiert haben und was wir tun, damit dies nicht noch einmal passiert. <a name="habracut"></a><br><br><h2>  Hintergrund </h2><br>  Der von Grafana Cloud gehostete Prometheus-Dienst basiert auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cortex</a> , einem CNCF-Projekt zur Erstellung eines horizontal skalierbaren, leicht zugänglichen Prometheus-Dienstes mit mehreren Mandanten.  Die Cortex-Architektur besteht aus einer Reihe separater Mikrodienste, von denen jeder seine Funktion erfüllt: Replikation, Speicherung, Anforderungen usw.  Cortex wird aktiv weiterentwickelt, bietet ständig neue Möglichkeiten und verbessert die Produktivität.  Wir stellen regelmäßig neue Cortex-Releases in Clustern bereit, damit Kunden diese Möglichkeiten nutzen können. Glücklicherweise kann Cortex ohne Ausfallzeiten aktualisiert werden. <br><br>  Für reibungslose Aktualisierungen benötigt der Ingester Cortex-Dienst während des Aktualisierungsprozesses ein zusätzliches Ingester-Replikat.  <i>( <b>Hinweis</b> : <a href="">Ingester</a> ist die Kernkomponente von Cortex. Seine Aufgabe besteht darin, einen konstanten Strom von Proben zu sammeln, diese in Prometheus-</i> Blöcken zu gruppieren <i>und in einer Datenbank wie DynamoDB, BigTable oder Cassandra zu speichern.)</i> Dies ermöglicht älteren Ingestern. Leiten Sie aktuelle Daten an neue Ingester weiter.  Es ist erwähnenswert, dass Ingesters Ressourcen fordern.  Für ihre Arbeit ist es notwendig, 4 Kerne und 15 GB Speicher pro Pod zu haben, d.h.  25% der Prozessorleistung und des Arbeitsspeichers der Basismaschine bei unseren Kubernetes-Clustern.  Im Allgemeinen verfügen wir in einem Cluster normalerweise über viel mehr nicht verwendete Ressourcen als 4 Kerne und 15 GB Arbeitsspeicher, sodass wir diese zusätzlichen Ingester bei Updates problemlos ausführen können. <br><br>  Es kommt jedoch häufig vor, dass während des normalen Betriebs keine dieser Maschinen über diese 25% der nicht beanspruchten Ressourcen verfügt.  Ja, wir streben nicht danach: CPU und Speicher sind für andere Prozesse immer nützlich.  Um dieses Problem zu lösen, haben wir uns für die Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Pod Priorities entschieden</a> .  Die Idee ist, Ingestern eine höhere Priorität als anderen (zustandslosen) Mikrodiensten einzuräumen.  Wenn wir einen zusätzlichen (N + 1) Ingester ausführen müssen, zwingen wir vorübergehend andere, kleinere Pods heraus.  Diese Pods werden auf freie Ressourcen auf anderen Computern übertragen, sodass ein ausreichend großes „Loch“ zum Starten eines zusätzlichen Ingesters verbleibt. <br><br>  Am Donnerstag, dem 18. Juli, haben wir vier neue Prioritätsstufen in unseren Clustern eingeführt: <b>kritisch</b> , <b>hoch</b> , <b>mittel</b> und <b>niedrig</b> .  Sie wurden etwa eine Woche lang in einem internen Cluster ohne Client-Verkehr getestet.  Standardmäßig erhielten Pods ohne bestimmte Priorität eine <b>mittlere</b> Priorität. Für Ingester wurde eine Klasse mit <b>hoher</b> Priorität festgelegt.  <b>Kritisch</b> war der Überwachung vorbehalten (Prometheus, Alertmanager, Node-Exporter, Kube-State-Metrics usw.).  Unsere Konfiguration ist geöffnet und siehe PR <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br><h2>  Unfall </h2><br>  Am Freitag, dem 19. Juli, startete einer der Ingenieure einen neuen dedizierten Cortex-Cluster für einen großen Kunden.  Die Konfiguration für diesen Cluster enthielt nicht die neuen Pod-Prioritäten, daher wurde allen neuen Pods die Standardpriorität " <b>Medium"</b> zugewiesen. <br><br>  Der Kubernetes-Cluster verfügte nicht über genügend Ressourcen für den neuen Cortex-Cluster, und der vorhandene Cortex-Produktionscluster wurde nicht aktualisiert (Ingester hatten keine <b>hohe</b> Priorität).  Da die Ingester des neuen Clusters standardmäßig die <b>mittlere</b> Priorität hatten und die vorhandenen Pods in der Produktion überhaupt ohne Priorität arbeiteten, haben die Ingester des neuen Clusters die Ingester aus dem vorhandenen Cortex-Produktionscluster vertrieben. <br><br>  ReplicaSet für extrudierten Ingester im Produktionscluster hat einen extrudierten Pod erkannt und einen neuen erstellt, um eine bestimmte Anzahl von Kopien beizubehalten.  Der neue Pod wurde standardmäßig auf <b>mittlere</b> Priorität eingestellt, und der nächste "alte" Ingester in der Produktion verlor Ressourcen.  Das Ergebnis war <b>ein Lawinen-ähnlicher Prozess</b> , der dazu führte, dass alle Pods von Ingester für Cortex-Produktionscluster verdrängt wurden. <br><br>  Ingester behalten den Status bei und speichern Daten für die letzten 12 Stunden.  Dies ermöglicht es uns, sie effizienter zu komprimieren, bevor wir in den Langzeitspeicher schreiben.  Zu diesem Zweck sendet Cortex Seriendaten mithilfe einer verteilten Hash-Tabelle (Distributed Hash Table, DHT) und repliziert jede Serie mithilfe der Quorum-Konsistenz im Dynamo-Stil auf drei Ingester.  Cortex schreibt keine Daten in Ingesters, die deaktiviert sind.  Wenn also eine große Anzahl von Ingestern DHT verlässt, kann Cortex keine ausreichende Replikation der Datensätze bereitstellen, und sie "fallen". <br><br><h2>  Erkennung und Beseitigung </h2><br>  Neue Prometheus-Benachrichtigungen basierend auf dem " <i>Fehlerbudget-basierten</i> " (Details erscheinen in einem zukünftigen Artikel) ertönten 4 Minuten nach Beginn des Herunterfahrens.  In den nächsten fünf Minuten haben wir eine Diagnose durchgeführt und den zugrunde liegenden Kubernetes-Cluster erweitert, um sowohl neue als auch vorhandene Produktionscluster aufzunehmen. <br><br>  Fünf Minuten später zeichneten die alten Ingester ihre Daten erfolgreich auf, und die neuen wurden gestartet, und die Cortex-Cluster wurden wieder verfügbar. <br><br>  Es dauerte weitere 10 Minuten, um OOM-Fehler (Out-of-Memory) von Reverse-Authentifizierungs-Proxys vor Cortex zu diagnostizieren und zu beheben.  OOM-Fehler wurden durch eine Verzehnfachung des QPS verursacht (wie wir glauben, aufgrund übermäßig aggressiver Anforderungen von Prometheus-Client-Servern). <br><br><h2>  Die Folgen </h2><br>  Die gesamte Ausfallzeit betrug 26 Minuten.  Es gingen keine Daten verloren.  Ingester haben alle In-Memory-Daten erfolgreich in den Langzeitspeicher hochgeladen.  Während eines Herunterfahrens haben die Prometheus-Client-Server die <i>Remote-</i> Einträge mithilfe der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuen</a> WAL-basierten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">remote_write-API</a> (erstellt von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Callum Styan</a> von Grafana Labs) im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Puffer</a> gespeichert und nach dem Fehler wiederholte fehlgeschlagene Einträge wiederholt. <br><br><img src="https://habrastorage.org/webt/ub/rv/3p/ubrv3po8fpxvn0r5ifuvwbcdogy.png"><br>  <i>Schreibvorgänge für Produktionscluster</i> <br><br><h2>  Schlussfolgerungen </h2><br>  Es ist wichtig, aus diesem Vorfall zu lernen und die erforderlichen Schritte zu unternehmen, um eine Wiederholung zu vermeiden. <br><br>  Rückblickend müssen wir zugeben, dass wir die Standardpriorität nicht auf <b>mittel setzen sollten,</b> bis alle Ingester in der Produktion eine <b>hohe</b> Priorität erhalten haben.  Außerdem hätten sie sich im Voraus um ihre <b>hohe</b> Priorität kümmern müssen.  Jetzt ist alles repariert.  Wir hoffen, dass unsere Erfahrung anderen Organisationen helfen wird, die Verwendung von Pod-Prioritäten in Kubernetes in Betracht zu ziehen. <br><br>  Wir werden eine zusätzliche Kontrollebene über die Bereitstellung zusätzlicher Objekte hinzufügen, deren Konfigurationen für den Cluster global sind.  Von nun an werden solche Änderungen von mehr Personen bewertet.  Darüber hinaus wurde die Änderung, die zum Fehler führte, für ein separates Projektdokument als zu unbedeutend angesehen - sie wurde nur in der GitHub-Ausgabe behandelt.  Von nun an werden alle derartigen Konfigurationsänderungen von einer entsprechenden Projektdokumentation begleitet. <br><br>  Schließlich automatisieren wir die Größenänderung des Proxys für die umgekehrte Authentifizierung, um OOM während einer Überlastung zu verhindern, die wir beobachtet haben, und analysieren die Standardeinstellungen von Prometheus in Bezug auf Rollback und Skalierung, um ähnliche Probleme in Zukunft zu vermeiden. <br><br>  Der erlebte Fehler hatte auch einige positive Konsequenzen: Nach Erhalt der erforderlichen Ressourcen erholte sich Cortex automatisch ohne zusätzliche Intervention.  Wir haben auch wertvolle Erfahrungen mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grafana Loki gesammelt</a> , unserem neuen Protokollaggregationssystem, mit dessen Hilfe sichergestellt wurde, dass sich alle Ingester während und nach dem Absturz ordnungsgemäß verhalten haben. <br><br><h2>  PS vom Übersetzer </h2><br>  Lesen Sie auch in unserem Blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Automatische Skalierung und Ressourcenverwaltung in Kubernetes (Überprüfung und Videobericht)</a> "; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Adventure Dailymotion: Aufbau einer Infrastruktur in den Wolken + vor Ort</a> “; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zunder-Migration zu Kubernetes</a> "; </li><li>  „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Erfolgsgeschichten in der Produktion.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 10: Reddit</a> . " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461807/">https://habr.com/ru/post/de461807/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461793/index.html">Ivan Ponomarev über die Kafka Streams API bei jug.msk.ru</a></li>
<li><a href="../de461797/index.html">Geschichten des Dienstes. Ein leichtfertiger Beitrag über ernsthafte Arbeit</a></li>
<li><a href="../de461801/index.html">DisplayPort-LVDS</a></li>
<li><a href="../de461803/index.html">Datenversionskontrolle (DVC): Datenversionierung und Experimentierreproduzierbarkeit</a></li>
<li><a href="../de461805/index.html">Monte-Carlo-Integrationsanwendung beim Rendern</a></li>
<li><a href="../de461813/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 470 (17.07.2019 - 22.07.2019)</a></li>
<li><a href="../de461815/index.html">Eine Revolution im Design von Computer-Netzteilen vor einem halben Jahrhundert</a></li>
<li><a href="../de461817/index.html">CMake und C ++ - Brüder für immer</a></li>
<li><a href="../de461819/index.html">Warum einfaches Website-Design wissenschaftlich besser ist</a></li>
<li><a href="../de461821/index.html">Neue Immuntherapie entfernte alle Tumoren bei einer Frau mit metastasiertem Brustkrebs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>