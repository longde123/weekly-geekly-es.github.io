<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüç≥ üë®üèæ‚Äçü§ù‚Äçüë®üèª üéª AERODISK Motor: Katastrophal. Teil 1 üéÖ üí® üë®üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Leser von Habr! Das Thema dieses Artikels ist die Implementierung von Katastrophenschutz in AERODISK Engine-Speichersystemen. Anfangs wollten wi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AERODISK Motor: Katastrophal. Teil 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/456348/"><p><img src="https://habrastorage.org/webt/2b/54/ub/2b54ub9jta3knw6ff5eseo1buyu.jpeg"></p><br><p> Hallo Leser von Habr!  Das Thema dieses Artikels ist die Implementierung von Katastrophenschutz in AERODISK Engine-Speichersystemen.  Anfangs wollten wir in einem Artikel √ºber beide Mittel schreiben: Replikation und Metro-Cluster, aber leider stellte sich heraus, dass der Artikel zu gro√ü war, und so teilten wir den Artikel in zwei Teile.  Gehen wir von einfach zu komplex.  In diesem Artikel konfigurieren und testen wir die synchrone Replikation - l√∂schen Sie ein Rechenzentrum und unterbrechen Sie den Kommunikationskanal zwischen den Rechenzentren, um zu sehen, was passiert. </p><a name="habracut"></a><br><p>  Unsere Kunden stellen uns h√§ufig unterschiedliche Fragen zur Replikation. Bevor Sie mit dem Einrichten und Testen der Replikationsimplementierung fortfahren, werden wir Ihnen ein wenig √ºber die Replikation in Speichersystemen erz√§hlen. </p><br><h2 id="nemnogo-teorii">  Ein bisschen Theorie </h2><br><p>  Die Replikation in den Speicher ist ein fortlaufender Prozess zur Sicherstellung der Datenidentit√§t auf mehreren Speichersystemen gleichzeitig.  Technisch wird die Replikation mit zwei Methoden durchgef√ºhrt. </p><br><p>  <strong>Die synchrone Replikation</strong> ist das Kopieren von Daten vom Hauptspeichersystem in das Sicherungssystem, gefolgt von der obligatorischen Best√§tigung beider Speichersysteme, dass die Daten aufgezeichnet und best√§tigt wurden.  Nach Best√§tigung von beiden Seiten (auf beiden Speichersystemen) gelten die Daten als aufgezeichnet, und Sie k√∂nnen mit ihnen arbeiten.  Dies stellt eine garantierte Datenidentit√§t auf allen am Replikat beteiligten Speichersystemen sicher. </p><br><p>  Die Vorteile dieser Methode: </p><br><ul><li>  Die Daten sind auf allen Speichersystemen immer identisch. </li></ul><br><p>  Nachteile: </p><br><ul><li>  Hohe Kosten der L√∂sung (schnelle Kommunikationskan√§le, teure Glasfaser, langwellige Transceiver usw.) </li><li>  Entfernungsbeschr√§nkungen (innerhalb weniger zehn Kilometer) </li><li>  Es gibt keinen Schutz gegen logische Datenbesch√§digung (wenn die Daten (wissentlich oder versehentlich) auf dem Hauptspeichersystem besch√§digt sind, werden sie automatisch und sofort im Sicherungsspeicher besch√§digt, da die Daten immer identisch sind (dies ist ein Paradoxon). </li></ul><br><p>  <strong>Bei der asynchronen Replikation</strong> werden auch Daten vom Hauptspeicher in die Sicherung kopiert, jedoch mit einer gewissen Verz√∂gerung und ohne dass der Datensatz auf der anderen Seite best√§tigt werden muss.  Sie k√∂nnen sofort nach dem Schreiben in den Hauptspeicher mit Daten arbeiten. Im Sicherungsspeicher sind die Daten nach einer Weile verf√ºgbar.  Die Identit√§t der Daten wird in diesem Fall nat√ºrlich √ºberhaupt nicht angegeben.  Daten im Backup-Speicher sind immer etwas "in der Vergangenheit". </p><br><p>  Vorteile der asynchronen Replikation: </p><br><ul><li>  Niedrige L√∂sungskosten (alle Kommunikationskan√§le, Optik optional) </li><li>  Keine Entfernungsbegrenzung </li><li>  Daten im Sicherungsspeicher sind nicht besch√§digt, wenn sie auf dem Hauptspeicher besch√§digt sind (zumindest f√ºr einige Zeit). Wenn die Daten besch√§digt wurden, k√∂nnen Sie das Replikat jederzeit stoppen, um eine Besch√§digung der Daten im Sicherungsspeicher zu verhindern </li></ul><br><p>  Nachteile: </p><br><ul><li>  Daten in verschiedenen Rechenzentren sind immer nicht identisch </li></ul><br><p>  Daher h√§ngt die Wahl des Replikationsmodus von den Aufgaben des Unternehmens ab.  Wenn es f√ºr Sie von entscheidender Bedeutung ist, dass das Backup-Rechenzentrum genau dieselben Daten wie die Hauptdaten enth√§lt (d. H. Gesch√§ftsanforderung f√ºr RPO = 0), m√ºssen Sie die Einschr√§nkungen des synchronen Replikats in Kauf nehmen.  Und wenn die Verz√∂gerung des Status der Daten zul√§ssig ist oder einfach kein Geld vorhanden ist, m√ºssen Sie auf jeden Fall die asynchrone Methode verwenden. </p><br><p>  Wir unterscheiden ein solches Regime (genauer gesagt bereits eine Topologie) auch separat als Metro-Cluster.  Der Metro-Cluster-Modus verwendet die synchrone Replikation. Im Gegensatz zu einem regul√§ren Replikat erm√∂glicht der Metro-Cluster jedoch, dass beide Speichersysteme im aktiven Modus arbeiten.  Das hei√üt,  Sie haben keine Trennung von Active-Standby-Rechenzentren.  Anwendungen arbeiten gleichzeitig mit zwei Speichersystemen, die sich physisch in verschiedenen Rechenzentren befinden.  Unfallausfallzeiten in einer solchen Topologie sind sehr gering (RTO, normalerweise Minuten).  In diesem Artikel werden wir unsere Implementierung des U-Bahn-Clusters nicht ber√ºcksichtigen, da dies ein sehr gro√ües und umfangreiches Thema ist. Daher werden wir ihm in Fortsetzung einen separaten, folgenden Artikel widmen. </p><br><p>  Auch wenn wir sehr oft √ºber die Replikation mit Speichersystemen sprechen, haben viele eine vern√ºnftige Frage:&gt; ‚ÄûViele Anwendungen haben ihre eigenen Replikationstools. Warum sollte die Replikation auf Speichersystemen verwendet werden?  Ist es besser oder schlechter? " </p><br><p>  Es gibt keine einzige Antwort, daher hier die Vor- und Nachteile: </p><br><p>  Argumente f√ºr die Speicherreplikation: </p><br><ul><li>  Die Einfachheit der L√∂sung.  Auf eine Weise k√∂nnen Sie ein ganzes Datenarray replizieren, unabh√§ngig vom Lasttyp oder der Anwendung.  Wenn Sie ein Replikat von Anwendungen verwenden, m√ºssen Sie jede Anwendung separat konfigurieren.  Wenn es mehr als zwei davon gibt, ist dies √§u√üerst zeitaufw√§ndig und teuer (f√ºr die Anwendungsreplikation ist in der Regel eine separate und nicht kostenlose Lizenz f√ºr jede Anwendung erforderlich. Mehr dazu weiter unten). </li><li>  Sie k√∂nnen alles replizieren - alle Anwendungen, alle Daten - und sie sind immer konsistent.  Viele (die meisten) Anwendungen verf√ºgen nicht √ºber Replikationsfunktionen, und Replikate von der Speicherseite sind die einzige M√∂glichkeit, Schutz vor Katastrophen zu bieten. </li><li>  Keine √úberzahlung f√ºr Anwendungsreplikationsfunktionen erforderlich.  In der Regel kostet es viel, genau wie Lizenzen f√ºr ein Replikatspeichersystem.  Sie m√ºssen die Speicherreplikationslizenz jedoch nur einmal bezahlen und die Lizenz f√ºr das Anwendungsreplikat f√ºr jede Anwendung separat erwerben.  Wenn es viele solcher Anwendungen gibt, kostet es einen h√ºbschen Cent und die Kosten f√ºr Lizenzen f√ºr die Replikation von Speicher werden zu einem Tropfen auf den hei√üen Stein. </li></ul><br><p>  Argumente GEGEN Speicherreplikation: </p><br><ul><li>  Das Replikat, das Anwendungstools verwendet, verf√ºgt aus Sicht der Anwendungen selbst √ºber mehr Funktionen. Die Anwendung kennt ihre Daten besser (was offensichtlich ist), sodass mehr Optionen f√ºr die Arbeit mit ihnen vorhanden sind. </li><li>  Hersteller einiger Anwendungen garantieren nicht die Konsistenz ihrer Daten, wenn die Replikation mit Tools von Drittanbietern erfolgt.  * * </li></ul><br><p>  * - eine kontroverse These.  Zum Beispiel hat ein bekanntes DBMS-Fertigungsunternehmen lange Zeit offiziell erkl√§rt, dass sein DBMS normalerweise nur mit seinen Mitteln repliziert werden kann und der Rest der Replikation (einschlie√ülich SHD-shnaya) ‚Äûnicht wahr‚Äú ist.  Aber das Leben hat gezeigt, dass dies nicht so ist.  H√∂chstwahrscheinlich (aber das ist nicht korrekt) ist dies einfach nicht der ehrlichste Versuch, mehr Lizenzen an Kunden zu verkaufen. </p><br><p>  Infolgedessen ist die Replikation von der Speicherseite in den meisten F√§llen besser, weil  Dies ist eine einfachere und kosteng√ºnstigere Option. Es gibt jedoch komplexe F√§lle, in denen Sie bestimmte Anwendungsfunktionen ben√∂tigen und mit der Replikation auf Anwendungsebene arbeiten m√ºssen. </p><br><h2 id="s-teoriey-zakonchili-teper-praktika">  Wenn die Theorie fertig ist, jetzt die Praxis </h2><br><p>  Wir werden in unserem Labor eine Replik erstellen.  Im Labor haben wir zwei Rechenzentren emuliert (tats√§chlich zwei benachbarte Racks, die sich in unterschiedlichen Geb√§uden zu befinden scheinen).  Der Stand besteht aus zwei Engine N2-Speichersystemen, die durch optische Kabel miteinander verbunden sind.  Ein physischer Server unter Windows Server 2016 mit 10-Gbit-Ethernet ist mit beiden Speichersystemen verbunden.  Der Stand ist recht einfach, √§ndert aber nichts an der Essenz. </p><br><p>  Schematisch sieht es so aus: </p><br><p><img src="https://habrastorage.org/webt/wj/u4/rc/wju4rcak9ilbms68pnffyvsb6ly.png"></p><br><p>  Die logische Replikation ist wie folgt organisiert: </p><br><p><img src="https://habrastorage.org/webt/yf/yh/dy/yfyhdy19cbj3sc0gpzp8jx6liz0.jpeg"></p><br><p>  Schauen wir uns nun die Replikationsfunktionalit√§t an, die wir jetzt haben. <br>  Es werden zwei Modi unterst√ºtzt: asynchron und synchron.  Es ist logisch, dass der Synchronmodus durch die Entfernung und den Kommunikationskanal begrenzt ist.  Insbesondere erfordert der synchrone Modus die Verwendung von Glasfaser als Physik und 10-Gigabit-Ethernet (oder h√∂her). </p><br><p>  Die unterst√ºtzte Entfernung f√ºr die synchrone Replikation betr√§gt 40 Kilometer, die Verz√∂gerung des Optikkanals zwischen den Rechenzentren betr√§gt bis zu 2 Millisekunden.  Im Allgemeinen funktioniert es mit gro√üen Verz√∂gerungen, aber w√§hrend der Aufzeichnung treten starke Bremsen auf (was ebenfalls logisch ist). Wenn Sie also eine synchrone Replikation zwischen Rechenzentren in Betracht ziehen, sollten Sie die Qualit√§t der Optik und Verz√∂gerungen √ºberpr√ºfen. </p><br><p>  Asynchrone Replikationsanforderungen sind nicht so ernst.  Genauer gesagt sind sie √ºberhaupt nicht.  Jede funktionierende Ethernet-Verbindung ist geeignet. </p><br><p>  Derzeit unterst√ºtzt der AERODISK ENGINE-Speicher die Replikation f√ºr Blockger√§te (LUNs) mithilfe des Ethernet-Protokolls (Kupfer oder Optik).  F√ºr Projekte, die notwendigerweise eine Replikation √ºber die Fibre Channel SAN-Factory erfordern, schlie√üen wir jetzt die entsprechende L√∂sung ab, aber bisher ist sie noch nicht fertig, in unserem Fall nur Ethernet. </p><br><p>  Die Replikation kann zwischen allen Speichersystemen der ENGINE-Serie (N1, N2, N4) von niedrigeren Systemen zu √§lteren und umgekehrt funktionieren. </p><br><p>  Die Funktionalit√§t beider Replikationsmodi ist v√∂llig identisch.  Im Folgenden erfahren Sie mehr dar√ºber, was ist: </p><br><ul><li>  Replikation "eins zu eins" oder "eins zu eins", dh die klassische Version mit zwei Rechenzentren, dem Haupt- und dem Backup </li><li>  Die Replikation ist "eins zu viele" oder "eins zu viele", d.h.  Eine LUN kann gleichzeitig auf mehrere Speichersysteme repliziert werden </li><li>  Aktivierung, Deaktivierung und "Umkehrung" der Replikation, um die Replikationsrichtung zu aktivieren, zu deaktivieren oder zu √§ndern </li><li>  Die Replikation ist sowohl f√ºr RDG-Pools (Raid Distributed Group) als auch f√ºr DDP-Pools (Dynamic Disk Pool) verf√ºgbar.  Die RDG-Pool-LUN kann jedoch nur auf eine andere RDG repliziert werden.  C DDP ist √§hnlich. </li></ul><br><p>  Es gibt viel mehr kleine Funktionen, aber die Auflistung macht nicht viel Sinn. Wir werden sie w√§hrend des Setups erw√§hnen. </p><br><h2 id="nastroyka-replikacii">  Replikationssetup </h2><br><p>  Der Einrichtungsprozess ist recht einfach und besteht aus drei Schritten. </p><br><ol><li>  Netzwerkeinrichtung </li><li>  Speicher-Setup </li><li>  Regeln (Links) einrichten und zuordnen </li></ol><br><p>  Ein wichtiger Punkt bei der Konfiguration der Replikation ist, dass die ersten beiden Stufen auf einem Remote-Speichersystem wiederholt werden sollten, die dritte Stufe - nur auf der Hauptstufe. </p><br><h3 id="nastroyka-setevyh-resursov">  Konfiguration der Netzwerkressourcen </h3><br><p>  Der erste Schritt besteht darin, die Netzwerkports zu konfigurieren, √ºber die der Replikationsverkehr √ºbertragen wird.  Dazu m√ºssen Sie die Ports aktivieren und im Abschnitt Front-End-Adapter IP-Adressen festlegen. </p><br><p>  Danach m√ºssen wir einen Pool (in unserem Fall RDG) und eine virtuelle IP f√ºr die Replikation (VIP) erstellen.  VIP ist eine Floating-IP-Adresse, die an zwei ‚Äûphysische‚Äú Adressen von Speichercontrollern gebunden ist (die Ports, die wir gerade konfiguriert haben).  Es wird die prim√§re Replikationsschnittstelle sein.  Sie k√∂nnen auch nicht mit VIP, sondern mit VLAN arbeiten, wenn Sie mit markiertem Datenverkehr arbeiten m√ºssen. </p><br><p><img src="https://habrastorage.org/webt/di/ye/5f/diye5fvbzya3cvtebg7memcs5jo.jpeg"></p><br><p>  Das Erstellen eines VIP f√ºr ein Replikat unterscheidet sich nicht wesentlich vom Erstellen eines VIP f√ºr E / A (NFS, SMB, iSCSI).  In diesem Fall erstellen wir ein VIP (ohne VLAN), geben jedoch unbedingt an, dass es f√ºr die Replikation vorgesehen ist (ohne diesen Zeiger k√∂nnen wir der Regel im n√§chsten Schritt kein VIP hinzuf√ºgen). </p><br><p><img src="https://habrastorage.org/webt/nd/i9/2d/ndi92dbmjvxuqjidju802r-vl7s.png"></p><br><p>  VIP muss sich im selben Subnetz befinden wie die IP-Ports, zwischen denen es ‚Äûschwebt‚Äú. </p><br><p><img src="https://habrastorage.org/webt/vm/no/9m/vmno9ms_uas_guk28o1etun7kg4.png"></p><br><p>  Wir wiederholen diese Einstellungen auf dem Remote-Speichersystem mit einem anderen IP-Shnik f√ºr sich. <br>  VIPs aus verschiedenen Speichersystemen k√∂nnen sich in verschiedenen Subnetzen befinden. Hauptsache, es sollte ein Routing zwischen ihnen bestehen.  In unserem Fall wird dieses Beispiel nur gezeigt (192.168.3.XX und 192.168.2.XX). </p><br><p><img src="https://habrastorage.org/webt/w5/r6/re/w5r6rexidxqry4rnfdrvgp5gzcq.jpeg"></p><br><p>  Damit ist die Vorbereitung des Netzwerkteils abgeschlossen. </p><br><h3 id="nastraivaem-hranilischa">  Speicher konfigurieren </h3><br><p>  Das Konfigurieren des Speichers f√ºr ein Replikat unterscheidet sich vom √ºblichen nur dadurch, dass wir die Zuordnung √ºber das spezielle Men√º ‚ÄûReplikationszuordnung‚Äú vornehmen.  Ansonsten ist alles wie bei der √ºblichen Einstellung.  Jetzt in Ordnung. </p><br><p>  Im zuvor erstellten R02-Pool m√ºssen Sie eine LUN erstellen.  Erstellen, nennen Sie es LUN1. </p><br><p><img src="https://habrastorage.org/webt/tg/l8/vk/tgl8vkdsqf-4oljacfssnh2_zus.jpeg"></p><br><p>  Wir m√ºssen dieselbe LUN auch auf einem Remote-Speichersystem mit identischem Volumen erstellen.  Wir schaffen.  Um Verwirrung zu vermeiden, wird die Remote-LUN LUN1R genannt </p><br><p><img src="https://habrastorage.org/webt/xm/kl/v4/xmklv4deigknjz1vadluit9pdds.jpeg"></p><br><p>  Wenn wir eine bereits vorhandene LUN verwenden m√ºssten, m√ºsste diese produktive LUN zum Zeitpunkt der Replikateinrichtung vom Host abgemeldet werden und auf dem Remote-Speichersystem einfach eine leere LUN mit identischer Gr√∂√üe erstellen. </p><br><p>  Wenn das Speicher-Setup abgeschlossen ist, fahren wir mit der Erstellung der Replikationsregel fort. </p><br><h3 id="nastroyka-pravil-replikacii-ili-replikacionnyh-svyazey">  Konfigurieren Sie Replikationsregeln oder Replikationslinks </h3><br><p>  Nach dem Erstellen von LUNs auf dem Speicher, der momentan die prim√§re sein wird, konfigurieren wir die Replikationsregel LUN1 auf SHD1 in LUN1R auf SHD2. </p><br><p>  Die Konfiguration erfolgt im Men√º Remote Replication. </p><br><p>  Erstellen Sie eine Regel.  Geben Sie dazu den Replikatempf√§nger an.  Wir geben auch den Namen der Verbindung und den Replikationstyp (synchron oder asynchron) an. </p><br><p><img src="https://habrastorage.org/webt/xk/yu/8f/xkyu8ftgcubcwu4-ul_ux3vc7lq.jpeg"></p><br><p>  F√ºgen Sie im Feld "Remote-Systeme" unseren SHD2 hinzu.  Zum Hinzuf√ºgen m√ºssen Sie den verwaltenden IP-Speicher (MGR) und den Namen der Remote-LUN verwenden, auf die wir replizieren m√∂chten (in unserem Fall LUN1R).  Das Verwalten von IPs wird nur in der Phase des Hinzuf√ºgens von Kommunikation ben√∂tigt. Der Replikationsverkehr √ºber diese IPs wird nicht √ºbertragen. Hierzu wird der zuvor konfigurierte VIP verwendet. </p><br><p>  Bereits zu diesem Zeitpunkt k√∂nnen wir mehr als ein Remote-System f√ºr die Topologie "Eins zu Viele" hinzuf√ºgen: Klicken Sie auf die Schaltfl√§che "Knoten hinzuf√ºgen", wie in der folgenden Abbildung dargestellt. </p><br><p><img src="https://habrastorage.org/webt/rv/xb/bh/rvxbbh4umovgds3gduoaxg3tfc8.jpeg"></p><br><p>  In unserem Fall ist das Remote-System eines, daher sind wir darauf beschr√§nkt. </p><br><p>  Die Regel ist fertig.  Beachten Sie, dass es automatisch allen Replikationsteilnehmern hinzugef√ºgt wird (in unserem Fall gibt es zwei davon).  Sie k√∂nnen beliebig viele Regeln f√ºr eine beliebige Anzahl von LUNs und in eine beliebige Richtung erstellen.  Um die Last auszugleichen, k√∂nnen wir beispielsweise einen Teil der LUNs von SHD1 nach SHD2 und den anderen Teil im Gegenteil von SHD2 nach SHD1 replizieren. </p><br><p>  SHD1.  Unmittelbar nach der Erstellung begann die Synchronisation. </p><br><p><img src="https://habrastorage.org/webt/y7/v8/gg/y7v8gg7bboqpit0zrow87pgvi5y.jpeg"></p><br><p>  SHD2.  Wir sehen die gleiche Regel, aber die Synchronisation ist bereits beendet. </p><br><p><img src="https://habrastorage.org/webt/tb/dl/0k/tbdl0k_anxtcwmecg31bk0s7fmo.jpeg"></p><br><p>  LUN1 auf SHD1 ist in der Rolle von Primary, dh es ist aktiv.  LUN1R auf SHD2 spielt die Rolle des Sekund√§rs, dh es wird gehalten, wenn SHD1 ausf√§llt. <br>  Jetzt k√∂nnen wir unsere LUN mit dem Host verbinden. </p><br><p>  Wir werden die Verbindung √ºber iSCSI herstellen, obwohl dies √ºber FC erfolgen kann.  Das Einrichten der Zuordnung f√ºr iSCSI-LUN in einem Replikat unterscheidet sich praktisch nicht vom √ºblichen Szenario, daher werden wir hier nicht n√§her darauf eingehen.  Wenn √ºberhaupt, wird dieser Vorgang im Artikel zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schnellen Einrichtung</a> beschrieben. </p><br><p>  Der einzige Unterschied besteht darin, dass wir die Zuordnung im Men√º "Replikationszuordnung" erstellen. </p><br><p><img src="https://habrastorage.org/webt/xn/uy/p9/xnuyp9dccwbpg93ahvefmhifmdq.jpeg"></p><br><p>  Richten Sie die Zuordnung ein und geben Sie dem Host die LUN.  Der Gastgeber sah eine LUN. </p><br><p><img src="https://habrastorage.org/webt/qg/y3/vm/qgy3vmfark_2-pvl8liqtozb2iu.jpeg"></p><br><p>  Formatieren Sie es in das lokale Dateisystem. </p><br><p><img src="https://habrastorage.org/webt/zd/8l/qg/zd8lqglmv194u9-zsctatxrsuzk.jpeg"></p><br><p>  Das Setup ist abgeschlossen.  Als n√§chstes werden Tests gehen. </p><br><h2 id="testirovanie">  Testen </h2><br><p>  Wir werden drei Hauptszenarien testen. </p><br><ol><li>  Personalwechselrollen Sekund√§r&gt; Prim√§r.  Ein regelm√§√üiger Rollenwechsel ist erforderlich, wenn wir beispielsweise haupts√§chlich ein Rechenzentrum ben√∂tigen, einige vorbeugende Vorg√§nge ausf√ºhren m√ºssen und w√§hrend dieser Zeit, damit die Daten verf√ºgbar sind, die Last an das Backup-Rechenzentrum √ºbertragen. </li><li>  Failover von Rollen Sekund√§r&gt; Prim√§r (Ausfall des Rechenzentrums).  Dies ist das Hauptszenario f√ºr die Replikation, mit dessen Hilfe ein vollst√§ndiger Ausfall des Rechenzentrums √ºberstanden werden kann, ohne das Unternehmen f√ºr l√§ngere Zeit anzuhalten. </li><li>  Unterbrochene Kommunikationskan√§le zwischen Rechenzentren.  √úberpr√ºfen des korrekten Verhaltens von zwei Speichersystemen unter Bedingungen, unter denen aus irgendeinem Grund der Kommunikationskanal zwischen den Rechenzentren nicht verf√ºgbar ist (z. B. ein Bagger, der an der falschen Stelle gegraben wurde und die dunkle Optik defekt hat). </li></ol><br><p>  Zun√§chst beginnen wir, Daten in unsere LUN zu schreiben (wir schreiben Dateien mit zuf√§lligen Daten).  Wir sehen sofort, dass der Kommunikationskanal zwischen den Speichersystemen genutzt wird.  Dies ist leicht zu verstehen, wenn Sie die Last√ºberwachung der Ports √∂ffnen, die f√ºr die Replikation verantwortlich sind. </p><br><p><img src="https://habrastorage.org/webt/s7/99/bt/s799bttjt3v6q24uhvxoywfrwne.jpeg"></p><br><p>  Auf beiden Speichersystemen gibt es jetzt "n√ºtzliche" Daten, wir k√∂nnen den Test starten. </p><br><p><img src="https://habrastorage.org/webt/r3/vs/dv/r3vsdvpsp9avchablad41pxrfdu.jpeg"></p><br><p>  Schauen wir uns f√ºr alle F√§lle die Hash-Summen einer der Dateien an und schreiben Sie sie auf. </p><br><p><img src="https://habrastorage.org/webt/e1/zi/st/e1zistvzwlkimqbupxjtgnltc9o.jpeg"></p><br><h3 id="shtatnoe-pereklyuchenie-roley">  Personalrollenwechsel </h3><br><p>  Das Wechseln der Rollen (√Ñndern der Replikationsrichtung) kann von jedem Speichersystem aus erfolgen. Sie m√ºssen jedoch weiterhin zu beiden wechseln, da Sie die Zuordnung auf der Prim√§rseite deaktivieren und auf der Sekund√§rseite aktivieren m√ºssen (die zur Prim√§rseite wird). </p><br><p>  Vielleicht stellt sich jetzt eine vern√ºnftige Frage: Warum nicht automatisieren?  Wir antworten: Alles ist einfach, die Replikation ist ein einfaches Katastrophenschutz-Tool, das ausschlie√ülich auf manuellen Vorg√§ngen basiert.  Um diese Vorg√§nge zu automatisieren, gibt es einen Metro-Cluster-Modus, der vollst√§ndig automatisiert ist, dessen Konfiguration jedoch viel komplizierter ist.  Wir werden im n√§chsten Artikel √ºber das Einrichten des U-Bahn-Clusters schreiben. </p><br><p>  Deaktivieren Sie die Zuordnung im Hauptspeicher, um sicherzustellen, dass die Aufzeichnung gestoppt wird. </p><br><p><img src="https://habrastorage.org/webt/jk/j4/1l/jkj41ltsncz2hmqoclkrecqvguy.jpeg"></p><br><p>  W√§hlen Sie dann auf einem der Speichersysteme (egal ob prim√§r oder Backup) im Men√º "Remote-Replikation" unsere REPL1-Verbindung aus und klicken Sie auf "Rolle √§ndern". </p><br><p><img src="https://habrastorage.org/webt/dc/bb/8t/dcbb8tv24xxhofmg_avtodfelas.jpeg"></p><br><p>  Nach einigen Sekunden wird LUN1R (Backup-Speicher) prim√§r. </p><br><p><img src="https://habrastorage.org/webt/-v/hf/l2/-vhfl2g0v20bnfnomxwupf0_9xk.jpeg"></p><br><p>  Wir machen das Mapping von LUN1R mit SHD2. </p><br><p><img src="https://habrastorage.org/webt/lh/uw/cy/lhuwcyscu0quljitysu2pkk35hg.jpeg"></p><br><p>  Danach klammert sich unser E: -Laufwerk automatisch an den Host, nur dass es diesmal mit LUN1R ‚Äûgeflogen‚Äú ist. </p><br><p>  Vergleichen Sie f√ºr alle F√§lle die Hash-Mengen. </p><br><p><img src="https://habrastorage.org/webt/g6/st/qh/g6stqhn-xr0yqlw84t7y4_y5sqm.png"></p><br><p>  Identisch.  Test bestanden. </p><br><h3 id="avariynoe-pereklyuchenie-otkaz-cod-a">  Failover  Rechenzentrumsfehler </h3><br><p>  Derzeit ist der Hauptspeicher nach dem regul√§ren Umschalten SHD2 bzw. LUN1R.  Um einen Unfall zu simulieren, schalten wir beide Controller SHD2 aus. <br>  Der Zugang dazu ist nicht mehr m√∂glich. </p><br><p>  Wir schauen uns an, was auf dem Speicher 1 passiert (Backup im Moment). </p><br><p><img src="https://habrastorage.org/webt/ai/oy/jt/aioyjtl8xqmmgngtidigkvoesai.jpeg"></p><br><p>  Wir sehen, dass die prim√§re LUN (LUN1R) nicht verf√ºgbar ist.  Eine Fehlermeldung wurde in den Protokollen, im Informationsbereich sowie in der Replikationsregel selbst angezeigt.  Dementsprechend sind Daten vom Host derzeit nicht verf√ºgbar. </p><br><p>  √Ñndern Sie die Rolle von LUN1 in Primary. </p><br><p><img src="https://habrastorage.org/webt/ef/vr/wv/efvrwvemqzysnrtebprwvmqnxw8.jpeg"></p><br><p>  Angelegenheiten, die dem Gastgeber zugeordnet sind. </p><br><p><img src="https://habrastorage.org/webt/o9/es/oj/o9esojg6xcl-uv_wbbj6afkrz18.jpeg"></p><br><p>  Stellen Sie sicher, dass Laufwerk E auf dem Host angezeigt wird. </p><br><p><img src="https://habrastorage.org/webt/rg/kj/0s/rgkj0s-0rgoumtmnzk98bd-bgl4.jpeg"></p><br><p>  √úberpr√ºfen Sie den Hash. </p><br><p><img src="https://habrastorage.org/webt/hn/yb/yq/hnybyqjm7w_g1il4bowg1-xgq5y.jpeg"></p><br><p>  Alles ist in Ordnung.  Das Speicherzentrum verzeichnete einen R√ºckgang des aktiven Rechenzentrums.  Die ungef√§hre Zeit, die wir f√ºr das Verbinden der "Umkehrung" der Replikation und das Verbinden der LUN vom Backup-Rechenzentrum aufgewendet haben, betrug ungef√§hr 3 Minuten.  Es ist klar, dass im realen Produkt alles viel komplizierter ist, und zus√§tzlich zu Aktionen mit Speichersystemen m√ºssen Sie viel mehr Vorg√§nge im Netzwerk, auf Hosts und in Anwendungen ausf√ºhren.  Und im Leben wird diese Zeitspanne viel l√§nger sein. </p><br><p>  Hier m√∂chte ich schreiben, dass alles, der Test erfolgreich abgeschlossen wurde, aber lasst uns nicht eilen.  Der Hauptspeicher "liegt", wir wissen, dass sie, als sie "fiel", in der Rolle der Grundschule war.  Was passiert, wenn sie sich pl√∂tzlich einschaltet?  Es wird zwei Hauptrollen geben, was einer Datenbesch√§digung entspricht.  Wir werden es jetzt √ºberpr√ºfen. <br>  Wir werden pl√∂tzlich den zugrunde liegenden Speicher einschalten. </p><br><p>  Es wird einige Minuten lang geladen und danach nach einer kurzen Synchronisation wieder in Betrieb genommen, jedoch bereits als sekund√§r. </p><br><p><img src="https://habrastorage.org/webt/27/hu/q6/27huq6b6guby7o-g7_xkz7quugy.jpeg"></p><br><p>  Alles ok.  Geteiltes Gehirn ist nicht passiert.  Wir haben dar√ºber nachgedacht und immer nach dem Fall des Speichersystems steigt die Rolle des Sekund√§rsystems an, unabh√§ngig davon, welche Rolle es "im Leben" war.  Jetzt k√∂nnen wir mit Sicherheit sagen, dass der Ausfalltest des Rechenzentrums erfolgreich war. </p><br><h3 id="otkaz-kanalov-svyazi-mezhdu-cod-ami">  Ausfall von Kommunikationskan√§len zwischen Rechenzentren </h3><br><p>  Die Hauptaufgabe dieses Tests besteht darin, sicherzustellen, dass das Speichersystem nicht ausflippt, wenn es vor√ºbergehend die Kommunikationskan√§le zwischen den beiden Speichersystemen verliert und dann wieder angezeigt wird. <br>  Also.  Wir trennen die Dr√§hte zwischen den Speichersystemen (stellen Sie sich vor, ein Bagger hat sie gegraben). </p><br><p>  Auf der Prim√§rseite sehen wir, dass es keine Verbindung mit der Sekundarstufe gibt. </p><br><p><img src="https://habrastorage.org/webt/yh/nf/ar/yhnfarhppjrnbaxotu4ds4szz5c.jpeg"></p><br><p>  Auf Secondary sehen wir, dass es keine Verbindung zu Primary gibt. </p><br><p><img src="https://habrastorage.org/webt/f4/k9/7h/f4k97hzr11uh3cytxpsjlq2anly.jpeg"></p><br><p>  Alles funktioniert einwandfrei, und wir schreiben weiterhin Daten in das Hauptspeichersystem, dh es wird bereits garantiert, dass sie sich von dem Sicherungssystem unterscheiden, dh sie sind "√ºbrig". </p><br><p>  In wenigen Minuten reparieren wir den Kommunikationskanal.  Sobald sich die Speichersysteme gesehen haben, wird die Datensynchronisation automatisch aktiviert.  Der Administrator ben√∂tigt nichts. </p><br><p><img src="https://habrastorage.org/webt/wo/os/yy/woosyydo-vvbauzsd7lgu4qwfos.jpeg"></p><br><p>     . </p><br><p><img src="https://habrastorage.org/webt/up/ne/es/upneeslicidwf8manqfmlvcaohu.jpeg"></p><br><p>  ,        ,      . </p><br><h2 id="vyvody">  Schlussfolgerungen </h2><br><p>    ‚Äì    ,  ,   .       . </p><br><p>        ,  -    .      .   ,        . </p><br><p>                   active-active,       ,       . </p><br><p>   ,       . </p><br><p>   . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456348/">https://habr.com/ru/post/de456348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456338/index.html">13 n√ºtzliche JavaScript-Einzeiler</a></li>
<li><a href="../de456340/index.html">Eine Geschichte dar√ºber, wie ein Team von Freiberuflern Full-Stack-JavaScript-Anwendungen schreibt</a></li>
<li><a href="../de456342/index.html">Eine Sprache, um alle zu regieren</a></li>
<li><a href="../de456344/index.html">Warum gibt ['1', '7', '11']. Map (parseInt) [1, NaN, 3] in Javascript zur√ºck?</a></li>
<li><a href="../de456346/index.html">Interaktive Roadmap f√ºr Webentwicklungslerner</a></li>
<li><a href="../de456350/index.html">Digitale Veranstaltungen in Moskau vom 17. bis 23. Juni</a></li>
<li><a href="../de456352/index.html">Drahtloses Objektkommunikationsmodul WISE-4000</a></li>
<li><a href="../de456354/index.html">Wie sammeln wir TV-Boxen?</a></li>
<li><a href="../de456358/index.html">Die 13 ber√ºchtigtsten Artikel des vergangenen Jahres</a></li>
<li><a href="../de456362/index.html">Level 6 Designer: Wie wir Designer motivieren und entwickeln</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>