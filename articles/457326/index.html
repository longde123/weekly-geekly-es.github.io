<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüéì üçì üî™ Cl√∫ster de conmutaci√≥n por error PostgreSQL + Patroni. Experiencia de implementacion ‚ô®Ô∏è üëÆ üë®üèΩ‚Äçüöí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el art√≠culo, le dir√© c√≥mo abordamos el problema de tolerancia a fallas de PostgreSQL, por qu√© esto se ha vuelto importante para nosotros y qu√© suce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cl√∫ster de conmutaci√≥n por error PostgreSQL + Patroni. Experiencia de implementacion</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/457326/">  En el art√≠culo, le dir√© c√≥mo abordamos el problema de tolerancia a fallas de PostgreSQL, por qu√© esto se ha vuelto importante para nosotros y qu√© sucedi√≥ al final. <br><br>  Tenemos un servicio altamente cargado: 2.5 millones de usuarios en todo el mundo, 50K + usuarios activos todos los d√≠as.  Los servidores est√°n ubicados en Amazone en una regi√≥n de Irlanda: hay constantemente m√°s de 100 servidores diferentes en funcionamiento, casi 50 de ellos con bases de datos. <br><br>  Todo el backend es una gran aplicaci√≥n Java monol√≠tica con estado que mantiene una conexi√≥n websocket constante con el cliente.  Con el trabajo simult√°neo de varios usuarios en una placa, todos ven los cambios en tiempo real, porque registramos cada cambio en la base de datos.  Tenemos aproximadamente 10K consultas por segundo a nuestras bases de datos.  En la carga m√°xima en Redis, escribimos a 80-100K consultas por segundo. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><a name="habracut"></a><br><br><h2>  ¬øPor qu√© cambiamos de Redis a PostgreSQL? </h2><br>  Inicialmente, nuestro servicio trabaj√≥ con Redis, un repositorio de valores clave que almacena todos los datos en la RAM del servidor. <br><br>  Pros de Redis: <br><br><ol><li>  Alta tasa de respuesta, como  todo se almacena en la memoria; </li><li>  Conveniencia de copia de seguridad y replicaci√≥n. </li></ol><br>  Contras Redis para nosotros: <br><br><ol><li> No hay transacciones reales.  Intentamos simularlos a nivel de nuestra aplicaci√≥n.  Desafortunadamente, esto no siempre funcion√≥ bien y requer√≠a escribir c√≥digo muy complejo. </li><li>  La cantidad de datos est√° limitada por la cantidad de memoria.  A medida que aumenta la cantidad de datos, la memoria crecer√° y, al final, nos toparemos con las caracter√≠sticas de la instancia seleccionada, lo que en AWS requiere detener nuestro servicio para cambiar el tipo de instancia. </li><li>  Es necesario mantener constantemente una baja latencia, ya que  Tenemos una gran cantidad de solicitudes.  El nivel de retraso √≥ptimo para nosotros es de 17-20 ms.  A un nivel de 30-40 ms, obtenemos respuestas largas a las solicitudes de nuestra aplicaci√≥n y la degradaci√≥n del servicio.  Desafortunadamente, esto sucedi√≥ con nosotros en septiembre de 2018, cuando una de las instancias de Redis por alguna raz√≥n recibi√≥ una latencia 2 veces mayor de lo habitual.  Para resolver el problema, detuvimos el servicio a mitad del d√≠a por mantenimiento no programado y reemplazamos la instancia problem√°tica de Redis. </li><li>  Es f√°cil obtener inconsistencias de datos incluso con errores menores en el c√≥digo y luego pasar mucho tiempo escribiendo c√≥digo para corregir estos datos. </li></ol><br>  Tomamos en cuenta las desventajas y nos dimos cuenta de que necesitamos pasar a algo m√°s conveniente, con transacciones normales y menos dependencia de la latencia.  Realic√© un estudio, analic√© muchas opciones y eleg√≠ PostgreSQL. <br><br>  Nos hemos mudado a una nueva base de datos durante 1,5 a√±os y solo hemos transferido una peque√±a parte de los datos, por lo que ahora estamos trabajando simult√°neamente con Redis y PostgreSQL.  M√°s informaci√≥n sobre las etapas de mover y cambiar datos entre bases de datos est√° escrita en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo de mi colega</a> . <br><br>  Cuando reci√©n comenzamos a movernos, nuestra aplicaci√≥n trabaj√≥ directamente con la base de datos y recurri√≥ al asistente de Redis y PostgreSQL.  El cl√∫ster PostgreSQL constaba de una r√©plica maestra y una r√©plica as√≠ncrona.  As√≠ es como se ve√≠a el esquema de operaci√≥n de la base de datos: <br><img src="https://habrastorage.org/webt/wc/wg/ef/wcwgefzqham9mw7hm-5xc37pfp0.png"><br><br><h2>  Despliegue de PgBouncer </h2><br>  Mientras nos mov√≠amos, el producto tambi√©n se desarroll√≥: aument√≥ la cantidad de usuarios y la cantidad de servidores que funcionaban con PostgreSQL, y comenzamos a perder conexiones.  PostgreSQL crea un proceso separado para cada conexi√≥n y consume recursos.  Puede aumentar el n√∫mero de conexiones hasta cierto punto, de lo contrario existe la posibilidad de obtener una operaci√≥n de base de datos no √≥ptima.  La opci√≥n ideal en esta situaci√≥n ser√≠a la elecci√≥n de un administrador de conexi√≥n que se parar√≠a frente a la base. <br><br>  Ten√≠amos dos opciones para el administrador de conexi√≥n: Pgpool y PgBouncer.  Pero el primero no admite el modo transaccional de trabajar con la base de datos, por lo que elegimos PgBouncer. <br><br>  Hemos configurado el siguiente esquema de trabajo: nuestra aplicaci√≥n accede a un PgBouncer, seguido de Masters PostgreSQL, y detr√°s de cada maestro, una r√©plica con replicaci√≥n asincr√≥nica. <br><img src="https://habrastorage.org/webt/ql/uq/vh/qluqvh64yeyzwvow79wc3tt0nm0.png"><br><br>  Al mismo tiempo, no pudimos almacenar la cantidad total de datos en PostgreSQL, y la velocidad de trabajar con la base de datos fue importante para nosotros, por lo que comenzamos a compartir PostgreSQL a nivel de aplicaci√≥n.  El esquema descrito anteriormente es relativamente conveniente para esto: al agregar un nuevo fragmento de PostgreSQL, es suficiente para actualizar la configuraci√≥n de PgBouncer y la aplicaci√≥n puede trabajar inmediatamente con el nuevo fragmento. <br><br><h3>  PgBouncer Fault Tolerance </h3><br>  Este esquema funcion√≥ hasta que la √∫nica instancia de PgBouncer muri√≥.  Estamos ubicados en AWS, donde todas las instancias se ejecutan en hardware que muere peri√≥dicamente.  En tales casos, la instancia simplemente se mueve al nuevo hardware y funciona nuevamente.  Esto sucedi√≥ con PgBouncer, pero no estuvo disponible.  El resultado de esta ca√≠da fue la inaccesibilidad de nuestro servicio durante 25 minutos.  AWS recomienda el uso de redundancia en el lado del usuario para tales situaciones, que no se implement√≥ con nosotros en ese momento. <br><br>  Despu√©s de eso, pensamos seriamente en la tolerancia a fallas de los cl√∫steres PgBouncer y PostgreSQL, porque una situaci√≥n similar podr√≠a ocurrir nuevamente con cualquier instancia en nuestra cuenta de AWS. <br><br>  Creamos el esquema de tolerancia a fallos PgBouncer de la siguiente manera: todos los servidores de aplicaciones acceden al equilibrador de carga de red, detr√°s del cual hay dos PgBouncer.  Cada uno de los PgBouncer mira el mismo PostgreSQL maestro de cada fragmento.  Si la instancia de AWS falla nuevamente, todo el tr√°fico se redirige a trav√©s de otro PgBouncer.  Tolerancia a fallos Network Load Balancer proporciona AWS. <br><br>  Este esquema le permite agregar f√°cilmente nuevos servidores PgBouncer. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br><h2>  Crear un cl√∫ster de conmutaci√≥n por error de PostgreSQL </h2><br>  Al resolver este problema, consideramos diferentes opciones: failover auto-escrito, repmgr, AWS RDS, Patroni. <br><br><h3>  Escrituras autoescritas </h3><br>  Pueden monitorear el trabajo del maestro y, en caso de ca√≠da, promover la r√©plica al maestro y actualizar la configuraci√≥n de PgBouncer. <br><br>  Las ventajas de este enfoque son la m√°xima simplicidad, porque usted mismo escribe scripts y comprende exactamente c√≥mo funcionan. <br><br>  Contras: <br><br><ul><li>  El maestro podr√≠a no morir; en cambio, podr√≠a ocurrir una falla en la red.  La conmutaci√≥n por error, sin saber esto, avanzar√° la r√©plica al maestro, y el viejo maestro continuar√° funcionando.  Como resultado, obtenemos dos servidores en el rol de maestro y no sabemos cu√°l de ellos tiene los datos reales m√°s recientes.  Esta situaci√≥n tambi√©n se llama cerebro dividido; </li><li>  Nos quedamos sin una r√©plica.  En nuestra configuraci√≥n, el maestro y una r√©plica, despu√©s de cambiar la r√©plica, se mueve al maestro y ya no tenemos r√©plicas, por lo que debemos agregar manualmente una nueva r√©plica; </li><li>  Necesitamos monitoreo adicional de la operaci√≥n de failover, mientras que tenemos 12 fragmentos PostgreSQL, lo que significa que debemos monitorear 12 cl√∫steres.  Si aumenta el n√∫mero de fragmentos, debe recordar actualizar la conmutaci√≥n por error. </li></ul><br>  La conmutaci√≥n por error autoescrita parece muy complicada y requiere un soporte no trivial.  Con un solo cl√∫ster PostgreSQL, esta ser√° la opci√≥n m√°s f√°cil, pero no escala, por lo que no es adecuado para nosotros. <br><br><h3>  Repmgr </h3><br>  Administrador de replicaci√≥n para cl√∫steres de PostgreSQL, que puede administrar el funcionamiento de un cl√∫ster de PostgreSQL.  Al mismo tiempo, no hay una conmutaci√≥n por error autom√°tica "fuera de la caja", por lo que para el trabajo deber√° escribir su propio "envoltorio" en la parte superior de la soluci√≥n final.  Por lo tanto, todo puede resultar a√∫n m√°s complicado que con secuencias de comandos autoescritas, por lo que ni siquiera probamos Repmgr. <br><br><h3>  AWS RDS </h3><br>  Admite todo lo que necesita para nosotros, sabe c√≥mo hacer copias de seguridad y admite un grupo de conexiones.  Tiene conmutaci√≥n autom√°tica: a la muerte del maestro, la r√©plica se convierte en el nuevo maestro, y AWS cambia el registro dns al nuevo maestro, mientras que las r√©plicas pueden estar en diferentes AZ. <br><br>  Las desventajas incluyen la falta de configuraciones sutiles.  Como ejemplo de ajuste: en nuestras instancias hay restricciones para las conexiones tcp, que, desafortunadamente, no se pueden hacer en RDS: <br><br><pre><code class="python hljs">net.ipv4.tcp_keepalive_time=<span class="hljs-number"><span class="hljs-number">10</span></span> net.ipv4.tcp_keepalive_intvl=<span class="hljs-number"><span class="hljs-number">1</span></span> net.ipv4.tcp_keepalive_probes=<span class="hljs-number"><span class="hljs-number">5</span></span> net.ipv4.tcp_retries2=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br>  Adem√°s, el precio de AWS RDS es casi dos veces m√°s alto que el precio de la instancia regular, que fue la raz√≥n principal para rechazar esta decisi√≥n. <br><br><h3>  Patroni </h3><br>  Esta es una plantilla de Python para administrar PostgreSQL con buena documentaci√≥n, failover autom√°tico y c√≥digo fuente de github. <br><br>  Pros de Patroni: <br><br><ul><li>  Cada par√°metro de configuraci√≥n est√° pintado, est√° claro c√≥mo funciona; </li><li>  La conmutaci√≥n por error autom√°tica funciona de f√°brica; </li><li>  Est√° escrito en python, y dado que nosotros mismos escribimos mucho en python, ser√° m√°s f√°cil para nosotros lidiar con los problemas y, posiblemente, incluso ayudar al desarrollo del proyecto; </li><li>  Controla completamente PostgreSQL, le permite cambiar la configuraci√≥n en todos los nodos del cl√∫ster a la vez, y si se requiere reiniciar el cl√∫ster para aplicar la nueva configuraci√≥n, esto se puede hacer nuevamente usando Patroni. </li></ul><br>  Contras: <br><br><ul><li>  De la documentaci√≥n no est√° claro c√≥mo trabajar con PgBouncer.  Aunque es dif√≠cil llamarlo un menos, porque la tarea de Patroni es administrar PostgreSQL, y c√≥mo van a ser las conexiones con Patroni es nuestro problema; </li><li>  Hay pocos ejemplos de implementaci√≥n de Patroni en grandes vol√∫menes, mientras que muchos ejemplos de implementaci√≥n desde cero. </li></ul><br>  Como resultado, para crear un cl√∫ster de conmutaci√≥n por error, elegimos Patroni. <br><br><h2>  Proceso de Implementaci√≥n Patroni </h2><br>  Antes de Patroni, ten√≠amos 12 fragmentos de PostgreSQL en configuraci√≥n, un maestro y una r√©plica con replicaci√≥n asincr√≥nica.  Los servidores de aplicaciones accedieron a las bases de datos a trav√©s del Network Load Balancer, detr√°s del cual hab√≠a dos instancias con PgBouncer, y detr√°s de ellos estaban todos los servidores PostgreSQL. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br>  Para implementar Patroni, necesit√°bamos seleccionar un repositorio de configuraci√≥n de cl√∫ster distribuido.  Patroni trabaja con sistemas de almacenamiento de configuraci√≥n distribuida como etcd, Zookeeper, Consul.  Solo tenemos un cl√∫ster de c√≥nsul completo en prod que funciona en conjunto con Vault y ya no lo usamos.  Una gran raz√≥n para comenzar a usar Consul para el prop√≥sito previsto. <br><br><h3>  C√≥mo trabaja Patroni con el c√≥nsul </h3><br>  Tenemos un grupo C√≥nsul, que consta de tres nodos, y un grupo Patroni, que consiste en un l√≠der y una r√©plica (en Patroni, un maestro se llama l√≠der del grupo, y los esclavos se llaman r√©plicas).  Cada instancia de un cl√∫ster Patroni env√≠a constantemente informaci√≥n de estado del cl√∫ster al c√≥nsul.  Por lo tanto, desde C√≥nsul siempre puede averiguar la configuraci√≥n actual del cl√∫ster Patroni y qui√©n es el l√≠der en este momento. <br><br><img src="https://habrastorage.org/webt/jx/j5/is/jxj5ispkzoegn8x80dw-qtynw3q.png"><br><br>  Para conectar Patroni a Consul, es suficiente estudiar la documentaci√≥n oficial, que dice que debe especificar el host en formato http o https, dependiendo de c√≥mo trabajemos con Consul, y el esquema de conexi√≥n, opcionalmente: <br><br><pre> <code class="plaintext hljs">host: the host:port for the Consul endpoint, in format: http(s)://host:port scheme: (optional) http or https, defaults to http</code> </pre> <br>  Parece simple, pero aqu√≠ comienzan las trampas.  Con Consul estamos trabajando en una conexi√≥n segura a trav√©s de https y nuestra configuraci√≥n de conexi√≥n se ver√° as√≠: <br><br><pre> <code class="python hljs">consul: host: https://server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br>  Pero eso no funciona.  Al principio, Patroni no puede conectarse con el c√≥nsul, porque de todos modos intenta seguir http. <br><br>  El c√≥digo fuente de Patroni ayud√≥ a resolver el problema.  Lo bueno es que est√° escrito en python.  Resulta que el par√°metro del host no se analiza en absoluto, y el protocolo debe especificarse en el esquema.  Aqu√≠ est√° el bloque de configuraci√≥n de trabajo para trabajar con Consul con nosotros: <br><br><pre> <code class="python hljs">consul: host: server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> scheme: https verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br><h3>  C√≥nsul-plantilla </h3><br>  Por lo tanto, hemos elegido el almacenamiento para una configuraci√≥n.  Ahora debe comprender c√≥mo PgBouncer cambiar√° su configuraci√≥n al cambiar el l√≠der en el cl√∫ster Patroni.  La documentaci√≥n no responde a esta pregunta, porque  all√≠, en principio, no se describe el trabajo con PgBouncer. <br><br>  En busca de una soluci√≥n, encontramos un art√≠culo (no recuerdo el nombre, desafortunadamente), donde estaba escrito que la plantilla de C√≥nsul ayud√≥ mucho a conectar a PgBouncer y Patroni.  Esto nos llev√≥ a estudiar el trabajo de la plantilla Consul. <br><br>  Result√≥ que la plantilla de Consul supervisa constantemente la configuraci√≥n del cl√∫ster PostgreSQL en Consul.  Cuando el l√≠der cambia, actualiza la configuraci√≥n de PgBouncer y env√≠a un comando para reiniciarlo. <br><br><img src="https://habrastorage.org/webt/iv/_f/j-/iv_fj-sjbnqtfabqlnmu986sa0o.png"><br><br>  La gran ventaja de la plantilla es que se almacena como c√≥digo, por lo que al agregar un nuevo fragmento, es suficiente realizar una nueva confirmaci√≥n y actualizar la plantilla en modo autom√°tico, lo que respalda el principio de Infraestructura como c√≥digo. <br><br><h3>  Nueva arquitectura con Patroni </h3><br>  Como resultado, obtuvimos este esquema de trabajo: <br><img src="https://habrastorage.org/webt/7b/-m/-v/7b-m-vyorrbbuognzt2qx2-fymm.png"><br><br>  Todos los servidores de aplicaciones acceden al equilibrador ‚Üí PgBouncer de dos instancias est√°n detr√°s de √©l ‚Üí en cada instancia se lanza una plantilla onsul, que monitorea el estado de cada cl√∫ster Patroni y monitorea la relevancia de la configuraci√≥n de PgBouncer, que env√≠a solicitudes al l√≠der actual de cada cl√∫ster. <br><br><h3>  Prueba manual </h3><br>  Antes de iniciar el programa, lanzamos este circuito en un entorno de prueba peque√±o y verificamos el funcionamiento de la conmutaci√≥n autom√°tica.  Abrieron el tablero, movieron la pegatina y en ese momento "mataron" al l√≠der del grupo.  En AWS, simplemente apague la instancia a trav√©s de la consola. <br><br><img src="https://habrastorage.org/webt/ly/yf/aj/lyyfaj6bxodfoaqrsds5j00ycnw.gif"><br><br>  La pegatina regres√≥ en 10-20 segundos, y luego nuevamente comenz√≥ a moverse normalmente.  Esto significa que el cl√∫ster Patroni funcion√≥ correctamente: cambi√≥ el l√≠der, envi√≥ la informaci√≥n al C√≥nsul y la plantilla del C√≥nsul inmediatamente recogi√≥ esta informaci√≥n, reemplaz√≥ la configuraci√≥n de PgBouncer y envi√≥ el comando para recargar. <br><br><h2>  ¬øC√≥mo sobrevivir bajo una carga alta y mantener un tiempo de inactividad m√≠nimo? </h2><br>  ¬°Todo funciona muy bien!  Pero surgen nuevas preguntas: ¬øC√≥mo funcionar√° bajo alta carga?  ¬øC√≥mo llevar todo de forma r√°pida y segura a producci√≥n? <br><br>  El entorno de prueba en el que realizamos pruebas de carga nos ayuda a responder la primera pregunta.  Es completamente id√©ntico a la producci√≥n en arquitectura y ha generado datos de prueba, que son aproximadamente iguales en volumen a la producci√≥n.  Decidimos simplemente "matar" a uno de los asistentes de PostgreSQL durante la prueba y ver qu√© sucede.  Pero antes de eso, es importante verificar el desplazamiento autom√°tico, ya que en este entorno tenemos varios fragmentos PostgreSQL, por lo que obtendremos excelentes pruebas de los scripts de configuraci√≥n antes de vender. <br><br>  Ambas tareas parecen ambiciosas, pero tenemos PostgreSQL 9.6.  ¬øTal vez vamos a actualizar inmediatamente a 11.2? <br><br>  Decidimos hacer esto en 2 etapas: primero actualice a 11.2, luego inicie Patroni. <br><br><h3>  Actualizaci√≥n de PostgreSQL </h3><br>  Para actualizar r√°pidamente la versi√≥n de PostgreSQL, debe usar la opci√≥n <b>-k</b> , que crea un enlace duro en el disco y no es necesario copiar sus datos.  En bases de 300-400 GB, la actualizaci√≥n lleva 1 segundo. <br><br>  Tenemos muchos fragmentos, por lo que la actualizaci√≥n debe hacerse autom√°ticamente.  Para hacer esto, escribimos el libro de jugadas Ansible, que realiza todo el proceso de actualizaci√≥n para nosotros: <br><br><pre> <code class="plaintext hljs">/usr/lib/postgresql/11/bin/pg_upgrade \ &lt;b&gt;--link \&lt;/b&gt; --old-datadir='' --new-datadir='' \ --old-bindir='' --new-bindir='' \ --old-options=' -c config_file=' \ --new-options=' -c config_file='</code> </pre> <br>  Es importante tener en cuenta aqu√≠ que antes de comenzar la actualizaci√≥n, es necesario ejecutarlo con el par√°metro <b>--check</b> para estar seguro de la posibilidad de una actualizaci√≥n.  Nuestro script tambi√©n hace la sustituci√≥n de configuraciones para la actualizaci√≥n.  El gui√≥n que completamos en 30 segundos, este es un excelente resultado. <br><br><h3>  Lanzar Patroni </h3><br>  Para resolver el segundo problema, solo mira la configuraci√≥n de Patroni.  En el repositorio oficial hay una configuraci√≥n de ejemplo con initdb, que se encarga de inicializar una nueva base de datos cuando se inicia Patroni por primera vez.  Pero como tenemos una base de datos preparada, acabamos de eliminar esta secci√≥n de la configuraci√≥n. <br><br>  Cuando comenzamos a instalar Patroni en un cl√∫ster PostgreSQL listo para usar y ejecutarlo, nos enfrentamos a un nuevo problema: ambos servidores comenzaron como l√≠deres.  Patroni no sabe nada sobre el estado inicial del cl√∫ster e intenta iniciar ambos servidores como dos cl√∫steres separados con el mismo nombre.  Para resolver este problema, elimine el directorio de datos en el esclavo: <br><br><pre> <code class="plaintext hljs">rm -rf /var/lib/postgresql/</code> </pre> <br>  <b>¬°Esto debe hacerse solo en esclavo!</b> <br><br>  Al conectar una r√©plica limpia, Patroni crea un l√≠der de respaldo base y lo restaura a la r√©plica, y luego se pone al d√≠a con el estado actual mediante registros de wal. <br><br>  Otra dificultad que encontramos es que todos los cl√∫steres de PostgreSQL se denominan main por defecto.  Cuando cada grupo no sabe nada del otro, esto es normal.  Pero cuando desea utilizar Patroni, todos los cl√∫steres deben tener un nombre √∫nico.  La soluci√≥n es cambiar el nombre del cl√∫ster en la configuraci√≥n de PostgreSQL. <br><br><h3>  Prueba de carga </h3><br>  Lanzamos una prueba que simula el trabajo de los usuarios en los tableros.  Cuando la carga alcanz√≥ nuestro valor diario promedio, repetimos exactamente la misma prueba, apagamos una instancia con el l√≠der PostgreSQL.  La conmutaci√≥n por error autom√°tica funcion√≥ como esper√°bamos: Patroni cambi√≥ el l√≠der, Consul-template actualiz√≥ la configuraci√≥n de PgBouncer y envi√≥ el comando para recargar.  De acuerdo con nuestros gr√°ficos en Grafana, estaba claro que hay demoras de 20-30 segundos y una peque√±a cantidad de errores de los servidores relacionados con la conexi√≥n a la base de datos.  Esta es una situaci√≥n normal, tales valores son v√°lidos para nuestra conmutaci√≥n por error y definitivamente mejores que el tiempo de inactividad del servicio. <br><br><h2>  La producci√≥n de Patroni a la producci√≥n. </h2><br>  Como resultado, obtuvimos el siguiente plan: <br><br><ul><li>  Implemente la plantilla Consul en el servidor PgBouncer y ejec√∫tela; </li><li>  PostgreSQL actualiza a la versi√≥n 11.2; </li><li>  Cambio de nombre del cl√∫ster; </li><li>  Iniciando un grupo Patroni. </li></ul><br>  Al mismo tiempo, nuestro esquema le permite hacer el primer elemento en casi cualquier momento, podemos turnarnos para eliminar cada PgBouncer del trabajo y ejecutar una implementaci√≥n en √©l y lanzar la plantilla de c√≥nsul.  Entonces lo hicimos. <br><br>  Para un rodaje r√°pido, utilizamos Ansible, ya que ya verificamos todo el libro de jugadas en un entorno de prueba, y el tiempo de ejecuci√≥n del gui√≥n completo fue de 1.5 a 2 minutos por cada fragmento.  Podr√≠amos desplegar todo alternativamente para cada fragmento sin detener nuestro servicio, pero tendr√≠amos que apagar cada PostgreSQL durante unos minutos.  En este caso, los usuarios cuyos datos se encuentran en este fragmento no podr√≠an funcionar completamente en este momento, y esto es inaceptable para nosotros. <br><br>  La soluci√≥n a esta situaci√≥n fue el mantenimiento planificado, que se realiza cada 3 meses.  Esta es una ventana para el trabajo programado cuando apagamos completamente nuestro servicio y actualizamos las instancias de la base de datos.  Faltaba una semana para la pr√≥xima ventana, y decidimos esperar y prepararnos m√°s.  Durante la espera, tambi√©n nos aseguramos: para cada fragmento de PostgreSQL levantamos una r√©plica de repuesto en caso de falla para guardar los √∫ltimos datos, y agregamos una nueva instancia para cada fragmento, que deber√≠a convertirse en una nueva r√©plica en el cl√∫ster Patroni para no ejecutar un comando para eliminar datos .  Todo esto ayud√≥ a minimizar el riesgo de error. <br><img src="https://habrastorage.org/webt/ps/ge/yh/psgeyhvrazg-zd1nrochwhl1hag.png"><br><br>  Reiniciamos nuestro servicio, todo funcion√≥ como deber√≠a, los usuarios continuaron trabajando, pero en los gr√°ficos notamos una carga anormalmente alta en el servidor Consul. <br><img src="https://habrastorage.org/webt/uk/b9/gu/ukb9guaj4wfabq60v262qe098am.png"><br><br>  ¬øPor qu√© no lo vimos en el entorno de prueba?  Este problema ilustra muy bien que es necesario seguir el principio de Infraestructura como c√≥digo y refinar toda la infraestructura, comenzando con entornos de prueba y terminando con la producci√≥n.  De lo contrario, es muy f√°cil obtener el tipo de problema que tenemos.  Que paso  Consul primero apareci√≥ en producci√≥n y luego en entornos de prueba, como resultado, en entornos de prueba, la versi√≥n de Consul fue m√°s alta que en producci√≥n.  Solo en una de las versiones, se resolvi√≥ una fuga de CPU al trabajar con consul-template.  Por lo tanto, acabamos de actualizar Consul, resolviendo as√≠ el problema. <br><br><h3>  Reiniciar el cl√∫ster Patroni </h3><br>  Sin embargo, tenemos un nuevo problema del que ni siquiera √©ramos conscientes.  Al actualizar Consul, simplemente eliminamos el nodo Consul del cl√∫ster utilizando el comando c√≥nsul leave ‚Üí Patroni se conecta a otro servidor Consul ‚Üí todo funciona.  Pero cuando llegamos a la √∫ltima instancia del grupo C√≥nsul y le enviamos el comando de abandono, todos los grupos Patroni simplemente se reiniciaron, y en los registros vimos el siguiente error: <br><br><pre> <code class="plaintext hljs">ERROR: get_cluster Traceback (most recent call last): ... RetryFailedError: 'Exceeded retry deadline' ERROR: Error communicating with DCS &lt;b&gt;LOG: database system is shut down&lt;/b&gt;</code> </pre> <br>  El cl√∫ster Patroni no pudo obtener informaci√≥n sobre su cl√∫ster y se reinici√≥. <br><br>  Para encontrar una soluci√≥n, contactamos a los autores de Patroni a trav√©s del n√∫mero en github.  Sugirieron mejoras en nuestros archivos de configuraci√≥n: <br><br><pre> <code class="python hljs">consul: consul.checks: [] bootstrap: dcs: retry_timeout: <span class="hljs-number"><span class="hljs-number">8</span></span></code> </pre> <br>  Pudimos repetir el problema en un entorno de prueba y probamos estos par√°metros all√≠, pero, desafortunadamente, no funcionaron. <br><br>  El problema a√∫n no se ha resuelto.  Planeamos probar las siguientes soluciones: <br><br><ul><li>  Utilice el C√≥nsul-agente en cada instancia del cl√∫ster Patroni; </li><li>  Soluciona el problema en el c√≥digo. </li></ul><br>  Entendemos d√≥nde se produjo el error: el problema probablemente sea usar el tiempo de espera predeterminado, que no se anula a trav√©s del archivo de configuraci√≥n.  Cuando se elimina el √∫ltimo servidor Consul del cl√∫ster, todo el cl√∫ster Consul se congela durante m√°s de un segundo, debido a esto Patroni no puede obtener el estado del cl√∫ster y reinicia completamente el cl√∫ster completo. <br><br>  Afortunadamente, no encontramos m√°s errores. <br><br><h2>  Resultados de usar Patroni </h2><br>  Despu√©s del exitoso lanzamiento de Patroni, agregamos una r√©plica adicional en cada cl√∫ster.  Ahora en cada grupo hay una apariencia de qu√≥rum: un l√≠der y dos r√©plicas, para asegurarse contra el caso de cerebro dividido al cambiar. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><br>  Patroni ha estado trabajando en producci√≥n durante m√°s de tres meses.  Durante este tiempo, ya ha logrado ayudarnos.  Recientemente, el l√≠der de uno de los cl√∫steres muri√≥ en AWS, la conmutaci√≥n por error autom√°tica funcion√≥ y los usuarios continuaron trabajando.  Patroni complet√≥ su tarea principal. <br><br>  <b>Un peque√±o resumen del uso de Patroni:</b> <br><br><ul><li>  La conveniencia del cambio de una configuraci√≥n.  Es suficiente cambiar la configuraci√≥n en una instancia y se colocar√° sobre todo el cl√∫ster.  Si se requiere un reinicio para aplicar la nueva configuraci√≥n, Patroni lo informar√°.  Patroni puede reiniciar todo el cl√∫ster con un solo comando, lo que tambi√©n es muy conveniente. </li><li>  La conmutaci√≥n por error autom√°tica funciona y ya ha logrado ayudarnos. </li><li>  Actualizaci√≥n de PostgreSQL sin tiempo de inactividad de la aplicaci√≥n.  Primero debe actualizar las r√©plicas a la nueva versi√≥n, luego cambiar el l√≠der en el cl√∫ster Patroni y actualizar el l√≠der anterior.  En este caso, se realizan las pruebas necesarias de conmutaci√≥n por error autom√°tica. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/457326/">https://habr.com/ru/post/457326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457308/index.html">En el camino de Sergey Pavlovich Korolev. Proyecto tripulado ruso moderno. Parte 2. Cohete</a></li>
<li><a href="../457310/index.html">Biolog√≠a de la dependencia de la informaci√≥n</a></li>
<li><a href="../457312/index.html">Introducci√≥n a la teor√≠a de conjuntos</a></li>
<li><a href="../457316/index.html">C√≥mo se organiza el juego de rol en el mundo real para los invitados de Armenia con viajes por la mitad del pa√≠s</a></li>
<li><a href="../457324/index.html">Eventos digitales en Mosc√∫ del 24 al 30 de junio</a></li>
<li><a href="../457328/index.html">Categor√≠as en lugar de directorios, o el sistema de archivos sem√°ntico para Linux</a></li>
<li><a href="../457330/index.html">¬øC√≥mo verificar r√°pidamente las advertencias interesantes que brinda el analizador PVS-Studio para el c√≥digo C y C ++?</a></li>
<li><a href="../457332/index.html">¬øC√≥mo ver r√°pidamente advertencias interesantes generadas por el analizador PVS-Studio para c√≥digo C y C ++?</a></li>
<li><a href="../457334/index.html">TacacsGUI, Administrador de configuraci√≥n</a></li>
<li><a href="../457336/index.html">Las consecuencias de la extracci√≥n prematura de las muelas del juicio</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>