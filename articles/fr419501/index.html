<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíì üë©‚Äçüöí üòæ Deep Learning: reconna√Ætre les sc√®nes et les points de rep√®re dans les images ü•ê üëÇüèæ üéôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il est temps de reconstituer la tirelire de bons rapports en langue russe sur l'apprentissage automatique! La tirelire elle-m√™me ne sera pas r√©approvi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep Learning: reconna√Ætre les sc√®nes et les points de rep√®re dans les images</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jugru/blog/419501/">  Il est temps de reconstituer la tirelire de bons rapports en langue russe sur l'apprentissage automatique!  La tirelire elle-m√™me ne sera pas r√©approvisionn√©e! <br><br>  Cette fois, nous allons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©couvrir l‚Äô</a> histoire fascinante d‚Äô <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Andrei Boyarov</a> sur la reconnaissance des sc√®nes.  Andrey est un chercheur en vision par ordinateur engag√© dans la vision industrielle chez Mail.Ru Group. <br><br>  La reconnaissance de sc√®nes est l'un des domaines les plus utilis√©s de la vision industrielle.  Cette t√¢che est plus compliqu√©e que la reconnaissance √©tudi√©e des objets: la sc√®ne est un concept plus complexe et moins formalis√©, il est plus difficile de distinguer les traits.  La reconnaissance des sites d√©coule de la reconnaissance des sc√®nes: vous devez mettre en √©vidence les endroits connus sur la photo, en garantissant un faible niveau de faux positifs. <br><br>  Il s'agit de <b>30 minutes de</b> vid√©o de la conf√©rence Smart Data 2017. La vid√©o est pratique √† regarder √† la maison et en d√©placement.  Pour ceux qui ne sont pas pr√™ts √† s'asseoir autant √† l'√©cran, ou qui pr√©f√®rent percevoir les informations sous forme de texte, nous appliquons un d√©cryptage de texte int√©gral, con√ßu sous la forme d'habrosta. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dL1-OrjtMvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  Je fais de la vision industrielle sur Mail.ru.  Aujourd'hui, je vais parler de la fa√ßon dont nous utilisons l'apprentissage en profondeur pour reconna√Ætre les images de sc√®nes et d'attractions. <br><br>  La soci√©t√© a ressenti le besoin de baliser et de rechercher par les images des utilisateurs, et pour cela nous avons d√©cid√© de cr√©er notre propre API de vision par ordinateur, dont une partie sera un outil de balisage de sc√®ne.  Gr√¢ce √† cet outil, nous voulons obtenir quelque chose comme celui montr√© dans l'image ci-dessous: l'utilisateur fait une demande, par exemple, "cath√©drale", et re√ßoit toutes ses photos avec des cath√©drales. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d18/5f8/a3b/d185f8a3bd0c73280cdae408fe84cca1.png"><br><br>  Dans Computer Vision-community, le sujet de la reconnaissance d'objets dans les images a √©t√© assez bien √©tudi√©.  Il existe un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">concours ImageNet</a> bien connu qui se d√©roule depuis plusieurs ann√©es et dont la partie principale est la reconnaissance d'objets. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/989/f18/b8a989f189970cabb0de00be7ff48afa.png"><br><br>  Nous devons essentiellement localiser un objet et le classer.  Avec les sc√®nes, la t√¢che est un peu plus compliqu√©e, car la sc√®ne est un objet plus complexe, elle se compose d'un grand nombre d'autres objets et du contexte qui les unit, les t√¢ches sont donc diff√©rentes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/be7/a6b/ba4be7a6bc9a4e0c2c7929644540799c.png"><br><br>  Sur Internet, il existe des services disponibles d'autres soci√©t√©s qui mettent en ≈ìuvre une telle fonctionnalit√©.  Il s'agit en particulier de l'API Google Vision ou de l'API Microsoft Computer Vision, qui peut trouver des sc√®nes dans des images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/758/fd9/095/758fd909536fd930db97c025ca61ef38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/b28/6df/eeeb286df22a31809f507da7730b7d89.png"><br><br>  Nous avons r√©solu ce probl√®me √† l'aide de l'apprentissage automatique, nous avons donc besoin de donn√©es.  Il existe maintenant deux bases principales pour la reconnaissance de sc√®nes en libre acc√®s.  Le premier d'entre eux est apparu en 2013 - il s'agit de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la base SUN</a> de l'Universit√© de Princeton.  Cette base comprend des centaines de milliers d'images et 397 classes. <br><br>  La deuxi√®me base sur laquelle nous nous sommes form√©s est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la base Places2</a> du MIT.  Elle est apparue en 2013 en deux versions.  Le premier est Places2-Standart, une base plus √©quilibr√©e avec 1,8 million d'images et 365 classes.  La deuxi√®me option - Places2-Challenge, contient huit millions d'images et 365 classes, mais le nombre d'images entre les classes n'est pas √©quilibr√©.  Dans le concours ImageNet 2016, la section Reconnaissance de sc√®ne comprenait le Places2-Challenge, et le gagnant a montr√© le meilleur r√©sultat d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">erreur de classement dans le Top 5</a> d'environ 9%. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a10/ea2/881/a10ea2881a5248449dce8202ceaa28ff.png"><br><br>  Nous nous sommes entra√Æn√©s sur la base de Places2.  Voici un exemple d'image √† partir de l√†: c'est un canyon, une piste, une cuisine, un terrain de football.  Ce sont des objets complexes compl√®tement diff√©rents sur lesquels nous devons apprendre √† reconna√Ætre. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cf/9b2/3ee/1cf9b23eedff87ae9bcbb2d8c4090551.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/602/617/1d1/6026171d19b38452ce0e4a071ba4e836.png"><br><br>  Avant d'√©tudier, nous avons adapt√© les bases que nous devons adapter √† nos besoins.  Il existe une astuce pour la reconnaissance d'objets lorsque vous exp√©rimentez avec des mod√®les sur de petites bases CIFAR-10 et CIFAR-100 au lieu d'ImageNet, et ce n'est qu'alors que les meilleurs s'entra√Ænent sur ImageNet. <br><br>  Nous avons d√©cid√© de suivre le m√™me chemin, avons pris la base de donn√©es SUN, l'avons r√©duite, obtenu 89 classes, 50 000 images dans le train et 10 000 images lors de la validation.  En cons√©quence, avant de nous entra√Æner sur Places2, nous avons mis en place des exp√©riences et test√© nos mod√®les bas√©s sur SUN.  La formation ne prend que 6 √† 10 heures, contrairement √† plusieurs jours sur Places2, ce qui a permis de mener beaucoup plus d'exp√©riences et de le rendre plus efficace. <br><br>  Nous avons √©galement examin√© la base de donn√©es Places2 elle-m√™me et r√©alis√© que nous n'avions pas besoin de certaines classes.  Soit pour des raisons de production, soit parce qu'il y a trop peu de donn√©es √† leur sujet, nous supprimons des classes comme, par exemple, un aqueduc, une cabane dans les arbres, une porte de grange. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f8/31f/60f/4f831f60fd70ec7f74ea2bb0a29c69f5.png"><br><br>  En cons√©quence, apr√®s toutes les manipulations, nous avons obtenu la base de donn√©es Places2, qui contient 314 classes et un demi-million d'images (dans sa version standard), dans la version Challenge environ 7,5 millions d'images.  Nous avons construit la formation sur ces bases. <br><br>  De plus, lors de la visualisation des classes restantes, nous avons d√©couvert qu'il y en avait trop pour la production, elles sont trop d√©taill√©es.  Et pour cela, nous avons appliqu√© le m√©canisme de mappage de sc√®ne lorsque certaines classes sont combin√©es en une seule.  Par exemple, nous avons connect√© tout ce qui est li√© aux for√™ts √† une for√™t, tout ce qui est li√© aux h√¥pitaux - √† un h√¥pital, aux h√¥tels - √† un h√¥tel. <br><br>  Nous utilisons le mappage de sc√®ne uniquement pour les tests et pour l'utilisateur final, car c'est plus pratique.  En formation, nous utilisons toutes les classes standard 314.  Nous avons appel√© la base r√©sultante Places Sift. <br><br><h2>  Approches, solutions </h2><br>  Consid√©rez maintenant les approches que nous avons utilis√©es pour r√©soudre ce probl√®me.  En fait, ces t√¢ches sont li√©es √† l'approche classique - les r√©seaux de neurones convolutionnels profonds. <br><br>  L'image ci-dessous montre l'un des premiers r√©seaux classiques, mais il contient d√©j√† les principaux blocs de construction utilis√©s dans les r√©seaux modernes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a4/61f/7f7/0a461f7f7ae8a9264f5fadb69271f29d.png"><br><br>  Ce sont des couches convolutives, ce sont des couches de traction, des couches enti√®rement connect√©es.  Afin de d√©terminer l'architecture, nous avons v√©rifi√© les sommets des concours ImageNet et Places2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7dc/658/7d0/7dc6587d046a968988d4f52fa7bcbb3f.png"><br><br>  On peut dire que les principales architectures principales peuvent √™tre divis√©es en deux familles: Inception et la famille ResNet (r√©seau r√©siduel).  Au cours des exp√©riences, nous avons d√©couvert que la famille ResNet est mieux adapt√©e √† notre t√¢che et nous avons men√© la prochaine exp√©rience sur cette famille. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/828/85b/1a882885b70533dd0341bd853d2818fd.png"><br><br>  ResNet est un r√©seau profond qui se compose d'un grand nombre de blocs r√©siduels.  C'est son bloc de construction principal, qui se compose de plusieurs couches avec des poids et une connexion de raccourci.  Gr√¢ce √† cette conception, cet appareil apprend √† quel point le signal d'entr√©e x diff√®re de la sortie f (x).  En cons√©quence, nous pouvons construire des r√©seaux de tels blocs, et pendant l'entra√Ænement, le r√©seau dans les derni√®res couches peut faire des poids proches de z√©ro. <br><br>  Ainsi, nous pouvons dire que le r√©seau lui-m√™me d√©cide de la profondeur qu'il doit avoir pour r√©soudre certaines t√¢ches.  Gr√¢ce √† cette architecture, il est possible de construire des r√©seaux de tr√®s grande profondeur avec un tr√®s grand nombre de couches.  Le gagnant d'ImageNet 2014 ne contenait que 22 couches, ResNet a d√©pass√© ce r√©sultat et contenait d√©j√† 152 couches. <br><br>  La recherche principale de ResNet est d'am√©liorer et de construire correctement un bloc r√©siduel.  L'image ci-dessous montre une version empiriquement et math√©matiquement saine qui donne le meilleur r√©sultat.  Une telle construction du bloc vous permet de traiter l'un des probl√®mes fondamentaux de l'apprentissage en profondeur - un gradient de d√©coloration. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/020/7a4/398/0207a439827163bc14895a47a1489ddc.png"><br><br>  Pour former nos r√©seaux, nous avons utilis√© le framework Torch √©crit en Lua en raison de sa flexibilit√© et de sa vitesse, et pour ResNet nous avons bifurqu√© l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">impl√©mentation de ResNet √† partir de Facebook</a> .  Pour valider la qualit√© du r√©seau, nous avons utilis√© trois tests. <br><br>  Le premier test Places val est la validation de nombreux sets Places Sift.  Le deuxi√®me test est le criblage des lieux utilisant la cartographie des sc√®nes, et le troisi√®me est le test Cloud le plus proche de la situation de combat.  Images d'employ√©s prises depuis le cloud et √©tiquet√©es manuellement.  Dans l'image ci-dessous, il y a deux exemples de telles images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/796/6d0/066/7966d0066c9257617e81f290fbbc6283.png"><br><br>  Nous avons commenc√© √† mesurer et √† former des r√©seaux, √† les comparer les uns aux autres.  Le premier est le benchmark ResNet-152, qui vient avec Places2, le second est ResNet-50, que nous avons form√© sur ImageNet et form√© sur notre base, le r√©sultat √©tait d√©j√† meilleur.  Ensuite, ils ont pris ResNet-200, √©galement form√© sur ImageNet, et cela a finalement donn√© le meilleur r√©sultat. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d9/0de/320/6d90de3208968f8120c1f4be1e79f74e.png"><br><br>  Voici des exemples de travaux.  Il s'agit d'une r√©f√©rence ResNet-152.  Les pr√©dictions sont les √©tiquettes originales que le r√©seau distribue.  Les √©tiquettes mapp√©es sont les √©tiquettes qui sont apparues apr√®s le mappage de sc√®ne.  On peut voir que le r√©sultat n'est pas tr√®s bon.  Autrement dit, elle semble donner quelque chose sur l'affaire, mais pas tr√®s bien. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86e/892/ceb/86e892ceb75b85dcf22bec203c0f4f3d.png"><br><br>  L'exemple suivant est le fonctionnement de ResNet-200.  D√©j√† tr√®s ad√©quat. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa6/621/d0d/fa6621d0dd29ac9d77da10378eec9454.png"><br><br><h2>  Am√©lioration ResNet </h2><br>  Nous avons d√©cid√© d'essayer d'am√©liorer notre r√©seau, et au d√©but, nous avons juste essay√© d'augmenter la profondeur du r√©seau, mais apr√®s cela, il est devenu beaucoup plus difficile √† former.  C'est un probl√®me connu, l'ann√©e derni√®re plusieurs articles ont √©t√© publi√©s sur ce sujet, qui disent que ResNet, en fait, est un ensemble d'un grand nombre de r√©seaux ordinaires de diff√©rentes profondeurs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df2/0ad/023/df20ad0234f5eba5597939468f8afaf6.png"><br><br>  Les res-blocs, qui sont √† la fin de la grille, contribuent peu √† la formation du r√©sultat final.  Il semble plus prometteur d'augmenter non pas la profondeur du r√©seau, mais sa largeur, c'est-√†-dire le nombre de filtres √† l'int√©rieur du bloc Res. <br><br>  Cette id√©e est mise en ≈ìuvre par le Wide Residual Network, qui est apparu en 2016.  Nous avons fini par utiliser WRN-50-2, qui est le ResNet-50 habituel avec deux fois le nombre de filtres dans la convolution 3x3 du goulot d'√©tranglement interne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5be/f29/aed/5bef29aed5cfce3b0121ce69ee072c35.png"><br><br>  Le r√©seau affiche sur ImageNet des r√©sultats similaires avec le ResNet-200, que nous avons d√©j√† utilis√©, mais, surtout, il est presque deux fois plus rapide.  Voici deux impl√©mentations du bloc r√©siduel sur Torch; le param√®tre qui est doubl√© est mis en surbrillance.  Il s'agit du nombre de filtres dans la convolution interne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1a/3fe/d24/f1a3fed24bf735ed30cfc407aac6ca79.png"><br><br>  Ce sont des mesures sur les tests ResNet-200 ImageNet.  Au d√©but, nous avons pris le WRN-22-6, le r√©sultat a √©t√© pire.  Ensuite, ils ont pris WRN-50-2-ImageNet, l'ont form√©, ont pris WRN-50-2, form√© sur ImageNet et l'ont form√© sur Places2-challenge, et il a montr√© le meilleur r√©sultat. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/736/fd6/afe/736fd6afe39702160b87ad095ef0edcd.png"><br><br>  Voici un exemple du WRN-50-2 - un r√©sultat tout √† fait ad√©quat dans nos photos que vous avez d√©j√† vues. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0a/442/e1f/b0a442e1f87a4f2e2a6c43a96677ce3d.png"><br><br>  Et ceci est un exemple de travail sur des photographies de combat, √©galement avec succ√®s. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/715/6bf/9247156bf459bd5b005b4180d36a1da5.png"><br><br>  Il y a, bien s√ªr, des ≈ìuvres pas tr√®s r√©ussies.  Le pont d'Alexandre III √† Paris n'√©tait pas reconnu comme un pont. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dd/c66/9ec/3ddc669ecf4aabe61b103dafbc4aabb2.png"><br><br><h2>  Am√©lioration du mod√®le </h2><br>  Nous avons r√©fl√©chi √† la mani√®re d'am√©liorer ce mod√®le.  La famille ResNet continue de s'am√©liorer, avec de nouveaux articles √† para√Ætre.  En particulier, en 2016, un article int√©ressant PyramidNet a √©t√© publi√©, qui a montr√© des r√©sultats prometteurs sur CIFAR-10/100 et ImageNet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdd/816/e1b/fdd816e1b970be4465ff8200b7f0cb56.png"><br><br>  L'id√©e n'est pas d'augmenter fortement la largeur du bloc r√©siduel, mais de le faire progressivement.  Nous avons form√© plusieurs options pour ce r√©seau, mais, malheureusement, il a donn√© des r√©sultats l√©g√®rement pires que notre mod√®le de combat. <br><br>  Au printemps 2018, le mod√®le ResNext est sorti, √©galement une id√©e prometteuse: diviser le bloc r√©siduel en plusieurs blocs parall√®les de plus petite taille, de plus petite largeur.  Ceci est similaire √† l'id√©e d'Inception, nous l'avons √©galement exp√©riment√©e.  Mais, malheureusement, elle a montr√© des r√©sultats pires que notre mod√®le. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/3c4/e0f/7123c4e0f7f826f9dab8a1791d4eb3cd.png"><br><br>  Nous avons √©galement exp√©riment√© diff√©rentes approches ¬´cr√©atives¬ª pour am√©liorer nos mod√®les.  En particulier, nous avons essay√© d'utiliser le mappage d'activation de classe (CAM), c'est-√†-dire que ce sont les objets que le r√©seau examine lorsqu'il classe l'image. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bf/639/ea4/1bf639ea430bd03bafeeacf8acb29a09.png"><br><br>  Notre id√©e √©tait que les instances de la m√™me sc√®ne devraient avoir les m√™mes objets ou des objets similaires √† une classe CAM.  Nous avons essay√© d'utiliser cette approche.  Au d√©but, ils ont pris deux r√©seaux.  L'un est form√© par ImageNet, le second est notre mod√®le, que nous voulons am√©liorer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47d/ac5/538/47dac55383f098852c9a69a3d50f2cc1.png"><br><br>  Nous prenons l'image, parcourons le r√©seau 2, ajoutons le CAM pour la couche, puis la transmettons √† l'entr√©e du r√©seau 1. Parcourez le r√©seau 1, ajoutez les r√©sultats √† la fonction de perte du r√©seau 2, continuez avec les nouvelles fonctions de perte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/c3e/9c3/860c3e9c37f6d6ffd91d3cdd36d67682.png"><br><br>  La deuxi√®me option consiste √† ex√©cuter l'image via le r√©seau 2, √† prendre le CAM, √† l'alimenter √† l'entr√©e du r√©seau 1, puis √† partir de ces donn√©es, nous formons simplement le r√©seau 1 et utilisons l'ensemble √† partir des r√©sultats du r√©seau 1 et du r√©seau 2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/140/95b/5fb14095b117b05582a79a43ee0eadb6.png"><br><br>  Nous avons recycl√© notre mod√®le sur WRN-50-2, en tant que r√©seau 1, nous avons utilis√© ResNet-50 ImageNet, mais il n'a pas √©t√© possible d'augmenter de mani√®re significative la qualit√© de notre mod√®le. <br><br>  Mais nous poursuivons nos recherches pour am√©liorer nos r√©sultats: nous formons de nouvelles architectures CNN, en particulier la famille ResNet.  Nous essayons d'exp√©rimenter avec la FAO et consid√©rons diff√©rentes approches avec un traitement plus intelligent des patchs d'images - il nous semble que cette approche est assez prometteuse. <br><br><h2>  Reconnaissance historique </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/11b/9da/4c3/11b9da4c376bcee091afb70d3dbb0745.png"><br><br>  Nous avons un bon mod√®le pour reconna√Ætre les sc√®nes, mais maintenant nous voulons d√©couvrir des endroits embl√©matiques, c'est-√†-dire des sites.  De plus, les utilisateurs les prennent souvent en photo ou prennent des photos sur leur arri√®re-plan. <br><br>  Nous voulons que le r√©sultat ne soit pas seulement les cath√©drales, comme dans l'image sur la diapositive, mais le syst√®me pour dire: "Il y a Notre Dame de Paris et les cath√©drales √† Prague." <br><br>  Lorsque nous avons r√©solu ce probl√®me, nous avons rencontr√© quelques difficult√©s. <br><br><ol><li>  Il n'y a pratiquement aucune √©tude sur ce sujet et il n'y a pas de donn√©es toutes faites dans le domaine public. <br></li><li>  Un petit nombre d'images "propres" dans le domaine public pour chaque attraction. <br></li><li>  On ne sait pas exactement ce qu'est un point de rep√®re dans les b√¢timents.  Par exemple, une maison avec des tours sur Sq.  Leo Tolstoy √† Petersburg, TripAdvisor ne consid√®re pas les attractions, mais Google le consid√®re. <br></li></ol><br>  Nous avons commenc√© par collecter une base de donn√©es, compil√© une liste de 100 villes, puis utilis√© l'API Google Places pour t√©l√©charger des donn√©es JSON pour les points d'int√©r√™t de ces villes. <br><br>  Les donn√©es ont √©t√© filtr√©es et analys√©es, et selon la liste, nous avons t√©l√©charg√© 20 images de la recherche Google pour chaque attraction.  Le nombre 20 est tir√© de consid√©rations empiriques.  En cons√©quence, nous avons obtenu une base de 2827 attractions et environ 56 000 images.  C'est sur cette base que nous avons form√© notre mod√®le.  Pour valider notre mod√®le, nous avons utilis√© deux tests. <br><br>  Test cloud - ce sont des images de nos employ√©s, √©tiquet√©es manuellement.  Il contient 200 photos dans 15 villes et 10 mille images sans attractions.  Le second est le test de recherche.  Il a √©t√© construit en utilisant la recherche Mail.ru, qui contient de 3 √† 10 images pour chaque attraction, mais, malheureusement, ce test est sale. <br><br>  Nous avons form√© les premiers mod√®les, mais ils ont montr√© de mauvais r√©sultats sur le test Cloud dans les photos de combat. <br><br>  Voici un exemple de l'image sur laquelle nous avons √©t√© form√©s, et un exemple de photographie de combat.  Le probl√®me chez les gens, c'est qu'ils sont souvent photographi√©s sur fond de vues.  Dans ces images que nous avons obtenues de la recherche, il n'y avait personne. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b1d/8a6/63b/b1d8a663bf52f5910cf35415a93c296a.png"><br><br>  Pour lutter contre cela, nous avons ajout√© une augmentation ¬´humaine¬ª pendant l'entra√Ænement.  Autrement dit, nous avons utilis√© des approches standard: rotations al√©atoires, d√©coupe al√©atoire d'une partie de l'image, etc.  Mais √©galement dans le processus d'apprentissage, nous avons ajout√© des personnes au hasard √† certaines images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12c/6c7/b54/12c6c7b54ae4d64065ee7fefa18acb72.png"><br><br>  Cette approche nous a aid√©s √† r√©soudre le probl√®me avec les gens et √† obtenir un mod√®le de qualit√© acceptable. <br><br><h2>  Mod√®les de sc√®ne de r√©glage fin </h2><br>  Comment nous avons form√© le mod√®le: il existe une base de formation, mais elle est assez petite.  Mais nous savons qu'une attraction touristique est un cas particulier de la sc√®ne.  Et nous avons un assez bon mod√®le de sc√®ne.  Nous avons d√©cid√© de la former pour les sites touristiques.  Pour ce faire, nous avons ajout√© plusieurs couches enti√®rement connect√©es et BN au-dessus du r√©seau, les avons form√©es ainsi que les trois premiers blocs r√©siduels.  Le reste du r√©seau √©tait gel√©. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5e/b4a/e80/a5eb4ae80781b95eec46821dd05d5cba.png"><br><br>  De plus, pour la formation, nous utilisons la fonction de perte centrale non standard.  Pendant la formation, Center Loss essaie de ¬´s√©parer¬ª les repr√©sentants de diff√©rentes classes en diff√©rents groupes, comme le montre l'image. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/2e8/bbc/8cc2e8bbc85f79dfb6301742d7c3d4ac.png"><br><br>  Lors de la formation, nous avons ajout√© une autre classe ¬´pas une attraction touristique¬ª.  Et la perte centrale n'a pas √©t√© appliqu√©e √† cette classe.  Sur une telle fonction de perte mixte, une formation a √©t√© dispens√©e. <br><br>  Apr√®s avoir form√© le r√©seau, nous en coupons la derni√®re couche de classification, et lorsque l'image passe √† travers le r√©seau, elle se transforme en un vecteur num√©rique appel√© int√©gration. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf2/ebc/0fd/cf2ebc0fd7dc3af7770e6dd2c616fc74.png"><br><br>  Pour continuer √† construire un syst√®me de reconnaissance historique, nous avons construit des vecteurs de r√©f√©rence pour chaque classe.  Nous avons pris chaque classe d'attractions de la multitude et parcouru les images √† travers le r√©seau.  Ils ont obtenu des plongements et ont pris leur vecteur du milieu, qui √©tait appel√© le vecteur de r√©f√©rence de classe. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e9/8ef/f32/5e98eff32e669f6c3c0b7245c25bc4ea.png"><br><br>  Pour d√©terminer les sites sur la photo, nous ex√©cutons l'image d'entr√©e via le r√©seau et son int√©gration est compar√©e au vecteur de r√©f√©rence de chaque classe.  Si le r√©sultat de la comparaison est inf√©rieur au seuil, alors nous pensons que l'image n'a pas d'attrait.  Sinon, nous prenons la classe avec la valeur de comparaison la plus √©lev√©e. <br><br><h2>  R√©sultats des tests </h2><br><ul><li>  Lors du test sur les nuages, la pr√©cision des vues √©tait de 0,616, pas des vues - 0,981 </li><li>  La pr√©cision moyenne de 0,669 a √©t√© obtenue lors du test de recherche et la compl√©tude moyenne √©tait de 0,576. </li></ul><br>  Sur Search, ils n'ont pas obtenu de tr√®s bons r√©sultats, mais cela s'explique par le fait que le premier est assez "sale" et le second a des caract√©ristiques - parmi les attractions, il y a diff√©rents jardins botaniques qui sont similaires dans toutes les villes. <br><br>  Il y avait une id√©e de reconnaissance de sc√®ne pour former d'abord le r√©seau, qui d√©terminera le masque de sc√®ne, c'est-√†-dire en supprimera les objets du premier plan, puis l'introduira dans le mod√®le lui-m√™me, qui reconna√Æt les sc√®nes d'image sans ces zones, o√π l'arri√®re-plan est obstru√©.  Mais il n'est pas tr√®s clair ce qui doit √™tre retir√© de la couche avant, quel masque est n√©cessaire. <br><br>  Ce sera quelque chose d'assez compliqu√© et intelligent, car tout le monde ne comprend pas quels objets appartiennent √† la sc√®ne et lesquels sont superflus.  Par exemple, des personnes dans un restaurant peuvent √™tre n√©cessaires.  C'est une d√©cision non triviale, nous avons essay√© de faire quelque chose de similaire, mais cela n'a pas donn√© de bons r√©sultats. <br><br>  Voici un exemple de travail dans les photographies de combat. <br><br>  Exemples de travail r√©ussi: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/ca4/413/c53ca44130bc3b9a845a7f21a681a8e0.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/774/c01/642774c0174cb07eeb52c055cb92b8d4.png"><br><br>  Mais le travail a √©chou√©: aucune vue n'a √©t√© trouv√©e.  Le principal probl√®me de notre mod√®le pour le moment n'est pas que le r√©seau confond les vues, mais qu'il ne les trouve pas sur la photo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c9/aca/7ab/6c9aca7ab564401b4c9b5cea126fc68a.png"><br><br>  √Ä l'avenir, nous pr√©voyons de collecter une base pour un nombre encore plus grand de villes, de trouver de nouvelles m√©thodes pour former le r√©seau √† cette t√¢che et de d√©terminer les possibilit√©s d'augmenter le nombre de classes sans recycler le r√©seau. <br><br><h2>  Conclusions </h2><br>  Aujourd'hui, nous: <br><br><ul><li>  Nous avons examin√© les ensembles de donn√©es disponibles pour la reconnaissance de sc√®ne; <br></li><li>  Nous avons vu que le Wide Residual Network est le meilleur mod√®le; <br></li><li>  A discut√© d'autres possibilit√©s d'am√©liorer la qualit√© de ce mod√®le; <br></li><li>  Nous avons examin√© la t√¢che de reconna√Ætre les vues, les difficult√©s qui surgissent; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nous avons d√©crit l'algorithme de collecte de la base et les m√©thodes d'enseignement du mod√®le pour reconna√Ætre les attractions. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Je peux dire que les t√¢ches sont int√©ressantes, mais peu √©tudi√©es dans la communaut√©. </font><font style="vertical-align: inherit;">Il est int√©ressant de les traiter, car vous pouvez appliquer des approches non standard qui ne sont pas appliqu√©es dans la reconnaissance habituelle d'objets.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Minute de publicit√©. </font><font style="vertical-align: inherit;">Si vous avez aim√© ce rapport de la conf√©rence SmartData, veuillez noter que </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SmartData 2018</font></font></b></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> se tiendra √† Saint-P√©tersbourg le 15 octobre, une </font><font style="vertical-align: inherit;">conf√©rence pour ceux qui sont plong√©s dans le monde de l'apprentissage automatique, de l'analyse et du traitement des donn√©es. </font><font style="vertical-align: inherit;">Le programme aura beaucoup de choses int√©ressantes, le site a d√©j√† ses premiers intervenants et rapports.</font></font></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr419501/">https://habr.com/ru/post/fr419501/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr419485/index.html">Il y a une application pour √ßa: annonce de Mobius 2018 Moscou</a></li>
<li><a href="../fr419491/index.html">Fonctionnement de STP</a></li>
<li><a href="../fr419493/index.html">Pourquoi avez-vous besoin de Splunk? Analyse des √©v√©nements de s√©curit√©</a></li>
<li><a href="../fr419495/index.html">Qui a "invent√©" la conduction osseuse, pourquoi est-elle utilis√©e et est-elle s√ªre pour l'audition</a></li>
<li><a href="../fr419497/index.html">Critique de la grande imprimante 3D Hercules Strong</a></li>
<li><a href="../fr419503/index.html">Le livre ¬´Algorithmes et structures de donn√©es. R√©cup√©ration d'informations Java ¬ª</a></li>
<li><a href="../fr419507/index.html">Pr√©sentation de l'imprimante 3D russe PICASO 3D Designer X de 3Dtool</a></li>
<li><a href="../fr419509/index.html">R√©seau neuronal artificiel photonique</a></li>
<li><a href="../fr419511/index.html">typeof (T) vs. TypeOf‚ü®T‚ü©</a></li>
<li><a href="../fr419513/index.html">Configurer la politique de s√©curit√© des mots de passe dans Zimbra</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>