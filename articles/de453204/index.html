<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👌 🤱 😪 Container, Microservices und Service Meshes 👩🏾‍🏭 🏇🏼 ⌛️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es gibt Unmengen von Artikeln im Service-Mesh im Internet, und hier ist noch einer. Hurra! Aber warum? Dann möchte ich meine Meinung zum Ausdruck brin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Container, Microservices und Service Meshes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453204/">  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unmengen von</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikeln</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im</a> Service-Mesh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im</a> Internet, und hier ist noch einer.  Hurra!  Aber warum?  Dann möchte ich meine Meinung zum Ausdruck bringen, dass es besser wäre, wenn Service-Meshes vor 10 Jahren vor dem Aufkommen von Containerplattformen wie Docker und Kubernetes auftauchen würden.  Ich behaupte nicht, dass meine Sichtweise besser oder schlechter ist als die anderer, aber da Dienstnetze recht komplexe Tiere sind, hilft die Vielzahl der Sichtweisen, sie besser zu verstehen. <br><br>  Ich werde über die dotCloud-Plattform sprechen, die auf mehr als hundert Microservices basiert und Tausende von Anwendungen in Containern unterstützt.  Ich werde die Probleme erläutern, auf die wir während der Entwicklung und des Starts gestoßen sind, und wie Service-Meshes helfen können (oder nicht). <br><a name="habracut"></a><br><h1>  Geschichte von dotCloud </h1><br>  Ich habe bereits über die Geschichte von dotCloud und die Wahl der Architektur für diese Plattform geschrieben, aber ein wenig über die Netzwerkebene gesprochen.  Wenn Sie nicht in den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel</a> über dotCloud eintauchen möchten, finden Sie hier eine kurze Zusammenfassung: Es handelt sich um eine PaaS-Plattform als Service, mit der Clients eine Vielzahl von Anwendungen (Java, PHP, Python ...) mit Unterstützung für eine Vielzahl von Datendiensten (MongoDB,) starten können. MySQL, Redis ...) und ein Workflow wie Heroku: Wenn Sie Ihren Code auf die Plattform hochladen, werden Bilder von Containern erstellt und bereitgestellt. <br><br>  Ich werde Ihnen sagen, wie der Datenverkehr auf die dotCloud-Plattform geleitet wurde.  Nicht weil es besonders cool war (obwohl das System für seine Zeit gut funktioniert hat!), Sondern vor allem, weil mit Hilfe moderner Tools ein solches Design von einem bescheidenen Team in kurzer Zeit problemlos implementiert werden kann, wenn es eine Möglichkeit benötigt, den Verkehr zwischen einer Reihe von Microservices oder zu leiten eine Reihe von Anwendungen.  So können Sie die Optionen vergleichen: Was passiert, wenn Sie alles selbst entwickeln oder das vorhandene Service-Mesh verwenden?  Standardwahl: Selber machen oder kaufen. <br><br><h1>  Verkehrsrouting für gehostete Anwendungen </h1><br>  DotCloud-Anwendungen können HTTP- und TCP-Endpunkte bereitstellen. <br><br>  <b>HTTP-Endpunkte werden</b> dynamisch zur Konfiguration des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hipache</a> Load Balancer-Clusters hinzugefügt.  Dies ähnelt dem, was Kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress-</a> Ressourcen und ein Load Balancer wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Traefik heute tun</a> . <br><br>  Clients stellen über ihre jeweiligen Domänen eine Verbindung zu HTTP-Endpunkten her, sofern der Domänenname auf dotCloud-Load-Balancer verweist.  Nichts Besonderes. <br><br>  <b>TCP-Endpunkte</b> sind einer Portnummer zugeordnet, die dann über Umgebungsvariablen an alle Container dieses Stapels übergeben wird. <br><br>  Clients können über den entsprechenden Hostnamen (z. B. gateway-X.dotcloud.com) und die Portnummer eine Verbindung zu TCP-Endpunkten herstellen. <br><br>  Dieser Hostname wird in den Servercluster "nats" (nicht mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NATS</a> verwandt) aufgelöst, der eingehende TCP-Verbindungen an den richtigen Container (oder bei Diensten mit Lastenausgleich an die richtigen Container) weiterleitet. <br><br>  Wenn Sie mit Kubernetes vertraut sind, werden Sie wahrscheinlich an <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NodePort-</a> Dienste erinnert. <br><br>  Auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dotCloud-Plattform</a> gab es kein Äquivalent zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ClusterIP-</a> Diensten: Der Einfachheit halber war der Zugriff auf die Dienste sowohl von innen als auch von außen auf der Plattform gleich. <br><br>  Alles war ganz einfach organisiert: die ersten Implementierungen der HTTP- und TCP-Routing-Netzwerke, wahrscheinlich nur ein paar hundert Zeilen Python.  Einfache (ich würde sagen naive) Algorithmen, die mit dem Wachstum der Plattform und dem Aufkommen zusätzlicher Anforderungen fertiggestellt wurden. <br><br>  Ein umfangreiches Refactoring des vorhandenen Codes war nicht erforderlich.  Insbesondere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">12-Faktor-Anwendungen</a> können die durch Umgebungsvariablen erhaltene Adresse direkt verwenden. <br><br><h1>  Wie unterscheidet sich dies von einem modernen Service-Mesh? </h1><br>  Eingeschränkte <b>Sichtbarkeit</b> .  Wir hatten im Allgemeinen keine Metriken für das TCP-Routing-Grid.  Was das HTTP-Routing betrifft, so wurden in späteren Versionen detaillierte HTTP-Metriken mit Fehlercodes und Antwortzeiten angezeigt. Moderne Service-Meshes gehen jedoch noch weiter und ermöglichen die Integration in Metrik-Erfassungssysteme wie beispielsweise Prometheus. <br><br>  Die Sichtbarkeit ist nicht nur aus betrieblicher Sicht wichtig (um Probleme zu beheben), sondern auch, wenn neue Funktionen veröffentlicht werden.  Es geht um eine sichere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">blaugrüne Bereitstellung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bereitstellung von Kanaren</a> . <br><br>  <b>Die Routing-Effizienz ist</b> ebenfalls begrenzt.  Im dotCloud-Routing-Grid musste der gesamte Datenverkehr durch einen Cluster dedizierter Routing-Knoten geleitet werden.  Dies bedeutete ein mögliches Überschreiten mehrerer AZ-Grenzen (Zugänglichkeitszonen) und eine signifikante Zunahme der Verzögerung.  Ich erinnere mich, wie ich Probleme mit Code behoben habe, der mehr als hundert SQL-Abfragen pro Seite durchgeführt und für jede Abfrage eine neue Verbindung zum SQL-Server hergestellt hat.  Beim lokalen Start wird die Seite sofort geladen, in dotCloud dauert das Laden jedoch einige Sekunden, da für jede TCP-Verbindung (und nachfolgende SQL-Abfrage) mehrere zehn Millisekunden benötigt werden.  In diesem speziellen Fall lösten dauerhafte Verbindungen das Problem. <br><br>  Moderne Service-Meshes können mit solchen Problemen besser umgehen.  Zunächst überprüfen sie, ob die Verbindungen <i>an der Quelle weitergeleitet werden</i> .  Der logische Ablauf ist der gleiche: <code> →  → </code> , aber jetzt funktioniert das Mesh lokal und nicht auf Remote-Knoten, sodass die <code> → </code> Verbindung lokal und sehr schnell ist (Mikrosekunden statt Millisekunden). <br><br>  Moderne Service-Meshes implementieren auch intelligentere Lastausgleichsalgorithmen.  Durch die Steuerung der Leistung von Backends können sie mehr Datenverkehr an schnellere Backends senden, was zu einer Steigerung der Gesamtleistung führt. <br><br>  <b>Sicherheit</b> ist auch besser.  Das dotCloud-Routing-Grid funktionierte vollständig auf EC2 Classic und verschlüsselte den Datenverkehr nicht (vorausgesetzt, wenn jemand es geschafft hat, den EC2-Netzwerkverkehr zu überwachen, haben Sie bereits große Probleme).  Moderne Service-Meshes schützen unseren gesamten Datenverkehr transparent, beispielsweise durch gegenseitige TLS-Authentifizierung und anschließende Verschlüsselung. <br><br><h1>  Verkehrsrouting für Plattformdienste </h1><br>  Ok, wir haben den Datenverkehr zwischen Anwendungen besprochen, aber was ist mit der dotCloud-Plattform selbst? <br><br>  Die Plattform selbst bestand aus etwa hundert Mikrodiensten, die für verschiedene Funktionen verantwortlich waren.  Einige erhielten Anfragen von anderen, und einige waren Hintergrundarbeiter, die sich mit anderen Diensten verbanden, aber keine Verbindungen akzeptierten.  In jedem Fall muss jeder Dienst die Endpunkte der Adressen kennen, zu denen eine Verbindung hergestellt werden muss. <br><br>  Viele High-Level-Dienste können das oben beschriebene Routing-Grid verwenden.  Tatsächlich wurden viele der mehr als Hunderte von dotCloud-Mikrodiensten als reguläre Anwendungen auf der dotCloud-Plattform selbst bereitgestellt.  Eine kleine Anzahl von Diensten auf niedriger Ebene (insbesondere, die dieses Routing-Raster implementieren) benötigte jedoch etwas Einfacheres mit weniger Abhängigkeiten (da sie sich bei der Arbeit nicht auf sich selbst verlassen konnten - ein gutes altes Henne-Ei-Problem). <br><br>  Diese wichtigen Dienste auf niedriger Ebene wurden bereitgestellt, indem Container direkt auf mehreren Schlüsselknoten ausgeführt wurden.  Gleichzeitig waren Standardplattformdienste nicht beteiligt: ​​Linker, Scheduler und Runner.  Wenn Sie mit modernen Containerplattformen vergleichen möchten, ist dies wie das Starten einer Steuerebene mit <code>docker run</code> direkt auf den Knoten ausgeführt wird, anstatt die Kubernetes-Aufgabe zu delegieren.  Dies ist dem Konzept der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">statischen Module (Herde)</a> ziemlich ähnlich, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubeadm</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bootkube</a> beim Laden eines eigenständigen Clusters verwenden. <br><br>  Diese Dienste wurden auf einfache und grobe Weise verfügbar gemacht: Ihre Namen und Adressen wurden in der YAML-Datei aufgeführt;  und jeder Client musste eine Kopie dieser YAML-Datei für die Bereitstellung erstellen. <br><br>  Einerseits ist es äußerst zuverlässig, da kein externer Schlüssel- / Wertspeicher wie Zookeeper unterstützt werden muss (vergessen Sie nicht, dass zu diesem Zeitpunkt etcd oder Consul noch nicht vorhanden waren).  Auf der anderen Seite war es schwierig, Dienste zu verschieben.  Bei jedem Umzug sollten alle Clients eine aktualisierte YAML-Datei erhalten haben (und möglicherweise neu starten).  Nicht sehr praktisch! <br><br>  Anschließend haben wir ein neues Schema eingeführt, bei dem jeder Client mit einem lokalen Proxyserver verbunden ist.  Anstelle der Adresse und des Ports reicht es aus, nur die Portnummer des Dienstes zu kennen und eine Verbindung über <code>localhost</code> .  Der lokale Proxyserver verarbeitet diese Verbindung und leitet sie an den eigentlichen Server weiter.  Wenn Sie das Backend auf einen anderen Computer verschieben oder skalieren, anstatt alle Clients zu aktualisieren, müssen Sie nur alle diese lokalen Proxys aktualisieren.  Ein Neustart ist nicht mehr erforderlich. <br><br>  (Es war auch geplant, den Datenverkehr in TLS-Verbindungen zu kapseln und einen anderen Proxyserver auf die Empfangsseite zu stellen sowie TLS-Zertifikate ohne Teilnahme des Empfangsdienstes zu überprüfen, der so konfiguriert ist, dass Verbindungen nur auf <code>localhost</code> akzeptiert werden. Mehr dazu später). <br><br>  Dies ist dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SmartStack</a> von Airbnb sehr ähnlich, aber der wesentliche Unterschied besteht darin, dass SmartStack in der Produktion implementiert und bereitgestellt wird, während das interne dotCloud-Routing-System in einer Box untergebracht wurde, als dotCloud zu Docker wurde. <br><br>  Ich persönlich betrachte SmartStack als einen der Vorgänger von Systemen wie Istio, Linkerd und Consul Connect, da alle dem gleichen Muster folgen: <br><br><ul><li>  Ausführen von Proxys auf jedem Knoten. <br></li><li>  Clients stellen eine Verbindung zum Proxy her. <br></li><li>  Die Verwaltungsebene aktualisiert die Proxy-Konfiguration beim Ändern von Backends. <br></li><li>  ... Gewinn! </li></ul><br><h1>  Moderne Implementierung eines Service-Mesh </h1><br>  Wenn wir heute ein ähnliches Raster implementieren müssen, können wir ähnliche Prinzipien anwenden.  Konfigurieren Sie beispielsweise die interne DNS-Zone, indem Sie Dienstnamen Adressen in <code>127.0.0.0/8</code> .  Führen Sie dann HAProxy auf jedem Knoten des Clusters aus, akzeptieren Sie Verbindungen zu jeder Dienstadresse ( <code>127.0.0.0/8</code> in diesem Subnetz) und leiten Sie die Last zu den entsprechenden Backends um.  Die HAProxy-Konfiguration kann über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">confd</a> gesteuert werden, sodass Sie Backend-Informationen in etcd oder Consul speichern und die aktualisierte Konfiguration bei Bedarf automatisch an HAProxy senden können. <br><br>  So funktioniert Istio!  Aber mit einigen Unterschieden: <br><br><ul><li>  Verwendet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Envoy Proxy</a> anstelle von HAProxy. <br></li><li>  Speichert die Backend-Konfiguration über die Kubernetes-API anstelle von etcd oder Consul. <br></li><li>  Den Diensten werden Adressen im internen Subnetz (Kubernetes ClusterIP-Adressen) anstelle von 127.0.0.0/8 zugewiesen. <br></li><li>  Es verfügt über eine optionale Komponente (Citadel) zum Hinzufügen einer gegenseitigen TLS-Authentifizierung zwischen Client und Servern. <br></li><li>  Unterstützt neue Funktionen wie Unterbrechung, verteilte Verfolgung, Bereitstellung von Kanarienvögeln usw. </li></ul><br>  Lassen Sie uns einen kurzen Blick auf einige der Unterschiede werfen. <br><br><h3>  Stellvertreter des Gesandten </h3><br>  Enftoy Proxy wurde von Lyft [Uber Konkurrent auf dem Taximarkt geschrieben - ca.  trans.].  Es ist anderen Proxys in vielerlei Hinsicht sehr ähnlich (zum Beispiel HAProxy, Nginx, Traefik ...), aber Lyft hat ihre eigenen geschrieben, weil sie Funktionen benötigten, die nicht in anderen Proxys enthalten sind, und es schien vernünftiger, eine neue zu erstellen, als die vorhandene zu erweitern. <br><br>  Der Gesandte kann alleine eingesetzt werden.  Wenn ich einen bestimmten Dienst habe, der eine Verbindung zu anderen Diensten herstellen soll, kann ich ihn für die Verbindung mit Envoy konfigurieren und dann Envoy dynamisch mit dem Standort anderer Dienste konfigurieren und neu konfigurieren, während ich viele hervorragende zusätzliche Funktionen erhalte, z. B. Sichtbarkeit.  Anstelle einer benutzerdefinierten Clientbibliothek oder der Einbettung der Anrufverfolgung in den Code leiten wir den Datenverkehr an Envoy weiter und sammeln Metriken für uns. <br><br>  Envoy kann aber auch als Datenebene für ein Service-Mesh arbeiten.  Dies bedeutet, dass Envoy für dieses Servicenetz jetzt <i>von der</i> Steuerebene konfiguriert <i>wird</i> . <br><br><h3>  Steuerebene </h3><br>  In der Verwaltungsebene verlässt sich Istio auf die Kubernetes-API.  <i>Dies unterscheidet sich nicht wesentlich von der Verwendung von confd</i> , bei der etcd oder Consul zum Anzeigen eines Schlüsselsatzes in einem Data Warehouse verwendet werden.  Istio zeigt über die Kubernetes-API den Kubernetes-Ressourcensatz an. <br><br>  <i>Zwischen dem Fall</i> : Ich persönlich fand diese <a href="">Beschreibung der Kubernetes-API nützlich</a> , die lautet: <br><br><blockquote>  Der Kubernetes API Server ist ein „dummer Server“, der Speicherung, Versionierung, Validierung, Aktualisierung und Semantik von API-Ressourcen bietet. </blockquote><br>  Istio wurde für die Zusammenarbeit mit Kubernetes entwickelt.  Wenn Sie es außerhalb von Kubernetes verwenden möchten, müssen Sie eine Instanz des Kubernetes-API-Servers (und des Hilfsdienstes usw.) ausführen. <br><br><h3>  Serviceadressen </h3><br>  Istio stützt sich auf die von Kubernetes zugewiesenen ClusterIP-Adressen, sodass Istio-Dienste eine interne Adresse erhalten (nicht im Bereich <code>127.0.0.0/8</code> ). <br><br>  Der Datenverkehr zur ClusterIP-Adresse für einen bestimmten Dienst im Kubernetes-Cluster ohne Istio wird vom kube-Proxy abgefangen und an den Serverteil dieses Proxys gesendet.  Wenn Sie an technischen Details interessiert sind, legt kube-proxy die iptables-Regeln (oder IPVS-Load-Balancer, je nachdem, wie Sie sie konfigurieren) fest, um die Ziel-IP-Adressen der Verbindungen zur ClusterIP-Adresse neu zu schreiben. <br><br>  Nach der Installation von Istio im Kubernetes-Cluster ändert sich nichts, bis es für den angegebenen Verbraucher oder sogar den gesamten Namespace explizit aktiviert wird, indem der <code>sidecar</code> in benutzerdefinierte Herde eingeführt wird.  Dieser Container startet eine Instanz von Envoy und legt eine Reihe von iptables-Regeln fest, um den Verkehr zu anderen Diensten abzufangen und diesen Verkehr zu Envoy umzuleiten. <br><br>  Bei der Integration in Kubernetes DNS bedeutet dies, dass unser Code über den Namen des Dienstes eine Verbindung herstellen kann und alles "einfach funktioniert".  Mit anderen Worten, unser Code gibt Anforderungen wie <code>http://api/v1/users/4242</code> , dann löst <code>api</code> die Anforderung in <code>10.97.105.48</code> , iptables-Regeln fangen Verbindungen von 10.97.105.48 ab und leiten sie an den lokalen Envoy-Proxy weiter, und dieser lokale Proxy leitet sie weiter Anfrage für die eigentliche Backend-API.  Fuh! <br><br><h3>  Extra kleine Dinger </h3><br>  Istio bietet auch End-to-End-Verschlüsselung und Authentifizierung über mTLS (gegenseitiges TLS).  Verantwortlich dafür ist die Komponente <i>Citadel</i> . <br><br>  Es gibt auch eine <i>Mixer-</i> Komponente, die Envoy für <i>jede</i> Anfrage anfordern kann, um eine spezielle Entscheidung über diese Anfrage zu treffen, abhängig von verschiedenen Faktoren wie Headern, Laden des Backends usw. (keine Sorge: Es gibt viele Möglichkeiten, um sicherzustellen, dass der Mixer funktioniert und sogar Wenn es abstürzt, arbeitet Envoy weiterhin normal als Proxy. <br><br>  Und natürlich haben wir die Sichtbarkeit erwähnt: Envoy sammelt eine große Anzahl von Metriken und bietet verteilte Ablaufverfolgung.  Wenn in der Architektur von Microservices eine API-Anforderung die Microservices A, B, C und D durchlaufen muss, fügt der verteilte Trace bei der Anmeldung am System der Anforderung eine eindeutige Kennung hinzu und speichert diese Kennung über Unterabfragen an alle diese Mikrodienste, sodass Sie alle zugehörigen Aufrufe aufzeichnen können Verzögerungen usw. <br><br><h1>  Entwickeln oder kaufen </h1><br>  Istio hat den Ruf, ein komplexes System zu sein.  Im Gegensatz dazu ist das Erstellen eines Routing-Rasters, das ich zu Beginn dieses Beitrags beschrieben habe, mit vorhandenen Tools relativ einfach.  Ist es also sinnvoll, stattdessen ein eigenes Service-Mesh zu erstellen? <br><br>  Wenn wir bescheidene Bedürfnisse haben (Sie brauchen keine Sichtbarkeit, einen Leistungsschalter und andere Feinheiten), dann denken Sie darüber nach, Ihr eigenes Werkzeug zu entwickeln.  Wenn wir jedoch Kubernetes verwenden, ist dies möglicherweise nicht einmal erforderlich, da Kubernetes bereits grundlegende Tools für die Serviceerkennung und den Lastenausgleich bereitstellt. <br><br>  Wenn wir jedoch erweiterte Anforderungen haben, scheint der „Kauf“ eines Service-Netzes eine viel bessere Option zu sein.  (Dies ist nicht immer ein „Kauf“, da Istio mit Open Source-Code geliefert wird. Wir müssen jedoch noch Engineering-Zeit investieren, um seine Arbeit zu verstehen, ihn bereitzustellen und zu verwalten.) <br><br><h1>  Was zur Auswahl: Istio, Linkerd oder Consul Connect? </h1><br>  Bisher haben wir nur über Istio gesprochen, aber dies ist nicht das einzige Service-Mesh.  Eine beliebte Alternative ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Linkerd</a> , und es gibt auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Consul Connect</a> . <br><br>  Was soll ich wählen? <br><br>  Ehrlich gesagt, ich weiß es nicht.  Im Moment halte ich mich nicht für kompetent genug, um diese Frage zu beantworten.  Es gibt einige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interessante</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel, in</a> denen diese Tools und sogar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benchmarks</a> verglichen werden. <br><br>  Ein vielversprechender Ansatz ist die Verwendung eines Tools wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SuperGloo</a> .  Es implementiert eine Abstraktionsschicht, um die von Service-Meshes bereitgestellten APIs zu vereinfachen und zu vereinheitlichen.  Anstatt bestimmte (und meiner Meinung nach relativ komplexe) APIs verschiedener Service-Meshes zu untersuchen, können wir einfachere SuperGloo-Konstruktionen verwenden - und einfach von einer zur anderen wechseln, als hätten wir ein Zwischenkonfigurationsformat, das HTTP-Schnittstellen und beschreibt Backends, die die eigentliche Konfiguration für Nginx, HAProxy, Traefik, Apache ... generieren können. <br><br>  Ich habe mich ein wenig mit Istio und SuperGloo beschäftigt, und im nächsten Artikel möchte ich zeigen, wie man Istio oder Linkerd mit SuperGloo zu einem vorhandenen Cluster hinzufügt und wie viel dieser mit seiner Arbeit fertig wird, dh Sie können von einem Service-Mesh zu einem anderen wechseln, ohne die Konfigurationen neu zu schreiben. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de453204/">https://habr.com/ru/post/de453204/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de453190/index.html">ANPR mit RoR & React Native</a></li>
<li><a href="../de453192/index.html">Synchronisation und Asynchronität von Prozessen</a></li>
<li><a href="../de453194/index.html">Wir lösen das Best Reverser-Problem mit PHDays 9</a></li>
<li><a href="../de453196/index.html">Forrester Research: Ein Vergleich von zehn führenden Anbietern von Software-Zusammensetzungsanalysen</a></li>
<li><a href="../de453200/index.html">Diskussion: Das OpenROAD-Projekt soll die Aufgabe lösen, das Design von Prozessoren zu automatisieren</a></li>
<li><a href="../de453206/index.html">Interview mit Kelsey Moody: Wie man ein Unternehmen aufbaut und altersbedingte Pathologien beendet</a></li>
<li><a href="../de453212/index.html">Verbraucherberichte: Der neueste Autopilot von Tesla ist alles andere als perfekt</a></li>
<li><a href="../de453214/index.html">Wie und warum Sie sich fit halten, wenn Sie eine IT-Person an einem Remotestandort sind</a></li>
<li><a href="../de453216/index.html">Verkehrsüberwachungssysteme in VoIP-Netzen. Zweiter Teil - Organisationsprinzipien</a></li>
<li><a href="../de453218/index.html">Die Hauptsache bei YaC 2019: Hundert Drohnen auf den Straßen, Yandex.Module, Essen, Smart Home</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>