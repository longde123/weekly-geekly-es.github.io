<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëå ü§± üò™ Container, Microservices und Service Meshes üë©üèæ‚Äçüè≠ üèáüèº ‚åõÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Es gibt Unmengen von Artikeln im Service-Mesh im Internet, und hier ist noch einer. Hurra! Aber warum? Dann m√∂chte ich meine Meinung zum Ausdruck brin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Container, Microservices und Service Meshes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453204/">  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unmengen von</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikeln</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im</a> Service-Mesh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im</a> Internet, und hier ist noch einer.  Hurra!  Aber warum?  Dann m√∂chte ich meine Meinung zum Ausdruck bringen, dass es besser w√§re, wenn Service-Meshes vor 10 Jahren vor dem Aufkommen von Containerplattformen wie Docker und Kubernetes auftauchen w√ºrden.  Ich behaupte nicht, dass meine Sichtweise besser oder schlechter ist als die anderer, aber da Dienstnetze recht komplexe Tiere sind, hilft die Vielzahl der Sichtweisen, sie besser zu verstehen. <br><br>  Ich werde √ºber die dotCloud-Plattform sprechen, die auf mehr als hundert Microservices basiert und Tausende von Anwendungen in Containern unterst√ºtzt.  Ich werde die Probleme erl√§utern, auf die wir w√§hrend der Entwicklung und des Starts gesto√üen sind, und wie Service-Meshes helfen k√∂nnen (oder nicht). <br><a name="habracut"></a><br><h1>  Geschichte von dotCloud </h1><br>  Ich habe bereits √ºber die Geschichte von dotCloud und die Wahl der Architektur f√ºr diese Plattform geschrieben, aber ein wenig √ºber die Netzwerkebene gesprochen.  Wenn Sie nicht in den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel</a> √ºber dotCloud eintauchen m√∂chten, finden Sie hier eine kurze Zusammenfassung: Es handelt sich um eine PaaS-Plattform als Service, mit der Clients eine Vielzahl von Anwendungen (Java, PHP, Python ...) mit Unterst√ºtzung f√ºr eine Vielzahl von Datendiensten (MongoDB,) starten k√∂nnen. MySQL, Redis ...) und ein Workflow wie Heroku: Wenn Sie Ihren Code auf die Plattform hochladen, werden Bilder von Containern erstellt und bereitgestellt. <br><br>  Ich werde Ihnen sagen, wie der Datenverkehr auf die dotCloud-Plattform geleitet wurde.  Nicht weil es besonders cool war (obwohl das System f√ºr seine Zeit gut funktioniert hat!), Sondern vor allem, weil mit Hilfe moderner Tools ein solches Design von einem bescheidenen Team in kurzer Zeit problemlos implementiert werden kann, wenn es eine M√∂glichkeit ben√∂tigt, den Verkehr zwischen einer Reihe von Microservices oder zu leiten eine Reihe von Anwendungen.  So k√∂nnen Sie die Optionen vergleichen: Was passiert, wenn Sie alles selbst entwickeln oder das vorhandene Service-Mesh verwenden?  Standardwahl: Selber machen oder kaufen. <br><br><h1>  Verkehrsrouting f√ºr gehostete Anwendungen </h1><br>  DotCloud-Anwendungen k√∂nnen HTTP- und TCP-Endpunkte bereitstellen. <br><br>  <b>HTTP-Endpunkte werden</b> dynamisch zur Konfiguration des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hipache</a> Load Balancer-Clusters hinzugef√ºgt.  Dies √§hnelt dem, was Kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress-</a> Ressourcen und ein Load Balancer wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Traefik heute tun</a> . <br><br>  Clients stellen √ºber ihre jeweiligen Dom√§nen eine Verbindung zu HTTP-Endpunkten her, sofern der Dom√§nenname auf dotCloud-Load-Balancer verweist.  Nichts Besonderes. <br><br>  <b>TCP-Endpunkte</b> sind einer Portnummer zugeordnet, die dann √ºber Umgebungsvariablen an alle Container dieses Stapels √ºbergeben wird. <br><br>  Clients k√∂nnen √ºber den entsprechenden Hostnamen (z. B. gateway-X.dotcloud.com) und die Portnummer eine Verbindung zu TCP-Endpunkten herstellen. <br><br>  Dieser Hostname wird in den Servercluster "nats" (nicht mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NATS</a> verwandt) aufgel√∂st, der eingehende TCP-Verbindungen an den richtigen Container (oder bei Diensten mit Lastenausgleich an die richtigen Container) weiterleitet. <br><br>  Wenn Sie mit Kubernetes vertraut sind, werden Sie wahrscheinlich an <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NodePort-</a> Dienste erinnert. <br><br>  Auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dotCloud-Plattform</a> gab es kein √Ñquivalent zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ClusterIP-</a> Diensten: Der Einfachheit halber war der Zugriff auf die Dienste sowohl von innen als auch von au√üen auf der Plattform gleich. <br><br>  Alles war ganz einfach organisiert: die ersten Implementierungen der HTTP- und TCP-Routing-Netzwerke, wahrscheinlich nur ein paar hundert Zeilen Python.  Einfache (ich w√ºrde sagen naive) Algorithmen, die mit dem Wachstum der Plattform und dem Aufkommen zus√§tzlicher Anforderungen fertiggestellt wurden. <br><br>  Ein umfangreiches Refactoring des vorhandenen Codes war nicht erforderlich.  Insbesondere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">12-Faktor-Anwendungen</a> k√∂nnen die durch Umgebungsvariablen erhaltene Adresse direkt verwenden. <br><br><h1>  Wie unterscheidet sich dies von einem modernen Service-Mesh? </h1><br>  Eingeschr√§nkte <b>Sichtbarkeit</b> .  Wir hatten im Allgemeinen keine Metriken f√ºr das TCP-Routing-Grid.  Was das HTTP-Routing betrifft, so wurden in sp√§teren Versionen detaillierte HTTP-Metriken mit Fehlercodes und Antwortzeiten angezeigt. Moderne Service-Meshes gehen jedoch noch weiter und erm√∂glichen die Integration in Metrik-Erfassungssysteme wie beispielsweise Prometheus. <br><br>  Die Sichtbarkeit ist nicht nur aus betrieblicher Sicht wichtig (um Probleme zu beheben), sondern auch, wenn neue Funktionen ver√∂ffentlicht werden.  Es geht um eine sichere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">blaugr√ºne Bereitstellung</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bereitstellung von Kanaren</a> . <br><br>  <b>Die Routing-Effizienz ist</b> ebenfalls begrenzt.  Im dotCloud-Routing-Grid musste der gesamte Datenverkehr durch einen Cluster dedizierter Routing-Knoten geleitet werden.  Dies bedeutete ein m√∂gliches √úberschreiten mehrerer AZ-Grenzen (Zug√§nglichkeitszonen) und eine signifikante Zunahme der Verz√∂gerung.  Ich erinnere mich, wie ich Probleme mit Code behoben habe, der mehr als hundert SQL-Abfragen pro Seite durchgef√ºhrt und f√ºr jede Abfrage eine neue Verbindung zum SQL-Server hergestellt hat.  Beim lokalen Start wird die Seite sofort geladen, in dotCloud dauert das Laden jedoch einige Sekunden, da f√ºr jede TCP-Verbindung (und nachfolgende SQL-Abfrage) mehrere zehn Millisekunden ben√∂tigt werden.  In diesem speziellen Fall l√∂sten dauerhafte Verbindungen das Problem. <br><br>  Moderne Service-Meshes k√∂nnen mit solchen Problemen besser umgehen.  Zun√§chst √ºberpr√ºfen sie, ob die Verbindungen <i>an der Quelle weitergeleitet werden</i> .  Der logische Ablauf ist der gleiche: <code> ‚Üí  ‚Üí </code> , aber jetzt funktioniert das Mesh lokal und nicht auf Remote-Knoten, sodass die <code> ‚Üí </code> Verbindung lokal und sehr schnell ist (Mikrosekunden statt Millisekunden). <br><br>  Moderne Service-Meshes implementieren auch intelligentere Lastausgleichsalgorithmen.  Durch die Steuerung der Leistung von Backends k√∂nnen sie mehr Datenverkehr an schnellere Backends senden, was zu einer Steigerung der Gesamtleistung f√ºhrt. <br><br>  <b>Sicherheit</b> ist auch besser.  Das dotCloud-Routing-Grid funktionierte vollst√§ndig auf EC2 Classic und verschl√ºsselte den Datenverkehr nicht (vorausgesetzt, wenn jemand es geschafft hat, den EC2-Netzwerkverkehr zu √ºberwachen, haben Sie bereits gro√üe Probleme).  Moderne Service-Meshes sch√ºtzen unseren gesamten Datenverkehr transparent, beispielsweise durch gegenseitige TLS-Authentifizierung und anschlie√üende Verschl√ºsselung. <br><br><h1>  Verkehrsrouting f√ºr Plattformdienste </h1><br>  Ok, wir haben den Datenverkehr zwischen Anwendungen besprochen, aber was ist mit der dotCloud-Plattform selbst? <br><br>  Die Plattform selbst bestand aus etwa hundert Mikrodiensten, die f√ºr verschiedene Funktionen verantwortlich waren.  Einige erhielten Anfragen von anderen, und einige waren Hintergrundarbeiter, die sich mit anderen Diensten verbanden, aber keine Verbindungen akzeptierten.  In jedem Fall muss jeder Dienst die Endpunkte der Adressen kennen, zu denen eine Verbindung hergestellt werden muss. <br><br>  Viele High-Level-Dienste k√∂nnen das oben beschriebene Routing-Grid verwenden.  Tats√§chlich wurden viele der mehr als Hunderte von dotCloud-Mikrodiensten als regul√§re Anwendungen auf der dotCloud-Plattform selbst bereitgestellt.  Eine kleine Anzahl von Diensten auf niedriger Ebene (insbesondere, die dieses Routing-Raster implementieren) ben√∂tigte jedoch etwas Einfacheres mit weniger Abh√§ngigkeiten (da sie sich bei der Arbeit nicht auf sich selbst verlassen konnten - ein gutes altes Henne-Ei-Problem). <br><br>  Diese wichtigen Dienste auf niedriger Ebene wurden bereitgestellt, indem Container direkt auf mehreren Schl√ºsselknoten ausgef√ºhrt wurden.  Gleichzeitig waren Standardplattformdienste nicht beteiligt: ‚Äã‚ÄãLinker, Scheduler und Runner.  Wenn Sie mit modernen Containerplattformen vergleichen m√∂chten, ist dies wie das Starten einer Steuerebene mit <code>docker run</code> direkt auf den Knoten ausgef√ºhrt wird, anstatt die Kubernetes-Aufgabe zu delegieren.  Dies ist dem Konzept der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">statischen Module (Herde)</a> ziemlich √§hnlich, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubeadm</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bootkube</a> beim Laden eines eigenst√§ndigen Clusters verwenden. <br><br>  Diese Dienste wurden auf einfache und grobe Weise verf√ºgbar gemacht: Ihre Namen und Adressen wurden in der YAML-Datei aufgef√ºhrt;  und jeder Client musste eine Kopie dieser YAML-Datei f√ºr die Bereitstellung erstellen. <br><br>  Einerseits ist es √§u√üerst zuverl√§ssig, da kein externer Schl√ºssel- / Wertspeicher wie Zookeeper unterst√ºtzt werden muss (vergessen Sie nicht, dass zu diesem Zeitpunkt etcd oder Consul noch nicht vorhanden waren).  Auf der anderen Seite war es schwierig, Dienste zu verschieben.  Bei jedem Umzug sollten alle Clients eine aktualisierte YAML-Datei erhalten haben (und m√∂glicherweise neu starten).  Nicht sehr praktisch! <br><br>  Anschlie√üend haben wir ein neues Schema eingef√ºhrt, bei dem jeder Client mit einem lokalen Proxyserver verbunden ist.  Anstelle der Adresse und des Ports reicht es aus, nur die Portnummer des Dienstes zu kennen und eine Verbindung √ºber <code>localhost</code> .  Der lokale Proxyserver verarbeitet diese Verbindung und leitet sie an den eigentlichen Server weiter.  Wenn Sie das Backend auf einen anderen Computer verschieben oder skalieren, anstatt alle Clients zu aktualisieren, m√ºssen Sie nur alle diese lokalen Proxys aktualisieren.  Ein Neustart ist nicht mehr erforderlich. <br><br>  (Es war auch geplant, den Datenverkehr in TLS-Verbindungen zu kapseln und einen anderen Proxyserver auf die Empfangsseite zu stellen sowie TLS-Zertifikate ohne Teilnahme des Empfangsdienstes zu √ºberpr√ºfen, der so konfiguriert ist, dass Verbindungen nur auf <code>localhost</code> akzeptiert werden. Mehr dazu sp√§ter). <br><br>  Dies ist dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SmartStack</a> von Airbnb sehr √§hnlich, aber der wesentliche Unterschied besteht darin, dass SmartStack in der Produktion implementiert und bereitgestellt wird, w√§hrend das interne dotCloud-Routing-System in einer Box untergebracht wurde, als dotCloud zu Docker wurde. <br><br>  Ich pers√∂nlich betrachte SmartStack als einen der Vorg√§nger von Systemen wie Istio, Linkerd und Consul Connect, da alle dem gleichen Muster folgen: <br><br><ul><li>  Ausf√ºhren von Proxys auf jedem Knoten. <br></li><li>  Clients stellen eine Verbindung zum Proxy her. <br></li><li>  Die Verwaltungsebene aktualisiert die Proxy-Konfiguration beim √Ñndern von Backends. <br></li><li>  ... Gewinn! </li></ul><br><h1>  Moderne Implementierung eines Service-Mesh </h1><br>  Wenn wir heute ein √§hnliches Raster implementieren m√ºssen, k√∂nnen wir √§hnliche Prinzipien anwenden.  Konfigurieren Sie beispielsweise die interne DNS-Zone, indem Sie Dienstnamen Adressen in <code>127.0.0.0/8</code> .  F√ºhren Sie dann HAProxy auf jedem Knoten des Clusters aus, akzeptieren Sie Verbindungen zu jeder Dienstadresse ( <code>127.0.0.0/8</code> in diesem Subnetz) und leiten Sie die Last zu den entsprechenden Backends um.  Die HAProxy-Konfiguration kann √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">confd</a> gesteuert werden, sodass Sie Backend-Informationen in etcd oder Consul speichern und die aktualisierte Konfiguration bei Bedarf automatisch an HAProxy senden k√∂nnen. <br><br>  So funktioniert Istio!  Aber mit einigen Unterschieden: <br><br><ul><li>  Verwendet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Envoy Proxy</a> anstelle von HAProxy. <br></li><li>  Speichert die Backend-Konfiguration √ºber die Kubernetes-API anstelle von etcd oder Consul. <br></li><li>  Den Diensten werden Adressen im internen Subnetz (Kubernetes ClusterIP-Adressen) anstelle von 127.0.0.0/8 zugewiesen. <br></li><li>  Es verf√ºgt √ºber eine optionale Komponente (Citadel) zum Hinzuf√ºgen einer gegenseitigen TLS-Authentifizierung zwischen Client und Servern. <br></li><li>  Unterst√ºtzt neue Funktionen wie Unterbrechung, verteilte Verfolgung, Bereitstellung von Kanarienv√∂geln usw. </li></ul><br>  Lassen Sie uns einen kurzen Blick auf einige der Unterschiede werfen. <br><br><h3>  Stellvertreter des Gesandten </h3><br>  Enftoy Proxy wurde von Lyft [Uber Konkurrent auf dem Taximarkt geschrieben - ca.  trans.].  Es ist anderen Proxys in vielerlei Hinsicht sehr √§hnlich (zum Beispiel HAProxy, Nginx, Traefik ...), aber Lyft hat ihre eigenen geschrieben, weil sie Funktionen ben√∂tigten, die nicht in anderen Proxys enthalten sind, und es schien vern√ºnftiger, eine neue zu erstellen, als die vorhandene zu erweitern. <br><br>  Der Gesandte kann alleine eingesetzt werden.  Wenn ich einen bestimmten Dienst habe, der eine Verbindung zu anderen Diensten herstellen soll, kann ich ihn f√ºr die Verbindung mit Envoy konfigurieren und dann Envoy dynamisch mit dem Standort anderer Dienste konfigurieren und neu konfigurieren, w√§hrend ich viele hervorragende zus√§tzliche Funktionen erhalte, z. B. Sichtbarkeit.  Anstelle einer benutzerdefinierten Clientbibliothek oder der Einbettung der Anrufverfolgung in den Code leiten wir den Datenverkehr an Envoy weiter und sammeln Metriken f√ºr uns. <br><br>  Envoy kann aber auch als Datenebene f√ºr ein Service-Mesh arbeiten.  Dies bedeutet, dass Envoy f√ºr dieses Servicenetz jetzt <i>von der</i> Steuerebene konfiguriert <i>wird</i> . <br><br><h3>  Steuerebene </h3><br>  In der Verwaltungsebene verl√§sst sich Istio auf die Kubernetes-API.  <i>Dies unterscheidet sich nicht wesentlich von der Verwendung von confd</i> , bei der etcd oder Consul zum Anzeigen eines Schl√ºsselsatzes in einem Data Warehouse verwendet werden.  Istio zeigt √ºber die Kubernetes-API den Kubernetes-Ressourcensatz an. <br><br>  <i>Zwischen dem Fall</i> : Ich pers√∂nlich fand diese <a href="">Beschreibung der Kubernetes-API n√ºtzlich</a> , die lautet: <br><br><blockquote>  Der Kubernetes API Server ist ein ‚Äûdummer Server‚Äú, der Speicherung, Versionierung, Validierung, Aktualisierung und Semantik von API-Ressourcen bietet. </blockquote><br>  Istio wurde f√ºr die Zusammenarbeit mit Kubernetes entwickelt.  Wenn Sie es au√üerhalb von Kubernetes verwenden m√∂chten, m√ºssen Sie eine Instanz des Kubernetes-API-Servers (und des Hilfsdienstes usw.) ausf√ºhren. <br><br><h3>  Serviceadressen </h3><br>  Istio st√ºtzt sich auf die von Kubernetes zugewiesenen ClusterIP-Adressen, sodass Istio-Dienste eine interne Adresse erhalten (nicht im Bereich <code>127.0.0.0/8</code> ). <br><br>  Der Datenverkehr zur ClusterIP-Adresse f√ºr einen bestimmten Dienst im Kubernetes-Cluster ohne Istio wird vom kube-Proxy abgefangen und an den Serverteil dieses Proxys gesendet.  Wenn Sie an technischen Details interessiert sind, legt kube-proxy die iptables-Regeln (oder IPVS-Load-Balancer, je nachdem, wie Sie sie konfigurieren) fest, um die Ziel-IP-Adressen der Verbindungen zur ClusterIP-Adresse neu zu schreiben. <br><br>  Nach der Installation von Istio im Kubernetes-Cluster √§ndert sich nichts, bis es f√ºr den angegebenen Verbraucher oder sogar den gesamten Namespace explizit aktiviert wird, indem der <code>sidecar</code> in benutzerdefinierte Herde eingef√ºhrt wird.  Dieser Container startet eine Instanz von Envoy und legt eine Reihe von iptables-Regeln fest, um den Verkehr zu anderen Diensten abzufangen und diesen Verkehr zu Envoy umzuleiten. <br><br>  Bei der Integration in Kubernetes DNS bedeutet dies, dass unser Code √ºber den Namen des Dienstes eine Verbindung herstellen kann und alles "einfach funktioniert".  Mit anderen Worten, unser Code gibt Anforderungen wie <code>http://api/v1/users/4242</code> , dann l√∂st <code>api</code> die Anforderung in <code>10.97.105.48</code> , iptables-Regeln fangen Verbindungen von 10.97.105.48 ab und leiten sie an den lokalen Envoy-Proxy weiter, und dieser lokale Proxy leitet sie weiter Anfrage f√ºr die eigentliche Backend-API.  Fuh! <br><br><h3>  Extra kleine Dinger </h3><br>  Istio bietet auch End-to-End-Verschl√ºsselung und Authentifizierung √ºber mTLS (gegenseitiges TLS).  Verantwortlich daf√ºr ist die Komponente <i>Citadel</i> . <br><br>  Es gibt auch eine <i>Mixer-</i> Komponente, die Envoy f√ºr <i>jede</i> Anfrage anfordern kann, um eine spezielle Entscheidung √ºber diese Anfrage zu treffen, abh√§ngig von verschiedenen Faktoren wie Headern, Laden des Backends usw. (keine Sorge: Es gibt viele M√∂glichkeiten, um sicherzustellen, dass der Mixer funktioniert und sogar Wenn es abst√ºrzt, arbeitet Envoy weiterhin normal als Proxy. <br><br>  Und nat√ºrlich haben wir die Sichtbarkeit erw√§hnt: Envoy sammelt eine gro√üe Anzahl von Metriken und bietet verteilte Ablaufverfolgung.  Wenn in der Architektur von Microservices eine API-Anforderung die Microservices A, B, C und D durchlaufen muss, f√ºgt der verteilte Trace bei der Anmeldung am System der Anforderung eine eindeutige Kennung hinzu und speichert diese Kennung √ºber Unterabfragen an alle diese Mikrodienste, sodass Sie alle zugeh√∂rigen Aufrufe aufzeichnen k√∂nnen Verz√∂gerungen usw. <br><br><h1>  Entwickeln oder kaufen </h1><br>  Istio hat den Ruf, ein komplexes System zu sein.  Im Gegensatz dazu ist das Erstellen eines Routing-Rasters, das ich zu Beginn dieses Beitrags beschrieben habe, mit vorhandenen Tools relativ einfach.  Ist es also sinnvoll, stattdessen ein eigenes Service-Mesh zu erstellen? <br><br>  Wenn wir bescheidene Bed√ºrfnisse haben (Sie brauchen keine Sichtbarkeit, einen Leistungsschalter und andere Feinheiten), dann denken Sie dar√ºber nach, Ihr eigenes Werkzeug zu entwickeln.  Wenn wir jedoch Kubernetes verwenden, ist dies m√∂glicherweise nicht einmal erforderlich, da Kubernetes bereits grundlegende Tools f√ºr die Serviceerkennung und den Lastenausgleich bereitstellt. <br><br>  Wenn wir jedoch erweiterte Anforderungen haben, scheint der ‚ÄûKauf‚Äú eines Service-Netzes eine viel bessere Option zu sein.  (Dies ist nicht immer ein ‚ÄûKauf‚Äú, da Istio mit Open Source-Code geliefert wird. Wir m√ºssen jedoch noch Engineering-Zeit investieren, um seine Arbeit zu verstehen, ihn bereitzustellen und zu verwalten.) <br><br><h1>  Was zur Auswahl: Istio, Linkerd oder Consul Connect? </h1><br>  Bisher haben wir nur √ºber Istio gesprochen, aber dies ist nicht das einzige Service-Mesh.  Eine beliebte Alternative ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Linkerd</a> , und es gibt auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Consul Connect</a> . <br><br>  Was soll ich w√§hlen? <br><br>  Ehrlich gesagt, ich wei√ü es nicht.  Im Moment halte ich mich nicht f√ºr kompetent genug, um diese Frage zu beantworten.  Es gibt einige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interessante</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel, in</a> denen diese Tools und sogar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benchmarks</a> verglichen werden. <br><br>  Ein vielversprechender Ansatz ist die Verwendung eines Tools wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SuperGloo</a> .  Es implementiert eine Abstraktionsschicht, um die von Service-Meshes bereitgestellten APIs zu vereinfachen und zu vereinheitlichen.  Anstatt bestimmte (und meiner Meinung nach relativ komplexe) APIs verschiedener Service-Meshes zu untersuchen, k√∂nnen wir einfachere SuperGloo-Konstruktionen verwenden - und einfach von einer zur anderen wechseln, als h√§tten wir ein Zwischenkonfigurationsformat, das HTTP-Schnittstellen und beschreibt Backends, die die eigentliche Konfiguration f√ºr Nginx, HAProxy, Traefik, Apache ... generieren k√∂nnen. <br><br>  Ich habe mich ein wenig mit Istio und SuperGloo besch√§ftigt, und im n√§chsten Artikel m√∂chte ich zeigen, wie man Istio oder Linkerd mit SuperGloo zu einem vorhandenen Cluster hinzuf√ºgt und wie viel dieser mit seiner Arbeit fertig wird, dh Sie k√∂nnen von einem Service-Mesh zu einem anderen wechseln, ohne die Konfigurationen neu zu schreiben. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de453204/">https://habr.com/ru/post/de453204/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de453190/index.html">ANPR mit RoR & React Native</a></li>
<li><a href="../de453192/index.html">Synchronisation und Asynchronit√§t von Prozessen</a></li>
<li><a href="../de453194/index.html">Wir l√∂sen das Best Reverser-Problem mit PHDays 9</a></li>
<li><a href="../de453196/index.html">Forrester Research: Ein Vergleich von zehn f√ºhrenden Anbietern von Software-Zusammensetzungsanalysen</a></li>
<li><a href="../de453200/index.html">Diskussion: Das OpenROAD-Projekt soll die Aufgabe l√∂sen, das Design von Prozessoren zu automatisieren</a></li>
<li><a href="../de453206/index.html">Interview mit Kelsey Moody: Wie man ein Unternehmen aufbaut und altersbedingte Pathologien beendet</a></li>
<li><a href="../de453212/index.html">Verbraucherberichte: Der neueste Autopilot von Tesla ist alles andere als perfekt</a></li>
<li><a href="../de453214/index.html">Wie und warum Sie sich fit halten, wenn Sie eine IT-Person an einem Remotestandort sind</a></li>
<li><a href="../de453216/index.html">Verkehrs√ºberwachungssysteme in VoIP-Netzen. Zweiter Teil - Organisationsprinzipien</a></li>
<li><a href="../de453218/index.html">Die Hauptsache bei YaC 2019: Hundert Drohnen auf den Stra√üen, Yandex.Module, Essen, Smart Home</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>