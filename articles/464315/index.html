<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéà üõ°Ô∏è ‚òÑÔ∏è La pel√≠cula en la que hab√≠a tierra. Investigaci√≥n de Yandex y una breve historia de b√∫squeda por significado ‚è© üíî ‚õπÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A veces las personas recurren a Yandex para encontrar una pel√≠cula cuyo nombre sali√≥ de sus cabezas. Describen la trama, escenas memorables, detalles ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La pel√≠cula en la que hab√≠a tierra. Investigaci√≥n de Yandex y una breve historia de b√∫squeda por significado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/464315/">  A veces las personas recurren a Yandex para encontrar una pel√≠cula cuyo nombre sali√≥ de sus cabezas.  Describen la trama, escenas memorables, detalles v√≠vidos: por ejemplo, [c√≥mo se llama la pel√≠cula en la que un hombre elige una pastilla roja o azul].  Decidimos estudiar las descripciones de las pel√≠culas olvidadas y descubrir qu√© es lo que m√°s recuerdan las personas en las pel√≠culas. <br><br>  Hoy no solo compartiremos un enlace a nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">investigaci√≥n</a> , sino que tambi√©n hablaremos brevemente sobre c√≥mo evolucion√≥ la b√∫squeda sem√°ntica de Yandex.  Aprender√° qu√© tecnolog√≠as ayudan a la b√∫squeda a encontrar la respuesta, incluso cuando es simplemente imposible formular la solicitud exacta. <br><br>  Y tambi√©n agregamos controles deslizantes de acertijos con ejemplos de solicitudes de personas reales: si√©ntase como un motor de b√∫squeda e intente adivinar la respuesta. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a>  Todos los motores de b√∫squeda comenzaron con una b√∫squeda de palabras.  Yandex ya al principio pudo tener en cuenta la morfolog√≠a del idioma ruso, pero segu√≠a siendo la misma b√∫squeda de palabras de una consulta en las p√°ginas de la red.  Mantuvimos listas de todas las p√°ginas conocidas para cada palabra.  Si la solicitud conten√≠a una frase, era suficiente para cruzar las listas de palabras: aqu√≠ est√° la respuesta.  Funcion√≥ muy bien en aquellos d√≠as cuando hab√≠a pocos sitios, y la cuesti√≥n de la clasificaci√≥n a√∫n no era tan aguda. <br><br>  Runet desarrollado, los sitios se hicieron m√°s y m√°s.  Se agregaron dos factores m√°s al factor de cruce de palabras.  Por un lado, los propios usuarios nos ayudaron.  Comenzamos a considerar qu√© sitios y para qu√© consultas eligen.  No hay una coincidencia exacta de palabras, pero ¬øresuelve el sitio el problema humano?  Esta es una se√±al √∫til.  Por otro lado, los enlaces entre sitios que ayudaron a evaluar la importancia de las p√°ginas fueron al rescate. <br><br>  Tres factores son muy pocos.  Especialmente cuando a menudo son probados por los talentosos optimizadores de motores de b√∫squeda.  Pero digerir m√°s a mano era dif√≠cil.  Y aqu√≠ comenz√≥ la era del aprendizaje autom√°tico.  En 2009, presentamos Matrixnet basado en el aumento de gradiente (m√°s tarde esta tecnolog√≠a form√≥ la base de la biblioteca de c√≥digo abierto m√°s avanzada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CatBoost</a> ). <br><br>  Desde entonces, ha habido m√°s y m√°s factores, porque ya no tenemos que buscar relaciones entre ellos manualmente.  Un auto lo hizo por nosotros. <br><br>  Para la historia de todos los cambios posteriores en la B√∫squeda, no solo la publicaci√≥n, sino tambi√©n los libros ser√°n suficientes, por lo que intentaremos centrarnos en los m√°s significativos. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  La clasificaci√≥n no es solo una comparaci√≥n de palabras de consulta y p√°gina durante mucho tiempo.  Dos ejemplos <br><br>  En 2014, presentamos la tecnolog√≠a de anotaci√≥n de documentos con consultas caracter√≠sticas.  Supongamos que en el pasado hubiera una solicitud [una serie de Brasil sobre el rey de la carne], para la cual ya se conoce una buena respuesta.  Luego, otro usuario ingresa una consulta [la serie brasile√±a en la que hab√≠a un rey de la carne y un rey de la leche], para la cual la m√°quina a√∫n no sabe la respuesta.  Pero estas consultas tienen muchas palabras comunes.  Esta es una se√±al de que la p√°gina encontrada en la primera solicitud puede ser relevante en la segunda. <br><br>  Otro ejemplo.  Hagamos preguntas [la serie brasile√±a en la que hab√≠a un rey de la carne y un rey de la leche] y [una herencia fatal en serie].  Del total, solo tienen una palabra: "serie", y esto no es suficiente para la coincidencia expl√≠cita de solicitudes.  En este caso, comenzamos a tener en cuenta el historial de la b√∫squeda.  Si se solicitan dos solicitudes diferentes en los mismos sitios en la emisi√≥n, entonces podemos suponer que las solicitudes son intercambiables.  Esto es √∫til porque ahora utilizaremos el texto de ambas consultas para buscar y encontrar p√°ginas m√°s √∫tiles.  Pero esto solo funciona para solicitudes repetidas cuando ya hay al menos algunas estad√≠sticas.  ¬øQu√© hacer con las nuevas solicitudes? <br><br>  La falta de estad√≠sticas puede compensarse mediante an√°lisis de contenido.  Y en el an√°lisis de datos homog√©neos (texto, voz, im√°genes) las redes neuronales se muestran mejor.  En 2016, primero le contamos a la comunidad Habr sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la tecnolog√≠a Palekh</a> , que se convirti√≥ en el punto de partida para el uso m√°s amplio de las redes neuronales en la B√∫squeda. <br><br>  Comenzamos a entrenar la red neuronal para comparar la proximidad sem√°ntica (sem√°ntica) del texto de la solicitud y el t√≠tulo de la p√°gina.  Dos textos se representan en forma de vectores en el espacio multidimensional, de modo que el coseno del √°ngulo entre ellos predice bien la probabilidad de que una persona elija una p√°gina y, por lo tanto, la proximidad sem√°ntica.  Esto le permite evaluar la cercan√≠a de los significados de incluso aquellos textos en los que no hay intersecci√≥n de palabras. <br><br><div class="spoiler">  <b class="spoiler_title">Un ejemplo de arquitectura de capas para curiosos</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/404/470/082/4044700822614d34976b92f9caa6a38c.png" alt="imagen"><br></div></div><br>  Del mismo modo, comenzamos a comparar textos de consulta para identificar enlaces entre ellos.  Un verdadero ejemplo bajo el cap√≥ de un motor de b√∫squeda: para una consulta [serie estadounidense sobre c√≥mo se hierve la metanfetamina], es la red neuronal la que encuentra las frases [que significan mal] y [que rompen mal] con un significado similar. <br><br>  Las solicitudes y los encabezados ya son buenos, pero no perdimos la esperanza de usar redes neuronales en el texto completo de las p√°ginas.  Adem√°s, cuando recibimos una solicitud del usuario, comenzamos a seleccionar las mejores p√°ginas entre millones de p√°ginas de √≠ndice, pero en Palekh usamos modelos de redes neuronales solo en las √∫ltimas etapas de clasificaci√≥n (L3), a aproximadamente 150 de los mejores documentos.  Esto puede conducir a la p√©rdida de buenas respuestas. <br><br><img src="https://habrastorage.org/web/0fa/5fb/280/0fa5fb280cf74efeab59bd9657aaeb00.png" alt="imagen"><br><br>  La raz√≥n es predecible: recursos limitados y altos requisitos de velocidad de respuesta.  Las limitaciones estrictas de los c√°lculos est√°n relacionadas con un hecho simple: no puede obligar al usuario a esperar.  Pero luego se nos ocurri√≥ algo. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  En 2017, presentamos la actualizaci√≥n de b√∫squeda de Korolev, que inclu√≠a no solo el uso ampliado de las redes neuronales, sino tambi√©n un trabajo serio en arquitectura para ahorrar recursos.  M√°s detalladamente, con diagramas de capas y otros detalles que ya contamos en otra publicaci√≥n sobre Habr√©, pero ahora recordaremos lo principal. <br><br>  En lugar de tomar el t√≠tulo del documento y calcular su vector sem√°ntico durante la ejecuci√≥n de la consulta, puede calcular previamente este vector y guardarlo en la base de datos de b√∫squeda.  En otras palabras, podemos hacer una parte sustancial del trabajo por adelantado.  Por supuesto, al mismo tiempo, necesit√°bamos m√°s espacio para almacenar vectores, pero esto nos ahorr√≥ tiempo de procesador.  Pero eso no es todo. <br><br><div class="spoiler">  <b class="spoiler_title">Otro esquema para los curiosos</b> <div class="spoiler_text"><img src="https://habrastorage.org/web/9ff/3a8/06a/9ff3a806a97647299ae6736cf02a7d06.png" alt="imagen"><br></div></div><br>  Construimos un √≠ndice adicional.  Se basa en la hip√≥tesis: si toma una lista suficientemente grande de los documentos m√°s relevantes para cada palabra o frase para una consulta de varias palabras, entre ellas habr√° documentos que sean relevantes al mismo tiempo para todas las palabras.  En la pr√°ctica, esto significa esto.  Para todas las palabras y pares de palabras populares, se forma un √≠ndice adicional con una lista de p√°ginas y su relevancia preliminar para la consulta.  Es decir, transferimos parte del trabajo de la etapa L0 a la etapa de indexaci√≥n y, nuevamente, lo guardamos. <br><br>  Como resultado, un cambio en la arquitectura y la redistribuci√≥n de las cargas nos permiti√≥ usar redes neuronales no solo en la etapa L3, sino tambi√©n para L2 y L1.  Adem√°s, la capacidad de formar un vector por adelantado y con requisitos de rendimiento menos estrictos nos permiti√≥ usar no solo el t√≠tulo de la p√°gina, sino tambi√©n su texto. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  M√°s es m√°s.  Con el tiempo, comenzamos a usar redes neuronales en la primera etapa de la clasificaci√≥n.  Ense√±amos redes neuronales para identificar patrones impl√≠citos en el orden de las palabras y sus posiciones relativas.  E incluso para revelar la similitud sem√°ntica de textos en diferentes idiomas.  Cada una de estas √°reas se dibuja en un art√≠culo separado, e intentaremos volver con ellas en un futuro pr√≥ximo. <br><br><hr><br>  Hoy, una vez m√°s, recordamos c√≥mo los motores de b√∫squeda aprenden a encontrar la respuesta en las condiciones de una consulta vaga y la falta de informaci√≥n.  La b√∫squeda de pel√≠culas por su descripci√≥n no es solo un caso especial de tales solicitudes, sino tambi√©n un gran tema para la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">investigaci√≥n</a> .  De ella aprender√°s: lo que m√°s recuerdan las personas en el cine, con el que se asocian diferentes g√©neros y cinematograf√≠as de diferentes pa√≠ses, los movimientos de la trama causan una impresi√≥n especial. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/464315/">https://habr.com/ru/post/464315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../464299/index.html">0, 0, 1, 0, 2, 0, 2, 2, 1, 6, 0, 5, 0, 2, 6, 5, 4, 0, 5, 3, 0, 3, 2, 9, 0, 4, 9, 3, 6, 14, 0, 6, 3, 5, 15, 0, 5, 3, 5 ...</a></li>
<li><a href="../464303/index.html">Datos de series de tiempo en un DBMS relacional. Extensiones TimescaleDB y PipelineDB para PostgreSQL</a></li>
<li><a href="../464305/index.html">Peque√±o, si. Unboxing del petardo microvirtual</a></li>
<li><a href="../464307/index.html">Pruebas de integraci√≥n de microservicios en Scala</a></li>
<li><a href="../464309/index.html">Bot√≥n de llamada de bricolaje. Raspberry Pi, MajorDoMo, Freeswitch y Linphonec</a></li>
<li><a href="../464317/index.html">Proyecto Konbanwa</a></li>
<li><a href="../464325/index.html">C√≥mo Scrumban une lo mejor de las metodolog√≠as Kanban y Scrum</a></li>
<li><a href="../464327/index.html">Comparaci√≥n del uso de memoria de diferentes GUI de kit de herramientas</a></li>
<li><a href="../464331/index.html">Beneficios in√∫tiles: s√≠ntesis de productos qu√≠micos absorbentes de UV a partir de anacardos</a></li>
<li><a href="../464333/index.html">Seguimiento del ciclo de vida de los usuarios sin alicates y cinta aislante.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>