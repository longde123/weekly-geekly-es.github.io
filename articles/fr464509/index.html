<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîå üìñ üíáüèº Le livre "Grok deep learning" ü§æüèº üèçÔ∏è ‚ùé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, habrozhiteli! Le livre jette les bases d'une ma√Ætrise accrue des technologies d'apprentissage en profondeur. Il commence par une description de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le livre "Grok deep learning"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/464509/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/go/gm/1s/gogm1solwetphsozljuyzuizcbs.jpeg" align="left" alt="image"></a>  Salut, habrozhiteli!  Le livre jette les bases d'une ma√Ætrise accrue des technologies d'apprentissage en profondeur.  Il commence par une description des bases des r√©seaux de neurones, puis examine en d√©tail les couches d'architecture suppl√©mentaires. <br><br>  Le livre est sp√©cialement √©crit dans le but de fournir le seuil d'entr√©e le plus bas possible.  Vous n'avez pas besoin de conna√Ætre l'alg√®bre lin√©aire, les m√©thodes num√©riques, les optimisations convexes et m√™me l'apprentissage automatique.  Tout ce qui est n√©cessaire pour comprendre l'apprentissage en profondeur sera clarifi√© au fur et √† mesure. <br><br>  Nous vous proposons de vous familiariser avec le passage "Qu'est-ce qu'un cadre d'apprentissage en profondeur?" <br><a name="habracut"></a><br>  <b>De bons outils r√©duisent les erreurs, acc√©l√®rent le d√©veloppement et augmentent la vitesse d'ex√©cution.</b> <br><br>  Si vous lisez beaucoup sur l'apprentissage en profondeur, vous √™tes probablement tomb√© sur des cadres bien connus tels que PyTorch, TensorFlow, Theano (r√©cemment d√©clar√© obsol√®te), Keras, Lasagne et DyNet.  Au cours des derni√®res ann√©es, les frameworks ont √©volu√© tr√®s rapidement, et malgr√© le fait que tous ces frameworks soient distribu√©s gratuitement et open source, chacun d'eux a un esprit de comp√©tition et de camaraderie. <br><br>  Jusqu'√† pr√©sent, j'ai √©vit√© de discuter de frameworks, car, tout d'abord, il √©tait extr√™mement important pour vous de comprendre ce qui se passait en coulisses, en impl√©mentant les algorithmes manuellement (en utilisant uniquement la biblioth√®que NumPy).  Mais maintenant, nous allons commencer √† utiliser de tels cadres, car les r√©seaux que nous allons former, les r√©seaux avec m√©moire √† court terme √† long terme (LSTM), sont tr√®s complexes, et le code qui les impl√©mente en utilisant NumPy est difficile √† lire, √† utiliser et √† d√©boguer (gradients dans ce code se trouvent partout). <br><br>  C'est cette complexit√© que les cadres d'apprentissage en profondeur sont con√ßus pour traiter.  Le cadre d'apprentissage en profondeur peut r√©duire consid√©rablement la complexit√© du code (ainsi que le nombre d'erreurs et augmenter la vitesse de d√©veloppement) et augmenter la vitesse de son ex√©cution, surtout si vous utilisez un processeur graphique (GPU) pour entra√Æner le r√©seau neuronal, ce qui peut acc√©l√©rer le processus de 10 √† 100 fois.  Pour ces raisons, les cadres sont utilis√©s presque partout dans la communaut√© de la recherche, et la compr√©hension des caract√©ristiques de leur travail vous sera utile dans votre carri√®re en tant qu'utilisateur et chercheur en apprentissage profond. <br><br>  Mais nous ne nous limiterons pas au cadre d'un cadre particulier, car cela vous emp√™chera d'apprendre comment fonctionnent tous ces mod√®les complexes (tels que LSTM).  Au lieu de cela, nous allons cr√©er notre propre framework l√©ger, en suivant les derni√®res tendances dans le d√©veloppement de frameworks.  En suivant ce chemin, vous saurez exactement ce que font les frameworks lorsque des architectures complexes sont cr√©√©es avec leur aide.  De plus, une tentative de cr√©er votre propre petit framework vous-m√™me vous aidera √† passer en douceur √† l'utilisation de v√©ritables frameworks d'apprentissage en profondeur, car vous connaissez d√©j√† les principes d'organisation d'une interface de programme (API) et ses fonctionnalit√©s.  Cet exercice m'a √©t√© tr√®s utile et les connaissances acquises lors de la cr√©ation de mon propre framework se sont av√©r√©es tr√®s utiles lors du d√©bogage de mod√®les de probl√®me. <br><br>  Comment le framework simplifie-t-il le code?  En termes abstraits, cela √©limine la n√©cessit√© d'√©crire encore et encore le m√™me code.  Plus pr√©cis√©ment, la fonctionnalit√© la plus pratique du cadre d'apprentissage en profondeur est la prise en charge de la r√©tropropagation automatique et de l'optimisation automatique.  Cela vous permet d'√©crire uniquement du code de distribution directe, et le framework se chargera automatiquement de la distribution arri√®re et de la correction des poids.  La plupart des frameworks modernes simplifient m√™me le code qui impl√©mente la distribution directe en offrant des interfaces de haut niveau pour d√©finir des couches typiques et des fonctions de perte. <br><br><h3>  Introduction aux tenseurs </h3><br>  <b>Les tenseurs sont une forme abstraite de vecteurs et de matrices</b> <br><br>  Jusqu'√† ce moment, nous utilisions des vecteurs et des matrices comme structures principales.  Permettez-moi de vous rappeler qu'une matrice est une liste de vecteurs, et un vecteur est une liste de scalaires (nombres simples).  Un tenseur est une forme abstraite pour repr√©senter des listes de nombres imbriqu√©es.  Le vecteur est un tenseur unidimensionnel.  La matrice est un tenseur √† deux dimensions, et les structures avec un grand nombre de dimensions sont appel√©es tenseurs √† n dimensions.  Commen√ßons donc √† cr√©er un nouveau cadre d'apprentissage en profondeur en d√©finissant un type de base, que nous appellerons Tenseur: <br><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class"> </span></span>= np.array(data) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) print(x) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>] y = x + x print(y) [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>]</code> </pre> <br>  Ceci est la premi√®re version de notre structure de donn√©es de base.  Notez qu'il stocke toutes les informations num√©riques dans le tableau NumPy (self.data) et prend en charge une op√©ration de tenseur unique (ajout).  Ajouter des op√©rations suppl√©mentaires n'est pas difficile du tout, ajoutez simplement des fonctions suppl√©mentaires avec la fonctionnalit√© correspondante √† la classe Tensor. <br><br><h3>  Introduction au calcul automatique du gradient (autograd) </h3><br>  <b>Auparavant, nous avons effectu√© une propagation arri√®re manuelle.</b>  <b>Maintenant, rendons-le automatique!</b> <br><br>  Dans le chapitre 4, nous avons introduit les d√©riv√©s.  Depuis lors, nous avons calcul√© manuellement ces d√©riv√©es dans chaque nouveau r√©seau neuronal.  Permettez-moi de vous rappeler que cela est r√©alis√© par un mouvement inverse √† travers le r√©seau de neurones: d'abord, le gradient √† la sortie du r√©seau est calcul√©, puis ce r√©sultat est utilis√© pour calculer la d√©riv√©e dans le composant pr√©c√©dent, et ainsi de suite, jusqu'√† ce que les gradients corrects soient d√©termin√©s pour tous les poids dans l'architecture.  Cette logique de calcul des gradients peut √©galement √™tre ajout√©e √† la classe des tenseurs.  Ce qui suit montre ce que j'avais en t√™te. <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">creators</span></span></span></span>=None, creation_op=None): self.data = np.array(data) self.creation_op = creation_op self.creators = creators self.grad = None def backward(self, grad): self.grad = grad <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(self.creation_op == <span class="hljs-string"><span class="hljs-string">"add"</span></span>): self.creators[<span class="hljs-number"><span class="hljs-number">0</span></span>].backward(grad) self.creators[<span class="hljs-number"><span class="hljs-number">1</span></span>].backward(grad) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data, creators=[self,other], creation_op=<span class="hljs-string"><span class="hljs-string">"add"</span></span>) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) y = Tensor([<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>]) z = x + y z.backward(Tensor(np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre> <br>  Cette m√©thode introduit deux innovations.  Tout d'abord, chaque tenseur re√ßoit deux nouveaux attributs.  creators est une liste de tous les tenseurs utilis√©s pour cr√©er le tenseur actuel (par d√©faut None).  Autrement dit, si le tenseur z est obtenu en ajoutant les deux autres tenseurs, x et y, l'attribut creators du tenseur z contiendra les tenseurs x et y.  creation_op est un attribut compagnon qui stocke les op√©rations utilis√©es dans le processus de cr√©ation de ce tenseur.  Autrement dit, l'instruction z = x + y cr√©era un graphe de calcul avec trois n≈ìuds (x, y et z) et deux ar√™tes (z -&gt; x et z -&gt; y).  Chaque bord est sign√© par l'op√©ration de creation_op, c'est-√†-dire ajouter.  Ce graphique aidera √† organiser la propagation r√©cursive vers l'arri√®re des gradients. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/4d/qz/fx4dqzrh6y62rtttfy2vrn1jley.png" alt="image"></div><br>  La premi√®re innovation de cette impl√©mentation est la cr√©ation automatique d'un graphe lors de chaque op√©ration math√©matique.  Si nous prenons z et effectuons une autre op√©ration, le graphe sera poursuivi dans une nouvelle variable r√©f√©ren√ßant z. <br><br>  La deuxi√®me innovation de cette version de la classe Tensor est la possibilit√© d'utiliser un graphique pour calculer les gradients.  Si vous appelez la m√©thode z.backward (), elle passera le gradient pour x et y, en tenant compte de la fonction avec laquelle le tenseur z (add) a √©t√© cr√©√©.  Comme le montre l'exemple ci-dessus, nous passons le vecteur de gradient (np.array ([1,1,1,1,1]]) √† z, et que l'on applique √† ses parents.  Comme vous vous en souvenez probablement du chapitre 4, la propagation vers l'arri√®re par l'addition signifie appliquer la propagation vers l'arri√®re.  Dans ce cas, nous n'avons qu'un seul gradient √† ajouter √† x et y, nous le copions donc de z √† x et y: <br><br><pre> <code class="javascript hljs">print(x.grad) print(y.grad) print(z.creators) print(z.creation_op) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])] add</code> </pre> <br>  La caract√©ristique la plus remarquable de cette forme de calcul automatique du gradient est qu'elle fonctionne r√©cursivement - chaque vecteur appelle la m√©thode .backward () de tous ses parents √† partir de la liste self.creators: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bt/ye/if/btyeiflumrhlbprlzqtzjxwipcs.png" alt="image"></div><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  25% de r√©duction sur les colporteurs - <b>Deep Learning</b> <br>  Lors du paiement de la version papier du livre, un livre √©lectronique est envoy√© par e-mail. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr464509/">https://habr.com/ru/post/fr464509/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr464491/index.html">Vivaldi 2.7 - Une vie intense en silence</a></li>
<li><a href="../fr464495/index.html">D√©veloppement d'√©quipe et r√©flexion comme communication manag√©riale du chef d'√©quipe</a></li>
<li><a href="../fr464497/index.html">JIRA comme rem√®de contre l'insomnie et les d√©pressions nerveuses</a></li>
<li><a href="../fr464499/index.html">"Mat. Mod√®le de Wall Street ¬ªou une tentative d'optimiser le co√ªt de l'infrastructure informatique cloud</a></li>
<li><a href="../fr464503/index.html">Correspondance de mot de passe Wi-Fi avec aircrack-ng</a></li>
<li><a href="../fr464511/index.html">Comment collecter des cohortes d'utilisateurs sous forme de graphiques dans Grafana [+ image docker avec un exemple]</a></li>
<li><a href="../fr464513/index.html">Duffle: Transformer de XD Design</a></li>
<li><a href="../fr464515/index.html">Comment faire des e-mails et ne pas g√¢cher: conseils pratiques</a></li>
<li><a href="../fr464517/index.html">Nouvelles cartes CUBA</a></li>
<li><a href="../fr464523/index.html">Syst√®mes de paiement (PSP) pour l'informatique: nous jouons gros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>