<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå™Ô∏è üö£üèΩ ‚úåüèº Stellen Sie Kubernetes Cluster mit Kubespray auf OpenStack bereit üõ¢Ô∏è üîë üë®üèø‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kubernetes hat sich schnell zum Standard f√ºr die Bereitstellung und Skalierung von Containeranwendungen und deren Verwaltung entwickelt. Dies ist eine...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stellen Sie Kubernetes Cluster mit Kubespray auf OpenStack bereit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/456792/"><p><img src="https://habrastorage.org/webt/oz/5b/xn/oz5bxnm9_ej5v-po5gjbk93otk0.jpeg"></p><br><p>  Kubernetes hat sich schnell zum Standard f√ºr die Bereitstellung und Skalierung von Containeranwendungen und deren Verwaltung entwickelt.  Dies ist eine sehr flexible und vielseitige Open Source-L√∂sung.  Es verf√ºgt √ºber eine umfangreiche Dokumentation und es ist nicht immer einfach, den richtigen Abschnitt darin zu finden.  Daher ist Kubernetes so schwer zu meistern.  Nach der Planung des Clusters m√ºssen Sie ihn noch installieren, aber auch hier ist nicht alles reibungslos.  Daher gibt es Bereitstellungstools wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubespray,</a> die die Arbeit vereinfachen.  Ich werde √ºber die automatische Bereitstellung des Kubernetes-Clusters mit Kubespray in der OpenStack-Cloud (Open Telekom Cloud) sprechen. </p><a name="habracut"></a><br><p>  F√ºr die automatische Bereitstellung von Kubernetes verwendet Kubespray das Tool zur Initialisierung, Konfiguration und Bereitstellung von Ansible-Anwendungen.  Kubespray bietet eine Bibliothek zum Initialisieren von Ressourcen auf verschiedenen Cloud-Plattformen.  Verwenden Sie dazu das Tool "Infrastruktur als Code" Terraform.  Das Kubespray-Projekt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unterst√ºtzt</a> jetzt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Terraform</a> f√ºr AWS-, OpenStack- und Packet-Clouds.  Dieses Tool wird in Verbindung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der OpenStack-Bibliothek verwendet</a> , um die Infrastruktur in diesem Szenario bereitzustellen. </p><br><h2 id="trebovaniya">  Anforderungen </h2><br><p>  Schauen wir uns zun√§chst die Bereitstellungsvoraussetzungen an.  Sie sind in zwei Kategorien unterteilt: Anforderungen an Kubespray und Anforderungen an die Anbieterbibliothek. </p><br><p>  Kubespray ben√∂tigt folgende Komponenten: </p><br><ul><li>  Python 2.7 (oder h√∂her) </li><li>  Ansible 2.7 (oder h√∂her) </li><li>  Jinja 2.9 (oder h√∂her) </li></ul><br><p>  OpenStack Provider Library Anforderungen: </p><br><ul><li>  Terraform 0,11 (oder h√∂her) </li></ul><br><p>  Um Terraform zu installieren, m√ºssen Sie das entsprechende Paket von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Hashicorp-Website herunterladen</a> und entpacken.  Dann muss der Pfad zur entpackten Datei in der Variablen PATH gespeichert werden.  Verwenden Sie den Befehl terraform, um zu √ºberpr√ºfen, ob alles installiert ist.  Hier erfahren Sie mehr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> </p><br><p>  Je nach Betriebssystem kann Ansible mit einigen Befehlen installiert werden.  Siehe die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ansible-Dokumentation.</a>  Hier benutze ich Ubuntu und installiere Ansible wie folgt. </p><br><pre><code class="plaintext hljs">sudo apt update sudo apt install ansible</code> </pre> <br><p>  Dann m√ºssen Sie die Kubespray-Abh√§ngigkeiten installieren.  Dies erfolgt mit dem folgenden Befehl.  Aber zuerst m√ºssen Sie das Repository klonen. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-sigs/kubespray sudo pip install -r requirements.txt</code> </pre> <br><p>  Um Open Telekom Cloud zu verwenden, konfigurieren Sie die Zugriffsdaten mit .ostackrc im Stammverzeichnis und laden Sie die Umgebungsvariablen. </p><br><h2 id="planirovanie-klastera">  Clusterplanung </h2><br><p>  Kubernetes ist sehr flexibel, sodass der Cluster an Ihre Bed√ºrfnisse angepasst werden kann.  Hier werden wir keine unterschiedlichen Optionen f√ºr Cluster betrachten.  Sie k√∂nnen dies in der Kubernetes-Dokumentation unter Erstellen eines benutzerdefinierten Clusters von Grund auf nachlesen.  Zum Beispiel erstellen wir aus dem Assistenten einen Cluster mit etcd und zwei Arbeitsknoten.  Der Cluster verf√ºgt nicht √ºber eine Floating-IP, sodass er im Internet nicht verf√ºgbar ist. </p><br><p>  Wir m√ºssen auch CNI (Container Network Interface) w√§hlen.  Es gibt verschiedene Optionen (Cilium, Kaliko, Flanell, Webnetz usw.), aber wir nehmen ein Flanell, das nicht konfiguriert werden muss.  Calico funktioniert, aber Sie m√ºssen <a href="">OpenStack Neutron-Ports</a> f√ºr Subnetze von Diensten und Pods <a href="">konfigurieren</a> . </p><br><p>  Um die Cluster im Kubernetes-Dashboard nach der Bereitstellung zu verwalten, m√ºssen wir dieses Dashboard installieren. </p><br><h2 id="nastroyka-konfiguracii-klastera">  Cluster-Konfiguration </h2><br><p>  F√ºhren Sie die folgenden Befehle im Repository-Verzeichnis aus und geben Sie den gew√ºnschten Namen in der Variablen $ CLUSTER an. </p><br><pre> <code class="plaintext hljs">cp -LRp contrib/terraform/openstack/sample-inventory \ inventory/$CLUSTER cd inventory/$CLUSTER ln -s ../../contrib/terraform/openstack/hosts ln -s ../../contrib</code> </pre> <br><p>  Bearbeiten Sie nach dem Ausf√ºhren der Befehle die Datei inventar / $ CLUSTER / cluster.tf. </p><br><pre> <code class="plaintext hljs"># your Kubernetes cluster name here cluster_name = "k8s-test-cluster" az_list=["eu-de-01", "eu-de-02"] dns_nameservers=["100.125.4.25", "8.8.8.8"] # SSH key to use for access to nodes public_key_path = "~/.ssh/id_rsa.pub" # image to use for bastion, masters, standalone etcd instances, and nodes image = "Standard_CentOS_7_latest" # user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.) ssh_user = "linux" # 0|1 bastion nodes number_of_bastions = 0 flavor_bastion = "s2.xlarge.4" # standalone etcds number_of_etcd = 0 flavor_etcd = "s2.xlarge.4" # masters number_of_k8s_masters = 0 number_of_k8s_masters_no_etcd = 0 number_of_k8s_masters_no_floating_ip = 1 number_of_k8s_masters_no_floating_ip_no_etcd = 0 flavor_k8s_master = "s2.xlarge.4" # nodes number_of_k8s_nodes = 0 number_of_k8s_nodes_no_floating_ip = 2 flavor_k8s_node = "s2.xlarge.4" # GlusterFS # either 0 or more than one #number_of_gfs_nodes_no_floating_ip = 1 #gfs_volume_size_in_gb = 150 # Container Linux does not support GlusterFS image_gfs = "Standard_CentOS_7_latest" # May be different from other nodes #ssh_user_gfs = "linux" #flavor_gfs_node = "s2.xlarge.4" # networking network_name = "k8s-test-network" external_net = "Externel_Network_ID" subnet_cidr = "192.168.100.0/24" floatingip_pool = "admin_external_net" bastion_allowed_remote_ips = ["0.0.0.0/0"]</code> </pre> <br><p>  Beschreibung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> gelesenen Variablen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a>  In diesem Beispiel erstellen wir einen Cluster mit einem Master- und zwei Kubernetes-Arbeitsknoten basierend auf der neuesten Version von CentOS 7 und s2.xlarge.4.  etcd wird auch im Assistenten installiert. </p><br><h2 id="razvertyvanie-infrastruktury">  Bereitstellung der Infrastruktur </h2><br><p>  Jetzt k√∂nnen wir die Clusterinfrastruktur mit Terraform bereitstellen.  Die Abbildung zeigt, wie die Infrastruktur nach der Bereitstellung aussieht.  Details unten. </p><br><p><img src="https://habrastorage.org/webt/vk/2o/aw/vk2oawuj3cgahv3v3i7ockh_fbe.png"></p><br><p>  Um die Terraform-Bereitstellung zu starten, rufen Sie das Verzeichnis Inventar / $ CLUSTER / auf und f√ºhren Sie die folgenden Befehle aus.  Installieren Sie zuerst die erforderlichen Plugins.  Dazu ben√∂tigen wir das init-Argument und den Pfad zu den Plugins. </p><br><pre> <code class="plaintext hljs">terraform init ../../contrib/terraform/openstack</code> </pre> <br><p>  Dieser Vorgang ist sehr schnell.  Terraform ist jetzt bereit, die Infrastruktur mit dem Argument apply bereitzustellen. </p><br><pre> <code class="plaintext hljs">terraform apply -var-file=cluster.tf ../../contrib/terraform/openstack</code> </pre> <br><p>  Nach einigen Sekunden sollte Terraform ein √§hnliches Ergebnis anzeigen, und Instanzen stehen f√ºr die Arbeit zur Verf√ºgung. </p><br><pre> <code class="plaintext hljs">Apply complete! Resources: 3 added, 0 changed, 0 destroyed.</code> </pre> <br><p>  F√ºhren Sie den folgenden Befehl Ansible aus, um die Serververf√ºgbarkeit zu √ºberpr√ºfen. Anschlie√üend wechseln wir in den Stammordner des Repositorys. </p><br><pre> <code class="plaintext hljs">$ ansible -i inventory/$CLUSTER/hosts -m ping all example-k8s_node-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" } example-etcd-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" } example-k8s-master-1 | SUCCESS =&gt; { "changed": false, "ping": "pong" }</code> </pre> <br><h2 id="razvertyvanie-klastera-kubernetes">  Kubernetes Cluster-Bereitstellung </h2><br><p>  Die Infrastruktur wird bereitgestellt. Jetzt m√ºssen Sie den Kubernetes-Cluster installieren.  Konfigurieren Sie zun√§chst die Clustervariablen in der Datei Inventar / $ CLUSTER / group_vars / all / all.yml.  Hier m√ºssen Sie den Wert von cloud_provider auf openstack und f√ºr bin_dir auf den Pfad setzen, in dem die Dateien installiert werden.  In unserem Beispiel verwenden wir die folgende Konfiguration. </p><br><pre> <code class="plaintext hljs">## Directory where etcd data stored etcd_data_dir: /var/lib/etcd ## Directory where the binaries will be installed bin_dir: /usr/local/bin ## The access_ip variable is used to define how other nodes should access ## the node. This is used in flannel to allow other flannel nodes to see ## this node for example. The access_ip is really useful AWS and Google ## environments where the nodes are accessed remotely by the "public" ip, ## but don't know about that address themselves. #access_ip: 1.1.1.1 ## External LB example config ## apiserver_loadbalancer_domain_name: "elb.some.domain" #loadbalancer_apiserver: # address: 1.2.3.4 # port: 1234 ## Internal loadbalancers for apiservers #loadbalancer_apiserver_localhost: true ## Local loadbalancer should use this port instead, if defined. ## Defaults to kube_apiserver_port (6443) #nginx_kube_apiserver_port: 8443 ### OTHER OPTIONAL VARIABLES ## For some things, kubelet needs to load kernel modules. For example, dynamic kernel services are needed ## for mounting persistent volumes into containers. These may not be loaded by preinstall kubernetes ## processes. For example, ceph and rbd backed volumes. Set to true to allow kubelet to load kernel ## modules. #kubelet_load_modules: false ## Upstream dns servers used by dnsmasq #upstream_dns_servers: # - 8.8.8.8 # - 8.8.4.4 ## There are some changes specific to the cloud providers ## for instance we need to encapsulate packets with some network plugins ## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external' ## When openstack is used make sure to source in the openstack credentials ## like you would do when using nova-client before starting the playbook. ## Note: The 'external' cloud provider is not supported. ## TODO(riverzhang): https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager cloud_provider: openstack ## Set these proxy values in order to update package manager and docker daemon to use proxies #http_proxy: "" #https_proxy: "" ## Refer to roles/kubespray-defaults/defaults/main.yml before modifying no_proxy #no_proxy: "" ## Some problems may occur when downloading files over https proxy due to ansible bug ## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable ## SSL validation of get_url module. Note that kubespray will still be performing checksum validation. #download_validate_certs: False ## If you need exclude all cluster nodes from proxy and other resources, add other resources here. #additional_no_proxy: "" ## Certificate Management ## This setting determines whether certs are generated via scripts. ## Chose 'none' if you provide your own certificates. ## Option is "script", "none" ## note: vault is removed #cert_management: script ## Set to true to allow pre-checks to fail and continue deployment #ignore_assert_errors: false ## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable. #kube_read_only_port: 10255 ## Set true to download and cache container download_container: false ## Deploy container engine # Set false if you want to deploy container engine manually. #deploy_container_engine: true ## Set Pypi repo and cert accordingly #pyrepo_index: https://pypi.example.com/simple #pyrepo_cert: /etc/ssl/certs/ca-certificates.crt</code> </pre> <br><p>  Konfigurieren Sie nun die Datei Inventar / $ CLUSTER / group_vars / k8s-cluster / k8s-cluster.yml.  Legen Sie f√ºr die Variable kube_network_plugin Flanell oder Kaliko fest (Sie m√ºssen <a href="">OpenStack Neutron-Ports konfigurieren</a> ).  Wir werden diesen Flanell haben, der nicht konfiguriert werden muss.  Legen Sie f√ºr die Variable resolvconf_mode docker_dns fest.  Dieser Wert weist Kubespray an, die <a href="">Parameter des Docker-D√§mons</a> festzulegen <a href="">.</a>  Unten sehen Sie eine Beispielkonfiguration f√ºr unseren Cluster. </p><br><pre> <code class="plaintext hljs"># Kubernetes configuration dirs and system namespace. # Those are where all the additional config stuff goes # the kubernetes normally puts in /srv/kubernetes. # This puts them in a sane location and namespace. # Editing those values will almost surely break something. kube_config_dir: /etc/kubernetes kube_script_dir: "{{ bin_dir }}/kubernetes-scripts" kube_manifest_dir: "{{ kube_config_dir }}/manifests" # This is where all the cert scripts and certs will be located kube_cert_dir: "{{ kube_config_dir }}/ssl" # This is where all of the bearer tokens will be stored kube_token_dir: "{{ kube_config_dir }}/tokens" # This is where to save basic auth file kube_users_dir: "{{ kube_config_dir }}/users" kube_api_anonymous_auth: true ## Change this to use another Kubernetes version, eg a current beta release kube_version: v1.13.3 # kubernetes image repo define kube_image_repo: "gcr.io/google-containers" # Where the binaries will be downloaded. # Note: ensure that you've enough disk space (about 1G) local_release_dir: "/tmp/releases" # Random shifts for retrying failed ops like pushing/downloading retry_stagger: 5 # This is the group that the cert creation scripts chgrp the # cert files to. Not really changeable... kube_cert_group: kube-cert # Cluster Loglevel configuration kube_log_level: 2 # Directory where credentials will be stored credentials_dir: "{{ inventory_dir }}/credentials" # Users to create for basic auth in Kubernetes API via HTTP # Optionally add groups for user kube_api_pwd: "{{ lookup('password', credentials_dir + '/kube_user.creds length=15 chars=ascii_letters,digits') }}" kube_users: kube: pass: "{{kube_api_pwd}}" role: admin groups: - system:masters ## It is possible to activate / deactivate selected authentication methods (basic auth, static token auth) #kube_oidc_auth: false #kube_basic_auth: false #kube_token_auth: false ## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/ ## To use OpenID you have to deploy additional an OpenID Provider (eg Dex, Keycloak, ...) # kube_oidc_url: https:// ... # kube_oidc_client_id: kubernetes ## Optional settings for OIDC # kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem" # kube_oidc_username_claim: sub # kube_oidc_username_prefix: oidc: # kube_oidc_groups_claim: groups # kube_oidc_groups_prefix: oidc: # Choose network plugin (cilium, calico, contiv, weave or flannel) # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing kube_network_plugin: flannel # Setting multi_networking to true will install Multus: https://github.com/intel/multus-cni kube_network_plugin_multus: false # Kubernetes internal network for services, unused block of space. kube_service_addresses: 10.233.0.0/18 # internal network. When used, it will assign IP # addresses from this range to individual pods. # This network must be unused in your network infrastructure! kube_pods_subnet: 10.233.64.0/18 # internal network node size allocation (optional). This is the size allocated # to each node on your network. With these defaults you should have # room for 4096 nodes with 254 pods per node. kube_network_node_prefix: 24 # The port the API Server will be listening on. kube_apiserver_ip: "{{ kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') }}" kube_apiserver_port: 6443 # (https) #kube_apiserver_insecure_port: 8080 # (http) # Set to 0 to disable insecure port - Requires RBAC in authorization_modes and kube_api_anonymous_auth: true kube_apiserver_insecure_port: 0 # (disabled) # Kube-proxy proxyMode configuration. # Can be ipvs, iptables kube_proxy_mode: ipvs # A string slice of values which specify the addresses to use for NodePorts. # Values may be valid IP blocks (eg 1.2.3.0/24, 1.2.3.4/32). # The default empty string slice ([]) means to use all local addresses. # kube_proxy_nodeport_addresses_cidr is retained for legacy config kube_proxy_nodeport_addresses: &gt;- {%- if kube_proxy_nodeport_addresses_cidr is defined -%} [{{ kube_proxy_nodeport_addresses_cidr }}] {%- else -%} [] {%- endif -%} # If non-empty, will use this string as identification instead of the actual hostname #kube_override_hostname: &gt;- # {%- if cloud_provider is defined and cloud_provider in [ 'aws' ] -%} # {%- else -%} # {{ inventory_hostname }} # {%- endif -%} ## Encrypting Secret Data at Rest (experimental) kube_encrypt_secret_data: false # DNS configuration. # Kubernetes cluster name, also will be used as DNS domain cluster_name: cluster.local # Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods ndots: 2 # Can be dnsmasq_kubedns, kubedns, coredns, coredns_dual, manual or none dns_mode: coredns # Set manual server if using a custom cluster DNS server #manual_dns_server: 10.xxx # Enable nodelocal dns cache enable_nodelocaldns: False nodelocaldns_ip: 169.254.25.10 # Can be docker_dns, host_resolvconf or none resolvconf_mode: docker_dns # Deploy netchecker app to verify DNS resolve as an HTTP service deploy_netchecker: false # Ip address of the kubernetes skydns service skydns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') }}" skydns_server_secondary: "{{ kube_service_addresses|ipaddr('net')|ipaddr(4)|ipaddr('address') }}" dnsmasq_dns_server: "{{ kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') }}" dns_domain: "{{ cluster_name }}" ## Container runtime ## docker for docker and crio for cri-o. container_manager: docker ## Settings for containerized control plane (etcd/kubelet/secrets) etcd_deployment_type: docker kubelet_deployment_type: host helm_deployment_type: host # K8s image pull policy (imagePullPolicy) k8s_image_pull_policy: IfNotPresent # audit log for kubernetes kubernetes_audit: false # dynamic kubelet configuration dynamic_kubelet_configuration: false # define kubelet config dir for dynamic kubelet #kubelet_config_dir: default_kubelet_config_dir: "{{ kube_config_dir }}/dynamic_kubelet_dir" dynamic_kubelet_configuration_dir: "{{ kubelet_config_dir | default(default_kubelet_config_dir) }}" # pod security policy (RBAC must be enabled either by having 'RBAC' in authorization_modes or kubeadm enabled) podsecuritypolicy_enabled: false # Make a copy of kubeconfig on the host that runs Ansible in {{ inventory_dir }}/artifacts # kubeconfig_localhost: false # Download kubectl onto the host that runs Ansible in {{ bin_dir }} # kubectl_localhost: false # dnsmasq # dnsmasq_upstream_dns_servers: # - /resolvethiszone.with/10.0.4.250 # - 8.8.8.8 # Enable creation of QoS cgroup hierarchy, if true top level QoS and pod cgroups are created. (default true) # kubelet_cgroups_per_qos: true # A comma separated list of levels of node allocatable enforcement to be enforced by kubelet. # Acceptable options are 'pods', 'system-reserved', 'kube-reserved' and ''. Default is "". # kubelet_enforce_node_allocatable: pods ## Supplementary addresses that can be added in kubernetes ssl keys. ## That can be useful for example to setup a keepalived virtual IP # supplementary_addresses_in_ssl_keys: [10.0.0.1, 10.0.0.2, 10.0.0.3] ## Running on top of openstack vms with cinder enabled may lead to unschedulable pods due to NoVolumeZoneConflict restriction in kube-scheduler. ## See https://github.com/kubernetes-sigs/kubespray/issues/2141 ## Set this variable to true to get rid of this issue volume_cross_zone_attachment: false # Add Persistent Volumes Storage Class for corresponding cloud provider ( OpenStack is only supported now ) persistent_volumes_enabled: false ## Container Engine Acceleration ## Enable container acceleration feature, for example use gpu acceleration in containers # nvidia_accelerator_enabled: true ## Nvidia GPU driver install. Install will by done by a (init) pod running as a daemonset. ## Important: if you use Ubuntu then you should set in all.yml 'docker_storage_options: -s overlay2' ## Array with nvida_gpu_nodes, leave empty or comment if you dont't want to install drivers. ## Labels and taints won't be set to nodes if they are not in the array. # nvidia_gpu_nodes: # - kube-gpu-001 # nvidia_driver_version: "384.111" ## flavor can be tesla or gtx # nvidia_gpu_flavor: gtx</code> </pre> <br><p>  Bearbeiten Sie abschlie√üend die Datei Inventar / $ CLUSTER / group_vars / k8s-cluster / addons.yml und setzen Sie dashboard_enabled auf true, um das Dashboard einzurichten.  Konfigurationsbeispiel: </p><br><pre> <code class="plaintext hljs"># Kubernetes dashboard # RBAC required. see docs/getting-started.md for access details. dashboard_enabled: true # Helm deployment helm_enabled: false # Registry deployment registry_enabled: false # registry_namespace: kube-system # registry_storage_class: "" # registry_disk_size: "10Gi" # Metrics Server deployment metrics_server_enabled: false # metrics_server_kubelet_insecure_tls: true # metrics_server_metric_resolution: 60s # metrics_server_kubelet_preferred_address_types: "InternalIP" # Local volume provisioner deployment local_volume_provisioner_enabled: false # local_volume_provisioner_namespace: kube-system # local_volume_provisioner_storage_classes: # local-storage: # host_dir: /mnt/disks # mount_dir: /mnt/disks # fast-disks: # host_dir: /mnt/fast-disks # mount_dir: /mnt/fast-disks # block_cleaner_command: # - "/scripts/shred.sh" # - "2" # volume_mode: Filesystem # fs_type: ext4 # CephFS provisioner deployment cephfs_provisioner_enabled: false # cephfs_provisioner_namespace: "cephfs-provisioner" # cephfs_provisioner_cluster: ceph # cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789" # cephfs_provisioner_admin_id: admin # cephfs_provisioner_secret: secret # cephfs_provisioner_storage_class: cephfs # cephfs_provisioner_reclaim_policy: Delete # cephfs_provisioner_claim_root: /volumes # cephfs_provisioner_deterministic_names: true # Nginx ingress controller deployment ingress_nginx_enabled: false # ingress_nginx_host_network: false # ingress_nginx_nodeselector: # node.kubernetes.io/node: "" # ingress_nginx_tolerations: # - key: "node.kubernetes.io/master" # operator: "Equal" # value: "" # effect: "NoSchedule" # ingress_nginx_namespace: "ingress-nginx" # ingress_nginx_insecure_port: 80 # ingress_nginx_secure_port: 443 # ingress_nginx_configmap: # map-hash-bucket-size: "128" # ssl-protocols: "SSLv2" # ingress_nginx_configmap_tcp_services: # 9000: "default/example-go:8080" # ingress_nginx_configmap_udp_services: # 53: "kube-system/kube-dns:53" # Cert manager deployment cert_manager_enabled: false # cert_manager_namespace: "cert-manager"</code> </pre> <br><p>  F√ºhren Sie nach dem √Ñndern der Konfiguration ansible-playbook mit unserer Konfiguration aus, indem Sie den folgenden Befehl ausf√ºhren. </p><br><pre> <code class="plaintext hljs">ansible-playbook --become -i inventory/$CLUSTER/hosts cluster.yml</code> </pre> <br><p>  Ansible f√ºhrt mehrere Vorg√§nge aus. Wenn alle erfolgreich abgeschlossen wurden, sieht der Cluster wie in dieser Abbildung aus. </p><br><p><img src="https://habrastorage.org/webt/2d/sg/tz/2dsgtzytttaugaufa1aolqy06wy.png"></p><br><h2 id="testirovanie">  Testen </h2><br><p>  Um den Cluster zu testen, rufen Sie den Assistenten auf, wechseln Sie zum Root-Benutzer und f√ºhren Sie in kubectl den Befehl kubectl cluster-info aus, um Informationen zum Cluster abzurufen.  Sie sehen Informationen zum Endpunkt des Assistenten und der Dienste im Cluster.  Wenn mit dem Cluster alles in Ordnung ist, erstellen Sie den Kubernetes-Dashboard-Benutzer mit den folgenden Befehlen. </p><br><pre> <code class="plaintext hljs"># Create service account kubectl create serviceaccount cluster-admin-dashboard-sa # Bind ClusterAdmin role to the service account kubectl create clusterrolebinding cluster-admin-dashboard-sa \ --clusterrole=cluster-admin \ --serviceaccount=default:cluster-admin-dashboard-sa # Parse the token kubectl describe secret $(kubectl -n kube-system get secret | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}'</code> </pre> <br><p>  Jetzt k√∂nnen Sie das Dashboard mit dem Token aufrufen.  Zun√§chst m√ºssen Sie einen Tunnel zum Kubernetes-Assistenten erstellen, da das Dashboard f√ºr localhost an Port 8001 noch ge√∂ffnet ist. Danach k√∂nnen Sie √ºber die URL localhost: 8001 auf das Dashboard zugreifen.  W√§hlen Sie nun Token, geben Sie das Token ein und melden Sie sich an. </p><br><p><img src="https://habrastorage.org/webt/vr/gh/k2/vrghk2vujok6oexyyg-reeygrwk.png"></p><br><p>  Sie k√∂nnen jetzt mit der Arbeit im Kubernetes-Cluster beginnen.  In diesem Artikel haben Sie gesehen, wie einfach es ist, einen Kubernetes-Cluster in der OpenStack-Cloud bereitzustellen und zu konfigurieren. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456792/">https://habr.com/ru/post/de456792/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456772/index.html">M & A f√ºr Projektteams: Wie k√∂nnen Projektdaten und -prozesse effektiv verwaltet werden?</a></li>
<li><a href="../de456774/index.html">Was ich in Java nach der Arbeit mit Kotlin / Scala vermisse</a></li>
<li><a href="../de456780/index.html">9 M√∂glichkeiten, die Effizienz des Entwicklers mobiler Apps zu steigern</a></li>
<li><a href="../de456782/index.html">Modellorientiertes Design - wie man Tschernobyl nicht wiederholt</a></li>
<li><a href="../de456790/index.html">PostgreSQL News Digest. Ausgabe Nr. 16</a></li>
<li><a href="../de456794/index.html">Web-UI-Architektur: Eine h√∂lzerne Vergangenheit, eine seltsame Gegenwart und eine gl√§nzende Zukunft</a></li>
<li><a href="../de456796/index.html">Svalbard - neuer Name f√ºr das Projekt Have I Been Pwned vor dem Verkauf</a></li>
<li><a href="../de456798/index.html">SDL 2 Tutorials: Lektion 5 - Texturen</a></li>
<li><a href="../de456804/index.html">Folgen Sie dem Geld: Wie die RTM-Gruppe begann, die Adressen von C & C-Servern in einer Krypto-Brieftasche zu verstecken</a></li>
<li><a href="../de456806/index.html">Ein Bot aus allen Sorgen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>