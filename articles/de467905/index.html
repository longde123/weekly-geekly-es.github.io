<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∂üèø üë©üèª‚Äçüíª üë©üèæ‚Äçüåæ Wie wir in Cloud Mail.ru Meilensteine ‚Äã‚Äãerkannt haben und warum üë∞ üòÉ üí¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mit dem Aufkommen von Mobiltelefonen mit hochwertigen Kameras haben wir begonnen, immer mehr Bilder und Videos von hellen und unvergesslichen Momenten...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir in Cloud Mail.ru Meilensteine ‚Äã‚Äãerkannt haben und warum</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/467905/"><img src="https://habrastorage.org/webt/vz/bg/ov/vzbgovf2e5gessaexiyl0rnhvay.jpeg"><br><br>  Mit dem Aufkommen von Mobiltelefonen mit hochwertigen Kameras haben wir begonnen, immer mehr Bilder und Videos von hellen und unvergesslichen Momenten in unserem Leben zu machen.  Viele von uns haben Fotoarchive, die sich √ºber Jahrzehnte erstrecken und Tausende von Bildern umfassen, wodurch die Navigation immer schwieriger wird.  Denken Sie daran, wie lange es vor einigen Jahren gedauert hat, ein interessantes Bild zu finden. <br><br>  Eines der Ziele von Mail.ru Cloud ist es, die einfachsten Mittel f√ºr den Zugriff auf und die Suche in Ihren eigenen Foto- und Videoarchiven bereitzustellen.  Zu diesem Zweck haben wir vom Mail.ru Computer Vision Team Systeme f√ºr die intelligente Bildverarbeitung erstellt und implementiert: Suche nach Objekt, Szene, Gesicht usw.  Eine weitere spektakul√§re Technologie ist die Erkennung von Meilensteinen.  Heute werde ich Ihnen erz√§hlen, wie wir dies mit Deep Learning verwirklicht haben. <br><a name="habracut"></a><br>  Stellen Sie sich die Situation vor: Sie kehren mit einer Menge Fotos von Ihrem Urlaub zur√ºck.  Wenn Sie mit Ihren Freunden sprechen, werden Sie gebeten, ein Bild eines sehenswerten Ortes wie Palast, Burg, Pyramide, Tempel, See, Wasserfall, Berg usw. zu zeigen.  Sie beeilen sich, durch Ihren Galerieordner zu scrollen und versuchen, einen wirklich guten zu finden.  H√∂chstwahrscheinlich geht es unter Hunderten von Bildern verloren, und Sie sagen, Sie werden es sp√§ter zeigen. <br><br>  Wir l√∂sen dieses Problem, indem wir Benutzerfotos in Alben gruppieren.  So finden Sie mit wenigen Klicks die ben√∂tigten Bilder.  Jetzt haben wir Alben zusammengestellt nach Gesicht, Objekt und Szene sowie nach Wahrzeichen. <br><br>  Fotos mit Sehensw√ºrdigkeiten sind unerl√§sslich, da sie h√§ufig H√∂hepunkte unseres Lebens festhalten (z. B. Reisen).  Dies k√∂nnen Bilder mit Architektur oder Wildnis im Hintergrund sein.  Aus diesem Grund versuchen wir, solche Bilder zu finden und den Benutzern zur Verf√ºgung zu stellen. <br><br><h2>  Besonderheiten der Landmarkerkennung </h2><br>  Hier gibt es eine Nuance: Man lehrt ein Modell nicht nur und l√§sst es Landmarken sofort erkennen - es gibt eine Reihe von Herausforderungen. <br><br>  Erstens k√∂nnen wir nicht klar sagen, was ein ‚ÄûWahrzeichen‚Äú wirklich ist.  Wir k√∂nnen nicht sagen, warum ein Geb√§ude ein Wahrzeichen ist, ein anderes daneben nicht.  Es ist kein formalisiertes Konzept, was die Angabe der Erkennungsaufgabe erschwert. <br><br>  Zweitens sind Sehensw√ºrdigkeiten unglaublich vielf√§ltig.  Dies k√∂nnen Geb√§ude von historischem oder kulturellem Wert sein, wie ein Tempel, ein Palast oder eine Burg.  Alternativ k√∂nnen dies alle Arten von Denkm√§lern sein.  Oder nat√ºrliche Merkmale: Seen, Schluchten, Wasserf√§lle und so weiter.  Es gibt auch ein einziges Modell, das alle diese Orientierungspunkte finden sollte. <br><br>  Drittens gibt es nur sehr wenige Bilder mit Orientierungspunkten.  Nach unseren Sch√§tzungen machen sie nur 1 bis 3 Prozent der Benutzerfotos aus.  Deshalb k√∂nnen wir es uns nicht leisten, Fehler bei der Erkennung zu machen, denn wenn wir jemandem ein Foto ohne Orientierungspunkt zeigen, ist dies ziemlich offensichtlich und f√ºhrt zu einer negativen Reaktion.  Oder stellen Sie sich umgekehrt vor, Sie zeigen einer Person, die noch nie in den USA war, ein Bild mit einem interessanten Ort in New York.  Daher sollte das Erkennungsmodell einen niedrigen FPR (False Positive Rate) aufweisen. <br><br>  Viertens deaktivieren etwa 50% der Benutzer oder noch h√§ufiger das Speichern von Geodaten.  Wir m√ºssen dies ber√ºcksichtigen und nur das Bild selbst verwenden, um den Ort zu identifizieren.  Heutzutage verwenden die meisten Dienste, die in der Lage sind, Orientierungspunkte zu verarbeiten, Geodaten aus Bildeigenschaften.  Unsere anf√§nglichen Anforderungen waren jedoch strenger. <br><br>  Lassen Sie mich nun einige Beispiele zeigen. <br><br>  Hier sind drei gleichartige Objekte, drei gotische Kathedralen in Frankreich.  Auf der linken Seite befindet sich die Kathedrale von Amiens, in der Mitte die Kathedrale von Reims und auf der rechten Seite Notre-Dame de Paris. <br><br><img src="https://habrastorage.org/webt/bh/4f/ej/bh4fejtind2ngdha-cgxdgkqf9g.jpeg"><br><br>  Selbst ein Mensch braucht einige Zeit, um genau hinzuschauen und festzustellen, dass es sich um verschiedene Kathedralen handelt, aber der Motor sollte in der Lage sein, dasselbe zu tun und sogar schneller als ein Mensch. <br><br>  Hier ist eine weitere Herausforderung: Alle drei Fotos hier zeigen Notre-Dame de Paris aus verschiedenen Blickwinkeln.  Die Fotos sind sehr unterschiedlich, m√ºssen aber noch erkannt und abgerufen werden. <br><br><img src="https://habrastorage.org/webt/vx/de/wr/vxdewr0j1vdlm8knfq2_z_6n95u.jpeg"><br><br>  Nat√ºrliche Merkmale unterscheiden sich grundlegend von der Architektur.  Links ist Caesarea in Israel, rechts der Englische Garten in M√ºnchen. <br><br><img src="https://habrastorage.org/webt/y6/6o/mb/y66ombyco0nzwj3ghhjumufaiz4.jpeg"><br><br>  Diese Fotos geben dem Modell nur sehr wenige Hinweise. <br><br><h2>  Unsere Methode </h2><br>  Unsere Methode basiert vollst√§ndig auf tiefen Faltungs-Neuronalen Netzen.  Die Trainingsstrategie, die wir gew√§hlt haben, war das sogenannte Curriculum-Lernen, dh das Lernen in mehreren Schritten.  Um eine h√∂here Effizienz sowohl mit als auch ohne verf√ºgbare Geodaten zu erzielen, haben wir eine spezifische Schlussfolgerung gezogen.  Lassen Sie mich Ihnen jeden Schritt genauer erl√§utern. <br><br><h2>  Datensatz </h2><br>  Daten sind der Treibstoff des maschinellen Lernens.  Zuerst mussten wir den Datensatz zusammenstellen, um das Modell zu lehren. <br><br>  Wir haben die Welt in 4 Regionen unterteilt, die jeweils in einem bestimmten Schritt des Lernprozesses verwendet werden.  Dann haben wir L√§nder in jeder Region ausgew√§hlt, eine Liste von St√§dten f√ºr jedes Land ausgew√§hlt und eine Reihe von Fotos gesammelt.  Nachfolgend einige Beispiele. <br><br><img src="https://habrastorage.org/webt/cm/al/en/cmalenos8kpuchcb7ridv6m5rge.jpeg"><br><br>  Zuerst haben wir versucht, unser Modell aus der erhaltenen Datenbank lernen zu lassen.  Die Ergebnisse waren schlecht.  Unsere Analyse ergab, dass die Daten verschmutzt waren.  Es gab zu viel L√§rm, der die Erkennung jedes Orientierungspunkts st√∂rte.  Was sollten wir tun?  Es w√§re teuer, umst√§ndlich und nicht zu klug, den gesamten Datenbestand manuell zu √ºberpr√ºfen.  Daher haben wir ein Verfahren zur automatischen Datenbankbereinigung entwickelt, bei dem die manuelle Handhabung nur in einem Schritt verwendet wird: Wir haben 3 bis 5 Referenzfotos f√ºr jeden Orientierungspunkt ausgew√§hlt, die das gew√ºnschte Objekt definitiv in einem mehr oder weniger geeigneten Winkel zeigten.  Es funktioniert schnell genug, da die Menge solcher Referenzdaten im Vergleich zur gesamten Datenbank gering ist.  Dann wird eine automatische Reinigung basierend auf tiefen Faltungs-Neuronalen Netzen durchgef√ºhrt. <br><br>  Weiter werde ich den Begriff "Einbettung" verwenden, womit ich Folgendes meine.  Wir haben ein Faltungsnetzwerk.  Wir haben es trainiert, um Objekte zu klassifizieren, dann haben wir die letzte Klassifizierungsebene abgeschnitten, einige Bilder ausgew√§hlt, sie vom Netzwerk analysieren lassen und am Ausgang einen numerischen Vektor erhalten.  Das nenne ich Einbettung. <br><br>  Wie ich bereits sagte, haben wir unseren Lernprozess in mehreren Schritten angeordnet, die Teilen unserer Datenbank entsprechen.  Also nehmen wir zuerst entweder das neuronale Netzwerk aus dem vorhergehenden Schritt oder das Initialisierungsnetzwerk. <br><br>  Wir haben Referenzfotos eines Wahrzeichens, verarbeiten sie vom Netzwerk und erhalten mehrere Einbettungen.  Jetzt k√∂nnen wir mit der Datenbereinigung fortfahren.  Wir nehmen alle Bilder aus dem Datensatz f√ºr das Wahrzeichen auf und lassen jedes Bild auch vom Netzwerk verarbeiten.  Wir erhalten einige Einbettungen und bestimmen f√ºr jede die Entfernung zu Referenz-Einbettungen.  Dann bestimmen wir die durchschnittliche Entfernung und behandeln das Objekt als Nicht-Landmark, wenn es einen Schwellenwert √ºberschreitet, der ein Parameter des Algorithmus ist.  Wenn der durchschnittliche Abstand unter dem Schwellenwert liegt, behalten wir das Foto. <br><br><img src="https://habrastorage.org/webt/cl/s7/xu/cls7xusqgmt6mmx5uoyksvkvg4s.jpeg"><br><br>  Als Ergebnis hatten wir eine Datenbank, die √ºber 11.000 Sehensw√ºrdigkeiten aus √ºber 500 St√§dten in 70 L√§ndern enthielt, mehr als 2,3 Millionen Fotos.  Denken Sie daran, dass der gr√∂√üte Teil der Fotos √ºberhaupt keine Orientierungspunkte aufweist.  Wir m√ºssen es unseren Modellen irgendwie erz√§hlen.  Aus diesem Grund haben wir unserer Datenbank 900.000 Fotos ohne Orientierungspunkte hinzugef√ºgt und unser Modell mit dem resultierenden Datensatz trainiert. <br><br>  Wir haben einen Offline-Test eingef√ºhrt, um die Lernqualit√§t zu messen.  Da Landmarken nur in 1 bis 3% aller Fotos vorkommen, haben wir manuell einen Satz von 290 Bildern zusammengestellt, die eine Landmarke zeigten.  Diese Fotos waren sehr vielf√§ltig und komplex, mit einer gro√üen Anzahl von Objekten, die aus verschiedenen Winkeln aufgenommen wurden, um den Test f√ºr das Modell so schwierig wie m√∂glich zu machen.  Nach dem gleichen Muster haben wir 11.000 Fotos ohne Orientierungspunkte ausgew√§hlt, was ebenfalls ziemlich kompliziert ist, und wir haben versucht, Objekte zu finden, die den Orientierungspunkten in unserer Datenbank sehr √§hnlich sind. <br><br>  Um die Lernqualit√§t zu bewerten, messen wir die Genauigkeit unseres Modells anhand von Fotos mit und ohne Orientierungspunkte.  Dies sind unsere beiden Hauptmetriken. <br><br><h2>  Bestehende Ans√§tze </h2><br>  In der Literatur gibt es relativ wenige Informationen zur Erkennung von Orientierungspunkten.  Die meisten L√∂sungen basieren auf lokalen Funktionen.  Die Hauptidee ist, dass wir ein Abfragebild und ein Bild aus der Datenbank haben.  Lokale Merkmale - Schl√ºsselpunkte - werden gefunden und dann abgeglichen.  Wenn die Anzahl der √úbereinstimmungen gro√ü genug ist, schlie√üen wir, dass wir einen Orientierungspunkt gefunden haben. <br><br>  Derzeit ist die beste Methode DELF (Deep Local Features) von Google, bei der lokale Features mit Deep Learning kombiniert werden.  Indem ein Eingabebild vom Faltungsnetzwerk verarbeitet wird, erhalten wir einige DELF-Merkmale. <br><br><img src="https://habrastorage.org/webt/i9/-5/g-/i9-5g-dj0fkjpxlgnjpwaadwyec.jpeg"><br><br>  Wie funktioniert die Erkennung von Orientierungspunkten?  Wir haben eine Reihe von Fotos und ein Eingabebild und m√∂chten wissen, ob es eine Landmarke zeigt oder nicht.  Durch Ausf√ºhren des DELF-Netzwerks aller Fotos k√∂nnen entsprechende Funktionen f√ºr die Datenbank und das Eingabebild erhalten werden.  Dann f√ºhren wir eine Suche nach der Methode des n√§chsten Nachbarn durch und erhalten Kandidatenbilder mit Merkmalen am Ausgang.  Wir verwenden eine geometrische √úberpr√ºfung, um die Merkmale abzugleichen: Wenn dies erfolgreich ist, schlie√üen wir, dass das Bild einen Orientierungspunkt zeigt. <br><br><h2>  Faltungs-Neuronales Netzwerk </h2><br>  Das Pre-Training ist entscheidend f√ºr Deep Learning.  Deshalb haben wir eine Datenbank mit Szenen verwendet, um unser neuronales Netzwerk vorab zu trainieren.  Warum so?  Eine Szene ist ein Mehrfachobjekt, das eine gro√üe Anzahl anderer Objekte umfasst.  Landmark ist eine Instanz einer Szene.  Indem wir das Modell mit einer solchen Datenbank vorab trainieren, k√∂nnen wir ihm eine Vorstellung von einigen Funktionen auf niedriger Ebene geben, die dann f√ºr eine erfolgreiche Erkennung von Orientierungspunkten verallgemeinert werden k√∂nnen. <br><br>  Wir haben ein neuronales Netzwerk aus der Residual-Netzwerkfamilie als Modell verwendet.  Der entscheidende Unterschied solcher Netzwerke besteht darin, dass sie einen Restblock verwenden, der eine Sprungverbindung enth√§lt, die es einem Signal erm√∂glicht, √ºber Schichten mit Gewichten zu springen und frei zu passieren.  Eine solche Architektur erm√∂glicht es, tiefe Netzwerke mit einem hohen Ma√ü an Qualit√§t zu trainieren und verschwindende Gradienteneffekte zu kontrollieren, was f√ºr das Training wesentlich ist. <br><br>  Unser Modell ist Wide ResNet-50-2, eine Version von ResNet-50, bei der die Anzahl der Windungen im internen Engpassblock verdoppelt wird. <br><br><img src="https://habrastorage.org/webt/s0/mg/ra/s0mgravn9tobelwyraas7v-umfi.jpeg"><br><br>  Das Netzwerk funktioniert sehr gut.  Wir haben es mit unserer Szenendatenbank getestet und hier sind die Ergebnisse: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Modell <br></th><th>  Top 1 err <br></th><th>  Top 5 err <br></th></tr><tr><td>  ResNet-50 <br></td><td>  46,1% <br></td><td>  15,7% <br></td></tr><tr><td>  ResNet-200 <br></td><td>  42,6% <br></td><td>  12,9% <br></td></tr><tr><td>  SE-ResNext-101 <br></td><td>  42% <br></td><td>  12,1% <br></td></tr><tr><td>  WRN-50-2 (schnell!) <br></td><td>  41,8% <br></td><td>  11,8% <br></td></tr></tbody></table></div><br>  Wide ResNet arbeitete fast doppelt so schnell wie ResNet-200.  Schlie√ülich ist die Laufgeschwindigkeit entscheidend f√ºr die Produktion.  Angesichts all dieser √úberlegungen haben wir Wide ResNet-50-2 als unser wichtigstes neuronales Netzwerk ausgew√§hlt. <br><br><h2>  Schulung </h2><br>  Wir brauchen eine Verlustfunktion, um unser Netzwerk zu trainieren.  Wir haben uns f√ºr den metrischen Lernansatz entschieden: Ein neuronales Netzwerk wird so trainiert, dass Elemente derselben Klasse zu einem Cluster str√∂men, w√§hrend Cluster f√ºr verschiedene Klassen so weit wie m√∂glich voneinander entfernt sein sollen.  F√ºr Orientierungspunkte haben wir den Center-Verlust verwendet, der Elemente einer Klasse in Richtung eines Centers zieht.  Ein wichtiges Merkmal dieses Ansatzes ist, dass keine negative Abtastung erforderlich ist, was in sp√§teren Epochen ziemlich schwierig wird. <br><br><img src="https://habrastorage.org/webt/ix/xd/if/ixxdifizq_hbvfbz6vloiqc_ppk.jpeg"><br><br>  Denken Sie daran, dass wir n Klassen von Orientierungspunkten und eine weitere Klasse von ‚ÄûNicht-Orientierungspunkten‚Äú haben, f√ºr die der Verlust des Zentrums nicht verwendet wird.  Wir implizieren, dass ein Orientierungspunkt ein und dasselbe Objekt ist und eine Struktur hat. Daher ist es sinnvoll, sein Zentrum zu bestimmen.  Nicht-Landmarken k√∂nnen sich auf alles beziehen, daher ist es nicht sinnvoll, das Zentrum daf√ºr zu bestimmen. <br><br>  Wir setzen das alles dann zusammen und es gibt unser Modell f√ºr das Training.  Es besteht aus drei Hauptteilen: <br><br><ul><li>  Wide ResNet 50-2 Faltungs-Neuronales Netzwerk mit einer Datenbank von Szenen vorab trainiert; </li><li>  Einbettungsteil, umfassend eine vollst√§ndig verbundene Schicht und eine Chargennormschicht; </li><li>  Klassifikator, der eine vollst√§ndig verbundene Schicht ist, gefolgt von einem Paar aus Softmax-Verlust und Center-Verlust. </li></ul><br><img src="https://habrastorage.org/webt/pt/k0/g_/ptk0g_0cyy3qgfw0wp8d4zqfd3k.jpeg"><br><br>  Wie Sie sich erinnern, ist unsere Datenbank nach Regionen in 4 Teile unterteilt.  Wir verwenden diese 4 Teile in einem Lehrplan-Lernparadigma.  Wir haben einen aktuellen Datensatz und f√ºgen in jeder Lernphase einen weiteren Teil der Welt hinzu, um einen neuen Datensatz f√ºr das Training zu erhalten. <br><br>  Das Modell besteht aus drei Teilen, und wir verwenden f√ºr jeden Teil des Trainingsprozesses eine spezifische Lernrate.  Dies ist erforderlich, damit das Netzwerk sowohl Orientierungspunkte aus einem neuen Datensatzteil, den wir hinzugef√ºgt haben, lernen als auch bereits gelernte Daten speichern kann.  Viele Experimente haben gezeigt, dass dieser Ansatz am effizientesten ist. <br><br>  Also haben wir unser Modell trainiert.  Jetzt m√ºssen wir erkennen, wie es funktioniert.  Verwenden wir die Klassenaktivierungskarte, um den Teil des Bildes zu finden, auf den unser neuronales Netzwerk am schnellsten reagiert.  Das folgende Bild zeigt Eingabebilder in der ersten Zeile, und in der zweiten Zeile werden dieselben Bilder angezeigt, die mit der Klassenaktivierungskarte aus dem Netzwerk √ºberlagert sind, das wir im vorherigen Schritt trainiert haben. <br><br><img src="https://habrastorage.org/webt/_e/p6/x0/_ep6x0-7sjfjfkrmyyrxfhlunsq.jpeg"><br><br>  Die Heatmap zeigt, welche Teile des Bildes st√§rker vom Netzwerk besucht werden.  Wie die Klassenaktivierungskarte zeigt, hat unser neuronales Netzwerk das Konzept des Orientierungspunkts erfolgreich gelernt. <br><br><h2>  Folgerung </h2><br>  Jetzt m√ºssen wir dieses Wissen irgendwie nutzen, um Dinge zu erledigen.  Da wir den Center-Verlust f√ºr das Training verwendet haben, erscheint es im Falle von Inferenzen ziemlich logisch, auch Zentroide f√ºr Orientierungspunkte zu bestimmen. <br><br>  Zu diesem Zweck nehmen wir einen Teil der Bilder aus dem Trainingsset f√ºr ein Wahrzeichen, beispielsweise den Bronze-Reiter in Sankt Petersburg.  Dann lassen wir sie vom Netzwerk verarbeiten, Einbettungen erhalten, den Durchschnitt ermitteln und einen Schwerpunkt ableiten. <br><br><img src="https://habrastorage.org/webt/zy/di/6f/zydi6frte3yxetvtnnkwak2t9y4.jpeg"><br><br>  Hier ist jedoch eine Frage: Wie viele Zentroide pro Landmarke ist sinnvoll abzuleiten?  Anfangs schien es klar und logisch zu sein zu sagen: ein Schwerpunkt.  Nicht genau, wie sich herausstellte.  Wir haben uns zun√§chst entschieden, auch einen einzelnen Schwerpunkt zu erstellen, und das Ergebnis war nicht schlecht.  Warum also mehrere Zentroide? <br><br>  Erstens sind die Daten, die wir haben, nicht so sauber.  Obwohl wir den Datensatz bereinigt haben, haben wir nur offensichtliche Abfalldaten entfernt.  Es kann jedoch immer noch Bilder geben, die nicht offensichtlich verschwendet werden, sondern das Ergebnis nachteilig beeinflussen. <br><br>  Zum Beispiel habe ich einen Winterpalast in Sankt Petersburg.  Ich m√∂chte einen Schwerpunkt daf√ºr ableiten.  Der Datensatz enth√§lt jedoch einige Fotos mit dem Palastplatz und dem Bogen des Hauptquartiers, da diese Objekte nahe beieinander liegen.  Wenn der Schwerpunkt f√ºr alle Bilder bestimmt werden soll, ist das Ergebnis nicht so stabil.  Was wir tun m√ºssen, ist, ihre vom neuronalen Netzwerk abgeleiteten Einbettungen irgendwie zu gruppieren, nur den Schwerpunkt zu nehmen, der sich mit dem Winterpalast befasst, und die resultierenden Daten zu mitteln. <br><br><img src="https://habrastorage.org/webt/do/2n/lz/do2nlzvg9awjggqsodeuxn9hz3o.jpeg"><br><br>  Zweitens k√∂nnten Fotos aus verschiedenen Blickwinkeln aufgenommen worden sein. <br><br>  Hier ist ein Beispiel f√ºr ein solches Verhalten, das am Belfried von Br√ºgge veranschaulicht wird.  Daf√ºr wurden zwei Zentroide abgeleitet.  In der oberen Reihe des Bildes befinden sich die Fotos, die n√§her am ersten Schwerpunkt liegen, und in der zweiten Reihe diejenigen, die n√§her am zweiten Schwerpunkt liegen. <br><br><img src="https://habrastorage.org/webt/34/uo/n5/34uon5ifrysj8l9xlh3wwykzw6o.jpeg"><br><br>  Der erste Schwerpunkt befasst sich mit mehr ‚Äûgro√üartigen‚Äú Fotos, die auf kurzer Distanz auf dem Markt in Br√ºgge aufgenommen wurden.  Der zweite Schwerpunkt befasst sich mit Fotografien, die in bestimmten Stra√üen aus der Ferne aufgenommen wurden. <br><br>  Wie sich herausstellt, k√∂nnen wir durch Ableiten mehrerer Zentroide pro Orientierungspunktklasse unterschiedliche Inferenzwinkel f√ºr diesen Orientierungspunkt reflektieren. <br><br>  Wie erhalten wir diese Mengen zur Ableitung von Zentroiden?  Wir wenden hierarchisches Clustering (vollst√§ndiger Link) auf Datens√§tze f√ºr jeden Orientierungspunkt an.  Wir verwenden es, um g√ºltige Cluster zu finden, aus denen Zentroide abgeleitet werden sollen.  Mit g√ºltigen Clustern meinen wir diejenigen, die aufgrund von Clustering mindestens 50 Fotos umfassen.  Die anderen Cluster werden abgelehnt.  Als Ergebnis haben wir rund 20% der Orientierungspunkte mit mehr als einem Schwerpunkt erhalten. <br><br>  Nun zum Schluss.  Es wird in zwei Schritten erhalten: Erstens f√ºhren wir das Eingabebild in unser neuronales Faltungsnetzwerk ein und erhalten eine Einbettung. Anschlie√üend ordnen wir die Einbettung mit Hilfe des Punktprodukts den Schwerpunkten zu.  Wenn Bilder Geodaten enthalten, beschr√§nken wir die Suche auf Schwerpunkte, die sich auf Orientierungspunkte beziehen, die sich innerhalb eines Quadrats von 1 x 1 km vom Bildort befinden.  Dies erm√∂glicht eine genauere Suche und einen niedrigeren Schwellenwert f√ºr den nachfolgenden Abgleich.  Wenn der resultierende Abstand den Schwellenwert √ºberschreitet, der ein Parameter des Algorithmus ist, schlie√üen wir, dass ein Foto einen Orientierungspunkt mit dem maximalen Punktproduktwert hat.  Wenn es weniger ist, ist es ein Foto ohne Orientierungspunkt. <br><br><img src="https://habrastorage.org/webt/mi/pl/os/miplosde7vmgj1ty5qlsjzabc2u.png"><br><br>  Angenommen, ein Foto hat einen Orientierungspunkt.  Wenn wir Geodaten haben, verwenden wir diese und leiten eine Antwort ab.  Wenn keine Geodaten verf√ºgbar sind, f√ºhren wir eine zus√§tzliche √úberpr√ºfung durch.  Beim Bereinigen des Datensatzes haben wir f√ºr jede Klasse eine Reihe von Referenzbildern erstellt.  Wir k√∂nnen Einbettungen f√ºr sie bestimmen und dann den durchschnittlichen Abstand von ihnen zur Einbettung des Abfragebilds ermitteln.  Wenn ein bestimmter Schwellenwert √ºberschritten wird, wird die √úberpr√ºfung bestanden, und wir bringen Metadaten ein und leiten ein Ergebnis ab.  Es ist wichtig zu beachten, dass wir dieses Verfahren f√ºr mehrere Orientierungspunkte ausf√ºhren k√∂nnen, die in einem Bild gefunden wurden. <br><br><img src="https://habrastorage.org/webt/rw/kd/ht/rwkdhtwi78ko9fohfgj2dex-lro.png"><br><br><h2>  Testergebnisse </h2><br>  Wir haben unser Modell mit DELF verglichen, f√ºr das wir Parameter verwendet haben, mit denen es die beste Leistung in unserem Test zeigen w√ºrde.  Die Ergebnisse sind nahezu identisch. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Modell <br></th><th>  Wahrzeichen <br></th><th>  Nicht wegweisend <br></th></tr><tr><td>  Unser Modell <br></td><td>  80% <br></td><td>  99% <br></td></tr><tr><td>  Delf <br></td><td>  80,1% <br></td><td>  99% <br></td></tr></tbody></table></div><br>  Dann haben wir Orientierungspunkte in zwei Typen eingeteilt: h√§ufig (√ºber 100 Fotos in der Datenbank), die 87% aller Orientierungspunkte im Test ausmachten, und selten.  Unser Modell funktioniert gut mit den h√§ufigen: 85,3% Pr√§zision.  Mit seltenen Orientierungspunkten hatten wir 46%, was auch √ºberhaupt nicht schlecht war, was bedeutet, dass unser Ansatz selbst mit wenigen Daten ziemlich gut funktionierte. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Typ <br></th><th>  Pr√§zision <br></th><th>  Anteil an der Gesamtzahl <br></th></tr><tr><td>  H√§ufigkeit <br></td><td>  85,3% <br></td><td>  87% <br></td></tr><tr><td>  Selten <br></td><td>  46% <br></td><td>  13% <br></td></tr></tbody></table></div><br>  Dann haben wir einen A / B-Test mit Benutzerfotos durchgef√ºhrt.  Infolgedessen stieg die Conversion-Rate f√ºr den Kauf von Cloud-Speicherplatz um 10%, die Conversion-Rate f√ºr die Deinstallation von mobilen Apps um 3% und die Anzahl der Albumaufrufe um 13%. <br><br>  Vergleichen wir unsere Geschwindigkeit mit der von DELF.  Bei der GPU erfordert DELF 7 Netzwerkl√§ufe, da 7 Bildskalen verwendet werden, w√§hrend bei unserem Ansatz nur 1 verwendet wird. Bei der CPU verwendet DELF eine l√§ngere Suche nach der Methode des n√§chsten Nachbarn und eine sehr lange geometrische √úberpr√ºfung.  Am Ende war unsere Methode mit CPU 15-mal schneller.  Unser Ansatz zeigt in beiden F√§llen eine h√∂here Geschwindigkeit, was f√ºr die Produktion entscheidend ist. <br><br><h2>  Ergebnisse: Erinnerungen aus dem Urlaub </h2><br>  Am Anfang dieses Artikels erw√§hnte ich eine L√∂sung zum Scrollen und Finden der gew√ºnschten Landmarkenbilder.  Hier ist es. <br><br><img src="https://habrastorage.org/webt/ps/x_/7x/psx_7x8ptraq4s_tjodrfhi3g6o.jpeg"><br><br>  Dies ist meine Cloud, in der alle Fotos in Alben klassifiziert sind.  Es gibt Alben "People", "Objects" und "Attractions".  Im Album "Attraktionen" werden die Orientierungspunkte in Alben eingeteilt, die nach Stadt gruppiert sind.  Ein Klick auf Dresdner Zwinger √∂ffnet ein Album nur mit Fotos dieses Wahrzeichens. <br><br><img src="https://habrastorage.org/webt/sf/qa/bz/sfqabzwvyehdtq9nv4ko85rbnnu.jpeg" width="400"><br><br>  Eine praktische Funktion: Sie k√∂nnen in den Urlaub fahren, Fotos aufnehmen und in Ihrer Cloud speichern.  Wenn Sie sie sp√§ter auf Instagram hochladen oder mit Freunden und Familie teilen m√∂chten, m√ºssen Sie nicht zu lange suchen und ausw√§hlen - die gew√ºnschten Fotos sind mit nur wenigen Klicks verf√ºgbar. <br><br><h2>  Schlussfolgerungen </h2><br>  Ich m√∂chte Sie an die Hauptmerkmale unserer L√∂sung erinnern. <br><br><ol><li>  Halbautomatische Datenbankbereinigung.  F√ºr die anf√§ngliche Zuordnung ist ein wenig manuelle Arbeit erforderlich, und das neuronale Netzwerk erledigt den Rest.  Auf diese Weise k√∂nnen neue Daten schnell bereinigt und zum erneuten Trainieren des Modells verwendet werden. </li><li>  Wir verwenden tiefe Faltungs-Neuronale Netze und tiefes metrisches Lernen, wodurch wir die Struktur in Klassen effizient lernen k√∂nnen. </li><li>  Wir haben das Lernen von Lehrpl√§nen, d. H. Das Training in Teilen, als Trainingsparadigma verwendet.  Dieser Ansatz war f√ºr uns sehr hilfreich.  Wir verwenden mehrere Inferenzschwerpunkte, die es erm√∂glichen, sauberere Daten zu verwenden und unterschiedliche Ansichten von Orientierungspunkten zu finden. </li></ol><br>  Es scheint, dass die Objekterkennung eine triviale Aufgabe ist.  Bei der Untersuchung der realen Benutzeranforderungen stellen wir jedoch neue Herausforderungen wie die Erkennung von Orientierungspunkten.  Diese Technik erm√∂glicht es, Menschen mithilfe neuronaler Netze etwas Neues √ºber die Welt zu erz√§hlen.  Es ist sehr ermutigend und motivierend! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de467905/">https://habr.com/ru/post/de467905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de467893/index.html">Nur ein weiterer Qt-Wrapper f√ºr gRPC und Protobuf</a></li>
<li><a href="../de467895/index.html">Welche Muster finden neuronale Netze?</a></li>
<li><a href="../de467897/index.html">Autotest-Tools, Yandex Mapkit 3-Integration, cooles Design und Server Driven UI-Ansatz - Android Mitap-Ank√ºndigung</a></li>
<li><a href="../de467901/index.html">Widerlegen Sie vier Stereotypen √ºber die Programmiersprache Rust</a></li>
<li><a href="../de467903/index.html">Top 20 Navigationsfunktionen bei IntelliJ IDEA. Teil 1</a></li>
<li><a href="../de467907/index.html">Vor- und Nachteile des Outsourcings</a></li>
<li><a href="../de467909/index.html">Chatten Sie unter iOS: Verwenden von Sockets</a></li>
<li><a href="../de467913/index.html">Wie man das ‚ÄûBastard-Mineral‚Äú oder die neue Schnittstelle f√ºr das Solarpanel verbessert</a></li>
<li><a href="../de467915/index.html">√úberwachung von Postgres in Openshift</a></li>
<li><a href="../de467917/index.html">Verwaltungsvorlagen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>