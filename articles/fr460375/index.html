<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçå üå¶Ô∏è üòæ Livre "Machine Learning for Business and Marketing" üëçüèª üë©‚Äçüè´ ‚úçüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La science des donn√©es devient une partie int√©grante de toute activit√© marketing, et ce livre est un portrait vivant de la transformation num√©rique da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Livre "Machine Learning for Business and Marketing"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/460375/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/gq/td/mc/gqtdmc8joactk6gu7xrzcdr0f4i.jpeg" align="left" alt="image"></a>  La science des donn√©es devient une partie int√©grante de toute activit√© marketing, et ce livre est un portrait vivant de la transformation num√©rique dans le marketing.  L'analyse des donn√©es et des algorithmes intelligents automatisent les t√¢ches de marketing chronophages.  Le processus d√©cisionnel devient non seulement plus parfait, mais aussi plus rapide, ce qui est d'une grande importance dans un environnement concurrentiel en constante acc√©l√©ration. <br><br>  ¬´Ce livre est un portrait vivant de la transformation num√©rique en marketing.  Il montre comment la science des donn√©es devient une partie int√©grante de toute activit√© marketing.  Il d√©crit en d√©tail comment les approches bas√©es sur l'analyse des donn√©es et les algorithmes intelligents contribuent √† l'automatisation profonde des t√¢ches marketing traditionnellement √† forte intensit√© de main-d'≈ìuvre.  Le processus d√©cisionnel devient non seulement plus avanc√©, mais aussi plus rapide, ce qui est important dans notre environnement concurrentiel en constante acc√©l√©ration.  Ce livre doit √™tre lu par des sp√©cialistes de l'informatique et des sp√©cialistes du marketing, et il vaut mieux qu'ils le lisent ensemble. ¬ª  Andrey Sebrant, directeur du marketing strat√©gique, Yandex. <br><a name="habracut"></a><br><h3>  Extrait.  5.8.3.  Mod√®les √† facteurs cach√©s </h3><br>  Dans les algorithmes de filtrage conjoints examin√©s jusqu'√† pr√©sent, la plupart des calculs sont bas√©s sur les √©l√©ments individuels de la matrice de notation.  Les m√©thodes bas√©es sur la proximit√© √©valuent les notes manquantes directement √† partir des valeurs connues dans la matrice de notation.  Les m√©thodes bas√©es sur un mod√®le ajoutent une couche d'abstraction au-dessus de la matrice de notation, cr√©ant un mod√®le pr√©dictif qui capture certains mod√®les de relations entre les utilisateurs et les √©l√©ments, mais la formation du mod√®le d√©pend toujours fortement des propri√©t√©s de la matrice de notation.  Par cons√©quent, ces techniques de filtrage collaboratif sont g√©n√©ralement confront√©es aux probl√®mes suivants: <br><br>  La matrice de notation peut contenir des millions d'utilisateurs, des millions d'√©l√©ments et des milliards de notes connues, ce qui cr√©e de graves probl√®mes de complexit√© et d'√©volutivit√© des calculs. <br><br>  La matrice de notation est g√©n√©ralement tr√®s clairsem√©e (en pratique, environ 99% des notations peuvent manquer).  Cela affecte la stabilit√© de calcul des algorithmes de recommandation et conduit √† des estimations peu fiables lorsque l'utilisateur ou l'√©l√©ment n'a pas de voisins vraiment similaires.  Ce probl√®me est souvent aggrav√© par le fait que la plupart des algorithmes de base sont orient√©s utilisateur ou √©l√©ment, ce qui limite leur capacit√© √† enregistrer tous les types de similitudes et de relations disponibles dans la matrice de notation. <br><br>  Les donn√©es de la matrice de notation sont g√©n√©ralement fortement corr√©l√©es en raison des similitudes entre les utilisateurs et les √©l√©ments.  Cela signifie que les signaux disponibles dans la matrice de notation sont non seulement clairsem√©s, mais √©galement redondants, ce qui contribue √† l'aggravation du probl√®me d'√©volutivit√©. <br><br>  Les consid√©rations ci-dessus indiquent que la matrice de notation d'origine peut ne pas √™tre la repr√©sentation la plus optimale des signaux, et d'autres repr√©sentations alternatives qui conviennent mieux au filtrage conjoint devraient √™tre envisag√©es.  Pour explorer cette id√©e, revenons au point de d√©part et r√©fl√©chissons un peu √† la nature des services de recommandation.  En fait, le service de recommandation peut √™tre consid√©r√© comme un algorithme de pr√©vision des √©valuations bas√© sur une certaine mesure de similitude entre l'utilisateur et l'√©l√©ment: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/bc/6n/atbc6no-aj2vssp1mrgyctgu_oy.png" alt="image"></div><br>  Une fa√ßon de d√©terminer cette mesure de similitude consiste √† utiliser l'approche des facteurs cach√©s et √† mapper les utilisateurs et les √©l√©ments √† des points dans un espace k-dimensionnel afin que chaque utilisateur et chaque √©l√©ment soit repr√©sent√© par un vecteur k-dimensionnel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/ck/no/9eckno4ntrf2yl0hw1q2tr2irbk.png" alt="image"></div><br>  Les vecteurs doivent √™tre construits de mani√®re √† ce que les dimensions correspondantes p et q soient comparables.  En d'autres termes, chaque dimension peut √™tre consid√©r√©e comme un signe ou un concept, c'est-√†-dire que puj est une mesure de proximit√© de l'utilisateur u et du concept j, et qij, respectivement, est une mesure de l'√©l√©ment i et du concept j.  Dans la pratique, ces dimensions sont souvent interpr√©t√©es comme des genres, des styles et d'autres attributs qui s'appliquent simultan√©ment aux utilisateurs et aux √©l√©ments.  La similitude entre l'utilisateur et l'√©l√©ment et, par cons√©quent, la note peuvent √™tre d√©finis comme le produit des vecteurs correspondants: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k3/1w/9w/k31w9wqmwas5pfvhr8_mopo7ibg.png" alt="image"></div><br>  √âtant donn√© que chaque note peut √™tre d√©compos√©e en un produit de deux vecteurs qui appartiennent √† un espace conceptuel qui n'est pas directement observ√© dans la matrice de notation d'origine, p et q sont appel√©s facteurs cach√©s.  Le succ√®s de cette approche abstraite d√©pend bien entendu enti√®rement de la mani√®re dont les facteurs cach√©s sont d√©termin√©s et construits.  Pour r√©pondre √† cette question, nous notons que l'expression 5.92 peut √™tre r√©√©crite sous forme matricielle comme suit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2i/bp/j9/2ibpj9pmptpun2amylvxf41okjw.png" alt="image"></div><br>  o√π P est la matrice n √ó k assembl√©e √† partir des vecteurs p, et Q est la matrice m √ó k assembl√©e √† partir des vecteurs q, comme le montre la Fig.  5.13.  L'objectif principal d'un syst√®me de filtrage conjoint est g√©n√©ralement de minimiser les erreurs de pr√©diction de la note, ce qui vous permet de d√©terminer directement le probl√®me d'optimisation par rapport √† la matrice des facteurs cach√©s: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9h/nm/mx/9hnmmxregnvn9snp9empxqf91qk.png" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z-/vl/nu/z-vlnuz-v9git5dmm0nj79etl5g.png" alt="image"></div><br>  En supposant que le nombre de dimensions cach√©es k est fixe et k ‚â§ n et k ‚â§ m, le probl√®me d'optimisation 5.94 se r√©duit au probl√®me d'approximation de bas rang, que nous avons consid√©r√© au chapitre 2. Pour d√©montrer l'approche de la solution, supposons un instant que la matrice de notation est compl√®te.  Dans ce cas, le probl√®me d'optimisation a une solution analytique en termes de d√©composition en valeurs singuli√®res (SVD) de la matrice de notation.  En particulier, en utilisant l'algorithme SVD standard, la matrice peut √™tre d√©compos√©e en le produit de trois matrices: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7z/ic/x0/7zicx0iwp156hctp6t7axd5bzu4.png" alt="image"></div><br>  o√π U est la matrice n √ó n orthonormalis√©e par des colonnes, Œ£ est la matrice diagonale n √ó m et V est la matrice m √ó m orthonormalis√©e par des colonnes.  Une solution optimale au probl√®me 5.94 peut √™tre obtenue en fonction de ces facteurs, tronqu√©s aux k dimensions les plus significatives: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/i-/os/mwi-oskhgcs-ehf_bhepytyo_os.png" alt="image"></div><br>  Par cons√©quent, des facteurs cach√©s qui sont optimaux en termes de pr√©cision de pr√©diction peuvent √™tre obtenus par d√©composition singuli√®re, comme indiqu√© ci-dessous: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uc/ih/yl/ucihyltxuqye29vkocknsbzyguw.png" alt="image"></div><br>  Ce mod√®le de facteur cach√© bas√© sur SVD aide √† r√©soudre les probl√®mes de co-filtrage d√©crits au d√©but de cette section.  Premi√®rement, il remplace une grande matrice d'√©valuation n √ó m par des matrices n √ó k et m √ó k, qui sont g√©n√©ralement beaucoup plus petites, car en pratique le nombre optimal de dimensions cach√©es k est souvent petit.  Par exemple, il y a un cas o√π la matrice de notation avec 500 000 utilisateurs et 17 000 √©l√©ments a pu √™tre assez bien approxim√©e √† l'aide de 40 mesures [Funk, 2016].  De plus, SVD √©limine la corr√©lation dans la matrice de notation: les matrices de facteurs latents d√©finies par 5.97 sont orthonorm√©es dans les colonnes, c'est-√†-dire que les dimensions cach√©es ne sont pas corr√©l√©es.  Si, ce qui est g√©n√©ralement vrai dans la pratique, SVD r√©sout √©galement le probl√®me de raret√©, car le signal pr√©sent dans la matrice de notation initiale est effectivement concentr√© (rappelons que nous s√©lectionnons k dimensions avec l'√©nergie de signal la plus √©lev√©e), et les matrices de facteurs latents ne sont pas rares.  La figure 5.14 illustre cette propri√©t√©.  L'algorithme de proximit√© bas√© sur l'utilisateur (5.14, a) r√©duit les vecteurs de notation clairsem√©s pour un √©l√©ment donn√© et un utilisateur donn√© pour obtenir un score de notation.  Le mod√®le √† facteurs cach√©s (5.14, b), au contraire, estime la cotation par convolution de deux vecteurs de dimension r√©duite et de densit√© √©nerg√©tique plus √©lev√©e. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/te/fm/v3tefmto-k2yf54og0xn4rpbxlq.png" alt="image"></div><br>  L'approche qui vient d'√™tre d√©crite ressemble √† une solution coh√©rente au probl√®me des facteurs cach√©s, mais en fait elle pr√©sente un s√©rieux inconv√©nient en raison de l'hypoth√®se que la matrice de notation est compl√®te.  Si la matrice de notation est clairsem√©e, ce qui est presque toujours le cas, l'algorithme SVD standard ne peut pas √™tre appliqu√© directement, car il ne peut pas traiter les √©l√©ments manquants (non d√©finis).  La solution la plus simple dans ce cas est de remplir les notes manquantes avec une valeur par d√©faut, mais cela peut conduire √† un s√©rieux biais dans les pr√©visions.  De plus, il est inefficace sur le plan informatique car la complexit√© de calcul d'une telle solution est √©gale √† la complexit√© SVD pour la matrice n √ó m compl√®te, alors qu'il est souhaitable d'avoir une m√©thode de complexit√© proportionnelle au nombre de notations connues.  Ces probl√®mes peuvent √™tre r√©solus √† l'aide des m√©thodes de d√©composition alternatives d√©crites dans les sections suivantes. <br><br><h3>  5.8.3.1.  D√©composition illimit√©e </h3><br>  L'algorithme SVD standard est une solution analytique au probl√®me d'approximation de bas rang.  Cependant, ce probl√®me peut √™tre consid√©r√© comme un probl√®me d'optimisation et des m√©thodes d'optimisation universelles peuvent √©galement lui √™tre appliqu√©es.  L'une des approches les plus simples consiste √† utiliser la m√©thode de descente en gradient pour affiner de mani√®re it√©rative les valeurs des facteurs cach√©s.  Le point de d√©part est la d√©finition de la fonction de co√ªt J comme erreur de pr√©vision r√©siduelle: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g8/d_/gk/g8d_gkpksz8-d3wyg0xs9ca60a8.png" alt="image"></div><br>  Veuillez noter que cette fois, nous n'imposons aucune restriction, telle que l'orthogonalit√©, √† la matrice des facteurs cach√©s.  En calculant le gradient de la fonction de co√ªt par rapport aux facteurs cach√©s, on obtient le r√©sultat suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/_l/cu/jn_lcurj48vk0kuluh8mojadgcy.png" alt="image"></div><br>  o√π E est la matrice d'erreur r√©siduelle: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6z/wt/at/6zwtat7l4h9qoo0fq7ejzatlobi.png" alt="image"></div><br>  L'algorithme de descente de gradient minimise la fonction de co√ªt en se d√©pla√ßant √† chaque √©tape dans la direction n√©gative du gradient.  Par cons√©quent, vous pouvez trouver des facteurs cach√©s qui minimisent l'erreur quadratique de pr√©diction de notation en modifiant de mani√®re it√©rative les matrices P et Q pour qu'elles convergent, conform√©ment aux expressions suivantes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/uw/gu/f9uwguspa118fw__nuchwxu1ycy.png" alt="image"></div><br>  o√π Œ± est la vitesse d'apprentissage.  L'inconv√©nient de la m√©thode de descente de gradient est la n√©cessit√© de calculer la matrice enti√®re des erreurs r√©siduelles et de changer simultan√©ment toutes les valeurs des facteurs cach√©s √† chaque it√©ration.  Une approche alternative, qui peut √™tre mieux adapt√©e aux grandes matrices, est la descente de gradient stochastique [Funk, 2016].  L'algorithme de descente de gradient stochastique utilise le fait que l'erreur de pr√©vision totale J est la somme des erreurs pour les √©l√©ments individuels de la matrice de notation; par cons√©quent, le gradient g√©n√©ral J peut √™tre approxim√© par un gradient √† un point de donn√©es et les facteurs cach√©s peuvent √™tre modifi√©s √©l√©ment par √©l√©ment.  L'impl√©mentation compl√®te de cette id√©e est montr√©e dans l'algorithme 5.1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/le/c5/ct/lec5ctumggjq0twtb30wnamvj0g.png" alt="image"></div><br>  La premi√®re √©tape de l'algorithme est l'initialisation de la matrice des facteurs cach√©s.  Le choix de ces valeurs initiales n'est pas tr√®s important, mais dans ce cas, une distribution uniforme de l'√©nergie des notations connues parmi les facteurs cach√©s g√©n√©r√©s al√©atoirement est choisie.  Ensuite, l'algorithme optimise s√©quentiellement les dimensions du concept.  Pour chaque mesure, il parcourt de mani√®re r√©p√©t√©e toutes les notes de l'ensemble d'apprentissage, pr√©dit chaque note en utilisant les valeurs actuelles des facteurs cach√©s, estime l'erreur et corrige les valeurs des facteurs conform√©ment aux expressions 5.101.  L'optimisation de mesure est termin√©e lorsque la condition de convergence est remplie, apr√®s quoi l'algorithme passe √† la mesure suivante. <br><br>  L'algorithme 5.1 aide √† surmonter les limites de la m√©thode SVD standard.  Il optimise les facteurs cach√©s en parcourant les points de donn√©es individuels et √©vite ainsi les probl√®mes de notes manquantes et les op√©rations alg√©briques avec des matrices g√©antes.  L'approche it√©rative rend √©galement la descente de gradient stochastique plus pratique pour les applications pratiques que la descente de gradient, qui modifie des matrices enti√®res √† l'aide des expressions 5.101. <br><br><h3>  EXEMPLE 5.6 </h3><br>  En fait, une approche bas√©e sur des facteurs cach√©s est un ensemble de m√©thodes d'enseignement des repr√©sentations qui peuvent identifier des mod√®les implicites dans la matrice de notation et les repr√©senter explicitement sous forme de concepts.  Parfois, les concepts ont une interpr√©tation compl√®tement significative, en particulier ceux √† haute √©nergie, bien que cela ne signifie pas que tous les concepts ont toujours une signification significative.  Par exemple, l'application de l'algorithme de d√©composition matricielle √† une base de donn√©es de notes de films peut cr√©er des facteurs qui correspondent approximativement √† des dimensions psychographiques, telles que le m√©lodrame, la com√©die, l'horreur, etc. Illustrons ce ph√©nom√®ne avec un petit exemple num√©rique qui utilise la matrice de notation du tableau.  5.3: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/6a/vu/xw6avuizjd2x8aku8k-0r89ieem.png" alt="image"></div><br>  Tout d'abord, soustrayez la moyenne globale Œº = 2,82 de tous les √©l√©ments pour centrer la matrice, puis ex√©cutez l'algorithme 5.1 avec k = 3 mesures cach√©es et le taux d'apprentissage Œ± = 0,01 pour obtenir les deux matrices de facteurs suivantes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k-/rr/vd/k-rrvde1sezar3qo0e2vqfb7jh8.png" alt="image"></div><br>  Chaque ligne de ces matrices correspond √† un utilisateur ou √† un film, et les 12 vecteurs de ligne sont repr√©sent√©s sur la Fig.  5.15.  Veuillez noter que les √©l√©ments de la premi√®re colonne (le premier vecteur de concepts) ont les valeurs les plus √©lev√©es et que les valeurs des colonnes suivantes diminuent progressivement.  Cela s'explique par le fait que le premier vecteur conceptuel capture autant d'√©nergie de signal qu'il est possible de capturer avec une mesure, le deuxi√®me vecteur conceptuel ne capture qu'une partie de l'√©nergie r√©siduelle, etc. De plus, notez que le premier concept peut √™tre interpr√©t√© s√©mantiquement comme l'axe dramatique. - film d'action, o√π la direction positive correspond au genre du film d'action, et la n√©gative - au genre dramatique.  Les notes dans cet exemple sont fortement corr√©l√©es, il est donc clairement visible que les trois premiers utilisateurs et les trois premiers films ont de grandes valeurs n√©gatives dans le premier vecteur conceptuel (films dramatiques et utilisateurs qui aiment ces films), tandis que les trois derniers utilisateurs et les trois derniers les films ont de grandes significations positives dans la m√™me colonne (films d'action et utilisateurs qui pr√©f√®rent ce genre).  La deuxi√®me dimension dans ce cas particulier correspond principalement au biais de l'utilisateur ou de l'√©l√©ment, qui peut √™tre interpr√©t√© comme un attribut psychographique (criticit√© des jugements de l'utilisateur? Popularit√© du film?).  D'autres concepts peuvent √™tre consid√©r√©s comme du bruit. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z0/s6/t9/z0s6t9vbnwdtgonx0p6rswo_ufe.png" alt="image"></div><br>  La matrice de facteurs r√©sultante n'est pas compl√®tement orthogonale dans les colonnes, mais a tendance √† √™tre orthogonale, car cela d√©coule de l'optimalit√© de la solution SVD.  Cela peut √™tre vu en regardant les produits de PTP et QTQ, qui sont proches des matrices diagonales: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/zr/sf/pbzrsfbqm0hjvmoqgh1xlurslnm.png" alt="image"></div><br>  Les matrices 5.103 sont essentiellement un mod√®le pr√©dictif qui peut √™tre utilis√© pour √©valuer les cotes connues et manquantes.  Les estimations peuvent √™tre obtenues en multipliant deux facteurs et en ajoutant la moyenne mondiale: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qg/du/d2/qgdud2i8fqm_ut6doi-8jnpllq8.png" alt="image"></div><br>  Les r√©sultats reproduisent fid√®lement les notes connues et pr√©disent les notes manquantes conform√©ment aux attentes intuitives.  La pr√©cision des estimations peut √™tre augment√©e ou diminu√©e en modifiant le nombre de mesures, et le nombre optimal de mesures peut √™tre d√©termin√© dans la pratique en recoupant et en choisissant un compromis raisonnable entre la complexit√© de calcul et la pr√©cision. <br><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  25% de r√©duction sur les colporteurs - <b>Machine Learning</b> <br><br>  Lors du paiement de la version papier du livre, un livre √©lectronique est envoy√© par e-mail. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr460375/">https://habr.com/ru/post/fr460375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr460361/index.html">Grande FAQ sur la cybers√©curit√© des syst√®mes d'information m√©dicale</a></li>
<li><a href="../fr460363/index.html">7 facteurs manquants dans l'approche 12 Factor App</a></li>
<li><a href="../fr460365/index.html">Trace distribu√©e: nous avons tout mal fait</a></li>
<li><a href="../fr460367/index.html">Chaos Engineering: l'art de la destruction intentionnelle. Partie 1</a></li>
<li><a href="../fr460373/index.html">Under the Hood Turbo Pages: Architecture de la technologie de t√©l√©chargement rapide des pages Web</a></li>
<li><a href="../fr460377/index.html">Utilisation de Liquibase pour g√©rer la structure de la base de donn√©es dans une application Spring Boot. Partie 1</a></li>
<li><a href="../fr460381/index.html">Qu'est-ce que l'assertivit√© et pourquoi est-elle n√©cessaire</a></li>
<li><a href="../fr460383/index.html">Les transitions d'√©cran dans Legend of Zelda utilisent les fonctionnalit√©s non document√©es de NES</a></li>
<li><a href="../fr460387/index.html">Guide du d√©butant SELinux</a></li>
<li><a href="../fr460393/index.html">Contexte: √† quoi s'attendre de Fedora Silverblue</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>