<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëØ üåø üßìüèΩ Wie wir den Cache in der Tarantool-Datenbank implementiert haben ü§±üèø ü•™ üí™üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Guten Tag! 

 Ich m√∂chte Ihnen eine Geschichte √ºber die Implementierung des Cache in der Tarantool-Datenbank und meine Arbeitsfunktionen mitteilen. 
 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir den Cache in der Tarantool-Datenbank implementiert haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Guten Tag! <br><br>  Ich m√∂chte Ihnen eine Geschichte √ºber die Implementierung des Cache in der Tarantool-Datenbank und meine Arbeitsfunktionen mitteilen. <br>  Ich arbeite als Java-Entwickler in einem Telekommunikationsunternehmen.  Die Hauptaufgabe: die Implementierung der Gesch√§ftslogik f√ºr die Plattform, die das Unternehmen vom Anbieter gekauft hat.  Von den ersten Merkmalen ist dies Seifenarbeit und das fast vollst√§ndige Fehlen von Caching, au√üer im JVM-Speicher.  All dies ist nat√ºrlich gut, bis die Anzahl der Anwendungsinstanzen zwei Dutzend √ºberschreitet ... <br><br>  Im Laufe der Arbeit und der Entstehung eines Verst√§ndnisses der Funktionen der Plattform wurde versucht, das Caching durchzuf√ºhren.  Zu diesem Zeitpunkt war MongoDB bereits gestartet, und als Ergebnis haben wir keine besonderen positiven Ergebnisse wie im Test erhalten. <br><br>  Bei einer weiteren Suche nach Alternativen und Ratschl√§gen meines guten Freundes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">mr_elzor</a> wurde beschlossen, die Tarantool-Datenbank auszuprobieren. <br><a name="habracut"></a><br>  In einer fl√ºchtigen Studie tauchten in Lua nur Zweifel auf, da ich nicht vom Wort "vollst√§ndig" darauf geschrieben hatte.  Aber er schob alle Zweifel beiseite und machte sich an die Installation.  Ich denke, dass nur wenige Menschen an geschlossenen Netzwerken und Firewalls interessiert sind, aber ich rate Ihnen, zu versuchen, sie zu umgehen und alles aus √∂ffentlichen Quellen zu beziehen. <br><br>  Testserver mit Konfiguration: 8 CPU, 16 GB RAM, 100 GB Festplatte, Debian 9.4. <br><br>  Die Installation erfolgte gem√§√ü den Anweisungen auf der Website.  Und so bekam ich eine Beispieloption.  Es entstand sofort die Idee einer visuellen Oberfl√§che, mit der die Unterst√ºtzung bequem funktionieren w√ºrde.  W√§hrend einer Schnellsuche habe ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tarantool-admin</a> gefunden und konfiguriert.  Arbeitet bei Docker und deckt Support-Aufgaben zumindest vorerst zu 100% ab. <br><br>  Aber lassen Sie uns √ºber interessanter sprechen. <br><br>  Der n√§chste Gedanke war, meine Version in der Master-Slave-Konfiguration auf demselben Server zu konfigurieren, da die Dokumentation nur Beispiele mit zwei verschiedenen Servern enth√§lt. <br><br>  Nachdem ich einige Zeit damit verbracht habe, Lua zu verstehen und die Konfiguration zu beschreiben, starte ich den Assistenten. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Ich gerate sofort in einen Stupor und verstehe nicht, warum der Fehler vorliegt, aber ich sehe, dass er sich im Status "Laden" befindet. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ‚óè tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service ‚îî‚îÄ20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Ich laufe Sklave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Und ich sehe den gleichen Fehler.  Hier fange ich im Allgemeinen an, mich zu belasten und nicht zu verstehen, was passiert, da es in der Dokumentation √ºberhaupt nichts dar√ºber gibt ... Aber wenn ich den Status √ºberpr√ºfe, sehe ich, dass er √ºberhaupt nicht gestartet wurde, obwohl er besagt, dass der Status "l√§uft": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ‚óè tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Gleichzeitig begann der Meister zu arbeiten: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Ein Neustart des Slaves hilft nicht.  Ich frage mich warum? <br><br>  Ich halte den Meister auf.  F√ºhren Sie die Aktionen in umgekehrter Reihenfolge aus. <br><br>  Ich sehe, dass der Sklave versucht zu starten. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Ich starte den Assistenten und sehe, dass er nicht gestiegen ist und im Allgemeinen in den Waisenstatus gewechselt ist, w√§hrend der Sklave im Allgemeinen gefallen ist. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Es wird noch interessanter. <br><br>  Ich sehe in den Protokollen auf Slave, dass er sogar den Master gesehen und versucht hat zu synchronisieren. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  Und der Versuch war erfolgreich: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  Es fing sogar an: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Aber aus unbekannten Gr√ºnden verlor er die Verbindung und fiel: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Der Versuch, den Slave erneut zu starten, hilft nicht. <br><br>  L√∂schen Sie nun die von den Instanzen erstellten Dateien.  In meinem Fall l√∂sche ich alles aus dem Verzeichnis / var / lib / tarantool. <br><br>  Ich starte zuerst den Sklaven und erst dann den Meister.  Und siehe da ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Ich habe keine Erkl√§rung f√ºr dieses Verhalten gefunden, au√üer als "Funktion dieser Software". <br>  Diese Situation tritt jedes Mal auf, wenn Ihr Server vollst√§ndig neu gestartet wurde. <br><br>  Bei einer weiteren Analyse der Architektur dieser Software stellt sich heraus, dass nur eine vCPU f√ºr eine Instanz verwendet werden soll und viele weitere Ressourcen frei bleiben. <br><br>  In der Ideologie von n vCPU k√∂nnen wir den Master und n-2 Slaves zum Lesen anheben. <br><br>  Da auf dem Testserver 8 vCPU der Master und 6 Instanzen zum Lesen ausgel√∂st werden k√∂nnen. <br>  Ich kopiere die Datei f√ºr den Slave, korrigiere die Ports und f√ºhre sie aus, d.h.  ein paar weitere Slaves werden hinzugef√ºgt. <br><br>  Wichtig!  Wenn Sie eine weitere Instanz hinzuf√ºgen, m√ºssen Sie diese im Assistenten registrieren. <br>  Sie m√ºssen jedoch zuerst einen neuen Slave starten und erst dann den Master neu starten. <br><br><h4>  Beispiel </h4><br>  Ich hatte bereits eine Konfiguration mit einem Assistenten und zwei Slaves. <br><br>  Ich beschloss, einen dritten Sklaven hinzuzuf√ºgen. <br><br>  Ich habe es beim Master registriert und den Master zuerst neu gestartet, und das habe ich gesehen: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Das hei√üt,  Unser Meister wurde ein Einzelg√§nger, und die Replikation fiel auseinander. <br><br>  Das Starten eines neuen Slaves hilft nicht mehr und f√ºhrt zu einem Fehler: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Und in den Protokollen sah ich einen kleinen informativen Eintrag: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Wir stoppen den Assistenten und starten einen neuen Sklaven.  Wie beim ersten Start wird auch ein Fehler auftreten, aber wir werden sehen, dass es sich um den Ladestatus handelt. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Wenn Sie jedoch den Master starten, st√ºrzt der neue Slave ab und der Master geht nicht in den laufenden Status √ºber. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  In dieser Situation gibt es nur einen Ausweg.  Wie ich bereits geschrieben habe, l√∂sche ich Dateien, die von Instanzen erstellt wurden, und f√ºhre zuerst Slaves und dann Master aus. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Alles begann erfolgreich. <br><br>  Auf diese Weise habe ich durch Ausprobieren herausgefunden, wie die Replikation richtig konfiguriert und gestartet werden kann. <br><br>  Als Ergebnis wurde die folgende Konfiguration zusammengestellt: <br><br>  <i>2 Server.</i> <i><br></i>  <i>2 Meister.</i>  <i>Hot Reserve.</i> <i><br></i>  <i>12 Sklaven.</i>  <i>Alle sind aktiv.</i> <br><br>  In der Logik von tarantool wurde http.server verwendet, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://github.com/tarantool/">um den</a> zus√§tzlichen Adapter nicht zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://github.com/tarantool/">blockieren</a> (Hersteller, Plattform und Seife beachten) oder die Bibliothek an jedem Gesch√§ftsprozess zu befestigen. <br><br>  Um eine Diskrepanz zwischen den Mastern auf dem Balancer (NetScaler, HAProxy oder einem anderen Ihrer Favoriten) zu vermeiden, legen wir die Reserveregel fest, d. H.  Einf√ºge-, Aktualisierungs- und L√∂schvorg√§nge gehen nur zum ersten aktiven Master. <br><br>  Zu diesem Zeitpunkt repliziert der zweite einfach die Datens√§tze des ersten.  Die Slaves selbst sind aus der Konfiguration mit dem ersten angegebenen Master verbunden, was wir in dieser Situation ben√∂tigen. <br><br>  Auf lua wurden CRUD-Operationen f√ºr den Schl√ºsselwert implementiert.  Im Moment reicht dies aus, um das Problem zu l√∂sen. <br><br>  In Anbetracht der Merkmale der Arbeit mit Seife wurde ein Proxy-Gesch√§ftsprozess implementiert, in dem die Logik der Arbeit mit einer Vogelspinne √ºber http festgelegt wurde. <br><br>  Wenn die Schl√ºsseldaten vorhanden sind, werden sie sofort zur√ºckgegeben.  Wenn nicht, wird eine Anforderung an das Mastersystem gesendet und in der Tarantool-Datenbank gespeichert. <br><br>  Infolgedessen verarbeitet ein Gesch√§ftsprozess in Tests bis zu 4.000 Anforderungen.  In diesem Fall betr√§gt die Reaktionszeit der Vogelspinne ~ 1 ms.  Die durchschnittliche Reaktionszeit betr√§gt bis zu 3 ms. <br><br>  Hier einige Informationen aus den Tests: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Es gab 50 Gesch√§ftsprozesse, die zu 4 Mastersystemen gehen und Daten in ihrem Speicher zwischenspeichern.  Duplizierung von Informationen in vollem Wachstum bei jeder Instanz.  Angesichts der Tatsache, dass Java die Erinnerung bereits liebt ... ist die Aussicht nicht die beste. <br><br><h4>  Jetzt </h4><br>  50 Gesch√§ftsprozesse fordern Informationen √ºber den Cache an.  Jetzt werden Informationen von 4 Instanzen des Assistenten an einem Ort gespeichert und nicht bei jeder Instanz im Speicher zwischengespeichert.  Es war m√∂glich, die Belastung des Mastersystems erheblich zu reduzieren, es gibt keine doppelten Informationen und der Speicherverbrauch bei Instanzen mit Gesch√§ftslogik ist gesunken. <br><br>  Ein Beispiel f√ºr die Gr√∂√üe des Informationsspeichers im Tarantelspeicher: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  Am Ende des Tages k√∂nnen sich diese Zahlen verdoppeln, aber es gibt keinen ‚ÄûLeistungsabfall‚Äú. <br><br>  Im Kampf erstellt die aktuelle Version 2k - 2,5k Anforderungen pro Sekunde realer Last.  Die durchschnittliche Reaktionszeit √§hnelt Tests bis zu 3 ms. <br><br>  Wenn Sie sich htop auf einem der Server mit tarantool ansehen, werden wir feststellen, dass sie "abk√ºhlen": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  Zusammenfassung </h4><br>  Trotz aller Feinheiten und Nuancen der Tarantool-Datenbank k√∂nnen Sie eine hervorragende Leistung erzielen. <br><br>  Ich hoffe, dass sich dieses Projekt entwickelt und diese unangenehmen Momente gel√∂st werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441686/">https://habr.com/ru/post/de441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441676/index.html">Wie man Freunde PLUTO und HDSDR macht</a></li>
<li><a href="../de441678/index.html">Spieltornado-Physik: Wie Aerodynamik in Just Cause 4 (Verkehr) implementiert wird</a></li>
<li><a href="../de441680/index.html">Lua In Moskau 2019 Konferenzprogramm</a></li>
<li><a href="../de441682/index.html">HyperX Fury 3D - SSD mit klarem Stammbaum</a></li>
<li><a href="../de441684/index.html">Vorhersagen: Wolken werden sich 2019 √§ndern</a></li>
<li><a href="../de441688/index.html">Spiele ver√§ndern die Welt: Wie Hellblade auf die Probleme von Menschen mit psychischen Erkrankungen aufmerksam macht</a></li>
<li><a href="../de441690/index.html">Sie brauchen keine Blockchain: Acht bekannte Anwendungsf√§lle und warum sie nicht funktionieren</a></li>
<li><a href="../de441692/index.html">Wie werden die Spuren in der Blockchain abgedeckt? Unser Transaktionsmischerkonzept</a></li>
<li><a href="../de441694/index.html">Warum Verkehrsdiagramme "l√ºgen"</a></li>
<li><a href="../de441696/index.html">Die Geschichte des kyrillischen LiveJournals: Wie das russische Management den Aufstieg des russischsprachigen Bloggens niedergeschlagen hat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>