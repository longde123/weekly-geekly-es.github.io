<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👯 🌿 🧓🏽 Wie wir den Cache in der Tarantool-Datenbank implementiert haben 🤱🏿 🥪 💪🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Guten Tag! 

 Ich möchte Ihnen eine Geschichte über die Implementierung des Cache in der Tarantool-Datenbank und meine Arbeitsfunktionen mitteilen. 
 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir den Cache in der Tarantool-Datenbank implementiert haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441686/"> Guten Tag! <br><br>  Ich möchte Ihnen eine Geschichte über die Implementierung des Cache in der Tarantool-Datenbank und meine Arbeitsfunktionen mitteilen. <br>  Ich arbeite als Java-Entwickler in einem Telekommunikationsunternehmen.  Die Hauptaufgabe: die Implementierung der Geschäftslogik für die Plattform, die das Unternehmen vom Anbieter gekauft hat.  Von den ersten Merkmalen ist dies Seifenarbeit und das fast vollständige Fehlen von Caching, außer im JVM-Speicher.  All dies ist natürlich gut, bis die Anzahl der Anwendungsinstanzen zwei Dutzend überschreitet ... <br><br>  Im Laufe der Arbeit und der Entstehung eines Verständnisses der Funktionen der Plattform wurde versucht, das Caching durchzuführen.  Zu diesem Zeitpunkt war MongoDB bereits gestartet, und als Ergebnis haben wir keine besonderen positiven Ergebnisse wie im Test erhalten. <br><br>  Bei einer weiteren Suche nach Alternativen und Ratschlägen meines guten Freundes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">mr_elzor</a> wurde beschlossen, die Tarantool-Datenbank auszuprobieren. <br><a name="habracut"></a><br>  In einer flüchtigen Studie tauchten in Lua nur Zweifel auf, da ich nicht vom Wort "vollständig" darauf geschrieben hatte.  Aber er schob alle Zweifel beiseite und machte sich an die Installation.  Ich denke, dass nur wenige Menschen an geschlossenen Netzwerken und Firewalls interessiert sind, aber ich rate Ihnen, zu versuchen, sie zu umgehen und alles aus öffentlichen Quellen zu beziehen. <br><br>  Testserver mit Konfiguration: 8 CPU, 16 GB RAM, 100 GB Festplatte, Debian 9.4. <br><br>  Die Installation erfolgte gemäß den Anweisungen auf der Website.  Und so bekam ich eine Beispieloption.  Es entstand sofort die Idee einer visuellen Oberfläche, mit der die Unterstützung bequem funktionieren würde.  Während einer Schnellsuche habe ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tarantool-admin</a> gefunden und konfiguriert.  Arbeitet bei Docker und deckt Support-Aufgaben zumindest vorerst zu 100% ab. <br><br>  Aber lassen Sie uns über interessanter sprechen. <br><br>  Der nächste Gedanke war, meine Version in der Master-Slave-Konfiguration auf demselben Server zu konfigurieren, da die Dokumentation nur Beispiele mit zwei verschiedenen Servern enthält. <br><br>  Nachdem ich einige Zeit damit verbracht habe, Lua zu verstehen und die Konfiguration zu beschreiben, starte ich den Assistenten. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@master Job for tarantool@master.service failed because the control process exited with error code. See "systemctl status tarantool@master.service" and "journalctl -xe" for details.</span></span></code> </pre> <br>  Ich gerate sofort in einen Stupor und verstehe nicht, warum der Fehler vorliegt, aber ich sehe, dass er sich im Status "Laden" befindet. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@master ● tarantool@master.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: activating (start) since Tue 2019-02-19 17:03:24 MSK; 17s ago Docs: man:tarantool(1) Process: 20111 ExecStop=/usr/bin/tarantoolctl stop master (code=exited, status=0/SUCCESS) Main PID: 20120 (tarantool) Status: "loading" Tasks: 5 (limit: 4915) CGroup: /system.slice/system-tarantool.slice/tarantool@master.service └─20120 tarantool master.lua &lt;loading&gt; Feb 19 17:03:24 tarantuldb-tst4 systemd[1]: Starting Tarantool Database Server... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Starting instance master... Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: Run console at unix/:/var/run/tarantool/master.control Feb 19 17:03:24 tarantuldb-tst4 tarantoolctl[20120]: started</span></span></code> </pre><br>  Ich laufe Sklave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl start tarantool@slave2 Job for tarantool@slave2.service failed because the control process exited with error code. See "systemctl status tarantool@slave2.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Und ich sehe den gleichen Fehler.  Hier fange ich im Allgemeinen an, mich zu belasten und nicht zu verstehen, was passiert, da es in der Dokumentation überhaupt nichts darüber gibt ... Aber wenn ich den Status überprüfe, sehe ich, dass er überhaupt nicht gestartet wurde, obwohl er besagt, dass der Status "läuft": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl status tarantool@slave2 ● tarantool@slave2.service - Tarantool Database Server Loaded: loaded (/lib/systemd/system/tarantool@.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2019-02-19 17:04:52 MSK; 27s ago Docs: man:tarantool(1) Process: 20258 ExecStop=/usr/bin/tarantoolctl stop slave2 (code=exited, status=0/SUCCESS) Process: 20247 ExecStart=/usr/bin/tarantoolctl start slave2 (code=exited, status=1/FAILURE) Main PID: 20247 (code=exited, status=1/FAILURE) Status: "running" Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Service hold-off time over, scheduling restart. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Stopped Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Start request repeated too quickly. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: Failed to start Tarantool Database Server. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Unit entered failed state. Feb 19 17:04:52 tarantuldb-tst4 systemd[1]: tarantool@slave2.service: Failed with result 'exit-code'.</span></span></code> </pre><br>  Gleichzeitig begann der Meister zu arbeiten: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20158 1 0 17:04 ? 00:00:00 tarantool master.lua &lt;running&gt; root 20268 2921 0 17:06 pts/1 00:00:00 grep taran</span></span></code> </pre><br>  Ein Neustart des Slaves hilft nicht.  Ich frage mich warum? <br><br>  Ich halte den Meister auf.  Führen Sie die Aktionen in umgekehrter Reihenfolge aus. <br><br>  Ich sehe, dass der Sklave versucht zu starten. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20399 1 0 17:09 ? 00:00:00 tarantool slave2.lua &lt;loading&gt;</span></span></code> </pre><br>  Ich starte den Assistenten und sehe, dass er nicht gestiegen ist und im Allgemeinen in den Waisenstatus gewechselt ist, während der Sklave im Allgemeinen gefallen ist. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep taran taranto+ 20428 1 0 17:09 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Es wird noch interessanter. <br><br>  Ich sehe in den Protokollen auf Slave, dass er sogar den Master gesehen und versucht hat zu synchronisieren. <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: binding to 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto/101/main I&gt; binary: bound to 0.0.0.0:3302 2019-02-19 17:13:45.113 [20751] iproto/101/main D&gt; binary: listening on 0.0.0.0:3302... 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] iproto D&gt; cpipe_flush_cb: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: locking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main D&gt; cbus_endpoint_fetch: unlocking &amp;endpoint-&gt;mutex 2019-02-19 17:13:45.113 [20751] main/101/slave2 I&gt; connecting to 1 replicas 2019-02-19 17:13:45.113 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECT 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t I&gt; remote master 825af7c3-f8df-4db0-8559-a866b8310077 at 10.78.221.74:3301 running Tarantool 1.10.2 2019-02-19 17:13:45.114 [20751] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; CONNECTED 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; connected to 1 replicas 2019-02-19 17:13:45.114 [20751] coio V&gt; loading vylog 14 2019-02-19 17:13:45.114 [20751] coio V&gt; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> loading vylog 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovery start 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; recovering from `/var/lib/tarantool/cache_slave2/00000000000000000014.snap<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(47) = 0x7f99a4000080 2019-02-19 17:13:45.114 [20751] main/101/slave2 I&gt; cluster uuid 4035b563-67f8-4e85-95cc-e03429f1fa4d 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(11) = 0x7f99a4004080 2019-02-19 17:13:45.114 [20751] main/101/slave2 D&gt; memtx_tuple_new(17) = 0x7f99a4008068</span></span></code> </pre><br>  Und der Versuch war erfolgreich: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a40004c0 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 1 to replica 825af7c3-f8df-4db0-8559-a866b8310077 2019-02-19 17:13:45.118 [20751] main/101/slave2 D&gt; memtx_tuple_new(40) = 0x7f99a4000500 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; assigned id 2 to replica 403c0323-5a9b-480d-9e71-5ba22d4ccf1b 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; recover from `/var/lib/tarantool/slave2/00000000000000000014.xlog<span class="hljs-string"><span class="hljs-string">' 2019-02-19 17:13:45.118 [20751] main/101/slave2 I&gt; done `/var/lib/tarantool/slave2/00000000000000000014.xlog'</span></span></code> </pre><br>  Es fing sogar an: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.119 [20751] main/101/slave2 D&gt; systemd: sending message <span class="hljs-string"><span class="hljs-string">'STATUS=running'</span></span></code> </pre><br>  Aber aus unbekannten Gründen verlor er die Verbindung und fiel: <br><br><pre> <code class="bash hljs">2019-02-19 17:13:45.129 [20751] main/101/slave2 D&gt; SystemError at /build/tarantool-1.10.2.146/src/coio_task.c:416 2019-02-19 17:13:45.129 [20751] main/101/slave2 tarantoolctl:532 E&gt; Start failed: /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/share/lua/5.1/http/server.lua:1146: Can<span class="hljs-string"><span class="hljs-string">'t create tcp_server: Input/output error</span></span></code> </pre><br>  Der Versuch, den Slave erneut zu starten, hilft nicht. <br><br>  Löschen Sie nun die von den Instanzen erstellten Dateien.  In meinem Fall lösche ich alles aus dem Verzeichnis / var / lib / tarantool. <br><br>  Ich starte zuerst den Sklaven und erst dann den Meister.  Und siehe da ... <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 17:20 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 20933 1 1 17:21 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Ich habe keine Erklärung für dieses Verhalten gefunden, außer als "Funktion dieser Software". <br>  Diese Situation tritt jedes Mal auf, wenn Ihr Server vollständig neu gestartet wurde. <br><br>  Bei einer weiteren Analyse der Architektur dieser Software stellt sich heraus, dass nur eine vCPU für eine Instanz verwendet werden soll und viele weitere Ressourcen frei bleiben. <br><br>  In der Ideologie von n vCPU können wir den Master und n-2 Slaves zum Lesen anheben. <br><br>  Da auf dem Testserver 8 vCPU der Master und 6 Instanzen zum Lesen ausgelöst werden können. <br>  Ich kopiere die Datei für den Slave, korrigiere die Ports und führe sie aus, d.h.  ein paar weitere Slaves werden hinzugefügt. <br><br>  Wichtig!  Wenn Sie eine weitere Instanz hinzufügen, müssen Sie diese im Assistenten registrieren. <br>  Sie müssen jedoch zuerst einen neuen Slave starten und erst dann den Master neu starten. <br><br><h4>  Beispiel </h4><br>  Ich hatte bereits eine Konfiguration mit einem Assistenten und zwei Slaves. <br><br>  Ich beschloss, einen dritten Sklaven hinzuzufügen. <br><br>  Ich habe es beim Master registriert und den Master zuerst neu gestartet, und das habe ich gesehen: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:29 tarantool slave3.lua &lt;running&gt; taranto+ 21519 1 0 09:16 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  Das heißt,  Unser Meister wurde ein Einzelgänger, und die Replikation fiel auseinander. <br><br>  Das Starten eines neuen Slaves hilft nicht mehr und führt zu einem Fehler: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># systemctl restart tarantool@slave4 Job for tarantool@slave4.service failed because the control process exited with error code. See "systemctl status tarantool@slave4.service" and "journalctl -xe" for details.</span></span></code> </pre><br>  Und in den Protokollen sah ich einen kleinen informativen Eintrag: <br><br><pre> <code class="bash hljs">2019-02-20 09:20:10.616 [21601] main/101/slave4 I&gt; bootstrapping replica from 3c77eb9d-2fa1-4a27-885f-e72defa5cd96 at 10.78.221.74:3301 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t I&gt; can<span class="hljs-string"><span class="hljs-string">'t join/subscribe 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t xrow.c:896 E&gt; ER_READONLY: Can'</span></span>t modify data because this instance is <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>-only mode. 2019-02-20 09:20:10.617 [21601] main/106/applier/replicator@tarantuldb-t D&gt; =&gt; STOPPED 2019-02-20 09:20:10.617 [21601] main/101/slave4 xrow.c:896 E&gt; ER_READONLY: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode. 2019-02-20 09:20:10.617 [21601] main/101/slave4 F&gt; can'</span></span>t initialize storage: Can<span class="hljs-string"><span class="hljs-string">'t modify data because this instance is in read-only mode.</span></span></code> </pre><br>  Wir stoppen den Assistenten und starten einen neuen Sklaven.  Wie beim ersten Start wird auch ein Fehler auftreten, aber wir werden sehen, dass es sich um den Ladestatus handelt. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21659 1 0 09:23 ? 00:00:00 tarantool slave4.lua &lt;loading&gt;</span></span></code> </pre><br>  Wenn Sie jedoch den Master starten, stürzt der neue Slave ab und der Master geht nicht in den laufenden Status über. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tara taranto+ 20922 1 0 Feb19 ? 00:00:29 tarantool slave2.lua &lt;running&gt; taranto+ 20965 1 0 Feb19 ? 00:00:30 tarantool slave3.lua &lt;running&gt; taranto+ 21670 1 0 09:23 ? 00:00:00 tarantool master.lua &lt;orphan&gt;</span></span></code> </pre><br>  In dieser Situation gibt es nur einen Ausweg.  Wie ich bereits geschrieben habe, lösche ich Dateien, die von Instanzen erstellt wurden, und führe zuerst Slaves und dann Master aus. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep tarantool taranto+ 21892 1 0 09:30 ? 00:00:00 tarantool slave4.lua &lt;running&gt; taranto+ 21907 1 0 09:30 ? 00:00:00 tarantool slave3.lua &lt;running&gt; taranto+ 21922 1 0 09:30 ? 00:00:00 tarantool slave2.lua &lt;running&gt; taranto+ 21931 1 0 09:30 ? 00:00:00 tarantool master.lua &lt;running&gt;</span></span></code> </pre><br>  Alles begann erfolgreich. <br><br>  Auf diese Weise habe ich durch Ausprobieren herausgefunden, wie die Replikation richtig konfiguriert und gestartet werden kann. <br><br>  Als Ergebnis wurde die folgende Konfiguration zusammengestellt: <br><br>  <i>2 Server.</i> <i><br></i>  <i>2 Meister.</i>  <i>Hot Reserve.</i> <i><br></i>  <i>12 Sklaven.</i>  <i>Alle sind aktiv.</i> <br><br>  In der Logik von tarantool wurde http.server verwendet, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://github.com/tarantool/">um den</a> zusätzlichen Adapter nicht zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://github.com/tarantool/">blockieren</a> (Hersteller, Plattform und Seife beachten) oder die Bibliothek an jedem Geschäftsprozess zu befestigen. <br><br>  Um eine Diskrepanz zwischen den Mastern auf dem Balancer (NetScaler, HAProxy oder einem anderen Ihrer Favoriten) zu vermeiden, legen wir die Reserveregel fest, d. H.  Einfüge-, Aktualisierungs- und Löschvorgänge gehen nur zum ersten aktiven Master. <br><br>  Zu diesem Zeitpunkt repliziert der zweite einfach die Datensätze des ersten.  Die Slaves selbst sind aus der Konfiguration mit dem ersten angegebenen Master verbunden, was wir in dieser Situation benötigen. <br><br>  Auf lua wurden CRUD-Operationen für den Schlüsselwert implementiert.  Im Moment reicht dies aus, um das Problem zu lösen. <br><br>  In Anbetracht der Merkmale der Arbeit mit Seife wurde ein Proxy-Geschäftsprozess implementiert, in dem die Logik der Arbeit mit einer Vogelspinne über http festgelegt wurde. <br><br>  Wenn die Schlüsseldaten vorhanden sind, werden sie sofort zurückgegeben.  Wenn nicht, wird eine Anforderung an das Mastersystem gesendet und in der Tarantool-Datenbank gespeichert. <br><br>  Infolgedessen verarbeitet ein Geschäftsprozess in Tests bis zu 4.000 Anforderungen.  In diesem Fall beträgt die Reaktionszeit der Vogelspinne ~ 1 ms.  Die durchschnittliche Reaktionszeit beträgt bis zu 3 ms. <br><br>  Hier einige Informationen aus den Tests: <br><br><img src="https://habrastorage.org/webt/n6/ae/lg/n6aelg4tipin2jgomzrgsw_8nie.png"><br><br>  Es gab 50 Geschäftsprozesse, die zu 4 Mastersystemen gehen und Daten in ihrem Speicher zwischenspeichern.  Duplizierung von Informationen in vollem Wachstum bei jeder Instanz.  Angesichts der Tatsache, dass Java die Erinnerung bereits liebt ... ist die Aussicht nicht die beste. <br><br><h4>  Jetzt </h4><br>  50 Geschäftsprozesse fordern Informationen über den Cache an.  Jetzt werden Informationen von 4 Instanzen des Assistenten an einem Ort gespeichert und nicht bei jeder Instanz im Speicher zwischengespeichert.  Es war möglich, die Belastung des Mastersystems erheblich zu reduzieren, es gibt keine doppelten Informationen und der Speicherverbrauch bei Instanzen mit Geschäftslogik ist gesunken. <br><br>  Ein Beispiel für die Größe des Informationsspeichers im Tarantelspeicher: <br><br><img src="https://habrastorage.org/webt/es/93/ex/es93exozhrhnihbnq6-zpzjobma.png"><br><br>  Am Ende des Tages können sich diese Zahlen verdoppeln, aber es gibt keinen „Leistungsabfall“. <br><br>  Im Kampf erstellt die aktuelle Version 2k - 2,5k Anforderungen pro Sekunde realer Last.  Die durchschnittliche Reaktionszeit ähnelt Tests bis zu 3 ms. <br><br>  Wenn Sie sich htop auf einem der Server mit tarantool ansehen, werden wir feststellen, dass sie "abkühlen": <br><br><img src="https://habrastorage.org/webt/jt/mt/vf/jtmtvfat0ohxhugounnve47l7ju.png"><br><br><h4>  Zusammenfassung </h4><br>  Trotz aller Feinheiten und Nuancen der Tarantool-Datenbank können Sie eine hervorragende Leistung erzielen. <br><br>  Ich hoffe, dass sich dieses Projekt entwickelt und diese unangenehmen Momente gelöst werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441686/">https://habr.com/ru/post/de441686/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441676/index.html">Wie man Freunde PLUTO und HDSDR macht</a></li>
<li><a href="../de441678/index.html">Spieltornado-Physik: Wie Aerodynamik in Just Cause 4 (Verkehr) implementiert wird</a></li>
<li><a href="../de441680/index.html">Lua In Moskau 2019 Konferenzprogramm</a></li>
<li><a href="../de441682/index.html">HyperX Fury 3D - SSD mit klarem Stammbaum</a></li>
<li><a href="../de441684/index.html">Vorhersagen: Wolken werden sich 2019 ändern</a></li>
<li><a href="../de441688/index.html">Spiele verändern die Welt: Wie Hellblade auf die Probleme von Menschen mit psychischen Erkrankungen aufmerksam macht</a></li>
<li><a href="../de441690/index.html">Sie brauchen keine Blockchain: Acht bekannte Anwendungsfälle und warum sie nicht funktionieren</a></li>
<li><a href="../de441692/index.html">Wie werden die Spuren in der Blockchain abgedeckt? Unser Transaktionsmischerkonzept</a></li>
<li><a href="../de441694/index.html">Warum Verkehrsdiagramme "lügen"</a></li>
<li><a href="../de441696/index.html">Die Geschichte des kyrillischen LiveJournals: Wie das russische Management den Aufstieg des russischsprachigen Bloggens niedergeschlagen hat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>