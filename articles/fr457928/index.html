<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéüÔ∏è ü•â üèØ Classement d√©taill√© pour comparer deux images üôáüèæ üíÖüèΩ üñïüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous pr√©sente la traduction de l'article ¬´Image Similarity using Deep Ranking¬ª par Akarsh Zingade. 

 Algorithme de classement profo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Classement d√©taill√© pour comparer deux images</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/457928/">  Bonjour, Habr!  Je vous pr√©sente la traduction de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´Image Similarity using Deep Ranking¬ª</a> par Akarsh Zingade. <br><br><h2>  Algorithme de classement profond </h2><br>  Le concept de " <i>similitude de deux images</i> " n'a pas √©t√© introduit, introduisons donc ce concept au moins dans le cadre de l'article. <br><br>  <b>La similitude des deux images</b> r√©sulte de la comparaison de deux images selon certains crit√®res.  Sa mesure quantitative d√©termine le degr√© de similitude entre les diagrammes d'intensit√© de deux images.  √Ä l'aide d'une mesure de similitude, certaines fonctionnalit√©s d√©crivant les images sont compar√©es.  Comme mesure de similitude, la distance de Hamming, la distance euclidienne, la distance de Manhattan, etc. <br><a name="habracut"></a><br>  <b>Deep Ranking</b> - √©tudie la similitude de l'image √† grain fin, caract√©risant le rapport de similitude de l'image finement divis√©e √† l'aide d'un ensemble de triplets. <br><br><h3>  Qu'est-ce qu'un triplet? </h3><br>  Le triplet contient l'image de la demande, l'image positive et n√©gative.  O√π une image positive ressemble plus √† une image de demande qu'√† une image n√©gative. <br><br>  <b>Un exemple d'un ensemble de triplets:</b> <br><br><img src="https://cdn-images-1.medium.com/max/1000/0*vw4M7uZ5exyLZfLv." alt="image"><br><br>  La premi√®re, la deuxi√®me et la troisi√®me ligne correspondent √† l'image de la demande.  La deuxi√®me ligne (images positives) ressemble plus √† des images de demande que la troisi√®me (images n√©gatives). <br><br><h3>  Architecture de r√©seau de classement profond </h3><br>  Le r√©seau se compose de 3 parties: l'√©chantillonnage en triplets, ConvNet et une couche de classement. <br>  Le r√©seau accepte des triplets d'images en entr√©e.  Un triplet d'images contient une image de demande <math> </math> $ inline $ p_i $ inline $   image positive <math> </math> $ en ligne $ p_i ^ + $ en ligne $   et image n√©gative <math> </math> $ inline $ p_i ^ - $ inline $   qui sont transmis ind√©pendamment √† trois r√©seaux de neurones profonds identiques. <br><br>  La couche de classement la plus √©lev√©e - √©value la fonction de perte de triplet.  Cette erreur est corrig√©e dans les couches inf√©rieures afin de minimiser la fonction de perte. <br><img src="https://cdn-images-1.medium.com/max/1000/0*ICR1FjUFC8xHXoh1." alt="image"><br><br>  Examinons maintenant de plus pr√®s la couche interm√©diaire: <br><br><img src="https://cdn-images-1.medium.com/max/1000/0*tOd5HC3R-kFqobpM." alt="image"><br><br>  ConvNet peut √™tre n'importe quel r√©seau neuronal profond (cet article traitera de l'une des impl√©mentations du r√©seau neuronal convolutionnel VGG16).  ConvNet contient des couches convolutives, une couche de pool maximum, des couches de normalisation locale et des couches enti√®rement connect√©es. <br>  Les deux autres parties re√ßoivent des images avec un taux d'√©chantillonnage r√©duit et effectuent l'√©tape de convolution et la mise en commun maximale.  Ensuite, l'√©tape de normalisation des trois parties a lieu et √† la fin, elles sont combin√©es avec une couche lin√©aire avec normalisation ult√©rieure. <br><br><h3>  Formation de triplets </h3><br>  Il existe plusieurs fa√ßons de cr√©er un fichier triplet, par exemple, utiliser une √©valuation d'expert.  Mais cet article utilisera l'algorithme suivant: <br><br><ol><li>  Chaque image de la classe forme une image de demande. </li><li>  Chaque image, √† l'exception de l'image de demande, formera une image positive.  Mais vous pouvez limiter le nombre d'images positives pour chaque demande d'image </li><li>  Une image n√©gative est s√©lectionn√©e au hasard dans n'importe quelle classe qui n'est pas une classe d'image de demande </li></ol><br><h3>  Fonction de perte de triplet </h3><br>  L'objectif est de former une fonction qui attribue une petite distance pour les images les plus similaires et une grande pour les diff√©rentes.  Cela peut s'exprimer comme suit: <br><img src="https://cdn-images-1.medium.com/max/1000/0*kC_DghyAeP7ulsGL." alt="image"><br>  O√π <b>l</b> est le coefficient de perte pour les triplets, <b>g</b> est le coefficient d'√©cart entre la distance entre deux paires d'images: ( <math> </math> $ inline $ p_i $ inline $   , <math> </math> $ en ligne $ p_i ^ + $ en ligne $   ) et ( <math> </math> $ inline $ p_i $ inline $   , <math> </math> $ inline $ p_i ^ - $ inline $   ), <b>f</b> - fonction d'int√©gration qui affiche l'image dans un vecteur, <math> </math> $ inline $ p_i $ inline $   Est l'image de la demande, <math> </math> $ en ligne $ p_i ^ + $ en ligne $   Est une image positive, <math> </math> $ inline $ p_i ^ - $ inline $   Est une image n√©gative et <b>D</b> est la distance euclidienne entre deux points euclidiens. <br><br><h3>  Impl√©mentation d'un algorithme de classement en profondeur </h3><br>  Impl√©mentation avec Keras. <br><br>  Trois r√©seaux parall√®les sont utilis√©s pour la requ√™te, l'image positive et n√©gative. <br><br>  La mise en ≈ìuvre comprend trois parties principales: <br><br><ol><li>  Impl√©mentation de trois r√©seaux neuronaux multi√©chelles parall√®les </li><li>  Impl√©mentation de la fonction de perte </li><li>  G√©n√©ration de triplets </li></ol><br>  L'apprentissage de trois r√©seaux profonds parall√®les consommera beaucoup de ressources m√©moire.  Au lieu de trois r√©seaux profonds parall√®les qui re√ßoivent une image de demande, une image positive et une image n√©gative, ces images seront transmises s√©quentiellement √† un r√©seau neuronal profond √† l'entr√©e d'un r√©seau neuronal.  Le tenseur transf√©r√© √† la couche de perte contiendra une pi√®ce jointe image dans chaque ligne.  Chaque ligne correspond √† chaque image d'entr√©e dans un paquet.  √âtant donn√© que l'image de demande, l'image positive et l'image n√©gative sont transmises s√©quentiellement, la premi√®re ligne correspondra √† l'image de demande, la seconde √† l'image positive et la troisi√®me √† l'image n√©gative, puis r√©p√©t√©e jusqu'√† la fin du paquet.  Ainsi, la couche de classement re√ßoit une int√©gration de toutes les images.  Apr√®s cela, la fonction de perte est calcul√©e. <br><br>  Pour impl√©menter la couche de classement, nous devons √©crire notre propre fonction de perte, qui calculera la distance euclidienne entre l'image de demande et l'image positive, ainsi que la distance euclidienne entre l'image de demande et l'image n√©gative. <br><br><div class="spoiler">  <b class="spoiler_title">Impl√©mentation de la fonction de calcul des pertes</b> <div class="spoiler_text"><pre><code class="python hljs">_EPSILON = K.epsilon() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_loss_tensor</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> y_pred = K.clip(y_pred, _EPSILON, <span class="hljs-number"><span class="hljs-number">1.0</span></span>-_EPSILON) loss = tf.convert_to_tensor(<span class="hljs-number"><span class="hljs-number">0</span></span>,dtype=tf.float32) <span class="hljs-comment"><span class="hljs-comment"># initialise the loss variable to zero g = tf.constant(1.0, shape=[1], dtype=tf.float32) # set the value for constant 'g' for i in range(0,batch_size,3): try: q_embedding = y_pred[i+0] # procure the embedding for query image p_embedding = y_pred[i+1] # procure the embedding for positive image n_embedding = y_pred[i+2] # procure the embedding for negative image D_q_p = K.sqrt(K.sum((q_embedding - p_embedding)**2)) # calculate the euclidean distance between query image and positive image D_q_n = K.sqrt(K.sum((q_embedding - n_embedding)**2)) # calculate the euclidean distance between query image and negative image loss = (loss + g + D_q_p - D_q_n ) # accumulate the loss for each triplet except: continue loss = loss/(batch_size/3) # Average out the loss zero = tf.constant(0.0, shape=[1], dtype=tf.float32) return tf.maximum(loss,zero)</span></span></code> </pre> <br></div></div><br>  La taille du paquet doit toujours √™tre un multiple de 3. Puisqu'un triplet contient 3 images et que les images du triplet sont transmises s√©quentiellement (nous envoyons chaque image √† un r√©seau neuronal profond s√©quentiellement) <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le reste du code est ici</a> <br><br><div class="spoiler">  <b class="spoiler_title">Les r√©f√©rences</b> <div class="spoiler_text">  [1] Reconnaissance d'objets √† partir de caract√©ristiques invariantes √† l'√©chelle locale - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.cs.ubc.ca/~lowe/papers/iccv99.pdf</a> <br><br>  [2] Histogrammes de gradients orient√©s pour la d√©tection humaine - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">courses.engr.illinois.edu/ece420/fa2017/hog_for_human_detection.pdf</a> <br><br>  [3] Apprentissage de la similitude des images √† grain fin avec le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">classement approfondi - static.googleusercontent.com/media/research.google.com/en//pubs/archive/42945.pdf</a> <br><br>  [4] Classification ImageNet avec les r√©seaux de neurones √† convolution profonde- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a> <br><br>  [5] Abandon: un moyen simple d'emp√™cher les r√©seaux de neurones de sur- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">adapter - www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a> <br><br>  [6] ImageNet: une base de donn√©es d'images hi√©rarchiques √† grande √©chelle - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.image-net.org/papers/imagenet_cvpr09.pdf</a> <br><br>  [7] Fast Multiresolution Image <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Querying-grail.cs.washington.edu/projects/query/mrquery.pdf</a> <br><br>  [8] R√©cup√©ration d'image √† grande √©chelle avec des vecteurs Fisher compress√©s - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.401.9140&amp;rep=rep1&amp;type=pdf</a> <br><br>  [9] Au-del√† des sacs de fonctionnalit√©s: correspondance des pyramides spatiales pour reconna√Ætre les cat√©gories de sc√®nes naturelles - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ieeexplore.ieee.org/document/1641019</a> <br><br>  [10] √âchantillonnage coh√©rent am√©lior√©, minhash pond√©r√© et esquisse L1 - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">static.googleusercontent.com/media/research.google.com/en//pubs/archive/36928.pdf</a> <br><br>  [11] Apprentissage en ligne √† grande √©chelle de la similitude d'images par le biais du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">classement- jmlr.csail.mit.edu/papers/volume11/chechik10a/chechik10a.pdf</a> <br><br>  [12] Apprentissage de la similitude des images √† grain fin avec Deep Ranking- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">users.eecs.northwestern.edu/~jwa368/pdfs/deep_ranking.pdf</a> <br><br>  [13] Similitude d'image avec Deep Ranking- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">medium.com/@akarshzingade/image-similarity-using-deep-ranking-c1bd83855978</a> <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr457928/">https://habr.com/ru/post/fr457928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr457902/index.html">Mitap pour la science des donn√©es</a></li>
<li><a href="../fr457904/index.html">Radio atomique - la toute premi√®re √©mission musicale</a></li>
<li><a href="../fr457906/index.html">Les m√©decins pensent que dans un avenir proche, des dispositifs de fabrication de vaccins appara√Ætront dans les maisons et les pharmacies</a></li>
<li><a href="../fr457910/index.html">WebFPGA - D√©veloppement Verilog dans le navigateur</a></li>
<li><a href="../fr457920/index.html">Jet world: acc√®s libre et gratuit aux rapports de la conf√©rence Joker 2018 + bilan du top 10</a></li>
<li><a href="../fr457930/index.html">Saisie dynamique statiquement s√©curis√©e √† la Python</a></li>
<li><a href="../fr457932/index.html">Analyse du concours IDS Bypass aux Positive Hack Days 9</a></li>
<li><a href="../fr457936/index.html">Nous vous invitons √† la premi√®re conf√©rence Zabbix en Russie</a></li>
<li><a href="../fr457940/index.html">Comment scruter la contrepartie</a></li>
<li><a href="../fr457942/index.html">Ce que j'ai appris sur l'optimisation en Python</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>