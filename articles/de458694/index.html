<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⬜️ 👨‍✈️ 💳 Ist Python GIL wirklich tot? 👨🏼‍🚀 👨🏼‍🏭 🖕🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Am kommenden Montag beginnen die Kurse in der neuen Gruppe des Python Developer- Kurses. Dies bedeutet, dass wir Zeit haben, ein wei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ist Python GIL wirklich tot?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/458694/">  Hallo allerseits!  Am kommenden Montag beginnen die Kurse in der neuen Gruppe des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python Developer-</a> Kurses. Dies bedeutet, dass wir Zeit haben, ein weiteres interessantes Material zu veröffentlichen, das wir jetzt durchführen werden.  Gute Lektüre. <br><br><img src="https://habrastorage.org/webt/jb/cq/wj/jbcqwjrmctxos6x_uzhptngfd9y.png"><br><br>  Bereits 2003 veröffentlichte Intel den neuen Pentium 4 „HT“ -Prozessor.  Dieser Prozessor übertaktete auf 3 GHz und unterstützte die Hyper-Threading-Technologie. <a name="habracut"></a><br><br><img src="https://habrastorage.org/webt/9d/z1/es/9dz1esccmgms80liftaeolcqiui.jpeg"><br><br>  In den folgenden Jahren hatten Intel und AMD Probleme, die beste Desktop-Leistung zu erzielen, indem sie die Busgeschwindigkeit, die L2-Cache-Größe und die Matrixgröße erhöhten, um die Latenz zu minimieren.  Im Jahr 2004 wurde das HT-Modell mit einer Frequenz von 3 GHz durch das 580 Prescott-Modell mit Übertaktung auf 4 GHz ersetzt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/AmwzUrL3vMc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Es schien, dass es nur notwendig war, die Taktfrequenz zu erhöhen, aber die neuen Prozessoren litten unter hohem Stromverbrauch und Wärmeableitung. <br><br>  Liefert Ihr Desktop-Prozessor heute 4 GHz?  Dies ist unwahrscheinlich, da der Weg zur Verbesserung der Leistung letztendlich in der Erhöhung der Busgeschwindigkeit und der Erhöhung der Anzahl der Kerne liegt.  Im Jahr 2006 ersetzte Intel Core 2 den Pentium 4 und hatte eine viel niedrigere Taktrate. <br><br>  Neben der Veröffentlichung von Multi-Core-Prozessoren für ein breites Benutzerpublikum geschah 2006 noch etwas anderes.  Python 2.5 hat endlich das Licht erblickt!  Es kam bereits mit einer Beta-Version des with-Schlüsselworts, die Sie alle kennen und lieben. <br><br>  Python 2.5 hatte eine wesentliche Einschränkung bei der Verwendung von Intel Core 2 oder AMD Athlon X2. <br>  Es war eine Gil. <br><br><h2>  Was ist eine GIL? </h2><br>  GIL (Global Interpreter Lock) ist ein boolescher Wert im Python-Interpreter, der durch einen Mutex geschützt ist.  Die Sperre wird in der Haupt-CPython-Bytecode-Berechnungsschleife verwendet, um zu bestimmen, welcher Thread gerade Anweisungen ausführt. <br><br>  CPython unterstützt die Verwendung mehrerer Threads in einem einzelnen Interpreter. Threads müssen jedoch den Zugriff auf die GIL anfordern, um Operationen auf niedriger Ebene ausführen zu können.  Dies bedeutet wiederum, dass Python-Entwickler asynchronen Code und Multithreading verwenden können und sich nicht mehr um das Blockieren von Variablen oder Abstürze auf Prozessorebene während Deadlocks kümmern müssen. <br><br>  GIL vereinfacht die Multithread-Python-Programmierung. <br><br><img src="https://habrastorage.org/webt/lg/yz/3h/lgyz3hoq07fkumzp4axuuiqxplk.gif"><br><br>  GIL sagt uns auch, dass CPython zwar Multithread-fähig sein kann, jedoch jeweils nur ein Thread ausgeführt werden kann.  Dies bedeutet, dass Ihr Quad-Core-Prozessor so etwas tut (mit Ausnahme des blauen Bildschirms hoffentlich). <br><br>  Die aktuelle Version von GIL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde 2009 geschrieben</a> , um asynchrone Funktionen zu unterstützen, und blieb auch nach vielen Versuchen, sie im Prinzip zu entfernen oder die Anforderungen dafür zu ändern, unberührt. <br><br>  Jeder Vorschlag, die GIL zu entfernen, wurde durch die Tatsache gerechtfertigt, dass das globale Sperren des Interpreters die Leistung von Single-Threaded-Code nicht beeinträchtigen sollte.  Jeder, der 2003 versucht hat, Hyperthreading zu aktivieren, wird verstehen, wovon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ich spreche</a> . <br><br><h2>  Gil Verlassenheit in CPython </h2><br>  Wenn Sie den Code in CPython wirklich parallelisieren möchten, müssen Sie mehrere Prozesse verwenden. <br><br>  In CPython 2.6 wurde das <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Multiprozessor-</a></i> Modul zur Standardbibliothek hinzugefügt.  Multiprocessing maskierte die Generierung von Prozessen in CPython (jeder Prozess mit seiner eigenen GIL). <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Process <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(name)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'hello'</span></span>, name <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: p = Process(target=f, args=(<span class="hljs-string"><span class="hljs-string">'bob'</span></span>,)) p.start() p.join()</code> </pre> <br><br>  Prozesse werden erstellt, Befehle werden mit kompilierten Modulen und Python-Funktionen an sie gesendet und dann wieder mit dem Hauptprozess verbunden. <br><br>  Multiprocessing unterstützt auch die Verwendung von Variablen über eine Warteschlange oder einen Kanal.  Sie hat ein Sperrobjekt, mit dem Objekte im Hauptprozess gesperrt und aus anderen Prozessen geschrieben werden. <br><br>  Multiprocessing hat einen großen Nachteil.  Es ist mit einer erheblichen Rechenlast verbunden, die sich sowohl auf die Verarbeitungszeit als auch auf die Speichernutzung auswirkt.  Die CPython-Startzeit beträgt auch ohne Site 100-200 ms (weitere Informationen finden Sie unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://hackernoon.com/which-is-the-fastest-version-of-python-2ae7c61a6b2b</a> ). <br><br>  Infolgedessen verfügen Sie möglicherweise über parallelen Code in CPython, müssen jedoch die Arbeit lang laufender Prozesse, die mehrere Objekte gemeinsam nutzen, sorgfältig planen. <br><br>  Eine andere Alternative kann darin bestehen, ein Paket eines Drittanbieters wie Twisted zu verwenden. <br><br><h2>  PEP554 und der Tod von GIL? </h2><br>  Ich möchte Sie daran erinnern, dass Multithreading in CPython einfach ist, in Wirklichkeit jedoch keine Parallelisierung, sondern Multiprocessing parallel ist, jedoch einen erheblichen Overhead mit sich bringt. <br><br>  <i>Was ist, wenn es einen besseren Weg gibt?</i> <br>  Der Schlüssel zur Umgehung der GIL liegt im Namen. Die globale Sperrung des Interpreters ist Teil des globalen Status des Interpreters.  CPython-Prozesse können mehrere Interpreter und daher mehrere Sperren haben. Diese Funktion wird jedoch selten verwendet, da der Zugriff nur über die C-API erfolgt. <br><br>  Eine der Funktionen von CPython 3.8 ist PEP554, eine Implementierung von Unterinterpreten und APIs mit einem neuen <code>interpreters</code> in der Standardbibliothek. <br><br>  Auf diese Weise können Sie in einem einzigen Prozess mehrere Interpreter aus Python erstellen.  Eine weitere Neuerung von Python 3.8 ist, dass alle Interpreter ihre eigene GIL haben. <br><br><img src="https://habrastorage.org/webt/bq/nc/m2/bqncm29jhm-ytakgrlkasbfe_6y.png"><br><br>  Da der Status des Interpreters eine im Speicher zugewiesene Region enthält, eine Sammlung aller Zeiger auf Python-Objekte (lokal und global), können Subinterpreter in PEP554 nicht auf die globalen Variablen anderer Interpreter zugreifen. <br><br>  Interpreter, die Objekte gemeinsam nutzen, bestehen wie Multiprocessing darin, sie zu serialisieren und das IPC-Formular (Netzwerk, Festplatte oder gemeinsam genutzter Speicher) zu verwenden.  Es gibt viele Möglichkeiten, Objekte in Python zu serialisieren, z. B. das <code>simplexml</code> Modul, das <code>pickle</code> Modul oder standardisierte Methoden wie <code>json</code> oder <code>simplexml</code> .  Jeder von ihnen hat seine Vor- und Nachteile, und alle geben eine Rechenlast. <br><br>  Es wäre am besten, einen gemeinsamen Speicherplatz zu haben, der durch einen bestimmten Prozess geändert und gesteuert werden kann.  Somit können Objekte vom Hauptinterpreter gesendet und von einem anderen Interpreter empfangen werden.  Dies ist der verwaltete Speicherplatz für die Suche nach PyObject-Zeigern, auf die jeder Interpreter zugreifen kann, während der Hauptprozess die Sperren verwaltet. <br><br><img src="https://habrastorage.org/webt/be/ww/d8/bewwd8ju-3akmyhs7ujq7xmyliy.png"><br><br>  Eine API dafür wird noch entwickelt, aber sie wird wahrscheinlich ungefähr so ​​aussehen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> _xxsubinterpreters <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> interpreters <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> threading <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> textwrap <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> marshal <span class="hljs-comment"><span class="hljs-comment"># Create a sub-interpreter interpid = interpreters.create() # If you had a function that generated some data arry = list(range(0,100)) # Create a channel channel_id = interpreters.channel_create() # Pre-populate the interpreter with a module interpreters.run_string(interpid, "import marshal; import _xxsubinterpreters as interpreters") # Define a def run(interpid, channel_id): interpreters.run_string(interpid, tw.dedent(""" arry_raw = interpreters.channel_recv(channel_id) arry = marshal.loads(arry_raw) result = [1,2,3,4,5] # where you would do some calculating result_raw = marshal.dumps(result) interpreters.channel_send(channel_id, result_raw) """), shared=dict( channel_id=channel_id ), ) inp = marshal.dumps(arry) interpreters.channel_send(channel_id, inp) # Run inside a thread t = threading.Thread(target=run, args=(interpid, channel_id)) t.start() # Sub interpreter will process. Feel free to do anything else now. output = interpreters.channel_recv(channel_id) interpreters.channel_release(channel_id) output_arry = marshal.loads(output) print(output_arry)</span></span></code> </pre> <br><br>  In diesem Beispiel wird NumPy verwendet.  Das Numpy-Array wird über den Kanal gesendet, es wird mithilfe des <code>marshal</code> serialisiert, und der Subinterpreter verarbeitet die Daten (auf einer separaten GIL), sodass möglicherweise ein Parallelisierungsproblem mit der CPU verbunden ist, das für Subinterpreter ideal ist. <br><br><h4>  <b>Es sieht ineffizient aus</b> </h4><br>  Das <code>marshal</code> arbeitet sehr schnell, aber nicht so schnell wie das direkte Freigeben von Objekten aus dem Speicher. <br><br>  PEP574 führt ein neues <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pickle-</a> Protokoll <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(v5) ein</a> , das die Fähigkeit unterstützt, Speicherpuffer getrennt vom Rest des Pickle-Streams zu verarbeiten.  Bei großen Datenobjekten bedeutet das Serialisieren aller Objekte auf einmal und das Deserialisieren von einem Subinterpreter viel Overhead. <br><br>  Die neue API kann (rein hypothetisch) wie folgt implementiert werden: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> _xxsubinterpreters <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> interpreters <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> threading <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> textwrap <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pickle <span class="hljs-comment"><span class="hljs-comment"># Create a sub-interpreter interpid = interpreters.create() # If you had a function that generated a numpy array arry = [5,4,3,2,1] # Create a channel channel_id = interpreters.channel_create() # Pre-populate the interpreter with a module interpreters.run_string(interpid, "import pickle; import _xxsubinterpreters as interpreters") buffers=[] # Define a def run(interpid, channel_id): interpreters.run_string(interpid, tw.dedent(""" arry_raw = interpreters.channel_recv(channel_id) arry = pickle.loads(arry_raw) print(f"Got: {arry}") result = arry[::-1] result_raw = pickle.dumps(result, protocol=5) interpreters.channel_send(channel_id, result_raw) """), shared=dict( channel_id=channel_id, ), ) input = pickle.dumps(arry, protocol=5, buffer_callback=buffers.append) interpreters.channel_send(channel_id, input) # Run inside a thread t = threading.Thread(target=run, args=(interpid, channel_id)) t.start() # Sub interpreter will process. Feel free to do anything else now. output = interpreters.channel_recv(channel_id) interpreters.channel_release(channel_id) output_arry = pickle.loads(output) print(f"Got back: {output_arry}")</span></span></code> </pre> <br><h4>  <b>Es sieht gemustert aus</b> </h4><br>  Im Wesentlichen basiert dieses Beispiel auf der Verwendung der API von Subinterpretern auf niedriger Ebene.  Wenn Sie die <code>multiprocessing</code> Bibliothek nicht verwendet haben, werden Ihnen einige Probleme bekannt vorkommen.  Es ist nicht so einfach wie die Stream-Verarbeitung. Sie können diese Funktion beispielsweise (vorerst) nicht einfach mit einer solchen Liste von Eingabedaten in separaten Interpreten ausführen. <br><br>  Sobald dieses PEP mit anderen verschmilzt, werden wir wahrscheinlich mehrere neue APIs in PyPi sehen. <br><br><h3>  Wie viel Overhead hat der Subinterpreter? </h3><br>  <b>Kurze Antwort:</b> Mehr als ein Stream, weniger als ein Prozess. <br>  <b>Lange Antwort: Der</b> Interpreter hat einen eigenen Status, daher muss er Folgendes klonen und initialisieren, obwohl PEP554 die Erstellung von Subinterpreten vereinfacht: <br><br><ul><li>  Module im <code>importlib</code> <code>__main__</code> und <code>importlib</code> ; </li><li>  Der Inhalt des <code>sys</code> ; </li><li>  Eingebaute Funktionen ( <code>print()</code> , <code>assert</code> usw.); </li><li>  Streams; </li><li>  Kernel-Konfiguration. </li></ul><br><br>  Die Kernelkonfiguration kann leicht aus dem Speicher geklont werden, aber das Importieren von Modulen ist nicht so einfach.  Das Importieren von Modulen in Python ist langsam. Wenn das Erstellen eines Subinterpreters das Importieren von Modulen in einen anderen Namespace jedes Mal bedeutet, werden die Vorteile verringert. <br><br><h3>  Was ist mit Asyncio? </h3><br>  Die vorhandene Implementierung der <code>asyncio</code> Ereignisschleife in der Standardbibliothek erstellt <code>asyncio</code> zur Auswertung und <code>asyncio</code> Status im Hauptinterpreter (und teilt daher die GIL). <br><br>  Nach dem Kombinieren von PEP554, wahrscheinlich bereits in Python 3.9, kann eine alternative Implementierung der Ereignisschleife verwendet werden (obwohl dies noch niemand getan hat), die asynchrone Methoden in Subinterpretern parallel ausführt. <br><br><h3>  Klingt cool, wickel mich auch ein! </h3><br>  Nun, nicht wirklich. <br>  Da CPython so lange auf demselben Interpreter ausgeführt wurde, verwenden viele Teile der Codebasis "Laufzeitstatus" anstelle von "Interpreterstatus". Wenn PEP554 jetzt eingeführt würde, gäbe es immer noch viele Probleme. <br><br>  Beispielsweise gehört der Status des Garbage Collector (in Version 3.7 &lt;) zur Laufzeit. <br><br>  Bei Änderungen während PyCon-Sprints begann sich der Status des Garbage Collectors auf den Interpreter zu verschieben, sodass jeder Subinterpreter seinen eigenen Garbage Collector hatte (wie es sein sollte). <br><br>  Ein weiteres Problem ist, dass es einige „globale“ Variablen gibt, die in der CPython-Codebasis zusammen mit vielen Erweiterungen in C verblieben sind. Als die Leute plötzlich anfingen, ihren Code korrekt zu parallelisieren, sahen wir einige Probleme. <br><br>  Ein weiteres Problem besteht darin, dass die Dateideskriptoren zum Prozess gehören. Wenn Sie also eine Datei zum Schreiben in einem Interpreter geöffnet haben, kann der Subinterpreter nicht auf diese Datei zugreifen (ohne weitere Änderungen an CPython). <br><br>  Kurz gesagt, es gibt noch viele Probleme, die angegangen werden müssen. <br><br><h2>  Fazit: Ist GIL noch wahr? </h2><br>  GIL wird weiterhin für Single-Threaded-Anwendungen verwendet.  Selbst wenn Sie PEP554 folgen, wird Ihr Single-Thread-Code daher plötzlich nicht mehr parallel. <br>  Wenn Sie parallelen Code in Python 3.8 schreiben möchten, treten Parallelisierungsprobleme im Zusammenhang mit dem Prozessor auf, dies ist jedoch auch ein Ticket für die Zukunft! <br><br><h2>  Wann? </h2><br>  Pickle v5 und Speicherfreigabe für die Mehrfachverarbeitung werden höchstwahrscheinlich in Python 3.8 (Oktober 2019) verfügbar sein, und Subinterpreter werden zwischen den Versionen 3.8 und 3.9 angezeigt. <br>  Wenn Sie mit den vorgestellten Beispielen herumspielen möchten, habe ich einen separaten Zweig mit dem erforderlichen Code erstellt: <a href="">https://github.com/tonybaloney/cpython/tree/subinterpreters.</a> <br><br>  Was denkst du darüber?  Schreiben Sie Ihre Kommentare und wir sehen uns auf dem Kurs. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458694/">https://habr.com/ru/post/de458694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458684/index.html">ICANN entfernt Preisschwelle für .org-Domain - warum die IT-Community dagegen ist und was als nächstes passieren wird</a></li>
<li><a href="../de458686/index.html">@ Pythonetc Juni 2019</a></li>
<li><a href="../de458688/index.html">Tipps und Tricks von meinem Telegramm-Kanal @pythonetc, Juni 2019</a></li>
<li><a href="../de458690/index.html">Automatisieren Sie es! Wie wir Integrationstests verbessert haben</a></li>
<li><a href="../de458692/index.html">"Vielleicht" -Monade durch Async / Warten in C # (Keine Aufgaben!)</a></li>
<li><a href="../de458696/index.html">Texturierung oder was Sie wissen müssen, um ein Oberflächenkünstler zu werden. Teil 3. Züchterrechte und Materialien</a></li>
<li><a href="../de458698/index.html">Der Weg des Friedens und der Weg des Krieges in IT-Projekten</a></li>
<li><a href="../de458702/index.html">Schlittenhunde: Was Sie über sie wissen müssen und wie sie gebracht wurden</a></li>
<li><a href="../de458704/index.html">Implementierung eines DLP-Systems am Beispiel des Einzelhandels</a></li>
<li><a href="../de458706/index.html">Gopniks sind jetzt auf ausländischen Märkten oder "Warum ist es so schwierig, einen normalen Programmierer zu finden?"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>