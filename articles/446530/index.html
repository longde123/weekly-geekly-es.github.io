<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤳🏼 🎿 💙 Word2vec en imágenes 🤳🏾 👧🏿 🎌</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="“ Todo oculta un patrón que es parte del universo. Tiene simetría, elegancia y belleza , cualidades que, en primer lugar, son captadas por todos los v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec en imágenes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">“ <b>Todo oculta un patrón que es parte del universo.</b></font>  <font color="gray"><b>Tiene simetría, elegancia y belleza</b> , cualidades que, en primer lugar, son captadas por todos los verdaderos artistas que capturan el mundo.</font>  <font color="gray">Este patrón puede verse en el cambio de estaciones, en cómo fluye la arena a lo largo de la pendiente, en las ramas enredadas de un arbusto de creosota, en el patrón de su hoja.</font> <font color="gray"><br><br></font>  <font color="gray">Estamos tratando de copiar este patrón en nuestra vida y en nuestra sociedad y, por lo tanto, amamos el ritmo, la canción, el baile, diversas formas que nos hacen felices y nos consuelan.</font>  <font color="gray">Sin embargo, también se puede discernir el peligro que acecha en la búsqueda de la perfección absoluta, ya que es obvio que el patrón perfecto no cambia.</font>  <font color="gray">Y, acercándose a la perfección, todas las cosas mueren "- <i>Dune</i> (1965)</font> </blockquote><br>  Creo que el concepto de incrustaciones es una de las ideas más notables en el aprendizaje automático.  Si alguna vez ha utilizado Siri, Google Assistant, Alexa, Google Translate o incluso un teclado de teléfono inteligente con la predicción de la siguiente palabra, entonces ya ha trabajado con el modelo de procesamiento de lenguaje natural basado en archivos adjuntos.  Durante las últimas décadas, este concepto ha experimentado un desarrollo significativo para los modelos neuronales (los desarrollos recientes incluyen la inserción de palabras contextualizadas en modelos avanzados como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BERT</a> y GPT2). <br><a name="habracut"></a><br>  Word2vec es un método efectivo de creación de inversiones desarrollado en 2013.  Además de trabajar con palabras, algunos de sus conceptos fueron efectivos para desarrollar mecanismos de recomendación y dar significado a los datos incluso en tareas comerciales y no lingüísticas.  Esta tecnología ha sido utilizada por compañías como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spotify</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Anghami</a> en sus motores de recomendación. <br><br>  En este artículo, veremos el concepto y la mecánica de generar archivos adjuntos usando word2vec.  Comencemos con un ejemplo para familiarizarse con la forma de representar objetos en forma vectorial.  ¿Sabes cuánto puede decir una lista de cinco números (vector) sobre tu personalidad? <br><br><h1>  Personalización: ¿qué eres? </h1><br><blockquote>  <font color="gray">“Te doy el camaleón del desierto;</font>  <font color="gray">su habilidad para fusionarse con la arena le dirá todo lo que necesita saber sobre las raíces de la ecología y las razones para preservar su personalidad ".</font>  <font color="gray">- <i>Niños de la duna</i></font> </blockquote><br>  En una escala de 0 a 100, ¿tiene un tipo de personalidad introvertida o extrovertida (donde 0 es el tipo más introvertido y 100 es el tipo más extrovertido)?  ¿Alguna vez ha pasado una prueba de personalidad: por ejemplo, MBTI, o mejor aún <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">, los Cinco Grandes</a> ?  Se le da una lista de preguntas y luego se evalúa en varios ejes, incluida la introversión / extroversión. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Ejemplo de los resultados de la prueba Big Five.</font></i>  <i><font color="gray">Realmente dice mucho sobre la personalidad y es capaz de predecir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el éxito</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">académico</a> , <a href="">personal</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">profesional</a> .</font></i>  <i><font color="gray">Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí</a> puedes verlo.</font></i> <br><br>  Supongamos que obtuve 38 de 100 para evaluar introversión / extraversión.  Esto se puede representar de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  O en una escala de −1 a +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  ¿Qué tan bien reconocemos a una persona solo de esta evaluación?  En realidad no  Los humanos son criaturas complejas.  Por lo tanto, agregamos una dimensión más: una característica más de la prueba. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Puede imaginar estas dos dimensiones como un punto en el gráfico o, mejor aún, como un vector desde el origen hasta este punto.</font></i>  <i><font color="gray">Hay excelentes herramientas vectoriales que serán útiles muy pronto.</font></i> <br><br>  No muestro qué rasgos de personalidad ponemos en la tabla para que no te apegues a rasgos específicos, sino que entiendas de inmediato la representación vectorial de la personalidad de una persona como un todo. <br><br>  Ahora podemos decir que este vector refleja parcialmente mi personalidad.  Esta es una descripción útil al comparar diferentes personas.  Supongamos que me atropelló un autobús rojo y necesita reemplazarme con una persona similar.  ¿Cuál de las dos personas en el siguiente cuadro se parece más a mí? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Cuando se trabaja con vectores, la similitud generalmente se calcula mediante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el coeficiente de Otiai</a> (coeficiente geométrico): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">La Persona No. 1 se</font> <font color="gray">parece más a mí en carácter.</font></i>  <i><font color="gray">Los vectores en una dirección (la longitud también es importante) dan un coeficiente de Otiai mayor</font></i> <br><br>  Nuevamente, dos dimensiones no son suficientes para evaluar a las personas.  Décadas de desarrollo de la ciencia psicológica han llevado a la creación de una prueba para cinco características básicas de la personalidad (con muchas más).  Entonces, usemos las cinco dimensiones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  El problema con las cinco dimensiones es que ya no será posible dibujar flechas ordenadas en 2D.  Este es un problema común en el aprendizaje automático, donde a menudo tiene que trabajar en un espacio multidimensional.  Es bueno que el coeficiente geométrico funcione con cualquier cantidad de mediciones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">El coeficiente geométrico funciona para cualquier cantidad de mediciones.</font></i>  <i><font color="gray">En cinco dimensiones, el resultado es mucho más preciso.</font></i> <br><br>  Al final de este capítulo quiero repetir dos ideas principales: <br><br><ol><li>  Las personas (y otros objetos) se pueden representar como vectores numéricos (¡lo cual es genial para los automóviles!). <br></li><li>  Podemos calcular fácilmente qué tan similares son los vectores. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Incrustación de palabras </h1><br><blockquote>  <font color="gray">"El don de las palabras es el don del engaño y la ilusión".</font>  <font color="gray">- <i>Niños de la duna</i></font> </blockquote><br>  Con esta comprensión, pasaremos a las representaciones vectoriales de las palabras obtenidas como resultado del entrenamiento (también se denominan adjuntos) y veremos sus interesantes propiedades. <br><br>  Aquí está el archivo adjunto para la palabra "rey" (vector GloVe, entrenado en Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Vemos una lista de 50 números, pero es difícil decir algo.  Vamos a visualizarlos para compararlos con otros vectores.  Pon los números en una fila: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Colorea las celdas por sus valores (rojo para cerca de 2, blanco para cerca de 0, azul para cerca de −2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Ahora olvídate de los números, y solo por colores contrastamos el "rey" con otras palabras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  ¿Ves que "hombre" y "mujer" están mucho más cerca uno del otro que del "rey"?  Dice algo.  Las representaciones vectoriales capturan mucha información / significado / asociaciones de estas palabras. <br><br>  Aquí hay otra lista de ejemplos (compare columnas con colores similares): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Hay varias cosas para notar: <br><br><ol><li>  A través de todas las palabras pasa una columna roja.  Es decir, estas palabras son similares en esta dimensión particular (y no sabemos lo que está codificado en ella). <br></li><li>  Puedes ver que "mujer" y "niña" son muy similares.  Lo mismo con "hombre" y "niño". <br></li><li>  "Niño" y "niña" también son similares en algunas dimensiones, pero difieren de "mujer" y "hombre".  ¿Podría ser una idea vaga codificada de la juventud?  Probablemente <br></li><li>  Todo, excepto la última palabra, son ideas de la gente.  Agregué un objeto (agua) para mostrar las diferencias entre las categorías.  Por ejemplo, puede ver cómo la columna azul baja y se detiene frente al vector de agua. <br></li><li>  Hay dimensiones claras donde el "rey" y la "reina" son similares entre sí y diferentes de los demás.  ¿Quizás se codifica un concepto vago de realeza allí? </li></ol><br><h1>  Analogías </h1><br><blockquote>  <font color="gray">“Las palabras soportan cualquier carga que deseamos.</font>  <font color="gray">Todo lo que se requiere es un acuerdo sobre la tradición, según el cual construimos conceptos ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Ejemplos famosos que muestran las increíbles propiedades de las inversiones son el concepto de analogías.  Podemos sumar y restar vectores de palabras, obteniendo resultados interesantes.  El ejemplo más famoso es la fórmula "rey - hombre + mujer": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Usando la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gensim</a> en python, podemos sumar y restar vectores de palabras, y la biblioteca encontrará las palabras más cercanas al vector resultante.</font></i>  <i><font color="gray">La imagen muestra una lista de las palabras más similares, cada una con un coeficiente de similitud geométrica.</font></i> <br><br>  Visualizamos esta analogía como antes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">El vector resultante del cálculo "rey - hombre + mujer" no es exactamente igual a la "reina", pero este es el resultado más cercano de 400,000 palabras adjuntas en el conjunto de datos</font></i> <br><br>  Después de considerar el apego de las palabras, aprendamos cómo se lleva a cabo el aprendizaje.  Pero antes de pasar a word2vec, debe echar un vistazo al antepasado conceptual de la inclusión de palabras: un modelo de lenguaje neuronal. <br><br><h1>  Modelo de idioma </h1><br><blockquote>  <font color="gray">“El profeta no está sujeto a las ilusiones del pasado, presente o futuro.</font>  <font color="gray"><b>La fijeza de las formas lingüísticas determina tales diferencias lineales.</b></font>  <font color="gray">Los profetas están sosteniendo la llave de la cerradura de la lengua.</font>  <font color="gray">Para ellos, la imagen física sigue siendo solo una imagen física y nada más.</font> <font color="gray"><br><br></font>  <font color="gray">Su universo no tiene las propiedades de un universo mecánico.</font>  <font color="gray">El observador asume una secuencia lineal de eventos.</font>  <font color="gray">Causa y efecto?</font>  <font color="gray">Es un asunto completamente diferente.</font>  <font color="gray">El Profeta pronuncia palabras fatídicas.</font>  <font color="gray">Ve un vistazo de un evento que debería suceder "de acuerdo con la lógica de las cosas".</font>  <font color="gray">Pero el profeta libera instantáneamente la energía del infinito poder milagroso.</font>  <font color="gray">El universo está experimentando un cambio espiritual ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Un ejemplo de PNL (procesamiento del lenguaje natural) es la función de predicción de la siguiente palabra en el teclado de un teléfono inteligente.  Miles de millones de personas lo usan cientos de veces al día. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Predecir la siguiente palabra es una tarea adecuada para <i>un modelo de lenguaje</i> .  Puede tomar una lista de palabras (digamos, dos palabras) e intentar predecir lo siguiente. <br><br>  En la captura de pantalla anterior, el modelo tomó estas dos palabras verdes ( <code>thou shalt</code> ) y devolvió una lista de opciones (muy probablemente para la palabra <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Podemos imaginar el modelo como una caja negra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Pero en la práctica, el modelo produce más de una palabra.  Deriva una estimación de la probabilidad de prácticamente todas las palabras conocidas (el "diccionario" del modelo varía de varios miles a más de un millón de palabras).  La aplicación de teclado luego encuentra las palabras con las puntuaciones más altas y se las muestra al usuario. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Un modelo de lenguaje neuronal da la probabilidad de todas las palabras conocidas.</font></i>  <i><font color="gray">Indicamos la probabilidad como un porcentaje, pero en el vector resultante el 40% se representará como 0.4</font></i> <br><br>  Después del entrenamiento, los primeros modelos neurales ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bengio 2003</a> ) calcularon el pronóstico en tres etapas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  El primer paso para nosotros es el más relevante, ya que hablamos de inversiones.  Como resultado de la capacitación, se crea una matriz con los archivos adjuntos de todas las palabras en nuestro diccionario.  Para obtener el resultado, simplemente buscamos las incrustaciones de las palabras de entrada y ejecutamos la predicción: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Ahora veamos el proceso de aprendizaje y descubramos cómo se crea esta matriz de inversiones. <br><br><h1>  Entrenamiento de modelo de lenguaje </h1><br><blockquote>  <font color="gray">“El proceso no se puede entender al terminarlo.</font>  <font color="gray">La comprensión debe avanzar junto con el proceso, fusionarse con su flujo y fluir con él "- <i>Dune</i></font> </blockquote><br>  Los modelos de lenguaje tienen una gran ventaja sobre la mayoría de los otros modelos de aprendizaje automático: pueden ser entrenados en textos que tenemos en abundancia.  Piense en todos los libros, artículos, materiales de Wikipedia y otras formas de datos textuales que tenemos.  Compare con otros modelos de aprendizaje automático que necesitan mano de obra y datos especialmente recopilados. <br><br><blockquote>  <b>"Debes aprender la palabra por su compañía" - J. R. Furs</b> </blockquote><br>  Los archivos adjuntos para palabras se calculan de acuerdo con las palabras circundantes, que con mayor frecuencia aparecen cerca.  La mecánica es la siguiente: <br><br><ol><li>  Obtenemos muchos datos de texto (por ejemplo, todos los artículos de Wikipedia) <br></li><li>  Establezca una ventana (por ejemplo, de tres palabras) que se deslice por todo el texto. <br></li><li>  Una ventana deslizante genera patrones para entrenar nuestro modelo. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Cuando esta ventana se desliza sobre el texto, nosotros (en realidad) generamos un conjunto de datos, que luego usamos para entrenar el modelo.  Para comprender, veamos cómo una ventana deslizante maneja esta frase: <br><br><blockquote>  <b>"Que no puedas construir una máquina dotada de la semejanza de la mente humana" - <i>Dune</i></b> </blockquote><br>  Cuando comenzamos, la ventana se encuentra en las primeras tres palabras de la oración: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Tomamos las dos primeras palabras para signos, y la tercera palabra para la etiqueta: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Generamos la primera muestra en un conjunto de datos que luego puede usarse para enseñar un modelo de lenguaje</font></i> <br><br>  Luego movemos la ventana a la siguiente posición y creamos una segunda muestra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Y muy pronto, estamos acumulando un conjunto de datos más grande: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  En la práctica, los modelos generalmente se entrenan directamente en el proceso de mover una ventana deslizante.  Pero lógicamente, la fase de "generación de conjunto de datos" es independiente de la fase de capacitación.  Además de los enfoques de redes neuronales, el método de N-gram a menudo se usaba antes para enseñar modelos de lenguaje (vea el tercer capítulo del libro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Procesamiento del habla y lenguaje"</a> ).  Para ver la diferencia al cambiar de N-gramos a modelos neuronales en productos reales, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí hay una publicación de 2015 en el blog Swiftkey</a> , el desarrollador de mi teclado Android favorito, que presenta su modelo de lenguaje neuronal y lo compara con el modelo N-gram anterior.  Me gusta este ejemplo porque muestra cómo se pueden describir las propiedades algorítmicas de las inversiones en un lenguaje de marketing. <br><br><h1>  Miramos a ambos lados </h1><br><blockquote>  <font color="gray">“Una paradoja es una señal de que deberíamos tratar de considerar lo que hay detrás.</font>  <font color="gray">Si la paradoja te preocupa, significa que estás luchando por lo absoluto.</font>  <font color="gray">Los relativistas ven la paradoja simplemente como un pensamiento interesante, quizás divertido, a veces aterrador, pero un pensamiento muy instructivo ".</font>  <font color="gray"><i>Dios emperador de la duna</i></font> </blockquote><br>  Con base en lo anterior, complete el vacío: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Como contexto, hay cinco palabras anteriores (y una referencia anterior a "bus").  Estoy seguro de que la mayoría de ustedes han adivinado que debería haber un "autobús".  Pero si te doy otra palabra después del espacio, ¿cambiará esto tu respuesta? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Esto cambia completamente la situación: ahora la palabra que falta es muy probablemente "roja".  Obviamente, las palabras tienen valor informativo antes y después de un espacio.  Resulta que la contabilidad en ambas direcciones (izquierda y derecha) le permite calcular mejores inversiones.  Veamos cómo configurar el entrenamiento modelo en tal situación. <br><br><h1>  Saltar gramo </h1><br><blockquote>  <font color="gray">"Cuando se desconoce una elección absolutamente inconfundible, el intelecto tiene la oportunidad de trabajar con datos limitados en la arena, donde los errores no solo son posibles sino también necesarios".</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Además de dos palabras antes del objetivo, puede tener en cuenta dos palabras más después de él. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Entonces el conjunto de datos para la capacitación modelo se verá así: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Esto se llama arquitectura CBOW (Continuous Bag of Words) y se describe en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">uno de los documentos word2vec</a> [pdf].  Hay otra arquitectura, que también muestra excelentes resultados, pero está organizada de manera un poco diferente: trata de adivinar las palabras vecinas por la palabra actual.  Una ventana deslizante se ve así: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">En la ranura verde está la palabra de entrada, y cada campo rosa representa una posible salida</font></i> <br><br>  Los rectángulos rosados ​​tienen diferentes tonos porque esta ventana deslizante en realidad crea cuatro patrones separados en nuestro conjunto de datos de entrenamiento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Este método se llama arquitectura <b>skip-gram</b> .  Puede visualizar una ventana deslizante de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Los siguientes cuatro ejemplos se agregan al conjunto de datos de entrenamiento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Luego movemos la ventana a la siguiente posición: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Lo que genera cuatro ejemplos más: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Pronto tendremos muchas más muestras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revisión de aprendizaje </h1><br><blockquote>  <font color="gray">“Muad'Dib aprendió rápido porque principalmente le enseñaron a aprender.</font>  <font color="gray">Pero la primera lección fue la asimilación de la creencia de que él puede aprender, y esa es la base de todo.</font>  <font color="gray">Es sorprendente cuántas personas no creen que puedan aprender y aprender, y cuántas personas más piensan que aprender es muy difícil ".</font>  <font color="gray">- <i>duna</i></font> </blockquote><br>  Ahora que tenemos el conjunto de omisión de gramo, lo usamos para entrenar el modelo neuronal básico del lenguaje que predice una palabra vecina. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Comencemos con la primera muestra de nuestro conjunto de datos.  Tomamos el letrero y lo enviamos al modelo no capacitado con la solicitud de predecir la siguiente palabra. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  El modelo pasa por tres pasos y muestra un vector de predicción (con probabilidad para cada palabra en el diccionario).  Como el modelo no está entrenado, en esta etapa su pronóstico probablemente sea incorrecto.  Pero eso no es nada.  Sabemos qué palabra predice: esta es la celda resultante en la fila que actualmente utilizamos para entrenar el modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Un "vector objetivo" es aquel en el que la palabra objetivo tiene una probabilidad de 1, y todas las demás palabras tienen una probabilidad de 0</font></i> <br><br>  ¿Qué tan equivocado estaba el modelo?  Reste el vector de pronóstico del objetivo y obtenga el vector de error: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Este vector de error ahora se puede usar para actualizar el modelo, por lo que la próxima vez es más probable que dé un resultado preciso en los mismos datos de entrada. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Aquí termina la primera etapa del entrenamiento.  Continuamos haciendo lo mismo con la siguiente muestra en el conjunto de datos, y luego con la siguiente, hasta que examinemos todas las muestras.  Este es el final de la primera era de aprendizaje.  Repetimos todo una y otra vez durante varias eras, y como resultado obtenemos un modelo entrenado: de él puede extraer la matriz de inversión y usarla en cualquier aplicación. <br><br>  Aunque aprendimos mucho, pero para comprender completamente cómo Word2vec realmente aprende, faltan un par de ideas clave. <br><br><h1>  Selección negativa </h1><br><blockquote>  <font color="gray">“Intentar comprender a Muad'Dib sin comprender a sus enemigos mortales, el Harkonnenov, es lo mismo que tratar de comprender la Verdad sin comprender qué es la Falsedad.</font>  <font color="gray">Este es un intento de conocer la Luz sin conocer la Oscuridad.</font>  <font color="gray">Esto es imposible ".</font>  <font color="gray">- <i>duna</i></font> </blockquote><br>  Recuerde los tres pasos de cómo un modelo neuronal calcula un pronóstico: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  El tercer paso es muy costoso desde el punto de vista computacional, especialmente si lo hace para cada muestra en el conjunto de datos (decenas de millones de veces).  Es necesario aumentar de alguna manera la productividad. <br><br>  Una forma es dividir el objetivo en dos etapas: <br><br><ol><li>  Cree archivos adjuntos de palabras de alta calidad (sin predecir la siguiente palabra). <br></li><li>  Utilice estas inversiones de alta calidad para enseñar el modelo de lenguaje (para pronosticar). </li></ol><br>  Este artículo se centrará en el primer paso.  Para aumentar la productividad, puede alejarse de predecir una palabra vecina ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... y cambie a un modelo que tome palabras de entrada y salida y calcule la probabilidad de su proximidad (de 0 a 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Una transición tan simple reemplaza la red neuronal con un modelo de regresión logística, por lo que los cálculos se vuelven mucho más simples y rápidos. <br><br>  Al mismo tiempo, necesitamos refinar la estructura de nuestro conjunto de datos: la etiqueta ahora es una nueva columna con valores 0 o 1. En nuestra tabla, las unidades están en todas partes, porque agregamos vecinos allí. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Tal modelo se calcula a una velocidad increíble: millones de muestras en minutos.  Pero necesitas cerrar una escapatoria.  Si todos nuestros ejemplos son positivos (objetivo: 1), se puede formar un modelo complicado que siempre devuelve 1, demostrando una precisión del 100%, pero no aprende nada y genera inversiones basura. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Para resolver este problema, debe ingresar <i>patrones negativos</i> en el conjunto de datos, palabras que definitivamente no son vecinas.  Para ellos, el modelo debe devolver 0. Ahora el modelo tendrá que trabajar duro, pero los cálculos aún van a gran velocidad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Para cada muestra en el conjunto de datos, agregue ejemplos negativos etiquetados 0</font></i> <br><br>  Pero, ¿qué presentar como palabras de salida?  Elige las palabras arbitrariamente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Esta idea nació bajo la influencia del método de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">comparación</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ruido</a> [pdf].  Hacemos coincidir la señal real (ejemplos positivos de palabras vecinas) con ruido (palabras seleccionadas al azar que no son vecinas).  Esto proporciona un excelente compromiso entre el rendimiento y el rendimiento estadístico. <br><br><h1>  Muestra negativa de salto de gramo (SGNS) </h1><br>  Observamos dos conceptos centrales de word2vec: juntos se denominan "skip-gram con muestreo negativo". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Aprendiendo word2vec </h1><br><blockquote>  <font color="gray">“Una máquina no puede prever todos los problemas que son importantes para una persona viva.</font>  <font color="gray">Hay una gran diferencia entre el espacio discreto y el continuo continuo.</font>  <font color="gray">Vivimos en un espacio y las máquinas existen en otro ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Habiendo examinado las ideas básicas de skip-gram y muestreo negativo, podemos proceder a una mirada más cercana al proceso de aprendizaje de word2vec. <br><br>  Primero, preprocesamos el texto en el que entrenamos el modelo.  Defina el tamaño del diccionario (lo llamaremos <code>vocab_size</code> ), digamos, en 10,000 archivos adjuntos y los parámetros de las palabras en el diccionario. <br><br>  Al comienzo de la capacitación, creamos dos matrices: <code>Embedding</code> y <code>Context</code> .  Los <code>vocab_size</code> adjuntos para cada palabra se almacenan en estas matrices en nuestro diccionario (por <code>vocab_size</code> tanto, <code>vocab_size</code> es uno de sus parámetros).  El segundo parámetro es la dimensión del archivo adjunto (generalmente <code>embedding_size</code> establece en 300, pero anteriormente vimos un ejemplo con 50 dimensiones). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Primero, inicializamos estas matrices con valores aleatorios.  Luego comenzamos el proceso de aprendizaje.  En cada etapa, tomamos un ejemplo positivo y los negativos asociados con él.  Aquí está nuestro primer grupo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Ahora tenemos cuatro palabras: la palabra de entrada <code>not</code> y las palabras de salida / contextuales <code>thou</code> (vecino real), <code>aaron</code> y <code>taco</code> (ejemplos negativos).  Comenzamos la búsqueda de sus archivos adjuntos en las matrices <code>Embedding</code> (para la palabra de entrada) y <code>Context</code> (para las palabras de contexto), aunque ambas matrices contienen archivos adjuntos para todas las palabras de nuestro diccionario. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Luego calculamos el producto escalar del adjunto de entrada con cada uno de los adjuntos contextuales.  En cada caso, se obtiene un número que indica la similitud de los datos de entrada y los archivos adjuntos contextuales. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Ahora necesitamos una forma de convertir estas estimaciones en un tipo de probabilidad: todas deben ser números positivos entre 0 y 1. Esta es una excelente tarea para las ecuaciones logísticas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sigmoideas</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  El resultado del cálculo sigmoide se puede considerar como la salida del modelo para estas muestras.  Como puede ver, el <code>taco</code> el puntaje más alto, mientras que <code>aaron</code> todavía tiene el puntaje más bajo, tanto antes como después de sigmoide. <br><br>  Cuando el modelo no entrenado hizo un pronóstico y tiene una marca de objetivo real para la comparación, calculemos cuántos errores hay en el pronóstico del modelo.  Para hacer esto, simplemente reste la puntuación sigmoidea de las etiquetas de destino. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Aquí es donde comienza la fase de "aprendizaje" del término "aprendizaje automático".  Ahora podemos usar esta estimación de error para ajustar las inversiones <code>not</code> , <code>thou</code> , <code>aaron</code> y <code>taco</code> , de modo que la próxima vez que el resultado esté más cerca de las estimaciones objetivo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Esto completa una etapa de entrenamiento.  Mejoramos un poco el apego de algunas palabras ( <code>not</code> , <code>thou</code> , <code>aaron</code> y <code>taco</code> ).  Ahora pasamos a la siguiente etapa (la siguiente muestra positiva y las negativas asociadas a ella) y repetimos el proceso. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Los archivos adjuntos continúan mejorando a medida que avanzamos por el conjunto de datos completo varias veces.  Luego puede detener el proceso, dejar a un lado la matriz de <code>Context</code> y usar la matriz de <code>Embeddings</code> capacitada para la siguiente tarea. <br><br><h1>  Tamaño de ventana y número de muestras negativas </h1><br>  En el proceso de aprendizaje de word2vec, dos hiperparámetros clave son el tamaño de la ventana y el número de muestras negativas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Diferentes tamaños de ventana son adecuados para diferentes tareas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Se ha observado</a> que los tamaños de ventana más pequeños (2-15) generan archivos adjuntos <i>intercambiables</i> con índices similares (tenga en cuenta que los antónimos a menudo son intercambiables cuando se miran las palabras circundantes: por ejemplo, las palabras "bueno" y "malo" a menudo se mencionan en contextos similares).  Los tamaños de ventana más grandes (15–50 o incluso más) generan archivos adjuntos <i>relacionados</i> con índices similares.  En la práctica, a menudo tiene que proporcionar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">anotaciones</a> para similitudes semánticas útiles en su tarea.  En Gensim, el tamaño de ventana predeterminado es 5 (dos palabras izquierda y derecha, además de la palabra de entrada en sí). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El número de muestras negativas es otro factor en el proceso de aprendizaje. </font><font style="vertical-align: inherit;">El documento original recomienda 5–20. </font><font style="vertical-align: inherit;">También dice que 2-5 muestras parecen ser suficientes cuando tiene un conjunto de datos suficientemente grande. </font><font style="vertical-align: inherit;">En Gensim, el valor predeterminado es 5 patrones negativos.</font></font><br><br><h1>  Conclusión </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Si tu comportamiento cae más allá de tus estándares, entonces eres una persona viva, no un autómata" - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dios-Emperador de Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que ahora entiendas la incorporación de palabras y la esencia del algoritmo word2vec. </font><font style="vertical-align: inherit;">También espero que ahora comprenda mejor los artículos que mencionan el concepto de "skip-gram con muestreo negativo" (SGNS), como en los sistemas de recomendación mencionados anteriormente.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referencias y lecturas adicionales </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Representaciones distribuidas de palabras y frases y su composición"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«      »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«   »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«   »</a>      —    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«      »</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> —      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">« word2vec»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«  »</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">«»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/446530/">https://habr.com/ru/post/446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446514/index.html">Gmail tiene 15 años</a></li>
<li><a href="../446516/index.html">Visualización del tiempo de renacimiento de Roshan</a></li>
<li><a href="../446518/index.html">Cortafuegos de aplicaciones web</a></li>
<li><a href="../446520/index.html">Cómo comenzó todo: la historia de los drones voladores</a></li>
<li><a href="../446522/index.html">Swift 5.1: ¿qué hay de nuevo?</a></li>
<li><a href="../446532/index.html">Upwork introduce una tarifa por el derecho a escribir a un cliente potencial</a></li>
<li><a href="../446534/index.html">Visual Studio 2019 lanzado</a></li>
<li><a href="../446536/index.html">Colas y JMeter: Intercambio con editor y suscriptor</a></li>
<li><a href="../446538/index.html">PhotoGuru cambió al "lado oscuro" y al "más sabio"</a></li>
<li><a href="../446546/index.html">Microsoft extiende la ventaja de Azure IP con nuevos beneficios de IP para los innovadores y nuevas empresas de Azure IoT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>