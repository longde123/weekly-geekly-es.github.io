<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§≥üèº üéø üíô Word2vec en im√°genes ü§≥üèæ üëßüèø üéå</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚Äú Todo oculta un patr√≥n que es parte del universo. Tiene simetr√≠a, elegancia y belleza , cualidades que, en primer lugar, son captadas por todos los v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec en im√°genes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">‚Äú <b>Todo oculta un patr√≥n que es parte del universo.</b></font>  <font color="gray"><b>Tiene simetr√≠a, elegancia y belleza</b> , cualidades que, en primer lugar, son captadas por todos los verdaderos artistas que capturan el mundo.</font>  <font color="gray">Este patr√≥n puede verse en el cambio de estaciones, en c√≥mo fluye la arena a lo largo de la pendiente, en las ramas enredadas de un arbusto de creosota, en el patr√≥n de su hoja.</font> <font color="gray"><br><br></font>  <font color="gray">Estamos tratando de copiar este patr√≥n en nuestra vida y en nuestra sociedad y, por lo tanto, amamos el ritmo, la canci√≥n, el baile, diversas formas que nos hacen felices y nos consuelan.</font>  <font color="gray">Sin embargo, tambi√©n se puede discernir el peligro que acecha en la b√∫squeda de la perfecci√≥n absoluta, ya que es obvio que el patr√≥n perfecto no cambia.</font>  <font color="gray">Y, acerc√°ndose a la perfecci√≥n, todas las cosas mueren "- <i>Dune</i> (1965)</font> </blockquote><br>  Creo que el concepto de incrustaciones es una de las ideas m√°s notables en el aprendizaje autom√°tico.  Si alguna vez ha utilizado Siri, Google Assistant, Alexa, Google Translate o incluso un teclado de tel√©fono inteligente con la predicci√≥n de la siguiente palabra, entonces ya ha trabajado con el modelo de procesamiento de lenguaje natural basado en archivos adjuntos.  Durante las √∫ltimas d√©cadas, este concepto ha experimentado un desarrollo significativo para los modelos neuronales (los desarrollos recientes incluyen la inserci√≥n de palabras contextualizadas en modelos avanzados como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BERT</a> y GPT2). <br><a name="habracut"></a><br>  Word2vec es un m√©todo efectivo de creaci√≥n de inversiones desarrollado en 2013.  Adem√°s de trabajar con palabras, algunos de sus conceptos fueron efectivos para desarrollar mecanismos de recomendaci√≥n y dar significado a los datos incluso en tareas comerciales y no ling√º√≠sticas.  Esta tecnolog√≠a ha sido utilizada por compa√±√≠as como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spotify</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Anghami</a> en sus motores de recomendaci√≥n. <br><br>  En este art√≠culo, veremos el concepto y la mec√°nica de generar archivos adjuntos usando word2vec.  Comencemos con un ejemplo para familiarizarse con la forma de representar objetos en forma vectorial.  ¬øSabes cu√°nto puede decir una lista de cinco n√∫meros (vector) sobre tu personalidad? <br><br><h1>  Personalizaci√≥n: ¬øqu√© eres? </h1><br><blockquote>  <font color="gray">‚ÄúTe doy el camale√≥n del desierto;</font>  <font color="gray">su habilidad para fusionarse con la arena le dir√° todo lo que necesita saber sobre las ra√≠ces de la ecolog√≠a y las razones para preservar su personalidad ".</font>  <font color="gray">- <i>Ni√±os de la duna</i></font> </blockquote><br>  En una escala de 0 a 100, ¬øtiene un tipo de personalidad introvertida o extrovertida (donde 0 es el tipo m√°s introvertido y 100 es el tipo m√°s extrovertido)?  ¬øAlguna vez ha pasado una prueba de personalidad: por ejemplo, MBTI, o mejor a√∫n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">, los Cinco Grandes</a> ?  Se le da una lista de preguntas y luego se eval√∫a en varios ejes, incluida la introversi√≥n / extroversi√≥n. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Ejemplo de los resultados de la prueba Big Five.</font></i>  <i><font color="gray">Realmente dice mucho sobre la personalidad y es capaz de predecir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el √©xito</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">acad√©mico</a> , <a href="">personal</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">profesional</a> .</font></i>  <i><font color="gray">Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> puedes verlo.</font></i> <br><br>  Supongamos que obtuve 38 de 100 para evaluar introversi√≥n / extraversi√≥n.  Esto se puede representar de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  O en una escala de ‚àí1 a +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  ¬øQu√© tan bien reconocemos a una persona solo de esta evaluaci√≥n?  En realidad no  Los humanos son criaturas complejas.  Por lo tanto, agregamos una dimensi√≥n m√°s: una caracter√≠stica m√°s de la prueba. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Puede imaginar estas dos dimensiones como un punto en el gr√°fico o, mejor a√∫n, como un vector desde el origen hasta este punto.</font></i>  <i><font color="gray">Hay excelentes herramientas vectoriales que ser√°n √∫tiles muy pronto.</font></i> <br><br>  No muestro qu√© rasgos de personalidad ponemos en la tabla para que no te apegues a rasgos espec√≠ficos, sino que entiendas de inmediato la representaci√≥n vectorial de la personalidad de una persona como un todo. <br><br>  Ahora podemos decir que este vector refleja parcialmente mi personalidad.  Esta es una descripci√≥n √∫til al comparar diferentes personas.  Supongamos que me atropell√≥ un autob√∫s rojo y necesita reemplazarme con una persona similar.  ¬øCu√°l de las dos personas en el siguiente cuadro se parece m√°s a m√≠? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Cuando se trabaja con vectores, la similitud generalmente se calcula mediante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el coeficiente de Otiai</a> (coeficiente geom√©trico): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">La Persona No. 1 se</font> <font color="gray">parece m√°s a m√≠ en car√°cter.</font></i>  <i><font color="gray">Los vectores en una direcci√≥n (la longitud tambi√©n es importante) dan un coeficiente de Otiai mayor</font></i> <br><br>  Nuevamente, dos dimensiones no son suficientes para evaluar a las personas.  D√©cadas de desarrollo de la ciencia psicol√≥gica han llevado a la creaci√≥n de una prueba para cinco caracter√≠sticas b√°sicas de la personalidad (con muchas m√°s).  Entonces, usemos las cinco dimensiones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  El problema con las cinco dimensiones es que ya no ser√° posible dibujar flechas ordenadas en 2D.  Este es un problema com√∫n en el aprendizaje autom√°tico, donde a menudo tiene que trabajar en un espacio multidimensional.  Es bueno que el coeficiente geom√©trico funcione con cualquier cantidad de mediciones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">El coeficiente geom√©trico funciona para cualquier cantidad de mediciones.</font></i>  <i><font color="gray">En cinco dimensiones, el resultado es mucho m√°s preciso.</font></i> <br><br>  Al final de este cap√≠tulo quiero repetir dos ideas principales: <br><br><ol><li>  Las personas (y otros objetos) se pueden representar como vectores num√©ricos (¬°lo cual es genial para los autom√≥viles!). <br></li><li>  Podemos calcular f√°cilmente qu√© tan similares son los vectores. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Incrustaci√≥n de palabras </h1><br><blockquote>  <font color="gray">"El don de las palabras es el don del enga√±o y la ilusi√≥n".</font>  <font color="gray">- <i>Ni√±os de la duna</i></font> </blockquote><br>  Con esta comprensi√≥n, pasaremos a las representaciones vectoriales de las palabras obtenidas como resultado del entrenamiento (tambi√©n se denominan adjuntos) y veremos sus interesantes propiedades. <br><br>  Aqu√≠ est√° el archivo adjunto para la palabra "rey" (vector GloVe, entrenado en Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Vemos una lista de 50 n√∫meros, pero es dif√≠cil decir algo.  Vamos a visualizarlos para compararlos con otros vectores.  Pon los n√∫meros en una fila: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Colorea las celdas por sus valores (rojo para cerca de 2, blanco para cerca de 0, azul para cerca de ‚àí2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Ahora olv√≠date de los n√∫meros, y solo por colores contrastamos el "rey" con otras palabras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  ¬øVes que "hombre" y "mujer" est√°n mucho m√°s cerca uno del otro que del "rey"?  Dice algo.  Las representaciones vectoriales capturan mucha informaci√≥n / significado / asociaciones de estas palabras. <br><br>  Aqu√≠ hay otra lista de ejemplos (compare columnas con colores similares): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Hay varias cosas para notar: <br><br><ol><li>  A trav√©s de todas las palabras pasa una columna roja.  Es decir, estas palabras son similares en esta dimensi√≥n particular (y no sabemos lo que est√° codificado en ella). <br></li><li>  Puedes ver que "mujer" y "ni√±a" son muy similares.  Lo mismo con "hombre" y "ni√±o". <br></li><li>  "Ni√±o" y "ni√±a" tambi√©n son similares en algunas dimensiones, pero difieren de "mujer" y "hombre".  ¬øPodr√≠a ser una idea vaga codificada de la juventud?  Probablemente <br></li><li>  Todo, excepto la √∫ltima palabra, son ideas de la gente.  Agregu√© un objeto (agua) para mostrar las diferencias entre las categor√≠as.  Por ejemplo, puede ver c√≥mo la columna azul baja y se detiene frente al vector de agua. <br></li><li>  Hay dimensiones claras donde el "rey" y la "reina" son similares entre s√≠ y diferentes de los dem√°s.  ¬øQuiz√°s se codifica un concepto vago de realeza all√≠? </li></ol><br><h1>  Analog√≠as </h1><br><blockquote>  <font color="gray">‚ÄúLas palabras soportan cualquier carga que deseamos.</font>  <font color="gray">Todo lo que se requiere es un acuerdo sobre la tradici√≥n, seg√∫n el cual construimos conceptos ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Ejemplos famosos que muestran las incre√≠bles propiedades de las inversiones son el concepto de analog√≠as.  Podemos sumar y restar vectores de palabras, obteniendo resultados interesantes.  El ejemplo m√°s famoso es la f√≥rmula "rey - hombre + mujer": <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Usando la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Gensim</a> en python, podemos sumar y restar vectores de palabras, y la biblioteca encontrar√° las palabras m√°s cercanas al vector resultante.</font></i>  <i><font color="gray">La imagen muestra una lista de las palabras m√°s similares, cada una con un coeficiente de similitud geom√©trica.</font></i> <br><br>  Visualizamos esta analog√≠a como antes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">El vector resultante del c√°lculo "rey - hombre + mujer" no es exactamente igual a la "reina", pero este es el resultado m√°s cercano de 400,000 palabras adjuntas en el conjunto de datos</font></i> <br><br>  Despu√©s de considerar el apego de las palabras, aprendamos c√≥mo se lleva a cabo el aprendizaje.  Pero antes de pasar a word2vec, debe echar un vistazo al antepasado conceptual de la inclusi√≥n de palabras: un modelo de lenguaje neuronal. <br><br><h1>  Modelo de idioma </h1><br><blockquote>  <font color="gray">‚ÄúEl profeta no est√° sujeto a las ilusiones del pasado, presente o futuro.</font>  <font color="gray"><b>La fijeza de las formas ling√º√≠sticas determina tales diferencias lineales.</b></font>  <font color="gray">Los profetas est√°n sosteniendo la llave de la cerradura de la lengua.</font>  <font color="gray">Para ellos, la imagen f√≠sica sigue siendo solo una imagen f√≠sica y nada m√°s.</font> <font color="gray"><br><br></font>  <font color="gray">Su universo no tiene las propiedades de un universo mec√°nico.</font>  <font color="gray">El observador asume una secuencia lineal de eventos.</font>  <font color="gray">Causa y efecto?</font>  <font color="gray">Es un asunto completamente diferente.</font>  <font color="gray">El Profeta pronuncia palabras fat√≠dicas.</font>  <font color="gray">Ve un vistazo de un evento que deber√≠a suceder "de acuerdo con la l√≥gica de las cosas".</font>  <font color="gray">Pero el profeta libera instant√°neamente la energ√≠a del infinito poder milagroso.</font>  <font color="gray">El universo est√° experimentando un cambio espiritual ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Un ejemplo de PNL (procesamiento del lenguaje natural) es la funci√≥n de predicci√≥n de la siguiente palabra en el teclado de un tel√©fono inteligente.  Miles de millones de personas lo usan cientos de veces al d√≠a. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Predecir la siguiente palabra es una tarea adecuada para <i>un modelo de lenguaje</i> .  Puede tomar una lista de palabras (digamos, dos palabras) e intentar predecir lo siguiente. <br><br>  En la captura de pantalla anterior, el modelo tom√≥ estas dos palabras verdes ( <code>thou shalt</code> ) y devolvi√≥ una lista de opciones (muy probablemente para la palabra <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Podemos imaginar el modelo como una caja negra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  Pero en la pr√°ctica, el modelo produce m√°s de una palabra.  Deriva una estimaci√≥n de la probabilidad de pr√°cticamente todas las palabras conocidas (el "diccionario" del modelo var√≠a de varios miles a m√°s de un mill√≥n de palabras).  La aplicaci√≥n de teclado luego encuentra las palabras con las puntuaciones m√°s altas y se las muestra al usuario. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Un modelo de lenguaje neuronal da la probabilidad de todas las palabras conocidas.</font></i>  <i><font color="gray">Indicamos la probabilidad como un porcentaje, pero en el vector resultante el 40% se representar√° como 0.4</font></i> <br><br>  Despu√©s del entrenamiento, los primeros modelos neurales ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Bengio 2003</a> ) calcularon el pron√≥stico en tres etapas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  El primer paso para nosotros es el m√°s relevante, ya que hablamos de inversiones.  Como resultado de la capacitaci√≥n, se crea una matriz con los archivos adjuntos de todas las palabras en nuestro diccionario.  Para obtener el resultado, simplemente buscamos las incrustaciones de las palabras de entrada y ejecutamos la predicci√≥n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Ahora veamos el proceso de aprendizaje y descubramos c√≥mo se crea esta matriz de inversiones. <br><br><h1>  Entrenamiento de modelo de lenguaje </h1><br><blockquote>  <font color="gray">‚ÄúEl proceso no se puede entender al terminarlo.</font>  <font color="gray">La comprensi√≥n debe avanzar junto con el proceso, fusionarse con su flujo y fluir con √©l "- <i>Dune</i></font> </blockquote><br>  Los modelos de lenguaje tienen una gran ventaja sobre la mayor√≠a de los otros modelos de aprendizaje autom√°tico: pueden ser entrenados en textos que tenemos en abundancia.  Piense en todos los libros, art√≠culos, materiales de Wikipedia y otras formas de datos textuales que tenemos.  Compare con otros modelos de aprendizaje autom√°tico que necesitan mano de obra y datos especialmente recopilados. <br><br><blockquote>  <b>"Debes aprender la palabra por su compa√±√≠a" - J. R. Furs</b> </blockquote><br>  Los archivos adjuntos para palabras se calculan de acuerdo con las palabras circundantes, que con mayor frecuencia aparecen cerca.  La mec√°nica es la siguiente: <br><br><ol><li>  Obtenemos muchos datos de texto (por ejemplo, todos los art√≠culos de Wikipedia) <br></li><li>  Establezca una ventana (por ejemplo, de tres palabras) que se deslice por todo el texto. <br></li><li>  Una ventana deslizante genera patrones para entrenar nuestro modelo. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Cuando esta ventana se desliza sobre el texto, nosotros (en realidad) generamos un conjunto de datos, que luego usamos para entrenar el modelo.  Para comprender, veamos c√≥mo una ventana deslizante maneja esta frase: <br><br><blockquote>  <b>"Que no puedas construir una m√°quina dotada de la semejanza de la mente humana" - <i>Dune</i></b> </blockquote><br>  Cuando comenzamos, la ventana se encuentra en las primeras tres palabras de la oraci√≥n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Tomamos las dos primeras palabras para signos, y la tercera palabra para la etiqueta: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Generamos la primera muestra en un conjunto de datos que luego puede usarse para ense√±ar un modelo de lenguaje</font></i> <br><br>  Luego movemos la ventana a la siguiente posici√≥n y creamos una segunda muestra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Y muy pronto, estamos acumulando un conjunto de datos m√°s grande: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  En la pr√°ctica, los modelos generalmente se entrenan directamente en el proceso de mover una ventana deslizante.  Pero l√≥gicamente, la fase de "generaci√≥n de conjunto de datos" es independiente de la fase de capacitaci√≥n.  Adem√°s de los enfoques de redes neuronales, el m√©todo de N-gram a menudo se usaba antes para ense√±ar modelos de lenguaje (vea el tercer cap√≠tulo del libro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Procesamiento del habla y lenguaje"</a> ).  Para ver la diferencia al cambiar de N-gramos a modelos neuronales en productos reales, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠ hay una publicaci√≥n de 2015 en el blog Swiftkey</a> , el desarrollador de mi teclado Android favorito, que presenta su modelo de lenguaje neuronal y lo compara con el modelo N-gram anterior.  Me gusta este ejemplo porque muestra c√≥mo se pueden describir las propiedades algor√≠tmicas de las inversiones en un lenguaje de marketing. <br><br><h1>  Miramos a ambos lados </h1><br><blockquote>  <font color="gray">‚ÄúUna paradoja es una se√±al de que deber√≠amos tratar de considerar lo que hay detr√°s.</font>  <font color="gray">Si la paradoja te preocupa, significa que est√°s luchando por lo absoluto.</font>  <font color="gray">Los relativistas ven la paradoja simplemente como un pensamiento interesante, quiz√°s divertido, a veces aterrador, pero un pensamiento muy instructivo ".</font>  <font color="gray"><i>Dios emperador de la duna</i></font> </blockquote><br>  Con base en lo anterior, complete el vac√≠o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Como contexto, hay cinco palabras anteriores (y una referencia anterior a "bus").  Estoy seguro de que la mayor√≠a de ustedes han adivinado que deber√≠a haber un "autob√∫s".  Pero si te doy otra palabra despu√©s del espacio, ¬øcambiar√° esto tu respuesta? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Esto cambia completamente la situaci√≥n: ahora la palabra que falta es muy probablemente "roja".  Obviamente, las palabras tienen valor informativo antes y despu√©s de un espacio.  Resulta que la contabilidad en ambas direcciones (izquierda y derecha) le permite calcular mejores inversiones.  Veamos c√≥mo configurar el entrenamiento modelo en tal situaci√≥n. <br><br><h1>  Saltar gramo </h1><br><blockquote>  <font color="gray">"Cuando se desconoce una elecci√≥n absolutamente inconfundible, el intelecto tiene la oportunidad de trabajar con datos limitados en la arena, donde los errores no solo son posibles sino tambi√©n necesarios".</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Adem√°s de dos palabras antes del objetivo, puede tener en cuenta dos palabras m√°s despu√©s de √©l. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Entonces el conjunto de datos para la capacitaci√≥n modelo se ver√° as√≠: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Esto se llama arquitectura CBOW (Continuous Bag of Words) y se describe en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">uno de los documentos word2vec</a> [pdf].  Hay otra arquitectura, que tambi√©n muestra excelentes resultados, pero est√° organizada de manera un poco diferente: trata de adivinar las palabras vecinas por la palabra actual.  Una ventana deslizante se ve as√≠: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">En la ranura verde est√° la palabra de entrada, y cada campo rosa representa una posible salida</font></i> <br><br>  Los rect√°ngulos rosados ‚Äã‚Äãtienen diferentes tonos porque esta ventana deslizante en realidad crea cuatro patrones separados en nuestro conjunto de datos de entrenamiento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Este m√©todo se llama arquitectura <b>skip-gram</b> .  Puede visualizar una ventana deslizante de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Los siguientes cuatro ejemplos se agregan al conjunto de datos de entrenamiento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Luego movemos la ventana a la siguiente posici√≥n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Lo que genera cuatro ejemplos m√°s: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Pronto tendremos muchas m√°s muestras: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revisi√≥n de aprendizaje </h1><br><blockquote>  <font color="gray">‚ÄúMuad'Dib aprendi√≥ r√°pido porque principalmente le ense√±aron a aprender.</font>  <font color="gray">Pero la primera lecci√≥n fue la asimilaci√≥n de la creencia de que √©l puede aprender, y esa es la base de todo.</font>  <font color="gray">Es sorprendente cu√°ntas personas no creen que puedan aprender y aprender, y cu√°ntas personas m√°s piensan que aprender es muy dif√≠cil ".</font>  <font color="gray">- <i>duna</i></font> </blockquote><br>  Ahora que tenemos el conjunto de omisi√≥n de gramo, lo usamos para entrenar el modelo neuronal b√°sico del lenguaje que predice una palabra vecina. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Comencemos con la primera muestra de nuestro conjunto de datos.  Tomamos el letrero y lo enviamos al modelo no capacitado con la solicitud de predecir la siguiente palabra. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  El modelo pasa por tres pasos y muestra un vector de predicci√≥n (con probabilidad para cada palabra en el diccionario).  Como el modelo no est√° entrenado, en esta etapa su pron√≥stico probablemente sea incorrecto.  Pero eso no es nada.  Sabemos qu√© palabra predice: esta es la celda resultante en la fila que actualmente utilizamos para entrenar el modelo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Un "vector objetivo" es aquel en el que la palabra objetivo tiene una probabilidad de 1, y todas las dem√°s palabras tienen una probabilidad de 0</font></i> <br><br>  ¬øQu√© tan equivocado estaba el modelo?  Reste el vector de pron√≥stico del objetivo y obtenga el vector de error: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Este vector de error ahora se puede usar para actualizar el modelo, por lo que la pr√≥xima vez es m√°s probable que d√© un resultado preciso en los mismos datos de entrada. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Aqu√≠ termina la primera etapa del entrenamiento.  Continuamos haciendo lo mismo con la siguiente muestra en el conjunto de datos, y luego con la siguiente, hasta que examinemos todas las muestras.  Este es el final de la primera era de aprendizaje.  Repetimos todo una y otra vez durante varias eras, y como resultado obtenemos un modelo entrenado: de √©l puede extraer la matriz de inversi√≥n y usarla en cualquier aplicaci√≥n. <br><br>  Aunque aprendimos mucho, pero para comprender completamente c√≥mo Word2vec realmente aprende, faltan un par de ideas clave. <br><br><h1>  Selecci√≥n negativa </h1><br><blockquote>  <font color="gray">‚ÄúIntentar comprender a Muad'Dib sin comprender a sus enemigos mortales, el Harkonnenov, es lo mismo que tratar de comprender la Verdad sin comprender qu√© es la Falsedad.</font>  <font color="gray">Este es un intento de conocer la Luz sin conocer la Oscuridad.</font>  <font color="gray">Esto es imposible ".</font>  <font color="gray">- <i>duna</i></font> </blockquote><br>  Recuerde los tres pasos de c√≥mo un modelo neuronal calcula un pron√≥stico: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  El tercer paso es muy costoso desde el punto de vista computacional, especialmente si lo hace para cada muestra en el conjunto de datos (decenas de millones de veces).  Es necesario aumentar de alguna manera la productividad. <br><br>  Una forma es dividir el objetivo en dos etapas: <br><br><ol><li>  Cree archivos adjuntos de palabras de alta calidad (sin predecir la siguiente palabra). <br></li><li>  Utilice estas inversiones de alta calidad para ense√±ar el modelo de lenguaje (para pronosticar). </li></ol><br>  Este art√≠culo se centrar√° en el primer paso.  Para aumentar la productividad, puede alejarse de predecir una palabra vecina ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... y cambie a un modelo que tome palabras de entrada y salida y calcule la probabilidad de su proximidad (de 0 a 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Una transici√≥n tan simple reemplaza la red neuronal con un modelo de regresi√≥n log√≠stica, por lo que los c√°lculos se vuelven mucho m√°s simples y r√°pidos. <br><br>  Al mismo tiempo, necesitamos refinar la estructura de nuestro conjunto de datos: la etiqueta ahora es una nueva columna con valores 0 o 1. En nuestra tabla, las unidades est√°n en todas partes, porque agregamos vecinos all√≠. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Tal modelo se calcula a una velocidad incre√≠ble: millones de muestras en minutos.  Pero necesitas cerrar una escapatoria.  Si todos nuestros ejemplos son positivos (objetivo: 1), se puede formar un modelo complicado que siempre devuelve 1, demostrando una precisi√≥n del 100%, pero no aprende nada y genera inversiones basura. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Para resolver este problema, debe ingresar <i>patrones negativos</i> en el conjunto de datos, palabras que definitivamente no son vecinas.  Para ellos, el modelo debe devolver 0. Ahora el modelo tendr√° que trabajar duro, pero los c√°lculos a√∫n van a gran velocidad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Para cada muestra en el conjunto de datos, agregue ejemplos negativos etiquetados 0</font></i> <br><br>  Pero, ¬øqu√© presentar como palabras de salida?  Elige las palabras arbitrariamente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Esta idea naci√≥ bajo la influencia del m√©todo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">comparaci√≥n</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ruido</a> [pdf].  Hacemos coincidir la se√±al real (ejemplos positivos de palabras vecinas) con ruido (palabras seleccionadas al azar que no son vecinas).  Esto proporciona un excelente compromiso entre el rendimiento y el rendimiento estad√≠stico. <br><br><h1>  Muestra negativa de salto de gramo (SGNS) </h1><br>  Observamos dos conceptos centrales de word2vec: juntos se denominan "skip-gram con muestreo negativo". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Aprendiendo word2vec </h1><br><blockquote>  <font color="gray">‚ÄúUna m√°quina no puede prever todos los problemas que son importantes para una persona viva.</font>  <font color="gray">Hay una gran diferencia entre el espacio discreto y el continuo continuo.</font>  <font color="gray">Vivimos en un espacio y las m√°quinas existen en otro ".</font>  <font color="gray">- <i>Dios el emperador de la duna</i></font> </blockquote><br>  Habiendo examinado las ideas b√°sicas de skip-gram y muestreo negativo, podemos proceder a una mirada m√°s cercana al proceso de aprendizaje de word2vec. <br><br>  Primero, preprocesamos el texto en el que entrenamos el modelo.  Defina el tama√±o del diccionario (lo llamaremos <code>vocab_size</code> ), digamos, en 10,000 archivos adjuntos y los par√°metros de las palabras en el diccionario. <br><br>  Al comienzo de la capacitaci√≥n, creamos dos matrices: <code>Embedding</code> y <code>Context</code> .  Los <code>vocab_size</code> adjuntos para cada palabra se almacenan en estas matrices en nuestro diccionario (por <code>vocab_size</code> tanto, <code>vocab_size</code> es uno de sus par√°metros).  El segundo par√°metro es la dimensi√≥n del archivo adjunto (generalmente <code>embedding_size</code> establece en 300, pero anteriormente vimos un ejemplo con 50 dimensiones). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Primero, inicializamos estas matrices con valores aleatorios.  Luego comenzamos el proceso de aprendizaje.  En cada etapa, tomamos un ejemplo positivo y los negativos asociados con √©l.  Aqu√≠ est√° nuestro primer grupo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Ahora tenemos cuatro palabras: la palabra de entrada <code>not</code> y las palabras de salida / contextuales <code>thou</code> (vecino real), <code>aaron</code> y <code>taco</code> (ejemplos negativos).  Comenzamos la b√∫squeda de sus archivos adjuntos en las matrices <code>Embedding</code> (para la palabra de entrada) y <code>Context</code> (para las palabras de contexto), aunque ambas matrices contienen archivos adjuntos para todas las palabras de nuestro diccionario. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Luego calculamos el producto escalar del adjunto de entrada con cada uno de los adjuntos contextuales.  En cada caso, se obtiene un n√∫mero que indica la similitud de los datos de entrada y los archivos adjuntos contextuales. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Ahora necesitamos una forma de convertir estas estimaciones en un tipo de probabilidad: todas deben ser n√∫meros positivos entre 0 y 1. Esta es una excelente tarea para las ecuaciones log√≠sticas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sigmoideas</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  El resultado del c√°lculo sigmoide se puede considerar como la salida del modelo para estas muestras.  Como puede ver, el <code>taco</code> el puntaje m√°s alto, mientras que <code>aaron</code> todav√≠a tiene el puntaje m√°s bajo, tanto antes como despu√©s de sigmoide. <br><br>  Cuando el modelo no entrenado hizo un pron√≥stico y tiene una marca de objetivo real para la comparaci√≥n, calculemos cu√°ntos errores hay en el pron√≥stico del modelo.  Para hacer esto, simplemente reste la puntuaci√≥n sigmoidea de las etiquetas de destino. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Aqu√≠ es donde comienza la fase de "aprendizaje" del t√©rmino "aprendizaje autom√°tico".  Ahora podemos usar esta estimaci√≥n de error para ajustar las inversiones <code>not</code> , <code>thou</code> , <code>aaron</code> y <code>taco</code> , de modo que la pr√≥xima vez que el resultado est√© m√°s cerca de las estimaciones objetivo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Esto completa una etapa de entrenamiento.  Mejoramos un poco el apego de algunas palabras ( <code>not</code> , <code>thou</code> , <code>aaron</code> y <code>taco</code> ).  Ahora pasamos a la siguiente etapa (la siguiente muestra positiva y las negativas asociadas a ella) y repetimos el proceso. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Los archivos adjuntos contin√∫an mejorando a medida que avanzamos por el conjunto de datos completo varias veces.  Luego puede detener el proceso, dejar a un lado la matriz de <code>Context</code> y usar la matriz de <code>Embeddings</code> capacitada para la siguiente tarea. <br><br><h1>  Tama√±o de ventana y n√∫mero de muestras negativas </h1><br>  En el proceso de aprendizaje de word2vec, dos hiperpar√°metros clave son el tama√±o de la ventana y el n√∫mero de muestras negativas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Diferentes tama√±os de ventana son adecuados para diferentes tareas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Se ha observado</a> que los tama√±os de ventana m√°s peque√±os (2-15) generan archivos adjuntos <i>intercambiables</i> con √≠ndices similares (tenga en cuenta que los ant√≥nimos a menudo son intercambiables cuando se miran las palabras circundantes: por ejemplo, las palabras "bueno" y "malo" a menudo se mencionan en contextos similares).  Los tama√±os de ventana m√°s grandes (15‚Äì50 o incluso m√°s) generan archivos adjuntos <i>relacionados</i> con √≠ndices similares.  En la pr√°ctica, a menudo tiene que proporcionar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">anotaciones</a> para similitudes sem√°nticas √∫tiles en su tarea.  En Gensim, el tama√±o de ventana predeterminado es 5 (dos palabras izquierda y derecha, adem√°s de la palabra de entrada en s√≠). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El n√∫mero de muestras negativas es otro factor en el proceso de aprendizaje. </font><font style="vertical-align: inherit;">El documento original recomienda 5‚Äì20. </font><font style="vertical-align: inherit;">Tambi√©n dice que 2-5 muestras parecen ser suficientes cuando tiene un conjunto de datos suficientemente grande. </font><font style="vertical-align: inherit;">En Gensim, el valor predeterminado es 5 patrones negativos.</font></font><br><br><h1>  Conclusi√≥n </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Si tu comportamiento cae m√°s all√° de tus est√°ndares, entonces eres una persona viva, no un aut√≥mata" - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dios-Emperador de Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero que ahora entiendas la incorporaci√≥n de palabras y la esencia del algoritmo word2vec. </font><font style="vertical-align: inherit;">Tambi√©n espero que ahora comprenda mejor los art√≠culos que mencionan el concepto de "skip-gram con muestreo negativo" (SGNS), como en los sistemas de recomendaci√≥n mencionados anteriormente.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referencias y lecturas adicionales </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Representaciones distribuidas de palabras y frases y su composici√≥n"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´      ¬ª</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´   ¬ª</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´   ¬ª</a>      ‚Äî    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´      ¬ª</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> ‚Äî      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´ word2vec¬ª</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´  ¬ª</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´¬ª</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/446530/">https://habr.com/ru/post/446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446514/index.html">Gmail tiene 15 a√±os</a></li>
<li><a href="../446516/index.html">Visualizaci√≥n del tiempo de renacimiento de Roshan</a></li>
<li><a href="../446518/index.html">Cortafuegos de aplicaciones web</a></li>
<li><a href="../446520/index.html">C√≥mo comenz√≥ todo: la historia de los drones voladores</a></li>
<li><a href="../446522/index.html">Swift 5.1: ¬øqu√© hay de nuevo?</a></li>
<li><a href="../446532/index.html">Upwork introduce una tarifa por el derecho a escribir a un cliente potencial</a></li>
<li><a href="../446534/index.html">Visual Studio 2019 lanzado</a></li>
<li><a href="../446536/index.html">Colas y JMeter: Intercambio con editor y suscriptor</a></li>
<li><a href="../446538/index.html">PhotoGuru cambi√≥ al "lado oscuro" y al "m√°s sabio"</a></li>
<li><a href="../446546/index.html">Microsoft extiende la ventaja de Azure IP con nuevos beneficios de IP para los innovadores y nuevas empresas de Azure IoT</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>