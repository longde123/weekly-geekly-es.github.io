<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèª‚Äçüî¨ üß° üïû Comment nous avons augment√© la productivit√© de Tensorflow au service de 70% üëÇüèΩ üï∫üèº ‚úãüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow est devenu la plate-forme standard pour l'apprentissage automatique (ML), populaire √† la fois dans l'industrie et dans la recherche. De nom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons augment√© la productivit√© de Tensorflow au service de 70%</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/">  Tensorflow est devenu la plate-forme standard pour l'apprentissage automatique (ML), populaire √† la fois dans l'industrie et dans la recherche.  De nombreuses biblioth√®ques, outils et frameworks gratuits ont √©t√© cr√©√©s pour la formation et la maintenance des mod√®les ML.  Le projet Tensorflow Serving aide √† maintenir les mod√®les ML dans un environnement de production distribu√©. <br><br>  Notre service Mux utilise Tensorflow Serving dans plusieurs parties de l'infrastructure, nous avons d√©j√† discut√© de l'utilisation de Tensorflow Serving dans l'encodage des titres vid√©o.  Aujourd'hui, nous allons nous concentrer sur les m√©thodes qui am√©liorent la latence en optimisant √† la fois le serveur de pr√©visions et le client.  Les pr√©visions de mod√®le sont g√©n√©ralement des op√©rations ¬´en ligne¬ª (sur le chemin critique de la demande d'une application), par cons√©quent, les principaux objectifs de l'optimisation sont de traiter de gros volumes de demandes avec le plus court d√©lai possible. <br><a name="habracut"></a><br><h1>  Qu'est-ce que Tensorflow Serving? </h1><br>  Tensorflow Serving fournit une architecture de serveur flexible pour le d√©ploiement et la maintenance des mod√®les ML.  Une fois que le mod√®le est form√© et pr√™t √† √™tre utilis√© pour les pr√©visions, Tensorflow Serving n√©cessite de l'exporter dans un format compatible (utilisable). <br><br>  <i>Servable</i> est une abstraction centrale qui enveloppe les objets Tensorflow.  Par exemple, un mod√®le peut √™tre repr√©sent√© comme un ou plusieurs objets Servables.  Ainsi, Servable sont les objets de base que le client utilise pour effectuer des calculs.  La taille utile est importante: les mod√®les plus petits prennent moins d'espace, utilisent moins de m√©moire et se chargent plus rapidement.  Pour t√©l√©charger et maintenir √† l'aide de l'API Predict, les mod√®les doivent √™tre au format SavedModel. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving combine les composants de base pour cr√©er un serveur gRPC / HTTP qui dessert plusieurs mod√®les ML (ou plusieurs versions), fournit des composants de surveillance et une architecture personnalis√©e. <br><br><h1>  Tensorflow au service de Docker </h1><br>  Jetons un coup d'≈ìil aux mesures de base de la latence dans les pr√©visions de performances avec les param√®tres de service Tensorflow standard (sans optimisation du processeur). <br><br>  Tout d'abord, t√©l√©chargez la derni√®re image √† partir du hub TensorFlow Docker: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  Dans cet article, tous les conteneurs s'ex√©cutent sur un h√¥te avec quatre c≈ìurs, 15 Go, Ubuntu 16.04. <br><br><h3>  Exporter le mod√®le Tensorflow vers SavedModel </h3><br>  Lorsqu'un mod√®le est form√© √† l'aide de Tensorflow, la sortie peut √™tre enregistr√©e en tant que points de contr√¥le variables (fichiers sur disque).  La sortie est effectu√©e directement en restaurant les points de contr√¥le du mod√®le ou dans un format de graphique fig√© fig√© (fichier binaire). <br><br>  Pour Tensorflow Serving, ce graphique fig√© doit √™tre export√© au format SavedModel.  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation Tensorflow</a> contient des exemples d'exportation de mod√®les form√©s au format SavedModel. <br><br>  Tensorflow fournit √©galement de nombreux mod√®les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">officiels et de recherche</a> comme point de d√©part pour l'exp√©rimentation, la recherche ou la production. <br><br>  √Ä titre d'exemple, nous utiliserons le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mod√®le</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seau neuronal r√©siduel profond (ResNet)</a> pour classer un ensemble de donn√©es ImageNet de 1000 classes.  T√©l√©chargez le mod√®le <code>ResNet-50 v2</code> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pr√©</a> - <code>ResNet-50 v2</code> , en particulier l' <i>option</i> Channels_last (NHWC) dans <i>SavedModel</i> : en r√®gle g√©n√©rale, cela fonctionne mieux sur le CPU. <br><br>  Copiez le r√©pertoire du mod√®le RestNet dans la structure suivante: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving attend une structure de r√©pertoires ordonn√©e num√©riquement pour le contr√¥le de version.  Dans notre cas, le r√©pertoire <code>1/</code> correspond au mod√®le version 1, qui contient l'architecture du mod√®le <code>saved_model.pb</code> avec un instantan√© des poids du mod√®le (variables). <br><br><h3>  Chargement et traitement de SavedModel </h3><br>  La commande suivante d√©marre le serveur de mod√®le Tensorflow Serving dans un conteneur Docker.  Pour charger SavedModel, vous devez monter le r√©pertoire model dans le r√©pertoire conteneur attendu. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  La v√©rification des journaux de conteneur montre que ModelServer est op√©rationnel pour g√©rer les demandes de sortie pour le mod√®le <code>resnet</code> aux points de terminaison gRPC et HTTP: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Client de pr√©visions </h3><br>  Tensorflow Serving d√©finit un sch√©ma d'API au format <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tampons de protocole</a> (protobufs).  Les impl√©mentations client GRPC pour l'API de pr√©vision sont empaquet√©es sous la forme d'un package Python <code>tensorflow_serving.apis</code> .  Nous aurons besoin d'un autre <code>tensorflow</code> package Python pour les fonctions utilitaires. <br><br>  Installez les d√©pendances pour cr√©er un client simple: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  Le mod√®le <code>ResNet-50 v2</code> attend l'entr√©e de tenseurs √† virgule flottante dans une structure de donn√©es format√©e channels_last (NHWC).  Par cons√©quent, l'image d'entr√©e est lue √† l'aide d'opencv-python et charg√©e dans le tableau numpy (hauteur √ó largeur √ó canaux) en tant que type de donn√©es float32.  Le script ci-dessous cr√©e un talon de client de pr√©diction et charge les donn√©es JPEG dans un tableau numpy, les convertit en tensor_proto pour effectuer une demande de pr√©vision pour gRPC: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Apr√®s avoir re√ßu une entr√©e JPEG, un client fonctionnel produira le r√©sultat suivant: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  Le tenseur r√©sultant contient une pr√©vision sous la forme d'une valeur enti√®re et d'une probabilit√© de signes. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  Pour une seule demande, un tel d√©lai n'est pas acceptable.  Mais rien de surprenant: le binaire Tensorflow Serving est par d√©faut con√ßu pour la plus large gamme d'√©quipements pour la plupart des cas d'utilisation.  Vous avez probablement remarqu√© les lignes suivantes dans les journaux du conteneur de service Tensorflow standard: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  Cela indique un binaire TensorFlow Serving ex√©cut√© sur une plate-forme CPU pour laquelle il n'a pas √©t√© optimis√©. <br><br><h3>  Construisez un binaire optimis√© </h3><br>  Selon la <a href="">documentation de</a> Tensorflow, il est recommand√© de compiler Tensorflow √† partir de la source avec toutes les optimisations disponibles pour le CPU sur l'h√¥te o√π le binaire fonctionnera.  Lors de l'assemblage, des drapeaux sp√©ciaux permettent l'activation des jeux d'instructions CPU pour une plate-forme sp√©cifique: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Jeu d'instructions </th><th>  Drapeaux </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  Tous pris en charge par le processeur </td><td>  --copt = -march = natif </td></tr></tbody></table></div><br>  Clonez un serveur Tensorflow d'une version sp√©cifique.  Dans notre cas, il s'agit de 1,13 (le dernier au moment de la publication de cet article): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  L'image de d√©veloppement Tensorflow Serving utilise l'outil de B√¢le pour cr√©er.  Nous le configurons pour des ensembles sp√©cifiques d'instructions CPU: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  S'il n'y a pas assez de m√©moire, limitez la consommation de m√©moire pendant le processus de g√©n√©ration avec l'indicateur <code>--local_resources=2048,.5,1.0</code> .  Pour plus d'informations sur les indicateurs, consultez l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aide de Tensorflow Serving et Docker</a> , ainsi que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation Bazel</a> . <br><br>  Cr√©ez une image de travail bas√©e sur l'image existante: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  ModelServer est configur√© √† l'aide des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">indicateurs TensorFlow</a> pour prendre en charge la concurrence.  Les options suivantes configurent deux pools de threads pour un fonctionnement parall√®le: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  contr√¥le le nombre maximal de threads pour l'ex√©cution parall√®le d'une op√©ration; <br></li><li>  utilis√© pour parall√©liser des op√©rations qui ont des sous-op√©rations de nature ind√©pendante. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  contr√¥le le nombre maximal de threads pour l'ex√©cution parall√®le d'op√©rations ind√©pendantes; <br></li><li>  Les op√©rations Tensorflow Graph, qui sont ind√©pendantes les unes des autres et, par cons√©quent, peuvent √™tre effectu√©es dans diff√©rents threads. </li></ul><br>  Par d√©faut, les deux param√®tres sont d√©finis sur <code>0</code> .  Cela signifie que le syst√®me lui-m√™me s√©lectionne le nombre appropri√©, ce qui signifie le plus souvent un thread par c≈ìur.  Cependant, le param√®tre peut √™tre modifi√© manuellement pour la simultan√©it√© multic≈ìur. <br><br>  Ex√©cutez ensuite le conteneur de service de la m√™me mani√®re que le pr√©c√©dent, cette fois avec une image Docker compil√©e √† partir des sources et avec des indicateurs d'optimisation Tensorflow pour un processeur sp√©cifique: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Les journaux de conteneur ne devraient plus afficher d'avertissements concernant un processeur non d√©fini.  Sans changer le code sur la m√™me demande de pr√©vision, le retard est r√©duit d'environ 35,8%: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Acc√©l√©rez la pr√©vision des clients </h3><br>  Est-il encore possible d'acc√©l√©rer?  Nous avons optimis√© le c√¥t√© serveur pour notre CPU, mais un d√©lai de plus d'une seconde semble encore trop important. <br><br>  Il se trouve que le chargement des biblioth√®ques <code>tensorflow_serving</code> et <code>tensorflow</code> apporte une contribution significative au retard.  Chaque appel inutile √† <code>tf.contrib.util.make_tensor_proto</code> ajoute √©galement une fraction de seconde. <br><br>  Vous pouvez demander: "N'avons-nous pas besoin de packages TensorFlow Python pour faire des demandes de pr√©diction au serveur Tensorflow?"  En fait, il n'y a pas vraiment <i>besoin</i> de <code>tensorflow_serving</code> et <code>tensorflow</code> . <br><br>  Comme indiqu√© pr√©c√©demment, les API de pr√©diction Tensorflow sont d√©finies comme des proto-tampons.  Par cons√©quent, deux d√©pendances externes peuvent √™tre remplac√©es par les <code>tensorflow</code> et <code>tensorflow_serving</code> correspondants - et vous n'avez alors pas besoin d'extraire la biblioth√®que Tensorflow enti√®re (lourde) sur le client. <br><br>  Tout d'abord, d√©barrassez-vous des <code>tensorflow_serving</code> <code>tensorflow</code> et <code>tensorflow_serving</code> et ajoutez le <code>grpcio-tools</code> . <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  <code>tensorflow/tensorflow</code> les <code>tensorflow/tensorflow</code> et <code>tensorflow/serving</code> et copiez les fichiers protobuf suivants dans le projet client: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Copiez ces fichiers protobuf dans le r√©pertoire <code>protos/</code> avec les chemins d'origine pr√©serv√©s: <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  Par souci de simplicit√©, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">prediction_service.proto</a> peut √™tre simplifi√© pour impl√©menter uniquement Predict RPC afin de ne pas t√©l√©charger les d√©pendances imbriqu√©es des autres RPC sp√©cifi√©s dans le service.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Voici</a> un exemple de <code>prediction_service.</code> simplifi√©. <br><br>  Cr√©ez des impl√©mentations gRPC Python √† l'aide de <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Maintenant, l'ensemble <code>tensorflow_serving</code> module <code>tensorflow_serving</code> peut √™tre supprim√©: <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... et remplacez par les protobuffers g√©n√©r√©s √† partir de <code>protos/tensorflow_serving/apis</code> : <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  La biblioth√®que Tensorflow est import√©e pour utiliser la fonction d'assistance <code>make_tensor_proto</code> , qui est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">n√©cessaire pour</a> encapsuler un objet python / numpy en tant qu'objet TensorProto. <br><br>  Ainsi, nous pouvons remplacer la d√©pendance et le fragment de code suivants: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  importer des protobuffers et construire un objet TensorProto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  Le script Python complet est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> .  Ex√©cutez un client de d√©marrage mis √† jour qui effectue une demande de pr√©diction pour un service Tensorflow optimis√©: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  Le diagramme suivant montre le temps d'ex√©cution pr√©vu dans la version optimis√©e de Tensorflow Serving par rapport √† la version standard, sur 10 ex√©cutions: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  Le retard moyen a diminu√© d'environ 3,38 fois. <br><br><h1>  Optimisation de la bande passante </h1><br>  Tensorflow Serving peut √™tre configur√© pour g√©rer de grandes quantit√©s de donn√©es.  L'optimisation de la bande passante est g√©n√©ralement effectu√©e pour le traitement par lots ¬´autonome¬ª, o√π les limites de latence serr√©es ne sont pas une exigence stricte. <br><br><h3>  Traitement par lots c√¥t√© serveur </h3><br>  Comme indiqu√© dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> , le traitement par lots c√¥t√© serveur est nativement pris en charge dans Tensorflow Serving. <br><br>  Les compromis entre latence et d√©bit sont d√©termin√©s par les param√®tres de traitement par lots.  Ils vous permettent d'atteindre le d√©bit maximal dont les acc√©l√©rateurs mat√©riels sont capables. <br><br>  Pour activer l'empaquetage, d√©finissez les <code>--batching_parameters_file</code> <code>--enable_batching</code> et <code>--batching_parameters_file</code> .  Les param√®tres sont d√©finis en fonction de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SessionBundleConfig</a> .  Pour les syst√®mes sur le CPU, d√©finissez <code>num_batch_threads</code> sur le nombre de c≈ìurs disponibles.  Pour le GPU, voir les param√®tres appropri√©s <a href="">ici</a> . <br><br>  Apr√®s avoir rempli l'int√©gralit√© du package c√¥t√© serveur, les demandes d'√©mission sont combin√©es en une seule grande requ√™te (tenseur) et envoy√©es √† la session Tensorflow avec une requ√™te combin√©e.  Dans cette situation, le parall√©lisme CPU / GPU est vraiment impliqu√©. <br><br>  Quelques utilisations courantes du traitement par lots Tensorflow: <br><br><ul><li>  Utilisation de demandes de clients asynchrones pour remplir des paquets c√¥t√© serveur <br></li><li>  Traitement par lots plus rapide en transf√©rant les composants du graphique du mod√®le vers le CPU / GPU <br></li><li>  Traitement des demandes de plusieurs mod√®les √† partir d'un seul serveur <br></li><li>  Le traitement par lots est fortement recommand√© pour le traitement "hors ligne" d'un grand nombre de demandes </li></ul><br><h3>  Traitement par lots c√¥t√© client </h3><br>  Le traitement par lots c√¥t√© client regroupe plusieurs demandes entrantes en une seule. <br><br>  √âtant donn√© que le mod√®le ResNet attend une entr√©e au format NHWC (la premi√®re dimension est le nombre d'entr√©es), nous pouvons combiner plusieurs images d'entr√©e en une seule demande RPC: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  Pour un paquet de N images, le tenseur de sortie dans la r√©ponse contiendra les r√©sultats de pr√©diction pour le m√™me nombre d'entr√©es.  Dans notre cas, N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Acc√©l√©ration mat√©rielle </h1><br>  Quelques mots sur les GPU. <br><br>  Le processus d'apprentissage utilise naturellement la parall√©lisation sur le GPU, car la construction de r√©seaux de neurones profonds n√©cessite des calculs massifs pour obtenir la solution optimale. <br><br>  Mais pour produire des r√©sultats, la parall√©lisation n'est pas si √©vidente.  Souvent, vous pouvez acc√©l√©rer la sortie d'un r√©seau de neurones vers un GPU, mais vous devez s√©lectionner et tester soigneusement l'√©quipement, et effectuer une analyse technique et √©conomique approfondie.  La parall√©lisation mat√©rielle est plus utile pour le traitement par lots de conclusions "autonomes" (volumes massifs). <br><br>  Avant de passer √† un GPU, tenez compte des besoins de l'entreprise avec une analyse minutieuse des co√ªts (mon√©taires, op√©rationnels, techniques) pour le plus grand avantage (latence r√©duite, bande passante √©lev√©e). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr445928/">https://habr.com/ru/post/fr445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr445918/index.html">20 ans de RollerCoaster Tycoon: une interview avec le cr√©ateur du jeu</a></li>
<li><a href="../fr445920/index.html">En direct: comment freiner le d√©veloppement iOS dans de grandes √©quipes</a></li>
<li><a href="../fr445922/index.html">Pourquoi regarder des √©missions en ligne si vous pouvez lire Habr</a></li>
<li><a href="../fr445924/index.html">TR√âSORS: quand les montres intelligentes deviennent √©tranges</a></li>
<li><a href="../fr445926/index.html">Le programme UFO Secret des √âtats-Unis a √©galement √©tudi√© les trous de ver et les dimensions suppl√©mentaires.</a></li>
<li><a href="../fr445932/index.html">S√©curit√© des applications client: conseils pratiques pour un d√©veloppeur frontal</a></li>
<li><a href="../fr445936/index.html">D√©veloppement √©lectronique. √Ä propos des microcontr√¥leurs sur les doigts</a></li>
<li><a href="../fr445940/index.html">AMA avec Habr, v 7.0. Citron, beignets et nouvelles</a></li>
<li><a href="../fr445946/index.html">MWC: mode d'emploi</a></li>
<li><a href="../fr445948/index.html">H√©ritage en C ++: d√©butant, interm√©diaire, avanc√©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>