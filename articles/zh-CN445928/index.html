<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🈚️ ☎️ 💙 我们如何将Tensorflow服务生产率提高70％ 👨‍👩‍👦‍👦 ⏯️ 🅾️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow已成为机器学习（ML）的标准平台，在行业和研究中都非常流行。 已经创建了许多免费的库，工具和框架来训练和维护ML模型。 Tensorflow Serving项目有助于在分布式生产环境中维护ML模型。 

 我们的Mux服务在基础架构的多个部分使用Tensorflow Servin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>我们如何将Tensorflow服务生产率提高70％</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/"> Tensorflow已成为机器学习（ML）的标准平台，在行业和研究中都非常流行。 已经创建了许多免费的库，工具和框架来训练和维护ML模型。  Tensorflow Serving项目有助于在分布式生产环境中维护ML模型。 <br><br> 我们的Mux服务在基础架构的多个部分使用Tensorflow Serving，我们已经讨论过Tensorflow Serving在视频标题编码中的使用。 今天，我们将专注于通过优化预测服务器和客户端来改善延迟的方法。 模型预测通常是“在线”操作（在请求应用程序的关键路径上），因此，优化的主要目标是以尽可能低的延迟处理大量请求。 <br><a name="habracut"></a><br><h1> 什么是Tensorflow服务？ </h1><br>  Tensorflow Serving提供了用于部署和维护ML模型的灵活服务器架构。 训练好模型并准备将其用于预测后，Tensorflow Serving需要将其导出为兼容（可服务）格式。 <br><br>  <i>Servable</i>是包装Tensorflow对象的中央抽象。 例如，模型可以表示为一个或多个可服务对象。 因此，Servable是客户端用于执行计算的基本对象。 可使用的大小很重要：较小的型号占用较少的空间，使用较少的内存并且加载速度更快。 要使用Predict API下载和维护，模型必须为SavedModel格式。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving结合了基本组件，创建了一个gRPC / HTTP服务器，该服务器可为多个ML模型（或多个版本）提供服务，提供监视组件和自定义架构。 <br><br><h1>  Tensorflow与Docker服务 </h1><br> 让我们看一下使用标准Tensorflow Serving设置（无需CPU优化）预测性能的延迟的基本指标。 <br><br> 首先，从TensorFlow Docker集线器下载最新映像： <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br> 在本文中，所有容器都在具有四个内核（15 GB，Ubuntu 16.04）的主机上运行。 <br><br><h3> 将Tensorflow模型导出到SavedModel </h3><br> 当使用Tensorflow训练模型时，可以将输出保存为变量控制点（磁盘上的文件）。 通过恢复模型的控制点或以冻结的冻结图形格式（二进制文件）直接执行输出。 <br><br> 对于Tensorflow Serving，此冻结的图需要导出为SavedModel格式。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Tensorflow文档</a>包含将经过训练的模型导出为SavedModel格式的示例。 <br><br>  Tensorflow还提供许多<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方和研究</a>模型，作为进行实验，研究或生产的起点。 <br><br> 例如，我们将使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">深度残留神经网络（ResNet）模型</a>对1000个类别的ImageNet数据集进行分类。 下载经过<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">预先</a> <code>ResNet-50 v2</code>模型，尤其是SavedModel中的Channels_last（NHWC） <i>选项</i> ：通常，它在CPU上效果更好。 <br><br> 将RestNet模型目录复制到以下结构中： <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving希望使用数字排序的目录结构进行版本控制。 在我们的示例中，目录<code>1/</code>对应于版本1模型，其中包含带有模型权重（变量）快照的<code>saved_model.pb</code>模型<code>saved_model.pb</code> 。 <br><br><h3> 加载和处理SavedModel </h3><br> 以下命令在Docker容器中启动Tensorflow Serving模型服务器。 要加载SavedModel，必须将模型目录安装在预期的容器目录中。 <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br> 检查容器日志后，表明ModelServer已启动并正在运行，以处理gRPC和HTTP端点对<code>resnet</code>模型的输出请求： <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3> 预测客户 </h3><br>  Tensorflow Serving以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">协议缓冲区</a> （protobufs）格式定义API模式。 预测API的GRPC客户端实现打包为Python包<code>tensorflow_serving.apis</code> 。 我们将需要另一个Python软件包<code>tensorflow</code>用于实用程序功能。 <br><br> 安装依赖项以创建一个简单的客户端： <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  <code>ResNet-50 v2</code>模型期望以格式化的channels_last（NHWC）数据结构输入浮点张量。 因此，使用opencv-python读取输入图像，并将其作为float32数据类型加载到numpy数组（高度×宽度×通道）中。 下面的脚本创建了一个预测客户端存根，并将JPEG数据加载到numpy数组中，将其转换为tensor_proto以发出对gRPC的预测请求： <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br> 收到JPEG输入后，工作中的客户端将产生以下结果： <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br> 所得张量包含整数值和符号概率形式的预测。 <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br> 对于单个请求，这样的延迟是不可接受的。 但是并不奇怪：Tensorflow Serving二进制文件默认情况下是针对大多数用例设计的，适用于最广泛的设备。 您可能在标准Tensorflow服务容器的日志中注意到以下几行： <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br> 这表示在尚未对其进行优化的CPU平台上运行的TensorFlow Serving二进制文件。 <br><br><h3> 构建优化的二进制文件 </h3><br> 根据Tensorflow <a href="">文档</a> ，建议使用二进制文件将在其上运行的主机上的CPU的所有优化方法从源代码编译Tensorflow。 组装时，特殊标志可激活特定平台的CPU指令集： <br><br><div class="scrollable-table"><table><tbody><tr><th> 指令集 </th><th> 标志 </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td> 上证4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td> 上证4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td> 全部受处理器支持 </td><td>  --copt = -march =本机 </td></tr></tbody></table></div><br> 克隆特定版本的Tensorflow服务。 在我们的情况下，这是1.13（本文发布时的最后一个）： <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  Tensorflow Serving开发人员图像使用Basel工具进行构建。 我们为特定的CPU指令集配置它： <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br> 如果没有足够的内存，请在标志<code>--local_resources=2048,.5,1.0</code>过程中使用<code>--local_resources=2048,.5,1.0</code>标志限制内存消耗。 有关标志的信息，请参见<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Tensorflow Serving和Docker</a>帮助以及<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Bazel文档</a> 。 <br><br> 根据现有图像创建一个工作图像： <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br> 使用<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">TensorFlow标志</a>配置ModelServer以支持并发。 以下选项将两个线程池配置为并行操作： <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li> 控制一个操作并行执行的最大线程数； <br></li><li> 用于并行化具有本质上独立的子操作的操作。 </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li> 控制并行执行独立操作的最大线程数； <br></li><li>  Tensorflow Graph操作彼此独立，因此可以在不同的线程中执行。 </li></ul><br> 默认情况下，两个参数都设置为<code>0</code> 。 这意味着系统本身会选择适当的数字，这通常意味着每个内核一个线程。 但是，可以为多核并发手动更改该参数。 <br><br> 然后以与上一个相同的方式运行Serving容器，这次使用从源代码编译的Docker映像，并使用针对特定处理器的Tensorflow优化标志： <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br> 容器日志不应再显示有关未定义CPU的警告。 在不更改同一预测请求的代码的情况下，延迟减少了约35.8％： <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3> 提高客户预测速度 </h3><br> 还有加速的可能吗？ 我们已经为CPU优化了服务器端，但是延迟超过1秒似乎仍然太大。 <br><br> 碰巧，加载<code>tensorflow_serving</code>和<code>tensorflow</code>库会对延迟产生重大影响。 对<code>tf.contrib.util.make_tensor_proto</code>每个不必要的调用也会增加一秒钟的时间。 <br><br> 您可能会问：“我们是否不需要TensorFlow Python软件包来实际向Tensorflow服务器发出预测请求？” 实际上，实际上并不<i>需要</i> <code>tensorflow_serving</code>和<code>tensorflow</code>包。 <br><br> 如前所述，Tensorflow预测API被定义为原型缓冲区。 因此，可以将两个外部依赖项替换为相应的<code>tensorflow</code>和<code>tensorflow_serving</code> -然后您无需在客户端上提取整个（沉重的）Tensorflow库。 <br><br> 首先，摆脱<code>tensorflow</code>和<code>tensorflow_serving</code>并添加<code>grpcio-tools</code>包。 <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br> 克隆<code>tensorflow/tensorflow</code>和<code>tensorflow/serving</code>存储库，并将以下protobuf文件复制到客户端项目： <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br> 将这些protobuf文件复制到<code>protos/</code>目录，并保留原始路径： <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br> 为了简单起见，可以将<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">prediction_service.proto</a>简化为仅实现Predict RPC，以便不下载服务中指定的其他RPC的嵌套依赖关系。  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">这是</a>简化的<code>prediction_service.</code>的示例。 <br><br> 使用<code>grpcio.tools.protoc</code>创建Python gRPC实现： <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br> 现在可以删除整个<code>tensorflow_serving</code>模块： <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ...并替换为从<code>protos/tensorflow_serving/apis</code>生成的protobuffers： <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br> 导入Tensorflow库以使用辅助函数<code>make_tensor_proto</code> ，这是<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">将</a> python / numpy对象<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">包装</a>为TensorProto对象所必需的。 <br><br> 因此，我们可以替换以下依赖项和代码片段： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br> 导入protobuffers并构建一个TensorProto对象： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br> 完整的Python脚本在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">这里</a> 。 运行更新的入门客户端，该客户端针对优化的Tensorflow Serving发出预测请求： <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br> 下图显示了优化版本的Tensorflow Serving与标准版本相比的预测执行时间（运行10多次）： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br> 平均延迟减少了约3.38倍。 <br><br><h1> 带宽优化 </h1><br>  Tensorflow服务可以配置为处理大量数据。 带宽优化通常是针对“独立”批处理执行的，其中严格的延迟边界不是严格的要求。 <br><br><h3> 服务器端批处理 </h3><br> 如<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文档</a>中所述，Tensorflow Serving本机支持服务器端批处理。 <br><br> 延迟和吞吐量之间的权衡由批处理参数确定。 它们使您可以实现硬件加速器能够提供的最大吞吐量。 <br><br> 要启用打包，请设置<code>--enable_batching</code>和<code>--batching_parameters_file</code> 。 根据<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">SessionBundleConfig</a>设置参数。 对于CPU上的系统，将<code>num_batch_threads</code>设置为可用内核数。 对于GPU，请参阅<a href="">此处</a>的适当参数。 <br><br> 在服务器端填满整个程序包后，发布请求将合并为一个大请求（张量），并与合并的请求一起发送到Tensorflow会话。 在这种情况下，确实涉及CPU / GPU并行性。 <br><br>  Tensorflow批处理的一些常见用途： <br><br><ul><li> 使用异步客户端请求填充服务器端数据包 <br></li><li> 通过将模型图的组件传输到CPU / GPU，可以更快地进行批处理 <br></li><li> 在一台服务器上处理来自多个模型的请求 <br></li><li> 强烈建议将批处理用于“离线”处理大量请求 </li></ul><br><h3> 客户端批处理 </h3><br> 客户端批处理将几个传入请求分组为一个。 <br><br> 由于ResNet模型正在等待NHWC格式的输入（第一个维度是输入数），因此我们可以将多个输入图像组合成一个RPC请求： <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br> 对于N个图像的数据包，响应中的输出张量将包含相同数量输入的预测结果。 在我们的情况下，N = 2： <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1> 硬件加速 </h1><br> 关于GPU的几句话。 <br><br> 学习过程自然会在GPU上使用并行化，因为深度神经网络的构建需要大量计算才能获得最佳解决方案。 <br><br> 但是对于输出结果，并行化不是很明显。 通常，您可以加快神经网络向GPU的输出速度，但是您需要仔细选择和测试设备，并进行深入的技术和经济分析。 硬件并行化对于“自治”结论（大量）的批处理更有价值。 <br><br> 在转向GPU之前，请仔细考虑业务需求，并仔细分析成本（货币，运营，技术），以获取最大利益（减少延迟，高吞吐量）。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN445928/">https://habr.com/ru/post/zh-CN445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN445918/index.html">RollerCoaster Tycoon成立20周年：游戏创造者访谈</a></li>
<li><a href="../zh-CN445920/index.html">直播：如何遏制大型团队的iOS开发</a></li>
<li><a href="../zh-CN445922/index.html">如果您可以阅读Habr，为什么还要观看在线广播</a></li>
<li><a href="../zh-CN445924/index.html">珍宝：智能手表何时变得怪异</a></li>
<li><a href="../zh-CN445926/index.html">美国UFO秘密计划也一直在研究虫洞和其他尺寸。</a></li>
<li><a href="../zh-CN445932/index.html">客户端应用程序安全性：前端开发人员的实用技巧</a></li>
<li><a href="../zh-CN445936/index.html">电子开发。 关于手指上的微控制器</a></li>
<li><a href="../zh-CN445940/index.html">带有Habr的AMA，v 7.0。 柠檬，甜甜圈和新闻</a></li>
<li><a href="../zh-CN445946/index.html">MWC：使用说明</a></li>
<li><a href="../zh-CN445948/index.html">C ++中的继承：初学者，中级，高级</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>