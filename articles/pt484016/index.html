<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßöüèæ üçß ‚òòÔ∏è No√ß√µes b√°sicas de aprendizado profundo no exemplo do autoencoder de depura√ß√£o, n√∫mero da pe√ßa 1 üê≤ üë¶üèº üíõ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Se voc√™ ler o treinamento sobre codificadores autom√°ticos no site keras.io, uma das primeiras mensagens √© mais ou menos assim: na pr√°tica, os codifica...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>No√ß√µes b√°sicas de aprendizado profundo no exemplo do autoencoder de depura√ß√£o, n√∫mero da pe√ßa 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/484016/"><p>  Se voc√™ ler o treinamento sobre codificadores autom√°ticos no site keras.io, uma das primeiras mensagens √© mais ou menos assim: na pr√°tica, os codificadores autom√°ticos quase nunca s√£o usados, mas s√£o frequentemente mencionados nos treinamentos e as pessoas aparecem, por isso decidimos escrever nosso pr√≥prio tutorial: </p><br><p>  <em>Sua principal reivindica√ß√£o √† fama vem do fato de aparecer em muitas aulas introdut√≥rias de aprendizado de m√°quina dispon√≠veis on-line.</em>  <em>Como resultado, muitos rec√©m-chegados ao campo amam absolutamente os auto-codificadores e n√£o conseguem obter o suficiente deles.</em>  <em>Esta √© a raz√£o pela qual este tutorial existe!</em> </p><br><p>  No entanto, uma das tarefas pr√°ticas para as quais elas podem ser aplicadas √© a busca de anomalias, e eu pessoalmente precisei disso no √¢mbito do projeto noturno. </p><br><p>  Na Internet, existem muitos tutoriais sobre codificadores autom√°ticos, o que escrever mais um?  Bem, para ser sincero, havia v√°rias raz√µes para isso: </p><br><ul><li>  Havia um sentimento de que, de fato, os tutoriais eram cerca de 3 ou 4, todos os demais foram reescritos com suas pr√≥prias palavras; </li><li>  Quase tudo - no long√¢nimo MNIST'e com fotos 28x28; </li><li>  Na minha humilde opini√£o - eles n√£o desenvolvem uma intui√ß√£o sobre como tudo isso deve funcionar, mas simplesmente se oferecem para repetir; </li><li>  E o fator mais importante - pessoalmente, quando substitu√≠ o MNIST pelo <strong>meu pr√≥prio conjunto de dados - tudo estupidamente parou de funcionar</strong> . </li></ul><br><p>  A seguir, descrevemos meu caminho no qual os cones s√£o recheados.  Se voc√™ pegar qualquer um dos modelos planos n√£o convolucionais da massa de tutoriais e col√°-lo estupidamente, nada surpreendentemente n√£o funcionar√°.  O objetivo do artigo √© entender o porqu√™ e, ao que me parece, obter algum tipo de entendimento intuitivo de como tudo isso funciona. </p><br><p>  N√£o sou especialista em aprendizado de m√°quina e uso as abordagens com as quais estou acostumado no trabalho di√°rio.  Para cientistas de dados experientes, provavelmente este artigo inteiro ser√° uma loucura, mas para iniciantes, parece-me, algo novo pode surgir. </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">que tipo de projeto</b> <div class="spoiler_text"><p>  Em poucas palavras sobre o projeto, embora o artigo n√£o seja sobre ele.  Existe um receptor ADS-B, que captura dados das aeronaves que voam e as grava, aeronaves, coordenadas na base.  √Äs vezes, os avi√µes se comportam de maneira incomum - eles circulam para queimar combust√≠vel antes do pouso, ou simplesmente voos particulares passam por rotas padr√£o (corredores).  √â interessante isolar cerca de mil aeronaves por dia daquelas que n√£o se comportaram como as demais.  Admito plenamente que os desvios b√°sicos podem ser calculados com mais facilidade, mas estava interessado em tentar <del>  a magia </del>  redes neurais. </p></div></div><br><p>  Vamos come√ßar.  Eu tenho um conjunto de dados de 4000 imagens em preto e branco de 64x64 pixels, √© algo como isto: </p><br><p><img src="https://habrastorage.org/webt/vw/5r/mh/vw5rmhetruoniuc7p4ksjulshde.png"></p><br><p>  Apenas algumas linhas em um fundo preto e na imagem de 64x64 cerca de 2% dos pontos s√£o preenchidos.  Se voc√™ observar muitas fotos, √© claro que a maioria das linhas √© bem parecida. </p><br><p>  N√£o vou entrar em detalhes de como o conjunto de dados foi carregado, processado, porque o objetivo do artigo, novamente, n√£o √© esse.  Apenas mostre um peda√ßo assustador de c√≥digo. </p><br><div class="spoiler">  <b class="spoiler_title">C√≥digo</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># only for google colab %tensorflow_version 2.x import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import os import zipfile import datetime import tensorflow_addons as tfa BATCH_SIZE = 128 AUTOTUNE=tf.data.experimental.AUTOTUNE def load_image(fpath): img_raw = tf.io.read_file(fpath) img = tf.io.decode_png(img_raw, channels=1, dtype=tf.uint8) return tf.image.convert_image_dtype(img, dtype=tf.float32) ## for splitting test/train def is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x,y) ## for image augmentation def random_flip_flop(img): return tf.image.random_flip_left_right(img) def transform_aug(shift_val): def random_transform(img): return tfa.image.translate(img,tf.random.uniform([2], -1*shift_val, shift_val)) return random_transform def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000, transform=0, flip=False): if cache: if isinstance(cache, str): ds = ds.cache(cache) else: ds = ds.cache() ds = ds.shuffle(buffer_size=shuffle_buffer_size) if transform != 0: ds = ds.map(transform_aug(transform)) if flip: ds = ds.map(random_flip_flop) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds def prepare_input_output(x): return (x, x) list_ds = tf.data.Dataset.list_files("/content/planes64/*") imgs_df = list_ds.map(load_image) train = imgs_df.enumerate().filter(is_train).map(lambda x,y: y) train_ds = prepare_for_training(train, transform=10, flip=True) train_ds = train_ds.map(prepare_input_output) val = imgs_df.enumerate().filter(is_test).map(lambda x, y: y) val_ds = val.map(prepare_input_output).batch(BATCH_SIZE, drop_remainder=True)</span></span></code> </pre> </div></div><br><p>  Aqui, por exemplo, √© o primeiro modelo proposto com o keras.io, no qual eles trabalharam e treinaram no mnist: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># this is the size of our encoded representations encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats # this is our input placeholder input_img = Input(shape=(784,)) # "encoded" is the encoded representation of the input encoded = Dense(encoding_dim, activation='relu')(input_img) # "decoded" is the lossy reconstruction of the input decoded = Dense(784, activation='sigmoid')(encoded)</span></span></code> </pre> <br><p>  No meu caso, o modelo √© definido assim: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Existem pequenas diferen√ßas que eu aplico e remodelo diretamente no modelo, e que eu "compresso" n√£o 25 vezes, mas apenas 10. Isso n√£o deve afetar nada. </p><br><p>  Como uma fun√ß√£o de perda - erro quadr√°tico m√©dio, o otimizador n√£o √© fundamental, deixe Adam.  A seguir, treinamos 20 √©pocas, 100 etapas por √©poca. </p><br><p>  Se voc√™ olhar para as m√©tricas - tudo est√° pegando fogo.  Precis√£o == 0,993.  Se voc√™ observar os hor√°rios de treinamento - tudo √© um pouco mais triste, chegamos a um plat√¥ na regi√£o da terceira era. </p><br><p><img src="https://habrastorage.org/webt/fo/rq/r7/forqr7krelrj1xq1qis_jg2jeoq.png"></p><br><p>  Bem, se voc√™ olhar diretamente para o resultado do codificador, obt√©m uma imagem geralmente triste (o original est√° no topo e o resultado da decodifica√ß√£o de codifica√ß√£o est√° abaixo): </p><br><p><img src="https://habrastorage.org/webt/xo/mn/oq/xomnoqmjj80uaal0echobmal9xm.png"></p><br><p>  Em geral, quando voc√™ tenta descobrir por que algo n√£o est√° funcionando, √© uma abordagem suficientemente boa para dividir toda a funcionalidade em blocos grandes e verificar cada um deles isoladamente.  Ent√£o vamos l√°. </p><br><p>  No original do tutorial - dados simples s√£o fornecidos √† entrada do modelo e s√£o obtidos na sa√≠da.  Por que n√£o conferir minhas a√ß√µes achatar e remodelar.  Aqui est√° um modelo n√£o operacional: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Resultado: <br><img src="https://habrastorage.org/webt/vx/z0/aj/vxz0ajadu2qiktdq5ndzj5xje8w.png"></p><br><p>  N√£o h√° nada a ser ensinado aqui.  Bem, ao mesmo tempo, provou que minha fun√ß√£o de visualiza√ß√£o tamb√©m funciona. </p><br><p>  Em seguida, tente tornar o modelo n√£o operacional, mas o mais burro poss√≠vel - apenas recorte a camada de compress√£o, deixe uma camada do tamanho da entrada.  Como eles dizem em todos os tutoriais, eles dizem que √© muito importante que seu modelo aprenda recursos, e n√£o apenas uma fun√ß√£o de identidade.  Bem, √© exatamente isso que tentaremos obter, vamos apenas passar a imagem resultante para a sa√≠da. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Ela est√° aprendendo algo, precis√£o == 0,995 e novamente trope√ßa em um plat√¥. <br><img src="https://habrastorage.org/webt/ro/t6/jk/rot6jkf2ertweb7weeh8d_layui.png"></p><br><p>  Mas, em geral, √© claro que isso n√£o funciona muito bem.  Enfim - o que aprender l√°, passe pela entrada da sa√≠da e pronto. </p><br><p>  Se voc√™ ler a documenta√ß√£o do keras sobre camadas densas, ele descrever√° o que eles fazem: <code>output = activation(dot(input, kernel) + bias)</code> <br>  Para que a sa√≠da coincida com a entrada, duas coisas simples s√£o suficientes - vi√©s = 0 e kernel - a matriz de identidade (√© importante n√£o deixar a matriz cheia de unidades aqui - s√£o coisas muito diferentes).  Felizmente, isso e aquilo podem ser feitos com bastante facilidade a partir da documenta√ß√£o para o mesmo <code>Dense</code> . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation = <span class="hljs-string"><span class="hljs-string">"sigmoid"</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer = tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Porque  definimos o peso imediatamente, ent√£o voc√™ n√£o pode aprender nada - logo √© bom: <br><img src="https://habrastorage.org/webt/dm/tk/5a/dmtk5ardo9xksg8c5v5febzikdo.png"></p><br><p>  Mas se voc√™ come√ßar a treinar, ele come√ßar√°, √† primeira vista, surpreendentemente - o modelo come√ßa com precis√£o == 1,0, mas cai rapidamente. <br>  Avalie o resultado antes do treinamento: <code>8/Unknown - 1s 140ms/step - loss: 0.2488 - accuracy: 1.0000[0.24875330179929733, 1.0]</code> .  Treinamento: </p><br><pre> <code class="plaintext hljs">Epoch 1/20 100/100 [==============================] - 6s 56ms/step - loss: 0.1589 - accuracy: 0.9990 - val_loss: 0.0944 - val_accuracy: 0.9967 Epoch 2/20 100/100 [==============================] - 5s 51ms/step - loss: 0.0836 - accuracy: 0.9964 - val_loss: 0.0624 - val_accuracy: 0.9958 Epoch 3/20 100/100 [==============================] - 5s 50ms/step - loss: 0.0633 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9958 Epoch 4/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0520 - accuracy: 0.9961 - val_loss: 0.0423 - val_accuracy: 0.9961 Epoch 5/20 100/100 [==============================] - 5s 48ms/step - loss: 0.0457 - accuracy: 0.9962 - val_loss: 0.0357 - val_accuracy: 0.9962</code> </pre> <br><p>  Sim, e n√£o est√° muito claro, j√° temos um modelo ideal - a imagem sai 1 em 1 e a perda (erro quadr√°tico m√©dio) mostra quase 0,25. </p><br><p>  A prop√≥sito, essa √© uma pergunta frequente nos f√≥runs - a perda est√° caindo, mas a precis√£o n√£o est√° aumentando, como pode ser isso? <br>  Aqui vale a pena recordar mais uma vez a defini√ß√£o da camada Densa: <code>output = activation(dot(input, kernel) + bias)</code> e a palavra ativa√ß√£o mencionada, que eu ignorei com sucesso acima.  Com pesos da matriz de identidade e sem vi√©s, obtemos <code>output = activation(input)</code> . </p><br><p>  Na verdade, a fun√ß√£o de ativa√ß√£o em nosso c√≥digo fonte j√° est√° indicada, sigmoide, eu a copiei estupidamente e √© isso.  E nos tutoriais, √© aconselh√°vel us√°-lo em qualquer lugar.  Mas voc√™ tem que descobrir. </p><br><p>  Para iniciantes, voc√™ pode ler na documenta√ß√£o o que eles escrevem sobre isso: <code>The sigmoid activation: (1.0 / (1.0 + exp(-x)))</code> .  Pessoalmente, isso n√£o me diz nada, porque eu n√£o sou fantasma uma vez para criar esses gr√°ficos na minha cabe√ßa. <br>  Mas voc√™ pode construir com canetas: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.sigmoid(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/tp/bs/1b/tpbs1bjym9vqahdihjhm1aubrne.png"></p><br><p>  E aqui fica claro que em zero o sigm√≥ide assume o valor 0,5 e na unidade - em torno de 0,73.  E os pontos que temos s√£o pretos (0,0) ou brancos (1,0).  Portanto, o erro quadrado m√©dio da fun√ß√£o de identidade permanece diferente de zero. </p><br><p>  Voc√™ pode at√© olhar as canetas, aqui est√° uma linha da imagem resultante: </p><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.7310586</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> , <span class="hljs-number"><span class="hljs-number">0.5</span></span> ], dtype=float32)</code> </pre> <br><p>  E isso √© tudo, de fato, muito legal, porque v√°rias perguntas aparecem ao mesmo tempo: </p><br><ul><li>  por que isso n√£o era vis√≠vel na visualiza√ß√£o acima? </li><li>  Por que, ent√£o, precis√£o == 1,0, porque as imagens originais s√£o 0 e 1. </li></ul><br><p>  Com a visualiza√ß√£o, tudo √© surpreendentemente simples.  Para exibir as imagens, usei matplotlib: <code>plt.imshow(res_imgs[i][:, :, 0])</code> .  E, como de costume, se voc√™ for para a documenta√ß√£o, tudo ser√° escrito l√°: <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code> <code>The Normalize instance used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling mapping the lowest value to 0 and the highest to 1 is used.</code>  I.e.  a biblioteca normalizou cuidadosamente meus 0,5 e 0,73 no intervalo de 0 a 1. Altere o c√≥digo: </p><br><pre> <code class="python hljs">plt.imshow(res_imgs[i][:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>], norm=matplotlib.colors.Normalize(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/rn/qn/ta/rnqntag1dohhikail8y6myq7siy.png"></p><br><p>  E aqui est√° a pergunta com precis√£o.  Para come√ßar, por h√°bito, vamos √† documenta√ß√£o, leia <code>tf.keras.metrics.Accuracy</code> e parece que eles escrevem compreens√≠vel: </p><br><pre> <code class="plaintext hljs">For example, if y_true is [1, 2, 3, 4] and y_pred is [0, 2, 3, 4] then the accuracy is 3/4 or .75.</code> </pre> <br><p>  Mas, nesse caso, nossa precis√£o deveria ter sido 0. Eu, como resultado, me enterrei na fonte e √© bastante claro para mim: </p><br><pre> <code class="plaintext hljs"> When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.</code> </pre> <br><p>  Al√©m disso, na documenta√ß√£o do site, por algum motivo, este par√°grafo n√£o est√° na descri√ß√£o de <code>.compile</code> . </p><br><p>  Aqui est√° um peda√ßo de c√≥digo de <a href="https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py">https://github.com/tensorflow/tensorflow/blob/66c48046f169f3565d12e5fea263f6d731f9bfd2/tensorflow/python/keras/engine/compile_utils.py</a> </p><br><pre> <code class="python hljs">y_t_rank = len(y_t.shape.as_list()) y_p_rank = len(y_p.shape.as_list()) y_t_last_dim = y_t.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] y_p_last_dim = y_p.shape.as_list()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] is_binary = y_p_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> is_sparse_categorical = ( y_t_rank &lt; y_p_rank <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> y_t_last_dim == <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> y_p_last_dim &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> metric <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, <span class="hljs-string"><span class="hljs-string">'acc'</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_binary: metric_obj = metrics_mod.binary_accuracy <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> is_sparse_categorical: metric_obj = metrics_mod.sparse_categorical_accuracy <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: metric_obj = metrics_mod.categorical_accuracy</code> </pre> <br><p>  <code>y_t</code> √© y_true, ou a sa√≠da esperada, y_p √© y_predicted, ou o resultado previsto. <br>  Temos o formato de dados: <code>shape=(64,64,1)</code> , portanto, a precis√£o √© considerada como precis√£o bin√°ria.  Interesse em prol de como √© considerado: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">binary_accuracy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred, threshold=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> threshold = math_ops.cast(threshold, y_pred.dtype) y_pred = math_ops.cast(y_pred &gt; threshold, y_pred.dtype) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.mean(math_ops.equal(y_true, y_pred), axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br><p>  √â engra√ßado que aqui tenhamos sorte - por padr√£o, tudo √© considerado uma unidade com mais de 0,5 e 0,5 e menos - zero.  Portanto, a precis√£o sai cem por cento para o nosso modelo de identidade, embora, na verdade, os n√∫meros n√£o sejam iguais.  Bem, est√° claro que, se realmente queremos, podemos corrigir o limiar e reduzir a precis√£o a zero, por exemplo, apenas isso n√£o √© realmente necess√°rio.  Essa √© uma m√©trica, n√£o afeta o treinamento, voc√™ s√≥ precisa entender que pode calcul√°-la de mil maneiras diferentes e obter indicadores completamente diferentes.  Apenas como exemplo, voc√™ pode extrair v√°rias m√©tricas com canetas e transferir nossos dados para elas: </p><br><pre> <code class="python hljs">m = tf.keras.metrics.BinaryAccuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nos dar√° <code>1.0</code> . </p><br><p>  E aqui </p><br><pre> <code class="python hljs">m = tf.keras.metrics.Accuracy() m.update_state(x_batch, res_imgs) print(m.result().numpy())</code> </pre> <br><p>  Nos dar√° <code>0.0</code> nos mesmos dados. </p><br><p>  A prop√≥sito, o mesmo trecho de c√≥digo pode ser usado para brincar com fun√ß√µes de perda e entender como elas funcionam.  Se voc√™ ler os tutoriais sobre codificadores autom√°ticos, basicamente eles sugerem o uso de uma das duas fun√ß√µes de perda: erro quadrado m√©dio ou 'binary_crossentropy'.  Voc√™ tamb√©m pode v√™-los ao mesmo tempo. </p><br><p>  Lembro que para <code>mse</code> j√° dei modelos de <code>evaluate</code> : </p><br><pre> <code class="plaintext hljs">8/Unknown - 2s 221ms/step - loss: 0.2488 - accuracy: 1.0000[0.24876083992421627, 1.0]</code> </pre> <br><p>  I.e.  perda == 0,2488.  Vamos ver porque √© isso.  Parece-me pessoalmente que √© o mais simples e mais compreens√≠vel: a diferen√ßa entre y_true e y_predict √© subtra√≠da pixel por pixel, cada resultado √© elevado ao quadrado e, em seguida, a m√©dia √© pesquisada. </p><br><pre> <code class="python hljs">tf.keras.backend.mean(tf.math.squared_difference(x_batch[<span class="hljs-number"><span class="hljs-number">0</span></span>], res_imgs[<span class="hljs-number"><span class="hljs-number">0</span></span>]))</code> </pre> <br><p>  E na sa√≠da: </p><br><pre> <code class="plaintext hljs">&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.24826494&gt;</code> </pre> <br><p>  Aqui a intui√ß√£o √© muito simples - a maioria dos pixels vazios, o modelo produz 0,5, obt√©m 0,25 - diferen√ßa ao quadrado para eles. </p><br><p>  Com a crossenttrtopy bin√°ria, as coisas s√£o um pouco mais complicadas, e h√° artigos completos sobre como isso funciona, mas pessoalmente sempre foi mais f√°cil para mim ler as fontes, e parece algo assim: </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> from_logits: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> isinstance(output, (ops.EagerTensor, variables_module.Variable)): output = _backtrack_identity(output) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> output.op.type == <span class="hljs-string"><span class="hljs-string">'Sigmoid'</span></span>: <span class="hljs-comment"><span class="hljs-comment"># When sigmoid activation function is used for output operation, we # use logits from the sigmoid function directly to compute loss in order # to prevent collapsing zero when training. assert len(output.op.inputs) == 1 output = output.op.inputs[0] return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output) # Compute cross entropy from probabilities. bce = target * math_ops.log(output + epsilon()) bce += (1 - target) * math_ops.log(1 - output + epsilon()) return -bce</span></span></code> </pre> <br><p>  Para ser sincero, trabalhei muito nessas poucas linhas de c√≥digo por muito tempo.  Primeiro, fica imediatamente claro que duas implementa√ß√µes podem funcionar: <code>sigmoid_cross_entropy_with_logits</code> ser√° chamado ou o √∫ltimo par de linhas funcionar√°.  A diferen√ßa √© que <code>sigmoid_cross_entropy_with_logits</code> trabalha com logits (como o nome indica, doh) e o c√≥digo principal trabalha com probabilidades. </p><br><p>  Quem s√£o os logits?  Se voc√™ ler um milh√£o de artigos diferentes sobre o assunto, eles mencionar√£o defini√ß√µes matem√°ticas, f√≥rmulas, outra coisa.  Na pr√°tica, tudo parece surpreendentemente simples (me corrija se estiver errado).  A sa√≠da bruta da previs√£o √© de logits.  Bem, ou log-odds, as probabilidades logar√≠tmicas que s√£o medidas no logistic un <strong>its</strong> - papagaios log√≠sticos. </p><br><div class="spoiler">  <b class="spoiler_title">H√° uma pequena digress√£o - por que existem logaritmos</b> <div class="spoiler_text"><p>  As probabilidades s√£o a propor√ß√£o do n√∫mero de eventos que precisamos para o n√∫mero de eventos que n√£o precisamos (em contraste com a probabilidade, que √© a propor√ß√£o dos eventos que precisamos para o n√∫mero de todos os eventos em geral).  Por exemplo - o n√∫mero de vit√≥rias de nossa equipe e o n√∫mero de suas derrotas.  E h√° um problema.  Continuando o exemplo com as vit√≥rias das equipes, nossa equipe pode perder no meio e ter a chance de vencer 1/2 (um a dois), e talvez extremamente perdedor - e ter a chance de ganhar 1/100.  E na dire√ß√£o oposta - √≠ngreme m√©dio e 2/1, mais √≠ngreme que as montanhas mais altas - e depois 100/1.  E acontece que toda a gama de equipes perdedoras √© descrita por n√∫meros de 0 a 1 e equipes legais - de 1 ao infinito.  Como resultado, √© inconveniente comparar, n√£o h√° simetria, trabalhar com isso em geral √© inconveniente para todos, a matem√°tica √© feia.  E se voc√™ tomar o logaritmo das probabilidades, tudo se tornar√° sim√©trico: </p><br><pre> <code class="plaintext hljs">ln(1/2) == -0.69 ln(2/1) == 0.69 ln(1/100) == -4.6 ln(100/1) == 4.6</code> </pre> </div></div><br><p>  No caso do tensorflow, isso √© bastante arbitr√°rio, porque, estritamente falando, a sa√≠da da camada n√£o √© matematicamente sem log-odds, mas j√° √© aceita.  Se o valor bruto for de -‚àû a + ‚àû -, o logit  Ent√£o eles podem ser convertidos em probabilidades.  Existem duas op√ß√µes para isso: softmax e seu caso especial, sigm√≥ide.  Softmax - Pegue um vetor de logits e converta-os em um vetor de probabilidades, e mesmo assim a soma da probabilidade de todos os eventos contenha 1. Sigmoid (no caso de tf) tamb√©m aceita um vetor de logits, mas converte cada um deles em probabilidades separadamente, independentemente do resto. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1+ln(0.5) == 0.30685281944 tf.math.softmax(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25, 0.5 , 0.25], dtype=float32)&gt; tf.math.sigmoid(tf.constant([0.30685281944, 1.0, 0.30685281944])) ## &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.57611686, 0.7310586 , 0.57611686], dtype=float32)&gt;</span></span></code> </pre> <br><p>  Voc√™ pode olhar dessa maneira.  Existem tarefas de classifica√ß√£o de v√°rios r√≥tulos, h√° tarefas de classifica√ß√£o de v√°rios tipos.  Multiclasse - isto √©, se voc√™ precisar determinar as ma√ß√£s na imagem ou as laranjas e talvez at√© os abacaxis.  E o r√≥tulo m√∫ltiplo √© quando pode haver um vaso de frutas na foto e voc√™ precisa dizer que ele tem ma√ß√£s e laranjas, mas n√£o h√° abacaxi.  Se queremos multiclasse - precisamos de softmax, se queremos multilabel - precisamos de sigm√≥ide. <br>  Aqui temos o caso de multilabel - √© necess√°rio que cada pixel individual (classe) diga se est√° instalado. </p><br><p>  Voltando ao fluxo tensor e por que na entropia cruzada bin√°ria (pelo menos em outras fun√ß√µes de entropia cruzada √© a mesma coisa), existem dois ramos globais.  A crossentropy sempre funciona com probabilidades, falaremos sobre isso um pouco mais tarde.  Depois, h√° simplesmente duas maneiras: as probabilidades j√° entram na entrada ou os logits chegam √† entrada - e depois o sigmoide √© aplicado primeiro a elas para obter a probabilidade.  Aconteceu que a aplica√ß√£o de entropia cruzada sigm√≥ide e de c√°lculo acabou sendo melhor do que apenas calcular entropia cruzada a partir de probabilidades (a sa√≠da matem√°tica das raz√µes est√° na fonte da fun√ß√£o <code>sigmoid_cross_entropy_with_logits</code> , al√©m dos curiosos, √© poss√≠vel pesquisar no Google 'estabilidade num√©rica entropia cruzada'), portanto, mesmo os desenvolvedores do fluxo tensorial recomendam n√£o passar a probabilidade para insira fun√ß√µes de crossentropy e retorne logits brutos.  Bem, no c√≥digo, as fun√ß√µes de perda s√£o verificadas se a √∫ltima camada √© sigm√≥ide, ent√£o elas a cortam e recebem a entrada de ativa√ß√£o, em vez de sua sa√≠da, para calcular, enviando tudo a ser considerado em <code>sigmoid_cross_entropy_with_logits</code> . </p><br><p>  Ok, resolvido, agora binary_crossentropy.  Existem duas explica√ß√µes "intuitivas" populares que medem a entropia cruzada. </p><br><p>  Mais formal: imagine que existe um certo modelo que para n classes conhece a probabilidade de ocorr√™ncia (y <sub>0</sub> , y <sub>1</sub> , ..., y <sub>n</sub> ).  E agora na vida, cada uma dessas classes surgiu k <sub>n</sub> vezes (k <sub>1</sub> , k <sub>1</sub> , ..., k <sub>n</sub> ).  A probabilidade de tal evento √© o produto da probabilidade para cada classe individual - (y <sub>1</sub> ^ k <sub>1</sub> ) (y <sub>2</sub> ^ k <sub>2</sub> ) ... (y <sub>n</sub> ^ k <sub>n</sub> ).  Em princ√≠pio - essa j√° √© uma defini√ß√£o normal de entropia cruzada - a probabilidade de um conjunto de dados √© expressa em termos da probabilidade de outro conjunto de dados.  O problema com essa defini√ß√£o √© que ela passar√° de 0 a 1 e geralmente ser√° muito pequena; n√£o √© conveniente comparar esses valores. <br>  Se tirarmos o logaritmo disso, k <sub>1</sub> log (y <sub>1</sub> ) + k <sub>2</sub> log (y <sub>2</sub> ) sair√° e assim por diante.  O intervalo de valores passa de -‚àû a 0. Multiplique tudo isso por -1 / n - e o intervalo de 0 a + ‚àû sai, al√©m disso, porque  √© expressa como a soma dos valores para cada classe, a mudan√ßa em cada classe √© refletida no valor geral de uma maneira muito previs√≠vel. </p><br><p>  Mais simples: a entropia cruzada mostra quantos bits extras s√£o necess√°rios para expressar a amostra em termos do modelo original.  Se estiv√©ssemos l√° para fazer um logaritmo com base 2, ent√£o ir√≠amos diretamente para os bits.  Usamos logaritmos naturais em todos os lugares, para que eles mostrem o n√∫mero de nat ( <a href="https://en.wikipedia.org/wiki/Nat_(unit">https://en.wikipedia.org/wiki/Nat_(unit</a> )), n√£o bits. </p><br><p>  A entropia cruzada bin√°ria, por sua vez, √© um caso especial de entropia cruzada comum quando o n√∫mero de classes √© duas.  Ent√£o, temos conhecimento suficiente da probabilidade de ocorr√™ncia de uma classe - y <sub>1</sub> , e a probabilidade de ocorr√™ncia da segunda ser√° (1-y <sub>1</sub> ). </p><br><p>  Mas, me parece, um pouco derrapou em mim.  Deixe-me lembr√°-lo, da √∫ltima vez que tentamos criar um codificador autom√°tico de identidade, ele nos mostrou uma imagem bonita e at√© precis√£o de 1,0, mas, na verdade, os n√∫meros foram horr√≠veis.  Para o experimento, voc√™ pode realizar mais alguns testes: <br>  1) a ativa√ß√£o pode ser totalmente removida, haver√° uma identidade limpa <br>  2) voc√™ pode tentar outras fun√ß√µes de ativa√ß√£o, por exemplo, o mesmo </p><br><p>  Sem ativa√ß√£o: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Temos o modelo de identidade perfeito: </p><br><pre> <code class="python hljs">model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  O treinamento, a prop√≥sito, n√£o levar√° a nada, porque a perda == 0,0. </p><br><p>  Agora com relu.  O gr√°fico dele fica assim: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plticker range_tensor = tf.range(<span class="hljs-number"><span class="hljs-number">-4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=tf.float32) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(range_tensor.numpy(), tf.keras.activations.relu(range_tensor).numpy()) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'major'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'-'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) ax.grid(which=<span class="hljs-string"><span class="hljs-string">'minor'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">':'</span></span>, linewidth=<span class="hljs-string"><span class="hljs-string">'0.5'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) ax.yaxis.set_major_locator(plticker.MultipleLocator(base=<span class="hljs-number"><span class="hljs-number">1</span></span>) ) plt.minorticks_on()</code> </pre> <br><p><img src="https://habrastorage.org/webt/wq/ph/iw/wqphiwwmhtmxwfogld4nstyfp9w.png"></p><br><p>  Abaixo de zero - zero, acima - y = x, ou seja,  em teoria, devemos obter o mesmo efeito que na aus√™ncia de ativa√ß√£o - um modelo ideal. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, kernel_initializer=tf.keras.initializers.Identity())) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">"adam"</span></span>, loss=<span class="hljs-string"><span class="hljs-string">"binary_crossentropy"</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>]) model.evaluate(x=val.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x,x)).batch(BATCH_SIZE, drop_remainder=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 8/Unknown - 1s 158ms/step - loss: 0.0000e+00 - accuracy: 1.0000[0.0, 1.0]</span></span></code> </pre> <br><p>  Certo, descobrimos o modelo de identidade, mesmo com alguma parte da teoria que ficou mais clara.  Agora vamos tentar treinar o mesmo modelo para que ele se torne identidade. </p><br><p>  Por divers√£o, conduzirei esse experimento em tr√™s fun√ß√µes de ativa√ß√£o.  Para come√ßar - relu, porque ele se mostrou bem mais cedo (tudo est√° como antes, mas o kernel_initializer √© removido, ent√£o, por padr√£o, ser√° <code>glorot_uniform</code> ): </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Aprende maravilhosamente: </p><br><p><img src="https://habrastorage.org/webt/hv/wz/oo/hvwzoodkp6dxopq7pjkopeeuorw.png"></p><br><p>  O resultado foi muito bom, precis√£o: 0,9999, perda (mse): 2e-04 ap√≥s 20 eras e voc√™ pode treinar mais. </p><br><p><img src="https://habrastorage.org/webt/4y/io/jt/4yiojttsafqnf796ha6all0qmsy.png"></p><br><p>  Em seguida, tente com sigmoide: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  Eu j√° ensinei algo semelhante antes, com a √∫nica diferen√ßa √© que o vi√©s √© desativado aqui.  Ele estuda minuciosamente, fica em um plat√¥ na regi√£o da era 50, precis√£o: 0,9970, perda: 0,01 ap√≥s 60 √©pocas. </p><br><p>  O resultado novamente n√£o √© impressionante: </p><br><p><img src="https://habrastorage.org/webt/_7/fl/o_/_7flo_xh5gkgh8oennqthe2ymhk.png"></p><br><p>  Bem, verifique tamb√©m tanh: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  O resultado √© compar√°vel ao relu - precis√£o: 0,9999, perda: 6e-04 ap√≥s 20 eras e voc√™ pode treinar ainda mais: </p><br><p><img src="https://habrastorage.org/webt/m4/ib/xc/m4ibxctlge5cxs7eqozjt5uwfsc.png"></p><br><p><img src="https://habrastorage.org/webt/sk/r3/p8/skr3p8etvlatcf-mnc6q9sabtfk.png"></p><br><p>  De fato, estou atormentado com a quest√£o de saber se algo pode ser feito para fazer com que o sigmoide mostre um resultado compar√°vel.  Exclusivamente por interesse esportivo. </p><br><p>  Por exemplo, voc√™ pode tentar adicionar BatchNormalization: </p><br><pre> <code class="python hljs">model = tf.keras.Sequential() model.add(tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(tf.keras.layers.BatchNormalization()) model.add(tf.keras.layers.Reshape(target_shape=(<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">64</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)))</code> </pre> <br><p>  E ent√£o algum tipo de m√°gica acontece.  Na 13¬™ era, precis√£o: 1.0.  E os resultados ardentes: </p><br><p><img src="https://habrastorage.org/webt/6x/md/di/6xmddijppdnstc8ire1mxlcwkca.png"></p><br><p>  Iiiii ... neste cabide, terminarei a primeira parte, pois o texto j√° √© muito engra√ßado, e n√£o est√° claro se algu√©m precisa ou n√£o.  Na segunda parte, vou entender o que aconteceu com a m√°gica, experimentar diferentes otimizadores, tentar construir um codificador-decodificador honesto, bater a cabe√ßa na mesa.  Espero que algu√©m esteja interessado e √∫til. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt484016/">https://habr.com/ru/post/pt484016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt484004/index.html">@Pythonetc dezembro de 2019</a></li>
<li><a href="../pt484006/index.html">Dicas e truques do meu canal Telegram @pythonetc, dezembro de 2019</a></li>
<li><a href="../pt484008/index.html">O que √© ser um l√≠der de equipe</a></li>
<li><a href="../pt484012/index.html">Simplifique o processo de escrita em um bloco de notas</a></li>
<li><a href="../pt484014/index.html">10 mitos de SEO para deixar para tr√°s em 2020</a></li>
<li><a href="../pt484018/index.html">Lado t√©cnico de TI do iatismo</a></li>
<li><a href="../pt484020/index.html">Quem voc√™ est√° tentando impressionar com seus prazos?</a></li>
<li><a href="../pt484026/index.html">Parte 6: Portando o MemTest86 + para o RISC-V</a></li>
<li><a href="../pt484028/index.html">Horseshoe Bend - tablet convers√≠vel com visor dobr√°vel</a></li>
<li><a href="../pt484034/index.html">Implementa√ß√£o do esquema de trabalho do armazenamento direcionado de mercadorias com base na unidade cont√°bil do armaz√©m 1C Integrated Automation 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>