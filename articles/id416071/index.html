<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📗 👍🏻 🐽 Jaringan saraf, prinsip dasar operasi, keanekaragaman dan topologi 😅 🕳️ 🔹</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Jaringan saraf telah merevolusi bidang pengenalan pola, tetapi karena interpretabilitas yang tidak jelas dari prinsip operasi, mereka tidak digunakan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jaringan saraf, prinsip dasar operasi, keanekaragaman dan topologi</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416071/">  Jaringan saraf telah merevolusi bidang pengenalan pola, tetapi karena interpretabilitas yang tidak jelas dari prinsip operasi, mereka tidak digunakan di bidang-bidang seperti kedokteran dan penilaian risiko.  Ini membutuhkan representasi visual dari jaringan, yang akan membuatnya bukan kotak hitam, tetapi setidaknya "tembus".  <b>Cristopher Olah, dalam Neural Networks, Manifolds, dan Topology, mendemonstrasikan prinsip-prinsip operasi jaringan saraf dan menghubungkannya dengan teori matematika tentang topologi dan keanekaragaman, yang berfungsi sebagai dasar untuk artikel ini.</b>  Untuk menunjukkan operasi jaringan saraf, digunakan jaringan saraf dalam dimensi rendah. <br><br>  Memahami perilaku jaringan saraf yang dalam umumnya bukan tugas yang sepele.  Lebih mudah untuk mengeksplorasi jaringan saraf dalam dimensi rendah - jaringan di mana hanya ada beberapa neuron di setiap lapisan.  Untuk jaringan dimensi rendah, Anda dapat membuat visualisasi untuk memahami perilaku dan pelatihan jaringan tersebut.  Perspektif ini akan memberikan pemahaman yang lebih dalam tentang perilaku jaringan saraf dan mengamati koneksi yang menggabungkan jaringan saraf dengan bidang matematika yang disebut topologi. <br><br>  Sejumlah hal menarik mengikuti dari ini, termasuk batas bawah fundamental pada kompleksitas jaringan saraf yang mampu mengklasifikasikan set data tertentu. <br><br>  Pertimbangkan prinsip jaringan menggunakan contoh <br><a name="habracut"></a><br>  Mari kita mulai dengan kumpulan data sederhana - dua kurva di pesawat.  Tugas jaringan akan belajar untuk mengklasifikasikan titik-titik milik kurva. <br><br><img src="https://habrastorage.org/webt/m4/od/cv/m4odcvui3bls-vx-zzfhprjatzg.png"><br><br>  Cara yang jelas untuk memvisualisasikan perilaku jaringan saraf, untuk melihat bagaimana algoritma mengklasifikasikan semua objek yang mungkin (dalam contoh kami, poin) dari kumpulan data. <br><br>  Mari kita mulai dengan kelas jaringan saraf yang paling sederhana, dengan satu lapisan input dan output.  Jaringan semacam itu mencoba memisahkan dua kelas data dengan membaginya dengan satu baris. <br><br><img src="https://habrastorage.org/webt/ii/sz/eo/iiszeobw-_fe8mam77ku7o62g7g.png"><br><br>  Jaringan seperti itu tidak digunakan dalam praktik.  Jaringan saraf modern biasanya memiliki beberapa lapisan antara input dan output mereka, yang disebut lapisan "tersembunyi". <br><br><img src="https://habrastorage.org/webt/ej/bp/dw/ejbpdwhjcvzouhxnir9a-zeavye.jpeg"><br><br><h3>  Diagram jaringan sederhana </h3><br>  Kami memvisualisasikan perilaku jaringan ini, mengamati apa yang dilakukannya dengan titik yang berbeda di bidangnya.  Jaringan lapisan tersembunyi memisahkan data dari kurva yang lebih kompleks daripada garis. <br><br><img src="https://habrastorage.org/webt/wp/cr/vf/wpcrvf_a0tiqrhftmxhddovy8tk.png"><br><br>  Dengan setiap lapisan, jaringan mengubah data, menciptakan tampilan baru.  Kita dapat melihat data di setiap tampilan ini dan bagaimana jaringan dengan lapisan tersembunyi mengklasifikasikannya.  Ketika algoritma mencapai presentasi akhir, jaringan saraf akan menarik garis melalui data (atau dalam dimensi yang lebih tinggi - hyperplane). <br><br>  Dalam visualisasi sebelumnya, data dalam tampilan mentah dipertimbangkan.  Anda dapat membayangkan ini dengan melihat pada layer input.  Sekarang, pertimbangkan setelah dikonversi ke lapisan pertama.  Anda dapat membayangkan ini dengan melihat lapisan yang tersembunyi. <br>  Setiap pengukuran berhubungan dengan aktivasi neuron pada lapisan. <br><br><img src="https://habrastorage.org/webt/fc/ha/ch/fchachtyqdnmtcgaxonj6d1xiu0.png"><br><br>  Lapisan tersembunyi dilatih pada tampilan sehingga data dapat dipisahkan secara linear. <br><br>  <b>Render lapisan berkelanjutan</b> <br><br>  Dalam pendekatan yang dijelaskan pada bagian sebelumnya, kita belajar untuk memahami jaringan dengan melihat presentasi yang sesuai dengan setiap lapisan.  Ini memberi kita daftar pandangan tersendiri. <br><br>  Bagian yang tidak sepele adalah memahami bagaimana kita berpindah dari satu ke yang lain.  Untungnya, level jaringan saraf memiliki properti yang memungkinkan hal ini. <br>  Ada banyak jenis lapisan yang digunakan dalam jaringan saraf. <br><br>  Pertimbangkan layer tanh untuk contoh spesifik.  Tanh-tanh-layer (Wx + b) terdiri dari: <br><br><ol><li>  Transformasi linear dari matriks "berat" W </li><li>  Penerjemahan menggunakan vektor b </li><li>  Aplikasi tempat tanh. </li></ol><br>  Kami dapat mewakili ini sebagai transformasi berkelanjutan sebagai berikut: <br><br><img src="https://habrastorage.org/webt/cn/vl/rb/cnvlrblwsnmp9mxxzbawagywuj0.gif"><br><br>  Prinsip operasi ini sangat mirip dengan lapisan standar lainnya yang terdiri dari transformasi affine, diikuti oleh penerapan fungsi aktivasi monotonik secara langsung. <br>  Metode ini dapat digunakan untuk memahami jaringan yang lebih kompleks.  Jadi, jaringan berikut mengklasifikasikan dua spiral yang sedikit kusut menggunakan empat lapisan tersembunyi.  Seiring waktu, dapat dilihat bahwa jaringan saraf bergerak dari pandangan mentah ke tingkat yang lebih tinggi yang dipelajari jaringan untuk mengklasifikasikan data.  Sementara spiral awalnya kusut, menjelang akhir mereka terpisah secara linear. <br><br><img src="https://habrastorage.org/webt/g0/-y/kg/g0-ykgrem8udnrx-xiua2rolthq.gif"><br><br>  Di sisi lain, jaringan berikutnya, yang juga menggunakan beberapa level, tetapi tidak dapat mengklasifikasikan dua spiral, yang lebih kusut. <br><br><img src="https://habrastorage.org/webt/gv/f3/wc/gvf3wc_-a7yw2h-3odoobbsie7u.gif"><br><br>  Perlu dicatat bahwa tugas-tugas ini memiliki kompleksitas terbatas, karena jaringan saraf dimensi rendah digunakan.  Jika jaringan yang lebih luas digunakan, pemecahan masalah disederhanakan. <br><br><h3>  Layers Tang </h3><br>  Setiap lapisan meregangkan dan menekan ruang, tetapi tidak pernah memotong, tidak pecah, dan tidak melipatnya.  Secara intuitif, kita melihat bahwa sifat topologis dipertahankan pada setiap lapisan. <br><br>  Transformasi semacam itu yang tidak mempengaruhi topologi disebut homomorfisma (Wiki - Ini adalah pemetaan sistem aljabar A yang mempertahankan operasi dasar dan hubungan dasar).  Secara formal, mereka adalah bijections yang merupakan fungsi kontinu di kedua arah.  Dalam pemetaan bijective, setiap elemen dari satu set berkorespondensi dengan tepat satu elemen dari set lainnya, dan pemetaan invers yang memiliki properti yang sama didefinisikan. <br><br>  <b>Teorema</b> <br><br>  Lapisan dengan input N dan output N adalah homomorfisme jika matriks bobot W tidak mengalami degenerasi.  (Anda harus berhati-hati tentang domain dan jangkauan.) <br><br><div class="spoiler">  <b class="spoiler_title">Bukti:</b> <div class="spoiler_text">  1. Asumsikan bahwa W memiliki determinan bukan-nol.  Maka itu adalah fungsi linier bijective dengan invers linier.  Fungsi linear kontinu.  Jadi, perkalian dengan W adalah homeomorfisme. <br>  2. Pemetaan - homomorfisme <br>  3. tanh (baik sigmoid dan softplus, tetapi bukan ReLU) adalah fungsi kontinu dengan invers kontinu.  Mereka adalah bijections jika kita berhati-hati tentang area dan jangkauan yang kita pertimbangkan.  Penggunaannya secara langsung adalah homomorfisme. <br><br>  Jadi, jika W memiliki determinan bukan-nol, seratnya adalah homeomorfik. <br></div></div><br><h3>  Topologi dan klasifikasi </h3><br>  Pertimbangkan kumpulan data dua dimensi dengan dua kelas A, B⊂R2: <br><br>  A = {x |  d (x, 0) &lt;1/3} <br><br>  B = {x |  2/3 &lt;d (x, 0) &lt;1} <br><br><img src="https://habrastorage.org/webt/ow/2l/b1/ow2lb1ozxk5p4d-s4ho_lq6_ds8.png"><br><br>  Merah, B biru <br><br>  Persyaratan: Jaringan saraf tidak dapat mengklasifikasikan dataset ini tanpa 3 atau lebih lapisan tersembunyi, berapa pun lebarnya. <br><br>  Seperti disebutkan sebelumnya, klasifikasi dengan fungsi sigmoid atau lapisan softmax setara dengan mencoba menemukan hyperplane (atau dalam hal ini garis) yang memisahkan A dan B dalam representasi akhir.  Dengan hanya dua lapisan tersembunyi, jaringan secara topologi tidak dapat berbagi data dengan cara ini, dan akan mengalami kegagalan dalam kumpulan data ini. <br>  Dalam visualisasi berikutnya, kami mengamati tampilan laten saat jaringan berlatih bersama dengan garis klasifikasi. <br><br><img src="https://habrastorage.org/webt/ap/nt/xe/apntxeprybgn8yf_jchwskwes44.gif"><br><br>  Untuk jaringan pelatihan ini tidak cukup untuk mencapai hasil seratus persen. <br>  Algoritma jatuh ke minimum lokal non-produktif, tetapi mampu mencapai ~ 80% akurasi klasifikasi. <br><br>  Dalam contoh ini, hanya ada satu lapisan tersembunyi, tetapi tidak berhasil. <br>  Pernyataan  Entah setiap lapisan adalah homomorfisme, atau matriks bobot dari lapisan memiliki determinan 0. <br><br><div class="spoiler">  <b class="spoiler_title">Bukti:</b> <div class="spoiler_text">  Jika ini adalah homomorfisme, maka A masih dikelilingi oleh B, dan garis tidak dapat memisahkan mereka.  Tapi anggap itu memiliki penentu 0: maka kumpulan data runtuh pada beberapa sumbu.  Karena kita berhadapan dengan sesuatu yang homeomorfik dengan dataset asli, A dikelilingi oleh B, dan runtuh pada sumbu apa pun berarti bahwa kita akan memiliki beberapa titik dari campuran A dan B, dan ini membuatnya tidak mungkin untuk dibedakan. <br></div></div><br>  Jika kita menambahkan elemen tersembunyi ketiga, masalahnya akan menjadi sepele.  Jaringan saraf mengenali representasi berikut: <br><br><img src="https://habrastorage.org/webt/y1/p8/ol/y1p8olobp-zdo3shlaooa62hy8k.png"><br><br>  Tampilan memungkinkan untuk memisahkan dataset dengan hyperplane. <br>  Untuk lebih memahami apa yang sedang terjadi, mari kita lihat pada kumpulan data yang lebih sederhana, yaitu satu dimensi: <br><br><img src="https://habrastorage.org/webt/ud/by/hr/udbyhrfnqgfdr9r9lvh4cg7apw0.png"><br><br>  A = [- 1 / 3,1 / 3] <br>  B = [- 1, −2 / 3] ∪ [2 / 3,1] <br>  Tanpa menggunakan lapisan dua atau lebih elemen tersembunyi, kami tidak dapat mengklasifikasikan dataset ini.  Tetapi, jika kita menggunakan jaringan dengan dua elemen, kita akan belajar bagaimana merepresentasikan data sebagai kurva yang baik yang memungkinkan kita untuk memisahkan kelas menggunakan garis: <br><br><img src="https://habrastorage.org/webt/8_/w-/bo/8_w-boyjlwhtufafsnqpljuo8u0.gif"><br><br>  Apa yang sedang terjadi  Satu elemen tersembunyi belajar menembak ketika x&gt; -1/2, dan satu elemen belajar menembak ketika x&gt; 1/2.  Ketika yang pertama dipicu, tetapi bukan yang kedua, kita tahu bahwa kita berada di A. <br><br><h3>  Berbagai dugaan </h3><br>  Apakah ini berlaku untuk kumpulan data dari dunia nyata, seperti kumpulan gambar?  Jika Anda serius tentang hipotesis keragaman, saya pikir itu penting. <br><br>  Hipotesis multidimensi adalah bahwa data alami membentuk manifold dimensi rendah di ruang implantasi.  Ada alasan teoretis [1] dan eksperimental [2] untuk percaya bahwa ini benar.  Jika demikian, maka tugas algoritma klasifikasi adalah untuk memisahkan bundel manifold terjerat. <br><br>  Dalam contoh sebelumnya, satu kelas benar-benar mengelilingi yang lain.  Namun, tidak mungkin bahwa berbagai gambar anjing benar-benar dikelilingi oleh koleksi gambar kucing.  Tetapi ada situasi topologis lain yang lebih masuk akal yang mungkin masih muncul, seperti yang akan kita lihat di bagian selanjutnya. <br><br><h3>  Koneksi dan homotopies </h3><br>  Dataset menarik lainnya adalah dua terhubung tori A dan B. <br><br><img src="https://habrastorage.org/webt/wl/er/ns/wlernsr6ibyfnuq2cbkepwgxxq8.png"><br><br>  Seperti kumpulan data sebelumnya yang kami kaji, kumpulan data ini tidak dapat dibagi tanpa menggunakan n +1 dimensi, yaitu dimensi keempat. <br><br>  Koneksi dipelajari dalam teori simpul, di bidang topologi.  Kadang-kadang, ketika kita melihat koneksi, tidak segera jelas apakah itu inkoherensi (banyak hal yang kusut tetapi dapat dipisahkan oleh deformasi terus-menerus) atau tidak. <br><br><img src="https://habrastorage.org/webt/lg/1h/bu/lg1hbu1mehjrj872e61gd73_3vi.png"><br><br>  Inkoherensi yang relatif sederhana. <br><br>  Jika jaringan saraf menggunakan lapisan dengan hanya tiga unit dapat mengklasifikasikannya, maka itu tidak koheren.  (Pertanyaan: Dapatkah semua inkoherensi diklasifikasikan melalui jaringan hanya dengan tiga inkoherensi, secara teoritis?) <br><br>  Dari sudut pandang node ini, visualisasi kontinu dari representasi yang dibuat oleh jaringan saraf adalah prosedur untuk mengurai koneksi.  Dalam topologi, kita akan menyebut isotop ambient ini antara tautan asli dan yang dipisahkan. <br><br>  Secara formal, isotop ruang sekitar antara varietas A dan B adalah fungsi kontinu F: [0,1] × X → Y sedemikian sehingga setiap Ft adalah homeomorfisme dari X hingga jangkauannya, F0 adalah fungsi identitas, dan F1 memetakan fungsi menjadi B. T .e.  Ft terus berpindah dari peta A ke dirinya sendiri, ke peta A ke B. <br><br>  Teorema: ada isotop ruang sekitar antara pintu masuk dan representasi tingkat jaringan jika: a) W tidak merosot, b) kami siap mentransfer neuron ke lapisan tersembunyi dan c) ada lebih dari 1 elemen tersembunyi. <br><br><div class="spoiler">  <b class="spoiler_title">Bukti:</b> <div class="spoiler_text">  1. Bagian tersulit adalah transformasi linear.  Untuk memungkinkan ini, kita perlu W untuk memiliki penentu positif.  Premis kami adalah bahwa itu tidak sama dengan nol, dan kami dapat membalikkan tanda jika negatif dengan mengganti dua neuron yang tersembunyi, dan oleh karena itu kami dapat menjamin bahwa determinannya positif.  Ruang matriks determinan positif terhubung, oleh karena itu terdapat p: [0,1] → GLn ®5 sedemikian sehingga p (0) = Id dan p (1) = W. Kita dapat terus berpindah dari fungsi identitas ke transformasi W menggunakan fungsi x → p (t) x, mengalikan x pada setiap titik waktu t dengan p (t) matriks yang terus-menerus lewat. <br>  2. Kita dapat terus berpindah dari fungsi identitas ke b-peta menggunakan fungsi x → x + tb. <br>  3. Kita dapat terus berpindah dari fungsi yang identik ke penggunaan σ dengan fungsi: x → (1-t) x + tσ (x) <br></div></div><br>  Sampai sekarang, hubungan yang kita bicarakan tidak mungkin muncul dalam data nyata, tetapi ada generalisasi tingkat yang lebih tinggi.  Adalah masuk akal bahwa fitur-fitur seperti itu mungkin ada dalam data nyata. <br><br>  Koneksi dan node adalah manifold satu dimensi, tetapi kita membutuhkan 4 dimensi sehingga jaringan dapat menguraikan semuanya.  Demikian pula, ruang dimensi yang lebih tinggi mungkin diperlukan untuk dapat memperluas manifold n-dimensi.  Semua manifold n-dimensional dapat diperluas dalam 2n + 2 dimensi.  [3] <br><br><h3>  Mudah keluar </h3><br>  Cara mudah adalah mencoba menarik manifold terpisah dan meregangkan bagian-bagian yang kusut mungkin.  Meskipun ini tidak akan dekat dengan solusi asli, solusi seperti itu dapat mencapai akurasi klasifikasi yang relatif tinggi dan menjadi minimum lokal yang dapat diterima. <br><br><img src="https://habrastorage.org/webt/7x/mf/fp/7xmffpy2eilztftxrcbv69xhsf4.png"><br><br>  Minima lokal seperti itu sama sekali tidak berguna dalam hal mencoba memecahkan masalah topologi, tetapi masalah topologi dapat memberikan motivasi yang baik untuk mempelajari masalah ini. <br><br>  Di sisi lain, jika kita hanya tertarik untuk mencapai hasil klasifikasi yang baik, pendekatannya dapat diterima.  Jika sedikit manifold data tertangkap pada manifold lain, apakah ini masalah?  Sangat mungkin bahwa akan dimungkinkan untuk memperoleh hasil klasifikasi yang baik secara sewenang-wenang, terlepas dari masalah ini. <br><br>  Lapisan yang ditingkatkan untuk memanipulasi manifold? <br><br>  Sulit membayangkan bahwa lapisan standar dengan transformasi affine benar-benar bagus untuk memanipulasi manifold. <br><br>  Mungkin masuk akal untuk memiliki lapisan yang sama sekali berbeda, yang dapat kita gunakan dalam komposisi dengan yang lebih tradisional? <br><br>  Studi tentang bidang vektor dengan arah di mana kami ingin menggeser manifold cukup menjanjikan: <br><br><img src="https://habrastorage.org/webt/2z/iw/at/2ziwat9d2bjwlclnjrixwmm5llc.png"><br><br>  Dan kemudian kita merusak ruang berdasarkan bidang vektor: <br><br><img src="https://habrastorage.org/webt/ig/qb/dr/igqbdrqbcl3gy85kv4rzbhirflk.png"><br><br>  Seseorang dapat mempelajari bidang vektor pada titik-titik tetap (hanya mengambil beberapa titik tetap dari data uji yang ditetapkan untuk digunakan sebagai jangkar) dan menginterpolasi entah bagaimana. <br><br><div class="spoiler">  <b class="spoiler_title">Bidang vektor di atas memiliki bentuk:</b> <div class="spoiler_text">  P (x) = (v0f0 (x) + v1f1 (x)) / (1 + 0 (x) + f1 (x)) <br></div></div><br>  Di mana v0 dan v1 adalah vektor, dan f0 (x) dan f1 (x) adalah n-dimensi Gaussians. <br><br><h3>  Lapisan Tetangga K-Terdekat </h3><br>  Keterpisahan linear dapat menjadi kebutuhan besar dan mungkin tidak masuk akal untuk jaringan saraf.  Wajar untuk menggunakan metode k-tetangga terdekat (k-NN).  Namun, keberhasilan k-NN sangat tergantung pada presentasi yang diklasifikasi, sehingga presentasi yang baik diperlukan sebelum k-NN dapat bekerja dengan baik. <br><br>  k-NN dapat dibedakan sehubungan dengan representasi di mana ia bertindak.  Dengan cara ini, kita bisa langsung melatih jaringan untuk mengklasifikasikan k-NN.  Ini dapat dilihat sebagai semacam lapisan "tetangga terdekat" yang bertindak sebagai alternatif untuk softmax. <br>  Kami tidak ingin memperingatkan dengan seluruh rangkaian pelatihan kami untuk setiap pesta kecil, karena itu akan menjadi prosedur yang sangat mahal.  Pendekatan yang diadaptasi adalah untuk mengklasifikasikan setiap elemen dari mini-lot berdasarkan kelas dari elemen-elemen lain dari mini-lot, memberikan masing-masing berat unit dibagi dengan jarak dari target klasifikasi. <br><br>  Sayangnya, bahkan dengan arsitektur yang kompleks, menggunakan k-NN mengurangi kemungkinan kesalahan - dan menggunakan arsitektur yang lebih sederhana menurunkan hasilnya. <br><br><h3>  Kesimpulan </h3><br>  Properti topologi data, seperti hubungan, dapat membuat pembagian linear kelas tidak mungkin menggunakan jaringan dimensi rendah, terlepas dari kedalamannya.  Bahkan dalam kasus di mana secara teknis dimungkinkan.  Misalnya, spiral, yang sangat sulit untuk dipisahkan. <br><br>  Untuk klasifikasi data yang akurat, jaringan saraf membutuhkan lapisan lebar.  Selain itu, lapisan tradisional jaringan saraf tidak cocok untuk mewakili manipulasi penting dengan manifold;  bahkan jika kita mengatur bobot secara manual, akan sulit untuk secara kompak mewakili transformasi yang kita inginkan. <br><br><div class="spoiler">  <b class="spoiler_title">Tautan ke sumber dan penjelasan</b> <div class="spoiler_text">  [1] Banyak transformasi alami yang mungkin ingin Anda lakukan pada gambar, seperti menerjemahkan atau menskalakan objek di dalamnya, atau mengubah pencahayaan, akan membentuk kurva kontinu dalam ruang gambar jika Anda melakukannya secara terus menerus. <br><br>  [2] Carlsson et al.  menemukan bahwa tambalan lokal berupa botol klein. <br>  [3] Hasil ini disebutkan dalam ayat Wikipedia tentang versi Isotop. <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id416071/">https://habr.com/ru/post/id416071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id416059/index.html">Mobio Talks dengan Daniil Shuleiko (Yandex.Taxi) tentang merger dengan Uber, pasar taksi, dan persaingan</a></li>
<li><a href="../id416061/index.html">Begitu-begitu-begitu, saya melihat semuanya</a></li>
<li><a href="../id416063/index.html">Negosiasi Rusia tidak punya catatan</a></li>
<li><a href="../id416067/index.html">Gambaran Umum Kerentanan Mikrotik Winbox. Atau file besar</a></li>
<li><a href="../id416069/index.html">Migrasi data ElasticSearch Lossless</a></li>
<li><a href="../id416073/index.html">Bot perdagangan cryptocurrency sederhana</a></li>
<li><a href="../id416075/index.html">FSB ingin memperkenalkan tanggung jawab atas penggunaan tersembunyi perekam suara dan kamera di telepon pintar [dan tidak hanya]</a></li>
<li><a href="../id416077/index.html">PlantUML - Semuanya Analis Bisnis Perlu Membuat Grafik dalam Dokumentasi Perangkat Lunak</a></li>
<li><a href="../id416079/index.html">Corona Native untuk Android - menggunakan kode Java khusus dalam permainan yang ditulis dalam Corona</a></li>
<li><a href="../id416081/index.html">Ada yang salah dengan kembalinya Habr</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>