<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëºüèø üë®üèº‚ÄçüöÄ ‚úäüèª O dia em que Dodo parou. Script ass√≠ncrono üôã üöò üññüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Cada SRE da nossa equipe sonhava em dormir em paz √† noite. Sonhos se tornam realidade. Neste artigo, falarei sobre isso e como alcan√ßamos o ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O dia em que Dodo parou. Script ass√≠ncrono</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dodopizzadev/blog/461081/">  Ol√° Habr!  Cada SRE da nossa equipe sonhava em dormir em paz √† noite.  Sonhos se tornam realidade.  Neste artigo, falarei sobre isso e como alcan√ßamos o desempenho e a estabilidade do nosso sistema Dodo IS. <br><br><img src="https://habrastorage.org/webt/wk/2o/t6/wk2ot6razkmzgly1s69fdwz5quq.png"><a name="habracut"></a><br><blockquote>  <b>Uma s√©rie de artigos sobre o colapso do sistema Dodo IS *</b> : <br><br>  1. O <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">dia em que Dodo est√° parado.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Script s√≠ncrono.</a> <br>  2. O dia em que o Dodo est√° parado.  Script ass√≠ncrono. <br><br>  * Os <i>materiais foram escritos com base no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">meu desempenho no DotNext 2018 em Moscou</a></i> . </blockquote>  Em um artigo anterior, analisamos o bloqueio de problemas de c√≥digo no paradigma da multitarefa preemptiva.  Supunha-se que era necess√°rio reescrever o c√≥digo de bloqueio em ass√≠ncrono / espera.  Ent√£o n√≥s fizemos.  Agora vamos falar sobre quais problemas surgiram quando fizemos isso. <br><br><h2>  Introduzimos o termo Concorr√™ncia </h2><br>  Antes de voc√™ ass√≠ncrono, voc√™ deve inserir o termo simultaneidade. <br><blockquote>  Na teoria das filas, <b>simultaneidade</b> √© o n√∫mero de clientes que est√£o atualmente dentro do sistema.  A simultaneidade √†s vezes √© confundida com o paralelismo, mas na realidade essas s√£o duas coisas diferentes. </blockquote>  Para aqueles que s√£o novos no Concurrency pela primeira vez, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">recomendo o v√≠deo de Rob Pike</a> .  Concorr√™ncia √© quando estamos lidando com muitas coisas ao mesmo tempo, e Paralelismo √© quando estamos fazendo muitas coisas ao mesmo tempo. <br><br>  Nos computadores, muitas coisas n√£o acontecem em paralelo.  Uma coisa √© computar em v√°rios processadores.  O grau de paralelismo √© limitado pelo n√∫mero de threads da CPU. <br><br>  De fato, o Threads faz parte do conceito de Multitarefa Preemptiva, uma maneira de modelar a Concorr√™ncia em um programa quando contamos com o sistema operacional na pergunta Concorr√™ncia.  Esse modelo permanece √∫til desde que entendamos que estamos lidando especificamente com o modelo de simultaneidade, e n√£o com simultaneidade. <br><br>  Async / waitit √© o a√ß√∫car sint√°tico do State Machine, outro modelo de simultaneidade √∫til que pode ser executado em um ambiente de thread √∫nico.  Em ess√™ncia, isso √© multitarefa cooperativa - o modelo em si n√£o leva em conta o paralelismo.  Em combina√ß√£o com o Multithreading, colocamos um modelo em cima do outro, e a vida √© muito complicada. <br><br><h2>  Compara√ß√£o dos dois modelos </h2><br><h4>  Como funcionou no modelo Multitarefa Preemptiva </h4><br>  Digamos que tenhamos 20 threads e 20 solicita√ß√µes em processamento por segundo.  A imagem mostra um pico - 200 solicita√ß√µes no sistema ao mesmo tempo.  Como isso p√¥de acontecer: <br><br><ul><li>  as solicita√ß√µes podem ser agrupadas se 200 clientes clicarem em um bot√£o ao mesmo tempo; </li><li>  o coletor de lixo pode interromper os pedidos de v√°rias dezenas de milissegundos; </li><li>  os pedidos podem ser atrasados ‚Äã‚Äãem qualquer fila se o proxy suportar a fila. </li></ul><br>  H√° muitas raz√µes pelas quais as solicita√ß√µes por um curto per√≠odo de tempo se acumularam e v√™m em um √∫nico pacote.  De qualquer forma, nada de terr√≠vel aconteceu, eles ficaram na fila do Pool de Threads e terminaram lentamente.  N√£o h√° mais picos, tudo continua, como se nada tivesse acontecido. <br><br>  Suponha que o algoritmo inteligente do Pool de Threads (e h√° elementos de aprendizado de m√°quina l√°) tenha decidido que at√© o momento n√£o h√° raz√£o para aumentar o n√∫mero de Threads.  O conjunto de conex√µes no MySql tamb√©m √© 20 porque Threads = 20.  Portanto, precisamos apenas de 20 conex√µes com o SQL. <br><br><img src="https://habrastorage.org/webt/gm/ch/pz/gmchpzxpvegoyljdauranwzgn7k.png"><br><br>  Nesse caso, o n√≠vel de simultaneidade do servidor do ponto de vista do sistema externo = 200. O servidor j√° recebeu essas solicita√ß√µes, mas ainda n√£o as concluiu.  No entanto, para um aplicativo em execu√ß√£o no paradigma Multithreading, o n√∫mero de solicita√ß√µes simult√¢neas √© limitado pelo tamanho atual do Conjunto de Encadeamentos = 20. Portanto, estamos lidando com o grau de Concorr√™ncia = 20. <br><br><h4>  Como tudo agora funciona no modelo ass√≠ncrono </h4><br><img width="33%" height="33%" src="https://habrastorage.org/webt/dg/yz/pz/dgyzpzj-nl9rawn5ctxdfr3gxgm.png"><br><br>  Vamos ver o que acontece em um aplicativo executando ass√≠ncrono / aguardar com a mesma carga e distribui√ß√£o de solicita√ß√µes.  N√£o h√° fila antes de criar uma tarefa, e a solicita√ß√£o √© processada imediatamente.  Obviamente, o Thread do ThreadPool √© usado por um curto per√≠odo de tempo e a primeira parte da solicita√ß√£o, antes de entrar em contato com o banco de dados, √© executada imediatamente.  Como o Thread retorna rapidamente ao Pool de Threads, n√£o precisamos de muitos Threads para processar.  Neste diagrama, n√£o exibimos nenhum conjunto de threads, ele √© transparente. <br><br><img src="https://habrastorage.org/webt/bm/6h/to/bm6hto7o6gxnrlg9ruzhfsxfet0.png"><br><br>  O que isso significa para a nossa aplica√ß√£o?  A imagem externa √© a mesma - o n√≠vel de simultaneidade = 200. Ao mesmo tempo, a situa√ß√£o interna mudou.  Anteriormente, as solicita√ß√µes eram "agrupadas" na fila do ThreadPool, agora o grau de simultaneidade do aplicativo tamb√©m √© 200, porque n√£o temos restri√ß√µes por parte do TaskScheduler.  Viva!  Atingimos o objetivo de ass√≠ncrono - o aplicativo "lida" com praticamente qualquer grau de simultaneidade! <br><br><h4>  Consequ√™ncias: degrada√ß√£o n√£o linear do sistema </h4><br>  O aplicativo tornou-se transparente do ponto de vista da simultaneidade; agora, a simultaneidade √© projetada no banco de dados.  Agora precisamos de um conjunto de conex√µes do mesmo tamanho = 200. O banco de dados √© a CPU, mem√≥ria, rede, armazenamento.  Este √© o mesmo servi√ßo com seus problemas, como qualquer outro.  Quanto mais solicita√ß√µes tentamos executar ao mesmo tempo, mais lento elas s√£o executadas. <br><br>  Em plena carga no banco de dados, na melhor das hip√≥teses, o Tempo de resposta diminui linearmente: voc√™ fez o dobro de consultas e come√ßou a funcionar duas vezes mais lentamente.  Na pr√°tica, devido √† concorr√™ncia de consultas, a sobrecarga ocorrer√° necessariamente e pode resultar que o sistema se degradar√° de maneira n√£o linear. <br><br><h4>  Por que isso est√° acontecendo? </h4><br>  Raz√µes para a segunda ordem: <br><br><ul><li>  Agora, o banco de dados precisa ser mantido simultaneamente na mem√≥ria da estrutura de dados para atender a mais solicita√ß√µes; </li><li>  Agora, o banco de dados precisa atender a cole√ß√µes maiores (e isso √© algoritmicamente desvantajoso). </li></ul><br>  Motivo de primeira ordem: <br><br><ul><li>  conten√ß√£o, que foi discutida um pouco <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no artigo anterior</a> . </li></ul><br>  No final, async luta contra recursos limitados e ... vence!  O banco de dados falha e come√ßa a ficar mais lento.  Com isso, o servidor aumenta ainda mais a simultaneidade e o sistema n√£o pode mais sair dessa situa√ß√£o com honra. <br><br><h2>  S√≠ndrome da Morte S√∫bita do Servidor </h2><br>  √Äs vezes ocorre uma situa√ß√£o interessante.  N√≥s temos um servidor.  Ele trabalha para si mesmo assim, est√° tudo em ordem.  Existem recursos suficientes, mesmo com uma margem.  De repente, recebemos uma mensagem dos clientes de que o servidor est√° ficando mais lento.  Observamos o gr√°fico e vemos que houve um aumento na atividade do cliente, mas agora tudo est√° normal.  Pensando em um ataque ou coincid√™ncia do DOS.  Agora tudo parece estar bem.  S√≥ que agora o servidor continua est√∫pido e tudo fica mais dif√≠cil at√© que os tempos limite cheguem.  Depois de algum tempo, outro servidor que usa o mesmo banco de dados tamb√©m come√ßa a se curvar.  Uma situa√ß√£o familiar? <br><br><h4>  Por que o sistema morreu? </h4><br>  Voc√™ pode tentar explicar isso pelo fato de que, em algum momento, o servidor recebeu um n√∫mero m√°ximo de solicita√ß√µes e "quebrou".  Mas sabemos que a carga foi reduzida e o servidor depois disso n√£o melhorou por muito tempo, at√© que a carga desapareceu completamente. <br><br>  A pergunta ret√≥rica: o servidor deveria quebrar devido √† carga excessiva?  Eles fazem isso? <br><br><h4>  Simulamos uma situa√ß√£o de falha do servidor </h4><br>  Aqui n√£o analisaremos gr√°ficos de um sistema de produ√ß√£o real.  No momento da falha do servidor, muitas vezes n√£o conseguimos esse agendamento.  O servidor est√° ficando sem recurso da CPU e, como resultado, n√£o pode gravar logs, fornecer m√©tricas.  Nos diagramas da √©poca do desastre, muitas vezes √© observada uma quebra em todos os gr√°ficos. <br><br>  Os SREs devem ser capazes de produzir sistemas de monitoramento menos propensos a esse efeito.  Sistemas que, em qualquer situa√ß√£o, fornecem pelo menos algumas informa√ß√µes e, ao mesmo tempo, s√£o capazes de analisar sistemas post-mortem usando informa√ß√µes fragmentadas.  Para fins educacionais, usamos uma abordagem ligeiramente diferente neste artigo. <br><br>  Vamos tentar criar um modelo que matematicamente funcione como um servidor sob carga.  A seguir, estudaremos as caracter√≠sticas do servidor.  Descartamos a n√£o linearidade de servidores reais e simulamos uma situa√ß√£o em que a desacelera√ß√£o linear ocorre quando a carga cresce acima do nominal.  Duas vezes o n√∫mero de solicita√ß√µes necess√°rias - atendemos duas vezes mais devagar. <br><br>  Essa abordagem permitir√°: <br><br><ul><li>  considere o que acontecer√° na melhor das hip√≥teses; </li><li>  fa√ßa m√©tricas precisas. </li></ul><br>  Navega√ß√£o agendada: <br><br><ul><li>  azul - o n√∫mero de solicita√ß√µes para o servidor; </li><li>  respostas do servidor verde; </li><li>  amarelo - timeouts; </li><li>  cinza escuro - solicita√ß√µes que foram inclu√≠das nos recursos do servidor porque o cliente n√£o esperou por uma resposta de tempo limite.  √Äs vezes, um cliente pode relatar isso ao servidor por uma desconex√£o, mas, em geral, esse luxo pode n√£o ser tecnicamente vi√°vel, por exemplo, se o servidor executar um trabalho vinculado √† CPU sem coopera√ß√£o com o cliente. </li></ul><br><br><img src="https://habrastorage.org/webt/8d/r8/lr/8dr8lr7gizm-ovozc0laylwadaa.png"><br><br>  Por que o gr√°fico de solicita√ß√µes do cliente (azul no diagrama) acabou sendo assim?  Normalmente, a programa√ß√£o de pedidos em nossas pizzarias cresce suavemente pela manh√£ e diminui √† noite.  Mas observamos tr√™s picos no fundo da curva uniforme usual.  Esta forma do gr√°fico n√£o foi escolhida para o modelo por acaso, mas sim.  O modelo nasceu durante a investiga√ß√£o de um incidente real com o servidor do contact center da pizzaria na R√∫ssia durante a Copa do Mundo. <br><br><h2>  Caso "Copa do Mundo" </h2><br>  Sentamos e esperamos por mais pedidos.  Preparados para o campeonato, agora os servidores poder√£o passar por um teste de for√ßa. <br><br>  O primeiro pico - os f√£s de futebol v√£o assistir ao campeonato, est√£o com fome e compram pizza.  Durante o primeiro semestre, eles est√£o ocupados e n√£o podem pedir.  Mas as pessoas que s√£o indiferentes ao futebol podem, ent√£o, no gr√°fico, tudo continua como sempre. <br><br>  E ent√£o a primeira metade termina e o segundo pico chega.  Os f√£s ficaram nervosos, famintos e fizeram tr√™s vezes mais pedidos do que no primeiro pico.  Pizza √© comprada a uma taxa terr√≠vel.  Ent√£o a segunda metade come√ßa, e novamente n√£o para pizza. <br><br>  Enquanto isso, o servidor do contact center come√ßa a se curvar lentamente e a atender solicita√ß√µes cada vez mais lentamente.  O componente do sistema, neste caso, o servidor da web Call Center, est√° desestabilizado. <br><br>  O terceiro pico chegar√° quando a partida terminar.  Os f√£s e o sistema aguardam uma penalidade. <br><br><h4>  Analisamos os motivos da falha do servidor </h4><br>  O que aconteceu  O servidor pode conter 100 solicita√ß√µes condicionais.  Entendemos que ele foi projetado para esse poder e n√£o o suportar√° mais.  Chega um pico, que por si s√≥ n√£o √© t√£o grande.  Mas a √°rea cinzenta da simultaneidade √© muito maior. <br><br>  O modelo foi projetado para que a simultaneidade seja numericamente igual ao n√∫mero de pedidos por segundo; portanto, visualmente no gr√°fico, ele deve ter a mesma escala.  No entanto, √© muito maior porque se acumula. <br><br>  Vemos uma sombra do gr√°fico aqui - s√£o solicita√ß√µes que come√ßaram a retornar ao cliente, executadas (mostradas pela primeira seta vermelha).  A escala de tempo √© condicional para ver o deslocamento da hora.  O segundo pico j√° derrubou nosso servidor.  Ele caiu e come√ßou a processar quatro vezes menos solicita√ß√µes do que o habitual. <br><br><img src="https://habrastorage.org/webt/n1/92/qw/n192qwxtwdrfatt_a-lb8y8eire.png"><br><br>  Na segunda metade do gr√°fico, fica claro que alguns pedidos ainda foram executados no in√≠cio, mas depois apareceram pontos amarelos - os pedidos pararam completamente. <br><br><img src="https://habrastorage.org/webt/pi/la/nv/pilanvzebdl_vl3k3hno9vwezga.png"><br><br>  Mais uma vez toda a programa√ß√£o.  Pode-se ver que a simultaneidade est√° ficando louca.  Uma montanha enorme aparece. <br><br><img src="https://habrastorage.org/webt/ci/l6/kb/cil6kblebzhjokmuvzkwolmngvo.png"><br><br>  Normalmente, analis√°vamos m√©tricas completamente diferentes: qu√£o lentamente a solicita√ß√£o foi conclu√≠da, quantas solicita√ß√µes por segundo.  Nem olhamos para a simultaneidade, nem pensamos nessa m√©trica.  Mas em v√£o, porque √© exatamente essa quantidade que melhor mostra o momento da falha do servidor. <br><br>  Mas de onde veio uma montanha t√£o grande?  O maior pico de carga j√° passou! <br><br><h2>  Little Law </h2><br>  A lei de Little governa a simultaneidade. <br><br>  <i>L (n√∫mero de clientes dentro do sistema) = Œª (velocidade da estadia) ‚àó W (tempo que eles passam dentro do sistema)</i> <br><br>  Esta √© uma m√©dia.  No entanto, nossa situa√ß√£o est√° se desenvolvendo dramaticamente, a m√©dia n√£o nos conv√©m.  Vamos diferenciar essa equa√ß√£o e depois integrar.  Para fazer isso, olhe o livro de John Little, que inventou essa f√≥rmula, e veja a integral l√°. <br><br><img src="https://habrastorage.org/webt/sx/f5/dz/sxf5dzgwc9l7low8fild5cpsrf0.png"><br><br>  Temos o n√∫mero de entradas no sistema e o n√∫mero daqueles que saem do sistema.  A solicita√ß√£o chega e sai quando tudo estiver conclu√≠do.  Abaixo est√° uma regi√£o de crescimento gr√°fico correspondente ao crescimento linear da simultaneidade. <br><br><img src="https://habrastorage.org/webt/ax/rv/du/axrvdu3vyx1iw9afieohv5pe-cs.png"><br><br>  Existem alguns pedidos ecol√≥gicos.  Estes s√£o os que realmente est√£o sendo implementados.  Os azuis s√£o aqueles que v√™m.  Entre os tempos, temos o n√∫mero usual de solicita√ß√µes, a situa√ß√£o √© est√°vel.  Mas a concorr√™ncia ainda est√° crescendo.  O servidor n√£o ir√° mais lidar com esta situa√ß√£o em si.  Isso significa que ele cair√° em breve. <br><br>  Mas por que a concorr√™ncia est√° aumentando?  Observamos a integral da constante.  Nada muda em nosso sistema, mas a integral parece uma fun√ß√£o linear que cresce apenas. <br><br><h2>  Vamos jogar? </h2><br>  A explica√ß√£o com integrais √© complicada se voc√™ n√£o se lembra da matem√°tica.  Aqui, proponho me aquecer e jogar o jogo. <br><br><h4>  N√∫mero do jogo 1 </h4><br>  <b>Pr√©</b> - <b>requisitos</b> : O servidor recebe solicita√ß√µes, cada uma requer tr√™s per√≠odos de processamento na CPU.  O recurso da CPU √© dividido igualmente entre todas as tarefas.  √â semelhante √† maneira como os recursos da CPU s√£o consumidos durante a multitarefa preemptiva.  O n√∫mero na c√©lula significa a quantidade de trabalho restante ap√≥s esta medida.  Para cada etapa condicional, uma nova solicita√ß√£o chega. <br><br>  Imagine que voc√™ recebeu uma solicita√ß√£o.  Apenas 3 unidades de trabalho. No final do primeiro per√≠odo de processamento, restam 2 unidades. <br><br>  No segundo per√≠odo, outra solicita√ß√£o √© em camadas, agora as duas CPUs est√£o ocupadas.  Eles fizeram uma unidade de trabalho para as duas primeiras consultas.  Resta completar 1 e 2 unidades para a primeira e a segunda solicita√ß√£o, respectivamente. <br><br>  Agora o terceiro pedido chegou e a divers√£o come√ßa.  Parece que a primeira solicita√ß√£o deveria ter sido conclu√≠da, mas nesse per√≠odo tr√™s solicita√ß√µes j√° compartilham o recurso da CPU; portanto, o grau de conclus√£o das tr√™s solicita√ß√µes agora √© fracion√°rio no final do terceiro per√≠odo de processamento: <br><br><img src="https://habrastorage.org/webt/k-/tq/mv/k-tqmvjsqabbv0zgwt_vmkfkhy4.png"><br><br>  Ainda mais interessante!  A quarta solicita√ß√£o foi adicionada e agora o grau de simultaneidade j√° √© 4, pois todas as quatro solicita√ß√µes exigiram um recurso nesse per√≠odo.  Enquanto isso, a primeira solicita√ß√£o at√© o final do quarto per√≠odo j√° foi conclu√≠da, n√£o segue para o pr√≥ximo per√≠odo e possui 0 trabalhos restantes para a CPU. <br><br>  Como a primeira solicita√ß√£o j√° foi conclu√≠da, vamos resumir para ele: ficou um ter√ßo a mais do que esper√°vamos.  Supunha-se que a dura√ß√£o de cada tarefa horizontalmente idealmente = 3, de acordo com a quantidade de trabalho.  Marcamos com laranja, como um sinal de que n√£o estamos completamente satisfeitos com o resultado. <br><br><img src="https://habrastorage.org/webt/ap/9n/uy/ap9nuyfqun_gd1ggeidqdlomxuy.png"><br><br>  O quinto pedido chega.  O grau de simultaneidade ainda √© 4, mas vemos que na quinta coluna o trabalho restante √© mais total.  Isso ocorre porque resta mais trabalho na quarta coluna do que na terceira. <br><br>  Continuamos mais tr√™s per√≠odos.  √Ä espera de respostas. <br>  - Servidor, ol√°! <br>  - ... <br><br><img src="https://habrastorage.org/webt/tv/2m/8r/tv2m8r8selzumgub75zkvs78deu.png"><br><br>  "Sua liga√ß√£o √© muito importante para n√≥s ..." <br><br><img src="https://habrastorage.org/webt/ud/k9/rk/udk9rk7ynqduovmhyymp7shqe0q.png"><br><br>  Bem, finalmente veio a resposta para o segundo pedido.  Os tempos de resposta s√£o duas vezes maiores que o esperado. <br><br><img src="https://habrastorage.org/webt/tt/7m/iv/tt7mivq-stfincjhlwxm7wqznsq.png"><br><br>  O grau de simultaneidade j√° triplicou e nada indica que a situa√ß√£o mudar√° para melhor.  N√£o desenhei mais, porque o tempo de resposta √† terceira solicita√ß√£o n√£o se encaixa mais na imagem. <br><br><blockquote>  Nosso servidor entrou em um estado indesej√°vel, do qual nunca sair√° por conta pr√≥pria.  <b>Fim do jogo</b> </blockquote><br><h2>  O que caracteriza o estado GameOver do servidor? </h2><br>  As solicita√ß√µes s√£o acumuladas na mem√≥ria indefinidamente.  Cedo ou tarde, a mem√≥ria simplesmente terminar√°.  Al√©m disso, com um aumento de escala, a sobrecarga da CPU para atender a v√°rias estruturas de dados aumenta.  Por exemplo, o pool de conex√µes agora deve rastrear o tempo limite para mais conex√µes, o coletor de lixo agora deve verificar novamente mais objetos na pilha e assim por diante. <br><br>  Explorar todas as poss√≠veis conseq√º√™ncias do ac√∫mulo de objetos ativos n√£o √© o objetivo deste artigo, mas mesmo um simples ac√∫mulo de dados na RAM j√° √© suficiente para encher o servidor.  Al√©m disso, j√° vimos que o servidor cliente projeta seus problemas de simultaneidade no servidor de banco de dados e em outros servidores que ele usa como cliente. <br><br>  O mais interessante: agora, mesmo que voc√™ envie uma carga menor ao servidor, ele ainda n√£o ser√° recuperado.  Todas as solicita√ß√µes terminam com um tempo limite e o servidor consome todos os recursos dispon√≠veis. <br><br>  E o que realmente esper√°vamos ?!  Afinal, conscientemente, demos ao servidor uma quantidade de trabalho que ele n√£o conseguiu lidar. <br><br>  Ao lidar com a arquitetura de sistema distribu√≠da, √© √∫til pensar em como as pessoas comuns resolvem esses problemas.  Tome, por exemplo, uma boate.  Parar√° de funcionar se muitas pessoas entrarem nele.  O seguran√ßa lida com o problema simplesmente: parece quantas pessoas est√£o l√° dentro.  Uma esquerda - lan√ßa outra.  Um novo convidado chegar√° e apreciar√° o tamanho da fila.  Se a fila for longa, ele ir√° para casa.  E se voc√™ aplicar esse algoritmo ao servidor? <br><br><img src="https://habrastorage.org/webt/wg/tn/p3/wgtnp3n6qq57dqzfi-ac9s0rj1u.png"><br><br>  Vamos jogar de novo. <br><br><h4>  N√∫mero do jogo 2 </h4><br>  <b>Pr√©</b> - <b>requisitos</b> : Novamente, temos duas CPUs, as mesmas tarefas de 3 unidades, chegando a cada per√≠odo, mas agora definiremos o seguran√ßa e as tarefas ser√£o inteligentes - se elas virem que o comprimento da fila √© 2, voltam para casa imediatamente. <br><br><img src="https://habrastorage.org/webt/b4/cs/gt/b4csgtkcmi3hw1qog4fko6aus2o.png"><br><br><img src="https://habrastorage.org/webt/uw/gi/-v/uwgi-vopihzere8fs_c5f5yctfo.png"><br><br>  O terceiro pedido chegou.  Nesse per√≠odo, ele fica na fila.  Ele tem o n√∫mero 3 no final do per√≠odo.  N√£o h√° n√∫meros fracion√°rios nos res√≠duos, porque duas CPUs executam duas tarefas, uma por um per√≠odo. <br><br>  Embora tenhamos tr√™s solicita√ß√µes em camadas, o grau de simultaneidade dentro do sistema = 2. A terceira est√° na fila e n√£o conta. <br><br><img src="https://habrastorage.org/webt/nm/x4/yw/nmx4ywd6xdqyte5b6vkwgco3hdq.png"><br><br>  O quarto veio - a mesma imagem, embora mais trabalho j√° tenha sido acumulado. <br><br><img src="https://habrastorage.org/webt/uq/jb/_5/uqjb_5b8whjwjzzetwqwrzjusfy.png"><br>  ... <br>  ... <br><br>  No sexto per√≠odo, a terceira solicita√ß√£o foi conclu√≠da com um terceiro atraso e o grau de simultaneidade j√° √© = 4. <br><br><img src="https://habrastorage.org/webt/xs/i1/sf/xsi1sf60jniqqbamvlko-bmd_xa.png"><br><br>  O grau de simultaneidade dobrou.  Ela n√£o pode mais crescer, porque estabelecemos uma proibi√ß√£o clara disso.  Com velocidade m√°xima, apenas os dois primeiros pedidos foram conclu√≠dos - aqueles que vieram primeiro ao clube, enquanto havia espa√ßo suficiente para todos. <br><br>  As solicita√ß√µes amarelas ficaram no sistema por mais tempo, mas permaneceram na fila e n√£o atrasaram o recurso da CPU.  Portanto, aqueles que estavam l√° dentro estavam se divertindo.  Isso poderia continuar at√© que um homem chegasse e dissesse que n√£o ficaria na fila, mas que voltaria para casa.  Este √© um pedido com falha: <br><br><img src="https://habrastorage.org/webt/tf/qf/qr/tfqfqrsfqvhxphy3wzrdqf_cpvk.png"><br><br>  A situa√ß√£o pode ser repetida sem parar, enquanto o tempo de execu√ß√£o da consulta permanece no mesmo n√≠vel - exatamente o dobro do tempo que gostar√≠amos. <br><br><img src="https://habrastorage.org/webt/xh/l2/8-/xhl28-uoe_jao9hjppy5zc30in8.png"><br><br>  Vemos que uma simples restri√ß√£o no n√≠vel de simultaneidade elimina o problema de viabilidade do servidor. <br><br><h4>  Como aumentar a viabilidade do servidor por meio do limite de n√≠vel de simultaneidade </h4><br>  Voc√™ pode escrever o "bouncer" mais simples.  Abaixo est√° o c√≥digo usando o sem√°foro.  N√£o h√° limite para o comprimento da linha externa.    ,    . <br><br><pre><code class="swift hljs">const int <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span> = <span class="hljs-number"><span class="hljs-number">100</span></span>; <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span> bulkhead = new <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span>(<span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>, <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> async <span class="hljs-type"><span class="hljs-type">Task</span></span> <span class="hljs-type"><span class="hljs-type">ProcessRequest</span></span>() { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!await bulkhead.<span class="hljs-type"><span class="hljs-type">WaitAsync</span></span>()) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> new <span class="hljs-type"><span class="hljs-type">OperationCanceledException</span></span>(); } <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { await <span class="hljs-type"><span class="hljs-type">ProcessRequestInternal</span></span>(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } finally { bulkhead.<span class="hljs-type"><span class="hljs-type">Release</span></span>(); } }</code> </pre> <br>  Para criar uma fila limitada, voc√™ precisa de dois sem√°foros.  Para isso, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">a biblioteca Polly</a> , recomendada pela Microsoft, √© adequada.  Preste aten√ß√£o ao padr√£o Antepara.  Traduzido literalmente como "anteparo" - um elemento estrutural que permite que o navio n√£o afunde.  Para ser sincero, acho que o termo seguran√ßa √© mais adequado.  O importante √© que esse padr√£o permita ao servidor sobreviver em situa√ß√µes sem esperan√ßa. <br><br>  Primeiro, esprememos tudo o que √© poss√≠vel no banco de carga do servidor at√© determinarmos quantas solicita√ß√µes ele pode conter.  Por exemplo, determinamos que √© 100. Colocamos anteparo. <br><br>  Al√©m disso, o servidor ignorar√° apenas o n√∫mero necess√°rio de solicita√ß√µes, o restante ficar√° na fila.  Seria sensato escolher um n√∫mero um pouco menor para que haja estoque.  N√£o tenho recomenda√ß√µes prontas sobre esse assunto, porque existe uma forte depend√™ncia do contexto e da situa√ß√£o espec√≠fica. <br><br><ol><li>  Se o comportamento do servidor depender de forma est√°vel da carga em termos de recursos, esse n√∫mero poder√° se aproximar do limite. </li><li>  Se o meio estiver sujeito a flutua√ß√µes de carga, um n√∫mero mais conservador deve ser escolhido, levando em considera√ß√£o o tamanho dessas flutua√ß√µes.  Tais flutua√ß√µes podem ocorrer por v√°rios motivos, por exemplo, o ambiente de desempenho com o GC √© caracterizado por pequenos picos de carga na CPU. </li><li>  Se o servidor executar tarefas peri√≥dicas em um agendamento, isso tamb√©m deve ser considerado.  Voc√™ pode at√© desenvolver um anteparo adapt√°vel que calcule quantas consultas podem ser enviadas simultaneamente sem degrada√ß√£o do servidor (mas isso j√° est√° al√©m do escopo deste estudo). </li></ol><br><h2>  Experi√™ncias de consulta </h2><br>  D√™ uma olhada neste post-mortem por √∫ltimo, n√£o veremos isso novamente. <br><img src="https://habrastorage.org/webt/n1/qn/1i/n1qn1irfrgm-fezifzonark7j94.png"><br>  Toda essa pilha cinzenta se correlaciona inequivocamente com a falha do servidor.  Gray √© a morte para o servidor.  Vamos cortar e ver o que acontece.  Parece que um certo n√∫mero de solicita√ß√µes vai para casa, simplesmente n√£o ser√° atendido.  Mas quanto? <br><br><h4>  100 dentro, 100 fora </h4><br><img src="https://habrastorage.org/webt/u0/6c/m5/u06cm5odrt-dxltnomhwatmhwze.png"><br>  Aconteceu que nosso servidor come√ßou a viver muito bem e divertido.  Ele constantemente ara na pot√™ncia m√°xima.  √â claro que, quando ocorre um pico, o chuta, mas n√£o por muito tempo. <br><br>  Inspirados pelo sucesso, tentaremos garantir que ele n√£o seja devolvido.  Vamos tentar aumentar o comprimento da fila. <br><br><h4>  100 dentro, 500 fora </h4><br><img src="https://habrastorage.org/webt/sk/5i/gi/sk5igi9cfgyjajp8lujawwunace.png"><br><br>  Melhorou, mas a cauda cresceu.  Essas s√£o as solicita√ß√µes que s√£o executadas por um longo tempo depois. <br><br><h4>  100 dentro, 1000 fora </h4><br>  Como algo se tornou melhor, vamos tentar lev√°-lo ao ponto do absurdo.  Vamos resolver o comprimento da fila 10 vezes mais do que podemos atender simultaneamente: <br><br><img src="https://habrastorage.org/webt/2g/qd/s6/2gqds68piszleawyid_yk_rtdhg.png"><br><br>  Se falamos da met√°fora do clube e dos seguran√ßas, essa situa√ß√£o √© quase imposs√≠vel - ningu√©m quer esperar mais na entrada do que passar um tempo no clube.  Tamb√©m n√£o vamos fingir que esta √© uma situa√ß√£o normal para o nosso sistema. <br><br>  √â melhor n√£o servir o cliente do que atorment√°-lo no site ou no aplicativo m√≥vel carregando cada tela por 30 segundos e estragando a reputa√ß√£o da empresa.  √â melhor dizer imediatamente a uma pequena parte dos clientes que agora n√£o podemos atend√™-los.  Caso contr√°rio, atenderemos a todos os clientes v√°rias vezes mais devagar, porque o gr√°fico mostra que a situa√ß√£o persiste por algum tempo. <br><br>  H√° mais um risco - outros componentes do sistema podem n√£o ser projetados para esse comportamento do servidor e, como j√° sabemos, a simultaneidade √© projetada nos clientes. <br><br>  Portanto, voltamos √† primeira op√ß√£o "100 por 100" e pensamos em como dimensionar nossas capacidades. <br><br><h4>  Vencedor - 100 dentro, 100 fora </h4><br><img src="https://habrastorage.org/webt/z5/cg/pv/z5cgpvn4qs9abj5ifewfelapa6g.png"><br><br>  ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  Com esses par√¢metros, a maior degrada√ß√£o no tempo de execu√ß√£o √© exatamente 2 vezes a "nominal".  Ao mesmo tempo, √© 100% de degrada√ß√£o no tempo de execu√ß√£o da consulta. <br><br>  Se o seu cliente √© sens√≠vel ao tempo de execu√ß√£o (e isso geralmente acontece tanto com clientes humanos quanto com servidores), voc√™ pode pensar em reduzir ainda mais o comprimento da fila.  Nesse caso, podemos obter uma porcentagem da simultaneidade interna e teremos certeza de que o servi√ßo n√£o se degradar√° no tempo de resposta em mais do que essa porcentagem, em m√©dia. <br><br>  Na verdade, n√£o estamos tentando criar uma fila, estamos tentando nos proteger de flutua√ß√µes de carga.  Aqui, assim como no caso de determinar o primeiro par√¢metro da antepara (quantidade interna), √© √∫til determinar quais flutua√ß√µes na carga o cliente pode causar.  Portanto, saberemos em que casos, grosso modo, perderemos o lucro de um servi√ßo em potencial. <br><br>  √â ainda mais importante determinar quais flutua√ß√µes da lat√™ncia podem suportar outros componentes do sistema que interagem com o servidor.  Portanto, saberemos que estamos realmente tirando o m√°ximo proveito do sistema existente sem o risco de perder completamente o servi√ßo. <br><br><h2>  Diagn√≥stico e tratamento </h2><br>  Estamos tratando simultaneidade n√£o controlada com isolamento de anteparo. <br>  Esse m√©todo, como os outros discutidos nesta s√©rie de artigos, √© convenientemente implementado pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">biblioteca Polly</a> . <br><br>  A vantagem do m√©todo √© que ser√° extremamente dif√≠cil desestabilizar um componente individual do sistema como tal.  O sistema adquire um comportamento muito previs√≠vel em termos de tempo para solicita√ß√µes bem-sucedidas e chances muito maiores de solicita√ß√µes completas bem-sucedidas. <br><br>  No entanto, n√£o resolvemos todos os problemas.  Por exemplo, o problema da energia insuficiente do servidor.  Nessa situa√ß√£o, voc√™ deve obviamente decidir ‚Äúsoltar o lastro‚Äù no caso de um salto na carga, que consideramos excessivo. <br><br>  Outras medidas que nosso estudo n√£o aborda podem incluir, por exemplo, escala din√¢mica. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt461081/">https://habr.com/ru/post/pt461081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt461071/index.html">Otimiza√ß√£o de consultas ao banco de dados no exemplo de servi√ßo B2B para construtores</a></li>
<li><a href="../pt461073/index.html">Conectamos mapas on-line ao navegador no smartphone. Parte 3 - OverpassTurbo</a></li>
<li><a href="../pt461075/index.html">Intelig√™ncia de neg√≥cios. Objetos de TI, componentes, ferramentas</a></li>
<li><a href="../pt461077/index.html">Como s√£o cozidos os pentesters? Teste de entrada para estagi√°rios de seguran√ßa digital</a></li>
<li><a href="../pt461079/index.html">Cidade sem engarrafamentos</a></li>
<li><a href="../pt461083/index.html">Software de grava√ß√£o com a funcionalidade dos utilit√°rios cliente-servidor Windows, parte 02</a></li>
<li><a href="../pt461085/index.html">Alternar idioma no aplicativo Android</a></li>
<li><a href="../pt461087/index.html">Gerando masmorras e cavernas para o meu jogo</a></li>
<li><a href="../pt461091/index.html">L√¢mpadas LED Camelion</a></li>
<li><a href="../pt461093/index.html">Not√≠cias do mundo do OpenStreetMap n¬∫ 469 (09/07/2019 - 07/07/2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>