<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø üßóüèæ ‚úåüèø Neuronale Netze und tiefes Lernen, Kapitel 3, Teil 2: Warum tr√§gt Regularisierung dazu bei, die Umschulung zu reduzieren? üëØ üí™üèø üë¶üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inhalt 

- Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen 
- Kapitel 2: Funktionsweise des Backpropagation-Algorithmus 
-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und tiefes Lernen, Kapitel 3, Teil 2: Warum tr√§gt Regularisierung dazu bei, die Umschulung zu reduzieren?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  Empirisch haben wir gesehen, dass Regularisierung dazu beitr√§gt, die Umschulung zu reduzieren.  Das ist inspirierend - aber leider ist nicht klar, warum Regularisierung hilft.  Normalerweise erkl√§ren die Leute es auf irgendeine Weise: In gewissem Sinne sind kleinere Gewichte weniger komplex, was eine einfachere und effizientere Erkl√§rung der Daten erm√∂glicht, weshalb sie bevorzugt werden sollten.  Dies ist jedoch eine zu kurze Erkl√§rung, und einige Teile davon m√∂gen zweifelhaft oder mysteri√∂s erscheinen.  Lassen Sie uns diese Geschichte entfalten und mit kritischem Blick untersuchen.  Nehmen wir dazu an, wir haben einen einfachen Datensatz, f√ºr den wir ein Modell erstellen m√∂chten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br>  In Bezug auf die Bedeutung untersuchen wir hier das Ph√§nomen der realen Welt, und x und y bezeichnen reale Daten.  Unser Ziel ist es, ein Modell zu erstellen, mit dem wir y als Funktion von x vorhersagen k√∂nnen.  Wir k√∂nnten versuchen, ein neuronales Netzwerk zu verwenden, um ein solches Modell zu erstellen, aber ich schlage etwas Einfacheres vor: Ich werde versuchen, y als Polynom in x zu modellieren.  Ich werde dies anstelle von neuronalen Netzen tun, da die Verwendung von Polynomen die Erkl√§rung besonders deutlich macht.  Sobald wir uns mit dem Fall des Polynoms befassen, werden wir zur Nationalversammlung √ºbergehen.  In der obigen Grafik gibt es zehn Punkte, was bedeutet, dass wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ein eindeutiges Polynom</a> 9. Ordnung y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... + a <sub>9 finden k√∂nnen</sub> , das genau zu den Daten passt.  Und hier ist der Graph dieses Polynoms. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br>  Perfekter Hit.  Mit dem linearen Modell y = 2x k√∂nnen wir jedoch eine gute Ann√§herung erhalten <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br>  Welches ist besser?  Welches ist wahrscheinlicher wahr?  Was l√§sst sich besser auf andere Beispiele des gleichen Ph√§nomens der realen Welt verallgemeinern? <br><br>  Schwierige Fragen.  Und sie k√∂nnen ohne zus√§tzliche Informationen √ºber das zugrunde liegende Ph√§nomen der realen Welt nicht genau beantwortet werden.  Schauen wir uns jedoch zwei M√∂glichkeiten an: (1) Ein Modell mit einem Polynom 9. Ordnung beschreibt das Ph√§nomen der realen Welt wirklich und verallgemeinert sich daher perfekt;  (2) Das richtige Modell ist y = 2x, aber mit dem Messfehler ist zus√§tzliches Rauschen verbunden, sodass das Modell nicht perfekt passt. <br><br>  A priori kann man nicht sagen, welche der beiden M√∂glichkeiten richtig ist (oder dass es keine dritte gibt).  Logischerweise kann sich jeder von ihnen als wahr herausstellen.  Und der Unterschied zwischen ihnen ist nicht trivial.  Ja, basierend auf den verf√ºgbaren Daten kann gesagt werden, dass es nur einen geringen Unterschied zwischen den Modellen gibt.  Angenommen, wir m√∂chten den Wert von y vorhersagen, der einem gro√üen Wert von x entspricht, der viel gr√∂√üer ist als der in der Grafik gezeigte.  Wenn wir dies versuchen, wird ein gro√üer Unterschied zwischen den Vorhersagen der beiden Modelle auftreten, da der Term x <sup>9</sup> im Polynom 9. Ordnung dominiert und das lineare Modell linear bleibt. <br><br>  Ein Gesichtspunkt in Bezug auf das Geschehen ist die Feststellung, dass eine einfachere Erkl√§rung in der Wissenschaft verwendet werden sollte, wenn dies m√∂glich ist.  Wenn wir ein einfaches Modell finden, das viele Bezugspunkte erkl√§rt, wollen wir nur rufen: "Eureka!"  Schlie√ülich ist es unwahrscheinlich, dass eine einfache Erkl√§rung rein zuf√§llig erscheint.  Wir vermuten, dass das Modell eine mit dem Ph√§nomen verbundene Wahrheit hervorbringen sollte.  In diesem Fall scheint das Modell y = 2x + Rauschen viel einfacher zu sein als y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... Es w√§re √ºberraschend, wenn die Einfachheit zuf√§llig entstanden w√§re, sodass wir vermuten, dass y = 2x + Rauschen etwas ausdr√ºckt zugrunde liegende Wahrheit.  Unter diesem Gesichtspunkt untersucht das Modell 9. Ordnung lediglich die Wirkung von lokalem Rauschen.  Obwohl das Modell 9. Ordnung f√ºr diese spezifischen Referenzpunkte perfekt funktioniert, kann es nicht auf andere Punkte verallgemeinert werden, wodurch das lineare Modell mit Rauschen bessere Vorhersagef√§higkeiten aufweist. <br><br>  Mal sehen, was dieser Standpunkt f√ºr neuronale Netze bedeutet.  Angenommen, in unserem Netzwerk gibt es haupts√§chlich geringe Gewichte, wie dies normalerweise in regulierten Netzwerken der Fall ist.  Aufgrund seines geringen Gewichts √§ndert sich das Netzwerkverhalten nicht wesentlich, wenn hier und da mehrere zuf√§llige Eingaben ge√§ndert werden.  Infolgedessen ist es f√ºr das regulierte Netzwerk schwierig, die Auswirkungen des in den Daten vorhandenen lokalen Rauschens zu lernen.  Dies √§hnelt dem Wunsch, sicherzustellen, dass einzelne Beweise die Leistung des gesamten Netzwerks nicht stark beeinflussen.  Stattdessen wird das regulierte Netzwerk geschult, um auf Beweise zu reagieren, die h√§ufig in Trainingsdaten enthalten sind.  Umgekehrt kann ein Netzwerk mit gro√üen Gewichten sein Verhalten als Reaktion auf kleine √Ñnderungen der Eingabedaten ziemlich stark √§ndern.  Daher kann ein unregelm√§√üiges Netzwerk gro√üe Gewichte verwenden, um ein komplexes Modell zu trainieren, das viele Rauschinformationen in Trainingsdaten enth√§lt.  Kurz gesagt, die Einschr√§nkungen regulierter Netzwerke erm√∂glichen es ihnen, relativ einfache Modelle auf der Grundlage von Mustern zu erstellen, die h√§ufig in Trainingsdaten zu finden sind, und sie sind resistent gegen Abweichungen, die durch Rauschen in Trainingsdaten verursacht werden.  Es besteht die Hoffnung, dass unsere Netzwerke dadurch das Ph√§nomen selbst untersuchen und das gewonnene Wissen besser verallgemeinern k√∂nnen. <br><br>  Nach alledem sollte die Idee, einfacheren Erkl√§rungen den Vorzug zu geben, Sie nerv√∂s machen.  Manchmal nennen die Leute diese Idee ‚ÄûOccams Rasiermesser‚Äú und wenden sie eifrig an, als h√§tte sie den Status eines allgemeinen wissenschaftlichen Prinzips.  Dies ist nat√ºrlich kein allgemeines wissenschaftliches Prinzip.  Es gibt keinen a priori logischen Grund, einfache Erkl√§rungen komplexen vorzuziehen.  Manchmal ist eine kompliziertere Erkl√§rung richtig. <br><br>  Lassen Sie mich zwei Beispiele beschreiben, wie sich eine komplexere Erkl√§rung als richtig herausstellte.  In den 1940er Jahren k√ºndigte der Physiker Marcel Shane die Entdeckung eines neuen Teilchens an.  Das Unternehmen, f√ºr das er arbeitete, General Electric, war begeistert und verbreitete die Ver√∂ffentlichung dieser Veranstaltung in gro√üem Umfang.  Der Physiker Hans Bethe war jedoch skeptisch.  Bethe besuchte Shane und studierte die Platten mit Spuren von Shane's neuem Partikel.  Shane zeigte Beta Platte f√ºr Platte, aber Bete fand bei jedem von ihnen ein Problem, das darauf hinwies, dass diese Daten abgelehnt werden mussten.  Schlie√ülich zeigte Shane Beta einen Rekord, der fit aussah.  Bethe sagte, es sei wahrscheinlich nur eine statistische Abweichung.  Shane: Ja, aber die Wahrscheinlichkeit, dass dies auf Statistiken zur√ºckzuf√ºhren ist, selbst nach Ihrer eigenen Formel, ist eins zu f√ºnf.  Bethe: "Ich habe mir aber schon f√ºnf Platten angesehen."  Schlie√ülich sagte Shane: "Aber Sie haben jede meiner Aufzeichnungen, jedes gute Bild mit einer anderen Theorie erkl√§rt, und ich habe eine Hypothese, die alle Aufzeichnungen auf einmal erkl√§rt, woraus folgt, dass wir √ºber ein neues Teilchen sprechen."  Bethe antwortete: ‚ÄûDer einzige Unterschied zwischen meinen und Ihren Erkl√§rungen besteht darin, dass Ihre falsch und meine richtig sind.  Deine einzige Erkl√§rung ist falsch und alle meine Erkl√§rungen sind richtig. ‚Äú  Anschlie√üend stellte sich heraus, dass die Natur mit Bethe √ºbereinstimmte und Shane's Partikel verdampften. <br><br>  Im zweiten Beispiel entdeckte der Astronom Urbain Jean Joseph Le Verrier 1859, dass die Form der Merkurbahn nicht Newtons Theorie der universellen Gravitation entspricht.  Es gab eine winzige Abweichung von dieser Theorie, und dann wurden mehrere Optionen zur L√∂sung des Problems vorgeschlagen, die darauf hinausliefen, dass Newtons Theorie insgesamt korrekt ist und nur eine geringf√ºgige √Ñnderung erfordert.  Und 1916 zeigte Einstein, dass diese Abweichung mit seiner allgemeinen Relativit√§tstheorie, die sich radikal von der Newtonschen Schwerkraft unterscheidet und auf einer viel komplexeren Mathematik basiert, gut erkl√§rt werden kann.  Trotz dieser zus√§tzlichen Komplexit√§t wird heute allgemein angenommen, dass Einsteins Erkl√§rung richtig ist und die Newtonsche Schwerkraft <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">selbst in modifizierter Form</a> falsch ist.  Dies geschieht insbesondere, weil wir heute wissen, dass Einsteins Theorie viele andere Ph√§nomene erkl√§rt, mit denen Newtons Theorie Schwierigkeiten hatte.  Noch erstaunlicher ist, dass Einsteins Theorie einige Ph√§nomene genau vorhersagt, die die Newtonsche Schwerkraft √ºberhaupt nicht vorhergesagt hat.  Diese beeindruckenden Eigenschaften waren jedoch in der Vergangenheit nicht offensichtlich.  Auf der Grundlage der Einfachheit zu urteilen, h√§tte eine modifizierte Form der Newtonschen Theorie attraktiver ausgesehen. <br><br>  Aus diesen Geschichten lassen sich drei Moralit√§ten ableiten.  Erstens ist es manchmal ziemlich schwierig zu entscheiden, welche der beiden Erkl√§rungen ‚Äûeinfacher‚Äú sein wird.  Zweitens muss die Einfachheit, selbst wenn wir eine solche Entscheidung getroffen haben, √§u√üerst sorgf√§ltig gef√ºhrt werden!  Drittens ist der wahre Test des Modells nicht die Einfachheit, sondern wie gut es neue Ph√§nomene unter neuen Verhaltensbedingungen vorhersagt. <br><br>  In Anbetracht all dessen und Vorsicht werden wir eine empirische Tatsache akzeptieren - regulierte NS sind normalerweise besser verallgemeinert als irregul√§re.  Daher werden wir sp√§ter in diesem Buch h√§ufig die Regularisierung verwenden.  Die erw√§hnten Geschichten werden nur ben√∂tigt, um zu erkl√§ren, warum noch niemand eine v√∂llig √ºberzeugende theoretische Erkl√§rung daf√ºr entwickelt hat, warum Regularisierung Netzwerken bei der Verallgemeinerung hilft.  Die Forscher ver√∂ffentlichen weiterhin Arbeiten, in denen sie versuchen, verschiedene Ans√§tze zur Regularisierung auszuprobieren, sie zu vergleichen, herauszufinden, was am besten funktioniert, und zu verstehen, warum verschiedene Ans√§tze schlechter oder besser funktionieren.  Regularisierung kann also wie eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wolke</a> behandelt werden.  Wenn es ziemlich oft hilft, haben wir kein v√∂llig zufriedenstellendes systemisches Verst√§ndnis dessen, was passiert - nur unvollst√§ndige heuristische und praktische Regeln. <br><br>  Hier liegt eine tiefere Reihe von Problemen, die das Herz der Wissenschaft betreffen.  Dies ist ein Verallgemeinerungsproblem.  Durch die Regularisierung erhalten wir einen rechnergest√ºtzten Zauberstab, mit dem unsere Netzwerke Daten besser verallgemeinern k√∂nnen. Sie vermittelt jedoch kein grundlegendes Verst√§ndnis daf√ºr, wie die Generalisierung funktioniert und wie sie am besten angegangen werden kann. <br><br>  Diese Probleme gehen auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Problem der Induktion zur√ºck</a> , dessen bekannte Interpretation vom schottischen Philosophen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David Hume</a> in dem Buch " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">A Study on Human Cognition</a> " (1748) durchgef√ºhrt wurde.  Das Induktionsproblem ist Gegenstand des " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Theorems √ºber das Fehlen freier Mahlzeiten</a> " von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David Walpert und William Macredie</a> (1977). <br><br>  Und das ist besonders √§rgerlich, weil Menschen im normalen Leben ph√§nomenal gut in der Lage sind, Daten zu verallgemeinern.  Zeigen Sie dem Kind einige Bilder des Elefanten, und es wird schnell lernen, andere Elefanten zu erkennen.  Nat√ºrlich kann er manchmal einen Fehler machen, zum Beispiel ein Nashorn mit einem Elefanten verwechseln, aber im Allgemeinen funktioniert dieser Prozess √ºberraschend genau.  Jetzt haben wir ein System - das menschliche Gehirn - mit einer gro√üen Menge freier Parameter.  Und nachdem ihm ein oder mehrere Trainingsbilder gezeigt wurden, lernt das System, sie auf andere Bilder zu verallgemeinern.  In gewisser Weise kann unser Gehirn erstaunlich gut regulieren!  Aber wie machen wir das?  Dies ist uns derzeit nicht bekannt.  Ich denke, dass wir in Zukunft leistungsf√§higere Regularisierungstechnologien in k√ºnstlichen neuronalen Netzen entwickeln werden, Techniken, die es der Nationalversammlung letztendlich erm√∂glichen, Daten auf der Grundlage noch kleinerer Datens√§tze zu verallgemeinern. <br><br>  Tats√§chlich verallgemeinern sich unsere Netzwerke bereits viel besser als a priori zu erwarten w√§re.  Ein Netzwerk mit 100 versteckten Neuronen hat fast 80.000 Parameter.  Wir haben nur 50.000 Bilder in Trainingsdaten.  Dies ist dasselbe wie der Versuch, ein Polynom von 80.000 Ordnung √ºber 50.000 Referenzpunkte zu strecken.  Nach allen Angaben muss unser Netzwerk furchtbar umgeschult werden.  Und doch, wie wir gesehen haben, verallgemeinert sich ein solches Netzwerk tats√§chlich ziemlich gut.  Warum passiert das?  Dies ist nicht ganz klar.  Es wurde die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hypothese aufgestellt,</a> dass "die Dynamik des Lernens durch Gradientenabstieg in mehrschichtigen Netzwerken der Selbstregulierung unterliegt".  Dies ist ein extremes Verm√∂gen, aber auch eine ziemlich beunruhigende Tatsache, da wir nicht verstehen, warum dies geschieht.  In der Zwischenzeit werden wir einen pragmatischen Ansatz verfolgen und, wo immer m√∂glich, die Regularisierung anwenden.  Dies wird f√ºr unsere Nationalversammlung von Vorteil sein. <br><br>  Lassen Sie mich diesen Abschnitt beenden, indem ich zu dem zur√ºckkehre, was ich zuvor nicht erkl√§rt habe: Die Regularisierung von L2 begrenzt die Verschiebungen nicht.  Nat√ºrlich w√§re es einfach, das Regularisierungsverfahren so zu √§ndern, dass Verschiebungen reguliert werden.  Empirisch √§ndert dies jedoch h√§ufig nichts an den Ergebnissen, weshalb es in gewissem Ma√üe eine Frage der √úbereinstimmung ist, sich mit der Regularisierung von Verzerrungen zu befassen oder nicht.  Es ist jedoch erw√§hnenswert, dass eine gro√üe Verschiebung ein Neuron nicht f√ºr Eingaben wie gro√üe Gewichte empfindlich macht.  Daher m√ºssen wir uns keine Gedanken √ºber gro√üe Offsets machen, die es unseren Netzwerken erm√∂glichen, das Rauschen in den Trainingsdaten zu lernen.  Gleichzeitig machen wir unsere Netzwerke flexibler in ihrem Verhalten, indem wir gro√üe Verschiebungen zulassen - insbesondere erleichtern gro√üe Verschiebungen die S√§ttigung von Neuronen, die wir m√∂chten.  Aus diesem Grund ber√ºcksichtigen wir normalerweise keine Offsets in der Regularisierung. <br><br><h2>  Andere Regularisierungstechniken </h2><br>  Neben L2 gibt es viele Regularisierungstechniken.  Tats√§chlich wurden bereits so viele Techniken entwickelt, dass ich bei allem Wunsch nicht alle kurz beschreiben konnte.  In diesem Abschnitt werde ich kurz drei weitere Ans√§tze zur Reduzierung der Umschulung beschreiben: Regularisierung von L1, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abbruch</a> und k√ºnstliche Erh√∂hung des Trainingssatzes.  Wir werden sie nicht so gr√ºndlich studieren wie die vorherigen Themen.  Stattdessen lernen wir sie nur kennen und sch√§tzen gleichzeitig die Vielfalt der vorhandenen Regularisierungstechniken. <br><br><h3>  Regularisierung L1 </h3><br>  Bei diesem Ansatz modifizieren wir die unregelm√§√üige Kostenfunktion, indem wir die Summe der absoluten Werte der Gewichte addieren: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>95</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="40.447ex" height="2.66ex" viewBox="0 -832 17414.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2B" x="3486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="4736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="5287" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="5738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="6268" y="0"></use><g transform="translate(6701,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="10140" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="10991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-75" x="11460" y="0"></use><g transform="translate(12033,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-7C" x="13518" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="13797" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-7C" x="14513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="15042" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="15403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="15933" y="0"></use><g transform="translate(16413,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-35" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>95</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \ frac {\ lambda} {n} \ sum_w | w | \ tag {95} </script></p><br><br>  Intuitiv √§hnelt dies der Regularisierung von L2, die f√ºr gro√üe Gewichte Geldstrafen berechnet und das Netzwerk dazu veranlasst, niedrige Gewichte zu bevorzugen.  Nat√ºrlich ist der Regularisierungsterm L1 nicht wie der Regularisierungsterm L2, daher sollten Sie nicht genau dasselbe Verhalten erwarten.  Versuchen wir zu verstehen, wie sich das Verhalten eines mit Regularisierung L1 trainierten Netzwerks von einem mit Regularisierung L2 trainierten Netzwerk unterscheidet. <br><br>  Schauen Sie sich dazu die partiellen Ableitungen der Kostenfunktion an.  Durch Differenzieren (95) erhalten wir: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>C</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>w</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>w</mi></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>96</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="97.046ex" height="2.66ex" viewBox="0 -832 41783.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="3971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="4441" y="0"></use></g><g transform="translate(7416,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="3971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="4441" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-3D" x="12851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="14158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="14708" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="15160" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="15689" y="0"></use><g transform="translate(16123,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="3971" y="0"></use><g transform="translate(4441,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(21733,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="3971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="4441" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2B" x="27113" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="28363" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="28914" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="29365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="29895" y="0"></use><g transform="translate(30328,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="33767" y="0"></use><g transform="translate(34535,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="701" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="1580" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="2049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="2530" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-28" x="37665" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="38055" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-29" x="38771" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="39411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="39772" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="40302" y="0"></use><g transform="translate(40782,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-36" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>C</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>w</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>s</mi><mi>w</mi></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>96</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \ frac {\ partielles C} {\ partielles w} = \ frac {\ partielles C_0} {\ partielles w} + \ frac {\ lambda} {n} \, {\ rm sgn} (w) \ tag {96 } </script></p><br><br>  wobei sgn (w) das Vorzeichen von w ist, dh +1, wenn w positiv ist, und -1, wenn w negativ ist.  Mit diesem Ausdruck modifizieren wir die R√ºckausbreitung geringf√ºgig, so dass ein stochastischer Gradientenabstieg unter Verwendung der Regularisierung L1 durchgef√ºhrt wird.  Die endg√ºltige Aktualisierungsregel f√ºr das L1-regulierte Netzwerk: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>97</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="95.51ex" height="2.78ex" viewBox="0 -883.9 41122.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2212" x="9100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="10901" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="11353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="11882" y="0"></use><g transform="translate(12316,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="17362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="18213" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-62" x="19091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6F" x="19521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-78" x="20006" y="0"></use><g transform="translate(20579,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="950" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-28" x="22129" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="22519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-29" x="23235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2212" x="23847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="25098" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="25564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="25926" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="26705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="27256" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="27707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="28237" y="0"></use><g transform="translate(28670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><g transform="translate(3971,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33811,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="3971" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="38749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="39111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="39640" y="0"></use><g transform="translate(40121,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-37" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>97</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \ rightarrow w '= w- \ frac {\ eta \ lambda} {n} \ mbox {sgn} (w) - \ eta \ frac {\ partielle C_0} {\ partielle w} \ tag {97} </script></p><br><br>  wobei wie √ºblich ‚àÇC / ‚àÇw optional unter Verwendung des Durchschnittswerts des Minipakets gesch√§tzt werden kann.  Vergleichen Sie dies mit der Regularisierungsaktualisierungsregel L2 (93): <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>98</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="95.533ex" height="2.78ex" viewBox="0 -883.9 41132.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="9128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="9426" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="9893" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="10443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-28" x="10805" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-31" x="11194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2212" x="11917" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="13168" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="13718" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="14170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="14699" y="0"></use><g transform="translate(15133,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6E" x="20179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="21030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="21481" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="21827" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-68" x="22307" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="22884" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-29" x="23245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2212" x="23857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="25108" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="25574" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="25936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-66" x="26715" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="27266" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="27717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-63" x="28247" y="0"></use><g transform="translate(28680,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><g transform="translate(3971,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33821,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="2908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="3206" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="3505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-77" x="3971" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="38759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="39121" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="39650" y="0"></use><g transform="translate(40131,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>w</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>98</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \ rightarrow w '= w \ left (1 - \ frac {\ eta \ lambda} {n} \ right) - \ eta \ frac {\ partielle C_0} {\ partielle w} \ tag {98} </script></p><br><br>  In beiden Ausdr√ºcken bewirkt die Regularisierung eine Gewichtsreduzierung.  Dies stimmt mit der intuitiven Vorstellung √ºberein, dass beide Arten der Regularisierung gro√üe Gewichte benachteiligen.  Gewichte werden jedoch auf unterschiedliche Weise reduziert.  Bei der Regularisierung von L1 nehmen die Gewichte um einen konstanten Wert ab, der gegen 0 tendiert. Bei der Regularisierung von L2 nehmen die Gewichte um einen Wert proportional zu w ab.  Wenn daher ein gewisses Gewicht einen gro√üen Wert | w | hat, verringert die Regularisierung von L1 das Gewicht nicht so sehr wie L2.  Und umgekehrt, wenn | w |  klein, Regularisierung von L1 reduziert das Gewicht viel mehr als Regularisierung von L2.  Infolgedessen tendiert die Regularisierung von L1 dazu, die Netzwerkgewichte auf eine relativ kleine Anzahl von Bindungen von hoher Bedeutung zu konzentrieren, w√§hrend andere Gewichte gegen Null tendieren. <br><br>  Ich habe ein Problem in der vorherigen Diskussion leicht gegl√§ttet - die partielle Ableitung ‚àÇC / ‚àÇw ist nicht definiert, wenn w = 0 ist.  Dies liegt daran, dass die Funktion | w |  es gibt einen akuten "Knick" am Punkt w = 0, daher kann er dort nicht unterschieden werden.  Das ist aber nicht be√§ngstigend.  Wir wenden nur die √ºbliche unregelm√§√üige Regel f√ºr den stochastischen Gradientenabstieg an, wenn w = 0 ist.  Intuitiv ist daran nichts auszusetzen - Regularisierung sollte Gewichte reduzieren, und offensichtlich kann sie Gewichte, die bereits gleich 0 sind, nicht reduzieren. Genauer gesagt werden wir die Gleichungen (96) und (97) mit der Bedingung verwenden, dass sgn (0) = 0.  Dies gibt uns eine bequeme und kompakte Regel f√ºr den stochastischen Gradientenabstieg mit der Regularisierung L1. <br><br><h3>  Ausnahme [Aussetzer] </h3><br>  Eine Ausnahme bildet eine v√∂llig andere Regularisierungstechnik.  Im Gegensatz zur Regularisierung von L1 und L2 betrifft die Ausnahme keine √Ñnderung der Kostenfunktion.  Stattdessen √§ndern wir das Netzwerk selbst.  Lassen Sie mich die grundlegenden Mechanismen der Funktionsweise einer Ausnahme erl√§utern, bevor ich mich mit dem Thema befasse, warum sie funktioniert und mit welchen Ergebnissen. <br><br>  Angenommen, wir versuchen, ein Netzwerk zu trainieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br>  Nehmen wir insbesondere an, wir haben die Trainingseingabe x und die entsprechende gew√ºnschte Ausgabe y.  Normalerweise trainieren wir es, indem wir x direkt √ºber das Netzwerk verteilen und uns dann zur√ºck ausbreiten, um den Beitrag des Gradienten zu bestimmen.  Eine Ausnahme √§ndert diesen Prozess.  Wir beginnen damit, zuf√§llig und vor√ºbergehend die H√§lfte der versteckten Neuronen im Netzwerk zu entfernen, wobei die Eingabe- und Ausgabe-Neuronen unver√§ndert bleiben.  Danach werden wir ungef√§hr ein solches Netzwerk haben.  Beachten Sie, dass ausgeschlossene Neuronen, die vor√ºbergehend entfernt werden, weiterhin im Diagramm markiert sind: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br>  Wir √ºbergeben x durch direkte Verteilung √ºber das ge√§nderte Netzwerk und verteilen das Ergebnis dann auch √ºber das ge√§nderte Netzwerk zur√ºck.  Nachdem wir dies mit einem Mini-Paket von Beispielen getan haben, aktualisieren wir die entsprechenden Gewichte und Offsets.  Dann wiederholen wir diesen Vorgang, indem wir zuerst die ausgeschlossenen Neuronen wiederherstellen, dann eine neue zuf√§llige Teilmenge versteckter Neuronen zum Entfernen ausw√§hlen, den Gradienten f√ºr ein anderes Minipaket auswerten und die Netzwerkgewichte und -vers√§tze aktualisieren. <br><br>  Wenn wir diesen Vorgang immer wieder wiederholen, erhalten wir ein Netzwerk, das einige Gewichte und Verschiebungen gelernt hat.  Nat√ºrlich wurden diese Gewichte und Verschiebungen unter Bedingungen gelernt, bei denen die H√§lfte der verborgenen Neuronen ausgeschlossen war.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und wenn wir das Netzwerk vollst√§ndig starten, werden wir doppelt so viele aktive versteckte Neuronen haben. </font><font style="vertical-align: inherit;">Um dies zu kompensieren, halbieren wir die Gewichte, die von versteckten Neuronen kommen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das Ausschlussverfahren mag seltsam und willk√ºrlich erscheinen. Warum sollte sie bei der Regularisierung helfen? Um zu erkl√§ren, was passiert, m√∂chte ich, dass Sie die Ausnahme f√ºr eine Weile vergessen und die Ausbildung der Nationalversammlung auf √ºbliche Weise pr√§sentieren. Stellen Sie sich insbesondere vor, wir trainieren mehrere verschiedene NS mit denselben Trainingsdaten. Nat√ºrlich k√∂nnen Netzwerke zun√§chst variieren, und manchmal kann Training zu unterschiedlichen Ergebnissen f√ºhren. In solchen F√§llen k√∂nnten wir eine Art Mittelungs- oder Abstimmungsschema anwenden, um zu entscheiden, welche der Ergebnisse akzeptiert werden sollen. Wenn wir beispielsweise f√ºnf Netzwerke trainiert haben und drei von ihnen die Zahl als ‚Äû3‚Äú klassifizieren, ist dies wahrscheinlich die wahre Drei. Und die anderen beiden Netzwerke sind wahrscheinlich einfach falsch. Ein solches Mittelungsschema ist oft ein n√ºtzlicher (wenn auch teurer) Weg, um die Umschulung zu reduzieren. Der Grund istdass verschiedene Netzwerke auf unterschiedliche Weise umgeschult werden k√∂nnen und die Mittelwertbildung dazu beitragen kann, eine solche Umschulung zu vermeiden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie h√§ngt das alles mit Ausnahmen zusammen? Heuristisch gesehen ist es so, als w√ºrden wir verschiedene NS trainieren, wenn wir verschiedene Neutronens√§tze ausschlie√üen. Daher √§hnelt das Ausschlussverfahren den Mittelungseffekten √ºber eine sehr gro√üe Anzahl verschiedener Netzwerke. Verschiedene Netzwerke werden auf unterschiedliche Weise umgeschult, so dass gehofft wird, dass der durchschnittliche Effekt des Ausschlusses die Umschulung verringert. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine verwandte heuristische Erkl√§rung der Vorteile des Ausschlusses findet sich in </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">einem der fr√ºhesten Werke</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit dieser Technik: ‚ÄûDiese Technik reduziert die komplexe Gelenkanpassung von Neuronen, da sich das Neuron nicht auf die Anwesenheit bestimmter Nachbarn verlassen kann. Am Ende muss er zuverl√§ssigere Eigenschaften lernen, die bei der Zusammenarbeit mit vielen verschiedenen zuf√§lligen Untergruppen von Neuronen n√ºtzlich sein k√∂nnen. ‚Äú Mit anderen Worten, wenn wir uns unsere Nationalversammlung als ein Modell vorstellen, das Vorhersagen macht, wird eine Ausnahme eine M√∂glichkeit sein, die Stabilit√§t des Modells gegen√ºber dem Verlust einzelner Beweismittelteile zu gew√§hrleisten. In diesem Sinne √§hnelt die Technik den Regularisierungen von L1 und L2, die darauf abzielen, die Gewichte zu reduzieren und auf diese Weise das Netzwerk widerstandsf√§higer gegen den Verlust einzelner Verbindungen im Netzwerk zu machen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das wahre Ma√ü f√ºr die N√ºtzlichkeit des Ausschlusses ist nat√ºrlich sein enormer Erfolg bei der Verbesserung der Effizienz neuronaler Netze. In der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Originalarbeit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wo diese Methode eingef√ºhrt wurde, wurde sie auf viele verschiedene Aufgaben angewendet. </font><font style="vertical-align: inherit;">Wir sind besonders daran interessiert, dass die Autoren die Ausnahme auf die Klassifizierung von Zahlen aus MNIST angewendet haben, indem sie ein einfaches direktes Vertriebsnetz verwendet haben, das dem von uns untersuchten √§hnlich ist. </font><font style="vertical-align: inherit;">Das Papier stellt fest, dass bis dahin das beste Ergebnis f√ºr eine solche Architektur eine Genauigkeit von 98,4% war. </font><font style="vertical-align: inherit;">Sie verbesserten es auf 98,7% unter Verwendung einer Kombination aus Ausschluss und einer modifizierten Form der Regularisierung L2. </font><font style="vertical-align: inherit;">Ebenso beeindruckende Ergebnisse wurden f√ºr viele andere Aufgaben erzielt, einschlie√ülich Muster- und Spracherkennung sowie Verarbeitung nat√ºrlicher Sprache. </font><font style="vertical-align: inherit;">Die Ausnahme war besonders n√ºtzlich beim Training gro√üer tiefer Netzwerke, bei denen h√§ufig das Problem der Umschulung auftritt.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Trainingsdatensatz k√ºnstlich erweitern </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben zuvor gesehen, dass unsere MNIST-Klassifizierungsgenauigkeit auf 80 Prozent gesunken ist, als wir nur 1.000 Trainingsbilder verwendet haben. Kein Wunder, dass unser Netzwerk mit weniger Daten weniger M√∂glichkeiten zum Schreiben von Zahlen durch Personen bietet. Versuchen wir, unser Netzwerk aus 30 versteckten Neuronen zu trainieren, indem wir verschiedene Volumina des Trainingssatzes verwenden, um die √Ñnderung der Effizienz zu untersuchen. Wir trainieren mit der Minipaketgr√∂√üe 10, der Lerngeschwindigkeit Œ∑ = 0,5, dem Regularisierungsparameter Œª = 5,0 und der Kostenfunktion mit Kreuzentropie. Wir werden ein Netzwerk von 30 Epochen mit einem vollst√§ndigen Datensatz trainieren und die Anzahl der Epochen proportional zur Verringerung des Volumens der Trainingsdaten erh√∂hen. Um den gleichen Gewichtsreduktionsfaktor f√ºr verschiedene S√§tze von Trainingsdaten zu gew√§hrleisten, verwenden wir den Regularisierungsparameter Œª = 5,0 mit einem vollst√§ndigen Trainingssatz und reduzieren Sie ihn proportional mit einer Verringerung des Datenvolumens.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist ersichtlich, dass die Klassifizierungsgenauigkeit mit der Zunahme der Trainingsdaten signifikant zunimmt. Dieses Wachstum d√ºrfte sich mit einem weiteren Mengenanstieg fortsetzen. Nach der obigen Grafik zu urteilen, n√§hern wir uns nat√ºrlich der S√§ttigung. Angenommen, wir wiederholen dieses Diagramm in einer logarithmischen Abh√§ngigkeit von der Menge der Trainingsdaten:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist zu sehen, dass das Diagramm am Ende immer noch tendenziell steigt. Dies deutet darauf hin, dass wir wahrscheinlich ein viel besser funktionierendes Netzwerk erhalten werden, selbst wenn wir eine viel gr√∂√üere Datenmenge verwenden - zum Beispiel Millionen oder sogar Milliarden handgeschriebener Beispiele anstelle von 50.000. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist eine gute Idee, mehr Trainingsdaten zu erhalten. Leider kann dies teuer sein, so dass es in der Praxis nicht immer m√∂glich ist. Es gibt jedoch eine andere Idee, die fast genauso gut funktionieren kann - den Datensatz k√ºnstlich zu vergr√∂√üern. Nehmen wir zum Beispiel an, wir nehmen ein Bild einer F√ºnf von MNIST und drehen es ein wenig, Grad um 15:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist eindeutig die gleiche Zahl. Auf Pixelebene unterscheidet es sich jedoch stark von den in der MNIST-Datenbank verf√ºgbaren Bildern. Es ist anzunehmen, dass das Hinzuf√ºgen dieses Bildes zum Trainingsdatensatz unserem Netzwerk helfen kann, mehr √ºber die Bildklassifizierung zu erfahren. Dar√ºber hinaus sind wir nat√ºrlich nicht auf die M√∂glichkeit beschr√§nkt, nur ein Bild hinzuzuf√ºgen. Wir k√∂nnen unsere Trainingsdaten erweitern, indem wir alle Trainingsbilder von MNIST ein paar kleine Umdrehungen machen und dann den erweiterten Satz von Trainingsdaten verwenden, um die Effizienz des Netzwerks zu steigern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Idee ist sehr m√§chtig und weit verbreitet. Schauen wir uns die Ergebnisse der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wissenschaftlichen Arbeit an</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der verschiedene Variationen dieser Idee auf MNIST anwendete. Eine der Architekturen der von ihnen betrachteten Netzwerke √§hnelte der von uns verwendeten - ein direktes Verteilungsnetzwerk mit 800 versteckten Neuronen, das die Kostenfunktion mit Kreuzentropie verwendet. Durch den Start dieses Netzwerks mit dem Standard-MNIST-Trainingssatz wurde eine Klassifizierungsgenauigkeit von 98,4% erreicht. Aber dann erweiterten sie die Trainingsdaten und verwendeten nicht nur die oben beschriebene Drehung, sondern auch die √úbertragung und Verzerrung von Bildern. Nachdem sie das Netzwerk auf fortgeschrittene Daten geschult hatten, erh√∂hten sie die Genauigkeit auf 98,9%. Sie experimentierten auch mit den sogenannten "Elastische Verzerrung", eine spezielle Art der Bildverzerrung, um die zuf√§lligen Vibrationen der Handmuskeln zu beseitigen. Mit elastischen Verzerrungen zur Erweiterung der Daten erreichten sie eine Genauigkeit von 99,3%. Im Wesentlichen erweiterten sie ihre Netzwerkerfahrung,Geben Sie ihr verschiedene handschriftliche Variationen, die in echter Handschrift zu finden sind.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Varianten dieser Idee k√∂nnen verwendet werden, um die Leistung vieler Lernaufgaben zu verbessern, nicht nur zur Handschrifterkennung. Das allgemeine Prinzip besteht darin, die Trainingsdaten zu erweitern, indem Operationen auf sie angewendet werden, die die in der Realit√§t auftretenden Abweichungen widerspiegeln. Solche Variationen sind leicht zu finden. Angenommen, wir erstellen NS f√ºr die Spracherkennung. Menschen k√∂nnen Sprache auch bei Verzerrungen wie Hintergrundger√§uschen erkennen. Daher k√∂nnen Sie die Daten durch Hinzuf√ºgen von Hintergrundger√§uschen erweitern. Wir k√∂nnen auch beschleunigte und langsame Sprache erkennen. Dies ist eine weitere M√∂glichkeit, Trainingsdaten zu erweitern. Diese Techniken werden nicht immer verwendet. Anstatt beispielsweise den Trainingssatz durch Hinzuf√ºgen von Rauschen zu erweitern, kann es effizienter sein, die Eingabe durch Anwenden eines Rauschfilters zu bereinigen. Es lohnt sich jedoch, die Idee zu ber√ºcksichtigen, das Trainingsset zu erweitern.und suchen Sie nach M√∂glichkeiten, es zu verwenden.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbung </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie oben erl√§utert, besteht eine M√∂glichkeit, die Trainingsdaten von MNIST zu erweitern, darin, kleine Rotationen der Trainingsbilder zu verwenden. </font><font style="vertical-align: inherit;">Welches Problem kann auftreten, wenn wir die Drehung der Bilder in beliebigen Winkeln zulassen?</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Big-Data-Exkurs und die Bedeutung des Vergleichs der Klassifizierungsgenauigkeit </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Lassen Sie uns noch einmal einen Blick darauf werfen, wie sich die Genauigkeit unserer NS in Abh√§ngigkeit von der Gr√∂√üe des Trainingssatzes √§ndert: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Angenommen, wir w√ºrden anstelle von NS eine andere Technologie f√ºr maschinelles Lernen verwenden, um Zahlen zu klassifizieren. Wir werden beispielsweise versuchen, die SVM-Methode (Support Vector Machine) zu verwenden, die wir in Kapitel 1 kurz kennengelernt haben. Machen Sie sich also keine Sorgen, wenn Sie mit SVM nicht vertraut sind. Wir m√ºssen die Details nicht verstehen. Wir werden SVM √ºber die Scikit-Learn-Bibliothek verwenden. So variiert die Effektivit√§t von SVM mit der Gr√∂√üe des Trainingssatzes. Zum Vergleich habe ich den Zeitplan und die Ergebnisse der Nationalversammlung aufgestellt.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wahrscheinlich das erste, was auff√§llt - NS √ºbertrifft SVM in jeder Gr√∂√üe des Trainingssatzes. Das ist gut, obwohl es sich nicht lohnt, daraus weitreichende Schlussfolgerungen zu ziehen, da ich die vordefinierten Einstellungen f√ºr das Scikit-Lernen verwendet habe und wir ziemlich ernsthaft an unserem NS gearbeitet haben. Eine weniger anschauliche, aber interessantere Tatsache, die sich aus der Grafik ergibt, ist, dass wenn wir unsere SVM mit 50.000 Bildern trainieren, sie besser funktioniert (94,48% Genauigkeit) als unsere mit 5000 Bildern trainierte NS ( 93,24%). Mit anderen Worten, eine Zunahme des Trainingsdatenvolumens gleicht manchmal den Unterschied in den MO-Algorithmen aus.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es kann etwas Interessanteres passieren. Angenommen, wir versuchen, ein Problem mit zwei Algorithmen MO, A und B zu l√∂sen. Manchmal kommt es vor, dass Algorithmus A bei einem Satz von Trainingsdaten vor Algorithmus B und Algorithmus B bei einem anderen Satz von Trainingsdaten vor Algorithmus A liegt. Wir haben das oben nicht gesehen - dann w√ºrden sich die Graphen schneiden - aber </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">das passiert</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Die richtige Antwort auf die Frage: "Ist Algorithmus A Algorithmus B √ºberlegen?" in der Tat: "Welchen Trainingsdatensatz verwenden Sie?"</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All dies muss sowohl bei der Entwicklung als auch beim Lesen wissenschaftlicher Arbeiten ber√ºcksichtigt werden. Viele Arbeiten konzentrieren sich darauf, neue Tricks zu finden, um bessere Ergebnisse mit Standardmessdatens√§tzen zu erzielen. ‚ÄûMit unserer Superfood-Technologie konnten wir den Standard-Vergleichssatz Y um X% verbessern‚Äú - das kanonische Antragsformular in einer solchen Studie. Manchmal sind solche Aussagen tats√§chlich interessant, aber es lohnt sich zu verstehen, dass sie nur im Kontext eines bestimmten Trainingssatzes anwendbar sind. Stellen Sie sich eine alternative Geschichte vor, in der Personen, die urspr√ºnglich ein Vergleichsset erstellt haben, ein gr√∂√üeres Forschungsstipendium erhalten haben. Sie k√∂nnten zus√§tzliches Geld verwenden, um zus√§tzliche Daten zu sammeln. Es ist m√∂glich, dass die ‚ÄûVerbesserung‚Äú der Super-Duper-Technologie in einem gr√∂√üeren Datensatz verschwindet. Mit anderen Worten,Das Wesentliche der Verbesserung kann nur ein Unfall sein. Daraus sollte die folgende Moral in den Bereich der praktischen Anwendung aufgenommen werden: Wir brauchen sowohl verbesserte Algorithmen als auch verbesserte Trainingsdaten. Es ist nichts Falsches daran, nach verbesserten Algorithmen zu suchen, aber stellen Sie sicher, dass Sie sich nicht darauf konzentrieren und den einfacheren Weg zum Gewinnen ignorieren, indem Sie das Volumen oder die Qualit√§t der Trainingsdaten erh√∂hen.</font></font><br><br><h3>  Herausforderung </h3><br><ul><li>  .              ?                 .        ‚Äì  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> ,  ,   ,      .   ,             .        -   ?   ,      . </li></ul><br><h3>  Zusammenfassung </h3><br>  Wir haben unser Eintauchen in Umschulung und Regularisierung abgeschlossen.  Nat√ºrlich werden wir auf diese Probleme zur√ºckkommen.  Wie ich bereits mehrfach erw√§hnt habe, ist die Umschulung im Bereich der NS ein gro√ües Problem, insbesondere wenn Computer leistungsf√§higer werden und wir gr√∂√üere Netzwerke trainieren k√∂nnen.  Infolgedessen ist es dringend erforderlich, wirksame Regularisierungstechniken zu entwickeln, um die Umschulung zu verringern. Daher ist dieser Bereich heute sehr aktiv. <br><br><h2>  Gewichtsinitialisierung </h2><br>  Wenn wir unsere NS erstellen, m√ºssen wir die Anfangswerte von Gewichten und Offsets ausw√§hlen.  Bisher haben wir sie gem√§√ü den in Kapitel 1 kurz beschriebenen Richtlinien ausgew√§hlt. Ich m√∂chte Sie daran erinnern, dass wir Gewichte und Offsets basierend auf einer unabh√§ngigen Gau√üschen Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 ausgew√§hlt haben. Dieser Ansatz hat gut funktioniert, scheint aber ziemlich willk√ºrlich zu sein, also lohnt es sich √úberarbeiten Sie es und √ºberlegen Sie, ob es m√∂glich ist, die anf√§nglichen Gewichte und Verschiebungen besser zuzuweisen und unseren NSs m√∂glicherweise dabei zu helfen, schneller zu lernen. <br><br>  Es stellt sich heraus, dass der Initialisierungsprozess im Vergleich zur normalisierten Gau√üschen Verteilung erheblich verbessert werden kann.  Um dies zu verstehen, nehmen wir an, wir arbeiten mit einem Netzwerk mit einer gro√üen Anzahl von Eingangsneuronen, beispielsweise mit 1000. Nehmen wir an, wir haben die normalisierte Gau√üsche Verteilung verwendet, um Gewichte zu initialisieren, die mit der ersten verborgenen Schicht verbunden sind.  Bisher werde ich mich nur auf die Skalen konzentrieren, die die Eingangsneuronen mit dem ersten Neuron in der verborgenen Schicht verbinden, und den Rest des Netzwerks ignorieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br>  Stellen wir uns der Einfachheit halber vor, wir versuchen, das Netzwerk mit Eingang x zu trainieren, in dem die H√§lfte der Eingangsneuronen eingeschaltet ist, dh einen Wert von 1 hat und die H√§lfte ausgeschaltet ist, dh einen Wert von 0. Das n√§chste Argument funktioniert in einem allgemeineren Fall, aber es ist einfacher f√ºr Sie werde ihn an diesem speziellen Beispiel verstehen.  Betrachten Sie die gewichtete Summe z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b der Eingaben f√ºr ein verstecktes Neuron.  500 Mitglieder der Summe verschwinden, weil die entsprechenden x <sub>j</sub> 0 sind. Daher ist z die Summe von 501 normalisierten Gau√üschen Zufallsvariablen, 500 Gewichten und 1 zus√§tzlichen Versatz.  Daher hat der z-Wert selbst eine Gau√üsche Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von ‚àö501 ‚âà 22,4.  Das hei√üt, z hat eine ziemlich breite Gau√üsche Verteilung ohne scharfe Spitzen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br>  Insbesondere zeigt dieser Graph, dass | z | wahrscheinlich ziemlich gro√ü ist, dh z ‚â§ 1 oder z ‚â§ -1.  In diesem Fall liegt die Ausgabe der versteckten Neuronen œÉ (z) sehr nahe bei 1 oder 0. Dies bedeutet, dass unser verstecktes Neuron ges√§ttigt ist.  Und wenn dies geschieht, f√ºhren kleine Gewichts√§nderungen, wie wir bereits wissen, zu geringf√ºgigen √Ñnderungen der Aktivierung eines verborgenen Neurons.  Diese winzigen √Ñnderungen wirken sich wiederum praktisch nicht auf die verbleibenden Neutronen im Netzwerk aus, und wir werden die entsprechenden winzigen √Ñnderungen in der Kostenfunktion sehen.  Infolgedessen werden diese Gewichte sehr langsam trainiert, wenn wir den Gradientenabstiegsalgorithmus verwenden.  Dies √§hnelt der Aufgabe, die wir bereits in diesem Kapitel besprochen haben, bei der mit falschen Werten ges√§ttigte Ausgangsneuronen das Lernen verlangsamen.  Fr√ºher haben wir dieses Problem gel√∂st, indem wir eine Kostenfunktion geschickt ausgew√§hlt haben.  Obwohl dies bei ges√§ttigten Ausgangsneuronen hilfreich war, hilft es leider √ºberhaupt nicht bei der S√§ttigung versteckter Neuronen. <br><br>  Jetzt sprach ich √ºber die eingehenden Skalen der ersten verborgenen Schicht.  Die gleichen Argumente gelten nat√ºrlich f√ºr die folgenden verborgenen Ebenen: Wenn die Gewichte in den sp√§teren verborgenen Ebenen mit normalisierten Gau√üschen Verteilungen initialisiert werden, liegt ihre Aktivierung h√§ufig nahe bei 0 oder 1, und das Training verl√§uft sehr langsam. <br><br>  Gibt es eine M√∂glichkeit, die besten Initialisierungsoptionen f√ºr Gewichte und Offsets auszuw√§hlen, damit wir keine solche S√§ttigung erhalten und Lernverz√∂gerungen vermeiden k√∂nnen?  Angenommen, wir haben ein Neuron mit der Anzahl der eingehenden Gewichte n <sub>in</sub> .  Dann m√ºssen wir diese Gewichte mit zuf√§lligen Gau√üschen Verteilungen mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 / ‚àön <sub>in</sub> initialisieren.  Das hei√üt, wir komprimieren die Gau√üschen und verringern die Wahrscheinlichkeit einer S√§ttigung des Neurons.  Dann w√§hlen wir eine Gau√üsche Verteilung f√ºr Verschiebungen mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 aus Gr√ºnden, auf die ich etwas sp√§ter zur√ºckkommen werde.  Nachdem wir diese Wahl getroffen haben, stellen wir erneut fest, dass z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b eine Zufallsvariable mit einer Gau√üschen Verteilung mit einer mathematischen Erwartung von 0, aber mit einem viel ausgepr√§gteren Peak als zuvor ist.  Nehmen wir nach wie vor an, dass 500 Eingaben 0 und 500 1 sind. Dann ist es einfach zu zeigen (siehe √úbung unten), dass z eine Gau√üsche Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von ‚àö (3/2) = 1,22 hat ... Dieses Diagramm hat einen viel sch√§rferen Peak, so dass selbst im Bild unten die Situation etwas untertrieben ist, da ich den Ma√üstab der vertikalen Achse im Vergleich zum vorherigen Diagramm √§ndern musste: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br>  Ein solches Neuron wird mit einer viel geringeren Wahrscheinlichkeit ges√§ttigt sein und dementsprechend mit einer geringeren Wahrscheinlichkeit auf eine Verlangsamung des Lernens sto√üen. <br><br><h3>  √úbung </h3><br><ul><li>  Best√§tigen Sie, dass die Standardabweichung von z = ‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b vom vorherigen Absatz ‚àö (3/2) ist.  √úberlegungen dazu: Die Varianz der Summe der unabh√§ngigen Zufallsvariablen ist gleich der Summe der Varianzen der einzelnen Zufallsvariablen;  Die Varianz entspricht dem Quadrat der Standardabweichung. </li></ul><br>  Ich habe oben erw√§hnt, dass wir weiterhin Verschiebungen initialisieren werden, basierend auf einer unabh√§ngigen Gau√üschen Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1. Und dies ist normal, da dies die Wahrscheinlichkeit der S√§ttigung unserer Neuronen nicht stark erh√∂ht.  Tats√§chlich spielt die Initialisierung von Offsets keine gro√üe Rolle, wenn es uns gelingt, das S√§ttigungsproblem zu vermeiden.  Einige versuchen sogar, alle Offsets auf Null zu initialisieren, und verlassen sich auf die Tatsache, dass der Gradientenabstieg die entsprechenden Offsets lernen kann.  Da die Wahrscheinlichkeit, dass sich dies auf etwas auswirkt, gering ist, werden wir weiterhin das gleiche Initialisierungsverfahren wie zuvor verwenden. <br><br>  Vergleichen wir die Ergebnisse des alten und des neuen Ansatzes zur Initialisierung von Gewichten mithilfe der Aufgabe, Zahlen aus MNIST zu klassifizieren.  Nach wie vor werden wir 30 versteckte Neuronen, ein Minipaket der Gr√∂√üe 10, einen Regularisierungsparameter &amp; lambda = 5,0 und eine Kostenfunktion mit Kreuzentropie verwenden.  Wir werden die Lerngeschwindigkeit schrittweise von Œ∑ = 0,5 auf 0,1 reduzieren, da auf diese Weise die Ergebnisse in den Diagrammen etwas besser sichtbar sind.  Sie k√∂nnen mit der alten Gewichtsinitialisierungsmethode lernen: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.large_weight_initializer() &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Sie k√∂nnen auch lernen, wie Sie mit dem neuen Ansatz Gewichte initialisieren.  Dies ist noch einfacher, da network2 standardm√§√üig Gewichte mit einem neuen Ansatz initialisiert.  Dies bedeutet, dass wir den Aufruf net.large_weight_initializer () fr√ºher weglassen k√∂nnen: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Wir zeichnen (mit dem Programm weight_initialization.py): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br>  In beiden F√§llen wird eine Klassifizierungsgenauigkeit von 96% erhalten.  Die resultierende Genauigkeit ist in beiden F√§llen nahezu gleich.  Aber die neue Initialisierungstechnik erreicht diesen Punkt viel, viel schneller.  Am Ende der letzten √Ñra des Trainings erreicht der alte Ansatz zur Initialisierung von Gewichten eine Genauigkeit von 87%, und der neue Ansatz n√§hert sich bereits 93%.  Anscheinend beginnt ein neuer Ansatz zum Initialisieren von Gewichten an einer viel besseren Position, sodass wir viel schneller gute Ergebnisse erzielen.  Das gleiche Ph√§nomen wird beobachtet, wenn wir die Ergebnisse f√ºr ein Netzwerk mit 100 Neuronen konstruieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br>  In diesem Fall treten zwei Kurven nicht auf.  Meine Experimente besagen jedoch, dass die Genauigkeit fast zusammenf√§llt, wenn Sie etwas mehr Epochen hinzuf√ºgen.  Auf der Grundlage dieser Experimente k√∂nnen wir daher sagen, dass die Verbesserung der Initialisierung von Gewichten das Training nur beschleunigt, aber die gesamte Netzwerkeffizienz nicht ver√§ndert.  In Kapitel 4 werden wir jedoch Beispiele f√ºr NSs sehen, bei denen die langfristige Effizienz infolge der Initialisierung von Gewichten durch 1 / ‚àön <sub>in</sub> erheblich verbessert wird.  Daher verbessert es nicht nur die Lerngeschwindigkeit, sondern manchmal auch die daraus resultierende Effektivit√§t. <br><br>  Der Ansatz zum Initialisieren von Gewichten durch 1 / ‚àön <sub>in</sub> hilft, das Training neuronaler Netze zu verbessern.  Es wurden andere Techniken zum Initialisieren von Gewichten vorgeschlagen, von denen viele auf dieser Grundidee basieren.  Ich werde sie hier nicht ber√ºcksichtigen, da 1 / ‚àön <sub>in</sub> f√ºr unsere Zwecke gut funktioniert.  Wenn Sie interessiert sind, empfehle ich, die Diskussion auf den Seiten 14 und 15 in einem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von</a> Yoshua Benggio aus dem Jahr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2012 zu</a> lesen. <br><br><h3>  Herausforderung </h3><br><ul><li>  Die Kombination aus Regularisierung und einer verbesserten Gewichtsinitialisierungsmethode.  Manchmal liefert die Regularisierung von L2 automatisch Ergebnisse, die einer neuen Methode zum Initialisieren von Gewichten √§hneln.  Angenommen, wir verwenden den alten Ansatz zum Initialisieren von Gewichten.  Skizzieren Sie ein heuristisches Argument, das beweist, dass: (1) wenn Œª nicht zu klein ist, in den ersten Trainingsepochen die Schw√§chung der Gewichte fast vollst√§ndig dominiert;  (2) wenn Œ∑Œª ‚â™ n ist, werden die Gewichte in der Epoche e <sup>‚àíŒ∑Œª / m-</sup> mal schw√§cher;  (3) Wenn Œª nicht zu gro√ü ist, verlangsamt sich die Schw√§chung der Gewichte, wenn die Gewichte auf etwa 1 / ‚àön abnehmen, wobei n die Gesamtzahl der Gewichte im Netzwerk ist.  Beweisen Sie, dass diese Bedingungen in den Beispielen erf√ºllt sind, f√ºr die in diesem Abschnitt Diagramme erstellt wurden. </li></ul><br><br><h2>  Zur√ºck zur Handschrifterkennung: Code </h2><br>  Lassen Sie uns die in diesem Kapitel beschriebenen Ideen umsetzen.  Wir werden ein neues Programm entwickeln, network2.py, eine verbesserte Version des Programms network.py, das wir in Kapitel 1 erstellt haben. Wenn Sie den Code lange nicht gesehen haben, m√ºssen Sie ihn m√∂glicherweise schnell durchgehen.  Dies sind nur 74 Codezeilen, und es ist leicht zu verstehen. <br><br>  Wie bei network.py ist der Star von network2.py die Network-Klasse, mit der wir unsere NSs darstellen.  Wir initialisieren die Klasseninstanz mit einer Liste der Gr√∂√üen der entsprechenden Netzwerkschichten, und bei Auswahl der Kostenfunktion handelt es sich standardm√§√üig um eine Kreuzentropie: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes, cost=CrossEntropyCost)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost</code> </pre> <br>  Die ersten Zeilen der __init__ -Methode sind mit network.py identisch und werden von sich aus verstanden.  Die n√§chsten beiden Zeilen sind neu und wir m√ºssen im Detail verstehen, was sie tun. <br><br>  Beginnen wir mit der Methode default_weight_initializer.  Er verwendet einen neuen, verbesserten Ansatz zum Initialisieren von Gewichten.  Wie wir gesehen haben, werden bei diesem Ansatz die in das Neuron eintretenden Gewichte auf der Grundlage einer unabh√§ngigen Gau√üschen Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 geteilt durch die Quadratwurzel der Anzahl der eingehenden Verbindungen zum Neuron initialisiert.  Au√üerdem initialisiert diese Methode die Offsets unter Verwendung der Gau√üschen Verteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1. Hier ist der Code: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">default_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Um es zu verstehen, m√ºssen Sie sich daran erinnern, dass np eine Numpy-Bibliothek ist, die sich mit linearer Algebra befasst.  Wir haben es zu Beginn des Programms importiert.  Beachten Sie auch, dass wir keine Verschiebungen in der ersten Schicht von Neuronen initialisieren.  Die erste Schicht ist eingehend, daher werden keine Offsets verwendet.  Das gleiche war network.py. <br><br>  Zus√§tzlich zur default_weight_initializer-Methode erstellen wir eine large_weight_initializer-Methode.  Es initialisiert Gewichte und Offsets nach dem alten Ansatz aus Kapitel 1, bei dem Gewichte und Offsets basierend auf einer unabh√§ngigen Gau√üschen Verteilung mit einer mathematischen Erwartung von 0 und einer Standardabweichung von 1 initialisiert werden. Dieser Code unterscheidet sich nat√ºrlich nicht wesentlich von default_weight_initializer: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">large_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Ich habe diese Methode haupts√§chlich aufgenommen, weil es f√ºr uns bequemer war, die Ergebnisse dieses Kapitels und von Kapitel 1 zu vergleichen. Ich kann mir keine wirklichen Optionen vorstellen, bei denen ich die Verwendung empfehlen w√ºrde! <br><br>  Die zweite Neuheit der __init__ -Methode ist die Initialisierung des Kostenattributs.  Um zu verstehen, wie dies funktioniert, schauen wir uns die Klasse an, die wir zur Darstellung der entropie√ºbergreifenden Kostenfunktion verwenden (die Direktive @staticmethod teilt dem Interpreter mit, dass diese Methode unabh√§ngig vom Objekt ist, sodass der Parameter self nicht an die Methoden fn und delta √ºbergeben wird). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropyCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-a))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay)</code> </pre> <br>  Lass es uns herausfinden.  Das erste, was hier zu sehen ist, ist, dass Kreuzentropie zwar eine Funktion aus mathematischer Sicht ist, wir sie jedoch als Python-Klasse implementieren, nicht als Python-Funktion.  Warum habe ich mich dazu entschieden?  In unserem Netzwerk spielt Wert zwei verschiedene Rollen.  Offensichtlich - es ist ein Ma√ü daf√ºr, wie gut die Ausgangsaktivierung a dem gew√ºnschten Ausgang y entspricht.  Diese Rolle wird von der CrossEntropyCost.fn-Methode bereitgestellt.  (Beachten Sie √ºbrigens, dass der Aufruf von np.nan_to_num in CrossEntropyCost.fn sicherstellt, dass Numpy den Logarithmus von Zahlen nahe Null korrekt verarbeitet.)  Die Kostenfunktion wird in unserem Netzwerk jedoch auf die zweite Weise verwendet.  Wir erinnern uns aus Kapitel 2 daran, dass wir beim Starten des Backpropagation-Algorithmus den Ausgabefehler des Netzwerks Œ¥ <sup>L</sup> ber√ºcksichtigen m√ºssen <sup>.</sup>  Die Form des Ausgabefehlers h√§ngt von der Kostenfunktion ab: Unterschiedliche Kostenfunktionen haben unterschiedliche Formen des Ausgabefehlers.  F√ºr die Kreuzentropie ist der Ausgabefehler wie folgt aus Gleichung (66) gleich: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>99</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.761ex" height="2.901ex" viewBox="0 -987.6 9799.8 1249" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-65" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-6C" x="1240" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="1538" y="0"></use><g transform="translate(1900,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-3D" x="3289" y="0"></use><g transform="translate(4345,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-2212" x="5679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-79" x="6679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-74" x="7427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-61" x="7788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMATHI-67" x="8318" y="0"></use><g transform="translate(8798,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhhe4e0jEhdcunduJS6oY8VpWJqcWg#MJMAIN-39" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>‚àí</mo><mi>y</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>99</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \ delta ^ L = a ^ L-y \ tag {99} </script></p><br><br>  Daher definiere ich eine zweite Methode, CrossEntropyCost.delta, deren Ziel es ist, dem Netzwerk zu erkl√§ren, wie der Ausgabefehler berechnet wird.  Und dann kombinieren wir diese beiden Methoden zu einer Klasse, die alles enth√§lt, was unser Netzwerk √ºber die Kostenfunktion wissen muss. <br><br>  Aus einem √§hnlichen Grund enth√§lt network2.py eine Klasse, die eine quadratische Kostenfunktion darstellt.  Einschlie√ülich dieser zum Vergleich mit den Ergebnissen von Kapitel 1, da wir in Zukunft haupts√§chlich Kreuzentropie verwenden werden.  Der Code ist unten.  Die QuadraticCost.fn-Methode ist eine einfache Berechnung der quadratischen Kosten, die mit der Ausgabe a und der gew√ºnschten Ausgabe y verbunden sind.  Der von QuadraticCost.delta zur√ºckgegebene Wert basiert auf dem Ausdruck (30) f√ºr den Ausgabefehler des quadratischen Werts, den wir in Kapitel 2 abgeleitet haben. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">QuadraticCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*np.linalg.norm(ay)**<span class="hljs-number"><span class="hljs-number">2</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay) * sigmoid_prime(z)</code> </pre> <br>  Jetzt haben wir die Hauptunterschiede zwischen network2.py und network2.py herausgefunden.  Alles ist sehr einfach.  Es gibt andere kleine √Ñnderungen, die ich unten beschreiben werde, einschlie√ülich der Implementierung der Regularisierung von L2.  Schauen wir uns vorher den vollst√§ndigen network2.py-Code an.  Es ist nicht notwendig, es im Detail zu studieren, aber Sie sollten die Grundstruktur verstehen, insbesondere die Kommentare lesen, um zu verstehen, was die einzelnen Teile des Programms tun.  Nat√ºrlich verbiete ich nicht, mich so oft mit dieser Frage zu besch√§ftigen, wie Sie m√∂chten!  Wenn Sie sich verlaufen haben, lesen Sie den Text nach dem Programm und kehren Sie erneut zum Code zur√ºck.  Im Allgemeinen ist es hier: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network2.py ~~~~~~~~~~~~~~   network.py,            .   ‚Äì      , ,   .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import json import random import sys #  import numpy as np ####   ,      class QuadraticCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. """ return 0.5*np.linalg.norm(ay)**2 @staticmethod def delta(z, a, y): """  delta   .""" return (ay) * sigmoid_prime(z) class CrossEntropyCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """  delta   .  ``z``    ,          delta     . """ return (ay) ####   Network class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,   ``self.default_weight_initializer`` (.  ). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """            0    1,       ,       .          0    1.    ,         ,           . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """          0    1.          0    1.    ,         ,           .         1,    .       . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """  ,  ``a``  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """     -    . ``training_data`` ‚Äì   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   . """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: {}".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: {} / {}".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: {}".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: {} / {}".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """    ,          -. ``mini_batch`` ‚Äì    ``(x, y)``, ``eta`` ‚Äì  , ``lmbda`` -  , ``n`` -     .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """  l      ,      . l = 1    , l = 2 ‚Äì ,   .    ,   python      . """ for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """    ``data``,      .   ‚Äì        .  ``convert``  False,    ‚Äì    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     ‚Äì       ,      .   ,      .       mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """      ``data``.  ``convert``   False,   ‚Äì  (),   True,   ‚Äì   . .    ,      ``accuracy``, . """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """    ``filename``.""" data = {"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)} f = open(filename, "w") json.dump(data, f) f.close() ####  Network def load(filename): """    ``filename``.    Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net ####   def vectorized_result(j): """  10-    1.0   j     .      (0..9)     . """ e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Zu den interessanteren √Ñnderungen geh√∂rt die Einbeziehung der L2-Regularisierung.  Obwohl dies eine gro√üe konzeptionelle √Ñnderung ist, ist sie so einfach zu implementieren, dass Sie sie im Code m√∂glicherweise nicht bemerken.  In den meisten F√§llen wird der Parameter lmbda einfach an verschiedene Methoden √ºbergeben, insbesondere an Network.SGD.  Alle Arbeiten werden in einer Programmzeile ausgef√ºhrt, die vierte vom Ende der Methode Network.update_mini_batch.  Dort √§ndern wir die Aktualisierungsregel f√ºr den Gradientenabstieg, um die Gewichtsreduzierung einzuschlie√üen.  Die √Ñnderung ist winzig, wirkt sich aber ernsthaft auf die Ergebnisse aus! <br><br>  Dies geschieht √ºbrigens h√§ufig bei der Implementierung neuer Techniken in neuronalen Netzen.  Wir haben Tausende von W√∂rtern damit verbracht, √ºber Regularisierung zu diskutieren.  Konzeptionell ist dies eine ziemlich subtile und schwer zu verstehende Sache.  Es kann jedoch trivial zum Programm hinzugef√ºgt werden!  Unerwarteterweise k√∂nnen komplexe Techniken mit geringf√ºgigen Code√§nderungen implementiert werden. <br><br>  Eine weitere kleine, aber wichtige √Ñnderung im Code ist das Hinzuf√ºgen mehrerer optionaler Flags zur stochastischen Gradientenabstiegsmethode Network.SGD.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Flags erm√∂glichen es, Kosten und Genauigkeit entweder anhand von Trainingsdaten oder Auswertungsdaten zu verfolgen, die an Network.SGD √ºbertragen werden k√∂nnen. </font><font style="vertical-align: inherit;">Zu Beginn des Kapitels haben wir diese Flags h√§ufig verwendet, aber lassen Sie mich zur Erinnerung ein Beispiel f√ºr ihre Verwendung geben:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir setzen Evaluierungsdaten durch Validierungsdaten. Wir k√∂nnen jedoch die Leistung von test_data und anderen Datens√§tzen verfolgen. Wir haben auch vier Flags, die die Notwendigkeit angeben, Kosten und Genauigkeit sowohl f√ºr Evaluierungsdaten als auch f√ºr Trainingsdaten zu verfolgen. Diese Flags sind standardm√§√üig auf False gesetzt. Sie sind jedoch hier enthalten, um die Effektivit√§t des Netzwerks zu verfolgen. Dar√ºber hinaus gibt die Network.SGD-Methode von network2.py ein Tupel mit vier Elementen zur√ºck, das die Verfolgungsergebnisse darstellt. Sie k√∂nnen es so verwenden:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>evaluation_cost, evaluation_accuracy, ... training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So ist evaluation_cost beispielsweise eine Liste von 30 Elementen, die die Kosten der gesch√§tzten Daten am Ende jeder √Ñra enthalten. Solche Informationen sind √§u√üerst n√ºtzlich, um das Verhalten eines neuronalen Netzwerks zu verstehen. Solche Informationen sind √§u√üerst n√ºtzlich, um das Netzwerkverhalten zu verstehen. Es kann zum Beispiel verwendet werden, um Diagramme des Netzwerklernens √ºber die Zeit zu zeichnen. So habe ich alle Grafiken aus diesem Kapitel erstellt. Wenn jedoch eines der Flags nicht gesetzt ist, ist das entsprechende Tupelelement eine leere Liste.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weitere Code-Erg√§nzungen sind die Network.save-Methode, mit der das Network-Objekt auf der Festplatte gespeichert wird, und die Funktion zum Laden in den Speicher. Das Speichern und Laden erfolgt √ºber JSON, nicht √ºber die Python-Pickle- oder cPickle-Module, die √ºblicherweise zum Speichern auf der Festplatte und zum Laden in Python verwendet werden. Die Verwendung von JSON erfordert mehr Code als f√ºr pickle oder cPickle erforderlich w√§re. Um zu verstehen, warum ich mich f√ºr JSON entschieden habe, stellen Sie sich vor, dass wir uns irgendwann in der Zukunft entschlossen haben, unsere Netzwerkklasse so zu √§ndern, dass es mehr als nur Sigmoid-Neuronen gibt. Um diese √Ñnderung zu implementieren, w√ºrden wir h√∂chstwahrscheinlich die in der Methode Network .__ init__ definierten Attribute √§ndern. Und wenn wir nur Gurke zum Speichern verwenden w√ºrden, w√ºrde unsere Ladefunktion nicht funktionieren. Die Verwendung von JSON mit expliziter Serialisierung erleichtert uns die Garantiedass √§ltere Versionen des Netzwerkobjekts heruntergeladen werden k√∂nnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt viele kleine √Ñnderungen im Code, aber dies sind nur kleine Variationen von network.py. </font><font style="vertical-align: inherit;">Das Endergebnis ist eine Erweiterung unseres Programms mit 74 Zeilen auf ein viel funktionaleres Programm mit 152 Zeilen.</font></font><br><br><h3>  Herausforderung </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√Ñndern Sie den folgenden Code, indem Sie die Regularisierung L1 einf√ºhren, und klassifizieren Sie damit MNIST-Ziffern nach einem Netzwerk mit 30 versteckten Neuronen. </font><font style="vertical-align: inherit;">K√∂nnen Sie einen Regularisierungsparameter ausw√§hlen, mit dem Sie das Ergebnis im Vergleich zu einem Netzwerk ohne Regularisierung verbessern k√∂nnen?</font></font></li><li>    Network.cost_derivative method  network.py.      .        ?     ,       ?  network2.py      Network.cost_derivative,     CrossEntropyCost.delta.      ? </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de459816/">https://habr.com/ru/post/de459816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de459802/index.html">Habr Weekly # 9 / Burnout in der Jugend, japanische Schnittstellen, neuronales Battle.net-Netzwerk, Spiele und Grausamkeit</a></li>
<li><a href="../de459804/index.html">Erstellen Sie Crowdsourcing-Hilfekarten in WordPress + shMapper</a></li>
<li><a href="../de459806/index.html">Wie wir die Katze Lapuna behandelt haben</a></li>
<li><a href="../de459810/index.html">Microservices oder Monolith: Suche nach einer L√∂sung</a></li>
<li><a href="../de459814/index.html">Was bist du, Rendering Engine? Oder wie das Browser-Anzeigemodul funktioniert</a></li>
<li><a href="../de459820/index.html">Wischen Sie einfach √ºber die Karte: Wie OS / 2 in der New Yorker U-Bahn verwendet wird</a></li>
<li><a href="../de459822/index.html">Ein Beispiel f√ºr ein einfaches neuronales Netzwerk zeigt daher, was was ist</a></li>
<li><a href="../de459824/index.html">Checkliste zum Schreiben gro√üartiger Visual Studio-Erweiterungen</a></li>
<li><a href="../de459828/index.html">W√∂chentliche Nachrichten: Hyperloop-Ticketpreis in Russland, Apollo-Computer-Mainstream-Mining, AI-Bot in StarCraft II</a></li>
<li><a href="../de459830/index.html">Nat√ºrlich gaben sie Kraft und eine Linie von einem Maschinengewehr. Krebs und mehr ... Erfahrung mit Medizin</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>