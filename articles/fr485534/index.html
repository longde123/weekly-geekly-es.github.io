<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©ğŸ» ğŸ¼ ğŸ˜’ HighLoad ++, Mikhail Makurov (Intersvyaz): expÃ©rience dans la crÃ©ation d'une sauvegarde et d'un service Zabbix en cluster ğŸ‘¨ğŸ»â€ğŸš’ ğŸ“¦ ğŸ§‘ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zabbix est un systÃ¨me de surveillance ouvert populaire utilisÃ© par un grand nombre d'entreprises. Je vais parler de l'expÃ©rience de crÃ©ation d'un clus...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>HighLoad ++, Mikhail Makurov (Intersvyaz): expÃ©rience dans la crÃ©ation d'une sauvegarde et d'un service Zabbix en cluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/485534/">  Zabbix est un systÃ¨me de surveillance ouvert populaire utilisÃ© par un grand nombre d'entreprises.  Je vais parler de l'expÃ©rience de crÃ©ation d'un cluster de surveillance. <br><br>  Dans le rapport, je mentionnerai briÃ¨vement les modifications apportÃ©es prÃ©cÃ©demment (correctifs), qui Ã©largissent considÃ©rablement les capacitÃ©s du systÃ¨me et prÃ©parent la base du cluster (tÃ©lÃ©chargement de l'historique vers Â«ClickhouseÂ», interrogation asynchrone).  Et j'examinerai en dÃ©tail les problÃ¨mes qui se sont posÃ©s lors du clustering du systÃ¨me - rÃ©solution des conflits d'identitÃ© dans la base de donnÃ©es, un peu sur le thÃ©orÃ¨me CAP et la surveillance avec des bases de donnÃ©es distribuÃ©es, sur les nuances du fonctionnement de Zabbix en mode cluster: sauvegarde et coordination des serveurs et des proxys, sur les "domaines de surveillance" et un nouveau look sur l'architecture du systÃ¨me. <br><br>  Je vais parler briÃ¨vement de la faÃ§on de dÃ©marrer un cluster Ã  la maison, oÃ¹ trouver les sources et quelles sources supplÃ©mentaires.  des paramÃ¨tres seront requis pour le cluster. <br><br><img src="https://habrastorage.org/webt/uv/cy/1r/uvcy1rw-6ttiumnzmjuizgl4ml4.jpeg"><br><br>  HighLoad ++ Siberia 2019. Salle Tomsk.  24 juin, 17 h  RÃ©sumÃ©s et <a href="https://www.highload.ru/siberia/2019/abstracts/5210">prÃ©sentation</a> .  La prochaine confÃ©rence HighLoad ++ se tiendra les 6 et 7 avril 2020 Ã  Saint-PÃ©tersbourg.  DÃ©tails et billets <a href="http://bit.ly/2sSxgBx">ici</a> . <a name="habracut"></a><br><br>  <b>Mikhaili Makurov (ci-aprÃ¨s - MM):</b> - Je travaille pour une entreprise prestataire.  Le fournisseur s'appelle Intersvyaz, il travaille dans la ville de Tcheliabinsk.  Nous avons environ 1,5 million de personnes.  Et pour que le fournisseur fonctionne, il existe une Ã©norme infrastructure.  Nous avons environ 70 000 Ã©quipements: commutateurs, dispositifs IoT ... - beaucoup de tout ce qui doit Ãªtre surveillÃ©.  Plus prÃ©cisÃ©ment, ce rapport traite de l'utilisation de Zabbix, de la crÃ©ation d'un cluster basÃ© sur Zabbix pour la surveillance de l'infrastructure. <br><br>  J'ai 12 ans chez le prestataire.  Maintenant, je ne fais plus du tout de technique, c'est plutÃ´t de gÃ©rer les gens.  Et ce (truc technique) est en fait mon hobby.  Je dÃ©velopperai un peu ce sujet. <br><br><h3>  ProblÃ¨mes de surveillance </h3><br>  Je pense que j'ai de la chance.  Il y a environ un an et demi, je me suis retrouvÃ© dans un projet qui ressemblait Ã  ceci: "Nous devons rÃ©soudre certains problÃ¨mes avec notre surveillance."  J'ai hÃ©ritÃ© d'une zone de responsabilitÃ© (monitoring), qui se composait d'un tas de serveurs, notamment de 21 serveurs: <br><br><img src="https://habrastorage.org/webt/eu/om/mx/euommxg6onyd7rdbncwqbnrhvtc.jpeg"><br><br>  Il y avait 4 serveurs puissants et 15 procurations - tout Ã©tait matÃ©riel.  Il y a eu quelques plaintes concernant cette surveillance.  La premiÃ¨re, c'est que c'Ã©tait beaucoup.  Nous n'avons pas un seul serveur avec le fournisseur a pris autant d'espace.  C'est de l'argent, de l'Ã©lectricitÃ© ... En fait, ce n'est pas un gros problÃ¨me. <br><br><img src="https://habrastorage.org/webt/ui/jf/u3/uijfu32ebg7upocsepwtvptenk4.jpeg"><br><br>  Le gros problÃ¨me Ã©tait que la surveillance ne correspondait pas Ã  ce que nous attendions de lui.  Pour ceux qui n'ont pas activement utilisÃ© Zabbix, voici un tableau de bord qui montre le retard sur les contrÃ´les: <br><br><img src="https://habrastorage.org/webt/ig/jd/ti/igjdtilbejkchus711uvbzzeuyo.jpeg"><br><br>  La plupart de nos chÃ¨ques Ã©taient dans la zone rouge.  Ils ont couru plus de 10 minutes plus lentement que nous le voulions, c'est-Ã -dire qu'ils avaient 10 minutes de retard.  Ce n'Ã©tait pas trÃ¨s agrÃ©able, mais il Ã©tait encore possible de vivre plus ou moins.  Le plus gros problÃ¨me Ã©tait le suivant: <br><br><img src="https://habrastorage.org/webt/vf/ai/nw/vfainwua91q6pgxnj2msbx_tzle.jpeg"><br><br>  C'Ã©tait un systÃ¨me de surveillance d'un rÃ©seau fonctionnel.  Lorsque les travaux prÃ©vus ont Ã©tÃ© rÃ©alisÃ©s, un segment de milliers de personnes est tombÃ© sur cinq commutateurs.  Avec ces commutateurs, le commutateur et la surveillance sont tombÃ©s dans l'oubli.  Lorsque tout a Ã©tÃ© restaurÃ©, deux heures plus tard et la surveillance a Ã©tÃ© rÃ©tablie.  C'Ã©tait douloureusement dÃ©sagrÃ©able, et cette phrase devrait figurer dans chaque rapport: <br><br><img src="https://habrastorage.org/webt/aw/o-/t3/awo-t3j-xbu181dyneis-q4ozjo.jpeg"><br><br><h3>  "Nous devons faire quelque chose avec ce projet!" </h3><br>  Et ici, je vais raconter deux histoires.  Ensuite, nous avons essayÃ© d'aller simultanÃ©ment de deux maniÃ¨res.  Nous avons un groupe d'intÃ©gration - il a choisi la faÃ§on de construire un systÃ¨me modulaire (il y avait un rapport trÃ¨s cool d'Avito Ã  Highload en novembre de l'annÃ©e derniÃ¨re Ã  Moscou - ils en ont parlÃ©): <br><br><img src="https://habrastorage.org/webt/qg/f5/nv/qgf5nvonycfltfd8vge1tbeloj8.jpeg"><br><br><h3>  Zabbix = personnes + API + efficacitÃ© </h3><br>  Les gars de petites piÃ¨ces ont commencÃ© Ã  construire un systÃ¨me.  Et avec plusieurs passionnÃ©s, j'ai continuÃ© Ã  travailler sur Zabbix.  Il y avait des raisons Ã  cela.  Quelles en sont les raisons? <br><br><ul><li>  Tout d'abord, il existe une API sympa.  Et lorsque vous disposez de 60 Ã  70 000 Ã©lÃ©ments de surveillance, il est clair que tout cela ne fonctionne que automatiquement - vous ne pouvez pas ajouter autant de mains sans erreurs. </li><li>  Personnel.  Il y a des quarts de surveillance en service qui se tiennent 24/7.  Ce ne sont pas des informaticiens, ce sont des gens de service.  Nous avons montrÃ© au Â«GrafanÂ» quelques autres systÃ¨mes - c'est difficile pour eux.  Il y a des administrateurs habituÃ©s Ã  la diversitÃ©, Ã  la commoditÃ© de la surveillance dans le Zabbix lui-mÃªme: modÃ¨les, dÃ©tection automatique - et tout cela est cool! </li><li>  Zabbix peut Ãªtre efficace. </li></ul><br><h3>  La base de donnÃ©es SQL ralentit-elle?  Une rÃ©ponse - Clickhouse </h3><br>  La premiÃ¨re raison Ã©tait Ã©vidente.  Nous avons ensuite travaillÃ© sur MySQL, et nous avons rencontrÃ© environ 6 Ã  7 000 mÃ©triques par seconde, nous avons constatÃ© des retards constants sur les disques. <br><br><img src="https://habrastorage.org/webt/kb/ke/wf/kbkewfpy7l6b2qdexbuljdwr6ia.jpeg"><br><br>  Aujourd'hui, il a dÃ©jÃ  sonnÃ© 100 fois: la seule rÃ©ponse est Clickhouse: <br><br><img src="https://habrastorage.org/webt/w-/lm/iw/w-lmiwpxynx-rmj4spjyv_gc1cc.jpeg"><br><br>  Dans la structure des requÃªtes, la majeure partie des requÃªtes (notre profilage en quelques heures) sont des enregistrements de mÃ©triques.  L'Ã©criture de mÃ©triques dans une base de donnÃ©es SQL coÃ»te extrÃªmement cher.  Ici TimeScaleDB est apparu ... Ensuite, nous avons eu un Â«ClickhouseÂ» en fonctionnement pendant environ un an pour d'autres tÃ¢ches (nous faisons du big data, nous avons une grosse application - en gÃ©nÃ©ral, un fournisseur est maintenant une entreprise informatique entiÃ¨re). <br><br>  AprÃ¨s avoir regardÃ© de beaux graphiques sur Internet (que le Â«ClickhouseÂ» est des centaines de fois plus rapide, qu'il a besoin de trÃ¨s peu d'espace) et ayant une expÃ©rience actuelle, nous avons Ã©crit notre module HistoryStorage pour Â«ZabbixÂ» afin qu'il puisse enregistrer directement les donnÃ©es Â«ClickhouseÂ» (c'est-Ã -dire, pas Ã  partir de l'exportation de fichiers, mais directement Ã  la volÃ©e). <br><br><img src="https://habrastorage.org/webt/tp/f4/p2/tpf4p2z5jftziqwqhtdtxsb99-g.jpeg"><br><br>  De plus, nous avons Ã©crit un module pour le Â«frontÂ».  Tous ces beaux graphiques dans le panneau d'administration Zabbix peuvent Ãªtre crÃ©Ã©s Ã  partir de Clickhouse.  Il est clair que l'API fonctionne Ã©galement. <br><br>  L'effet est Ã  peu prÃ¨s le mÃªme - le serveur SQL en tant qu'entitÃ© dÃ©diÃ©e n'est pas devenu complÃ¨tement, c'est-Ã -dire que la charge est tombÃ©e Ã  zÃ©ro.  Ce qui est le plus remarquable, nous avions dÃ©jÃ  un cluster Â«ClickhouseÂ» dÃ©diÃ©: lorsque nous y avons consacrÃ© toute notre charge, il est passÃ© de 6 Ã  10 000 mÃ©triques.  Les gars qui administrent ont dit: "Mais nous ne voyons pas quelque chose qui est arrivÃ©.  Non! " <br><br><h3>  Comment nous avons Ã©largi Clickhouse </h3><br>  Je dirai encore plus: pour les tests, nous avons essayÃ© de charger jusqu'Ã  140-150 mille mÃ©triques par seconde (nous ne pouvions plus presser depuis Zabbix, plus tard je dirai pourquoi), et le Clickhouse ne voit pas non plus cette charge.  Autrement dit, il est trÃ¨s confortable, charge fraÃ®che.  En gÃ©nÃ©ral, il existe un tel module. <br><br>  De plus, nous l'avons un peu Ã©largi: <br><br><img src="https://habrastorage.org/webt/_u/ha/aj/_uhaajlqcp9mcjuh-ih8sc-dn3k.jpeg"><br><br>  Dans notre version, vous pouvez dÃ©sactiver les nanosecondes.  Vous savez probablement: Zabbix Ã©crit des secondes et des nanosecondes dans deux champs.  Dans les champs Â«ClickhouseÂ» dans lesquels la variabilitÃ© est trÃ¨s importante, occupez beaucoup d'espace. <br><br>  Au fait, Ã  propos de l'endroit.  Une mÃ©trique dans Clickhouse (nous avons maintenant environ 700 milliards de mÃ©triques enregistrÃ©es) prend 2,9 octets.  Selon la documentation de Zabbix, une mÃ©trique dans les bases de donnÃ©es SQL prend de 40 Ã  100 octets.  La dÃ©sactivation des nanosecondes permet d'Ã©conomiser 40% supplÃ©mentaires, soit environ 1,5 octet par mÃ©trique.  Autrement dit, "Clickhouse" est trÃ¨s efficace en termes de localisation. <br><br>  Ã€ la demande de nos gars qui sont engagÃ©s dans l'apprentissage automatique, nous avons fait une option pour que nous puissions Ã©crire l'hÃ´te et le nom de la mÃ©trique.  Ã‰tant donnÃ© que la variabilitÃ© des donnÃ©es est importante, cela ne prend pas beaucoup de place supplÃ©mentaire, malgrÃ© le fait que les donnÃ©es textuelles peuvent Ãªtre importantes (elles n'ont pas encore Ã©tÃ© vÃ©rifiÃ©es avec de longs tests). <br><br>  De plus, nous avons fait deux ajouts, car nous avons dÃ©veloppÃ© Zabbix et avons souvent dÃ» le retirer.  Un ajout trÃ¨s cool: au dÃ©but, puisque "Clickhouse" vous permet de lire des millions d'enregistrements, nous pouvons remplir le cache historique.  Au dÃ©but, nous sommes retardÃ©s de 30 Ã  40 secondes supplÃ©mentaires, mais nous obtenons un service immÃ©diatement lancÃ© avec un cache chauffÃ©. <br><br>  Dans les cas oÃ¹ il est plus facile de collecter Ã  partir de l'infrastructure, il existe toujours une telle option: interdire la lecture du cache pendant un certain temps.  Il vaut mieux travailler rapidement pendant 5 minutes, sans compter les dÃ©clencheurs, puis le cache se remplira - si vous ne le faites pas, la stagnation des plombs dâ€™histoire commence. <br><br>  En gÃ©nÃ©ral, il existe un module Â«ClickhouseÂ».  Il peut Ãªtre utilisÃ©. <br><br><h3>  EfficacitÃ© d'interrogation </h3><br>  MalgrÃ© le fait que nous ayons ensuite rÃ©solu les problÃ¨mes avec la base, les freins et le problÃ¨me avec quinze mandataires restaient.  Ils Ã©taient liÃ©s Ã  cela: <br><br><img src="https://habrastorage.org/webt/un/ec/u8/unecu8ujsgk7scstaonbt3dueim.jpeg"><br><br>  Il s'agit du principal pipeline de traitement de donnÃ©es chez Zabbix.  Il y a une Ã©tape de collecte de donnÃ©es, il y a un prÃ©traitement et il y a des synchroniseurs historiques qui font tout le travail (calcul des dÃ©clencheurs, alertes, sauvegarde de l'historique).  Le goulot d'Ã©tranglement du cache s'est avÃ©rÃ© Ãªtre: <br><br><img src="https://habrastorage.org/webt/er/jg/ei/erjgein-xwdpitoa8eql3b5dw1g.jpeg"><br><br>  Pourquoi le vote est-il lent?  Parce que les threads qui effectuent les requÃªtes vont dans la file d'attente dans la configuration du cache pour les mesures d'unitÃ© et la bloquent.  Il y a d'autres endroits, mais ils ne sont pas si Ã©troits.  Par exemple, il y a un prÃ©traitement lui-mÃªme et il y a un cache d'historique.  Sur notre SQL, nous avons obtenu les restrictions suivantes: <br><br><img src="https://habrastorage.org/webt/-w/ys/-b/-wys-btlu1uxulrqcinsvbmac5a.jpeg"><br><br>  Cela est peut-Ãªtre dÃ» au fait que dans notre cas, la base est d'environ 5 millions de mesures, que nous supprimons.  Avec toutes les optimisations que nous avons faites, nous avons pu obtenir 70 000 mesures dans le goulot d'Ã©tranglement (sur le cache de configuration), mais uniquement dans le cas oÃ¹ nous les avons traitÃ©es en masse. <br><br>  Qu'est-ce que le traitement en vrac?  Poller va au cache de configuration et prend la tÃ¢che non pas pour une mÃ©trique, mais pour 4 ou 8 000.  En mÃªme temps, il a une autre merveilleuse opportunitÃ©: il peut dÃ©sormais effectuer des sondages de maniÃ¨re asynchrone, car il a obtenu 4 000 mÃ©triques ... Pourquoi font-ils l'un aprÃ¨s l'autre?  Vous pouvez tout demander immÃ©diatement! <br><br><h3>  L'interrogation asynchrone est plus efficace que le proxy! </h3><br>  Pour les principaux types utilisÃ©s par le fournisseur - ce sont SNMP et AGENT, nous avons rÃ©Ã©crit l'interrogation en mode asynchrone, et agrÃ©gÃ© cela a donnÃ© une augmentation de la vitesse de 100 Ã  200 fois.  Nous avions 15 procurations, nous les avons divisÃ©es en 150 - elles Ã©taient complÃ¨tement parties.  En consÃ©quence, tout cela s'est transformÃ© en deux banques, qui ne sont nÃ©cessaires que pour la rÃ©serve: <br><br><img src="https://habrastorage.org/webt/ax/7z/lk/ax7zlkk-yg9oafyuruz9blq7l-q.jpeg"><br><br>  Banque Uniprocesseur (un Xeon 1280 coÃ»te).  Voici mon temps: <br><br><img src="https://habrastorage.org/webt/zq/qf/nc/zqqfncyu4fxo38fiegvr0kunhhw.jpeg"><br><br>  Environ 60% est gratuit, mais cette sonnerie de 60% Ã  40% exÃ©cute des scripts pÃ©riodiques sur la machine elle-mÃªme (scripts externes).  Ils peuvent Ãªtre optimisÃ©s jusqu'Ã  la crÃ©ation de problÃ¨mes. <br><br>  L'Ã©chelle est quelque chose comme ceci: <br><br><img src="https://habrastorage.org/webt/km/pa/nj/kmpanjjtpdm3660spp8haz7ypo0.jpeg"><br><br>  Ce sont 62 000 hÃ´tes, environ 5 millions de mÃ©triques.  Notre besoin actuel est d'environ 20 000 mesures par seconde. <br><br>  Eh bien, comme tout?  Nous avons rÃ©solu les problÃ¨mes de performances, l'historique Ã©tendu, le sondage est gÃ©nial.  Le problÃ¨me est-il rÃ©solu?  Pas vraiment ... Tout serait trop simple. <br><br>  J'ai jouÃ© un tour sur le tableau prÃ©cÃ©dent (pas tous montrÃ©): <br><br><img src="https://habrastorage.org/webt/ds/ag/k5/dsagk5zrh8bog_ogrq35w1uvawy.jpeg"><br><br>  Il y a deux problÃ¨mes.  Je veux dire: "Fous, routes."  Il y a un facteur humain, il y a du matÃ©riel. <br><br>  Un serveur ne suffit toujours pas.  En environ un an de fonctionnement, il y a eu deux cas de problÃ¨mes matÃ©riels - un lecteur SSD et autre chose.  La plupart des problÃ¨mes sont le facteur humain lorsque les gens font des tests.  Dans notre entreprise, Zabbix est utilisÃ© comme un service: tous les dÃ©partements peuvent y Ã©crire quelque chose qui leur est propre. <br><br>  J'aimerais Ã©largir.  Je voudrais ne pas dÃ©pendre d'une seule boÃ®te.  Je voulais que nous soyons encore plus forts.  Et je voudrais Ã©voluer selon le principe de mise Ã  l'Ã©chelle.  Il n'y a mÃªme rien Ã  discuter ici: croÃ®tre, augmenter la capacitÃ© d'une boÃ®te, n'a plus d'importance depuis 20 ans dÃ©jÃ . <br><br><img src="https://habrastorage.org/webt/yl/hy/vn/ylhyvn7za42rcr2allqhjzlmw2i.jpeg"><br><br><h3>  Le cluster a demandÃ© ... </h3><br>  Quelque part en dÃ©cembre, la premiÃ¨re version est apparue.  Une unitÃ© de cluster atomique est ce qui est traitÃ© sur un hÃ´te distinct.  L'hÃ´te a Ã©tÃ© sÃ©lectionnÃ©. <br><br><img src="https://habrastorage.org/webt/9j/yo/sy/9jyosycj6yj7ecx7k5mn0esrfnu.jpeg"><br><br>  Le fait est que dans Zabbix, il existe des connexions assez fortes entre les Ã©lÃ©ments qui peuvent Ãªtre sur le mÃªme hÃ´te, c'est-Ã -dire que les dÃ©clencheurs peuvent Ãªtre connectÃ©s, ils peuvent Ãªtre traitÃ©s ensemble en prÃ©traitement.  Mais entre les hÃ´tes, la connectivitÃ© n'est pas si Ã©levÃ©e, il est donc normal d'utiliser ce cluster entre les nÅ“uds du cluster - il y aura beaucoup de trafic lÃ -bas.  La tÃ¢che principale des clusters est de convenir entre eux qui est engagÃ© dans quels hÃ´tes. <br><br>  Je voudrais contourner notre limite maximale de 60 Ã  70 000 mÃ©triques, car l'appÃ©tit vient avec l'alimentation.  Nous avons des gars qui sont engagÃ©s dans la QoE ... QualitÃ© de l'expÃ©rience - une analyse du fonctionnement d'Internet pour les abonnÃ©s basÃ©e sur les mÃ©triques de transit, c'est-Ã -dire que vous fournissez toutes les mÃ©triques TCP Ã  1,5 million de personnes, en les injectant dans la surveillance - il y a beaucoup de donnÃ©es. <br><br>  Et je voulais de la fiabilitÃ©.  Je le voulais si quelque chose se produisait ... L'officier de quart a appelÃ©, a dit: "Nous avons des problÃ¨mes avec le serveur", l'a Ã©teint, nous le dÃ©couvrirons demain. <br><br><h3>  Premier cluster </h3><br>  La premiÃ¨re version a Ã©tÃ© implÃ©mentÃ©e sur la base de etcd: <br><br><img src="https://habrastorage.org/webt/oh/kr/qu/ohkrqu9wh-gldd6q6yheyoztro4.jpeg"><br><br>  Etcd est un stockage de valeurs-clÃ©s distribuÃ© utilisÃ© dans de nombreux projets progressifs (pour autant que je sache, dans Kubernetes).  Tout Ã©tait super.  Etcd fournit des outils trÃ¨s intÃ©ressants - par exemple, il rÃ©sout le problÃ¨me du choix du serveur principal.  Mais un tel problÃ¨me ... <br><br>  Nous avions un "Zabbix" classique Ã  trois liens: "web" - la base - le serveur lui-mÃªme.  Et nous y avons ajoutÃ© "Clickhouse", et maintenant nous avons ajoutÃ© etcd aussi.  Les administrateurs ont commencÃ© Ã  se gratter derriÃ¨re la tÃªte: il y a trop de dÃ©pendances ici - ce ne sera probablement pas fiable.  Dans le processus de dÃ©veloppement, une autre chose est devenue claire: dans Zabbix lui-mÃªme, il existe dÃ©jÃ  un moyen intÃ©grÃ© de communication interserveur, il est juste utilisÃ© entre le serveur et le proxy, le soi-disant processus d'interrogation du proxy: <br><br><img src="https://habrastorage.org/webt/4x/cf/jh/4xcfjhgqaf-2h33nnzrjduw8rn8.jpeg"><br><br>  C'est assez cool pour la communication interserveur avec des changements minimes.  Cela a permis Ã  etcd de ne pas utiliser (au moins temporairement), de simplifier considÃ©rablement le code, et surtout, de travailler sur du code qui a Ã©tÃ© vÃ©rifiÃ© (il semble que ce code ait 5 ou 7 ans). <br><br><h3>  Comment les serveurs sont-ils coordonnÃ©s dans un cluster? </h3><br>  La coordination se fait par type, comme le protocole IGP.  Pour que les serveurs aient la prioritÃ© (je vais maintenant dire pourquoi c'est nÃ©cessaire) et pour Ã©viter les conflits dans la base de donnÃ©es SQL lors de l'Ã©criture des journaux, chaque serveur se voit attribuer un identifiant (jusqu'Ã  prÃ©sent manuellement) - c'est un nombre de 0 Ã  63 (63 - c'est juste une constante, peut-Ãªtre plus): <br><br><img src="https://habrastorage.org/webt/kb/ei/xf/kbeixf3xjk6fo4oucwf9yk9yi9c.jpeg"><br><br>  Le serveur avec l'identifiant maximum devient le "maÃ®tre".  Lorsque nous avons lancÃ© nos premiers clusters de tests, la premiÃ¨re chose que nos administrateurs ont dit Ã©tait: Â«Wow!  Et mettons-les sur diffÃ©rents sites.  Eh bien, gÃ©nial! Â»(Nous y reviendrons).  Et lorsque quelqu'un aura distribuÃ© des clusters, il sera possible de contrÃ´ler la redistribution de la topologie: oÃ¹ ira le rÃ´le du Â«maÃ®treÂ» en cas de chute du serveur principal Â«ZabbixÂ»: <br><br><img src="https://habrastorage.org/webt/fa/n8/nx/fan8nxkovjbepqscnyljtfbc00o.jpeg"><br><br>  Dans ce cas, comme ceci: <br><br><img src="https://habrastorage.org/webt/5l/qu/ou/5lquouk5h0tkzky-e6_m8dfdkn4.jpeg"><br><br><h3>  Stepping </h3><br>  Dans le Zabbix d'origine, cela se fait comme ceci: le serveur lui-mÃªme est responsable de la gÃ©nÃ©ration des index d'incrÃ©mentation automatique.  Pour empÃªcher de nombreuses instances de marcher sur les talons les uns des autres (afin de ne pas crÃ©er de journaux avec les mÃªmes index), le pas est utilisÃ©: Â«ZabbixÂ» avec l'identifiant Â«1Â» gÃ©nÃ©rera des multiples de un - 1, 11, 21;  avec l'identifiant "7" - 7, 17, 27 (avec des nuances). <br>  Nous avons conduit avec des modificateurs. <br><br><img src="https://habrastorage.org/webt/t3/fo/2-/t3fo2-qoqlx0zwmxds7vc_7y2pw.jpeg"><br><br><h3>  Comment les serveurs interagissent-ils entre eux? </h3><br>  Il s'agit de l'hÃ©ritage des paquets Hello IGP toutes les 5 secondes.  Les serveurs savent donc qu'ils ont des voisins.  Le Â«maÃ®treÂ» sait donc qu'il y a des voisins Ã  proximitÃ©, et sur cette base, le Â«maÃ®treÂ» dÃ©cide quels hÃ´tes peuvent Ãªtre distribuÃ©s sur quels serveurs. <br><br><img src="https://habrastorage.org/webt/um/vm/pn/umvmpn5x2hljbhbuzfp6b3umqpo.jpeg"><br><br>  En consÃ©quence, il existe une configuration.  Selon l'ancienne mÃ©moire, je l'appelle topologie.  Une topologie est essentiellement une liste de serveurs et d'hÃ´tes qui leur appartiennent. <br><br>  Le protocole est simple - c'est JSON: <br><br><img src="https://habrastorage.org/webt/qd/xx/rz/qdxxrzw2rj3akue1h8nwcqum1cc.jpeg"><br><br>  C'est Ã©galement l'hÃ©ritage du proxy Zabbix et de la communication du serveur Zabbix.  En gÃ©nÃ©ral, cela n'a aucun sens d'utiliser autre chose.  La seule chose est que dans le cas de Zabbix il y a 4 octets (ZBXD), mais ce n'est pas le point. <br><br>  Dans le paquet hello, l'identifiant du serveur est transmis: lorsque le serveur envoie le paquet, il indique son identifiant et sa version de la topologie - de cette faÃ§on, les serveurs dÃ©couvrent rapidement qu'il existe une nouvelle version de la topologie et sont mis Ã  jour trÃ¨s rapidement. <br><br>  En fait, la topologie elle-mÃªme n'est qu'une arborescence, une liste de serveurs.  Pour chaque serveur, une liste d'hÃ´tes qu'il prend en charge: <br><br><img src="https://habrastorage.org/webt/xp/dv/ec/xpdvecth8pp7eaukwxmykgysqiu.jpeg"><br><br>  Et puis un problÃ¨me intÃ©ressant se pose. <br><br><h3>  Il y a une telle phrase magique - surveiller les domaines </h3><br>  Ã€ quoi Ã§a sert?  Dans le Zabbix classique, tout Ã©tait simple - une attitude sans ambiguÃ¯tÃ©: cet hÃ´te est surveillÃ© par ce proxy, ce proxy donne des donnÃ©es au serveur.  Si le proxy n'a pas Ã©tÃ© installÃ© (ou n'est pas nÃ©cessaire), ce serveur surveille tous les hÃ´tes: <br><br><img src="https://habrastorage.org/webt/x2/ly/fh/x2lyfh4prsystuyggxb0elwclx4.jpeg"><br><br>  Quand nous avons beaucoup de serveurs, que faire?  De plus, il peut y avoir un problÃ¨me avec le fait que nous avons des serveurs gÃ©ographiquement distribuÃ©s, et le serveur dans un bureau qui fonctionne lentement Ã  Kemerovo commencera Ã  essayer de surveiller toute l'infrastructure de Novossibirsk. <br><br><img src="https://habrastorage.org/webt/ep/06/cy/ep06cyiqx3dx4kkyfer1wcgn8um.jpeg"><br><br>  Nous n'en voulons pas.  Nous voulons avoir une sorte de mÃ©canisme pour que tous les serveurs, mais ceux que nous avons sÃ©lectionnÃ©s (Ã©ventuellement en fonction de la gÃ©ographie) puissent surveiller un hÃ´te particulier.  En mÃªme temps, nous voulons gÃ©rer cela et nous voulons que ce soit simple.  Pour cela, l'idÃ©e de surveiller les domaines a Ã©tÃ© inventÃ©e.  En fait, ce sont de simples groupes - simplement, il y a dÃ©jÃ  des groupes dans le dossier. <br>  Et quand j'ai fait cela, les gars de l'opÃ©ration m'ont parlÃ© - ils ont dit: Â«Les groupes nous confondent beaucoup.  Nous commenÃ§ons toujours Ã  penser Ã  des groupes normaux. Â»  Par consÃ©quent, ce nom: domaines de surveillance. <br><br>  Les hÃ´tes se rapportent sans ambiguÃ¯tÃ©: un hÃ´te - un domaine: <br><br><img src="https://habrastorage.org/webt/6x/hi/z5/6xhiz5ocnajnjzoxin17log0liq.jpeg"><br><br>  Le domaine hÃ´te peut inclure n'importe quel nombre de serveurs.  Les serveurs peuvent Ãªtre dans n'importe quel nombre de domaines.  C'est une chose trÃ¨s flexible.  Afin d'Ã©tendre la flexibilitÃ© et de briser complÃ¨tement le cerveau, il existe Ã©galement un domaine par dÃ©faut: <br><br><img src="https://habrastorage.org/webt/ad/zl/2x/adzl2xgso7st_v9c7vg9-fyryvg.jpeg"><br><br>  Les serveurs qui sont membres du domaine par dÃ©faut sont surveillÃ©s par tous les hÃ´tes qui n'ont pas de serveurs actifs ou qui n'ont pas de domaine de surveillance. <br><br>  Cela nous permet simplement de lier topologiquement les hÃ´tes Ã  certains serveurs et de contrÃ´ler la faÃ§on dont les hÃ´tes sont distribuÃ©s en cas de panne d'un serveur: <br><br><img src="https://habrastorage.org/webt/fm/og/nr/fmognrgop74jgnsqkwtxdzddqgw.jpeg"><br><br>  Le prochain problÃ¨me que nous avons rencontrÃ© ... <br><br><h3>  Cluster: Penser diffÃ©remment </h3><br>  Quand nous avons beaucoup de serveurs, il y a de nouvelles opportunitÃ©s pour construire un cluster, pour construire une topologie.  C'est un tel classique lorsque nous avons une sorte de site central et qu'il y en a Ã  distance;  ou, disons, un proxy oÃ¹ la charge est dÃ©lÃ©guÃ©e: <br><br><img src="https://habrastorage.org/webt/rn/ar/qd/rnarqdcbs1knledntrvqoibsozs.jpeg"><br><br>  Dans le cas du cluster Zabbix, il peut Ãªtre implÃ©mentÃ© de deux maniÃ¨res.  Vous pouvez suivre la voie classique: il suffit de doubler l'infrastructure.  Au centre, nous avons deux serveurs qui forment un cluster, peuvent rÃ©organiser les hÃ´tes ou assumer la charge si le voisin tombe.  En consÃ©quence, vous pouvez lever des procurations supplÃ©mentaires sur les mÃªmes serveurs - nous obtenons une double rÃ©serve: <br><br><img src="https://habrastorage.org/webt/bx/es/g4/bxesg4ijxn4b9pirro22u3-omoa.jpeg"><br><br>  Vous pouvez utiliser les nouvelles "fonctionnalitÃ©s" et faire ceci: <br><br><img src="https://habrastorage.org/webt/v0/-o/ki/v0-okiiibpq1yyurxipbugjqfwq.jpeg"><br><br>  L'essentiel n'est pas d'aller dans une situation oÃ¹ un serveur gÃ©ographiquement distant surveille une grande infrastructure Ã  un autre endroit.  Il s'agit davantage d'un problÃ¨me d'administration (je l'appelle entreprise) car il s'agit d'un problÃ¨me de configuration. <br><br><h3>  Cluster: split brain et point de vue </h3><br>       ,    : <br><br><ul><li> split brain; </li><li> point of view ( ). </li></ul><br>   . Split brain â€“       ,         .     ,  -  â€“     ? ,      ,            ( ). <br><br>  point of view  : ,      ,          ,    .     . ,   RTT ,    . <br>       : <br><br><img src="https://habrastorage.org/webt/ab/qa/gc/abqagc2w3ky3kufvm6omjfsjheo.jpeg"><br><br>      ,   .   ,   ,       .    ,   â€“   .    ,       ,  ,  . <br><br><h3>  SQL- </h3><br> ,     ,    ,       .       .  , ,       â€¦      .    . <br><br> -,  , ,  -      â€“    . , Galera  MySQL. <br><br>        PostgreSQL.    Â«Â»   :    ,   ,           â€“   . Â«Â», ,    . <br><br><h3>    ? </h3><br>        ,    : <br><br><img src="https://habrastorage.org/webt/q4/u1/4s/q4u14s11rryz7afktbdfkn0s9sy.jpeg"><br><br><img src="https://habrastorage.org/webt/pj/0p/uq/pj0puqdn9zkhsm2uwolcaw40a1w.jpeg"><br><br>           â€“  .      : <br><br><ul><li>  -  (Logs),    .   problems, events  events recovery.  ,   â€“   ,   . </li><li>  15   (State).  â€“     (    â€“   â€“ Â«Â»   ).        .   ,  ;   â€“        â€¦ </li><li>  -       (Configuration update). </li></ul><br>   Â«.    Â«Â»,    SQL-: <br><br><img src="https://habrastorage.org/webt/ne/if/cc/neifccswcjikrxfetvhuip-agva.jpeg"><br><br> -,       : <br><br><img src="https://habrastorage.org/webt/4f/o4/ti/4fo4ticljuykrkh6zjmop4qc5sg.jpeg"><br><br>  .          -,   ,   â€¦    â€“   ,   2  !    : Â«  ,    Â».  -  ,       ,    . <br><br> .         ,           : <br><br><img src="https://habrastorage.org/webt/lj/wh/ug/ljwhugg1tdfbehcxjcrloiolwks.jpeg"><br><br>   ,  .  SQL-     .     ,   SQL-.       (   -  ),        Â«Â» (        ). â€¦ <br><br><h3> .  L'installation </h3><br> , , Â«Â»  . .       ,   ? <br><br><img src="https://habrastorage.org/webt/ci/yo/ne/ciyonezibe3bfla-7xodyyx_zwg.jpeg"><br><br>    Â«Â»- (. .  Â«Â» daemon).       (   ):   (  1  63,      Â«Â»)    (   ,       ). <br><br>      ServerIP  IP-.    ,      ,     IP-   . -    ,         proxy poller,    trapper  hello-,  proxy poller  . <br><br>   . ,       ,    Â« Â»: <br><br><img src="https://habrastorage.org/webt/dr/1z/pi/dr1zpi2hbe9penikzazqcycw-oa.jpeg"><br><br>      : <br><br><img src="https://habrastorage.org/webt/kf/pg/jb/kfpgjbghzsonjmsizlbcatyo3ja.jpeg"><br><br>  ,       default.       .  â€“   ,     IP-,    ,      (  ).      Â«Â» â€“   default. <br><br>  -, . <br><br><ul><li>   . </li><li> ,      : Â«    ,    Â».   . </li><li>  - ,   . </li><li>     ,    hello-time,  : Â«   Â»;    . </li><li>  . </li></ul><br> ,   , ,   .               30-40 .   ,      ,    ,   . <br><br><h3>    </h3><br>            ,      .      -   : Â«    ,  !Â»  -! <br><br><img src="https://habrastorage.org/webt/nq/ta/ah/nqtaahv3gipyew36m8nhrmwukew.jpeg"><br><br>   â€“  :  -  ,  - , ,  GitLab, CI/CD,   . , ,  â€“    . <br><br>  ,      ,       â€“  4.0.9  (4.2   ).   Roadmap â€“        -. -,    Â«Â»;   ,   RPM'. <br><br><img src="https://habrastorage.org/webt/er/kn/2r/erkn2r6h9law8l3xynd6i8_jwne.jpeg"><br><br>      (   )  Â«Â»       Â«Â»-.   .         ,    .   â€“   :   , -   â€¦  ? !..   Â«Â»,  . <br><br>        SQL-   ,    ,   . History Storage. <br><br><h3>  Les rÃ©fÃ©rences </h3><br>     5 .      . <br> -, ,   ,      , . .      -. <br><br><img src="https://habrastorage.org/webt/zp/3a/gr/zp3agr9epncvnjndf-hgpwt_zke.jpeg"><br><br><h3>     </h3><br> ,     ?     !  , , ,  .   -   - ,         . ,         ,        .       ,    : <br><br><img src="https://habrastorage.org/webt/ac/fo/pf/acfopfqzuwvtfvtni2n9rjx-ywy.jpeg"><br><br>  . Â«Â»- ,      . <br><br><ul><li>    ,         ,     , . </li><li>   Â«Â»  :  ,       Configuration Cache,   . </li><li>   ,           ,     .        ,    ,   . </li><li>   -    ,   .      ,    ,    .    200     ,     â€“   . </li></ul><br><h3>   </h3><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Remarque: le proxy passif n'est pas encore pris en charge! </font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J'ai supprimÃ© le code. </font><font style="vertical-align: inherit;">Cela est dÃ» au fait qu'il est difficile pour les gens de crÃ©er un autre mÃ©canisme, quel serveur sera toujours responsable de ce proxy. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les proxys actifs eux-mÃªmes vont aux serveurs. </font><font style="vertical-align: inherit;">Il existe une option Serveur pour cela (proxy standard). </font><font style="vertical-align: inherit;">Le proxy modifiÃ© a l'option Serveurs: </font></font><br><br><img src="https://habrastorage.org/webt/jm/ym/6t/jmym6tfhwlg_ix6o5xnnkbwphri.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Et que fait un tel serveur modifiÃ©? </font><font style="vertical-align: inherit;">Il conserve une connexion KPI avec tous les serveurs qui lui sont spÃ©cifiÃ©s; </font><font style="vertical-align: inherit;">demande la configuration, envoie les donnÃ©es au premier serveur disponible de la liste. </font><font style="vertical-align: inherit;">Cela rÃ©sout le problÃ¨me. </font><font style="vertical-align: inherit;">Supposons que si un proxy est configurÃ© sur le serveur Zabbix et que le serveur Zabbix est tombÃ©, il y en a un autre dans le cluster afin de ne pas rester sans proxy; </font><font style="vertical-align: inherit;">alors le proxy se connecte Ã  un autre.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Des questions </font></font></h3><br>  Question du public (ci-aprÃ¨s - A): - Je voudrais clarifier comment les choses se passent entre les serveurs?  Avec quel protocole communiquent-ils?  Y a-t-il une sorte de sÃ©curitÃ©?  Parce que ce nâ€™est pas trÃ¨s Â«sÃ©curisÃ©Â» pour amener la communication entre les serveurs sur Internetâ€¦ Comment Ã§a se passe? <br><br>  <b>MM:</b> - Je pense que c'est un candidat Ã  la meilleure question - au point!  En fait, lorsque nous sommes passÃ©s Ã  la communication standard, les serveurs de leur communication interserveur ont hÃ©ritÃ© de tous les jetons de protocole de communication qui existent entre le serveur et le proxy.  Je vais prÃ©ciser: il y a chiffrement, compression des donnÃ©es.  S'il vous plaÃ®t - de la mÃªme maniÃ¨re, tout est configurÃ© via le Web, car il est configurÃ© de maniÃ¨re standard pour le serveur et le proxy;  tout fonctionnera. <br><br>  <b>R:</b> - Comment Hauskiper fonctionne-t-il pour vous dans le cas de Clickhouse? <br><br>  <b>MM:</b> - Dans le Â«ZabbixÂ» standard, il n'y a pas d'interface entre la Â«femme de mÃ©nageÂ» et l'interface historique, c'est-Ã -dire que l'interface historique ne prend pas en charge la rotation des donnÃ©es (ElasticSearch, par exemple, ne prend pas en charge).  Peut-Ãªtre qu'en 4.2 c'est (je n'ai pas regardÃ©), mais jusqu'Ã  prÃ©sent en 4.0.9. <br><br>  Rendez-le facile!  Le nouveau "Clickhouse" a une partition.  Je voudrais le faire en dÃ©sengageant les partitions obsolÃ¨tes.  Il est clair qu'il n'y aura pas de rotation au niveau des Ã©lÃ©ments individuels, mais il y a une astuce dans Zabbix: vous pouvez spÃ©cifier des valeurs globales (par exemple, stocker l'historique complet pendant pas plus de 90 jours) - vous pouvez effacer tous les Ã©lÃ©ments, l'historique complet de ces valeurs globales .  Et ce sera fait!  Il y a plus sur ce sujet sur Gitlab. <br><br>  Nous aimerions faire le droit architectural: faut-il Ã©tendre lâ€™interface dâ€™histoire, de maniÃ¨re Ã  ce que ce soit essentiellement ... En gÃ©nÃ©ral, je ne veux pas laisser de dettes techniques, mais ce sera fait.  Parce que c'est nÃ©cessaire, plus Â«ClickhouseÂ» a commencÃ© Ã  supporter. <br><br>  <b>R:</b> - Comment vous sentez-vous Ã  ce sujet?  Il s'avÃ¨re que vous faites beaucoup de travail non-fournisseur. <br><br>  <b>MM:</b> - Je ne l'ai probablement pas dit trÃ¨s correctement.  C'est mon hobby!  Je ne suis pas vraiment un spÃ©cialiste technique - je suis un manager.  Dans mon temps libre, je pratique. <br><br>  <b>R:</b> - Je pensais que vous faisiez cela dans le cadre de votre activitÃ© principale ... <br><br>  <b>MM:</b> - Les affaires me donnent un endroit cool pour tester.  En fait, je recommande fortement - cela soulage le cerveau.  Quelque part sur la Â«choseÂ» managÃ©riale, je dirais ceci - quand vous pouvez passer des problÃ¨mes humains Ã  ceux-ci.  Ils sont tellement cool rÃ©solus!  Ce sont des problÃ¨mes techniques.  Vous avez programmÃ©, et cela fonctionne comme vous avez programmÃ©!  C'est dommage que les gens ne devraient pas faire Ã§a. <br><br>  <b>R:</b> - Ecrivez-vous Ã  Â«ClickhouseÂ» via un proxy ou directement? <br><br>  <b>MM:</b> - Directement.  En fait, l'interface historique modifiÃ©e, qui est utilisÃ©e pour "Elastix", est Ã©galement hÃ©ritÃ©e.  L'url est utilisÃ©e, c'est-Ã -dire via l'interface http "Zabbiks" envoie "Clickhouse".  Ce qui est cool, Zabbix agrÃ¨ge quand il y a un gros flux historique, des milliers de mÃ©triques dans un pack, et cela tombe trÃ¨s cool sur le Clickhouse. <br><br>  <b>R:</b> - En fait, il Ã©crit bachi pour lui? <br><br>  <b>MM:</b> - Oui.  Une requÃªte SQL exÃ©cutÃ©e par l'URL contient gÃ©nÃ©ralement un millier de mesures.  Admins "Clickhouse" tout simplement heureux. <br><br>  PrÃ©sentateur: - Ceci est la fin du programme dans cette salle.  Il y a un programme du soir qui est organisÃ©, et il y a quelque chose que vous seul pouvez faire.  Et je suggÃ¨re, pendant que vous communiquerez les uns avec les autres, de rÃ©flÃ©chir aux choses intÃ©ressantes que vous pouvez ... Lorsque vous vous parlez de vos cas, c'est trÃ¨s probablement ce dont vous pouvez parler.  En discutant les uns avec les autres, vous pouvez trouver juste pour trouver un aperÃ§u - le comitÃ© du programme acceptera votre candidature, examinera et aidera Ã  en faire une bonne histoire.  Peut-Ãªtre avez-vous une sorte d'histoire Ã  propos de la collaboration avec le comitÃ© de programme? <br><br>  <b>MM:</b> - En fait, beaucoup de commentaires sont donnÃ©s.  J'ai eu tellement de chance: une personne du comitÃ© du programme habite Ã  Chelyabinsk et Highload est la seule confÃ©rence qui travaille si Ã©troitement avec les confÃ©renciers.  Je n'ai jamais rien vu de tel ailleurs.  C'est trÃ¨s bÃ©nÃ©fique!  DiffÃ©rentes Ã©tapes: les gars regardent la vidÃ©o, font des commentaires sur les diapositives - Ã§a se passe beaucoup dans le sujet (orthographe, fautes d'Ã©criture).  TrÃ¨s cool!  Je le recommande!  Essayez-vous! <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wbIpn44Z2_8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Un peu de publicitÃ© :) </h3><br>  Merci de rester avec nous.  Aimez-vous nos articles?  Vous voulez voir des matÃ©riaux plus intÃ©ressants?  Soutenez-nous en passant une commande ou en recommandant Ã  vos amis <a href="https://ua-hosting.company/cloudvps/nl">des VPS basÃ©s sur le cloud pour les dÃ©veloppeurs Ã  partir de 4,99 $</a> , un <b>analogue unique de serveurs d'entrÃ©e de gamme que nous avons inventÃ©s pour vous:</b> <a href="https://habr.com/company/ua-hosting/blog/347386/">Toute la vÃ©ritÃ© sur les VPS (KVM) E5-2697 v3 (6 cÅ“urs) 10 Go DDR4 480 Go SSD 1 Gbit / s Ã  partir de 19 $ ou comment diviser le serveur?</a>  (les options sont disponibles avec RAID1 et RAID10, jusqu'Ã  24 cÅ“urs et jusqu'Ã  40 Go de DDR4). <br><br>  <b>Dell R730xd 2 fois moins cher au centre de donnÃ©es Equinix Tier IV Ã  Amsterdam?</b>  Nous avons seulement <b><a href="https://ua-hosting.company/serversnl">2 x Intel TetraDeca-Core Xeon 2x E5-2697v3 2.6GHz 14C 64GB DDR4 4x960GB SSD 1Gbps 100 TV Ã  partir de 199 $</a> aux Pays-Bas!</b>  <b><b>Dell R420 - 2x E5-2430 2.2Ghz 6C 128GB DDR3 2x960GB SSD 1Gbps 100TB - Ã  partir de 99 $!</b></b>  Pour en savoir plus sur la <a href="https://habr.com/company/ua-hosting/blog/329618/">crÃ©ation d'un bÃ¢timent d'infrastructure.</a>  <a href="https://habr.com/company/ua-hosting/blog/329618/">classe utilisant des serveurs Dell R730xd E5-2650 v4 coÃ»tant 9 000 euros pour un sou?</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr485534/">https://habr.com/ru/post/fr485534/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr485524/index.html">Le condensÃ© de matÃ©riaux intÃ©ressants pour le dÃ©veloppeur mobile # 330 (du 20 au 26 janvier)</a></li>
<li><a href="../fr485526/index.html">Qui et pourquoi veut rendre Internet Â«partagÃ©Â»</a></li>
<li><a href="../fr485528/index.html">Comment conclure un projet de dÃ©veloppement logiciel de la bonne faÃ§on</a></li>
<li><a href="../fr485530/index.html">Apprentissage guidÃ©</a></li>
<li><a href="../fr485532/index.html">Guide d'entrevue pour les programmeurs qui ne les comprennent pas</a></li>
<li><a href="../fr485536/index.html">Est-il possible de pirater un avion - 2</a></li>
<li><a href="../fr485538/index.html">Zabbix: surveillez tout de suite (sur l'exemple de Redis)</a></li>
<li><a href="../fr485542/index.html">Ajouter des graphiques Ã  Notion</a></li>
<li><a href="../fr485544/index.html">Les Ã©checs comme systÃ¨me dynamique</a></li>
<li><a href="../fr485546/index.html">L'apocalypse arrive</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>