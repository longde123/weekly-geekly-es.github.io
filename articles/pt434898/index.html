<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîì üíî üë®üèº Grasp2Vec: aprendendo a representar objetos por meio da captura de autoaprendizagem üë©üèΩ‚Äçü§ù‚Äçüë®üèæ üòÑ üë©‚Äçüë©‚Äçüëß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pessoas de uma idade surpreendentemente jovem j√° s√£o capazes de reconhecer seus objetos favoritos e busc√°-los, apesar do fato de n√£o serem ensinados e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grasp2Vec: aprendendo a representar objetos por meio da captura de autoaprendizagem</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434898/"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br>  Pessoas de uma idade surpreendentemente jovem j√° s√£o capazes de reconhecer seus objetos favoritos e busc√°-los, apesar do fato de n√£o serem ensinados especificamente sobre isso.  De acordo com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">estudos sobre o</a> desenvolvimento de habilidades cognitivas, a possibilidade de interagir com objetos do mundo ao nosso redor desempenha um papel cr√≠tico no desenvolvimento de habilidades como detectar e manipular objetos - por exemplo, captura direcionada.  Interagindo com o mundo exterior, as pessoas podem aprender corrigindo seus pr√≥prios erros: sabemos o que fizemos e aprendemos com os resultados.  Na rob√≥tica, esse tipo de treinamento com autocorre√ß√£o de erros √© estudado ativamente, pois permite que os sistemas rob√≥ticos aprendam sem uma enorme quantidade de dados de treinamento ou ajuste manual. <br><br>  N√≥s, no Google, inspirados no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conceito de persist√™ncia de objetos</a> , oferecemos o sistema <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Grasp2Vec</a> - um algoritmo simples, por√©m eficaz, para construir a representa√ß√£o de objetos.  O Grasp2Vec √© baseado em um entendimento intuitivo de que uma tentativa de elevar qualquer objeto nos dar√° algumas informa√ß√µes - se o rob√¥ pegar o objeto e peg√°-lo, ele precisar√° estar neste local antes de ser capturado.  Al√©m disso, o rob√¥ sabe que, se o objeto capturado estiver em sua captura, significa que o objeto n√£o est√° mais no local onde estava.  Usando essa forma de auto-aprendizado, o rob√¥ pode aprender a reconhecer um objeto devido √† mudan√ßa visual na cena ap√≥s a captura. <br><a name="habracut"></a><br>  Com base em nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">colabora√ß√£o com a X Robotics</a> , onde v√°rios rob√¥s foram treinados simultaneamente para capturar objetos dom√©sticos usando apenas uma c√¢mera como fonte de dados de entrada, usamos a captura rob√≥tica para capturar "inadvertidamente" objetos, e essa experi√™ncia nos permite ter uma id√©ia rica do objeto.  Essa ideia j√° pode ser usada para adquirir a capacidade de "captura intencional", quando o bra√ßo do rob√¥ pode levantar objetos sob demanda. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Criando uma fun√ß√£o de recompensa perceptiva </h2><br>  Em uma plataforma de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aprendizado por refor√ßo, o</a> sucesso de uma tarefa √© medido por meio de uma fun√ß√£o de recompensa.  Ao maximizar as recompensas, os rob√¥s aprendem v√°rias habilidades de captura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do zero</a> .  Criar uma fun√ß√£o de recompensa √© f√°cil quando o sucesso pode ser medido com simples leituras do sensor.  Um exemplo simples √© um bot√£o que transfere uma recompensa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">diretamente para a entrada de um rob√¥</a> , clicando nele. <br><br>  No entanto, criar uma fun√ß√£o de recompensa √© muito mais complicado quando o crit√©rio para o sucesso depende de uma compreens√£o perceptiva da tarefa.  Considere o problema de captura em um exemplo em que o rob√¥ recebe uma imagem do objeto desejado mantido na captura.  Depois que o rob√¥ tenta capturar o objeto, ele examina o conte√∫do da captura.  A fun√ß√£o de recompensa para esta tarefa depende da resposta √† quest√£o do reconhecimento de padr√µes: os objetos coincidem? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>√Ä esquerda, a al√ßa segura o pincel e v√°rios objetos s√£o vis√≠veis em segundo plano (um copo amarelo, um bloco de pl√°stico azul).</i>  <i>√Ä direita, a al√ßa segura o copo e o pincel est√° em segundo plano.</i>  <i>Se a imagem esquerda representasse o resultado desejado, uma boa fun√ß√£o de recompensa seria ‚Äúentender‚Äù que essas duas fotos correspondem a dois objetos diferentes.</i> <br><br>  Para resolver o problema de reconhecimento, precisamos de um sistema perceptivo que extraia conceitos significativos de objetos de imagens n√£o estruturadas (n√£o assinadas por pessoas) e aprenda a visualizar objetos sem um professor.  Essencialmente, os algoritmos de aprendizado sem professores funcionam criando suposi√ß√µes estruturais sobre os dados.  Sup√µe-se frequentemente que as imagens podem ser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">compactadas para um espa√ßo com menos dimens√µes</a> e os quadros de v√≠deo podem ser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">previstos a partir dos anteriores</a> .  No entanto, sem suposi√ß√µes adicionais sobre o conte√∫do dos dados, isso geralmente n√£o √© suficiente para aprender com representa√ß√µes n√£o relacionadas de objetos. <br><br>  E se us√°ssemos um rob√¥ para separar fisicamente objetos durante a coleta de dados?  A rob√≥tica oferece uma excelente oportunidade para aprender a representar objetos, pois os rob√¥s podem manipul√°-los, o que fornecer√° os fatores de varia√ß√£o necess√°rios.  Nosso m√©todo √© baseado na ideia de que capturar um objeto o remove da cena.  O resultado √© 1) uma imagem da cena antes da captura, 2) uma imagem da cena ap√≥s a captura e 3) uma vis√£o separada do objeto capturado. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>Esquerda - objetos para capturar.</i>  <i>No centro - ap√≥s a captura.</i>  <i>√Ä direita est√° o objeto capturado.</i> <br><br>  Se considerarmos uma fun√ß√£o interna que extrai um "conjunto de objetos" de imagens, ela deve preservar a seguinte rela√ß√£o de subtra√ß√£o: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>objetos antes da captura - objetos ap√≥s a captura = objeto capturado</i> <br><br>  Alcan√ßamos essa igualdade com a arquitetura convolucional e um algoritmo simples de aprendizado m√©trico.  Durante o treinamento, a arquitetura mostrada abaixo incorpora imagens antes e depois da captura em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mapa</a> denso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de propriedades espaciais</a> .  Esses mapas se transformam em vetores por meio de uma uni√£o m√©dia e a diferen√ßa entre os vetores "antes da captura" e "depois da captura" representa um conjunto de objetos.  Este vetor e a representa√ß√£o correspondente do vetor desse objeto percebido s√£o equacionados atrav√©s da fun√ß√£o de pares N. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br>  Ap√≥s o treinamento, nosso modelo naturalmente possui duas propriedades √∫teis. <br><br><h2>  1. Similaridade de objetos </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O coeficiente de cosseno da</a> dist√¢ncia entre as inser√ß√µes vetoriais nos permite comparar objetos e determinar se eles s√£o id√™nticos.  Isso pode ser usado para implementar a fun√ß√£o de recompensa por aprendizado refor√ßado e permite que os rob√¥s aprendam a capturar com exemplos sem marcar dados por humanos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2. Encontrar alvos </h2><br>  Podemos combinar mapas espaciais da cena e a incorpora√ß√£o de objetos para localizar o "objeto desejado" no espa√ßo da imagem.  Realizando a multiplica√ß√£o por elementos dos mapas de caracter√≠sticas espaciais e a correspond√™ncia vetorial do objeto desejado, podemos encontrar todos os pixels no mapa espacial que correspondem ao objeto de destino. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>Utilizando inlays Grasp2Vec para localizar objetos na cena.</i>  <i>Acima √† esquerda, h√° objetos na cesta.</i>  <i>Parte inferior esquerda - o objeto desejado a ser capturado.</i>  <i>O produto escalar do vetor do objeto de destino e os recursos espaciais da imagem nos fornecem um "mapa de ativa√ß√£o" por pixel (canto superior direito) da semelhan√ßa de uma determinada se√ß√£o da imagem com o alvo.</i>  <i>Este mapa pode ser usado para se aproximar do alvo.</i> <br><br>  Nosso m√©todo tamb√©m funciona quando v√°rios objetos correspondem ao destino, ou mesmo quando o destino consiste em v√°rios objetos (a m√©dia de dois vetores).  Por exemplo, nesse cen√°rio, o rob√¥ identifica v√°rios blocos laranja na cena. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>O "mapa de calor" resultante pode ser usado para planejar a aproxima√ß√£o do rob√¥ ao (s) objeto (s) alvo (s).</i>  <i>Combinamos a localiza√ß√£o do Grasp2Vec e o reconhecimento de exemplos com nossa pol√≠tica de "capturar qualquer coisa" e obtemos sucesso em 80% dos casos durante a coleta de dados e em 59% com novos objetos que o rob√¥ n√£o encontrou anteriormente.</i> <br><br><h2>  Conclus√£o </h2><br>  Em nosso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho,</a> mostramos como as habilidades de garras rob√≥ticas podem criar dados usados ‚Äã‚Äãpara ensinar representa√ß√µes de objetos.  Em seguida, podemos usar o treinamento de apresenta√ß√£o para adquirir rapidamente habilidades mais complexas, como capturar por exemplo, preservando todas as propriedades do treinamento para n√£o professores em nosso sistema de captura aut√¥nomo. <br><br>  Al√©m do nosso trabalho, v√°rios outros trabalhos recentes tamb√©m estudaram como a intera√ß√£o sem um professor pode ser usada para obter representa√ß√µes de objetos, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">capturando</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">empurrando</a> e outros tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">intera√ß√µes</a> com objetos no ambiente.  Estamos felizes em antecipar n√£o apenas o que o aprendizado de m√°quina pode dar √† rob√≥tica em termos de melhor percep√ß√£o e controle, mas tamb√©m o que a rob√≥tica pode dar ao aprendizado de m√°quina em termos de novos paradigmas de autoaprendizagem. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt434898/">https://habr.com/ru/post/pt434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt434888/index.html">Presente de Ano Novo do Distrito Bin√°rio</a></li>
<li><a href="../pt434890/index.html">O Qiwi Bank (JSC) atribui dinheiro aos usu√°rios</a></li>
<li><a href="../pt434892/index.html">C√≥digo de desenho no Swift, PaintCode</a></li>
<li><a href="../pt434894/index.html">A arte do xamanismo ou firmware personalizado para o Olinuxino. Parte 1</a></li>
<li><a href="../pt434896/index.html">Consumer Electronics Hall of Fame: As hist√≥rias dos melhores gadgets dos √∫ltimos 50 anos, Parte 1</a></li>
<li><a href="../pt434902/index.html">Criando um gerador de consultas personalizado no Spring Data Neo4j (parte 1)</a></li>
<li><a href="../pt434906/index.html">Testes em C ++ sem macros e mem√≥ria din√¢mica</a></li>
<li><a href="../pt434908/index.html">Educa√ß√£o para programadores - O qu√™? Onde Quando?</a></li>
<li><a href="../pt434912/index.html">O estoque anual da Porsche Taycan j√° est√° reservado, principalmente pelos propriet√°rios da Tesla</a></li>
<li><a href="../pt434924/index.html">O que ler sobre organiza√ß√£o de locais de trabalho, coworking e design de espa√ßos para trabalho remoto</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>