<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßû üçö üí≠ Suchen Sie mit einem Smartphone nach Text auf Schildern und Paketen üïØÔ∏è üíÄ üë©‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Problem der automatischen Suche nach Text in Bildern besteht zumindest seit Beginn der neunziger Jahre des letzten Jahrhunderts seit langem. Oldti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Suchen Sie mit einem Smartphone nach Text auf Schildern und Paketen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/472910/">  Das Problem der automatischen Suche nach Text in Bildern besteht zumindest seit Beginn der neunziger Jahre des letzten Jahrhunderts seit langem.  Oldtimer konnten sich an die weit verbreitete Verbreitung von ABBYY FineReader erinnern, der Dokumentenscans in ihre bearbeitbaren Versionen √ºbersetzen kann. <br><br>  An PCs angeschlossene Scanner funktionieren in Unternehmen hervorragend, aber der Fortschritt steht nicht still, und mobile Ger√§te haben die Welt erobert.  Der Aufgabenbereich f√ºr die Arbeit mit Text hat sich ebenfalls ge√§ndert.  Jetzt m√ºssen Sie den Text nicht auf perfekt geraden A4-Bl√§ttern mit schwarzem Text auf wei√üem Hintergrund suchen, sondern auf verschiedenen Visitenkarten, farbenfrohen Men√ºs, Ladenschildern und vielem mehr dar√ºber, was eine Person im Dschungel einer modernen Stadt treffen kann. <br><br> <a href=""><img src="https://habrastorage.org/webt/br/xk/fe/brxkfes7mckw4fwiswow98w4ajy.png"></a> <br>  <i>Ein echtes Beispiel f√ºr die Arbeit unseres neuronalen Netzwerks.</i>  <i>Das Bild ist anklickbar.</i> <br><br><h2>  Grundlegende Anforderungen und Einschr√§nkungen </h2><br>  Mit solch einer Vielzahl von Bedingungen f√ºr die Darstellung von Text k√∂nnen handgeschriebene Algorithmen nicht mehr fertig werden.  Hier kommen neuronale Netze mit ihrer F√§higkeit zur Verallgemeinerung zur Rettung.  In diesem Beitrag werden wir √ºber unseren Ansatz zur Erstellung einer neuronalen Netzwerkarchitektur sprechen, die Text auf komplexen Bildern mit guter Qualit√§t und hoher Geschwindigkeit erkennt. <br><a name="habracut"></a><br>  Mobile Ger√§te beschr√§nken die Wahl des Ansatzes zus√§tzlich: <br><br><ol><li>  Aufgrund des teuren Roaming-Verkehrs oder Datenschutzproblemen haben Benutzer nicht immer die M√∂glichkeit, √ºber ein Mobilfunknetz mit dem Server zu kommunizieren.  L√∂sungen wie Google Lens helfen hier also nicht weiter. </li><li>  Da wir uns auf die lokale Datenverarbeitung konzentrieren, w√§re es sch√∂n f√ºr unsere L√∂sung: <br><ul><li>  Es nahm wenig Ged√§chtnis in Anspruch; </li><li>  Mit den technischen Funktionen des Smartphones funktionierte es schnell. </li></ul></li><li>  Der Text kann gedreht werden und sich auf einem zuf√§lligen Hintergrund befinden. </li><li>  W√∂rter k√∂nnen sehr lang sein.  In neuronalen Faltungsnetzen deckt der Umfang des Faltungskerns normalerweise nicht das verl√§ngerte Wort als Ganzes ab, so dass es eines Tricks bedarf, um diese Einschr√§nkung zu umgehen. <br><br><img src="https://habrastorage.org/webt/s5/qw/6h/s5qw6h1fmawausqg-sm_o-7fwm8.png" alt="Bild"><br></li><li> Die Textgr√∂√üen auf einem Foto k√∂nnen unterschiedlich sein: <br><br><img src="https://habrastorage.org/webt/ia/a-/om/iaa-omk2j8qxwl-sbqmg1_v1rlc.png" alt="Bild"><br></li></ol><br><h2>  L√∂sung </h2><br>  Die einfachste L√∂sung f√ºr das Problem der Textsuche besteht darin, das beste Netzwerk aus den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICDAR-</a> Wettbewerben (Internationale Konferenz f√ºr Dokumentenanalyse und -erkennung) zu entnehmen, die auf diese Aufgabe und dieses Gesch√§ft spezialisiert sind!  Leider erreichen solche Netzwerke aufgrund ihrer Sperrigkeit und Rechenkomplexit√§t Qualit√§t und eignen sich nur als Cloud-L√∂sung, die die Abs√§tze 1 und 2 unserer Anforderungen nicht erf√ºllt.  Aber was ist, wenn wir ein gro√ües Netzwerk verwenden, das in den Szenarien, die wir abdecken m√ºssen, gut funktioniert, und versuchen, es zu reduzieren?  Dieser Ansatz ist bereits interessanter. <br><br>  Baoguang Shi et al. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schlugen SegLink</a> [1] in ihrem neuronalen Netzwerk Folgendes vor: <br><br><ol><li>  Nicht ganze W√∂rter auf einmal finden (gr√ºne Bereiche im Bild <b>a</b> ), sondern ihre Teile, Segmente genannt, mit der Vorhersage ihrer Drehung, Neigung und Verschiebung.  Lassen Sie uns diese Idee ausleihen. </li><li>  Sie m√ºssen nach Wortsegmenten auf mehreren Skalen gleichzeitig suchen, um die Anforderung 5 zu erf√ºllen. Die Segmente werden in Bild <b>b</b> durch gr√ºne Rechtecke angezeigt. </li><li>  Um zu verhindern, dass eine Person erfindet, wie diese Segmente kombiniert werden, lassen wir das neuronale Netzwerk einfach Verbindungen (Verkn√ºpfungen) zwischen Segmenten vorhersagen, die sich auf dasselbe Wort beziehen <br><br>  a.  innerhalb des gleichen Ma√üstabs (rote Linien in Bild <b>c</b> ) <br><br>  b.  und zwischen Skalen (rote Linien in Bild <b>d</b> ), um das Problem von Abschnitt 4 der Anforderungen zu l√∂sen. </li></ol><br>  Die blauen Quadrate im Bild unten zeigen die Sichtbarkeitsbereiche der Pixel der Ausgabeschichten des neuronalen Netzwerks mit verschiedenen Ma√üst√§ben, die zumindest einen Teil des Wortes "sehen". <br><br><img src="https://habrastorage.org/webt/qx/eu/zs/qxeuzsrcszz4pxafddjyma3cwqq.png" alt="Bild"><br>  <i>Segment- und Linkbeispiele</i> <br><br>  SegLink verwendet die bekannte VGG-16-Architektur als Grundlage.  Die Vorhersage von Segmenten und Verkn√ºpfungen erfolgt auf 6 Skalen.  Als erstes Experiment haben wir mit der Implementierung der urspr√ºnglichen Architektur begonnen.  Es stellte sich heraus, dass das Netzwerk 23 Millionen Parameter (Gewichte) enth√§lt, die in einer Datei mit einer Gr√∂√üe von 88 Megabyte gespeichert werden m√ºssen.  Wenn Sie eine auf VGG basierende Anwendung erstellen, ist sie einer der ersten Kandidaten f√ºr die Entfernung, wenn nicht gen√ºgend Speicherplatz vorhanden ist, und die Textsuche selbst funktioniert sehr langsam, sodass das Netzwerk dringend abnehmen muss. <br><br><img src="https://habrastorage.org/webt/oj/lp/xo/ojlpxo224mtzrerf6o4urty-gfk.png" alt="Bild"><br>  <i>SegLink-Netzwerkarchitektur</i> <br><br><h2>  Das Geheimnis unserer Ern√§hrung </h2><br>  Sie k√∂nnen die Gr√∂√üe des Netzwerks reduzieren, indem Sie einfach die Anzahl der Schichten und Kan√§le darin oder die Faltung selbst und die Verbindungen zwischen ihnen √§ndern.  Mark Sandler und seine Mitarbeiter haben die Architektur in ihrem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNetV2-</a> Netzwerk [2] gerade noch rechtzeitig aufgegriffen, damit sie auf Mobilger√§ten schnell funktioniert, wenig Platz beansprucht und die Arbeitsqualit√§t derselben VGG immer noch nicht beeintr√§chtigt.  Das Geheimnis der Geschwindigkeit und Reduzierung des Speicherverbrauchs liegt in drei Hauptschritten: <br><br><ol><li>  Die Anzahl der Kan√§le mit Feature-Maps am Eingang des Blocks wird durch Punktfaltung auf die gesamte Tiefe (den sogenannten Engpass) ohne Aktivierungsfunktion reduziert. </li><li>  Die klassische Faltung wird durch eine pro Kanal trennbare Faltung ersetzt.  Eine solche Faltung erfordert weniger Gewicht und weniger Berechnung. </li><li>  Zeichenkarten nach dem Engpass werden zur Summierung ohne zus√§tzliche Windungen an den Eingang des n√§chsten Blocks weitergeleitet. </li></ol><br><br><img src="https://habrastorage.org/webt/s8/uq/zb/s8uqzb_r7i6z508gyddsvwztsfc.png" alt="Bild"><br>  <i>MobileNetV2-Basiseinheit</i> <i><br></i> <br><br><h2>  Resultierendes neuronales Netzwerk </h2><br>  Mit den oben genannten Ans√§tzen kamen wir zu folgender Netzwerkstruktur: <br><br><ul><li>  Wir verwenden Segmente und Links von SegLink </li><li>  Ersetzen Sie VGG durch ein weniger gefr√§√üiges MobileNetV2 </li><li>  Reduzieren Sie die Anzahl der Textsuchskalen aus Gr√ºnden der Geschwindigkeit von 6 auf 5 </li></ul><br><br><img src="https://habrastorage.org/webt/el/t6/8a/elt68a2truovh33paqx1omw1mjg.png" alt="Bild"><br>  <i>Textsuchzusammenfassungsnetzwerk</i> <i><br></i> <br><br><h3>  Entschl√ºsselung von Werten in Netzwerkarchitekturbl√∂cken </h3><br>  Der Schrittschritt und die Basisanzahl der Kan√§le in den Kan√§len werden als s &lt;Schritt&gt; c &lt;Kan√§le&gt; angegeben.  Zum Beispiel bedeutet s2c32 32 Kan√§le mit einer Verschiebung von 2. Die tats√§chliche Anzahl von Kan√§len in den Faltungsschichten wird erhalten, indem ihre Basiszahl mit einem Skalierungsfaktor Œ± multipliziert wird, wodurch Sie schnell unterschiedliche ‚ÄûDicken‚Äú des Netzwerks simulieren k√∂nnen.  Unten finden Sie eine Tabelle mit der Anzahl der Parameter im Netzwerk in Abh√§ngigkeit von Œ±. <br><br><img src="https://habrastorage.org/webt/ay/k7/ih/ayk7ihjypgcp6vbw609jf9nmtfi.png" alt="Bild"><br><br>  Blocktyp: <br><br><ul><li>  Conv2D - eine vollwertige Faltungsoperation; </li><li>  D-weise Konv - trennbare Faltung pro Kanal; </li><li>  Bl√∂cke - eine Gruppe von MobileNetV2-Bl√∂cken; </li><li>  Ausgabe - Faltung, um die Ausgabeschicht zu erhalten.  Numerische Werte vom Typ NxN geben die Gr√∂√üe des Empfangsfeldes des Pixels an. </li></ul><br>  Als Aktivierungsfunktion verwenden die Bausteine ‚Äã‚ÄãReLU6. <br><br><img src="https://habrastorage.org/webt/a3/yb/ft/a3ybftdxyu7_vuw04wsx-vclcb4.png" alt="Bild" width="400"><br><br>  Die Ausgangsschicht hat 31 Kan√§le: <br><br><img src="https://habrastorage.org/webt/se/3i/xc/se3ixchyxmnjwp2mdlp0l0czwr4.png" alt="Bild"><br><br>  Die ersten beiden Kan√§le der Ausgabeebene stimmen daf√ºr, dass das Pixel zum Text und nicht zum Text geh√∂rt.  Die folgenden f√ºnf Kan√§le enthalten Informationen zur genauen Rekonstruktion der Segmentgeometrie: vertikale und horizontale Verschiebungen relativ zur Position des Pixels, Faktoren f√ºr Breite und H√∂he (da das Segment normalerweise nicht quadratisch ist) und den Drehwinkel.  16 Werte von Intra-Channel-Links geben an, ob eine Verbindung zwischen acht benachbarten Pixeln auf derselben Skala besteht.  Die letzten 8 Kan√§le geben Auskunft √ºber das Vorhandensein von Links zu vier Pixeln der vorherigen Skala (die vorherige Skala ist immer zweimal gr√∂√üer).  Alle 2 Werte von Segmenten, Intra- und Cross-Scale-Links werden durch die Softmax-Funktion normalisiert.  Der Zugriff auf die allererste Skala verf√ºgt nicht √ºber skalierungs√ºbergreifende Links. <br><br><h2>  Wortassemblierung </h2><br>  Das Netzwerk sagt voraus, ob ein bestimmtes Segment und seine Nachbarn zum Text geh√∂ren.  Es bleibt, sie in Worte zu fassen. <br><br>  Kombinieren Sie zun√§chst alle Segmente, die durch Links verbunden sind.  Zu diesem Zweck erstellen wir ein Diagramm, in dem die Scheitelpunkte alle Segmente in allen Ma√üst√§ben und die Kanten Verkn√ºpfungen sind.  Dann finden wir die verbundenen Komponenten des Graphen.  F√ºr jede Komponente ist es jetzt m√∂glich, das umschlie√üende Rechteck des Wortes wie folgt zu berechnen: <br><br><ol><li>  Wir berechnen den Drehwinkel des Wortes Œ∏ <br><ul><li>  Oder als Durchschnittswert der Vorhersagen des Drehwinkels der Segmente, wenn es viele davon gibt, </li><li>  Oder als Drehwinkel der Linie, der durch Regression an den Punkten der Mittelpunkte der Segmente erhalten wird, wenn nur wenige Segmente vorhanden sind. </li></ul></li><li>  Der Mittelpunkt des Wortes wird als Schwerpunkt der Mittelpunkte der Segmente ausgew√§hlt. </li><li>  Erweitern Sie alle Segmente um -Œ∏, um sie horizontal anzuordnen.  Finde die Grenzen des Wortes. <br><ul><li>  Die linken und rechten Grenzen des Wortes werden als Grenzen der Segmente ganz links bzw. ganz rechts ausgew√§hlt. </li><li>  Um die obere Wortgrenze zu erhalten, werden die Segmente nach der H√∂he der oberen Kante sortiert, 20% der h√∂chsten werden abgeschnitten und der Wert des ersten Segments aus der Liste, der nach dem Filtern verbleibt, wird ausgew√§hlt. </li><li>  Die untere Grenze ergibt sich aus den untersten Segmenten mit einem Grenzwert von 20% der niedrigsten in Analogie zur oberen Grenze. </li></ul></li><li>  Drehen Sie das resultierende Rechteck zur√ºck auf Œ∏. </li></ol><br>  Die endg√ºltige L√∂sung hei√üt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><b>FaSTExt</b> : Fast and Small Text Extractor</a> [3] <br><br><h2>  Experimentierzeit! </h2><br><h3>  Trainingsdetails </h3><br>  Das Netzwerk selbst und seine Parameter wurden f√ºr eine gute Arbeit an einer gro√üen internen Stichprobe ausgew√§hlt, die das Hauptszenario der Verwendung der Anwendung auf dem Telefon widerspiegelt. Er richtete die Kamera auf ein Objekt mit Text und machte ein Foto.  Es stellte sich heraus, dass ein gro√ües Netzwerk mit Œ± = 1 in der Qualit√§t die Version mit Œ± = 0,5 nur um 2% umgeht.  Dieses Beispiel ist nicht gemeinfrei. Aus Gr√ºnden der √úbersichtlichkeit musste ich das Netzwerk auf das √∂ffentliche Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICDAR2013 trainieren</a> , bei dem die Aufnahmebedingungen unseren √§hnlich sind.  Die Stichprobe ist sehr klein, daher wurde das Netzwerk zuvor auf eine gro√üe Menge synthetischer Daten aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SynthText im Wild-</a> Datensatz <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">trainiert</a> .  Der Vorschulungsprozess dauerte f√ºr jedes Experiment mit der GTX 1080 Ti etwa 20 Tage. Daher wurde der Netzwerkbetrieb mit √∂ffentlichen Daten nur auf die Optionen Œ± = 0,75, 1 und 2 √ºberpr√ºft. <br><br>  Als Optimierer wurde die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AMSGrad-</a> Version von Adam verwendet. <br><br>  Fehlerfunktionen: <br><br><ul><li>  Kreuzentropie zur Klassifizierung von Segmenten und Verbindungen; </li><li>  Huber-Verlustfunktion f√ºr Segmentgeometrie. </li></ul><br><h2>  Ergebnisse </h2><br>  In Bezug auf die Qualit√§t der Netzwerkleistung im Zielszenario k√∂nnen wir sagen, dass es in Bezug auf die Qualit√§t nicht weit hinter den Wettbewerbern zur√ºckbleibt und sogar einige √ºberholt.  MS ist ein starkes Multiskalen-Netzwerk von Wettbewerbern. <br><br><img src="https://habrastorage.org/webt/74/3y/h4/743yh4dgvv_4kjz_e0uecgqxtf0.png" alt="Bild" width="450"><br>  <i>* In dem Artikel √ºber EAST gab es keine Ergebnisse f√ºr die von uns ben√∂tigte Probe, daher haben wir das Experiment selbst durchgef√ºhrt.</i> <br><br>  Das folgende Bild zeigt ein Beispiel daf√ºr, wie FaSTExt mit Bildern von ICDAR2013 funktioniert.  Die erste Zeile zeigt, dass die beleuchteten Buchstaben des Wortes ESPMOTO nicht markiert waren, das Netzwerk sie jedoch finden konnte.  Die weniger ger√§umige Version mit Œ± = 0,75 kam mit kleinem Text schlechter zurecht als die "dickeren" Versionen.  In der unteren Zeile werden erneut Markup-Fehler im Beispiel mit verlorenem Text in der Reflexion angezeigt.  FaSTExt sieht gleichzeitig einen solchen Text. <br><br><img src="https://habrastorage.org/webt/cc/dg/7f/ccdg7fs1wpqejmvk-hmorikeuwq.png" alt="Bild"><br><br>  Das Netzwerk f√ºhrt also seine Aufgaben aus.  Es bleibt zu pr√ºfen, ob es tats√§chlich auf Telefonen verwendet werden kann?  Die Modelle wurden auf 512 x 512 Farbbildern auf dem Huawei P20 mit der CPU und auf dem iPhone SE und iPhone XS mit der GPU gestartet, da Sie mit unserem maschinellen Lernsystem die GPU weiterhin nur unter iOS verwenden k√∂nnen.  Werte, die durch Mittelung von 100 Starts erhalten werden.  Unter Android haben wir eine Geschwindigkeit von 5 Bildern pro Sekunde erreicht, die f√ºr unsere Aufgabe akzeptabel ist.  Das iPhone XS zeigte einen interessanten Effekt mit einer Verringerung der durchschnittlichen Zeit, die f√ºr Berechnungen ben√∂tigt wird, w√§hrend das Netzwerk kompliziert wird.  Ein modernes iPhone erkennt Text mit minimaler Verz√∂gerung, was als Sieg bezeichnet werden kann. <br><br><img src="https://habrastorage.org/webt/mx/zb/kq/mxzbkqrxlnjhqbwfj3hkv_bamby.png" alt="Bild"><br><br><h3>  Referenzliste </h3><br>  [1] B. Shi, X. Bai und S. Belongie, ‚ÄûErkennen von orientiertem Text in nat√ºrlichen Bildern durch Verkn√ºpfen von Segmenten‚Äú, Hawaii, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [2] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov und L.-C.  Chen, ‚ÄûMobileNetV2: Invertierte Residuen und lineare Engp√§sse‚Äú, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [3] A. Filonenko, K. Gudkov, A. Lebedev, N. Orlov und I. Zagaynov, ‚ÄûFaSTExt: Schneller und kleiner Textextraktor‚Äú, im 8. Internationalen Workshop √ºber kamerabasierte Dokumentenanalyse und -erkennung, Sydney, 2019 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [4] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu und X. Bai, ‚ÄûMultiorientierte Texterkennung mit vollst√§ndig gefalteten Netzwerken‚Äú, Las Vegas, 2016. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [5] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He und J. Liang, ‚ÄûEAST: Ein effizienter und genauer Szenentextdetektor‚Äú, auf der IEEE-Konferenz 2017 √ºber Computer Vision und Muster, Honolulu, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [6] M. Liao, Z. Zhu, B. Shi, G.-s.  Xia und X. Bai, ‚ÄûRotationsempfindliche Regression f√ºr die Texterkennung orientierter Szenen‚Äú, auf der IEEE / CVF-Konferenz 2018 √ºber Computer Vision und Muster, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  [7] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao und J. Yan, ‚ÄûFots: Schnelle Texterkennung mit einem einheitlichen Netzwerk‚Äú, auf der IEEE / CVF-Konferenz 2018 √ºber Computer Vision und Muster, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> <br><br>  <i>Computer Vision Gruppe</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472910/">https://habr.com/ru/post/de472910/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472894/index.html">DartUP 2019: Dart- und Flatterkonferenz am 23. November in St. Petersburg</a></li>
<li><a href="../de472896/index.html">Hubschrauber von einem Drucker: Zum ersten Mal haben Wissenschaftler einen gro√üen Koffer eines Hubschraubermotors ‚Äûgedruckt‚Äú</a></li>
<li><a href="../de472902/index.html">Windows for IoT: Verbesserte Hardwareunterst√ºtzung und neue Funktionen f√ºr intelligente Ger√§te</a></li>
<li><a href="../de472904/index.html">Wie Amazon Stock Work in ein Spiel verwandelte</a></li>
<li><a href="../de472908/index.html">Dagaz: Episoden (Teil 2)</a></li>
<li><a href="../de472912/index.html">ClickHouse-Datenbank f√ºr Menschen oder Alien-Technologie</a></li>
<li><a href="../de472914/index.html">Wolfram Function Repository: Open Access-Plattform f√ºr Wolfram-Spracherweiterungen</a></li>
<li><a href="../de472916/index.html">Backend, maschinelles Lernen und Serverless sind die interessantesten auf der Juli-Habr-Konferenz</a></li>
<li><a href="../de472918/index.html">ZX Spectrum in Russland und der GUS: Wie sich das Streben nach Online offline ver√§ndert hat</a></li>
<li><a href="../de472922/index.html">Defender-Programmierer st√§rker als Entropie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>