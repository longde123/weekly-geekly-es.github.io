<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò© üë≤üèª üññüèº Pr√©sentation de PyTorch: Deep Learning in Natural Language Processing ‚ôåÔ∏è üßíüèæ üõåüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, habrozhiteli! Le traitement automatique du langage naturel (PNL) est une t√¢che extr√™mement importante dans le domaine de l'intelligence artific...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pr√©sentation de PyTorch: Deep Learning in Natural Language Processing</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/475488/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><img src="https://habrastorage.org/webt/gj/ib/ak/gjibakead8idlzldl2vxqq3gmuc.jpeg" align="left" alt="image"></a>  Salut, habrozhiteli!  Le traitement automatique du langage naturel (PNL) est une t√¢che extr√™mement importante dans le domaine de l'intelligence artificielle.  Une mise en ≈ìuvre r√©ussie permet d'utiliser des produits comme Amazon d'Amazon et Google Translate.  Ce livre vous aidera √† apprendre PyTorch, une biblioth√®que d'apprentissage en profondeur pour le langage Python, l'un des principaux outils pour les scientifiques des donn√©es et les d√©veloppeurs de logiciels NLP.  Delip Rao et Brian McMahan vous familiariseront avec la PNL et les algorithmes d'apprentissage en profondeur.  Et montrez comment PyTorch vous permet d'impl√©menter des applications qui utilisent l'analyse de texte. <br><br>  Dans ce livre ‚Ä¢ Graphiques computationnels et paradigme de l'apprentissage avec un enseignant.  ‚Ä¢ Principes de base de la biblioth√®que PyTorch optimis√©e pour travailler avec des tenseurs.  ‚Ä¢ Un aper√ßu des concepts et m√©thodes traditionnels de la PNL.  ‚Ä¢ R√©seaux de neurones proactifs (perceptron multicouches et autres).  ‚Ä¢ Am√©lioration du RNN avec m√©moire √† long terme √† court terme (LSTM) et blocs de r√©currence contr√¥l√©e ‚Ä¢ Mod√®les de pr√©diction et de transformation de s√©quence.  ‚Ä¢ Mod√®les de conception des syst√®mes PNL utilis√©s en production. <br><a name="habracut"></a><br><h3>  Extrait.  Imbrication de mots et autres types </h3><br>  Lors de la r√©solution de probl√®mes de traitement de textes en langues naturelles, il faut traiter diff√©rents types de types de donn√©es discrets.  L'exemple le plus √©vident est celui des mots.  Beaucoup de mots (dictionnaire) bien s√ªr.  Entre autres exemples, symboles, √©tiquettes de parties de discours, entit√©s nomm√©es, types nomm√©s d'entit√©s, attributs associ√©s √† l'analyse, positions dans le catalogue de produits, etc. En fait, toute fonction d'entr√©e tir√©e d'un fini (ou infini, mais d√©nombrable) ensembles. <br><br>  La base de l'application r√©ussie de l'apprentissage en profondeur dans la PNL est la repr√©sentation de types de donn√©es discrets (par exemple, des mots) sous la forme de vecteurs denses.  Les termes ¬´apprentissage de la repr√©sentation¬ª et ¬´int√©gration¬ª signifient l'apprentissage de l'affichage / de la repr√©sentation d'un type de donn√©es discret √† un point dans un espace vectoriel.  Si les types discrets sont des mots, une repr√©sentation vectorielle dense est appel√©e int√©gration de mots.  Nous avons d√©j√† vu des exemples de m√©thodes d'imbrication bas√©es sur le nombre d'occurrences, par exemple TF-IDF (¬´la fr√©quence des termes est la fr√©quence inverse d'un document¬ª) dans le chapitre 2. Dans ce chapitre, nous nous concentrerons sur les m√©thodes d'imbrication bas√©es sur la formation et les m√©thodes d'imbrication bas√©es sur la pr√©diction (voir article de Baroni et al. (Baroni et al., 2014]), dans lequel l'entra√Ænement √† la performance est effectu√© en maximisant la fonction objective pour une t√¢che d'apprentissage sp√©cifique;  par exemple, pr√©dire un mot par contexte.  Les m√©thodes d'investissement bas√©es sur la formation sont actuellement la norme en raison de leur large applicabilit√© et de leur haute efficacit√©.  En fait, l'incorporation de mots dans les t√¢ches de la PNL est si r√©pandue qu'ils sont appel√©s la ¬´sriracha de la PNL¬ª, car on peut s'attendre √† ce que leur utilisation dans n'importe quelle t√¢che augmente l'efficacit√© de la solution.  Mais ce surnom est un peu trompeur, car, contrairement √† syraci, les pi√®ces jointes ne sont g√©n√©ralement pas ajout√©es au mod√®le apr√®s coup, mais en sont le composant de base. <br><br>  Dans ce chapitre, nous aborderons les repr√©sentations vectorielles en relation avec les incorporations de mots: m√©thodes d'incorporation de mots, m√©thodes d'optimisation d'incorporation de mots pour les t√¢ches d'enseignement avec et sans professeur, m√©thodes de visualisation d'incorporation visuelle, ainsi que les m√©thodes de combinaison d'int√©gration de mots pour les phrases et les documents.  Cependant, n'oubliez pas que les m√©thodes d√©crites ici s'appliquent √† tout type discret. <br><br><h3>  Pourquoi la formation √† l'investissement </h3><br>  Dans les chapitres pr√©c√©dents, nous vous avons montr√© les m√©thodes habituelles de cr√©ation de repr√©sentations vectorielles de mots.  √Ä savoir, vous avez appris √† utiliser des repr√©sentations unitaires - des vecteurs d'une longueur correspondant √† la taille du dictionnaire, avec des z√©ros √† toutes les positions, √† l'exception d'un contenant la valeur 1 correspondant √† un mot sp√©cifique.  De plus, vous avez rencontr√© des repr√©sentations du nombre d'occurrences - vecteurs de longueur √©gale au nombre de mots uniques dans le mod√®le, contenant le nombre d'occurrences de mots dans la phrase aux positions correspondantes.  De telles repr√©sentations sont √©galement appel√©es repr√©sentations distributionnelles, car leur contenu / signification significatif est refl√©t√© par plusieurs dimensions du vecteur.  L'histoire des repr√©sentations distributives dure depuis de nombreuses d√©cennies (voir l'article de Firth [Firth, 1935]); elles sont excellentes pour de nombreux mod√®les d'apprentissage automatique et de r√©seaux de neurones.  Ces repr√©sentations sont construites heuristiquement1 et ne sont pas form√©es sur les donn√©es. <br><br>  La repr√©sentation distribu√©e a obtenu son nom parce que les mots qu'ils contiennent sont repr√©sent√©s par un vecteur dense d'une dimension beaucoup plus petite (par exemple, d = 100 au lieu de la taille de l'ensemble du dictionnaire, qui peut √™tre de l'ordre <img src="https://habrastorage.org/webt/vr/yx/0r/vryx0rlwodoo2krhnfdldvleqaq.png" alt="image">  ), et la signification et d'autres propri√©t√©s du mot sont r√©parties sur plusieurs dimensions de ce vecteur dense. <br><br>  Les repr√©sentations denses de faible dimension obtenues √† la suite de l'apprentissage pr√©sentent plusieurs avantages par rapport aux vecteurs unitaires contenant le nombre d'occurrences que nous avons rencontr√©es dans les chapitres pr√©c√©dents.  Premi√®rement, la r√©duction de dimensionnalit√© est efficace sur le plan des calculs.  Deuxi√®mement, les repr√©sentations bas√©es sur le nombre d'occurrences conduisent √† des vecteurs de haute dimension avec un codage excessif des m√™mes informations dans diff√©rentes dimensions, et leur puissance statistique n'est pas trop grande.  Troisi√®mement, une dimension trop grande des donn√©es d'entr√©e peut entra√Æner des probl√®mes d'apprentissage automatique et d'optimisation - un ph√©nom√®ne souvent appel√© la mal√©diction de la dimension ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://bit.ly/2CrhQXm</a> ).  Pour r√©soudre ce probl√®me avec la dimensionnalit√©, diverses m√©thodes de r√©duction de la dimension sont utilis√©es, par exemple, la d√©composition en valeurs singuli√®res (SVD) et la m√©thode d'analyse en composantes principales (PCA), mais, ironiquement, ces approches ne s'adaptent pas bien sur des dimensions de l'ordre de millions ( cas typique en PNL).  Quatri√®mement, les repr√©sentations apprises (ou ajust√©es sur la base de) donn√©es sp√©cifiques √† un probl√®me sont parfaitement adapt√©es √† cette t√¢che particuli√®re.  Dans le cas d'algorithmes heuristiques comme TF-IDF et de m√©thodes de r√©duction dimensionnelle comme SVD, il n'est pas clair si la fonction d'optimisation objective convient √† une t√¢che particuli√®re avec cette m√©thode d'int√©gration. <br><br><h3>  Efficacit√© d'investissement </h3><br>  Pour comprendre le fonctionnement des plongements, consid√©rons un exemple de vecteur unitaire par lequel la matrice de poids dans une couche lin√©aire est multipli√©e, comme le montre la Fig.  5.1.  Dans les chapitres 3 et 4, la taille des vecteurs unitaires co√Øncidait avec la taille du dictionnaire.  Un vecteur est appel√© unitaire car il contient 1 √† la position correspondant √† un mot particulier, indiquant ainsi sa pr√©sence. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/4q/ag/or4qagtvn_dh4sxqok-q0tcsfmi.png" alt="image"></div><br>  Fig.  5.1.  Un exemple de multiplication matricielle pour le cas d'un vecteur unitaire et d'une matrice de poids d'une couche lin√©aire.  Comme le vecteur unitaire contient tous les z√©ros et une seule unit√©, la position de cette unit√© joue le r√¥le de l'op√©rateur de choix lors de la multiplication de la matrice.  Ceci est montr√© sur la figure comme un assombrissement des cellules de la matrice de poids et du vecteur r√©sultant.  Bien que cette m√©thode de recherche fonctionne, elle n√©cessite une grande consommation de ressources informatiques et est inefficace, car le vecteur unitaire est multipli√© par chacun des nombres dans la matrice de poids et la somme est calcul√©e en lignes <br><br>  Par d√©finition, le nombre de lignes de la matrice de poids d'une couche lin√©aire recevant un vecteur unitaire en entr√©e doit √™tre √©gal √† la taille de ce vecteur unitaire.  Lors de la multiplication de la matrice, comme le montre la Fig.  5.1, le vecteur r√©sultant est en fait une cha√Æne correspondant √† un √©l√©ment non nul d'un vecteur unitaire.  Sur la base de cette observation, vous pouvez ignorer l'√©tape de multiplication et utiliser une valeur enti√®re comme index pour extraire la ligne souhait√©e. <br><br>  Une derni√®re note concernant la performance des investissements: malgr√© l'exemple de la figure  5.1, o√π la dimension de la matrice de poids co√Øncide avec la dimension du vecteur unitaire d'entr√©e, c'est loin d'√™tre toujours le cas.  En fait, les pi√®ces jointes sont souvent utilis√©es pour repr√©senter des mots d'un espace de dimension inf√©rieure √† ce qui serait n√©cessaire si vous utilisez un vecteur unitaire ou repr√©sentez le nombre d'occurrences.  Une taille d'investissement typique dans les articles scientifiques est de 25 √† 500 mesures, et le choix d'une valeur sp√©cifique est r√©duit √† la quantit√© de m√©moire GPU disponible. <br><br><h3>  Approches d'apprentissage de l'attachement </h3><br>  Le but de ce chapitre n'est pas de vous enseigner des techniques sp√©cifiques pour investir des mots, mais de vous aider √† comprendre ce que sont les investissements, comment et o√π ils peuvent √™tre appliqu√©s, comment les utiliser au mieux dans les mod√®les et quelles sont leurs limites.  Le fait est qu'en pratique, il est rarement n√©cessaire d'√©crire de nouveaux algorithmes d'apprentissage pour l'int√©gration de mots.  Cependant, dans cette sous-section, nous donnerons un bref aper√ßu des approches modernes de cette formation.  L'apprentissage de toutes les m√©thodes d'imbrication des mots se fait en utilisant uniquement des mots (c'est-√†-dire des donn√©es sans √©tiquette), mais avec un enseignant.  Cela est possible en raison de la cr√©ation de t√¢ches d'enseignement auxiliaire avec l'enseignant, dans lesquelles les donn√©es sont marqu√©es implicitement, pour les raisons que la repr√©sentation optimis√©e pour r√©soudre la t√¢che auxiliaire devrait capturer de nombreuses propri√©t√©s statistiques et linguistiques du corpus de texte afin d'apporter au moins un certain avantage.  Voici quelques exemples de telles t√¢ches d'assistance. <br><br><ul><li>  Pr√©disez le mot suivant dans une s√©quence de mots donn√©e.  Il porte √©galement le nom du probl√®me de mod√©lisation du langage. </li><li>  Pr√©disez un mot manquant par des mots situ√©s avant et apr√®s. </li><li>  Pr√©disez les mots dans une fen√™tre sp√©cifique, quelle que soit leur position, pour un mot donn√©. </li></ul><br>  Bien entendu, cette liste n'est pas compl√®te et le choix d'un probl√®me auxiliaire d√©pend de l'intuition du d√©veloppeur de l'algorithme et des co√ªts de calcul.  Les exemples incluent GloVe, Continuous Bag-of-Words (CBOW), Skipgrams, etc. Des d√©tails peuvent √™tre trouv√©s dans le chapitre 10 du livre de Goldberg (Goldberg, 2017), mais nous discuterons bri√®vement du mod√®le CBOW ici.  Cependant, dans la plupart des cas, il suffit d'utiliser des pi√®ces jointes pr√©-form√©es et de les adapter √† la t√¢che existante. <br><br><h3>  Application pratique des pi√®ces jointes de mots pr√©-form√©s </h3><br>  La majeure partie de ce chapitre, ainsi que le reste du livre, concerne l'utilisation de pi√®ces jointes pr√©-form√©es.  Pr√©-form√© en utilisant l'une des nombreuses m√©thodes d√©crites ci-dessus sur un grand corps - par exemple, le corps complet de Google News, Wikipedia ou Common Crawl1 - les pi√®ces jointes de mots peuvent √™tre t√©l√©charg√©es et utilis√©es librement.  Plus loin dans le chapitre, nous montrerons comment trouver et charger correctement ces pi√®ces jointes, √©tudier certaines propri√©t√©s des incorporations de mots et donner des exemples d'utilisation des incorporations de mots pr√©-form√©es dans les t√¢ches PNL. <br><br><h3>  T√©l√©charger les pi√®ces jointes </h3><br>  Les pi√®ces jointes de mots sont devenues si populaires et r√©pandues que de nombreuses options diff√©rentes sont disponibles pour t√©l√©chargement, du Word2Vec2 original √† Stanford GloVe ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://stanford.io/2PSIvPZ</a> ), y compris FastText3 de Facebook ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://fasttext.cc /</a> ) et bien d'autres.  Habituellement, les pi√®ces jointes sont livr√©es dans le format suivant: chaque ligne commence par un mot / type suivi d'une s√©quence de chiffres (c'est-√†-dire une repr√©sentation vectorielle).  La longueur de cette s√©quence est √©gale √† la dimension de la pr√©sentation (dimension de la pi√®ce jointe).  La dimension des investissements est g√©n√©ralement de l'ordre de centaines.  Le nombre de types de jetons est le plus souvent √©gal √† la taille du dictionnaire et s'√©l√®ve √† environ un million.  Par exemple, voici les sept premi√®res dimensions des vecteurs chiens et chats de GloVe. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/n5/g7/x0/n5g7x0mos74bxchmiuexmcl7yw8.png" alt="image"></div><br>  Pour un chargement et une manipulation efficaces des pi√®ces jointes, nous d√©crivons la classe d'assistance PreTrainedEmbeddings (exemple 5.1).  Il cr√©e un index de tous les vecteurs de mots stock√©s dans la RAM pour simplifier la recherche et les requ√™tes rapides des voisins les plus proches √† l'aide du package de calcul approximatif du plus proche voisin, ennuyeux. <br><br>  Exemple 5.1.  Utilisation de pi√®ces jointes pr√©-form√©es Word <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nx/hh/hh/nxhhhhz9wsdjl1tzms5ur8e4bvk.png" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y1/8u/vb/y18uvbmsjpbvwno_wp5dsui8_38.png" alt="image"></div><br>  Dans ces exemples, nous utilisons l'incorporation des mots GloVe.  Vous devez les t√©l√©charger et cr√©er une instance de la classe PreTrainedEmbeddings, comme indiqu√© dans Input [1] de l'exemple 5.1. <br><br><h3>  Relations entre les pi√®ces jointes Word </h3><br>  La propri√©t√© cl√© de l'int√©gration des mots est l'encodage des relations syntaxiques et s√©mantiques, qui se manifestent sous la forme de mod√®les d'utilisation des mots.  Par exemple, on parle g√©n√©ralement de fa√ßon tr√®s similaire des chats et des chiens (ils discutent de leurs animaux de compagnie, de leurs habitudes alimentaires, etc.).  En cons√©quence, les attachements pour les mots chats et chiens sont beaucoup plus proches les uns des autres que les attachements pour les noms d'autres animaux, disent les canards et les √©l√©phants. <br><br>  Il existe de nombreuses fa√ßons d'√©tudier les relations s√©mantiques cod√©es dans les pi√®ces jointes de mots.  L'une des m√©thodes les plus populaires consiste √† utiliser la t√¢che d'analogie (l'un des types courants de t√¢ches de r√©flexion logique dans les examens tels que SAT): <br><br>  Word1: Word2 :: Word3: ______ <br><br>  Dans cette t√¢che, il est n√©cessaire de d√©terminer le quatri√®me, √©tant donn√© la connexion entre les deux premiers, par les trois mots donn√©s.  √Ä l'aide de mots imbriqu√©s, ce probl√®me peut √™tre cod√© spatialement.  Tout d'abord, soustrayez Word2 de Word1.  Le vecteur de diff√©rence entre eux code la relation entre Word1 et Word2.  Cette diff√©rence peut ensuite √™tre ajout√©e √† Slovo3 et le r√©sultat sera le vecteur le plus proche du quatri√®me mot manquant.  Pour r√©soudre le probl√®me d'analogie, il suffit d'interroger les voisins les plus proches par index en utilisant ce vecteur obtenu.  La fonction correspondante montr√©e dans l'exemple 5.2 fait exactement ce qui est d√©crit ci-dessus: elle utilise l'arithm√©tique vectorielle et un indice approximatif des voisins les plus proches pour trouver l'√©l√©ment manquant dans l'analogie. <br><br>  Exemple 5.2.  R√©solution d'un probl√®me d'analogie √† l'aide des incorporations de mots <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q3/qv/sl/q3qvslovjualenm1jkjn6sftbtk.png" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1u/i_/hi/1ui_hizmvkfh5xqy1aqkegqqjfc.png" alt="image"></div><br><br>  Fait int√©ressant, en utilisant une simple analogie verbale, on peut d√©montrer comment les incorporations de mots peuvent capturer une vari√©t√© de relations s√©mantiques et syntaxiques (exemple 5.3). <br><br>  Exemple 5.3  Codage √† l'aide d'incorporation de mots de beaucoup de connexions linguistiques sur l'exemple de t√¢ches sur l'analogie de SAT <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/rf/v0/q1rfv0qixjwl9mob64rfru-2vgw.png" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hk/wt/j0/hkwtj0_ea88pxvm-2q3zelcjlfq.png" alt="image"></div><br>  Bien qu'il puisse sembler que les connexions refl√®tent clairement le fonctionnement de la langue, tout n'est pas si simple.  Comme le montre l'exemple 5.4, les connexions peuvent √™tre mal d√©finies car les vecteurs de mots sont d√©termin√©s en fonction de leur occurrence conjointe. <br><br>  Exemple 5.4.  Un exemple illustrant le danger de coder le sens des mots en fonction de la cooccurrence - parfois cela ne fonctionne pas! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b1/xf/t3/b1xft37n3jlu6lwivpjpi_shnmm.png" alt="image"></div><br>  L'exemple 5.5 illustre l'une des combinaisons les plus courantes lors du codage des r√¥les de genre. <br><br>  Exemple 5.5  Soyez prudent avec les attributs prot√©g√©s, tels que le sexe, cod√©s par des pi√®ces jointes de mots.  Ils peuvent conduire √† des biais ind√©sirables dans les futurs mod√®les. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ga/5y/cl/ga5yclk4wb32h5rpwujbl96yry0.png" alt="image"></div><br>  Il s‚Äôav√®re qu‚Äôil est assez difficile de faire la distinction entre les mod√®les linguistiques et les pr√©jug√©s culturels profond√©ment enracin√©s.  Par exemple, les m√©decins ne sont en aucun cas toujours des hommes et les infirmi√®res ne sont pas toujours des femmes, mais ces pr√©jug√©s sont si persistants qu'ils se refl√®tent dans la langue et, par cons√©quent, dans les vecteurs de mots, comme le montre l'exemple 5.6. <br><br>  Exemple 5.6.  Des pr√©jug√©s culturels ¬´cousus¬ª dans les vecteurs de mots <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oz/ap/xz/ozapxzj5jxnwwxdcq33sc_dig9q.png" alt="image"></div><br>  N'oublions pas d'√©ventuelles erreurs syst√©matiques dans les investissements, compte tenu de la croissance de leur popularit√© et de leur pr√©valence dans les applications PNL.  L'√©radication des erreurs syst√©matiques dans l'int√©gration de mots est un domaine nouveau et tr√®s int√©ressant de la recherche scientifique (voir l'article de Bolukbashi et al. [Bolukbasi et al., 2016]).  Nous vous recommandons de consulter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©thiqueinnlp.org</a> o√π vous pouvez trouver des informations √† jour sur l'√©thique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">transversale</a> et la PNL. <br><br><h3>  √Ä propos des auteurs </h3><br>  <b>Delip Rao</b> est le fondateur de la soci√©t√© de conseil bas√©e √† San Francisco, Joostware, sp√©cialis√©e dans l'apprentissage automatique et la recherche en PNL.  L'un des co-fondateurs du Fake News Challenge - une initiative con√ßue pour rassembler des pirates informatiques et des chercheurs dans le domaine de l'IA sur les t√¢ches de v√©rification des faits dans les m√©dias.  Delip a pr√©c√©demment travaill√© sur des produits de recherche et logiciels li√©s √† la PNL sur Twitter et Amazon (Alexa). <br><br>  <b>Brian McMahan</b> est chercheur √† Wells Fargo, se concentrant principalement sur la PNL.  A travaill√© auparavant chez Joostware. <br><br>  ¬ªPlus d'informations sur le livre sont disponibles sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le site Web de l'√©diteur</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Contenu</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Extrait</a> <br><br>  25% de r√©duction sur les <b>colporteurs</b> - <b>PyTorch</b> <br><br>  Lors du paiement de la version papier du livre, un livre √©lectronique est envoy√© par e-mail. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475488/">https://habr.com/ru/post/fr475488/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475476/index.html">Data Mesh: comment travailler avec des donn√©es sans monolithe</a></li>
<li><a href="../fr475478/index.html">Exp√©rience Netflix: Netflix Inside</a></li>
<li><a href="../fr475480/index.html">Tu es quoi Comment nous avons distingu√© la parodie de l'humain - et m√™me gagn√©</a></li>
<li><a href="../fr475482/index.html">Comment la t√¢che de test est-elle devenue une biblioth√®que de production</a></li>
<li><a href="../fr475486/index.html">Cr√©ateurs AR: l'√©mergence d'un nouveau m√©tier</a></li>
<li><a href="../fr475490/index.html">Travailler sous pression</a></li>
<li><a href="../fr475494/index.html">¬´Y a-t-il une vie apr√®s Signor?¬ª Ou de quoi parlerons-nous au SECR-2019</a></li>
<li><a href="../fr475496/index.html">Comment d√©terminer l'adresse d'un contrat intelligent avant le d√©ploiement: utiliser CREATE2 pour l'√©change cryptographique</a></li>
<li><a href="../fr475498/index.html">Windows Server Core vs GUI et compatibilit√© logicielle</a></li>
<li><a href="../fr475510/index.html">Sauvegarde incr√©mentale en une douzaine de lignes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>