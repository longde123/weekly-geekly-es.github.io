<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â˜¦ï¸ ğŸš¢ ğŸ˜‰ Bagaimana arsitektur web toleran-kesalahan diimplementasikan di platform Mail.ru Cloud Solutions ğŸ›ŒğŸ¾ ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ¼ ğŸŒ¥ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo, Habr! Saya Artyom Karamyshev, kepala tim administrasi sistem di Mail.Ru Cloud Solutions (MCS) . Selama setahun terakhir, kami telah meluncurkan ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana arsitektur web toleran-kesalahan diimplementasikan di platform Mail.ru Cloud Solutions</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/474180/"><img src="https://habrastorage.org/webt/gd/wp/de/gdwpdevye3ploqkbmd4rwjqkvva.jpeg"><br><br>  Halo, Habr!  Saya Artyom Karamyshev, kepala tim administrasi sistem di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Mail.Ru Cloud Solutions (MCS)</a> .  Selama setahun terakhir, kami telah meluncurkan banyak produk baru.  Kami ingin layanan API untuk skala dengan mudah, toleran terhadap kesalahan, dan siap untuk peningkatan beban pengguna yang cepat.  Platform kami diimplementasikan pada OpenStack, dan saya ingin memberi tahu Anda apa masalah toleransi kesalahan komponen yang harus kami tutup untuk mendapatkan sistem toleran kesalahan.  Saya pikir ini akan menarik bagi mereka yang juga mengembangkan produk di OpenStack. <br><br>  Toleransi kesalahan keseluruhan platform terdiri dari stabilitas komponennya.  Jadi kita akan secara bertahap melewati semua level di mana kita menemukan risiko dan menutupnya. <br><br>  Versi video dari cerita ini, sumber aslinya yang merupakan laporan di konferensi Uptime day 4 yang diselenggarakan oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ITSumma</a> , dapat dilihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di saluran YouTube Uptime Community</a> . <br><a name="habracut"></a><br>
<h2>  Toleransi kesalahan arsitektur fisik </h2><br>  Bagian publik cloud MCS sekarang berbasis di dua pusat data Tier III, di antaranya ada serat gelapnya sendiri, dicadangkan pada lapisan fisik dengan rute yang berbeda, dengan throughput 200 Gb / s.  Tingkat Tier III menyediakan tingkat ketahanan infrastruktur fisik yang diperlukan. <br><br>  Serat gelap dicadangkan di level fisik dan logis.  Proses reservasi saluran berulang, masalah muncul, dan kami terus meningkatkan komunikasi antara pusat data. <br><br><blockquote>  Misalnya, belum lama ini, ketika bekerja di sumur di sebelah salah satu pusat data, sebuah excavator meninju pipa, di dalam pipa ini ada kabel optik utama dan cadangan.  Saluran komunikasi kami yang toleran terhadap kesalahan dengan pusat data ternyata rentan pada satu titik, di sumur.  Karenanya, kami kehilangan beberapa infrastruktur.  Kami membuat kesimpulan, mengambil sejumlah tindakan, termasuk meletakkan optik tambahan di sepanjang sumur tetangga. </blockquote><br>  Di pusat data ada titik-titik keberadaan penyedia komunikasi tempat kami menyiarkan awalan kami melalui BGP.  Untuk setiap arah jaringan, metrik terbaik dipilih, yang memungkinkan pelanggan yang berbeda memberikan kualitas koneksi terbaik.  Jika komunikasi melalui satu penyedia terputus, kami membangun kembali perutean kami melalui penyedia yang tersedia. <br><br>  Jika terjadi kegagalan penyedia, kami secara otomatis beralih ke yang berikutnya.  Dalam hal kegagalan salah satu pusat data, kami memiliki salinan cermin dari layanan kami di pusat data kedua, yang menanggung semua beban sendiri. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d0/m8/ly/d0m8lykvifmc-gum-h9mbppyn3g.jpeg"></div><br>  <i>Ketahanan infrastruktur fisik</i> <br><br><h2>  Apa yang kami gunakan untuk toleransi kesalahan tingkat aplikasi </h2><br>  Layanan kami dibangun di atas sejumlah komponen opensource. <br><br>  <b>ExaBGP</b> adalah layanan yang mengimplementasikan sejumlah fungsi menggunakan protokol routing dinamis berdasarkan BGP.  Kami secara aktif menggunakannya untuk mengumumkan alamat IP putih kami di mana pengguna mendapatkan akses ke API. <br><br>  <b>HAProxy</b> adalah penyeimbang yang sarat muatan yang memungkinkan Anda mengonfigurasi aturan yang sangat fleksibel untuk menyeimbangkan lalu lintas di berbagai tingkat model OSI.  Kami menggunakannya untuk menyeimbangkan semua layanan: database, pialang pesan, layanan API, layanan web, proyek internal kami - semuanya ada di belakang HAProxy. <br><br>  <b>Aplikasi API</b> - <b>aplikasi</b> web yang ditulis dengan python, yang dengannya pengguna mengontrol infrastrukturnya, layanannya. <br><br>  <b>Aplikasi pekerja</b> (selanjutnya hanya disebut sebagai pekerja) - dalam layanan OpenStack ini adalah daemon infrastruktur yang memungkinkan Anda untuk menerjemahkan perintah API ke infrastruktur.  Misalnya, disk dibuat di pekerja, dan permintaan untuk pembuatan ada di API aplikasi. <br><br><h2>  Arsitektur Aplikasi OpenStack Standar </h2><br>  Sebagian besar layanan yang dikembangkan untuk OpenStack mencoba mengikuti paradigma tunggal.  Layanan biasanya terdiri dari 2 bagian: API dan pekerja (pelaksana backend).  Biasanya, API adalah aplikasi WSGI python yang berjalan baik sebagai proses mandiri (daemon) atau menggunakan server web Nginx yang sudah jadi, Apache.  API memproses permintaan pengguna dan meneruskan instruksi lebih lanjut ke aplikasi pekerja.  Transmisi terjadi menggunakan broker pesan, biasanya RabbitMQ, sisanya kurang didukung.  Ketika pesan sampai ke broker, mereka diproses oleh pekerja dan, jika perlu, kembalikan respons. <br><br>  Paradigma ini menyiratkan titik kegagalan umum yang terisolasi: RabbitMQ dan database.  Tetapi RabbitMQ terisolasi dalam satu layanan dan, secara teori, dapat bersifat individual untuk setiap layanan.  Jadi kami di MCS membagikan layanan ini sebanyak mungkin, untuk setiap proyek kami membuat basis data terpisah, sebuah RabbitMQ terpisah.  Pendekatan ini baik karena jika terjadi kecelakaan di beberapa titik rentan, tidak semua layanan rusak, tetapi hanya sebagian saja. <br><br>  Jumlah aplikasi pekerja tidak terbatas, sehingga API dapat dengan mudah mengukur secara horizontal di belakang penyeimbang untuk meningkatkan produktivitas dan toleransi kesalahan. <br><br><blockquote>  Beberapa layanan memerlukan koordinasi dalam layanan - ketika operasi berurutan yang kompleks terjadi antara API dan pekerja.  Dalam hal ini, pusat koordinasi tunggal digunakan, sistem cluster seperti Redis, Memcache, dll, yang memungkinkan satu pekerja untuk memberi tahu pekerja lain bahwa tugas ini diberikan kepadanya ("tolong jangan bawa").  Kami menggunakan etcd.  Sebagai aturan, pekerja aktif berkomunikasi dengan database, menulis dan membaca informasi dari sana.  Sebagai database, kami menggunakan mariadb, yang kami miliki di cluster multimaster. <br></blockquote><br>  Layanan pengguna tunggal klasik semacam itu diatur dengan cara yang diterima secara umum untuk OpenStack.  Ini dapat dianggap sebagai sistem tertutup, yang metode penskalaan dan toleransi kesalahan cukup jelas.  Misalnya, untuk toleransi kesalahan dari API, cukup menempatkan penyeimbang di depannya.  Penskalaan pekerja dicapai dengan meningkatkan jumlah mereka. <br><br>  Titik lemah dalam keseluruhan skema adalah RabbitMQ dan MariaDB.  Arsitektur mereka layak mendapatkan artikel yang terpisah. Dalam artikel ini saya ingin fokus pada toleransi kesalahan API. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1a/ab/i1/1aabi1ew0ctxnlrcm2j78nbhefk.jpeg"></div><br>  <i>Arsitektur Aplikasi Openstack</i>  <i>Perimbangan dan ketahanan platform cloud</i> <br><br><h2>  Membuat HAProxy Balancer Tangguh dengan ExaBGP </h2><br>  Untuk membuat API kami dapat diskalakan, cepat, dan toleran terhadap kesalahan, kami menetapkan penyeimbang di depannya.  Kami memilih HAProxy.  Menurut pendapat saya, ini memiliki semua karakteristik yang diperlukan untuk tugas kami: menyeimbangkan pada beberapa tingkat OSI, antarmuka manajemen, fleksibilitas dan skalabilitas, sejumlah besar metode penyeimbangan, dukungan untuk tabel sesi. <br><br>  Masalah pertama yang perlu dipecahkan adalah toleransi kesalahan penyeimbang itu sendiri.  Hanya dengan memasang penyeimbang juga menciptakan titik kegagalan: penyeimbang istirahat - layanan turun.  Untuk mencegah hal ini, kami menggunakan HAProxy bersama dengan ExaBGP. <br><br>  ExaBGP memungkinkan Anda menerapkan mekanisme untuk memeriksa status suatu layanan.  Kami menggunakan mekanisme ini untuk memeriksa fungsionalitas HAProxy dan jika ada masalah menonaktifkan layanan HAProxy dari BGP. <br><br>  <b>Skema ExaBGP + HAProxy</b> <br><br><ol><li>  Kami menginstal perangkat lunak yang diperlukan pada tiga server, ExaBGP dan HAProxy. </li><li>  Di setiap server kami membuat antarmuka loopback. </li><li>  Di ketiga server, kami menetapkan alamat IP putih yang sama untuk antarmuka ini. </li><li>  Alamat IP putih diumumkan di Internet melalui ExaBGP. </li></ol><br>  Toleransi kesalahan dicapai dengan mengumumkan alamat IP yang sama dari ketiga server.  Dari sudut pandang jaringan, alamat yang sama dapat diakses dari tiga harapan berikutnya yang berbeda.  Router melihat tiga rute yang identik, memilih prioritas yang paling tinggi menurut metriknya sendiri (ini biasanya pilihan yang sama), dan lalu lintas hanya menuju ke salah satu server. <br><br>  Jika terjadi masalah dengan operasi HAProxy atau kegagalan server, ExaBGP berhenti mengumumkan rute, dan lalu lintas dengan lancar beralih ke server lain. <br><br>  Dengan demikian, kami telah mencapai toleransi kesalahan penyeimbang. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ij/c_/cz/ijc_cz1jvhlug0axiwiiqjqpcww.jpeg"></div><br>  <i>Toleransi kesalahan penyeimbang HAProxy</i> <br><br>  Skema itu ternyata tidak sempurna: kami belajar cara memesan HAProxy, tetapi tidak belajar bagaimana mendistribusikan beban di dalam layanan.  Oleh karena itu, kami sedikit memperluas skema ini: kami beralih ke menyeimbangkan antara beberapa alamat IP putih. <br><br><h2>  BGP Berbasis Balancing Plus DNS </h2><br>  Masalah keseimbangan beban sebelum HAProxy kami tetap tidak terselesaikan.  Namun demikian, itu dapat diselesaikan dengan cukup sederhana, seperti yang kami lakukan di rumah. <br><br>  Untuk menyeimbangkan ketiga server, Anda akan membutuhkan 3 alamat IP putih dan DNS lama yang baik.  Masing-masing alamat ini didefinisikan pada antarmuka loopback masing-masing HAProxy dan diumumkan di Internet. <br><br>  OpenStack menggunakan katalog layanan untuk mengelola sumber daya, yang menetapkan API titik akhir layanan.  Dalam direktori ini kami meresepkan nama domain - public.infra.mail.ru, yang diselesaikan melalui DNS dengan tiga alamat IP yang berbeda.  Akibatnya, kami mendapatkan penyeimbangan beban antara tiga alamat melalui DNS. <br><br>  Tetapi karena ketika mengumumkan alamat IP putih, kami tidak mengontrol prioritas pemilihan server, sejauh ini ini tidak seimbang.  Sebagai aturan, hanya satu server akan dipilih dengan didahulukan dari alamat IP, dan dua lainnya akan menganggur, karena tidak ada metrik yang ditentukan dalam BGP. <br><br>  Kami mulai memberikan rute melalui ExaBGP dengan metrik yang berbeda.  Setiap penyeimbang mengumumkan ketiga alamat IP putih, tetapi salah satunya, yang utama untuk penyeimbang ini, diumumkan dengan metrik minimum.  Jadi sementara ketiga penyeimbang beroperasi, panggilan ke alamat IP pertama jatuh pada penyeimbang pertama, panggilan ke yang kedua ke yang kedua, ke yang ketiga ke yang ketiga. <br><br>  Apa yang terjadi ketika salah satu penyeimbang jatuh?  Dalam hal kegagalan penyeimbang berdasarkan basisnya, alamat masih diumumkan dari dua penyeimbang lainnya, lalu lintas di antara mereka didistribusikan kembali.  Dengan demikian, kami memberikan kepada pengguna melalui DNS beberapa alamat IP sekaligus.  Dengan menyeimbangkan DNS dan metrik yang berbeda, kami mendapatkan distribusi beban yang seragam pada ketiga penyeimbang.  Dan pada saat yang sama kita tidak kehilangan toleransi kesalahan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ek/xc/3z/ekxc3zsz5oazwdiqziwp_idfk7a.jpeg"></div><br>  <i>HAProxy Balancing Berdasarkan DNS + BGP</i> <br><br><h2>  Interaksi antara ExaBGP dan HAProxy </h2><br>  Jadi, kami menerapkan toleransi kesalahan jika server pergi, berdasarkan penghentian pengumuman rute.  Tetapi HAProxy juga dapat terputus karena alasan lain selain kegagalan server: kesalahan administrasi, kegagalan layanan.  Kami ingin menghapus penyeimbang yang rusak dari bawah beban dan dalam kasus ini, dan kami membutuhkan mekanisme lain. <br><br>  Oleh karena itu, memperluas skema sebelumnya, kami menerapkan detak jantung antara ExaBGP dan HAProxy.  Ini adalah implementasi perangkat lunak dari interaksi antara ExaBGP dan HAProxy, ketika ExaBGP menggunakan skrip khusus untuk memeriksa status aplikasi. <br><br>  Untuk melakukan ini, dalam konfigurasi ExaBGP, Anda harus mengonfigurasi pemeriksa kesehatan yang dapat memeriksa status HAProxy.  Dalam kasus kami, kami mengonfigurasi backend kesehatan di HAProxy, dan dari sisi ExaBGP kami memeriksa dengan permintaan GET sederhana.  Jika pengumuman berhenti terjadi, maka HAProxy kemungkinan besar tidak berfungsi, dan tidak perlu untuk mengumumkannya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w0/ac/5c/w0ac5cvqsvjtki2cqzcgtgyk4x4.jpeg"></div><br>  <i>Pemeriksaan Kesehatan HAProxy</i> <br><br><h2>  HAProxy Peers: sinkronisasi sesi </h2><br>  Hal selanjutnya yang harus dilakukan adalah menyinkronkan sesi.  Ketika bekerja melalui penyeimbang terdistribusi, sulit untuk mengatur penyimpanan informasi tentang sesi klien.  Tetapi HAProxy adalah salah satu dari sedikit penyeimbang yang dapat melakukan ini karena fungsi Peers - kemampuan untuk mentransfer tabel sesi antara berbagai proses HAProxy. <br><br>  Ada beberapa metode balancing: sederhana, seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">round-robin</a> , dan advanced, ketika sesi klien diingat, dan setiap kali ia sampai ke server yang sama seperti sebelumnya.  Kami ingin menerapkan opsi kedua. <br><br>  HAProxy menggunakan stick-tables untuk menyimpan sesi klien untuk mekanisme ini.  Mereka menyimpan sumber alamat IP klien, alamat target yang dipilih (backend) dan beberapa informasi layanan.  Biasanya, stick-tables digunakan untuk menyimpan pasangan sumber-IP + tujuan-IP, yang sangat berguna untuk aplikasi yang tidak dapat mentransmisikan konteks sesi pengguna saat beralih ke penyeimbang lain, misalnya, dalam mode penyeimbangan RoundRobin. <br><br>  Jika stick-table diajarkan untuk berpindah di antara berbagai proses HAProxy yang berbeda (di antaranya terjadi penyeimbangan), balancers kami akan dapat bekerja dengan satu kumpulan stick-tables.  Ini akan memungkinkan untuk beralih mulus jaringan klien ketika salah satu penyeimbang jatuh, bekerja dengan sesi klien akan melanjutkan pada backend yang sama yang dipilih sebelumnya. <br><br>  Untuk operasi yang tepat, alamat IP sumber penyeimbang dari mana sesi didirikan harus diselesaikan.  Dalam kasus kami, ini adalah alamat dinamis pada antarmuka loopback. <br><br>  Operasi rekan yang benar hanya dapat dicapai dalam kondisi tertentu.  Artinya, batas waktu TCP harus cukup besar atau sakelar harus cukup cepat sehingga sesi TCP tidak punya waktu untuk istirahat.  Namun, ini memungkinkan pergantian yang mulus. <br><br>  Kami di IaaS memiliki layanan yang dibangun di atas teknologi yang sama.  Ini adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Load Balancer sebagai layanan untuk OpenStack yang</a> disebut Octavia.  Ini berdasarkan pada dua proses HAProxy, ini awalnya termasuk dukungan rekan.  Mereka telah membuktikan diri dalam layanan ini. <br><br>  Gambar secara skematis menunjukkan pergerakan tabel rekan antara tiga instance HAProxy, konfigurasi disarankan, bagaimana ini dapat dikonfigurasi: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ov/ol/wr/ovolwrmp-gzrvyybotjjagtb-re.jpeg"></div><br>  <i>HAProxy Peers (sinkronisasi sesi)</i> <br><br>  Jika Anda menerapkan skema yang sama, pekerjaannya harus diuji dengan cermat.  Bukan fakta bahwa ini akan bekerja dengan cara yang sama dalam 100% kasus.  Tetapi setidaknya Anda tidak akan kehilangan tabel stick ketika Anda perlu mengingat IP sumber klien. <br><br><h2>  Membatasi jumlah permintaan simultan dari klien yang sama </h2><br>  Setiap layanan yang berada dalam domain publik, termasuk API kami, dapat dikenakan longsoran permintaan.  Alasan mereka bisa sangat berbeda, dari kesalahan pengguna, hingga serangan yang ditargetkan.  Kami secara berkala DDoS di alamat IP.  Klien sering membuat kesalahan dalam skrip mereka, mereka membuat kita mini-DDoS. <br><br>  Dengan satu atau lain cara, perlindungan tambahan harus disediakan.  Solusi yang jelas adalah membatasi jumlah permintaan API dan tidak membuang waktu CPU memproses permintaan jahat. <br><br>  Untuk menerapkan pembatasan seperti itu, kami menggunakan batas nilai, yang disusun berdasarkan HAProxy, menggunakan stick-tables yang sama.  Batas tersebut dikonfigurasi dengan cukup sederhana dan memungkinkan Anda membatasi pengguna dengan jumlah permintaan ke API.  Algoritma ini mengingat IP sumber dari mana permintaan dibuat, dan membatasi jumlah permintaan simultan dari satu pengguna.  Tentu saja, kami menghitung rata-rata profil pemuatan API untuk setiap layanan dan menetapkan batas â‰ˆ 10 kali dari nilai ini.  Sampai sekarang, kami terus memantau situasi dengan seksama, kami terus memantau. <br><br>  Seperti apa praktiknya?  Kami memiliki pelanggan yang terus-menerus menggunakan API skala otomatis kami.  Mereka membuat sekitar dua atau tiga ratus mesin virtual lebih dekat ke pagi hari dan menghapusnya lebih dekat ke malam hari.  Untuk OpenStack, buat mesin virtual, juga dengan layanan PaaS, setidaknya 1000 permintaan API, karena interaksi antara layanan juga terjadi melalui API. <br><br>  Pelemparan tugas seperti itu menyebabkan beban yang agak besar.  Kami memperkirakan beban ini, mengumpulkan puncak harian, meningkatkannya sepuluh kali lipat, dan ini menjadi batas kurs kami.  Kami menjaga jari kami pada denyut nadi.  Kita sering melihat bot, scanner, yang mencoba melihat kita, apakah kita memiliki skrip CGA yang dapat dijalankan, kita secara aktif memotongnya. <br><br><h2>  Cara memperbarui basis kode secara diam-diam untuk pengguna </h2><br>  Kami juga menerapkan toleransi kesalahan pada tingkat proses penyebaran kode.  Ada crash saat peluncuran, tetapi dampaknya pada ketersediaan layanan dapat diminimalkan. <br><br>  Kami terus memperbarui layanan kami dan harus memastikan proses memperbarui basis kode tanpa efek bagi pengguna.  Kami berhasil memecahkan masalah ini menggunakan kemampuan manajemen HAProxy dan penerapan Shutdown Graceful dalam layanan kami. <br><br>  Untuk mengatasi masalah ini, perlu untuk menyediakan kontrol penyeimbang dan penutupan layanan yang "benar": <br><br><ul><li>  Dalam kasus HAProxy, kontrol dilakukan melalui file statistik, yang pada dasarnya adalah sebuah soket dan didefinisikan dalam konfigurasi HAProxy.  Anda dapat mengirim perintah kepadanya melalui stdio.  Tetapi alat kontrol konfigurasi utama kami adalah memungkinkan, sehingga memiliki modul bawaan untuk mengelola HAProxy.  Yang kami gunakan secara aktif. </li><li>  Sebagian besar layanan API dan Mesin kami mendukung teknologi shutdown yang anggun: setelah shutdown, mereka menunggu tugas saat ini untuk diselesaikan, baik itu permintaan http atau semacam tugas utilitas.  Hal yang sama terjadi pada pekerja.  Dia tahu semua tugas yang dia lakukan, dan berakhir ketika dia telah berhasil menyelesaikan semuanya. </li></ul><br>  Berkat dua poin ini, algoritma aman penerapan kami adalah sebagai berikut. <br><br><ol><li>  Pengembang membuat paket kode baru (kami memiliki RPM), tes di lingkungan dev, tes di panggung, dan meninggalkannya di repositori panggung. </li><li>  Pengembang menempatkan tugas pada penyebaran dengan deskripsi paling rinci dari "artefak": versi paket baru, deskripsi fungsi baru dan detail lainnya tentang penyebaran, jika perlu. </li><li>  Administrator sistem memulai peningkatan.  Meluncurkan playbook Ansible, yang pada gilirannya melakukan hal berikut: <br><ul><li>  Dibutuhkan paket dari repositori panggung, memperbarui versi paket dalam repositori produk dengannya. </li><li>  Membuat daftar backend dari layanan yang diperbarui. </li><li>  Mematikan layanan yang pertama diperbarui di HAProxy dan menunggu akhir dari prosesnya.  Berkat shutdown anggun, kami yakin bahwa semua permintaan klien saat ini akan selesai dengan sukses. </li><li>  Setelah API, pekerja, dan HAProxy benar-benar dihentikan, kode diperbarui. </li><li>  Layanan peluncuran yang dimungkinkan. </li><li>  Untuk setiap layanan, ia menarik "pena" tertentu yang melakukan pengujian unit untuk sejumlah tes kunci yang telah ditentukan.  Pemeriksaan dasar dari kode baru terjadi. </li><li>  Jika tidak ada kesalahan yang ditemukan pada langkah sebelumnya, backend diaktifkan. </li><li>  Pergi ke backend berikutnya. </li></ul></li><li>  Setelah memperbarui semua backend, tes fungsional diluncurkan.  Jika tidak cukup, maka pengembang akan melihat fungsionalitas baru yang dia lakukan. </li></ol><br>  Pada penyebaran ini selesai. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-b/km/dt/-bkmdt98ituj53jetxiaay4uf4c.jpeg"></div><br>  <i>Siklus pembaruan layanan</i> <br><br>  Skema ini tidak akan berfungsi jika kami tidak memiliki satu aturan.  Kami mendukung versi lama dan baru dalam pertempuran.  Di muka, pada tahap pengembangan perangkat lunak, dinyatakan bahwa bahkan jika ada perubahan dalam database layanan, mereka tidak akan merusak kode sebelumnya.  Akibatnya, basis kode diperbarui secara bertahap. <br><br><h2>  Kesimpulan </h2><br>  Berbagi pemikiran saya sendiri tentang arsitektur WEB yang toleran terhadap kesalahan, saya ingin sekali lagi mencatat poin-poin utamanya: <br><br><ul><li>  toleransi kesalahan fisik; </li><li>  toleransi kesalahan jaringan (penyeimbang, BGP); </li><li>     . </li></ul><br>   uptime! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id474180/">https://habr.com/ru/post/id474180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id474166/index.html">Tutup Indeks untuk GiST</a></li>
<li><a href="../id474170/index.html">Pengakuan Desain - 15 November, Moscow, DI Telegraph</a></li>
<li><a href="../id474172/index.html">Denda 30 ribu euro untuk penggunaan cookie ilegal</a></li>
<li><a href="../id474176/index.html">11 video dari hari pertama DevFest 2019 di Kaliningrad</a></li>
<li><a href="../id474178/index.html">IVR di Webhook</a></li>
<li><a href="../id474184/index.html">Kami melewati tantangan dari Callum Macrae 100%</a></li>
<li><a href="../id474186/index.html">Menyangkal Mitos: Praktek IT Nyata di Armenia</a></li>
<li><a href="../id474192/index.html">Mengapa saya beralih dari UX ke PM lalu ke Lead PM dan apa yang telah berubah?</a></li>
<li><a href="../id474194/index.html">Tim kompas</a></li>
<li><a href="../id474196/index.html">10 tonggak terpenting dalam pengembangan AI saat ini</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>