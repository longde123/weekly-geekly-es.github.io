<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå•Ô∏è üíÜüèø üëéüèæ Como a Yandex ensinou intelig√™ncia artificial a encontrar erros nas not√≠cias üà≥ üèÇüèª üßëüèΩ‚Äçü§ù‚Äçüßëüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Costumamos falar sobre tecnologias e bibliotecas que se originaram e se formaram no Yandex. De fato, pelo menos aplicamos e desenvolvemos solu√ß√µes de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como a Yandex ensinou intelig√™ncia artificial a encontrar erros nas not√≠cias</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/479662/">  Costumamos falar sobre tecnologias e bibliotecas que se originaram e se formaram no Yandex.  De fato, pelo menos aplicamos e desenvolvemos solu√ß√µes de terceiros. <br><br>  Hoje vou contar √† comunidade Habr sobre um desses exemplos.  Voc√™ aprender√° por que ensinamos a rede neural BERT a encontrar erros de digita√ß√£o nas manchetes das not√≠cias e n√£o utilizou o modelo pronto, por que voc√™ n√£o pode executar e executar o BERT em v√°rias placas de v√≠deo e como usamos o recurso principal dessa tecnologia - o mecanismo de aten√ß√£o. <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br><a name="habracut"></a><h2>  Desafio </h2><br>  O Yandex.News √© um servi√ßo que coleta not√≠cias de publica√ß√µes conectadas a n√≥s.  Esta n√£o √© apenas a not√≠cia de m√≠dia mais lida e citada na p√°gina principal, mas tamb√©m se√ß√µes <a href="https://yandex.ru/sport">tem√°ticas</a> ou at√© sele√ß√µes pessoais de todas as publica√ß√µes.  De qualquer forma, s√£o milhares de sites e milh√µes de t√≠tulos, dos quais a m√°quina deve formar uma sele√ß√£o a cada poucos minutos. <br><br>  √â a m√°quina, porque nunca intervimos na foto do dia: n√£o adicionamos not√≠cias l√° manualmente, n√£o as removemos de l√° (n√£o importa o quanto gostar√≠amos), n√£o editamos as manchetes.  Em torno disso j√° foram quebradas muitas c√≥pias.  Uma abordagem totalmente algor√≠tmica tem pr√≥s e contras.  Algo que podemos melhorar com a tecnologia, algo que n√£o.  Mesmo se houver erros ortogr√°ficos ou erros de digita√ß√£o nos t√≠tulos, n√£o os corrigimos.  Adicionamos os favicons das publica√ß√µes √†s manchetes, para que fique claro de onde v√™m as not√≠cias.  Em parte, isso ajudou, mas n√£o chegamos a um acordo com os erros e come√ßamos a procurar uma maneira de nos livrar deles sem fazer altera√ß√µes no texto. <br><br>  Se for imposs√≠vel corrigir o erro, voc√™ poder√° treinar a m√°quina para encontrar cabe√ßalhos que, devido a erros, n√£o s√£o adequados para a parte superior.  Al√©m disso, a Yandex se especializou em morfologia russa desde o momento em que o nome ainda n√£o havia sido inventado.  Parece que adotamos uma rede neural - e o ponto est√° no chap√©u. <br><br><h2>  As ferramentas </h2><br>  A Yandex possui a tecnologia <a href="https://yandex.ru/dev/speller/">Speller</a> para encontrar e corrigir erros.  Gra√ßas √† <a href="https://habr.com/ru/company/yandex/blog/333522/">biblioteca de</a> aprendizado de m√°quina <a href="https://habr.com/ru/company/yandex/blog/333522/">CatBoost, o</a> Speller pode descriptografar palavras irreconhec√≠veis ("adjetivos" ‚Üí "colegas de classe") e levar em conta o contexto ao pesquisar erros de digita√ß√£o ("falta de m√∫sica" ‚Üí "baixar m√∫sica").  Pode parecer que Speller √© ideal para a nossa tarefa, mas n√£o. <br><br>  O verificador ortogr√°fico (conhecido internamente como respons√°vel pela pesquisa) j√° est√° aprimorado no n√≠vel da arquitetura para resolver uma tarefa completamente diferente: ajudar os usu√°rios a restaurar o formul√°rio de solicita√ß√£o correto.  Na Pesquisa, n√£o √© t√£o importante se o caso est√° selecionado corretamente, se h√° uma letra mai√∫scula ou uma v√≠rgula.  L√°, √© mais importante para a consulta de pesquisa "Haminguel" supor que a pessoa tinha em mente Hemingway. <br><br>  Os erros nas manchetes s√£o cometidos por pessoas relativamente alfabetizadas que dificilmente escrever√£o Haminguel.  Mas a aprova√ß√£o incorreta ("o voo atrasou"), falta de palavras ("o jovem experimentou o carro") e letras mai√∫sculas extras ("Presidente do Banco") s√£o comuns.  Finalmente, h√° uma frase formalmente correta: "Repararei a Gorky Street em Pskov", √† qual um guardi√£o normal n√£o se apega (bem, e se isso for uma promessa do autor?), Mas essa √© obviamente uma manchete de not√≠cias mimada.  Al√©m disso, no News, a tarefa n√£o era a mesma da Pesquisa: n√£o para corrigir erros de digita√ß√£o e erros, mas para detect√°-los. <br><br>  T√≠nhamos outras op√ß√µes, por exemplo, modelos baseados no DSSM (se interessante, falamos brevemente sobre essa abordagem em um post sobre <a href="https://habr.com/ru/company/yandex/blog/314222/">o algoritmo de Palekh</a> ), mas eles tamb√©m tiveram limita√ß√µes.  Por exemplo, a ordem das palavras n√£o foi perfeitamente levada em considera√ß√£o. <br><br>  Em geral, as ferramentas prontas ou n√£o eram adequadas para a nossa tarefa ou eram limitadas.  Ent√£o, voc√™ precisa criar o seu pr√≥prio - para treinar seu modelo.  E esse foi um bom motivo para trabalhar com a tecnologia BERT, que ficou dispon√≠vel para os desenvolvedores em 2018 e mostrou resultados impressionantes. <br><br><h2>  Apresentando o BERT </h2><br>  O principal problema dos problemas modernos de processamento de linguagem natural (PNL) √© encontrar exemplos suficientes marcados por pessoas para treinar uma rede neural.  Se voc√™ precisar de um crescimento de qualidade, a amostra de treinamento deve ser muito grande - milh√µes e bilh√µes de exemplos.  Ao mesmo tempo, h√° muitas tarefas na PNL e todas s√£o diferentes.  A coleta de dados em volumes semelhantes para cada tarefa √© longa, cara e muitas vezes imposs√≠vel.  Mesmo para as maiores empresas do mundo. <br><br>  Mas h√° uma op√ß√£o para contornar esse problema - com a ajuda do treinamento em duas etapas.  Inicialmente, a rede neural recebe uma estrutura de linguagem por um tempo longo e caro em um enorme corpo de bilh√µes de palavras (isso √© pr√©-treinamento).  Em seguida, a rede √© alterada de maneira r√°pida e barata para uma tarefa espec√≠fica - por exemplo, dividir as an√°lises em boas e ruins (isso √© ajuste fino).  Chega de cerca de 10 mil exemplos marcados em <a href="https://habr.com/ru/company/yandex/blog/305956/">Tolok</a> . <br><br>  A tecnologia BERT (Representa√ß√µes do codificador bidirecional dos transformadores) √© baseada nessa id√©ia.  A ideia em si n√£o √© nova e j√° foi aplicada antes, mas h√° uma diferen√ßa significativa.  Transformer √© uma arquitetura de rede neural que permite que voc√™ leve em considera√ß√£o todo o contexto de uma s√≥ vez, incluindo o outro extremo da frase e a rotatividade de particulados em algum lugar no meio.  E essa √© a diferen√ßa das arquiteturas da moda anteriores, que levaram em conta o contexto.  Por exemplo, em uma rede neural LSTM, o comprimento do contexto √© no m√°ximo dezenas de palavras, e aqui todas as 200. <br><br>  No <a href="https://github.com/google-research/bert">GitHub</a> , o c√≥digo fonte do TensorFlow e at√© um modelo universal pr√©-treinado em 102 idiomas est√£o dispon√≠veis, do russo ao volapyuk.  Pegue, ao que parece, a solu√ß√£o pronta para uso - e obtenha o resultado imediatamente.  Mas n√£o. <br><br>  Descobriu-se que o modelo universal em textos russos mostrava significativamente menos qualidade que o modelo em ingl√™s, quebrando recordes em textos em ingl√™s (o que, como voc√™ v√™, √© l√≥gico).  Nos textos em russo, ela perdeu para nossos modelos internos no DSSM. <br><br>  Ok, voc√™ pode se pr√©-educar - felizmente, o Yandex tem textos e experi√™ncia em russo suficientes em aprendizado de m√°quina.  Mas h√° uma nuance.  Demora um ano para aprender! <br><br>  O fato √© que o BERT √© voltado para processadores de tensores (TPUs) do Google; portanto, pronto para uso, ele pode funcionar com apenas uma placa de v√≠deo (GPU).  E √© imposs√≠vel paralelizar a testa com qualquer <a href="https://github.com/horovod">horovod</a> : transferir 400 megabytes de dados de um cart√£o para outro a cada passo √© muito caro, a paraleliza√ß√£o se torna in√∫til.  O que fazer <br><br><h2>  Otimiza√ß√£o </h2><br>  Eles come√ßaram a procurar id√©ias e solu√ß√µes que pudessem acelerar significativamente o assunto.  Antes de tudo, percebemos que cada n√∫mero em nosso modelo ocupava 32 bits de mem√≥ria (a flutua√ß√£o padr√£o para n√∫meros no computador).  Parece ser pequeno, mas quando voc√™ tem 100 milh√µes de pesos, isso √© cr√≠tico.  Como n√£o precis√°vamos de tanta precis√£o em todos os lugares, decidimos converter parcialmente os n√∫meros para o formato de 16 bits (isso √© chamado de treinamento de precis√£o mista). <br><br>  Ao longo do caminho, com a ajuda de muitos arquivos e muletas, estragamos a compila√ß√£o do XLA, contando com o <a href="https://github.com/google-research/bert/pull/255">comprometimento</a> ainda cru da NVIDIA.  Gra√ßas a isso, nossas placas NVIDIA Tesla V100 (um pequeno servidor delas fica como um apartamento em uma √°rea barata de Moscou) foram capazes de revelar completamente seu potencial devido √† aritm√©tica de 16 bits nos n√∫cleos tensores. <br><br>  Est√°vamos interessados ‚Äã‚Äãapenas nas manchetes em russo, mas o modelo multil√≠ngue, que tomamos como base, foi treinado em centenas de idiomas, incluindo at√© um volapuk artificial.  Palavras de todas as l√≠nguas traduzidas para o espa√ßo vetorial foram armazenadas no modelo.  Al√©m disso, voc√™ n√£o pode peg√°-los e remov√™-los de l√° - tive que suar para reduzir o tamanho do dicion√°rio. <br><br>  E mais uma coisa.  Se voc√™ √© um cientista e seu computador est√° embaixo da mesa, √© poss√≠vel reconfigurar tudo o que existe para cada tarefa espec√≠fica.  Mas em uma nuvem de computa√ß√£o real, onde milhares de m√°quinas s√£o configuradas da mesma maneira, √© bastante problem√°tico, por exemplo, reconstruir o kernel para cada novo recurso do TensorFlow.  Portanto, dedicamos muito esfor√ßo na coleta dessas vers√µes de pacotes que todos os chips novos podem executar, e n√£o exigem atualiza√ß√£o e reconfigura√ß√£o radicais das placas de v√≠deo na nuvem. <br><br>  Em geral, espremiam todos os sucos sempre que podiam.  E n√≥s fizemos isso.  O ano se transformou em uma semana. <br><br><h2>  Treinamento </h2><br>  Construir o conjunto de dados correto geralmente √© a parte mais dif√≠cil do trabalho.  Primeiro, aprendemos o classificador em tr√™s milh√µes de t√≠tulos marcados com tolokers.  Parece ser muito, mas apenas 30 mil deles - com erros de digita√ß√£o.  Onde obter mais exemplos? <br><br>  Decidimos ver quais t√≠tulos a pr√≥pria m√≠dia estava correta.  Existem mais de 2 milh√µes na hist√≥ria da Yandex.News.  Bingo!  Embora fosse muito cedo para se alegrar. <br><br>  Aconteceu que muitas vezes a m√≠dia refaz as manchetes n√£o por causa de erros.  Novos detalhes vieram √† tona - e o editor substituiu uma reda√ß√£o correta por outra.  Portanto, nos limitamos a corre√ß√µes com uma diferen√ßa entre vers√µes de at√© tr√™s letras (embora ainda houvesse algum ru√≠do aqui: foi "encontrado duas" - tornou-se "encontrado tr√™s").  Ent√£o, marcamos um milh√£o de erros de digita√ß√£o.  Estudamos primeiro nesta grande sele√ß√£o com ru√≠do e, depois, em uma pequena marca√ß√£o de tolker sem ru√≠do. <br><br><h2>  Qualidade </h2><br>  Em tais tarefas, √© habitual medir a precis√£o e a integridade.  No nosso caso, precis√£o √© a propor√ß√£o de veredictos corretos entre todos os veredictos sobre um erro no cabe√ßalho.  Completude - a propor√ß√£o de cabe√ßalhos de erro que capturamos entre todos os cabe√ßalhos de erro.  Tanto isso como outro no mundo ideal devem aspirar a 100%.  Mas nas tarefas de aprendizado de m√°quina, esses indicadores tendem a entrar em conflito.  Ou seja, quanto mais distorcemos a precis√£o, mais a integridade cai.  E vice-versa. <br><br>  Em nossa abordagem anterior baseada no DSSM, j√° alcan√ßamos 95% de precis√£o (ou seja, 5% de veredictos falsos positivos).  Este j√° √© um indicador bastante alto.  Portanto, decidimos manter o mesmo n√≠vel de precis√£o e ver como a integridade muda com o novo modelo.  E ela saltou de 21 para 78%.  E √© definitivamente um sucesso. <br><br>  Aqui seria poss√≠vel acabar com isso, mas lembro-me da promessa de falar sobre aten√ß√£o. <br><br><h2>  Rede neural com caneta de feltro </h2><br>  √â geralmente aceito que uma rede neural √© uma caixa preta.  Alimentamos algo na entrada e obtemos algo na sa√≠da.  Por que e como √© um mist√©rio. <br><br>  Essa limita√ß√£o tem como objetivo contornar redes neurais interpretadas.  O BERT √© um deles.  Sua interpretabilidade reside no mecanismo de aten√ß√£o.  Grosso modo, em cada camada da rede neural repetimos a mesma t√©cnica: olhamos para as palavras vizinhas com ‚Äúaten√ß√£o‚Äù diferente e levamos em conta a intera√ß√£o com elas.  Por exemplo, quando uma rede neural processa o pronome "ele", "olha atentamente" para o substantivo ao qual "ele" se refere. <br><br>  A figura abaixo mostra em diferentes tons de vermelho quais palavras o token ‚Äúolha‚Äù, o que acumula informa√ß√µes sobre o t√≠tulo inteiro da camada final do classificador.  Se um erro de digita√ß√£o na palavra - aten√ß√£o o destaca, se as palavras forem inconsistentes -, ent√£o ambas (e, possivelmente, dependem delas). <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br>  Nesse lugar, a prop√≥sito, pode-se discernir todo o potencial das redes neurais.  Em nenhum est√°gio do treinamento, nosso modelo sabe exatamente onde est√° localizado o erro de digita√ß√£o no exemplo: ele sabe apenas que todo o t√≠tulo est√° incorreto.  E ainda assim ela aprende que ‚Äúuma escola para 1224 vagas‚Äù √© incorreta ao escrever por causa de um n√∫mero inconsistente e destaca especificamente o n√∫mero 4. <br><br>  N√£o paramos com erros de digita√ß√£o e come√ßamos a aplicar uma nova abordagem n√£o apenas para procurar erros, mas tamb√©m para identificar cabe√ßalhos obsoletos.  Mas esta √© uma hist√≥ria completamente diferente com a qual esperamos retornar a Habr em um futuro pr√≥ximo. <br><br><h2>  Links √∫teis para quem deseja se aprofundar no t√≥pico </h2><br><ul><li>  <a href="https://github.com/google-research/bert">C√≥digo TensorFlow e modelos pr√©-treinados para BERT</a> </li><li>  <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">Open Sourcing BERT: Pr√©-treinamento de ponta para processamento de idiomas naturais</a> </li><li>  <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPUs vs GPUs para transformadores (BERT)</a> </li><li>  <a href="http://jalammar.github.io/illustrated-transformer/">O transformador ilustrado</a> </li><li>  <a href="https://news.developer.nvidia.com/nvidia-achieves-4x-speedup-on-bert-neural-network/">NVIDIA atinge velocidade 4X na rede neural BERT</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt479662/">https://habr.com/ru/post/pt479662/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt479646/index.html">Maus conselhos ou raz√µes para continuar a aprender ingl√™s ap√≥s o Intermedi√°rio</a></li>
<li><a href="../pt479650/index.html">Os 12 principais infogr√°ficos din√¢micos de TI mais interessantes</a></li>
<li><a href="../pt479654/index.html">Django vue generator</a></li>
<li><a href="../pt479656/index.html">Antipatterns PostgreSQL: estat√≠sticas em torno da cabe√ßa</a></li>
<li><a href="../pt479660/index.html">3. An√°lise de malware usando o Check Point forense. Sandblast mobile</a></li>
<li><a href="../pt479664/index.html">Como os kubernetes gerenciados e o OpenShift gerenciado funcionam no IBM Cloud. Parte 1 - Arquitetura e Seguran√ßa</a></li>
<li><a href="../pt479666/index.html">Golang: Em que um especialista Go se baseia em um mar de especialidades de TI?</a></li>
<li><a href="../pt479668/index.html">Controle de qualidade para iniciantes: como testar um foguete ou avi√£o?</a></li>
<li><a href="../pt479672/index.html">Farejador CAN</a></li>
<li><a href="../pt479676/index.html">ExtJS 7 e Spring Boot 2. Como criar um SPA que interaja com sua API e plugins ReactJS externos?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>