<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⌚️ 👨🏽‍🤝‍👨🏻 🧕 Anwendung des automatischen maschinellen Lernens auf neuronale Netze mit Transformatorarchitektur 🐍 👨🏿‍🎤 👩🏼‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aus dem Google AI-Blog 

 Seit der Veröffentlichung von Informationen über sie im Jahr 2017 wurden neuronale Netze der Transformatorarchitektur auf Au...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Anwendung des automatischen maschinellen Lernens auf neuronale Netze mit Transformatorarchitektur</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460099/"> <i>Aus dem Google AI-Blog</i> <br><br>  Seit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Veröffentlichung von Informationen</a> über sie im Jahr 2017 wurden neuronale Netze der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformatorarchitektur</a> auf Aufgaben verschiedener Art angewendet, von der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellung von Texten im Fantasy-Stil</a> bis zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schreiben musikalischer Harmonien</a> .  Was wichtig ist, die hohe Qualität der Arbeit von „Transformatoren“ hat gezeigt, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">direkte verteilte neuronale Netze</a> bei sequentiellen Aufgaben wie Sprachmodellierung und -übersetzung genauso effektiv sein können wie wiederkehrende.  Obwohl die Popularität von Transformatoren und anderen Direktverteilungsmodellen, die für sequentielle Aufgaben verwendet werden, zunimmt, werden ihre Architekturen fast immer manuell erstellt, im Gegensatz zum Bereich der Bildverarbeitung, in dem Ansätze für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fortgeschrittenes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschinelles Lernen</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AOM</a> ) bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fortgeschrittene Modelle entdeckt haben</a> , die den exponierten voraus sind manuelle Einstellung.  Wir waren natürlich daran interessiert, ob die Anwendung von AOM auf sequentielle Aufgaben den gleichen Erfolg erzielen kann. <br><a name="habracut"></a><br>  Nachdem wir eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">evolutionäre</a> Suche nach Neuroarchitektur (NAS) durchgeführt und die Übersetzung als Beispiel für sequentielle Aufgaben verwendet hatten, entdeckten wir einen sich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">entwickelnden Transformator</a> (ET) - eine neue Transformatorarchitektur, die Verbesserungen bei verschiedenen Aufgaben der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verarbeitung natürlicher Sprache</a> (OYA) demonstriert.  ET erzielt nicht nur topaktuelle Übersetzungsergebnisse, sondern zeigt auch eine verbesserte Effizienz bei der Modellierung der Sprache im Vergleich zum ursprünglichen Transformator.  Wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">veröffentlichen ein</a> neues Modell in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensor2Tensor-</a> Bibliothek, in dem es für jede sequentielle Aufgabe verwendet werden kann. <br><br><h2>  Technikerentwicklung </h2><br>  Um die evolutionäre Suche nach Neuroarchitektur zu beginnen, mussten wir neue Techniken entwickeln, da die Aufgabe, die zur Bewertung der „Fitness“ jeder Architektur, der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Übersetzung vom Englischen ins Deutsche WMT'14</a> , verwendet wurde, hohe Rechenressourcen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erforderte</a> .  Infolgedessen erweisen sich diese Suchvorgänge als anspruchsvoller als ähnliche Suchvorgänge im Bereich Computer Vision, die mit kleineren Datenbanken, beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CIFAR-10, betrieben werden können</a> .  Die erste dieser Techniken ist ein Warmstart, bei dem die ursprüngliche Evolutionspopulation mit transformatorartigen Architekturen anstelle von Zufallsmodellen gesät wird.  Dies hilft, die Suche auf den offensichtlich starken Bereich des Suchraums zu konzentrieren, sodass wir schnell die besten Modelle finden können. <br><br>  Die zweite Technik ist eine neue von uns entwickelte Methode namens Progressive Dynamic Hurdles (PDH).  Dieser Algorithmus ergänzt die evolutionäre Suche und ermöglicht es Ihnen, den stärksten Kandidaten mehr Ressourcen zuzuweisen, im Gegensatz zu früheren Arbeiten, bei denen jedem Kandidatenmodell im NAS die gleiche Menge an Ressourcen zugewiesen wurde.  Mit PDH können wir ein Modell früher evaluieren, wenn es furchtbar schlecht ist, und vielversprechende Architekturen mit zahlreichen Ressourcen belohnen. <br><br><h2>  Weiterentwickelter Transformator </h2><br>  Mit diesen Methoden führten wir eine umfangreiche NAS-Suche für unsere Übersetzungsaufgabe durch und entdeckten ETs.  Wie die meisten neuronalen Netzwerkarchitekturen vom Typ "Sequenz zu Sequenz" (Sequenz zu Sequenz, seq2seq) verfügt es über einen Codierer, der die Eingabesequenz in die Einfügungen codiert, und einen Decodierer, der diese Einfügungen verwendet, um die Ausgabesequenz zu erstellen.  Im Falle einer Übersetzung ist die Eingabesequenz ein Übersetzungsangebot und die Ausgabesequenz ist eine Übersetzung. <br><br>  Das interessanteste Merkmal von ETs sind die Faltungsschichten am unteren Rand der Module sowohl des Codierers als auch des Decodierers, die auf ähnliche Weise zu diesen beiden Stellen verzweigt sind (dh die Eingänge durchlaufen vor dem Falten zwei verschiedene Faltungsschichten). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/62d/1d1/b8a62d1d155203861756f0960becaaf0.png"><br>  <i>Vergleich der Architektur herkömmlicher Encoder und ET-Encoder.</i>  <i>Achten Sie auf die verzweigte Faltungsstruktur am unteren Rand des Moduls, die sowohl im Codierer als auch im Decodierer unabhängig voneinander gebildet wird.</i>  <i>Der Decoder wird in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserer Arbeit</a> ausführlich beschrieben.</i> <br><br>  Dies ist besonders interessant, da der Codierer und der Decodierer während des NAS keine Architekturen miteinander teilen und die Nützlichkeit dieser Architektur unabhängig voneinander im Codierer und Decodierer entdeckt wurde, was für ein solches Schema spricht.  Wenn sich der ursprüngliche Transformator ausschließlich darauf stützte, die Aufmerksamkeit auf dieselben Daten zu lenken, die er selbst erzeugt hat [Selbstaufmerksamkeit], ist ET ein Hybrid, der sowohl Selbstaufmerksamkeit als auch breite Faltung ausnutzt. <br><br><h2>  ET Punktzahl </h2><br>  Um die Wirksamkeit dieser neuen Architektur zu testen, haben wir sie zunächst mit dem ursprünglichen Transformator verglichen, der die Aufgabe hatte, aus dem Englischen ins Deutsche zu übersetzen, die wir bei der Suche verwendet haben.  Wir haben festgestellt, dass ET bei allen Parametergrößen die besten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BLEU-</a> Indikatoren und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konnektivität</a> aufweist. Der größte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Größengewinn</a> ist vergleichbar mit mobilen Geräten (~ 7 Millionen Parameter), was auf die effiziente Verwendung von Parametern hinweist.  Bei größeren Größen erzielt ET mit WMT '14 En-De mit einer BLEU von 29,8 und einer SacreBLEU von 29,2 Spitzenergebnisse. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a2/4c4/c60/1a24c4c6085c167a5398fdcea218f75c.png"></div><br>  <i>Vergleich von ET und Originaltransformator auf WMT'14 En-De mit unterschiedlichen Lautstärken.</i>  <i>Der größte Vorteil wird bei kleinen Größen erzielt, während ET bei größeren Größen eine gute Leistung zeigt, vor dem größten Transformator mit 37,6% weniger Parametern (vergleichbare Modelle sind in Kreisen).</i> <br><br>  Um die Generalisierbarkeit zu überprüfen, haben wir ET mit einem Transformator auf zusätzliche Probleme der Verarbeitung natürlicher Sprache verglichen.  Zuerst haben wir die Übersetzungen für verschiedene Sprachpaare überprüft und festgestellt, dass die Wirksamkeit von ET höher ist und die Trennung ungefähr der in der englisch-deutschen Übersetzung gezeigten entspricht.  und wieder wird dank der effizienten Verwendung von Parametern die größte Lücke bei mittelgroßen Modellen beobachtet.  Wir haben auch die Decoder beider Modelle zur Sprachmodellierung in <a href="">LM1B verglichen</a> und eine signifikante Verbesserung der Konnektivität <a href="">festgestellt</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c5/c96/1b6/6c5c961b6a09ba83905d9f886c9063ea.png"><br><br><h2>  Zukunftspläne </h2><br>  Diese Ergebnisse sind der erste Schritt bei der Untersuchung der Architektur-Suchanwendung für sequentielle Direktverteilungsmodelle.  ET wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Open Source</a> im Rahmen des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://www.google.com/url%3Fq%3D">Tensor2Tensor-</a> Projekts verteilt, wo es bei aufeinanderfolgenden Problemen verwendet werden kann.  Um die Reproduzierbarkeit zu verbessern, öffnen wir auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Suchbereichscode</a> , den wir bei unserer Suche verwendet haben, und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Colab</a> mit der PDH-Implementierung.  Wir freuen uns auf die Ergebnisse der mit neuen Modellen ausgestatteten Forschungsgemeinschaft und hoffen, dass andere diese neuen Suchtechniken als Grundlage nehmen können! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460099/">https://habr.com/ru/post/de460099/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460087/index.html">Pyramidensortierung (HeapSort)</a></li>
<li><a href="../de460089/index.html">Sicheres Update der Zimbra Collaboration Suite</a></li>
<li><a href="../de460091/index.html">Direkter Druck auf T-Shirts mit Epson SureColor SC - F und sein Unterschied zu Siebdruck, Abziehbild und Sublimation</a></li>
<li><a href="../de460095/index.html">Auf gitlab.com wurde ein Verbot für Fork DeepNude verhängt</a></li>
<li><a href="../de460097/index.html">Die Matrix bietet Ihnen: einen Überblick über Projekte mit MITRE ATT & CK</a></li>
<li><a href="../de460101/index.html">Cookie-basierter XSS-Betrieb | $ 2300 Bug Bounty Geschichte</a></li>
<li><a href="../de460107/index.html">ISPsystem, verzeihen und auf Wiedersehen! Warum und wie wir unser Server Control Panel geschrieben haben</a></li>
<li><a href="../de460109/index.html">Winkel: Wenn Sie die Anwendung sehen müssen, das Backend jedoch noch nicht fertig ist</a></li>
<li><a href="../de460111/index.html">Aktualisierte Version von SAP Business One 9.3: Was hat sich geändert?</a></li>
<li><a href="../de460113/index.html">Ein paar Geschichten aus dem Leben von JSOC CERT oder Unbanal Forensics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>