<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïµÔ∏è üé∂ ‚ôêÔ∏è Reconnaissance des √©motions √† l'aide d'un r√©seau neuronal convolutif ‚ÜôÔ∏è üö∂üèª üí¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La reconnaissance des √©motions a toujours √©t√© un d√©fi passionnant pour les scientifiques. R√©cemment, je travaille sur un projet SER exp√©rimental (Spee...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconnaissance des √©motions √† l'aide d'un r√©seau neuronal convolutif</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  La reconnaissance des √©motions a toujours √©t√© un d√©fi passionnant pour les scientifiques.  R√©cemment, je travaille sur un projet SER exp√©rimental (Speech Emotion Recognition) pour comprendre le potentiel de cette technologie - pour cela, j'ai s√©lectionn√© les r√©f√©rentiels les plus populaires sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Github</a> et en ai fait la base de mon projet. <br><br>  Avant de commencer √† comprendre le projet, il sera bon de se rappeler quel type de goulots d'√©tranglement SER a. <br><a name="habracut"></a><br><h2>  Obstacles principaux </h2><br><ul><li>  les √©motions sont subjectives, m√™me les gens les interpr√®tent diff√©remment.  Il est difficile de d√©finir le concept m√™me d '¬´√©motion¬ª; </li><li>  commenter l'audio est difficile.  Devrions-nous en quelque sorte marquer chaque mot, chaque phrase ou la communication dans son ensemble?  Un ensemble d'√©motions √† utiliser en reconnaissance? </li><li>  la collecte de donn√©es n'est pas non plus facile.  De nombreuses donn√©es audio peuvent √™tre collect√©es √† partir de films et d'actualit√©s.  Cependant, les deux sources sont ¬´biais√©es¬ª car les informations doivent √™tre neutres et les √©motions des acteurs se jouent.  Il est difficile de trouver une source ¬´objective¬ª de donn√©es audio. </li><li>  les donn√©es de balisage n√©cessitent des ressources humaines et temporelles importantes.  Contrairement au dessin de cadres sur des images, il faut du personnel sp√©cialement form√© pour √©couter des enregistrements audio entiers, les analyser et fournir des commentaires.  Et puis ces commentaires doivent √™tre appr√©ci√©s par <b>beaucoup d'</b> autres personnes, car les notes sont subjectives. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Description du projet </h2><br>  Utilisation d'un r√©seau de neurones convolutionnels pour reconna√Ætre les √©motions dans les enregistrements audio.  Et oui, le propri√©taire du r√©f√©rentiel n'a fait r√©f√©rence √† aucune source. <br><br><h2>  Description des donn√©es </h2><br>  Il y a deux jeux de donn√©es qui ont √©t√© utilis√©s dans les r√©f√©rentiels RAVDESS et SAVEE, je viens d'adapter RAVDESS dans mon mod√®le.  Il existe deux types de donn√©es dans le contexte RAVDESS: la parole et la chanson. <br><br>  Ensemble de donn√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RAVDESS (la base de donn√©es audiovisuelle Ryerson de la parole et des chansons √©motionnelles)</a> : <br><br><ul><li>  12 acteurs et 12 actrices ont enregistr√© leur discours et leurs chansons dans leur performance; </li><li>  l'acteur n ¬∞ 18 n'a enregistr√© aucune chanson; </li><li>  √©motions Le d√©go√ªt (d√©go√ªt), le neutre (neutre) et les surprises (surpris) sont absents dans les donn√©es du "chant". </li></ul><br>  R√©partition des √©motions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Tableau de r√©partition des √©motions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Extraction de fonctionnalit√©s </h3><br>  Lorsque nous travaillons avec des t√¢ches de reconnaissance vocale, les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">coefficients cepstraux (MFCC)</a> sont une technologie avanc√©e, malgr√© le fait qu'ils soient apparus dans les ann√©es 80. <br><br>  Citation du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tutoriel MFCC</a> : <br><blockquote>  Cette forme d√©termine le son de sortie.  Si nous pouvons identifier la forme, cela nous donnera une repr√©sentation pr√©cise du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">phon√®me</a> sonn√©.  La forme du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tractus vocal se</a> manifeste dans une enveloppe √† court spectre, et le travail du MFCC est d'afficher avec pr√©cision cette enveloppe. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Forme d'onde</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Spectrogramme</font> <br><br>  Nous utilisons MFCC comme fonctionnalit√© d'entr√©e.  Si vous souhaitez en savoir plus sur ce qu'est le MFCC, ce <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">didacticiel</a> est fait pour vous.  Le t√©l√©chargement des donn√©es et leur conversion au format MFCC peuvent √™tre facilement effectu√©s √† l'aide du paquet librosa Python. <br><br><h3>  Architecture du mod√®le par d√©faut </h3><br>  L'auteur a d√©velopp√© un mod√®le CNN √† l'aide du package Keras, cr√©ant 7 couches - six couches Con1D et une couche de densit√© (Dense). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  L'auteur a comment√© les couches 4 et 5 dans la derni√®re version (18 septembre 2018) et la taille finale du fichier de ce mod√®le ne correspond pas au r√©seau fourni, donc je ne peux pas obtenir le m√™me r√©sultat en pr√©cision - 72%. </blockquote><br>  Le mod√®le est simplement form√© avec les param√®tres <code>batch_size=16</code> et <code>batch_size=16</code> <code>epochs=700</code> , sans programme de formation, etc. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Ici, la <code>categorical_crossentropy</code> est fonction des pertes et la mesure de l'√©valuation est la pr√©cision. <br><br><h2>  Mon exp√©rience </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Analyse exploratoire des donn√©es</a> </h3><br>  Dans le jeu de donn√©es RAVDESS, chaque acteur montre 8 √©motions, pronon√ßant et chantant 2 phrases, 2 fois chacune.  En cons√©quence, 4 exemples de chaque √©motion sont obtenus de chaque acteur, √† l'exception des √©motions neutres, du d√©go√ªt et de la surprise mentionn√©s ci-dessus.  Chaque audio dure environ 4 secondes, dans les premi√®re et derni√®re secondes, il s'agit le plus souvent de silence. <br><br>  <b>Offres typiques</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Observation </h3><br>  Apr√®s avoir s√©lectionn√© un ensemble de donn√©es provenant d'un acteur et d'une actrice, puis √©cout√© tous leurs enregistrements, j'ai r√©alis√© que les hommes et les femmes exprimaient leurs √©motions diff√©remment.  Par exemple: <br><br><ul><li>  la col√®re masculine (Angry) est juste plus forte; </li><li>  la joie des hommes (heureux) et la frustration (triste) - une caract√©ristique dans les tons de rire et de pleurs pendant le ¬´silence¬ª; </li><li>  la joie f√©minine (heureuse), la col√®re (en col√®re) et la frustration (triste) sont plus fortes; </li><li>  le d√©go√ªt f√©minin (d√©go√ªt) contient le son des vomissements. </li></ul><br><h3>  R√©p√©tition de l'exp√©rience </h3><br>  L'auteur a supprim√© les classes neutres, d√©go√ªt√©es et surprises pour faire la reconnaissance RAVDESS 10 de l'ensemble de donn√©es.  En essayant de r√©p√©ter l'exp√©rience de l'auteur, j'ai obtenu ce r√©sultat: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  Cependant, j'ai d√©couvert qu'il y a une fuite de donn√©es lorsque l'ensemble de donn√©es pour la validation est identique √† l'ensemble de donn√©es de test.  Par cons√©quent, j'ai r√©p√©t√© la s√©paration des donn√©es, isolant les ensembles de donn√©es de deux acteurs et deux actrices afin qu'ils ne soient pas visibles pendant le test: <br><br><ul><li>  les acteurs 1 √† 20 sont utilis√©s pour les ensembles Train / Valid dans un rapport 8: 2; </li><li>  les acteurs 21 √† 24 sont isol√©s des tests; </li><li>  Param√®tres du train: (1248, 216, 1); </li><li>  Param√®tres Set valides: (312, 216, 1); </li><li>  Param√®tres de l'ensemble de tests: (320, 216, 1) - (isol√©). </li></ul><br>  J'ai re-form√© le mod√®le et voici le r√©sultat: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Test de performance </h3><br>  D'apr√®s le graphique Train Valid Gross, il est clair qu'il n'y a pas de convergence pour les 10 classes s√©lectionn√©es.  J'ai donc d√©cid√© de r√©duire la complexit√© du mod√®le et de ne laisser que des √©motions masculines.  J'ai isol√© deux acteurs dans l'ensemble de test, et mis le reste dans le train / ensemble valide, rapport 8: 2.  Cela garantit qu'il n'y a pas de d√©s√©quilibre dans l'ensemble de donn√©es.  Ensuite, j'ai coach√© ‚Äã‚Äãs√©par√©ment les donn√©es masculines et f√©minines pour effectuer le test. <br><br>  <b>Ensemble de donn√©es masculin</b> <br><br><ul><li>  Train Set - 640 √©chantillons des acteurs 1-10; </li><li>  Ensemble valide - 160 √©chantillons des acteurs 1-10; </li><li>  Test Set - 160 √©chantillons des acteurs 11-12. </li></ul><br>  <b>Ligne de r√©f√©rence: hommes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Ensemble de donn√©es f√©minin</b> <br><br><ul><li>  Train Set - 608 √©chantillons des actrices 1-10; </li><li>  Ensemble valide - 152 √©chantillons des actrices 1-10; </li><li>  Set de test - 160 √©chantillons des actrices 11-12. </li></ul><br>  <b>Ligne de r√©f√©rence: femmes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  Comme vous pouvez le voir, les matrices d'erreur sont diff√©rentes. <br><br>  Hommes: Angry et Happy sont les principales classes pr√©vues dans le mod√®le, mais elles ne se ressemblent pas. <br><br>  Femmes: d√©sordre (triste) et joie (heureux) - classes essentiellement pr√©dites dans le mod√®le;  la col√®re et la joie se confondent facilement. <br><br>  Rappelant les observations de l' <b>Intelligence Data Analysis</b> , je soup√ßonne que les femmes Angry et Happy sont similaires au point de confusion parce que leur mode d'expression est simplement d'√©lever la voix. <br><br>  En plus de cela, je suis curieux de savoir que si je simplifie encore plus le mod√®le, ne laisse que les classes positives, neutres et n√©gatives.  Ou seulement positif et n√©gatif.  En bref, j'ai regroup√© les √©motions en 2 et 3 classes, respectivement. <br><br>  <b>2 cours:</b> <br><br><ul><li>  Positif: joie (heureux), calme (calme); </li><li>  N√©gatif: col√®re, peur (peur), frustration (triste). </li></ul><br>  <b>3 classes:</b> <br><br><ul><li>  Positif: joie (heureux); </li><li>  Neutre: calme (calme), neutre (neutre); </li><li>  N√©gatif: col√®re, peur (peur), frustration (triste). </li></ul><br>  Avant de commencer l'exp√©rience, j'ai mis en place l'architecture du mod√®le en utilisant des donn√©es masculines, faisant une reconnaissance √† 5 classes. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  J'ai ajout√© 2 couches de Conv1D, une couche de MaxPooling1D et 2 couches de BarchNormalization;  J'ai √©galement chang√© la valeur d'abandon √† 0,25.  Enfin, j'ai chang√© l'optimiseur en SGD avec une vitesse d'apprentissage de 0,0001. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  Pour entra√Æner le mod√®le, j'ai appliqu√© une r√©duction du ¬´plateau d'entra√Ænement¬ª et enregistr√© uniquement le meilleur mod√®le avec une valeur minimale de <code>val_loss</code> .  Et voici les r√©sultats pour les diff√©rentes classes cibles. <br><br><h2>  Performances du nouveau mod√®le </h2><br>  <b>Hommes, 5 classes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Femmes, 5e ann√©e</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>Hommes, 2e ann√©e</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>Hommes, 3 classes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Augmentation (augmentation) </h2><br>  Lorsque j'ai renforc√© l'architecture du mod√®le, l'optimiseur et la vitesse d'entra√Ænement, il s'est av√©r√© que le mod√®le ne converge toujours pas en mode entra√Ænement.  J'ai sugg√©r√© qu'il s'agit d'un probl√®me de quantit√© de donn√©es, car nous n'avons que 800 √©chantillons.  Cela m'a conduit √† des m√©thodes d'augmentation de l'audio, √† la fin j'ai doubl√© les jeux de donn√©es.  Jetons un coup d'≈ìil √† ces m√©thodes. <br><br><h3>  Hommes, 5 classes </h3><br>  <b>Incr√©ment dynamique</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>R√©glage de la hauteur</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>D√©calage</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Ajout de bruit blanc</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  Il est √† noter que l'augmentation augmente consid√©rablement la pr√©cision, jusqu'√† 70 +% dans le cas g√©n√©ral.  Surtout dans le cas de l'ajout de blanc, qui augmente la pr√©cision √† 87,19% - cependant, la pr√©cision du test et la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mesure F1</a> baissent de plus de 5%.  Et puis j'ai eu l'id√©e de combiner plusieurs m√©thodes d'augmentation pour un meilleur r√©sultat. <br><br><h3>  Combiner plusieurs m√©thodes </h3><br>  <b>Bruit blanc + biais</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Test d'augmentation sur les hommes </h2><br><h3>  Hommes, 2e ann√©e </h3><br>  <b>Bruit blanc + biais</b> <br><br>  Pour tous les √©chantillons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>Bruit blanc + biais</b> <br><br>  Uniquement pour les √©chantillons positifs, car l'ensemble √† 2 classes est d√©s√©quilibr√© (vers les √©chantillons n√©gatifs). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Pitch + White Noise</b> <br>  Pour tous les √©chantillons <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Pitch + White Noise</b> <br><br>  Pour les √©chantillons positifs uniquement <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Conclusion </h2><br>  En fin de compte, je n'ai pu exp√©rimenter qu'avec un ensemble de donn√©es masculin.  J'ai re-divis√© les donn√©es afin d'√©viter les d√©s√©quilibres et, par cons√©quent, les fuites de donn√©es.  J'ai configur√© le mod√®le pour exp√©rimenter avec des voix masculines, car je voulais simplifier le mod√®le autant que possible pour commencer.  J'ai √©galement effectu√© des tests en utilisant diff√©rentes m√©thodes d'augmentation;  l'ajout de bruit blanc et de biais a bien fonctionn√© sur les donn√©es asym√©triques. <br><br><h2>  Conclusions </h2><br><ul><li>  les √©motions sont subjectives et difficiles √† corriger; </li><li>  il est n√©cessaire de d√©terminer √† l'avance quelles √©motions conviennent au projet; </li><li>  Ne faites pas toujours confiance au contenu avec Github, m√™me s'il a beaucoup d'√©toiles; </li><li>  partage de donn√©es - gardez cela √† l'esprit; </li><li>  l'analyse exploratoire des donn√©es donne toujours une bonne id√©e, mais il faut √™tre patient quand il s'agit de travailler avec des donn√©es audio; </li><li>  D√©terminez ce que vous donnerez √† l'entr√©e de votre mod√®le: une phrase, un dossier entier ou une exclamation? </li><li>  le manque de donn√©es est un facteur de r√©ussite important dans SER, cependant, cr√©er un bon ensemble de donn√©es avec des √©motions est une t√¢che complexe et co√ªteuse; </li><li>  simplifiez votre mod√®le en cas de manque de donn√©es. </li></ul><br><h2>  Nouvelle am√©lioration </h2><br><ul><li>  J'ai utilis√© uniquement les 3 premi√®res secondes comme entr√©e pour r√©duire la taille globale des donn√©es - le projet d'origine utilisait 2,5 secondes.  Je voudrais exp√©rimenter avec des enregistrements en taille r√©elle; </li><li>  vous pouvez pr√©traiter les donn√©es: couper le silence, normaliser la longueur en remplissant avec des z√©ros, etc.; </li><li>  essayez les r√©seaux de neurones r√©currents pour cette t√¢che. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr461435/">https://habr.com/ru/post/fr461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr461421/index.html">Comment vous assurer contre d'√©ventuelles pertes lors d'un investissement en bourse: produits structurels</a></li>
<li><a href="../fr461423/index.html">11 conseils: comment pr√©senter le travail UI / UX √† des ¬´non-concepteurs¬ª</a></li>
<li><a href="../fr461425/index.html">Comment devenir chef de produit et √©voluer</a></li>
<li><a href="../fr461431/index.html">¬´Aime et n'aime pas¬ª: DNS sur HTTPS</a></li>
<li><a href="../fr461433/index.html">Utilisation d'Identity Server 4 dans Net Core 3.0</a></li>
<li><a href="../fr461437/index.html">370 ampoules</a></li>
<li><a href="../fr461439/index.html">D√©marrage de la biblioth√®que de composants React et TypeScript</a></li>
<li><a href="../fr461441/index.html">Rapports sur l'√©tat du stockage √† l'aide de R. Calcul parall√®le, graphiques, xlsx, e-mail et tout cela</a></li>
<li><a href="../fr461443/index.html">Post-analyse: ce que l'on sait de la derni√®re attaque contre le r√©seau de serveurs de cl√©s cryptographiques SKS Keyserver</a></li>
<li><a href="../fr461447/index.html">L'√©pop√©e des administrateurs syst√®me en tant qu'esp√®ce menac√©e</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>