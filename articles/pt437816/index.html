<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äç‚úàÔ∏è üë®üèª‚Äçüè´ üîØ MPLS est√° em todo lugar. Como √© a infraestrutura de rede Yandex.Cloud üà∑Ô∏è üö¥üèª ü§ôüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Post preparado por: Alexander Virilin xscrew - autor, chefe do servi√ßo de infraestrutura de rede, Leonid Klyuyev - editor 

 Continuamos a familiariz√°...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>MPLS est√° em todo lugar. Como √© a infraestrutura de rede Yandex.Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437816/"> <sup><i>Post preparado por: Alexander Virilin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">xscrew</a> - autor, chefe do servi√ßo de infraestrutura de rede, Leonid Klyuyev - editor</i></sup> <br><br><img src="https://habrastorage.org/webt/td/uq/e-/tduqe-fvjvebbot1h11mdm0ri9g.png" align="right" width="400">  Continuamos a familiariz√°-lo com a estrutura interna do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Yandex.Cloud</a> .  Hoje falaremos sobre redes - mostraremos como a infraestrutura de rede funciona, por que ela usa o paradigma MPLS impopular para data centers, que outras decis√µes complexas tivemos que tomar no processo de constru√ß√£o de uma rede em nuvem, como gerenciamos e que tipo de monitoramento usamos. <br><br>  A rede na nuvem consiste em tr√™s camadas.  A camada inferior √© a infraestrutura j√° mencionada.  Essa √© uma rede f√≠sica "de ferro" dentro de data centers, entre data centers e em locais de conex√£o com redes externas.  Uma rede virtual √© constru√≠da sobre a infraestrutura de rede e os servi√ßos de rede s√£o constru√≠dos sobre a rede virtual.  Essa estrutura n√£o √© monol√≠tica: as camadas se cruzam, a rede virtual e os servi√ßos de rede interagem diretamente com a infraestrutura de rede.  Como a rede virtual costuma ser chamada de sobreposi√ß√£o, geralmente chamamos de infra-estrutura de rede. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/lr/c_/kr/lrc_krqlbldrs_spninjomwdekm.png"><br><br>  Agora, a infraestrutura em nuvem est√° baseada na regi√£o central da R√∫ssia e inclui tr√™s zonas de acesso - ou seja, tr√™s data centers independentes distribu√≠dos geograficamente.  Independente - independente um do outro no contexto de redes, engenharia e sistemas el√©tricos, etc. <br><br>  Sobre as caracter√≠sticas.  A geografia da localiza√ß√£o dos data centers √© tal que o tempo de ida e volta (RTT) entre eles √© sempre de 6 a 7 ms.  A capacidade total dos canais j√° ultrapassou 10 terabits e est√° em constante crescimento, porque a Yandex possui sua pr√≥pria rede de fibra √≥tica entre as zonas.  Como n√£o alugamos canais de comunica√ß√£o, podemos aumentar rapidamente a capacidade da faixa entre os CDs: cada um deles usa equipamento de multiplexa√ß√£o espectral. <br><br>  Aqui est√° a representa√ß√£o mais esquem√°tica das zonas: <br><br><img src="https://habrastorage.org/webt/iw/tz/11/iwtz11yihyj7beyxar1pefzif-4.png"><br><br>  A realidade, por sua vez, √© um pouco diferente: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/bf/fp/aibffptzbz0jtemoiczeiledcdm.png" width="500"></div><br>  Aqui est√° a atual rede de backbone Yandex na regi√£o.  Todos os servi√ßos Yandex funcionam em cima dele, parte da rede √© usada pela nuvem.  (Esta √© uma imagem para uso interno, portanto, as informa√ß√µes de servi√ßo s√£o deliberadamente ocultas. No entanto, √© poss√≠vel estimar o n√∫mero de n√≥s e conex√µes.) A decis√£o de usar a rede de backbone era l√≥gica: n√£o pod√≠amos inventar nada, mas reutilizar a infraestrutura atual - "sofrida" ao longo dos anos de desenvolvimento. <br><br>  Qual √© a diferen√ßa entre a primeira foto e a segunda?  Antes de tudo, as zonas de acesso n√£o est√£o diretamente relacionadas: sites t√©cnicos est√£o localizados entre elas.  Os sites n√£o cont√™m equipamentos de servidor - apenas dispositivos de rede para garantir a conectividade.  Os pontos de presen√ßa nos quais o Yandex e o Cloud se conectam ao mundo exterior s√£o conectados a sites t√©cnicos.  Todos os pontos de presen√ßa funcionam para toda a regi√£o.  A prop√≥sito, √© importante observar que, do ponto de vista do acesso externo da Internet, todas as zonas de acesso √† nuvem s√£o equivalentes.  Em outras palavras, eles fornecem a mesma conectividade - ou seja, a mesma velocidade e taxa de transfer√™ncia, al√©m de lat√™ncias igualmente baixas. <br><br>  Al√©m disso, h√° equipamentos nos pontos de presen√ßa, aos quais - se houver recursos locais e um desejo de expandir a infraestrutura local com instala√ß√µes em nuvem - os clientes podem se conectar por meio de um canal garantido.  Isso pode ser feito com a ajuda de parceiros ou por conta pr√≥pria. <br><br>  A rede principal √© usada pela nuvem como um transporte MPLS. <br><br><h2>  MPLS </h2><br><img src="https://habrastorage.org/webt/ul/kj/rv/ulkjrvkt7kk2igkl_sbjzivjfxk.png"><br><br>  A comuta√ß√£o de etiquetas multiprotocolo √© uma tecnologia amplamente usada em nossa ind√∫stria.  Por exemplo, quando um pacote √© transferido entre zonas de acesso ou entre uma zona de acesso e a Internet, o equipamento de transporte presta aten√ß√£o apenas ao r√≥tulo superior, "sem pensar" no que est√° por baixo.  Dessa maneira, o MPLS permite ocultar a complexidade da nuvem da camada de transporte.  Em geral, n√≥s na nuvem gostamos muito do MPLS.  At√© fizemos parte do n√≠vel mais baixo e o usamos diretamente na f√°brica de comuta√ß√£o no data center: <br><br><img src="https://habrastorage.org/webt/k-/iy/hg/k-iyhg3ru8bkto7rqrawug0ivra.png"><br><br>  (Na verdade, existem muitos links paralelos entre comutadores Leaf e Spines.) <br><br><h4>  Por que MPLS? </h4><br>  √â verdade que o MPLS geralmente n√£o √© encontrado em redes de data center.  Muitas vezes, tecnologias completamente diferentes s√£o usadas. <br><br>  Usamos o MPLS por v√°rios motivos.  Primeiro, achamos conveniente unificar as tecnologias do plano de controle e do plano de dados.  Ou seja, em vez de alguns protocolos na rede do data center, outros na rede principal e a jun√ß√£o desses protocolos - um √∫nico MPLS.  Assim, unificamos a pilha tecnol√≥gica e reduzimos a complexidade da rede. <br><br>  Em segundo lugar, na nuvem, usamos v√°rios dispositivos de rede, como o Cloud Gateway e o Network Load Balancer.  Eles precisam se comunicar, enviar tr√°fego para a Internet e vice-versa.  Esses dispositivos de rede podem ser dimensionados horizontalmente com o aumento da carga e, como a nuvem √© constru√≠da de acordo com o modelo de hiperconverg√™ncia, eles podem ser iniciados em absolutamente qualquer lugar do ponto de vista da rede no data center, ou seja, em um pool de recursos comum. <br><br>  Portanto, esses dispositivos podem iniciar atr√°s de qualquer porta do comutador de rack em que o servidor est√° localizado e come√ßar a se comunicar via MPLS com o restante da infraestrutura.  O √∫nico problema na constru√ß√£o dessa arquitetura era o alarme. <br><br><h2>  Alarme </h2><br>  A pilha de protocolos MPLS cl√°ssica √© bastante complexa.  A prop√≥sito, essa √© uma das raz√µes da n√£o prolifera√ß√£o do MPLS nas redes de data center. <br><br>  Por sua vez, n√£o usamos IGP (Interior Gateway Protocol), LDP (Label Distribution Protocol) ou outros protocolos de distribui√ß√£o de etiquetas.  Somente o Label-Unicast BGP (Border Gateway Protocol) √© usado.  Cada dispositivo, que √© executado, por exemplo, como uma m√°quina virtual, cria uma sess√£o BGP antes do comutador Leaf montado em rack. <br><br><img src="https://habrastorage.org/webt/z7/-o/03/z7-o03kr2x9-v-yuawoom5xegq4.png"><br><br>  Uma sess√£o BGP √© constru√≠da em um endere√ßo pr√©-conhecido.  N√£o h√° necessidade de configurar automaticamente o comutador para executar cada dispositivo.  Todos os comutadores s√£o pr√©-configurados e consistentes. <br><br>  Dentro de uma sess√£o BGP, cada dispositivo envia seu pr√≥prio loopback e recebe loopbacks do restante dos dispositivos com os quais precisar√° trocar tr√°fego.  Exemplos de tais dispositivos s√£o v√°rios tipos de refletores de rota, roteadores de borda e outros dispositivos.  Como resultado, informa√ß√µes sobre como entrar em contato aparecem nos dispositivos.  Do Cloud Gateway atrav√©s do comutador Leaf, do comutador Spine e da rede at√© o roteador de borda, √© criado um caminho de comutador de etiqueta.  Os comutadores s√£o comutadores L3 que se comportam como um roteador de comutador de etiquetas e n√£o conhecem a complexidade que os cerca. <br><br>  O MPLS em todos os n√≠veis da nossa rede, entre outras coisas, nos permitiu usar o conceito de Coma seu pr√≥prio alimento para c√£es. <br><br><h2>  Coma seu pr√≥prio alimento para c√£es </h2><br>  Do ponto de vista da rede, esse conceito implica que vivemos na mesma infraestrutura que fornecemos ao usu√°rio.  Aqui est√£o diagramas de racks nas √°reas de acessibilidade: <br><br><img src="https://habrastorage.org/webt/tz/zq/o0/tzzqo08b1pv-whl9mqu9e_xnups.png"><br><br>  O host da nuvem leva a carga do usu√°rio, cont√©m suas m√°quinas virtuais.  E, literalmente, um host vizinho em um rack pode transportar a carga da infraestrutura do ponto de vista da rede, incluindo refletores de rota, gerenciamento, servidores de monitoramento etc. <br><br>  Por que isso foi feito?  Houve uma tenta√ß√£o de executar refletores de rota e todos os elementos de infraestrutura em um segmento tolerante a falhas separado.  Ent√£o, se o segmento de usu√°rios quebrasse em algum lugar do data center, os servidores de infraestrutura continuariam a gerenciar toda a infraestrutura de rede.  Mas essa abordagem nos pareceu cruel - se n√£o confiamos em nossa pr√≥pria infraestrutura, como podemos fornec√™-la aos nossos clientes?  Afinal, absolutamente toda a nuvem, todas as redes virtuais, servi√ßos de usu√°rio e nuvem trabalham em cima dela. <br><br>  Portanto, abandonamos um segmento separado.  Nossos elementos de infraestrutura s√£o executados na mesma topologia e conectividade de rede.  Naturalmente, eles s√£o executados em uma inst√¢ncia tripla - assim como nossos clientes lan√ßam seus servi√ßos na nuvem. <br><br><h2>  F√°brica IP / MPLS </h2><br>  Aqui est√° um exemplo de diagrama de uma das zonas de disponibilidade: <br><br><img src="https://habrastorage.org/webt/jn/xi/rw/jnxirwsmzj62bwzvldfnrq3n2_4.png"><br><br>  Em cada zona de disponibilidade, existem cerca de cinco m√≥dulos e, em cada m√≥dulo, cerca de cem racks.  Switches montados em rack, em folha, s√£o conectados em seu m√≥dulo pelo n√≠vel Spine, e a conectividade entre m√≥dulos √© fornecida por meio da rede Interconnect.  Este √© o pr√≥ximo n√≠vel, que inclui os chamados comutadores Super-Spines e Edge, que j√° conectam as zonas de acesso.  N√≥s deliberadamente abandonamos o L2, estamos falando apenas da conectividade L3 IP / MPLS.  O BGP √© usado para distribuir informa√ß√µes de roteamento. <br><br>  De fato, h√° muito mais conex√µes paralelas do que na figura.  Um n√∫mero t√£o grande de conex√µes ECMP (caminhos m√∫ltiplos de custo igual) imp√µe requisitos especiais de monitoramento.  Al√©m disso, h√° limites inesperados, √† primeira vista, no equipamento - por exemplo, o n√∫mero de grupos ECMP. <br><br><h2>  Conex√£o com o servidor </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xf/ay/jo/xfayjofymnoqjc-1u1otd0axhgm.png"></div><br>  Devido a investimentos poderosos, o Yandex cria servi√ßos de tal maneira que a falha de um servidor, rack de servidor, m√≥dulo ou mesmo de um datacenter inteiro nunca leva a uma parada completa do servi√ßo.  Se houver algum tipo de problema de rede - suponha que um switch de montagem em rack esteja quebrado - os usu√°rios externos nunca ver√£o isso. <br><br>  Yandex.Cloud √© um caso especial.  N√£o podemos ditar ao cliente como criar seus pr√≥prios servi√ßos e decidimos nivelar esse poss√≠vel ponto √∫nico de falha.  Portanto, todos os servidores na nuvem est√£o conectados a dois comutadores de montagem em rack. <br><br>  Tamb√©m n√£o usamos protocolos de redund√¢ncia no n√≠vel L2, mas imediatamente come√ßamos a usar apenas L3 com BGP - novamente, por raz√µes de unifica√ß√£o de protocolo.  Essa conex√£o fornece a cada servi√ßo conectividade IPv4 e IPv6: alguns servi√ßos funcionam em IPv4 e alguns servi√ßos em IPv6. <br><br>  Fisicamente, cada servidor √© conectado por duas interfaces de 25 gigabit.  Aqui est√° uma foto do data center: <br><br><img src="https://habrastorage.org/webt/jz/sr/xj/jzsrxj39equkvhixj3bjoj5kn6a.png"><br><br>  Aqui voc√™ v√™ dois comutadores de montagem em rack com portas de 100 gigabit.  Cabos de interrup√ß√£o divergentes s√£o vis√≠veis, dividindo a porta de 100 gigabit do switch em 4 portas de 25 gigabits por servidor.  Chamamos esses cabos de "hidra". <br><br><h2>  Gerenciamento de infraestrutura </h2><br>  A infraestrutura de rede em nuvem n√£o cont√©m nenhuma solu√ß√£o de gerenciamento propriet√°ria: todos os sistemas s√£o de c√≥digo aberto com personaliza√ß√£o para a nuvem ou completamente auto-escritos. <br><br><img src="https://habrastorage.org/webt/-g/if/qu/-gifqu8wgzw3xwehpuy5_ejfapc.png"><br><br>  Como essa infraestrutura √© gerenciada?  N√£o √© proibido na nuvem, mas √© altamente desencorajado ir a um dispositivo de rede e fazer quaisquer ajustes.  Existe o estado atual do sistema e precisamos aplicar as altera√ß√µes: chegue a um novo estado de destino.  ‚ÄúExecutar um script‚Äù atrav√©s de todas as gl√¢ndulas, mude algo na configura√ß√£o - voc√™ n√£o deve fazer isso.  Em vez disso, fazemos altera√ß√µes nos modelos, em uma √∫nica fonte de sistema de verdade e comprometemos nossa altera√ß√£o no sistema de controle de vers√£o.  Isso √© muito conveniente, porque voc√™ sempre pode reverter o hist√≥rico, descobrir quem √© respons√°vel pelo commit, etc. <br><br>  Quando fizemos as altera√ß√µes, as configura√ß√µes s√£o geradas e as lan√ßamos na topologia de teste de laborat√≥rio.  Da perspectiva da rede, essa √© uma pequena nuvem que repete completamente toda a produ√ß√£o existente.  Veremos imediatamente se as altera√ß√µes desejadas quebram algo: primeiro, pelo monitoramento e, segundo, pelo feedback de nossos usu√°rios internos. <br><br>  Se o monitoramento diz que tudo est√° calmo, continuamos implementando - mas aplicamos a altera√ß√£o apenas a parte da topologia (duas ou mais acessibilidades ‚Äún√£o t√™m o direito‚Äù de interromper pelo mesmo motivo).  Al√©m disso, continuamos a monitorar de perto o monitoramento.  Este √© um processo bastante complicado, sobre o qual falaremos abaixo. <br><br>  Depois de garantir que tudo est√° bem, aplicamos a altera√ß√£o a toda a produ√ß√£o.  A qualquer momento, voc√™ pode reverter e retornar ao estado anterior da rede, rastrear e corrigir rapidamente o problema. <br><br><h4>  Monitoramento </h4><br>  Precisamos de um monitoramento diferente.  Um dos mais procurados √© o monitoramento da conectividade de ponta a ponta.  A qualquer momento, cada servidor deve poder se comunicar com qualquer outro servidor.  O fato √© que, se houver um problema em algum lugar, queremos descobrir exatamente onde o mais cedo poss√≠vel (ou seja, quais servidores t√™m problemas para acessar um ao outro).  Garantir a conectividade de ponta a ponta √© a nossa principal preocupa√ß√£o. <br><br>  Cada servidor lista um conjunto de todos os servidores com os quais deve poder se comunicar a qualquer momento.  O servidor pega um subconjunto aleat√≥rio desse conjunto e envia pacotes ICMP, TCP e UDP para todas as m√°quinas selecionadas.  Isso verifica se h√° perdas na rede, se o atraso aumentou etc. A rede inteira √© "chamada" dentro de uma das zonas de acesso e entre elas.  Os resultados s√£o enviados para um sistema centralizado que os visualiza para n√≥s. <br><br>  Aqui est√° a apar√™ncia dos resultados quando tudo n√£o √© muito bom: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yd/-1/bs/yd-1bs7mguqfcy-xhdko674rvu0.png"></div><br>  Aqui voc√™ pode ver quais segmentos de rede h√° um problema entre (neste caso, A e B) e onde est√° tudo bem (A e D).  Servidores espec√≠ficos, comutadores montados em rack, m√≥dulos e zonas de disponibilidade inteiras podem ser exibidas aqui.  Se alguma das alternativas acima se tornar a fonte do problema, n√≥s a veremos em tempo real. <br><br>  Al√©m disso, h√° monitoramento de eventos.  Monitoramos de perto todas as conex√µes, n√≠veis de sinal nos transceptores, sess√µes BGP, etc. Suponha que tr√™s sess√µes BGP sejam constru√≠das a partir de um segmento de rede, um dos quais foi interrompido √† noite.  Se configurarmos o monitoramento para que a queda de uma sess√£o do BGP n√£o seja cr√≠tica para n√≥s e possamos esperar at√© a manh√£ seguinte, o monitoramento n√£o ativar√° os engenheiros de rede.  Mas se a segunda das tr√™s sess√µes cair, um engenheiro liga automaticamente. <br><br>  Al√©m do monitoramento de ponta a ponta e de eventos, usamos uma cole√ß√£o centralizada de logs, suas an√°lises em tempo real e an√°lises subsequentes.  Voc√™ pode ver as correla√ß√µes, identificar problemas e descobrir o que estava acontecendo no equipamento de rede. <br><br>  O t√≥pico de monitoramento √© grande o suficiente, h√° uma enorme margem para melhorias.  Eu quero trazer o sistema para uma maior automa√ß√£o e autocura verdadeira. <br><br><h2>  O que vem a seguir? </h2><br>  Temos muitos planos.  √â necess√°rio melhorar os sistemas de controle, monitoramento, comuta√ß√£o de f√°bricas IP / MPLS e muito mais. <br><br>  Tamb√©m estamos buscando ativamente os interruptores de caixa branca.  Este √© um dispositivo "de ferro" pronto, um interruptor no qual voc√™ pode rolar o software.  Em primeiro lugar, se tudo for feito corretamente, ser√° poss√≠vel "tratar" os comutadores da mesma maneira que nos servidores, criar um processo de CI / CD realmente conveniente, implementar configura√ß√µes progressivamente, etc. <br><br>  Em segundo lugar, se houver algum problema, √© melhor manter um grupo de engenheiros e desenvolvedores que resolver√£o esses problemas do que esperar muito tempo por uma corre√ß√£o do fornecedor. <br><br>  Para que tudo d√™ certo, o trabalho est√° em andamento em duas dire√ß√µes: <br><br><ul><li>  Reduzimos significativamente a complexidade da f√°brica de IP / MPLS.  Por um lado, o n√≠vel da rede virtual e das ferramentas de automa√ß√£o disso, pelo contr√°rio, se tornou um pouco mais complicado.  Por outro lado, a pr√≥pria rede subjacente ficou mais f√°cil.  Em outras palavras, h√° uma certa "quantidade" de complexidade que n√£o pode ser salva.  Pode ser "lan√ßado" de um n√≠vel para outro - por exemplo, entre n√≠veis de rede ou do n√≠vel da rede para o n√≠vel do aplicativo.  E voc√™ pode distribuir corretamente essa complexidade, o que estamos tentando fazer. </li><li>  E, claro, estamos finalizando nosso conjunto de ferramentas para gerenciar toda a infraestrutura. </li></ul><br>  Isso √© tudo que quer√≠amos falar sobre nossa infraestrutura de rede.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aqui est√° um link</a> para o canal Cloud Telegram com not√≠cias e dicas. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt437816/">https://habr.com/ru/post/pt437816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt437806/index.html">EcmaScript 10 - JavaScript deste ano (ES2019)</a></li>
<li><a href="../pt437808/index.html">Perf e flamegraphs</a></li>
<li><a href="../pt437810/index.html">Realidade corporativa</a></li>
<li><a href="../pt437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 e outros betas</a></li>
<li><a href="../pt437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 e outras vers√µes beta</a></li>
<li><a href="../pt437818/index.html">Ensinamos um computador a distinguir sons: conhecendo o concurso DCASE e montando seu classificador de √°udio em 30 minutos</a></li>
<li><a href="../pt437820/index.html">50 tons de seguran√ßa Drupal</a></li>
<li><a href="../pt437824/index.html">Extens√£o Universal 1C para o Planilhas Google e o Google Docs - use e use</a></li>
<li><a href="../pt437826/index.html">Como migramos o banco de dados do Redis e do Riak KV para o PostgreSQL. Parte 1: o processo</a></li>
<li><a href="../pt437828/index.html">Abra o webinar "SELECT ordem de execu√ß√£o da consulta e plano de consulta no MS SQL Server"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>