<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∏üèª üì´ ‚õ±Ô∏è Configurez le cluster Kubernetes HA sur du m√©tal nu avec GlusterFS et MetalLB. Partie 2/3 üñçÔ∏è üëãüèø üë©üèΩ‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Partie 1/3 ici 
 Partie 3/3 ici 


 Bonjour et bienvenue! Il s'agit de la deuxi√®me partie de l'article sur la configuration d'un cluster Kubernetes su...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configurez le cluster Kubernetes HA sur du m√©tal nu avec GlusterFS et MetalLB. Partie 2/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Partie 1/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>ici</strong></a> <br>  <strong>Partie 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>ici</strong></a> </p><br><p>  Bonjour et bienvenue!  Il s'agit de la deuxi√®me partie de l'article sur la configuration d'un cluster Kubernetes sur du m√©tal nu.  Plus t√¥t, nous avons configur√© le cluster Kubernetes HA √† l'aide de etcd externe, ma√Ætre-ma√Ætre et √©quilibrage de charge.  Eh bien, il est maintenant temps de configurer un environnement et des utilitaires suppl√©mentaires pour rendre le cluster plus utile et aussi proche que possible de l'√©tat de fonctionnement. </p><br><p>  Dans cette partie de l'article, nous nous concentrerons sur la configuration de l'√©quilibreur de charge interne des services de cluster - ce sera MetalLB.  Nous allons √©galement installer et configurer le stockage de fichiers distribu√© entre nos n≈ìuds de travail.  Nous utiliserons GlusterFS pour les volumes persistants disponibles dans Kubernetes. <br>  Apr√®s avoir termin√© toutes les √©tapes, notre diagramme de cluster ressemblera √† ceci: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Configurez MetalLB en tant qu'√©quilibreur de charge interne. </h3><br><p>  Quelques mots sur MetalLB, directement depuis la page du document: </p><br><blockquote> MetalLB est une impl√©mentation d'√©quilibreur de charge pour les clusters de m√©taux nus Kubernetes avec des protocoles de routage standard. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes</a> ne propose pas l'impl√©mentation d'√©quilibreurs de charge r√©seau ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">type de service LoadBalancer</a> ) pour le m√©tal nu.  Toutes les options d'impl√©mentation de Network LB fournies par Kubernetes sont des middlewares et acc√®dent √† diverses plates-formes IaaS (GCP, AWS, Azure, etc.).  Si vous ne travaillez pas sur une plate-forme prise en charge par IaaS (GCP, AWS, Azure, etc.), le LoadBalancer restera en √©tat de ¬´veille¬ª pendant une p√©riode ind√©finie lors de sa cr√©ation. <br><br>  Les op√©rateurs de serveurs BM disposent de deux outils moins efficaces pour entrer le trafic utilisateur dans leurs clusters, les services NodePort et externalIPs.  Ces deux options pr√©sentent des lacunes importantes dans la production, ce qui transforme les grappes BM en citoyens de seconde classe dans l'√©cosyst√®me Kubernetes. <br><br>  MetalLB cherche √† corriger ce d√©s√©quilibre en proposant l'impl√©mentation Network LB, qui s'int√®gre √† l'√©quipement r√©seau standard, de sorte que les services externes sur les clusters BM "fonctionnent" √† la vitesse maximale. </blockquote><p>  Ainsi, √† l'aide de cet outil, nous lan√ßons des services dans le cluster Kubernetes √† l'aide d'un √©quilibreur de charge, dont un grand merci √† l'√©quipe MetalLB.  Le processus d'installation est vraiment simple et direct. </p><br><p>  Plus t√¥t dans l'exemple, nous avons s√©lectionn√© le sous-r√©seau 192.168.0.0/24 pour les besoins de notre cluster.  Prenez maintenant une partie de ce sous-r√©seau pour le futur √©quilibreur de charge. </p><br><p>  Nous entrons dans le syst√®me de la machine avec l'utilitaire <strong>kubectl</strong> configur√© et <strong>ex√©cutons</strong> : </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  Cela d√©ploiera MetalLB dans le cluster, dans l' <code>metallb-system</code> .  Assurez-vous que tous les composants MetalLB fonctionnent correctement: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Configurez maintenant MetalLB √† l'aide de configmap.  Dans cet exemple, nous utilisons la personnalisation de la couche 2. Pour plus d'informations sur les autres options de personnalisation, consultez la documentation MetalLB. </p><br><p>  Cr√©ez le <strong>fichier metallb-config.yaml</strong> dans n'importe quel r√©pertoire √† l'int√©rieur de la plage IP s√©lectionn√©e du sous-r√©seau de notre cluster: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p>  Et appliquez ce param√®tre: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  V√©rifiez et modifiez configmap plus tard si n√©cessaire: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  Nous avons maintenant notre propre √©quilibreur de charge local configur√©.  Voyons comment cela fonctionne, en utilisant le service Nginx comme exemple. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Cr√©ez ensuite un d√©ploiement de test et un service Nginx: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  Et maintenant - v√©rifiez le r√©sultat: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Cr√©√© 3 pods Nginx, comme nous l'avons indiqu√© dans le d√©ploiement plus t√¥t.  Le service Nginx dirigera le trafic vers tous ces pods selon le sch√©ma d'√©quilibrage cyclique.  Et vous pouvez √©galement voir l'adresse IP externe re√ßue de notre √©quilibreur de charge MetalLB. </p><br><p>  Essayez maintenant de passer √† l'adresse IP 192.168.0.240 et vous verrez la page Nginx index.html.  N'oubliez pas de supprimer le d√©ploiement de test et le service Nginx. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Eh bien, c'est tout avec MetalLB, passons √† autre chose - nous allons configurer GlusterFS pour les volumes Kubernetes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Configuration de GlusterFS avec Heketi sur les n≈ìuds de travail. </h3><br><p>  En fait, le cluster Kubernetes ne peut pas √™tre utilis√© sans volumes √† l'int√©rieur.  Comme vous le savez, les foyers sont √©ph√©m√®res, c'est-√†-dire  ils peuvent √™tre cr√©√©s et supprim√©s √† tout moment.  Toutes les donn√©es qu'elles contiennent seront perdues.  Ainsi, dans un v√©ritable cluster, un stockage distribu√© est n√©cessaire pour assurer l'√©change de param√®tres et de donn√©es entre les n≈ìuds et les applications qu'il contient. </p><br><p>  Dans Kubernetes, les volumes sont disponibles de diff√©rentes mani√®res; choisissez ceux que vous souhaitez.  Dans cet exemple, je vais montrer comment cr√©er un stockage GlusterFS pour toutes les applications internes, c'est comme des volumes persistants.  Plus t√¥t, j'ai utilis√© l'installation ¬´syst√®me¬ª de GlusterFS sur tous les n≈ìuds de travail Kubernetes pour cela, puis j'ai simplement cr√©√© des volumes comme hostPath dans les r√©pertoires GlusterFS. </p><br><p>  Nous avons maintenant un nouvel outil <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>Heketi</strong></a> pratique. </p><br><p>  Quelques mots de la documentation Heketi: </p><br><blockquote>  Infrastructure de gestion de volume RESTful pour GlusterFS. <br><br>  Heketi propose une interface de gestion RESTful qui peut √™tre utilis√©e pour g√©rer le cycle de vie des volumes GlusterFS.  Gr√¢ce √† Heketi, les services cloud tels que OpenStack Manila, Kubernetes et OpenShift peuvent fournir dynamiquement des volumes GlusterFS avec tout type de fiabilit√© pris en charge.  Heketi d√©termine automatiquement l'emplacement des blocs dans un cluster, en fournissant l'emplacement des blocs et de leurs r√©pliques dans diff√©rentes zones de d√©faillance.  Heketi prend √©galement en charge un nombre illimit√© de clusters GlusterFS, permettant aux services cloud d'offrir un stockage de fichiers en ligne, et pas seulement un seul cluster GlusterFS. </blockquote><p>  Cela semble bon et, en outre, cet outil rapprochera notre cluster de machines virtuelles des grands clusters de cloud de Kubernetes.  √Ä la fin, vous pourrez cr√©er des <strong>PersistentVolumeClaims</strong> , qui seront g√©n√©r√©s automatiquement, et bien plus encore. </p><br><p>  Vous pouvez prendre des disques durs syst√®me suppl√©mentaires pour configurer GlusterFS ou simplement cr√©er des p√©riph√©riques de blocs factices.  Dans cet exemple, j'utiliserai la deuxi√®me m√©thode. </p><br><p>  Cr√©ez des dispositifs de blocs factices sur les trois n≈ìuds de travail: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  Vous obtiendrez un fichier d'environ 10 Go.  Utilisez ensuite <strong>losetup</strong> - pour l'ajouter √† ces n≈ìuds, en tant que p√©riph√©rique de bouclage: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Remarque: si vous poss√©dez d√©j√† une sorte de p√©riph√©rique de bouclage 0, vous devrez choisir un autre num√©ro.</em> </blockquote><p>  J'ai pris le temps et j'ai d√©couvert pourquoi Heketi ne voulait pas fonctionner correctement.  Par cons√©quent, pour √©viter tout probl√®me dans les configurations futures, assurez-vous d'abord que nous avons charg√© le <strong>module du</strong> noyau <strong>dm_thin_pool</strong> et install√© le package <strong>glusterfs-client</strong> sur tous les n≈ìuds de travail. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Eh bien, vous avez maintenant besoin que le fichier <strong>/ home / gluster / image</strong> et le p√©riph√©rique <strong>/ dev / loop0</strong> soient pr√©sents sur tous les n≈ìuds de travail.  N'oubliez pas de cr√©er un service systemd qui d√©marrera automatiquement <strong>losetup</strong> et <strong>modprobe √†</strong> chaque d√©marrage de ces serveurs. </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  Et allumez-le: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  Les travaux pr√©paratoires sont termin√©s et nous sommes pr√™ts √† d√©ployer GlusterFS et Heketi dans notre cluster.  Pour cela, j'utiliserai ce <a href="">guide</a> sympa.  La plupart des commandes sont lanc√©es √† partir d'un ordinateur de contr√¥le externe, et de tr√®s petites commandes sont lanc√©es √† partir de n'importe quel n≈ìud ma√Ætre du cluster. </p><br><p>  Tout d'abord, copiez le r√©f√©rentiel et cr√©ez DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Maintenant, marquons nos trois n≈ìuds de travail pour GlusterFS;  apr√®s les avoir √©tiquet√©s, des pods GlusterFS seront cr√©√©s: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Cr√©ez maintenant un compte de service Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  Nous offrons √† ce compte de service la possibilit√© de g√©rer les modules Gluster.  Pour ce faire, cr√©ez une fonction de cluster requise pour notre compte de service nouvellement cr√©√©: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Cr√©ons maintenant une cl√© secr√®te Kubernetes qui bloque la configuration de notre instance Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Cr√©ez la premi√®re source sous Heketi, que nous utilisons pour les premi√®res op√©rations de configuration et supprimez ensuite: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  Apr√®s avoir cr√©√© et d√©marr√© le service Bootstrap Heketi, nous devrons basculer vers l'un de nos n≈ìuds principaux, l√† nous ex√©cuterons plusieurs commandes, car notre n≈ìud de contr√¥le externe n'est pas √† l'int√©rieur de notre cluster, nous ne pouvons donc pas acc√©der aux pods de travail et au r√©seau interne du cluster. </p><br><p>  Tout d'abord, t√©l√©chargeons l'utilitaire heketi-client et copions-le dans le dossier syst√®me bin: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Trouvez maintenant l'adresse IP du pod heketi et exportez-la en tant que variable syst√®me: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Fournissons maintenant √† Heketi des informations sur le cluster GlusterFS qu'il doit g√©rer.  Nous le fournissons via un fichier de topologie.  Une topologie est un manifeste JSON avec une liste de tous les n≈ìuds, disques et clusters utilis√©s par GlusterFS. </p><br><blockquote>  REMARQUE  Assurez-vous que <code>hostnames/manage</code> indique le nom exact, comme dans la section <code>kubectl get node</code> , et que <code>hostnames/storage</code> est l'adresse IP des n≈ìuds de stockage. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  T√©l√©chargez ensuite ce fichier: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Ensuite, nous utilisons Heketi pour fournir des volumes pour le stockage de la base de donn√©es.  Le nom de l'√©quipe est un peu √©trange, mais tout est en ordre.  Cr√©ez √©galement un r√©f√©rentiel heketi: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  Ce sont toutes les commandes dont vous avez besoin pour ex√©cuter √† partir du n≈ìud ma√Ætre.  Revenons au n≈ìud de contr√¥le et continuons √† partir de l√†;  Tout d'abord, assurez-vous que la derni√®re commande en cours d'ex√©cution a √©t√© ex√©cut√©e avec succ√®s: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  Et le travail heketi-storage-copy-job est termin√©. </p><br><blockquote>  S'il n'y a actuellement aucun package <strong>glusterfs-client</strong> install√© sur vos n≈ìuds de travail, une erreur se produit. </blockquote><p>  Il est temps de supprimer le fichier d'installation de Heketi Bootstrap et de faire un petit nettoyage: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  √Ä la derni√®re √©tape, nous devons cr√©er une copie √† long terme de Heketi: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  S'il n'y a actuellement aucun package glusterfs-client install√© sur vos n≈ìuds de travail, une erreur se produit.  Et nous avons presque termin√©, maintenant la base de donn√©es Heketi est stock√©e dans le volume GlusterFS et n'est pas r√©initialis√©e √† chaque red√©marrage du foyer Heketi. </p><br><p>  Pour commencer √† utiliser le cluster GlusterFS avec l'allocation dynamique des ressources, nous devons cr√©er un StorageClass. </p><br><p>  Tout d'abord, trouvons le point de terminaison de stockage Gluster, qui sera transmis √† StorageClass en tant que param√®tre (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Cr√©ez maintenant des fichiers: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Utilisez ces fichiers pour cr√©er de la classe et du PVC: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  On peut √©galement visualiser le volume PV: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Nous avons maintenant un volume GlusterFS cr√©√© dynamiquement associ√© √† <strong>PersistentVolumeClaim</strong> , et nous pouvons utiliser cette instruction dans n'importe quel sous-trac√©. </p><br><p>  Cr√©ez-en un simple sous Nginx et testez-le: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Naviguez sous (attendez quelques minutes, vous devrez peut-√™tre t√©l√©charger l'image si elle n'existe pas d√©j√†): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Maintenant, allez dans le conteneur et cr√©ez le fichier index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  Vous devrez trouver l'adresse IP interne du foyer et vous y recourber √† partir de n'importe quel n≈ìud ma√Ætre: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  Ce faisant, nous testons simplement notre nouveau volume persistant. </p><br><blockquote>  Certaines commandes utiles pour extraire le nouveau cluster GlusterFS sont: la <code>heketi-cli cluster list</code> <code>heketi-cli volume list</code> .  Ils peuvent √™tre ex√©cut√©s sur votre ordinateur si <strong>heketi-cli est install√©</strong> .  Dans cet exemple, il s'agit du n≈ìud <strong>master1</strong> . </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  √Ä ce stade, nous avons r√©ussi √† mettre en place un √©quilibreur de charge interne avec stockage de fichiers, et notre cluster est maintenant plus proche de l'√©tat op√©rationnel. </p><br><p>  Dans la prochaine partie de l'article, nous nous concentrerons sur la cr√©ation d'un syst√®me de surveillance de cluster et lancerons √©galement un projet de test pour utiliser toutes les ressources que nous avons configur√©es. </p><br><p>  Restez en contact et tout le meilleur! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443110/">https://habr.com/ru/post/fr443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443098/index.html">Les donn√©es sont √©crites sur le disque √† l'aide d'aimants et de lasers</a></li>
<li><a href="../fr443100/index.html">Compter les bogues dans la calculatrice Windows</a></li>
<li><a href="../fr443102/index.html">Le changement de comportement en tant que produit: pourquoi Marie Kondo l√®ve-t-elle une ronde de 40 millions de dollars avec Sequoia Capital?</a></li>
<li><a href="../fr443104/index.html">Calculer des expressions symboliques avec des nombres triangulaires flous en python</a></li>
<li><a href="../fr443106/index.html">USB4 annonc√©: ce que l'on sait de la norme</a></li>
<li><a href="../fr443112/index.html">√ätes-vous s√ªr de pouvoir faire confiance √† votre VPN?</a></li>
<li><a href="../fr443114/index.html">Prix ‚Äã‚ÄãDevProject: Mon discours √† DeveloperWeek 2019</a></li>
<li><a href="../fr443120/index.html">La Douma d'√âtat poursuivra la lutte contre la vente ill√©gale de cartes SIM</a></li>
<li><a href="../fr443122/index.html">Fuite de 809 millions d'adresses e-mail du service Verifications.io en raison de l'ouverture publique de MongoDB</a></li>
<li><a href="../fr443124/index.html">React.lazy? Mais que faire si vous n'avez pas de composant?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>