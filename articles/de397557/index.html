<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤾🏾 🏩 🤦🏿 Das neuronale Netzwerk für Bildverarbeitung wird in realistischen Computerspielen trainiert. 👩🏾 👨‍👨‍👧‍👦 👨🏽‍🤝‍👨🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aufnahmen aus dem Computerspiel Grand Theft Auto V und semantisches Markup zum Unterrichten eines
 
 neuronalen Maschinennetzwerks Neuronale Netze ste...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Das neuronale Netzwerk für Bildverarbeitung wird in realistischen Computerspielen trainiert.</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/397557/"><img src="https://habrastorage.org/files/47e/fcc/506/47efcc5062114f4b8ecc13630c9e361e.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aufnahmen aus dem Computerspiel Grand Theft Auto V und semantisches Markup zum Unterrichten eines</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
neuronalen </font><i><font style="vertical-align: inherit;">Maschinennetzwerks</font></i><font style="vertical-align: inherit;"> Neuronale Netze stellen in fast allen Computer-Vision-Wettbewerben neue Rekorde auf und werden zunehmend auch in anderen KI-Anwendungen verwendet. Eine der Schlüsselkomponenten einer solch unglaublichen Leistung eines neuronalen Netzwerks ist die Verfügbarkeit großer Datenmengen für Training und Evaluierung. Beispielsweise wird die Imagenet Large Scale Visual Recognition Challenge (ILSVRC) mit mehr als 1 Million Bildern zur Bewertung moderner neuronaler Netze verwendet. Nach den neuesten Ergebnissen zu urteilen (ResNet zeigt das Ergebnis von nur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3,57% der Fehler</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) müssen Forscher bald umfangreichere Datensätze zusammenstellen. Und dann - noch umfangreicher. Das Kommentieren solcher Fotos ist übrigens eine Menge Arbeit, die teilweise manuell erledigt werden muss. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Einige Entwickler von Computer-Vision-Systemen bieten eine alternative Möglichkeit, solche Systeme zu trainieren und zu testen. Anstatt Trainingsfotos manuell zu kommentieren, verwenden sie synthetisierte Frames aus realistischen Computerspielen.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist ein völlig logischer Ansatz. In modernen Spielen haben Grafiken einen solchen Realismus erreicht, dass sich die synthetisierten Bilder nur geringfügig von Fotografien der realen Welt unterscheiden. Gleichzeitig kann die Spiel-Engine eine unendliche Anzahl solcher Frames erzeugen - dies löst das Problem, Millionen von Fotos für das Training und die Auswertung des neuronalen Netzwerks zu sammeln, sofort dramatisch. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl die Spiel-Engine eine begrenzte Anzahl von Texturen verwendet, gibt es eine Vielzahl von Kombinationen aus Betrachtungswinkeln, Beleuchtung, Wetter und Detaillierungsgrad, die eine ausreichende Vielfalt an Datensätzen bereitstellen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Jahr haben zwei Forschergruppen in der Praxis geprüft, ob es möglich ist, die aus Computerspielen generierten Frames für das Training neuronaler Computer-Vision-Netzwerke zu verwenden. Eine Gruppe von Forschern der Informatikabteilung der University of British Columbia (Kanada) veröffentlichte einen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wissenschaftlichen Artikel,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> für den sie mehr als 60.000 Bilder aus einem Computerspiel mit Straßenansichten </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sammelten, die den Datensätzen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">CamVid</font></a><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cityscapes</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ähneln </font><font style="vertical-align: inherit;">. Den Forschern gelang es zu beweisen, dass das neuronale Netzwerk nach dem Training auf synthetischen Bildern ein ähnliches Fehlerniveau aufweist wie nach dem Training auf realen Fotos. Darüber hinaus zeigt das Training synthetisierter Bilder mit realen Fotos ein noch besseres Ergebnis.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle 60.000 Bilder wurden bei virtuell sonnigem Wetter um 11:00 Uhr mit einer Auflösung von 1024 × 768 und maximalen Grafikeinstellungen aufgenommen (der Name des Spiels wurde aus urheberrechtlichen Gründen nicht bekannt gegeben). Ein unbemanntes Fahrzeug fuhr versehentlich durch die Spielstraßen und beachtete die Straßenregeln. Die Bilder wurden einmal pro Sekunde aufgenommen. Jedes von ihnen wird von einer automatischen semantischen Segmentierung (Himmel, Fußgänger, Autos, Bäume, Hintergrund - die Segmentierung ist absolut genau und aus dem Spiel übernommen), einem tiefen Bild (Tiefenbild, Karte mit dem Markup von Objekten) sowie Normalen zur Oberfläche begleitet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zusätzlich zum Basis-VG-Datensatz haben die Forscher einen weiteren VG + -Datensatz mit vielen semantischen Informationen erstellt, der nicht auf fünf Bezeichnungen beschränkt ist - hier ist die Segmentierung nicht genau. Das Markup wurde automatisch mit </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SegNet durchgeführt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/files/192/934/07c/19293407c01d4ab09be60d46e67c09e5.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eng markierte Frames aus dem VG +</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
-Satz Um die Effektivität des neuronalen Netzwerktrainings zu vergleichen, wurden CamVid- und Cityscapes-Datensätze (fünf Tags) sowie CamVid + ​​und Cityscapes + mit erweiterten Tag-Sets erstellt. </font></font><br>
<br>
<img src="https://habrastorage.org/files/d2b/3dd/1e0/d2b3dd1e02e042ebb4ec45ea61c20ba4.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Original-CamVid-Fotos mit Anmerkungen </font></font></i><br>
<br>
<img src="https://habrastorage.org/files/65f/888/ed1/65f888ed140f4456942204011e0ffd4a.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zwei zufällige Bilder der Cityscapes + mit detaillierten Anmerkungen.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
Für die semantische Klassifizierung wurde ein </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">langes Faltungs-Neuronales Netzwerk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mit einer einfachen FCN8-Architektur auf dem 16- </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lagigen VGG-Netz von Simonyan und Sisserman verwendet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Forscher führten mehrere Experimente durch, um die Erkennungseffizienz von Objekten durch ein neuronales Netzwerk zu bewerten, das auf verschiedenen Datensätzen trainiert wurde. In fast allen Fällen zeigte ein neuronales Netzwerk, das auf synthetischen Daten trainiert wurde, ein besseres Ergebnis als ein neuronales Netzwerk, das auf realen Fotos trainiert wurde. Sie zeigte das beste Ergebnis, selbst wenn sie echte Fotos überprüfte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Tabelle zeigt beispielsweise die Leistung identischer neuronaler Netze, die an drei Datensätzen (reale Fotos, synthetische Daten aus dem Spiel, gemischter Satz) trainiert wurden, wenn Objekte in realen Fotos von CamVid + ​​- und Cityscapes + -Sätzen erkannt werden. </font></font><br>
<br>
<img src="https://habrastorage.org/files/a64/eac/b8a/a64eacb8a5404f64897af9eed4dab2eb.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen können, ist es beim Training eines neuronalen Netzwerks am besten, die synthetischen Bilder eines Computerspiels durch echte Fotos zu ergänzen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wissenschaftlicher Artikel</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">veröffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> am 5. August 2016 auf arXiv.org, die zweite Version ist der 15. August ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pdf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neben Forschern der University of British Columbia wurde fast gleichzeitig </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieselbe Arbeit von einer anderen Gruppe von Wissenschaftlern der Technischen Universität Darmstadt (Deutschland) und von Intel Labs geleistet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Sie nahmen 24.966 Bilder für das Training aus dem Open-World-Computerspiel Grand Theft Auto V. Die Forscher kamen zu dem gleichen Ergebnis: Bei Verwendung eines Trainingsdatensatzes, der aus 2/3 synthetischer Bilder und 1/3 CamVid-Fotos bestand, war die Genauigkeit Die Erkennung ist höher als nur bei Verwendung von CamVid-Fotos. </font></font><br>
<br>
<img src="https://habrastorage.org/files/19f/38c/bc3/19f38cbc32c74e2fa2af351b47113b0b.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Genauigkeit der Erkennung verschiedener Objekte in Fotos aus dem CamVid-Set beim Lernen mit herkömmlichen Methoden und bei Verwendung von Frames aus GTA V (unterste Zeile)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gleichzeitig reduziert die halbautomatische Annotation in einem speziell entwickelten Editor den Zeitaufwand für die Vorbereitung eines Datensatzes zum Trainieren eines neuronalen Netzwerks erheblich. Das Annotieren eines CamVid-Fotos dauert beispielsweise 60 Minuten, ein Cityscapes-Foto 90 Minuten und die halbautomatische Annotation von GTA V-Frames dauert durchschnittlich nur 7 Sekunden ( </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Video, Demonstration des Editors</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/JGAIfWG2MQQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Arbeit der Forscher von der </font><font style="vertical-align: inherit;">Technischen Universität Darmstadt und Intel Labs hat für die Europäische Konferenz über Computer Vision vorbereitet </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ECCV'16</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (11-14 Oktober) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">veröffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf der Website der </font><font style="vertical-align: inherit;">Universität. Die Autoren legten den </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quellcode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zum Lesen von Etiketten und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vollständigen Datensätzen fest</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : sowohl Quellfotos als auch detaillierte Bilder mit semantischem Markup. Der Quellcode des Editors für Anmerkungen wird voraussichtlich in Zukunft veröffentlicht.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dank der Fortschritte bei der Entwicklung realistischer Computerspiele steht Entwicklern künstlicher Intelligenz eine hervorragende Plattform zum Erlernen von Bildverarbeitungssystemen zur Verfügung. </font><font style="vertical-align: inherit;">Diese Systeme werden in unbemannten Fahrzeugen und Robotern eingesetzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vielleicht können Computerspiele nicht nur für die Bildverarbeitung verwendet werden, sondern auch zur Schaffung natürlicher Verhaltensmuster in der Gesellschaft. </font><font style="vertical-align: inherit;">Nur beim KI-Training sollten Sie bei der Auswahl eines Spiels vorsichtig sein.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de397557/">https://habr.com/ru/post/de397557/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de397547/index.html">Byrobot Petrone: Die besten (imho) Drohnen für Kinder. Und für Kämpfe</a></li>
<li><a href="../de397549/index.html">ProDOS 2.4 für Apple II: das erste Betriebssystem-Update für Apple II seit 23 Jahren</a></li>
<li><a href="../de397551/index.html">Alles Gute zum Geburtstag Stanislav Lem</a></li>
<li><a href="../de397553/index.html">Kick NOW! Будущее ближе с Kickstarter</a></li>
<li><a href="../de397555/index.html">Smart Home Apple HomeKit. Erster Eindruck</a></li>
<li><a href="../de397559/index.html">Europäische Forscher haben ein neues Verbundmaterial mit variabler Transparenz geschaffen</a></li>
<li><a href="../de397561/index.html">Audio Digest 9: Blogs zu Sound, Musik und Audiotechnologie</a></li>
<li><a href="../de397563/index.html">Die Echtheit des Code of Grolier, des vierten überlebenden Maya-Kodex, ist bewiesen.</a></li>
<li><a href="../de397565/index.html">Wie Lyft den Straßenverkehr in 10 Jahren sieht</a></li>
<li><a href="../de397567/index.html">Bei Nokia Bell Labs wurde eine Datenübertragung mit einer Geschwindigkeit von 1 Tbit / s über Glasfaser erreicht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>