<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§æüèæ üè© ü§¶üèø Das neuronale Netzwerk f√ºr Bildverarbeitung wird in realistischen Computerspielen trainiert. üë©üèæ üë®‚Äçüë®‚Äçüëß‚Äçüë¶ üë®üèΩ‚Äçü§ù‚Äçüë®üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aufnahmen aus dem Computerspiel Grand Theft Auto V und semantisches Markup zum Unterrichten eines
 
 neuronalen Maschinennetzwerks Neuronale Netze ste...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Das neuronale Netzwerk f√ºr Bildverarbeitung wird in realistischen Computerspielen trainiert.</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/397557/"><img src="https://habrastorage.org/files/47e/fcc/506/47efcc5062114f4b8ecc13630c9e361e.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aufnahmen aus dem Computerspiel Grand Theft Auto V und semantisches Markup zum Unterrichten eines</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
neuronalen </font><i><font style="vertical-align: inherit;">Maschinennetzwerks</font></i><font style="vertical-align: inherit;"> Neuronale Netze stellen in fast allen Computer-Vision-Wettbewerben neue Rekorde auf und werden zunehmend auch in anderen KI-Anwendungen verwendet. Eine der Schl√ºsselkomponenten einer solch unglaublichen Leistung eines neuronalen Netzwerks ist die Verf√ºgbarkeit gro√üer Datenmengen f√ºr Training und Evaluierung. Beispielsweise wird die Imagenet Large Scale Visual Recognition Challenge (ILSVRC) mit mehr als 1 Million Bildern zur Bewertung moderner neuronaler Netze verwendet. Nach den neuesten Ergebnissen zu urteilen (ResNet zeigt das Ergebnis von nur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3,57% der Fehler</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) m√ºssen Forscher bald umfangreichere Datens√§tze zusammenstellen. Und dann - noch umfangreicher. Das Kommentieren solcher Fotos ist √ºbrigens eine Menge Arbeit, die teilweise manuell erledigt werden muss. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Einige Entwickler von Computer-Vision-Systemen bieten eine alternative M√∂glichkeit, solche Systeme zu trainieren und zu testen. Anstatt Trainingsfotos manuell zu kommentieren, verwenden sie synthetisierte Frames aus realistischen Computerspielen.</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dies ist ein v√∂llig logischer Ansatz. In modernen Spielen haben Grafiken einen solchen Realismus erreicht, dass sich die synthetisierten Bilder nur geringf√ºgig von Fotografien der realen Welt unterscheiden. Gleichzeitig kann die Spiel-Engine eine unendliche Anzahl solcher Frames erzeugen - dies l√∂st das Problem, Millionen von Fotos f√ºr das Training und die Auswertung des neuronalen Netzwerks zu sammeln, sofort dramatisch. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl die Spiel-Engine eine begrenzte Anzahl von Texturen verwendet, gibt es eine Vielzahl von Kombinationen aus Betrachtungswinkeln, Beleuchtung, Wetter und Detaillierungsgrad, die eine ausreichende Vielfalt an Datens√§tzen bereitstellen.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In diesem Jahr haben zwei Forschergruppen in der Praxis gepr√ºft, ob es m√∂glich ist, die aus Computerspielen generierten Frames f√ºr das Training neuronaler Computer-Vision-Netzwerke zu verwenden. Eine Gruppe von Forschern der Informatikabteilung der University of British Columbia (Kanada) ver√∂ffentlichte einen </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wissenschaftlichen Artikel,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr den sie mehr als 60.000 Bilder aus einem Computerspiel mit Stra√üenansichten </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sammelten, die den Datens√§tzen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> von </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">CamVid</font></a><font style="vertical-align: inherit;"> und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cityscapes</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √§hneln </font><font style="vertical-align: inherit;">. Den Forschern gelang es zu beweisen, dass das neuronale Netzwerk nach dem Training auf synthetischen Bildern ein √§hnliches Fehlerniveau aufweist wie nach dem Training auf realen Fotos. Dar√ºber hinaus zeigt das Training synthetisierter Bilder mit realen Fotos ein noch besseres Ergebnis.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Alle 60.000 Bilder wurden bei virtuell sonnigem Wetter um 11:00 Uhr mit einer Aufl√∂sung von 1024 √ó 768 und maximalen Grafikeinstellungen aufgenommen (der Name des Spiels wurde aus urheberrechtlichen Gr√ºnden nicht bekannt gegeben). Ein unbemanntes Fahrzeug fuhr versehentlich durch die Spielstra√üen und beachtete die Stra√üenregeln. Die Bilder wurden einmal pro Sekunde aufgenommen. Jedes von ihnen wird von einer automatischen semantischen Segmentierung (Himmel, Fu√üg√§nger, Autos, B√§ume, Hintergrund - die Segmentierung ist absolut genau und aus dem Spiel √ºbernommen), einem tiefen Bild (Tiefenbild, Karte mit dem Markup von Objekten) sowie Normalen zur Oberfl√§che begleitet.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Zus√§tzlich zum Basis-VG-Datensatz haben die Forscher einen weiteren VG + -Datensatz mit vielen semantischen Informationen erstellt, der nicht auf f√ºnf Bezeichnungen beschr√§nkt ist - hier ist die Segmentierung nicht genau. Das Markup wurde automatisch mit </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SegNet durchgef√ºhrt</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br>
<br>
<img src="https://habrastorage.org/files/192/934/07c/19293407c01d4ab09be60d46e67c09e5.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eng markierte Frames aus dem VG +</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
-Satz Um die Effektivit√§t des neuronalen Netzwerktrainings zu vergleichen, wurden CamVid- und Cityscapes-Datens√§tze (f√ºnf Tags) sowie CamVid + ‚Äã‚Äãund Cityscapes + mit erweiterten Tag-Sets erstellt. </font></font><br>
<br>
<img src="https://habrastorage.org/files/d2b/3dd/1e0/d2b3dd1e02e042ebb4ec45ea61c20ba4.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Original-CamVid-Fotos mit Anmerkungen </font></font></i><br>
<br>
<img src="https://habrastorage.org/files/65f/888/ed1/65f888ed140f4456942204011e0ffd4a.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zwei zuf√§llige Bilder der Cityscapes + mit detaillierten Anmerkungen.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
F√ºr die semantische Klassifizierung wurde ein </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">langes Faltungs-Neuronales Netzwerk</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> mit einer einfachen FCN8-Architektur auf dem 16- </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lagigen VGG-Netz von Simonyan und Sisserman verwendet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Forscher f√ºhrten mehrere Experimente durch, um die Erkennungseffizienz von Objekten durch ein neuronales Netzwerk zu bewerten, das auf verschiedenen Datens√§tzen trainiert wurde. In fast allen F√§llen zeigte ein neuronales Netzwerk, das auf synthetischen Daten trainiert wurde, ein besseres Ergebnis als ein neuronales Netzwerk, das auf realen Fotos trainiert wurde. Sie zeigte das beste Ergebnis, selbst wenn sie echte Fotos √ºberpr√ºfte. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Tabelle zeigt beispielsweise die Leistung identischer neuronaler Netze, die an drei Datens√§tzen (reale Fotos, synthetische Daten aus dem Spiel, gemischter Satz) trainiert wurden, wenn Objekte in realen Fotos von CamVid + ‚Äã‚Äã- und Cityscapes + -S√§tzen erkannt werden. </font></font><br>
<br>
<img src="https://habrastorage.org/files/a64/eac/b8a/a64eacb8a5404f64897af9eed4dab2eb.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wie Sie sehen k√∂nnen, ist es beim Training eines neuronalen Netzwerks am besten, die synthetischen Bilder eines Computerspiels durch echte Fotos zu erg√§nzen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wissenschaftlicher Artikel</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ver√∂ffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> am 5. August 2016 auf arXiv.org, die zweite Version ist der 15. August ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pdf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Neben Forschern der University of British Columbia wurde fast gleichzeitig </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dieselbe Arbeit von einer anderen Gruppe von Wissenschaftlern der Technischen Universit√§t Darmstadt (Deutschland) und von Intel Labs geleistet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Sie nahmen 24.966 Bilder f√ºr das Training aus dem Open-World-Computerspiel Grand Theft Auto V. Die Forscher kamen zu dem gleichen Ergebnis: Bei Verwendung eines Trainingsdatensatzes, der aus 2/3 synthetischer Bilder und 1/3 CamVid-Fotos bestand, war die Genauigkeit Die Erkennung ist h√∂her als nur bei Verwendung von CamVid-Fotos. </font></font><br>
<br>
<img src="https://habrastorage.org/files/19f/38c/bc3/19f38cbc32c74e2fa2af351b47113b0b.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Genauigkeit der Erkennung verschiedener Objekte in Fotos aus dem CamVid-Set beim Lernen mit herk√∂mmlichen Methoden und bei Verwendung von Frames aus GTA V (unterste Zeile)</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Gleichzeitig reduziert die halbautomatische Annotation in einem speziell entwickelten Editor den Zeitaufwand f√ºr die Vorbereitung eines Datensatzes zum Trainieren eines neuronalen Netzwerks erheblich. Das Annotieren eines CamVid-Fotos dauert beispielsweise 60 Minuten, ein Cityscapes-Foto 90 Minuten und die halbautomatische Annotation von GTA V-Frames dauert durchschnittlich nur 7 Sekunden ( </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Video, Demonstration des Editors</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/JGAIfWG2MQQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Arbeit der Forscher von der </font><font style="vertical-align: inherit;">Technischen Universit√§t Darmstadt und Intel Labs hat f√ºr die Europ√§ische Konferenz √ºber Computer Vision vorbereitet </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ECCV'16</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (11-14 Oktober) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ver√∂ffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf der Website der </font><font style="vertical-align: inherit;">Universit√§t. Die Autoren legten den </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quellcode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> zum Lesen von Etiketten und </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vollst√§ndigen Datens√§tzen fest</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : sowohl Quellfotos als auch detaillierte Bilder mit semantischem Markup. Der Quellcode des Editors f√ºr Anmerkungen wird voraussichtlich in Zukunft ver√∂ffentlicht.</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Dank der Fortschritte bei der Entwicklung realistischer Computerspiele steht Entwicklern k√ºnstlicher Intelligenz eine hervorragende Plattform zum Erlernen von Bildverarbeitungssystemen zur Verf√ºgung. </font><font style="vertical-align: inherit;">Diese Systeme werden in unbemannten Fahrzeugen und Robotern eingesetzt. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Vielleicht k√∂nnen Computerspiele nicht nur f√ºr die Bildverarbeitung verwendet werden, sondern auch zur Schaffung nat√ºrlicher Verhaltensmuster in der Gesellschaft. </font><font style="vertical-align: inherit;">Nur beim KI-Training sollten Sie bei der Auswahl eines Spiels vorsichtig sein.</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de397557/">https://habr.com/ru/post/de397557/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de397547/index.html">Byrobot Petrone: Die besten (imho) Drohnen f√ºr Kinder. Und f√ºr K√§mpfe</a></li>
<li><a href="../de397549/index.html">ProDOS 2.4 f√ºr Apple II: das erste Betriebssystem-Update f√ºr Apple II seit 23 Jahren</a></li>
<li><a href="../de397551/index.html">Alles Gute zum Geburtstag Stanislav Lem</a></li>
<li><a href="../de397553/index.html">Kick NOW! –ë—É–¥—É—â–µ–µ –±–ª–∏–∂–µ —Å Kickstarter</a></li>
<li><a href="../de397555/index.html">Smart Home Apple HomeKit. Erster Eindruck</a></li>
<li><a href="../de397559/index.html">Europ√§ische Forscher haben ein neues Verbundmaterial mit variabler Transparenz geschaffen</a></li>
<li><a href="../de397561/index.html">Audio Digest 9: Blogs zu Sound, Musik und Audiotechnologie</a></li>
<li><a href="../de397563/index.html">Die Echtheit des Code of Grolier, des vierten √ºberlebenden Maya-Kodex, ist bewiesen.</a></li>
<li><a href="../de397565/index.html">Wie Lyft den Stra√üenverkehr in 10 Jahren sieht</a></li>
<li><a href="../de397567/index.html">Bei Nokia Bell Labs wurde eine Daten√ºbertragung mit einer Geschwindigkeit von 1 Tbit / s √ºber Glasfaser erreicht</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>