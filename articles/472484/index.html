<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí≤ üì∏ üö¥üèΩ Cerrar agujeros en un grupo de Kubernetes. Informe y transcripci√≥n con DevOpsConf üèº üçß üéâ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pavel Selivanov, arquitecto de soluciones de Southbridge y profesor de Slurm, dio una charla en DevOpsConf 2019. Esta charla es parte de Kubernetes Sl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cerrar agujeros en un grupo de Kubernetes. Informe y transcripci√≥n con DevOpsConf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/472484/"><p>  Pavel Selivanov, arquitecto de soluciones de Southbridge y profesor de Slurm, dio una charla en DevOpsConf 2019. Esta charla es parte de Kubernetes Slur Mega, un curso en profundidad sobre Kubernetes. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Slurm Basic: una introducci√≥n a Kubernetes</a> tiene lugar en Mosc√∫ del 18 al 20 de noviembre. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Slurm Mega: Miramos bajo el cap√≥ de Kubernetes</a> - Mosc√∫, 22-24 de noviembre. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Slurm Online: Ambos cursos de Kubernetes siempre est√°n</a> disponibles. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Gt4Q1du5FXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Debajo del corte - transcripci√≥n del informe. </p><a name="habracut"></a><br><p>  Buenas tardes, colegas y simpatizantes.  Hoy hablar√© sobre seguridad. </p><br><p>  Veo que hay muchos guardias de seguridad en el pasillo hoy.  Le pido disculpas de antemano si no voy a utilizar los t√©rminos del mundo de la seguridad de la manera habitual en usted. </p><br><p>  Dio la casualidad de que hace unos seis meses me puse en manos de un grupo p√∫blico de Kubernetes.  P√∫blico: significa que hay un en√©simo n√∫mero de espacios de nombres, en estos espacios de nombres hay usuarios aislados en sus espacios de nombres.  Todos estos usuarios pertenecen a diferentes empresas.  Bueno, se supon√≠a que este cl√∫ster deber√≠a usarse como CDN.  Es decir, le dan un cl√∫ster, le dan al usuario all√≠, usted va all√≠ en su espacio de nombres, despliega sus frentes. </p><br><p>  Intentaron vender ese servicio a mi empresa anterior.  Y me pidieron que hiciera un cl√∫ster sobre el tema: ¬øes esta soluci√≥n adecuada o no? </p><br><p>  Vine a este grupo.  Me dieron derechos limitados, espacio de nombres limitado.  All√≠, los chicos entendieron qu√© era la seguridad.  Leyeron qu√© es el control de acceso basado en Rober (RBAC) para Kubernetes, y lo torcieron para que no pudiera ejecutar pods por separado de la implementaci√≥n.  No recuerdo la tarea que estaba tratando de resolver ejecutando sin implementaci√≥n, pero realmente quer√≠a ejecutar justo debajo.  Decid√≠ por suerte ver qu√© derechos tengo en el cl√∫ster, qu√© puedo, qu√© no puedo, qu√© arruinaron all√≠.  Al mismo tiempo, le dir√© lo que han configurado incorrectamente en el RBAC. </p><br><p>  Dio la casualidad de que en dos minutos llev√© un administrador a su cl√∫ster, mir√© todos los espacios de nombres vecinos, vi los frentes de producci√≥n de las empresas que ya hab√≠an comprado el servicio y me qued√© estancado all√≠.  Apenas me detuve, para no acercarme a alguien en el frente y no poner ninguna palabra obscena en la p√°gina principal. </p><br><p>  Te dir√© con ejemplos c√≥mo hice esto y c√≥mo protegerme de esto. </p><br><p>  Pero primero, pres√©nteme.  Me llamo Pavel Selivanov.  Soy arquitecto en Southbridge.  Entiendo a Kubernetes, DevOps y todo tipo de cosas elegantes.  Los ingenieros de Southbridge y yo estamos construyendo todo esto, y lo aconsejo. </p><br><p>  Adem√°s de nuestro negocio principal, recientemente lanzamos proyectos llamados Slory.  Estamos tratando de llevar nuestra capacidad de trabajar con Kubernetes a las masas, para ense√±ar a otras personas c√≥mo trabajar con K8 tambi√©n. </p><br><p> De lo que hablar√© hoy.  El tema del informe es obvio: la seguridad del cl√∫ster de Kubernetes.  Pero quiero decir de inmediato que este tema es muy grande y, por lo tanto, quiero estipular de inmediato lo que no dir√© con certeza.  No voy a hablar de t√©rminos tristes que ya est√°n sobreexcitados en Internet.  Cualquier RBAC y certificados. </p><br><p>  Hablar√© sobre c√≥mo mis colegas y yo estamos hartos de la seguridad en el grupo de Kubernetes.  Vemos estos problemas tanto con los proveedores que proporcionan cl√∫steres de Kubernetes como con los clientes que acuden a nosotros.  E incluso con clientes que nos visitan desde otras empresas de consultor√≠a administrativa.  Es decir, la escala de la tragedia es, de hecho, muy grande. </p><br><p>  Literalmente tres puntos, de los que hablar√© hoy: </p><br><ol><li>  Derechos de usuario vs derechos de pod.  Los derechos de usuario y los derechos de hogar no son lo mismo. </li><li>  Recopilaci√≥n de informaci√≥n del cl√∫ster.  Le mostrar√© que desde el cl√∫ster puede recopilar toda la informaci√≥n que necesita sin tener derechos especiales en este cl√∫ster. </li><li>  Ataque DoS en el cl√∫ster.  Si no podemos recopilar informaci√≥n, podemos poner el cl√∫ster en cualquier caso.  Hablar√© sobre los ataques DoS en los controles de cl√∫ster. </li></ol><br><p>  Otra cosa com√∫n que mencionar√© es d√≥nde lo prob√© todo, lo que puedo decir con certeza de que todo funciona. </p><br><p>  Como base, tomamos la instalaci√≥n de un cl√∫ster de Kubernetes usando Kubespray.  Si alguien no sabe, este es en realidad un conjunto de roles para Ansible.  Lo estamos usando constantemente en nuestro trabajo.  Lo bueno es que puedes rodar en cualquier lugar, puedes rodar en las gl√°ndulas y en alg√∫n lugar de la nube.  Un m√©todo de instalaci√≥n es adecuado en principio para todo. </p><br><p>  En este cl√∫ster, tendr√© Kubernetes v1.14.5.  Todo el grupo de Cuba, que consideraremos, est√° dividido en espacios de nombres, cada espacio de nombres pertenece a un equipo separado, los miembros de este equipo tienen acceso a cada espacio de nombres.  No pueden ir a espacios de nombres diferentes, solo a los suyos.  Pero hay alguna cuenta de administrador que tiene derechos para todo el cl√∫ster. </p><br><p><img src="https://habrastorage.org/webt/xm/rj/rx/xmrjrxw6toktjj1ukm-tir_remm.jpeg"></p><br><p>  Promet√≠ que lo primero que tendremos ser√° obtener derechos de administrador para el cl√∫ster.  Necesitamos una c√°psula especialmente preparada que rompa el cl√∫ster de Kubernetes.  Todo lo que tenemos que hacer es aplicarlo al cl√∫ster de Kubernetes. </p><br><pre><code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Esta c√°psula llegar√° a uno de los maestros del grupo Kubernetes.  Y despu√©s de eso, el cl√∫ster nos devolver√° un archivo llamado admin.conf.  En Cuba, todos los certificados de administrador se almacenan en este archivo y, al mismo tiempo, se configura la API del cl√∫ster.  As√≠ es como puede obtener acceso de administrador, creo, al 98% de los grupos de Kubernetes. </p><br><p>  Repito, este pod fue creado por un desarrollador en su cl√∫ster que tiene acceso para implementar sus propuestas en un peque√±o espacio de nombres, todo est√° sujeto por RBAC.  No ten√≠a derechos.  Sin embargo, el certificado ha regresado. </p><br><p>  Y ahora sobre el hogar especialmente preparado.  Ejecutar en cualquier imagen.  Por ejemplo, tome debian: jessie. </p><br><p>  Tenemos tal cosa: </p><br><pre> <code class="plaintext hljs">tolerations: - effect: NoSchedule operator: Exists nodeSelector: node-role.kubernetes.io/master: ""</code> </pre> <br><p>  ¬øQu√© es la tolerancia?  Los maestros en el grupo de Kubernetes suelen estar marcados con una cosa llamada mancha ("infecci√≥n" en ingl√©s).  Y la esencia de esta "infecci√≥n": dice que las vainas no pueden asignarse a nodos maestros.  Pero nadie se molesta en indicar de ninguna manera que es tolerante con la "infecci√≥n".  La secci√≥n de Tolerancia solo dice que si NoSchedule est√° en alg√∫n nodo, entonces nuestra infecci√≥n es tolerante, y no hay problemas. </p><br><p>  Adem√°s, decimos que nuestro bajo no solo es tolerante, sino que tambi√©n quiere caer espec√≠ficamente en el maestro.  Porque los maestros son los m√°s deliciosos que necesitamos: todos los certificados.  Por lo tanto, decimos nodeSelector, y tenemos una etiqueta est√°ndar en los asistentes, que nos permite seleccionar exactamente aquellos nodos que son asistentes de todos los nodos del cl√∫ster. </p><br><p>  Con tales dos secciones, definitivamente vendr√° al maestro.  Y se le permitir√° vivir all√≠. </p><br><p>  Pero solo venir al maestro no es suficiente para nosotros.  No nos dar√° nada.  Por lo tanto, adem√°s tenemos estas dos cosas: </p><br><pre> <code class="plaintext hljs">hostNetwork: true hostPID: true</code> </pre> <br><p>  Indicamos que nuestro under, que estamos lanzando, vivir√° en el espacio de nombres del n√∫cleo, en el espacio de nombres de la red y en el espacio de nombres PID.  Tan pronto como se inicie en el asistente, podr√° ver todas las interfaces reales y en vivo de este nodo, escuchar todo el tr√°fico y ver el PID de todos los procesos. </p><br><p>  A continuaci√≥n, es peque√±o.  Toma etcd y lee lo que quieras. </p><br><p>  Lo m√°s interesante es esta caracter√≠stica de Kubernetes, que est√° presente all√≠ de forma predeterminada. </p><br><pre> <code class="plaintext hljs">volumeMounts: - mountPath: /host name: host volumes: - hostPath: path: / type: Directory name: host</code> </pre> <br><p>  Y su esencia es que podemos decir que queremos crear un volumen de tipo hostPath en el pod que ejecutamos, incluso sin los derechos de este cl√∫ster.  Significa tomar el camino desde el host en el que comenzaremos, y tomarlo como volumen.  Y luego ll√°malo nombre: host.  Todo este hostPath lo montamos dentro del hogar.  En este ejemplo, al directorio / host. </p><br><p>  Repito una vez m√°s.  Le dijimos al pod que viniera al maestro, obtenga hostNetwork y hostPID all√≠, y monte toda la ra√≠z del maestro dentro de este pod. </p><br><p>  Entiende que en Debian tenemos bash ejecut√°ndose, y este bash funciona bajo nuestra ra√≠z.  Es decir, acabamos de obtener la ra√≠z del maestro, sin tener ning√∫n derecho en el cl√∫ster de Kubernetes. </p><br><p>  Entonces, toda la tarea es ir al directorio bajo / host / etc / kubernetes / pki, si no me equivoco, recoger todos los certificados maestros del cl√∫ster all√≠ y, en consecuencia, convertirme en el administrador del cl√∫ster. </p><br><p>  Si mira de esta manera, estos son algunos de los derechos m√°s peligrosos en los pods, a pesar de los derechos del usuario: <br><img src="https://habrastorage.org/webt/ax/07/cj/ax07cjhm7y0dwpueikrj-qwqv2k.jpeg"></p><br><p>  Si tengo derechos para ejecutar en alg√∫n espacio de nombres de cl√∫ster, entonces este sub tiene estos derechos por defecto.  Puedo ejecutar pods privilegiados, y esto es generalmente todos los derechos, pr√°cticamente root en el nodo. </p><br><p>  Mi favorito es el usuario root.  Y Kubernetes tiene esa opci√≥n Ejecutar como no ra√≠z.  Este es un tipo de protecci√≥n contra piratas inform√°ticos.  ¬øSabes qu√© es el "virus de Moldavia"?  Si eres un hacker y vienes a mi cl√∫ster de Kubernetes, entonces, los administradores pobres, preguntamos: ‚ÄúPor favor, indica en tus pods con los que piratear√°s mi cl√∫ster, ejecuta como no root.  Y sucede que comienzas el proceso en tu hogar debajo de la ra√≠z, y ser√° muy f√°cil para ti hackearme.  Prot√©gete de ti mismo ". </p><br><p>  Volumen de ruta de host: en mi opini√≥n, la forma m√°s r√°pida de obtener el resultado deseado del cl√∫ster de Kubernetes. </p><br><p>  ¬øPero qu√© hacer con todo esto? </p><br><p>  Pensamientos que deber√≠an venir a cualquier administrador normal que se encuentre con Kubernetes: ‚ÄúS√≠, te lo dije, Kubernetes no funciona.  Hay agujeros en ella.  Y todo el cubo es una mierda ".  De hecho, existe una documentaci√≥n, y si mira all√≠, hay una secci√≥n de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pol√≠tica de Seguridad de Pod</a> . </p><br><p>  Este es un objeto yaml: podemos crearlo en el cl√∫ster de Kubernetes, que controla los aspectos de seguridad en la descripci√≥n de los hogares.  Es decir, de hecho, controla esos derechos para usar todo tipo de hostNetwork, hostPID, ciertos tipos de volumen, que est√°n en los pods durante el inicio.  Con la Pol√≠tica de seguridad de Pod, todo esto se puede describir. </p><br><p>  Lo m√°s interesante de la Pol√≠tica de seguridad de pod es que en el cl√∫ster de Kubernetes, todos los instaladores de PSP no solo no se describen en absoluto, simplemente se desactivan de forma predeterminada.  La pol√≠tica de seguridad de pod se habilita mediante el complemento de admisi√≥n. </p><br><p>  Bien, terminemos en una pol√≠tica de seguridad de pod de cl√∫ster, digamos que tenemos alg√∫n tipo de pod de servicio en el espacio de nombres, al que solo los administradores tienen acceso.  Digamos que en todas las dem√°s c√°psulas tienen derechos limitados.  Porque lo m√°s probable es que los desarrolladores no necesiten ejecutar pods privilegiados en su cl√∫ster. </p><br><p>  Y todo parece estar bien con nosotros.  Y nuestro cl√∫ster de Kubernetes no puede ser pirateado en dos minutos. </p><br><p>  Hay un problema  Lo m√°s probable es que si tiene un cl√∫ster de Kubernetes, la supervisi√≥n se instala en su cl√∫ster.  Incluso supongo que predecir que si hay monitoreo en su cl√∫ster, entonces se llama Prometheus. </p><br><p>  Lo que le dir√© ahora ser√° v√°lido tanto para el operador Prometheus como para el Prometheus entregado en su forma pura.  La pregunta es que si no puedo poner al administrador tan r√°pido en el cl√∫ster, significa que tengo que buscar m√°s.  Y puedo buscar usando su monitoreo. </p><br><p>  Probablemente, todos leen los mismos art√≠culos sobre Habr√©, y el monitoreo est√° en monitoreo.  Helm chart se llama aproximadamente igual para todos.  Supongo que si hace helm install stable / prometheus, obtendr√° aproximadamente los mismos nombres.  E incluso lo m√°s probable es que no tenga que adivinar el nombre DNS en su cl√∫ster.  Porque es est√°ndar. </p><br><p><img src="https://habrastorage.org/webt/o6/rv/fw/o6rvfw0idykw0wyxivpfmwkd_vy.jpeg"></p><br><p>  Adem√°s, tenemos un cierto dev ns, en √©l es posible lanzar un cierto under.  Y m√°s all√° de este hogar, es muy f√°cil hacer esto: </p><br><pre> <code class="plaintext hljs">$ curl http://prometheus-kube-state-metrics.monitoring</code> </pre> <br><p>  prometheus-kube-state-metrics es uno de los exportadores de prometheus que recopila m√©tricas de la API de Kubernetes.  Hay muchos datos que se ejecutan en su cl√∫ster, qu√© es, qu√© problemas tiene con √©l. </p><br><p>  Como un simple ejemplo: </p><br><p>  kube_pod_container_info {namespace = "kube-system", pod = "kube-apiserver-k8s-1", container = "kube-apiserver", image = </p><br><p>  <strong>"gcr.io/google-containers/kube-apiserver:v1.14.5"</strong> </p><br><p>  , Image_id = "ventana acoplable-pullable: //gcr.io/google-containers/kube- apiserver @ sha256: e29561119a52adad9edc72bfe0e7fcab308501313b09bf99df4a96 38ee634989", container_id = "ventana acoplable: // 7cbe7b1fea33f811fdd8f7e0e079191110268f2 853397d7daf08e72c22d3cf8b"} 1 </p><br><p>  Despu√©s de haber realizado una simple solicitud de curl desde un archivo sin privilegios, puede obtener dicha informaci√≥n.  Si no sabe en qu√© versi√≥n de Kubernetes se est√° ejecutando, se lo informar√° f√°cilmente. </p><br><p>  Y lo m√°s interesante es que, adem√°s del hecho de que recurra a kube-state-metrics, tambi√©n puede aplicar directamente a Prometheus.  Puede recopilar m√©tricas desde all√≠.  Incluso puedes construir m√©tricas desde all√≠.  Incluso te√≥ricamente, puede crear una solicitud de este tipo desde un cl√∫ster en Prometheus, que simplemente la desactiva.  Y su monitoreo generalmente deja de funcionar desde el cl√∫ster. </p><br><p>  Y aqu√≠ surge la pregunta de si alg√∫n monitoreo externo monitorea su monitoreo.  Acabo de tener la oportunidad de actuar en el grupo de Kubernetes sin ninguna consecuencia para m√≠.  Ni siquiera sabr√° que estoy actuando all√≠, ya que ya no hay monitoreo. </p><br><p>  Al igual que con PSP, parece que el problema es que todas estas tecnolog√≠as de moda, Kubernetes, Prometheus, simplemente no funcionan y est√°n llenas de agujeros.  En realidad no </p><br><p>  Existe tal cosa - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Pol√≠tica de red</a> . </p><br><p>  Si es un administrador normal, lo m√°s probable es que sobre la Pol√≠tica de red sepa que este es otro yaml, que en el cl√∫ster ya est√° desactivado.  Y algunas pol√≠ticas de red definitivamente no son necesarias.  E incluso si lee qu√© es la Pol√≠tica de red, qu√© es el cortafuegos yaml de Kubernetes, le permite restringir los derechos de acceso entre espacios de nombres, entre pods, entonces definitivamente decidi√≥ que el cortafuegos tipo yaml en Kubernetes est√° en las pr√≥ximas abstracciones ... No, no .  Esto definitivamente no es necesario. </p><br><p>  Incluso si a sus especialistas en seguridad no se les dijo que al usar sus Kubernetes, puede construir un firewall de manera f√°cil y simple, y es muy granular.  Si todav√≠a no saben esto y no lo atraen: "Bueno, den, den ..." En cualquier caso, necesita una Pol√≠tica de red para bloquear el acceso a algunos lugares oficiales que puede extraer de su cl√∫ster sin ninguna autorizaci√≥n. </p><br><p>  Como en el ejemplo que cit√©, puede extraer las m√©tricas de estado de kube de cualquier espacio de nombres en el cl√∫ster de Kubernetes sin tener ning√∫n derecho.  Las pol√≠ticas de red cerraron el acceso de todos los dem√°s espacios de nombres a la supervisi√≥n del espacio de nombres y, por as√≠ decirlo, todo: sin acceso, sin problemas.  En todos los gr√°ficos que existen, tanto el prometeus est√°ndar como ese prometeus que est√° en el operador, simplemente hay una opci√≥n en los valores de tim√≥n para habilitar simplemente pol√≠ticas de red para ellos.  Solo necesita encenderlo y funcionar√°n. </p><br><p>  Realmente hay un problema aqu√≠.  Al ser un administrador barbudo normal, lo m√°s probable es que hayas decidido que las pol√≠ticas de red no son necesarias.  Y despu√©s de leer todo tipo de art√≠culos sobre recursos como Habr, decidi√≥ que la franela, especialmente con el modo host-gateway, es lo mejor que puede elegir. </p><br><p>  Que hacer </p><br><p>  Puede intentar volver a implementar la soluci√≥n de red que est√° en su cl√∫ster de Kubernetes, intente reemplazarla con algo m√°s funcional.  En el mismo Calico, por ejemplo.  Pero inmediatamente quiero decir que la tarea de cambiar la soluci√≥n de red en el cl√∫ster de trabajo de Kubernetes es bastante trivial.  Lo resolv√≠ dos veces (ambas veces, sin embargo, te√≥ricamente), pero incluso mostramos c√≥mo hacer esto en Slurms.  Para nuestros estudiantes, mostramos c√≥mo cambiar la soluci√≥n de red en el cl√∫ster de Kubernetes.  En principio, puede intentar asegurarse de que no haya tiempo de inactividad en el cl√∫ster de producci√≥n.  Pero probablemente no tendr√°s √©xito. </p><br><p>  Y el problema se resuelve de manera muy simple.  Hay certificados en el cl√∫ster, y sabe que sus certificados ir√°n mal en un a√±o.  Bueno, y generalmente es una soluci√≥n normal con certificados en el cl√∫ster: ¬øpor qu√© vamos a utilizar vapor? Levantaremos un nuevo cl√∫ster junto a √©l, lo dejaremos podrido en el anterior y lo rehaceremos todo.  Es cierto que cuando va mal, todo se acostar√° en nuestros d√≠as, pero luego un nuevo grupo. </p><br><p>  Cuando levantes un nuevo grupo, al mismo tiempo inserta Calico en lugar de franela. </p><br><p>  ¬øQu√© hacer si tiene certificados emitidos durante cien a√±os y no va a reagrupar el cl√∫ster?  Existe tal cosa Kube-RBAC-Proxy.  Este es un desarrollo muy bueno, le permite integrarse como un contenedor de sidecar a cualquier hogar en el cl√∫ster de Kubernetes.  Y en realidad agrega autorizaci√≥n a trav√©s de Kubernetes RBAC a este pod. </p><br><p>  Hay un problema  Anteriormente, Kube-RBAC-Proxy se incorpor√≥ al prometeo del operador.  Pero luego se fue.  Ahora, las versiones modernas se basan en el hecho de que tienes una pol√≠tica de red y dejas de usarlas.  Y entonces tienes que reescribir un poco la tabla.  De hecho, si va a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este repositorio</a> , hay ejemplos de c√≥mo usarlo como sidecars, y tendr√° que reescribir los gr√°ficos m√≠nimamente. </p><br><p>  Hay otro peque√±o problema.  Prometheus no solo brinda sus m√©tricas a cualquiera que las obtenga.  Tenemos todos los componentes del cl√∫ster de Kubernetes, tambi√©n, pueden dar sus m√©tricas. </p><br><p>  Pero como dije, si no puede acceder al cl√∫ster y recopilar informaci√≥n, al menos puede hacer da√±o. </p><br><p>  As√≠ que le mostrar√© r√°pidamente dos formas en que puede estropear su cl√∫ster de Kubernetes. </p><br><p>  Te reir√°s cuando te lo diga, estos son dos casos de la vida real. </p><br><p>  La primera manera  Quedando sin recursos. </p><br><p>  Lanzamos un especial m√°s bajo.  Tendr√° esa secci√≥n. </p><br><pre> <code class="plaintext hljs">resources: requests: cpu: 4 memory: 4Gi</code> </pre> <br><p>  Como sabe, las solicitudes son la cantidad de CPU y memoria reservada en el host para pods espec√≠ficos con solicitudes.  Si tenemos un host de cuatro n√∫cleos en el cl√∫ster de Kubernetes, y cuatro CPU vienen con solicitudes, significa que no pueden venir m√°s pods con solicitudes a este host. </p><br><p>  Si ejecuto esto debajo, entonces har√© un comando: </p><br><pre> <code class="plaintext hljs">$ kubectl scale special-pod --replicas=...</code> </pre> <br><p>  Entonces nadie m√°s podr√° desplegarse en el cl√∫ster de Kubernetes.  Porque en todos los nodos las solicitudes finalizar√°n.  Y as√≠ detengo tu grupo de Kubernetes.  Si hago esto por la noche, entonces puedo detener la implementaci√≥n durante bastante tiempo. </p><br><p>  Si volvemos a mirar la documentaci√≥n de Kubernetes, veremos algo llamado Rango L√≠mite.  Establece recursos para objetos de cl√∫ster.  Puede escribir un objeto de L√≠mite de rango en yaml, aplicarlo a ciertos espacios de nombres, y adem√°s en este espacio de nombres puede decir que tiene recursos para los pods predeterminados, m√°ximos y m√≠nimos. </p><br><p>  Con la ayuda de tal cosa, podemos limitar a los usuarios en espacios de nombres de productos espec√≠ficos de equipos en la capacidad de indicar cualquier cosa desagradable en sus pods.  Pero desafortunadamente, incluso si le dice al usuario que es imposible ejecutar pods con solicitudes de m√°s de una CPU, hay un comando de escala tan maravilloso, bueno, o bien a trav√©s del tablero de instrumentos pueden escalar. </p><br><p>  Y de aqu√≠ viene el m√©todo n√∫mero dos.  Lanzamos 11 111 111 111 111 hogares.  Eso es once mil millones.  Esto no es porque se me ocurri√≥ ese n√∫mero, sino porque lo vi yo mismo. </p><br><p>  La verdadera historia  A √∫ltima hora de la tarde estaba a punto de abandonar la oficina.  Miro, un grupo de desarrolladores est√° sentado en la esquina y haciendo algo fren√©ticamente con las computadoras port√°tiles.  Me acerco a los chicos y les pregunto: "¬øQu√© te pas√≥?" </p><br><p>  Un poco antes, a las nueve de la noche, uno de los desarrolladores se iba a casa.  Y decidi√≥: "Voy a omitir mi solicitud hasta una ahora".  Hice clic un poco e Internet fue un poco aburrido.  Una vez m√°s hizo clic en la unidad, presion√≥ la unidad, hizo clic en Entrar.  Empuj√≥ todo lo que pudo.  Luego, Internet cobr√≥ vida, y todo comenz√≥ a escalar a esta fecha. </p><br><p>  Es cierto que esta historia no ocurri√≥ en Kubernetes, en ese momento era n√≥mada.  Termin√≥ con el hecho de que despu√©s de una hora de nuestros intentos de detener a Nomad de los obstinados intentos de permanecer juntos, Nomad respondi√≥ que no dejar√≠a de quedarse y no har√≠a nada m√°s.  "Estoy cansado, me voy".  Y acurrucado. </p><br><p>  Naturalmente intent√© hacer lo mismo en Kubernetes.  Los once mil millones de vainas de Kubernetes no estaban contentos, dijo: "No puedo.  Excede los protectores bucales internos ".  Pero 1,000,000,000 de hogares podr√≠an. </p><br><p>  En respuesta a mil millones, el Cubo no entr√≥.  Realmente comenz√≥ a escalar.  Cuanto m√°s avanzaba el proceso, m√°s tiempo le llev√≥ crear nuevos hogares.  Pero a√∫n as√≠ el proceso continu√≥.  El √∫nico problema es que si puedo ejecutar pods indefinidamente en mi espacio de nombres, entonces, incluso sin solicitudes y l√≠mites, puedo iniciar tal cantidad de pods con algunas tareas que con estas tareas los nodos comienzan a acumularse desde la memoria, desde la CPU.  Cuando ejecuto tantos hogares, la informaci√≥n de ellos debe ir al repositorio, es decir, etc.  Y cuando llega demasiada informaci√≥n all√≠, el almac√©n comienza a ceder muy lentamente, y en Kubernetes comienzan las cosas aburridas. </p><br><p>  Y un problema m√°s ... Como saben, los elementos de control de Kubernetes no son solo una cosa central, sino varios componentes.  All√≠, en particular, hay un administrador de controlador, un planificador, etc.  Todos estos tipos comenzar√°n a hacer un est√∫pido trabajo innecesario al mismo tiempo, lo que con el tiempo comenzar√° a tomar m√°s y m√°s tiempo.  El administrador del controlador crear√° nuevos pods.  El programador intentar√° encontrarles un nuevo nodo.  Los nuevos nodos en su cl√∫ster probablemente terminar√°n pronto.  El Cl√∫ster de Kubernetes comenzar√° a trabajar m√°s y m√°s lentamente. </p><br><p>  Pero decid√≠ ir a√∫n m√°s lejos.  Como saben, en Kubernetes existe una cosa llamada servicio.  Bueno, y de manera predeterminada en sus cl√∫steres, lo m√°s probable es que el servicio funcione con tablas IP. </p><br><p>  Si ejecuta mil millones de hogares, por ejemplo, y luego usa un script para obligar a Kubernetis a crear nuevos servicios: </p><br><pre> <code class="plaintext hljs">for i in {1..1111111}; do kubectl expose deployment test --port 80 \ --overrides="{\"apiVersion\": \"v1\", \"metadata\": {\"name\": \"nginx$i\"}}"; done</code> </pre> <br><p>  En todos los nodos del cl√∫ster, se generar√°n aproximadamente nuevas reglas de iptables aproximadamente de forma simult√°nea.  Adem√°s, para cada servicio, se generar√°n mil millones de reglas de iptables. </p><br><p>  Revis√© todo esto en varios miles, hasta una docena.  Y el problema es que ya en este umbral ssh en el nodo es bastante problem√°tico de hacer.  Debido a que los paquetes, al pasar tantas cadenas, comienzan a no sentirse muy bien. </p><br><p>  Y todo esto tambi√©n se resuelve con la ayuda de Kubernetes.  Existe tal objeto de cuota de recursos.  Establece el n√∫mero de recursos y objetos disponibles para un espacio de nombres en un cl√∫ster.  Podemos crear un objeto yaml en cada espacio de nombres del cl√∫ster de Kubernetes.  Con este objeto, podemos decir que hemos asignado un cierto n√∫mero de solicitudes, l√≠mites para este espacio de nombres, y luego podemos decir que en este espacio de nombres es posible crear 10 servicios y 10 pods.  Y un √∫nico desarrollador puede al menos exprimir las noches.  Kubernetes le dir√°: "No puedes poner tus c√°psulas a tal cantidad porque excede la cuota de recursos".  Todo, el problema est√° resuelto.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">La documentaci√≥n est√° aqu√≠</a> . </p><br><p>  Un punto problem√°tico surge en relaci√≥n con esto.  Siente lo dif√≠cil que resulta crear un espacio de nombres en Kubernetes.  Para crearlo, debemos considerar un mont√≥n de cosas. </p><br><p>  Cuota de recursos + Rango l√≠mite + RBAC <br>  ‚Ä¢ Crear un espacio de nombres <br>  ‚Ä¢ Crear dentro del rango l√≠mite <br>  ‚Ä¢ Crear dentro de la cuota de recursos <br>  ‚Ä¢ Crear una cuenta de servicio para CI <br>  ‚Ä¢ Crear un rolebinding para CI y usuarios <br>  ‚Ä¢ Opcionalmente, ejecute los pods de servicio necesarios </p><br><p>  Por lo tanto, aprovechando esta oportunidad, me gustar√≠a compartir mis desarrollos.  Existe tal cosa, llamada operador SDK.  Esta es una forma en el cl√∫ster de Kubernetes para escribir operadores para √©l.  Puedes escribir declaraciones usando Ansible. </p><br><p>  Primero, fue escrito en Ansible, y luego mir√© que hab√≠a un operador de SDK y reescrib√≠ el rol de Ansible en el operador.  Este operador le permite crear un objeto en el cl√∫ster de Kubernetes llamado equipo.       yaml    .       ,    - . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">    </a> . </p><br><p>   .     ? <br> . Pod Security Policy ‚Äî  .     ,            , -      . </p><br><p> Network Policy ‚Äî   -    .  ,     . </p><br><p> LimitRange/ResourceQuota ‚Äî   .     ,     ,     . ,   . </p><br><p>  ,      ,   ,    .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">   </a> . </p><br><p>      .  ,           warlocks ,   . </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a>  ,   ,   .      ,  ResourceQuota, Pod Security Policy .     . </p><br><p>  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472484/">https://habr.com/ru/post/472484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472468/index.html">ClusterJ - trabajando con MySQL NDB Cluster de Java</a></li>
<li><a href="../472470/index.html">Ratones transg√©nicos y antienvejecimiento</a></li>
<li><a href="../472472/index.html">Casa rural en invierno: ¬øser o no ser?</a></li>
<li><a href="../472474/index.html">Divertido error cosm√©tico en Google Chrome</a></li>
<li><a href="../472482/index.html">Accidente radiactivo: descubrimiento de una fase s√≥lida estable de plutonio</a></li>
<li><a href="../472486/index.html">Almacenamiento de datos a largo plazo. (Art√≠culo - discusi√≥n)</a></li>
<li><a href="../472488/index.html">Treinta informes de DevOops 2019: Tim Lister, Hadi Hariri, Roman Shaposhnik y otras estrellas de DevOps internacionales</a></li>
<li><a href="../472492/index.html">Analizando el C√≥digo de ROOT, Marco de An√°lisis de Datos Cient√≠ficos</a></li>
<li><a href="../472494/index.html">An√°lisis de c√≥digo ROOT: marco de an√°lisis de datos de investigaci√≥n</a></li>
<li><a href="../472496/index.html">Crear un dise√±o de Scrapbook en una cuadr√≠cula CSS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>