<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äç‚öñÔ∏è ‚öõÔ∏è üëäüèª Introduciendo el m√©todo de retropropagaci√≥n üñ§ üöÜ üë®üèæ‚Äçüè´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! Las vacaciones de A√±o Nuevo han llegado a su fin, lo que significa que estamos nuevamente listos para compartir material √∫til con usted....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introduciendo el m√©todo de retropropagaci√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>Hola a todos!</i>  <i>Las vacaciones de A√±o Nuevo han llegado a su fin, lo que significa que estamos nuevamente listos para compartir material √∫til con usted.</i>  <i>Se prepar√≥ una traducci√≥n de este art√≠culo en previsi√≥n del lanzamiento de una nueva transmisi√≥n en el curso <a href="https://otus.pw/h0mh/">"Algoritmos para desarrolladores"</a> .</i> <i><br><br></i>  <i>Vamos!</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br>  El m√©todo de error de propagaci√≥n hacia atr√°s es probablemente el componente m√°s fundamental de una red neuronal.  Fue descrita por primera vez en la d√©cada de 1960 y casi 30 a√±os despu√©s fue popularizada por Rumelhart, Hinton y Williams en un art√≠culo titulado <a href="https://www.nature.com/articles/323533a0">"Aprendizaje de representaciones por errores de propagaci√≥n inversa"</a> . <a name="habracut"></a><br><br>  El m√©todo se usa para entrenar de manera efectiva una red neuronal usando la llamada regla de la cadena (la regla de diferenciaci√≥n de una funci√≥n compleja).  En pocas palabras, despu√©s de cada pasada a trav√©s de la red, la propagaci√≥n inversa realiza una pasada en la direcci√≥n opuesta y ajusta los par√°metros del modelo (pesos y desplazamientos). <br><br>  En este art√≠culo, me gustar√≠a considerar en detalle desde un punto de vista matem√°tico el proceso de aprendizaje y optimizaci√≥n de una red neuronal simple de 4 capas.  Creo que esto ayudar√° al lector a comprender c√≥mo funciona la propagaci√≥n hacia atr√°s, as√≠ como a darse cuenta de su importancia. <br><br><h3>  Definiendo un modelo de red neuronal </h3><br>  La red neuronal de cuatro capas consta de cuatro neuronas en la capa de entrada, cuatro neuronas en las capas ocultas y 1 neurona en la capa de salida. <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>Una imagen simple de una red neuronal de cuatro capas.</i> <br><br><h3>  Capa de entrada </h3><br>  En la figura, las neuronas moradas representan la entrada.  Pueden ser cantidades escalares simples o m√°s complejas: vectores o matrices multidimensionales. <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>Ecuaci√≥n que describe las entradas xi.</i> <br><br>  El primer conjunto de activaciones (a) es igual a los valores de entrada.  "Activaci√≥n" es el valor de una neurona despu√©s de aplicar la funci√≥n de activaci√≥n.  Ver abajo para m√°s detalles. <br><br><h3>  Capas ocultas </h3><br>  Los valores finales en las neuronas ocultas (en la figura verde) se calculan utilizando entradas ponderadas z <sup>l</sup> en la capa I y activaciones <sup>I</sup> en la capa L. Para las capas 2 y 3, las ecuaciones ser√°n las siguientes: <br><br>  Para l = 2: <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br>  Para l = 3: <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup> y W <sup>3</sup> son los pesos en las capas 2 y 3, y b <sup>2</sup> y b <sup>3</sup> son los desplazamientos en estas capas. <br><br>  Las activaciones a <sup>2</sup> y a <sup>3</sup> se calculan utilizando la funci√≥n de activaci√≥n f.  Por ejemplo, esta funci√≥n f es no lineal (como <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoide</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> y <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">tangente hiperb√≥lica</a> ) y permite a la red estudiar patrones complejos en los datos.  No nos detendremos en c√≥mo funcionan las funciones de activaci√≥n, pero si est√° interesado, le recomiendo leer este maravilloso <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">art√≠culo</a> . <br><br>  Si observa detenidamente, ver√° que todas las x, z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> , a <sup>3</sup> , W <sup>1</sup> , W <sup>2</sup> , b <sup>1</sup> y b <sup>2</sup> no tienen los √≠ndices m√°s bajos que se muestran en la figura de la red neuronal de cuatro capas.  El hecho es que combinamos todos los valores de los par√°metros en matrices agrupadas por capas.  Esta es una forma est√°ndar de trabajar con redes neuronales, y es bastante c√≥moda.  Sin embargo, revisar√© las ecuaciones para que no haya confusi√≥n. <br><br>  Tomemos la capa 2 y sus par√°metros como ejemplo.  Las mismas operaciones se pueden aplicar a cualquier capa de la red neuronal. <br>  W <sup>1</sup> es la matriz de pesos de dimensi√≥n <i>(n, m)</i> , donde <i>n</i> es el n√∫mero de neuronas de salida (neuronas en la capa siguiente) <i>ym</i> es el n√∫mero de neuronas de entrada (neuronas en la capa anterior).  En nuestro caso, <i>n = 2</i> <i>ym = 4</i> . <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br>  Aqu√≠, el primer n√∫mero en el sub√≠ndice de cualquiera de los pesos corresponde al √≠ndice de neurona en la siguiente capa (en nuestro caso, esta es la segunda capa oculta), y el segundo n√∫mero corresponde al √≠ndice de neurona en la capa anterior (en nuestro caso, esta es la capa de entrada). <br><br>  <i>x</i> es el vector de entrada de dimensi√≥n ( <i>m</i> , 1), donde <i>m</i> es el n√∫mero de neuronas de entrada.  En nuestro caso, <i>m</i> = 4. <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup> es el vector de desplazamiento de dimensi√≥n ( <i>n</i> , 1), donde <i>n</i> es el n√∫mero de neuronas en la capa actual.  En nuestro caso, <i>n</i> = 2. <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br>  Siguiendo la ecuaci√≥n para z <sup>2,</sup> podemos usar las definiciones anteriores de W <sup>1</sup> , x y b <sup>1</sup> para obtener la ecuaci√≥n z <sup>2</sup> : <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br>  Ahora mire cuidadosamente la ilustraci√≥n de la red neuronal arriba: <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br>  Como puede ver, z <sup>2</sup> puede expresarse en t√©rminos de z <sub>1</sub> <sup>2</sup> y z <sub>2</sub> <sup>2</sup> , donde z <sub>1</sub> <sup>2</sup> y z <sub>2</sub> <sup>2</sup> son las sumas de los productos de cada valor de entrada x <sup>i</sup> por el peso correspondiente W <sub>ij</sub> <sup>1</sup> . <br><br>  Esto lleva a la misma ecuaci√≥n para z <sup>2</sup> y demuestra que las representaciones matriciales z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> y a <sup>3</sup> son verdaderas. <br><br><h3>  Capa de salida </h3><br>  La √∫ltima parte de la red neuronal es la capa de salida, que proporciona el valor predicho.  En nuestro ejemplo simple, se presenta en forma de una sola neurona te√±ida de azul y se calcula de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br>  Nuevamente, usamos la representaci√≥n matricial para simplificar la ecuaci√≥n.  Puede usar los m√©todos anteriores para comprender la l√≥gica subyacente. <br><br><h3>  Distribuci√≥n directa y evaluaci√≥n </h3><br>  Las ecuaciones anteriores forman una distribuci√≥n directa a trav√©s de la red neuronal.  Aqu√≠ hay un resumen r√°pido: <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>(1) - capa de entrada</i> <i><br></i>  <i>(2) - el valor de la neurona en la primera capa oculta</i> <i><br></i>  <i>(3) - valor de activaci√≥n en la primera capa oculta</i> <i><br></i>  <i>(4) - el valor de la neurona en la segunda capa oculta</i> <i><br></i>  <i>(5) - valor de activaci√≥n en el segundo nivel oculto</i> <i><br></i>  <i>(6) - capa de salida</i> <br><br>  El paso final en el pase directo es evaluar el valor de salida previsto <i>s</i> relativo al valor de salida esperado <i>y</i> . <br><br>  La salida y es parte del conjunto de datos de entrenamiento (x, y), donde <i>x</i> es la entrada (como recordamos de la secci√≥n anterior). <br><br>  La estimaci√≥n entre <i>sy</i> e ocurre a trav√©s de la funci√≥n de p√©rdida.  Puede ser simple como un <a href="https://en.wikipedia.org/wiki/Mean_squared_error">error est√°ndar</a> o m√°s complejo como <a href="http://neuralnetworksanddeeplearning.com/chap3.html">entrop√≠a cruzada</a> . <br><br>  Llamamos a esta funci√≥n de p√©rdida C y la denotamos de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br>  Donde el <i>costo</i> puede ser igual al error est√°ndar, la entrop√≠a cruzada o cualquier otra funci√≥n de p√©rdida. <br><br>  Basado en el valor de C, el modelo "sabe" cu√°nto deben ajustarse sus par√°metros para acercarse al valor de salida esperado de <i>y</i> .  Esto sucede usando el m√©todo de retropropagaci√≥n. <br><br><h3>  Propagaci√≥n inversa del error y c√°lculo de gradientes. </h3><br>  Basado en un art√≠culo de 1989, el m√©todo de retropropagaci√≥n: <br><br>  <i>Ajusta constantemente los pesos de las conexiones en la red para minimizar la medida de la diferencia entre el vector de salida real de la red y el vector de salida deseado</i> . <br>  y <br>  <i>... hace posible crear nuevas funciones √∫tiles que distingan la propagaci√≥n hacia atr√°s de m√©todos anteriores y m√°s simples ...</i> <br><br>  En otras palabras, la propagaci√≥n hacia atr√°s tiene como objetivo minimizar la funci√≥n de p√©rdida ajustando los pesos y las compensaciones de la red.  El grado de ajuste est√° determinado por los gradientes de la funci√≥n de p√©rdida con respecto a estos par√°metros. <br><br>  Surge una pregunta: <i>¬øPor qu√© calcular gradientes</i> ? <br><br>  Para responder a esta pregunta, primero debemos revisar algunos conceptos de computaci√≥n: <br><br>  El gradiente de la funci√≥n C (x <sup>1</sup> , x <sup>2</sup> , ..., x <sup>m</sup> ) en x es el <a href="https://en.wikipedia.org/wiki/Partial_derivative">vector de derivadas parciales de</a> C con <i>respecto</i> a <i>x</i> . <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br>  La derivada de la funci√≥n C refleja la sensibilidad a un cambio en el valor de la funci√≥n (valor de salida) en relaci√≥n con el cambio en su argumento <i>x</i> ( <a href="https://en.wikipedia.org/wiki/Derivative">valor de entrada</a> ).  En otras palabras, la derivada nos dice en qu√© direcci√≥n se mueve C. <br><br>  El gradiente muestra cu√°nto es necesario cambiar el par√°metro <i>x</i> (en la direcci√≥n positiva o negativa) para minimizar C. <br><br>  Estos gradientes se calculan utilizando un m√©todo llamado <a href="https://en.wikipedia.org/wiki/Chain_rule">regla de</a> cadena. <br>  Para un peso (w <sup>jk</sup> ) <sub>l, el</sub> gradiente es: <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>(1) regla de la cadena</i> <i><br></i>  <i>(2) Por definici√≥n, m es el n√∫mero de neuronas por l - 1 capa</i> <i><br></i>  <i>(3) C√°lculo derivado</i> <i><br></i>  <i>(4) valor final</i> <i><br></i>  <i>Se puede aplicar un conjunto similar de ecuaciones a (b <sup>j</sup> ) <sub>l</sub></i> : <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>(1) regla de la cadena</i> <i><br></i>  <i>(2) C√°lculo derivado</i> <i><br></i>  <i>(3) Valor final</i> <br>  La parte com√∫n en ambas ecuaciones a menudo se denomina "gradiente local" y se expresa de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br>  Un "gradiente local" se puede determinar f√°cilmente utilizando una regla de cadena.  No pintar√© este proceso ahora. <br><br>  Los gradientes permiten optimizar los par√°metros del modelo: <br><br>  Hasta que se alcanza el criterio de detenci√≥n, se realiza lo siguiente: <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>Algoritmo para optimizar pesos y compensaciones</i> (tambi√©n llamado descenso de gradiente) <br><ul><li>  Los valores iniciales de <i>w</i> y <i>b</i> se seleccionan al azar. </li><li>  Epsilon (e) es la velocidad de aprendizaje.  Determina el efecto del gradiente. </li><li>  <i>w</i> y <i>b</i> son representaciones matriciales de pesos y compensaciones. </li><li>  La derivada de C con respecto a <i>w</i> o <i>b</i> puede calcularse usando derivadas parciales de C con respecto a pesos o compensaciones individuales. </li><li>  La condici√≥n de terminaci√≥n se cumple tan pronto como se minimiza la funci√≥n de p√©rdida. </li></ul><br><br>  Quiero dedicar la parte final de esta secci√≥n a un ejemplo simple en el que calculamos el gradiente C con respecto a un peso (w <sup>22</sup> ) <sub>2</sub> . <br><br>  Acerqu√©monos al fondo de la red neuronal mencionada anteriormente: <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>Representaci√≥n visual de la retropropagaci√≥n en una red neuronal.</i> <br>  El peso (w <sup>22</sup> ) <sub>2</sub> conecta (a <sup>2</sup> ) <sub>2</sub> y (z <sup>2</sup> ) <sub>2</sub> , por lo que calcular el gradiente requiere aplicar la regla de la cadena en (z <sup>2</sup> ) <sub>3</sub> y (a <sup>2</sup> ) <sub>3</sub> : <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br>  El c√°lculo del valor final de la derivada de C a partir de (a <sup>2</sup> ) <sub>3</sub> requiere el conocimiento de la funci√≥n C. Dado que C depende de (a <sup>2</sup> ) <sub>3</sub> , el c√°lculo de la derivada debe ser simple. <br><br>  Espero que este ejemplo haya logrado arrojar algo de luz sobre las matem√°ticas detr√°s del c√°lculo de gradientes.  Si desea saber m√°s, le recomiendo que consulte la serie de art√≠culos de Stanford NLP, donde Richard Socher proporciona 4 excelentes explicaciones para la propagaci√≥n hacia atr√°s. <br><br><h3>  Comentario final </h3><br>  En este art√≠culo, expliqu√© en detalle c√≥mo la retropropagaci√≥n de un error funciona bajo el cap√≥ utilizando m√©todos matem√°ticos como el c√°lculo de gradientes, la regla de la cadena, etc.  Conocer los mecanismos de este algoritmo fortalecer√° su conocimiento de las redes neuronales y le permitir√° sentirse c√≥modo cuando trabaje con modelos m√°s complejos.  ¬°Buena suerte en tu viaje de aprendizaje profundo! <br><br>  <b><i>Eso es todo.</i></b>  <b><i>Invitamos a todos a un seminario web gratuito sobre el tema <a href="https://otus.pw/h0mh/">"√Årbol de segmentos: simple y r√°pido"</a> .</i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/483466/">https://habr.com/ru/post/483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../483448/index.html">Disney: el mejor bidireccional de la historia humana</a></li>
<li><a href="../483454/index.html">Cambiar de Mercurial a GIT en Atlassian Bitbucket con guardar archivos en cir√≠lico</a></li>
<li><a href="../483458/index.html">Asistente de base de datos GreenPig</a></li>
<li><a href="../483460/index.html">SQL HowTo: construir cadenas usando funciones de ventana</a></li>
<li><a href="../483462/index.html">C√°llate y toma mi dinero</a></li>
<li><a href="../483468/index.html">Pruebas de integraci√≥n de flutter: es f√°cil</a></li>
<li><a href="../483470/index.html">Coloque mosaicos de manera eficiente (Pro CSS, SVG, patr√≥n y m√°s)</a></li>
<li><a href="../483472/index.html">Eliminar todo: c√≥mo borrar datos y restaurar el SSD NVMe a la configuraci√≥n de f√°brica</a></li>
<li><a href="../483476/index.html">La moraleja del transporte rob√≥tico: el problema del carro, los riesgos y las consecuencias</a></li>
<li><a href="../483478/index.html">Sol, viento y agua ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>