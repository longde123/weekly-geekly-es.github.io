<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💜 👃🏼 👆🏿 Wie wir ein schnelles und zuverlässiges Repository für Anzeigenansichten erstellt haben ✌🏿 🧗🏾 😥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eine der unauffälligen, aber wichtigen Funktionen unserer Anzeigenseiten ist das Speichern und Anzeigen der Anzahl ihrer Aufrufe. Unsere Websites verf...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir ein schnelles und zuverlässiges Repository für Anzeigenansichten erstellt haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/kolesa/blog/431902/"> Eine der unauffälligen, aber wichtigen Funktionen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserer Anzeigenseiten</a> ist das Speichern und Anzeigen der Anzahl ihrer Aufrufe.  Unsere Websites verfolgen seit über 10 Jahren Anzeigenaufrufe.  Die technische Implementierung der Funktionalität hat sich in dieser Zeit mehrmals geändert. Jetzt handelt es sich um einen (Mikro-) Dienst für unterwegs, der mit Redis als Cache- und Task-Warteschlange und mit MongoDB als persistentem Speicher arbeitet.  Vor einigen Jahren lernte er, nicht nur mit der Summe der Anzeigenansichten, sondern auch mit Statistiken für jeden Tag zu arbeiten.  Aber das alles hat er erst vor kurzem sehr schnell und zuverlässig gelernt. <br><br><img src="https://habrastorage.org/webt/ee/nn/jp/eennjposwyrztis3a7s6cbkhq2e.png" alt="Bild"><br><br>  Insgesamt verarbeitet der Dienst ~ 300.000 Leseanforderungen und ~ 9.000 Schreibanforderungen pro Minute, von denen 99% bis zu 5 ms ausgeführt werden.  Dies sind natürlich keine astronomischen Indikatoren und kein Raketenstart auf dem Mars - aber auch keine so triviale Aufgabe, wie es eine einfache Speicherung von Zahlen erscheinen mag.  Es stellte sich heraus, dass all dies, um eine verlustfreie Datenspeicherung sicherzustellen und konsistente, relevante Werte zu lesen, einige Anstrengungen erfordert, die wir weiter unten diskutieren werden. <br><a name="habracut"></a><br><h3>  Projektaufgaben und Übersicht </h3><br>  Obwohl Ansichtszähler für das Geschäft nicht so wichtig sind wie beispielsweise die Verarbeitung von Zahlungen oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kreditanträgen</a> , sind sie für unsere Benutzer in erster <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Linie</a> wichtig.  Die Leute sind fasziniert davon, die Popularität ihrer Anzeigen zu verfolgen: Einige rufen sogar den Support an, wenn sie ungenaue Anzeigeinformationen bemerken (dies geschah bei einer der vorherigen Service-Implementierungen).  Darüber hinaus speichern und zeigen wir detaillierte Statistiken in den persönlichen Konten der Benutzer an (um beispielsweise die Effektivität der Nutzung kostenpflichtiger Dienste zu bewerten).  All dies sorgt dafür, dass jedes Anzeigeereignis gespeichert und die relevantesten Werte angezeigt werden. <br><br>  Im Allgemeinen sehen die Funktionalität und die Prinzipien des Projekts folgendermaßen aus: <br><br><ul><li>  Die Webseite oder der Anwendungsbildschirm stellen eine Anfrage hinter den Anzeigenanzeigezählern (die Anfrage ist normalerweise asynchron, um die Ausgabe grundlegender Informationen zu priorisieren).  Wenn die Seite der Anzeige selbst angezeigt wird, fordert der Kunde Sie stattdessen auf, die aktualisierte Anzahl der Aufrufe zu erhöhen und zurückzugeben. </li><li>  Durch die Verarbeitung von Leseanforderungen versucht der Dienst, Informationen aus dem Redis-Cache abzurufen, und ergänzt das Unbekannte, indem er eine Anforderung an MongoDB abschließt. </li><li>  Schreibanforderungen werden an zwei Strukturen im Rettich gesendet: die inkrementelle Aktualisierungswarteschlange (asynchron im Hintergrund verarbeitet) und den Cache der Gesamtzahl der Ansichten. </li><li>  Ein Hintergrundprozess im selben Dienst liest Elemente aus der Warteschlange, sammelt sie im lokalen Puffer und schreibt sie regelmäßig in MongoDB. </li></ul><br><h3>  Record View Counters: Fallstricke </h3><br>  Obwohl die oben beschriebenen Schritte recht einfach aussehen, besteht das Problem hier in der Organisation der Interaktion zwischen der Datenbank und den Microservice-Instanzen, damit die Daten nicht verloren gehen, nicht dupliziert werden und nicht verzögert werden. <br><br>  Die Verwendung nur eines Repositorys (z. B. nur MongoDB) würde einige dieser Probleme lösen.  Tatsächlich funktionierte der Service früher, bis wir auf die Probleme der Skalierung, Stabilität und Geschwindigkeit stießen. <br><br>  Eine naive Implementierung des Verschiebens von Daten zwischen Speichern könnte beispielsweise zu solchen Anomalien führen: <br><br><ul><li>  Datenverlust beim kompetitiven Schreiben in den Cache: <br><ol><li>  Prozess <b>A</b> erhöht die Anzahl der Ansichten im Redis-Cache, stellt jedoch fest, dass für diese Entität noch keine Daten vorhanden sind (es kann sich entweder um eine neue oder eine alte Deklaration handeln, die aus dem Cache extrudiert wurde). Daher muss der Prozess diesen Wert zuerst von MongoDB abrufen. <br></li><li>  Prozess <b>A</b> erhält die Anzahl der Ansichten von MongoDB - zum Beispiel die Nummer 5;  fügt dann 1 hinzu und schreibt an Redis <b>6</b> . </li><li>  Prozess <b>B</b> (beispielsweise von einem anderen Benutzer der Website initiiert, der ebenfalls dieselbe Anzeige eingegeben hat) führt gleichzeitig dasselbe aus. </li><li>  Prozess <b>A</b> schreibt einen Wert von <b>6</b> in Redis. </li><li>  Prozess <b>B</b> schreibt einen Wert von <b>6</b> in Redis. </li><li>  Infolgedessen geht beim Aufzeichnen von Daten eine Ansicht aufgrund des Rennens verloren. <br>  <i>Das Szenario ist nicht so unwahrscheinlich: Wir haben beispielsweise einen kostenpflichtigen Dienst, der eine Anzeige auf der Hauptseite der Website platziert.</i>  <i>Bei einer neuen Ankündigung kann ein solcher Ablauf dazu führen, dass aufgrund ihres plötzlichen Zustroms viele Ansichten gleichzeitig verloren gehen.</i> </li></ol></li><li>  Ein Beispiel für ein anderes Szenario ist der Datenverlust beim Verschieben von Ansichten von Redis nach MongoDb: <br><br><ol><li>  Der Prozess nimmt einen ausstehenden Wert von Redis auf und speichert ihn im Speicher, um später in MongoDB zu schreiben. </li><li>  Eine Schreibanforderung schlägt fehl (oder der Prozess stürzt ab, bevor er ausgeführt wird). </li><li>  Die Daten gehen wieder verloren, was beim nächsten Herausgeben des zwischengespeicherten Werts und Ersetzen durch den Wert aus der Datenbank deutlich wird. </li></ol><br></li></ul><br>  Andere Fehler können auftreten, deren Gründe auch in der nichtatomaren Natur von Operationen zwischen Datenbanken liegen, z. B. ein Konflikt beim Löschen und Erhöhen von Ansichten derselben Entität. <br><br><h3>  Anzahl der Aufzeichnungsansichten: Lösung </h3><br>  Unser Ansatz zum Speichern und Verarbeiten von Daten in diesem Projekt basiert auf der Erwartung, dass MongoDB zu jedem Zeitpunkt wahrscheinlicher ausfällt als Redis.  Dies ist natürlich keine absolute <i>Regel</i> - zumindest nicht für jedes Projekt -, aber in unserer Umgebung sind wir es gewohnt, periodische Zeitüberschreitungen für Abfragen in MongoDB zu beobachten, die durch die Ausführung von Festplattenoperationen verursacht werden, was zuvor einer der Gründe für den Verlust einiger Ereignisse war. <br><br>  Um viele der oben genannten Probleme zu vermeiden, verwenden wir Task-Warteschlangen für verzögertes Speichern und Lua-Skripte, die es ermöglichen, Daten in mehreren Rettichstrukturen gleichzeitig atomar zu ändern.  In diesem Sinne lauten die Details zum Speichern von Ansichten wie folgt: <br><br><ol><li>  Wenn eine Schreibanforderung in den Microservice fällt, wird das Lua-Skript <b>IncrementIfExists ausgeführt</b> , um den Zähler nur dann zu erhöhen, wenn er bereits im Cache vorhanden ist.  Das Skript gibt sofort <b>-1 zurück,</b> wenn keine Daten für die Entität vorhanden sind, die im Rettich angezeigt wird.  Andernfalls wird der Wert der Ansichten im Cache über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HINCRBY erhöht</a> , das Ereignis zur Warteschlange für die nachfolgende Speicherung in MongoDB ( <i>von</i> uns als <i>ausstehende Warteschlange bezeichnet</i> ) über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LPUSH hinzugefügt</a> und die aktualisierte Anzahl von Ansichten zurückgegeben. <br></li><li>  Wenn IncrementIfExists eine positive Zahl zurückgibt, wird dieser Wert an den Client zurückgegeben und die Anforderung endet. <br><br>  Andernfalls nimmt der Microservice den Ansichtszähler von MongoDb auf, erhöht ihn um 1 und sendet ihn an den Rettich. <br></li><li>  Das Schreiben auf den Rettich erfolgt über ein anderes Lua-Skript - <b>Upsert</b> -, das die Gesamtzahl der Ansichten im Cache speichert, wenn dieser noch leer ist, oder um 1 erhöht, wenn jemand anderes es geschafft hat, den Cache zwischen den Schritten 1 und 3 zu füllen. <br></li><li>  Upsert fügt der ausstehenden Warteschlange auch ein Ansichtsereignis hinzu und gibt einen aktualisierten Betrag zurück, der dann an den Client gesendet wird. <br></li></ol><br>  Aufgrund der Tatsache, dass Lua-Skripte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">atomar ausgeführt werden</a> , vermeiden wir viele potenzielle Probleme, die durch ein wettbewerbsfähiges Schreiben verursacht werden könnten. <br><br>  Ein weiteres wichtiges Detail ist die sichere Übertragung von Updates aus der ausstehenden Warteschlange an MongoDB.  Zu diesem Zweck haben wir die in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redis-Dokumentation</a> beschriebene Vorlage "Zuverlässige Warteschlange" verwendet, die das Risiko eines Datenverlusts erheblich verringert, indem eine Kopie der verarbeiteten Elemente in einer separaten, anderen Warteschlange erstellt wird, bis sie schließlich in einem dauerhaften Speicher gespeichert werden. <br><br>  Um die gesamten Prozessschritte besser zu verstehen, haben wir eine kleine Visualisierung vorbereitet.  Schauen wir uns zunächst ein normales, erfolgreiches Szenario an (die Schritte sind in der oberen rechten Ecke nummeriert und werden im Folgenden ausführlich beschrieben): <br><br><img src="https://habrastorage.org/webt/0v/al/bq/0valbqz3kj6du62z0foxfcb6bay.gif" alt="Bild"><br><br><ol><li>  Der Microservice erhält eine Schreibanforderung </li><li>  Der Request-Handler übergibt es an ein Lua-Skript, das die Suche in den Cache schreibt (um sie sofort lesbar zu machen) und zur weiteren Verarbeitung in die Warteschlange. </li><li>  Die Hintergrund-Goroutine führt (periodisch) die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BRPopLPush-</a> Operation aus, bei der ein Element atomar von einer Warteschlange in eine andere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschoben wird</a> (wir nennen es "Verarbeitungswarteschlange" - eine Warteschlange mit aktuell verarbeiteten Elementen).  Das gleiche Element wird dann in einem Puffer im Prozessspeicher gespeichert. <br></li><li>  Eine weitere Schreibanforderung kommt an und wird verarbeitet, sodass 2 Elemente im Puffer und 2 Elemente in der Verarbeitungswarteschlange verbleiben. </li><li>  Nach einiger Zeit entscheidet der Hintergrundprozess, den Puffer in MongoDB zu leeren.  Das Schreiben mehrerer Werte aus dem Puffer wird von einer einzelnen Anforderung ausgeführt, was sich positiv auf den Durchsatz auswirkt.  Außerdem versucht der Prozess vor der Aufzeichnung, mehrere Ansichten zu einer zusammenzufassen und deren Werte für dieselben Anzeigen zusammenzufassen. <br>  <i>In jedem unserer Projekte werden 3 Microservice-Instanzen mit jeweils einem eigenen Puffer verwendet, der alle 2 Sekunden in der Datenbank gespeichert wird.</i>  <i>Während dieser Zeit werden ungefähr 100 Elemente in einem Puffer akkumuliert.</i> <i><br></i> </li><li>  Nach einem erfolgreichen Schreibvorgang entfernt der Prozess Elemente aus der Verarbeitungswarteschlange und signalisiert, dass die Verarbeitung erfolgreich abgeschlossen wurde. <br></li></ol><br>  Wenn alle Subsysteme in Ordnung sind, scheinen einige dieser Schritte redundant zu sein.  Und der aufmerksame Leser hat möglicherweise auch eine Frage dazu, was der Gopher in der unteren linken Ecke tut. <br>  Alles wird erklärt, wenn man das Szenario betrachtet, in dem MongoDB nicht verfügbar ist: <br><br><img src="https://habrastorage.org/webt/hl/jd/bs/hljdbsmwzxn6i2k5xfgj2mm02em.gif" alt="Ein Beispiel für einen Dienst, wenn MongoDB abstürzt"><br><br><ol><li>  Der erste Schritt ist identisch mit den Ereignissen aus dem vorherigen Szenario: Der Dienst empfängt zwei Anforderungen zum Aufzeichnen und Verarbeiten von Ansichten. <br></li><li>  Der Prozess verliert die Verbindung zu MongoDB (der Prozess selbst weiß dies natürlich noch nicht). <br>  Der Gorutin-Handler versucht nach wie vor, seinen Puffer in die Datenbank zu leeren - diesmal jedoch ohne Erfolg.  Sie wartet wieder auf die nächste Iteration. <br></li><li>  Eine andere Hintergrund-Goroutine wacht auf und überprüft die Verarbeitungswarteschlange.  Sie entdeckt, dass die Elemente ihr vor langer Zeit hinzugefügt wurden;  Als sie zu dem Schluss kommt, dass ihre Verarbeitung fehlgeschlagen ist, verschiebt sie sie zurück in die ausstehende Warteschlange. <br></li><li>  Nach einer Weile wird die Verbindung mit MongoDB wiederhergestellt. <br></li><li>  Die erste Hintergrund-Goroutine versucht erneut, eine Schreiboperation auszuführen - diesmal erfolgreich - und entfernt schließlich Elemente dauerhaft aus der Verarbeitungswarteschlange. <br></li></ol><br>  In diesem Schema gibt es mehrere wichtige Zeitüberschreitungen und Heuristiken, die durch Tests und gesunden Menschenverstand abgeleitet wurden: Beispielsweise werden Elemente nach 15 Minuten Inaktivität von der Verarbeitungswarteschlange zurück in die ausstehende Warteschlange verschoben.  Darüber hinaus führt die für diese Aufgabe verantwortliche Goroutine vor der Ausführung eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sperre durch,</a> damit mehrere Instanzen des Mikrodienstes nicht gleichzeitig versuchen, die "eingefrorenen" Ansichten wiederherzustellen. <br><br>  Genau genommen bieten selbst diese Maßnahmen keine theoretisch fundierten Garantien (zum Beispiel ignorieren wir Szenarien wie das Einfrieren des Prozesses für 15 Minuten) - aber in der Praxis funktioniert dies recht zuverlässig. <br><br>  Auch in diesem Schema sind uns mindestens zwei weitere Sicherheitslücken bekannt, deren Kenntnis wichtig ist: <br><br><ul><li>  Wenn der Microservice unmittelbar nach dem erfolgreichen Speichern in MongoDb, jedoch vor dem Löschen der Verarbeitungswarteschlangenliste abgestürzt ist, werden diese Daten als nicht gespeichert betrachtet und nach 15 Minuten erneut gespeichert. <br>  <i>Um die Wahrscheinlichkeit eines solchen Szenarios zu verringern, haben wir wiederholt versucht, bei Fehlern aus der Verarbeitungswarteschlange zu entfernen.</i>  <i>In der Realität haben wir solche Fälle in der Produktion noch nicht beobachtet.</i> <br></li><li>  Beim Neustart kann der Rettich nicht nur den Cache verlieren, sondern auch einige nicht gespeicherte Ansichten aus den Warteschlangen, da er so konfiguriert ist, dass <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RDB-Snapshots</a> regelmäßig alle paar Minuten gespeichert werden. <br>  <i>Obwohl dies theoretisch ein ernstes Problem sein kann (insbesondere wenn sich das Projekt mit wirklich kritischen Daten befasst), werden Knoten in der Praxis äußerst selten neu gestartet.</i>  <i>Gleichzeitig verbringen Elemente laut Überwachung weniger als 3 Sekunden in Warteschlangen, dh die möglichen Verluste sind sehr begrenzt.</i> <br></li></ul><br>  Es scheint, dass es mehr Probleme gibt, als wir möchten.  Tatsächlich stellt sich jedoch heraus, dass das Szenario, gegen das wir uns ursprünglich verteidigt haben - das Scheitern von MongoDB - tatsächlich eine viel realere Bedrohung darstellt und das neue Datenverarbeitungsschema die Verfügbarkeit des Dienstes erfolgreich sicherstellt und Verluste verhindert. <br><br>  Ein anschauliches Beispiel dafür war, als die MongoDB-Instanz bei einem der Projekte die ganze Nacht über absurd nicht verfügbar war.  Während dieser ganzen Zeit sammelten sich die Anzahl der Ansichten und drehten sich im Rettich von einer Warteschlange zur nächsten, bis sie nach Behebung des Vorfalls schließlich in der Datenbank gespeichert wurden.  Die meisten Benutzer haben den Fehler nicht einmal bemerkt. <br><br><h3>  Leseansicht zählt </h3><br>  Leseanforderungen sind viel einfacher als Schreibanforderungen: Der Microservice überprüft zuerst den Cache im Rettich.  Alles, was nicht im Cache gefunden wird, wird mit Daten aus MongoDb gefüllt und an den Client zurückgegeben. <br><br>  Während der Lesevorgänge wird nicht durchgehend in den Cache geschrieben, um den Aufwand für den Schutz vor wettbewerbsfähigen Schreibvorgängen zu vermeiden.  Die Trefferquote des Caches bleibt gut, da er häufig dank anderer Schreibanforderungen bereits aufgewärmt wird. <br><br>  Tägliche Ansichtsstatistiken werden direkt aus MongoDB gelesen, da sie viel seltener angefordert werden und das Zwischenspeichern schwieriger ist.  Dies bedeutet auch, dass das Lesen von Statistiken nicht mehr funktioniert, wenn die Datenbank nicht verfügbar ist.  Es betrifft jedoch nur einen kleinen Teil der Benutzer. <br><br><h3>  MongoDB-Datenspeicherungsschema </h3><br>  Das MongoDB-Erfassungsschema für das Projekt basiert auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Empfehlungen der Datenbankentwickler selbst</a> und sieht folgendermaßen aus: <br><br><ul><li>  Ansichten werden in 2 Sammlungen gespeichert: in einer gibt es ihren Gesamtbetrag, in der anderen - Statistiken nach Tag. <br></li><li>  Die Daten in der Statistiksammlung basieren auf <b>einem Dokument pro Anzeige und Monat</b> .  Bei neuen Ankündigungen wird ein Dokument mit einunddreißig Null für den aktuellen Monat in die Sammlung eingefügt.  Gemäß dem oben genannten Artikel können Sie so sofort genügend Speicherplatz für ein Dokument auf der Festplatte zuweisen, sodass die Datenbank es beim Hinzufügen von Daten nicht verschieben muss. <br>  <i>Dieser Punkt macht das Lesen von Statistiken etwas umständlich (Anfragen müssen auf der Microservice-Seite monatelang generiert werden), aber insgesamt bleibt das Schema recht intuitiv.</i> <br></li><li>  Die <b>Upsert-</b> Operation wird zum Aufzeichnen verwendet, um innerhalb derselben Anforderung ein Dokument für die gewünschte Entität zu aktualisieren und gegebenenfalls zu erstellen. <br></li></ul><br>  Wir verwenden die Transaktionsfunktionen von MongoDb nicht, um mehrere Sammlungen gleichzeitig zu aktualisieren. Dies bedeutet, dass wir das Risiko eingehen, dass die Daten nur in eine Sammlung geschrieben werden können.  Vorerst melden wir uns einfach in solchen Fällen an;  Es gibt nur wenige davon, und dies stellt bislang nicht das gleiche signifikante Problem dar wie andere Szenarien. <br><br><h3>  Testen </h3><br>  Ich würde meinen eigenen Worten nicht vertrauen, dass die beschriebenen Szenarien wirklich funktionieren, wenn sie nicht durch Tests abgedeckt würden. <br><br>  Da der größte Teil des Projektcodes eng mit Radieschen und MongoDb zusammenarbeitet, handelt es sich bei den meisten darin enthaltenen Tests um Integrationstests.  Die Testumgebung wird durch Docker-Compose unterstützt, was bedeutet, dass sie schnell bereitgestellt werden kann, Reproduzierbarkeit durch Zurücksetzen und Wiederherstellen des Status bei jedem Start bietet und das Experimentieren ermöglicht, ohne die Datenbanken anderer Personen zu beeinträchtigen. <br><br>  In diesem Projekt gibt es drei Haupttestbereiche: <br><br><ol><li>  Validierung der Geschäftslogik in typischen Szenarien, den sogenannten  Glückspfad.  Diese Tests beantworten die Frage: Wenn alle Subsysteme in Ordnung sind, funktioniert der Service gemäß den funktionalen Anforderungen? </li><li>  Überprüfen negativer Szenarien, in denen der Dienst voraussichtlich seine Arbeit fortsetzen wird.  Verliert der Dienst beispielsweise wirklich keine Daten, wenn MongoDb abstürzt? <br>  Sind wir sicher, dass die Informationen mit regelmäßigen Zeitüberschreitungen, Einfrierungen und wettbewerbsfähigen Aufzeichnungsvorgängen übereinstimmen? </li><li>  Überprüfen Sie negative Szenarien, in denen wir nicht erwarten, dass der Dienst fortgesetzt wird, aber dennoch ein Mindestmaß an Funktionalität bereitgestellt werden sollte.  Beispielsweise besteht keine Chance, dass der Dienst weiterhin Daten speichert und weitergibt, wenn weder Rettich noch Mongo verfügbar sind. Wir möchten jedoch sicherstellen, dass er in solchen Fällen nicht abstürzt, sondern eine Systemwiederherstellung erwartet und dann wieder funktioniert. </li></ol><br>  Um nach erfolglosen Szenarien zu suchen, arbeitet der Service Business Logic Code mit den Datenbank-Client-Schnittstellen, die in den erforderlichen Tests durch Implementierungen ersetzt werden, die Fehler zurückgeben und / oder Netzwerkverzögerungen simulieren.  Wir simulieren auch den Parallelbetrieb mehrerer Dienstinstanzen mithilfe des Musters " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Umgebungsobjekt</a> ".  Dies ist eine Variante des bekannten "Control Inversion" -Ansatzes, bei dem Funktionen nicht auf die Abhängigkeiten selbst zugreifen, sondern sie über das in den Argumenten übergebene Umgebungsobjekt empfangen.  Mit diesem Ansatz können Sie unter anderem mehrere unabhängige Kopien des Dienstes in einem Test simulieren, von denen jede über einen eigenen Pool von Verbindungen zur Datenbank verfügt und die Produktionsumgebung mehr oder weniger effizient reproduziert.  Einige Tests führen jede dieser Instanzen parallel aus und stellen sicher, dass alle dieselben Daten sehen und keine Rennbedingungen vorliegen. <br><br>  Wir haben auch einen rudimentären, aber immer noch recht nützlichen Stresstest durchgeführt <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Belagerung</a> , die dazu beitrug, die zulässige Last und die Reaktionsgeschwindigkeit des Dienstes grob abzuschätzen. <br><br><h3>  Über die Leistung </h3><br>  Bei 90% der Anfragen ist die Verarbeitungszeit sehr gering und vor allem stabil.  Hier ist ein Beispiel für Messungen an einem der Projekte über mehrere Tage: <br><br><img src="https://habrastorage.org/webt/ln/bk/zy/lnbkzy7-wnykbaelv4vzsd8b53q.png" alt="Bild"><br><br>  Interessanterweise ist ein Datensatz (der eigentlich eine Schreib- + Leseoperation ist, da er aktualisierte Werte zurückgibt) etwas schneller als das Lesen (jedoch nur aus der Sicht eines Clients, der den tatsächlich ausstehenden Schreibvorgang nicht beobachtet). <br>  Eine regelmäßige Zunahme der Verzögerungen am Morgen ist ein Nebeneffekt der Arbeit unseres Analyseteams, das täglich eigene Statistiken auf der Grundlage der Daten des Dienstes sammelt und so eine „künstliche Hochlast“ für uns schafft. <br><br>      :           (          —          MongoDB),       (      ),     : <br><br><img src="https://habrastorage.org/webt/6f/pz/v9/6fpzv9b8lmswdsjhmrn4gk_qugw.png" alt="Bild"><br><br><h3>  Fazit </h3><br> ,  -  , ,   Redis                . <br><br>       , 95%    ,     .      ,                .           5. <br><br>        Go, Redis  MongoDB             .                 ,       .         ,      —      . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de431902/">https://habr.com/ru/post/de431902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de431888/index.html">„Ich denke, Teamideen sind bei der Entwicklung eines Produkts am wichtigsten.“</a></li>
<li><a href="../de431890/index.html">So geben Sie eine Bestellung an der freiberuflichen Börse auf</a></li>
<li><a href="../de431892/index.html">Wir verwenden Veeam Backup & Replication, um neue Systeme und Anwendungen vor dem Upgrade zu testen</a></li>
<li><a href="../de431894/index.html">Im Dezember werden sie über die obligatorische Registrierung von LPWAN-Basisstationen entscheiden</a></li>
<li><a href="../de431898/index.html">Alles dreht sich um Agile - 2: Agile Implementierungsfunktionen</a></li>
<li><a href="../de431904/index.html">Wie wir HR-Spezialisten entladen haben: Informationen zur Ausstellung von Zahlungsbögen</a></li>
<li><a href="../de431906/index.html">PIFR - eine Methode zur Erzeugung einer 3D-Maske, unabhängig vom Drehwinkel des Gesichts</a></li>
<li><a href="../de431908/index.html">Einrichten der Tinkoff Bank API. Wie ist deine Intuition ....? Oder ein Lied über Oauth 2.0</a></li>
<li><a href="../de431910/index.html">PSEFABRIC - ein neuer Ansatz für Netzwerkmanagement und -automatisierung. Schritt zum Ideal</a></li>
<li><a href="../de431912/index.html">Der größte Bot-No wurde in den USA festgenommen: Was bedeutet das für die digitale Community?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>