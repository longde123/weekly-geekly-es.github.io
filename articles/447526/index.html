<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòó üë©üèæ‚Äçüíº ‚ùé Bicicleta propia para sincronizar MariaDB y Sphinx üßëüèø üî† üö§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El 28 de febrero, hice una presentaci√≥n en el encuentro SphinxSearch , que se realiz√≥ en nuestra oficina. Habl√≥ sobre c√≥mo procedimos de la reconstruc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bicicleta propia para sincronizar MariaDB y Sphinx</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/superjob/blog/447526/"><p><img src="https://habrastorage.org/webt/t0/1g/vk/t01gvkcn0zx47xuioqcvfz5bqoc.png"></p><br><p>  El 28 de febrero, hice una presentaci√≥n en el encuentro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SphinxSearch</a> , que se realiz√≥ en nuestra oficina.  Habl√≥ sobre c√≥mo procedimos de la reconstrucci√≥n regular de √≠ndices para la b√∫squeda de texto completo y el env√≠o de actualizaciones en el c√≥digo "en su lugar" a los √≠ndices de tiempo ferroviario y la sincronizaci√≥n autom√°tica del estado del √≠ndice y la base de datos MariaDB.  Una grabaci√≥n de video de mi informe est√° disponible a trav√©s del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> , y para aquellos que prefieren leer a ver el video, escrib√≠ este art√≠culo. </p><a name="habracut"></a><br><p>  Comenzar√© con c√≥mo se organiz√≥ nuestra b√∫squeda y por qu√© comenzamos todo esto. </p><br><p>  Nuestra b√∫squeda se organiz√≥ de acuerdo con un esquema completamente est√°ndar. </p><br><p>  Desde el principio, las solicitudes de los usuarios llegan al servidor de aplicaciones escrito en PHP, y √©l, a su vez, se comunica con la base de datos (tenemos MariaDB).  Si necesitamos hacer una b√∫squeda, el servidor de aplicaciones recurre al equilibrador (tenemos haproxy), que lo conecta a uno de los servidores donde se est√° ejecutando la b√∫squeda, y ese servidor ya realiza una b√∫squeda y devuelve el resultado. </p><br><p>  Los datos de la base de datos caen en el √≠ndice de una manera bastante tradicional: de acuerdo con el cronograma, reconstruimos el √≠ndice cada pocos minutos con aquellos documentos que se actualizaron relativamente recientemente, y reconstruimos el √≠ndice con los llamados documentos "archivados" (es decir, aquellos con los que Durante mucho tiempo no pas√≥ nada).  Hay un par de m√°quinas asignadas para la indexaci√≥n, se ejecuta un script all√≠ en una programaci√≥n, que primero construye el √≠ndice, luego cambia el nombre de los archivos de √≠ndice de una manera especial y luego lo coloca en una carpeta separada.  Y en cada uno de los servidores con b√∫squeda, rsync se inicia una vez por minuto, lo que copia archivos de esta carpeta a la carpeta de √≠ndices buscados y luego, si se ha copiado algo, ejecuta una solicitud RELOAD INDEX. </p><br><p>  Sin embargo, para algunos cambios en curr√≠culums vitae y vacantes se requer√≠a que "alcanzaran" el √≠ndice lo antes posible.  Por ejemplo, si una vacante publicada en el dominio p√∫blico se elimina de la publicaci√≥n, entonces es razonable esperar desde el punto de vista del usuario que desaparecer√° del problema en unos segundos, no m√°s.  Por lo tanto, este tipo de cambios se env√≠an directamente a trav√©s de b√∫squedas mediante consultas ACTUALIZAR.  Y para que estos cambios se apliquen a todas las copias de √≠ndices en todos nuestros servidores, se configura un √≠ndice distribuido en cada b√∫squeda, que env√≠a actualizaciones de atributos a todas las instancias buscadas.  El servidor de aplicaciones a√∫n se conecta al equilibrador y env√≠a una solicitud para actualizar el √≠ndice distribuido;  por lo tanto, no necesita saber de antemano ni la lista de servidores con b√∫squeda, ni llegar√° a qu√© servidor con b√∫squeda exactamente. </p><br><p>  Todo esto funcion√≥ bastante bien, pero hubo problemas. </p><br><ol><li>  El retraso promedio entre la creaci√≥n del documento (tenemos este curr√≠culum o vacante) y su entrada en el √≠ndice fue directamente proporcional a su n√∫mero en nuestra base de datos. </li><li> Como utilizamos el √≠ndice distribuido para distribuir actualizaciones de atributos, no ten√≠amos garant√≠a de que estas actualizaciones se aplicaran a todas las copias del √≠ndice. </li><li> Los cambios "urgentes" que ocurrieron durante la reconstrucci√≥n del √≠ndice se perdieron cuando se ejecut√≥ el comando <code>RELOAD INDEX</code> (simplemente porque todav√≠a no estaban en el √≠ndice reci√©n construido), y solo entraron en el √≠ndice despu√©s de la pr√≥xima reindexaci√≥n. <img src="https://habrastorage.org/webt/rz/t6/v3/rzt6v3lfrnyayc3-texs56vlh48.png"></li><li>  Los scripts para actualizar √≠ndices en servidores con b√∫squedas se ejecutaron independientemente uno del otro, no hubo sincronizaci√≥n entre ellos.  Debido a esto, el retraso entre la actualizaci√≥n del √≠ndice en diferentes servidores podr√≠a llegar a varios minutos. </li><li>  Si era necesario probar algo relacionado con la b√∫squeda, era necesario reconstruir el √≠ndice despu√©s de cada cambio. </li></ol><br><p>  Cada uno de estos problemas por separado no val√≠a una reelaboraci√≥n cardinal de la infraestructura de b√∫squeda, pero en conjunto arruinaron la vida de manera tangible. </p><br><p>  Decidimos lidiar con los problemas anteriores utilizando los √≠ndices en tiempo real de Sphinx.  Adem√°s, la transici√≥n a los √≠ndices RT no fue suficiente para nosotros.  Para finalmente deshacerse de cualquier carrera de datos, era necesario asegurarse de que todas las actualizaciones de la aplicaci√≥n al √≠ndice pasaran por el mismo canal.  Adem√°s, era necesario guardar en alg√∫n lugar los cambios realizados en la base de datos mientras se reconstru√≠a el √≠ndice (porque despu√©s de todo, a veces hay que reconstruirlo, pero el procedimiento no es instant√°neo). </p><br><p>  Decidimos hacer la conexi√≥n utilizando el protocolo de replicaci√≥n MySQL, como un canal de transferencia de datos, y el binlog de MySQL es el lugar para guardar los cambios mientras se reconstruye el √≠ndice.  Esta soluci√≥n nos permiti√≥ deshacernos de escribir en Sphinx desde el c√≥digo de la aplicaci√≥n.  Y dado que ya hab√≠amos usado la replicaci√≥n basada en filas con una identificaci√≥n de transacci√≥n global para ese momento, el cambio entre las r√©plicas de la base de datos podr√≠a hacerse de manera bastante simple. </p><br><p>  La idea de conectarse directamente a la base de datos para obtener cambios desde all√≠ para enviarlos al √≠ndice, por supuesto, no es nueva: en 2016, los colegas de Avito <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hicieron una presentaci√≥n</a> donde describieron en detalle c√≥mo resolvieron el problema de sincronizar datos en Sphinx con la base de datos principal.  Decidimos usar su experiencia y crear un sistema similar para nosotros, con la diferencia de que no tenemos PostgreSQL, sino MariaDB y la antigua rama Sphinx (es decir, la versi√≥n 2.3.2). </p><br><p>  Hicimos un servicio que se suscribe a los cambios en MariaDB y actualiza el √≠ndice en Sphinx.  Sus responsabilidades son las siguientes: </p><br><ul><li>  conexi√≥n al servidor MariaDB a trav√©s del protocolo de replicaci√≥n y recepci√≥n de eventos desde el binlog; </li><li>  rastrear la posici√≥n actual de binlog y el n√∫mero de la √∫ltima transacci√≥n completada; </li><li>  filtrado de eventos binlog; </li><li>  averiguar qu√© documentos deben agregarse, eliminarse o actualizarse en el √≠ndice, y para documentos actualizados, qu√© campos deben actualizarse; </li><li>  solicitud de datos faltantes de MariaDB; </li><li>  generaci√≥n y ejecuci√≥n de solicitudes de actualizaci√≥n de √≠ndice; </li><li>  reconstruir el √≠ndice si es necesario. </li></ul><br><p>  Hicimos una conexi√≥n usando el protocolo de replicaci√≥n usando la biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">go-mysql</a> .  Ella es responsable de establecer una conexi√≥n con MariaDB, leer los eventos de replicaci√≥n y pasarlos a un controlador.  Este controlador comienza en goroutine, que es controlado por la biblioteca, pero nosotros mismos escribimos el c√≥digo del controlador.  En el c√≥digo del controlador, los eventos se verifican con una lista de tablas que nos interesan y los cambios en estas tablas se env√≠an para su procesamiento.  Nuestro controlador tambi√©n almacena el estado de la transacci√≥n.  Esto se debe al hecho de que los eventos en el protocolo de replicaci√≥n est√°n en orden: GTID (inicio de la transacci√≥n) -&gt; ROW (cambio de datos) -&gt; XID (final de la transacci√≥n), y solo el primero de ellos contiene informaci√≥n sobre el n√∫mero de transacci√≥n.  Es m√°s conveniente para nosotros transferir el n√∫mero de transacci√≥n junto con su finalizaci√≥n para guardar informaci√≥n sobre a qu√© posici√≥n en el binlog se han aplicado los cambios, y para esto necesitamos recordar el n√∫mero de la transacci√≥n actual entre su inicio y finalizaci√≥n. </p><br><pre> <code class="plaintext hljs">MySQL [(none)]&gt; describe sync_state; +-----------------+--------+ | Field | Type | +-----------------+--------+ | id | bigint | | dummy_field | field | | binlog_position | uint | | binlog_name | string | | gtid | string | | flavor | string | +-----------------+--------+</code> </pre> <br><p>  Guardamos el n√∫mero de la √∫ltima transacci√≥n completada en un √≠ndice especial de un documento en cada servidor con b√∫squeda.  Al inicio del servicio, verificamos que los √≠ndices se inicializan y tienen la estructura esperada, as√≠ como que la posici√≥n guardada en todos los servidores est√° presente y es la misma en todos los servidores.  Luego, si estas comprobaciones fueron exitosas y pudimos comenzar a leer el binlog desde la posici√≥n guardada, comenzamos el procedimiento de sincronizaci√≥n.  Si las comprobaciones fallan, o no fue posible comenzar a leer el binlog desde la posici√≥n guardada, restablecemos la posici√≥n guardada a la posici√≥n actual del servidor MariaDB y reconstruimos el √≠ndice. </p><br><p>  El procesamiento de eventos de replicaci√≥n comienza determinando qu√© documentos se ven afectados por un cambio particular en la base de datos.  Para hacer esto, en la configuraci√≥n de nuestro servicio, hicimos algo como enrutar los eventos de cambio de fila en las tablas que nos interesan, es decir, un conjunto de reglas para determinar c√≥mo deber√≠an indexarse ‚Äã‚Äãlos cambios en la base de datos. </p><br><pre> <code class="plaintext hljs">[[ingest]] table = "vacancy" id_field = "id" index = "vacancy" [ingest.column_map] user_id = ["user_id"] edited_at = ["date_edited"] profession = ["profession"] latitude = ["latitude_deg", "latitude_rad"] longitude = ["longitude_deg", "longitude_rad"] [[ingest]] table = "vacancy_language" id_field = "vacancy_id" index = "vacancy" [ingest.column_map] language_id = ["languages"] level = ["languages"] [[ingest]] table = "vacancy_metro_station" id_field = "vacancy_id" index = "vacancy" [ingest.column_map] metro_station_id = ["metro"]</code> </pre> <br><p>  Por ejemplo, con este conjunto de reglas, los cambios en las <code>vacancy_metro_station</code> <code>vacancy</code> , <code>vacancy_language</code> y <code>vacancy_metro_station</code> deben estar en el √≠ndice de <code>vacancy</code> .  El n√∫mero de documento puede tomarse en el campo <code>id</code> para la tabla de <code>vacancy</code> y en el campo <code>vacancy_id</code> para las otras dos tablas.  El campo <code>column_map</code> es una tabla de la dependencia de los campos de √≠ndice en los campos de diferentes tablas de bases de datos. </p><br><p>  Adem√°s, cuando recibimos la lista de documentos afectados por los cambios, necesitamos actualizarlos en el √≠ndice, pero no lo hacemos de inmediato.  Primero, acumulamos cambios para cada documento y enviamos los cambios al √≠ndice tan pronto como pase un breve per√≠odo de tiempo (tenemos 100 milisegundos) desde el √∫ltimo cambio de este documento. </p><br><p>  Decidimos hacer esto para evitar muchas actualizaciones de √≠ndice innecesarias, porque en muchos casos se produce un solo cambio l√≥gico en un documento con la ayuda de varias consultas SQL que afectan a diferentes tablas y, a veces, se ejecutan en transacciones completamente diferentes. </p><br><p>  Dar√© un ejemplo simple.  Supongamos que un usuario ha editado una vacante.  El c√≥digo responsable de guardar los cambios a menudo se escribe por simplicidad aproximadamente de esta manera: </p><br><pre> <code class="plaintext hljs">BEGIN; UPDATE vacancy SET edited_at = NOW() WHERE id = 123; DELETE FROM vacancy_language WHERE vacancy_id = 123; INSERT INTO vacancy_language (vacancy_id, language_id, level) VALUES (123, 1, "fluent"), (123, 2, "technical"); DELETE FROM vacancy_metro_station WHERE vacancy_id = 123; INSERT INTO vacancy_metro_station (vacancy_id, metro_station_id) VALUES (123, 55); ... COMMIT;</code> </pre> <br><p>  En otras palabras, primero todos los registros antiguos se eliminan de las tablas vinculadas y luego se insertan los nuevos.  Al mismo tiempo, seguir√° habiendo entradas en el binlog sobre estas eliminaciones e inserciones, incluso si nada ha cambiado en el documento. </p><br><p>  Para actualizar solo lo que se necesita, hicimos lo siguiente: clasificamos las l√≠neas cambiadas para que para cada par √≠ndice-documento todos los cambios puedan recuperarse en orden cronol√≥gico.  Luego, podemos aplicarlos a su vez para determinar qu√© campos en qu√© tablas han cambiado finalmente y cu√°les no, y luego usar la tabla <code>column_map</code> obtener una lista de campos y atributos de √≠ndice que deben actualizarse para cada documento afectado.  Adem√°s, los eventos relacionados con un documento pueden no llegar uno tras otro, sino como "de manera diferente" si se ejecutan en diferentes transacciones.  Pero, en nuestra capacidad de determinar qu√© documentos han cambiado, esto no afectar√°. </p><br><p>  Al mismo tiempo, este enfoque nos permiti√≥ actualizar solo los atributos del √≠ndice, si no hubo cambios en los campos de texto, as√≠ como combinar el env√≠o de cambios a Sphinx. </p><br><p>  Entonces, ahora podemos averiguar qu√© documentos deben actualizarse en el √≠ndice. </p><br><p>  En muchos casos, los datos del binlog no son suficientes para generar una solicitud para actualizar el √≠ndice, por lo que obtenemos los datos faltantes del mismo servidor desde donde leemos el binlog.  Para esto, hay una plantilla de solicitud para recibir datos en la configuraci√≥n de nuestro servicio. </p><br><pre> <code class="plaintext hljs">[data_source.vacancy] #               #   -      id     parts = 4 query = """ SELECT vacancy.id AS `:id`, vacancy.profession AS `profession_text:field`, GROUP_CONCAT(DISTINCT vacancy_language.language_id) AS `languages:attr_multi`, GROUP_CONCAT(DISTINCT vacancy_metro_station.metro_station_id) AS `metro:attr_multi` FROM vacancy LEFT JOIN vacancy_language ON vacancy_language.vacancy_id = vacancy.id LEFT JOIN vacancy_metro_station ON vacancy_metro_station.vacancy_id = vacancy.id GROUP BY vacancy.id """</code> </pre> <br><p>  En esta plantilla, todos los campos est√°n marcados con alias especiales: <code>[___]:___</code> . <br>  Se usa tanto en la formaci√≥n de una solicitud para recibir los datos faltantes como en la construcci√≥n del √≠ndice (m√°s sobre esto m√°s adelante). </p><br><p>  Formamos una solicitud de este tipo: </p><br><pre> <code class="plaintext hljs">SELECT vacancy.id AS `id`, vacancy.profession AS `profession_text`, GROUP_CONCAT(DISTINCT vacancy_language.language_id) AS `languages`, GROUP_CONCAT(DISTINCT vacancy_metro_station.metro_station_id) AS `metro` FROM vacancy LEFT JOIN vacancy_language ON vacancy_language.vacancy_id = vacancy.id LEFT JOIN vacancy_metro_station ON vacancy_metro_station.vacancy_id = vacancy.id WHERE vacancy.id IN (&lt; id ,   &gt;) GROUP BY vacancy.id</code> </pre> <br><p>  Luego, para cada documento, verificamos si es el resultado de esta solicitud.  Si no, significa que se elimin√≥ de la tabla principal, por lo que tambi√©n se puede eliminar del √≠ndice (ejecutamos la consulta <code>DELETE</code> para este documento).  Si es as√≠, vea si necesitamos actualizar los campos de texto para este documento.  Si los campos de texto no necesitan actualizarse, entonces hacemos una consulta de <code>UPDATE</code> para este documento, de lo contrario, <code>REPLACE</code> . </p><br><p>  Vale la pena se√±alar aqu√≠ que la l√≥gica de mantener la posici√≥n desde la cual puede comenzar a leer el binlog en caso de fallas ten√≠a que ser complicada, porque ahora es posible una situaci√≥n en la que no aplicamos todos los cambios le√≠dos del binlog. </p><br><p>  Para que la reanudaci√≥n de la lectura del binlog funcione correctamente, hicimos lo siguiente: para cada evento de cambio de fila en la base de datos, recuerde la identificaci√≥n de la √∫ltima transacci√≥n completada en el momento en que ocurri√≥ este evento.  Despu√©s de enviar los cambios a Sphinx, actualizamos el n√∫mero de transacci√≥n desde el que puede comenzar a leer de forma segura, de la siguiente manera.  Si no procesamos todos los cambios acumulados (porque algunos documentos no fueron "rastreados" en la cola), tomamos el n√∫mero de la transacci√≥n m√°s temprana de aquellos relacionados con los cambios que a√∫n no hemos podido aplicar.  Y si sucedi√≥ que aplicamos todos los cambios acumulados, simplemente tomamos el n√∫mero de la √∫ltima transacci√≥n completada. </p><br><p>  Lo que sucedi√≥ como resultado estuvo bien para nosotros, pero hab√≠a un punto m√°s importante: para que el rendimiento del √≠ndice en tiempo real se mantuviera en un nivel aceptable a lo largo del tiempo, era necesario que el tama√±o y el n√∫mero de "fragmentos" de este √≠ndice siguieran siendo peque√±os.  Para hacer esto, Sphinx tiene una solicitud <code>FLUSH RAMCHUNK</code> , que <code>FLUSH RAMCHUNK</code> un nuevo fragmento de disco, y una solicitud <code>OPTIMIZE INDEX</code> , que combina todos los fragmentos de disco en uno.  Inicialmente, pensamos que lo realizar√≠amos peri√≥dicamente y eso es todo.  Pero, desafortunadamente, result√≥ que en la versi√≥n 2.3.2 <code>OPTIMIZE INDEX</code> no funciona (es decir, con una probabilidad bastante alta conduce a una ca√≠da en la b√∫squeda).  Por lo tanto, decidimos simplemente reconstruir completamente el √≠ndice una vez al d√≠a, especialmente porque de vez en cuando todav√≠a tenemos que hacerlo (por ejemplo, si el esquema del √≠ndice o la configuraci√≥n del tokenizer cambian). </p><br><p>  El procedimiento para reconstruir el √≠ndice tiene lugar en varias etapas. </p><br><ol><li><p>  Generamos una configuraci√≥n para indexador </p><br><p>  Como se mencion√≥ anteriormente, hay una plantilla de consulta SQL en la configuraci√≥n del servicio.  Tambi√©n se usa para formar la configuraci√≥n del indexador. <br>  Tambi√©n en la configuraci√≥n hay otras configuraciones necesarias para construir el √≠ndice (configuraciones de tokenizer, diccionarios, varias restricciones en el consumo de recursos). </p><br></li><li><p>  Guardar la posici√≥n actual de MariaDB </p><br><p>  Desde esta posici√≥n, comenzaremos a leer el binlog, despu√©s de que el nuevo √≠ndice est√© disponible en todos los servidores con b√∫squeda. </p><br></li><li><p>  Empezamos indexador </p><br><p>  <code>indexer --config tmp.vacancy.indexer.0.conf --all</code> comandos del <code>indexer --config tmp.vacancy.indexer.0.conf --all</code> y esperamos su finalizaci√≥n.  Adem√°s, si el √≠ndice se divide en partes, entonces comenzamos la construcci√≥n de todas las partes en paralelo. </p><br></li><li><p>  Cargamos archivos de √≠ndice en servidores </p><br><p>  La descarga a cada servidor tambi√©n ocurre en paralelo, pero naturalmente esperamos hasta que todos los archivos se carguen en todos los servidores.  Para descargar archivos en la configuraci√≥n del servicio, hay una secci√≥n con una plantilla de comando para descargar archivos. </p><br><pre> <code class="plaintext hljs">[index_uploader] executable = "rsync" arguments = [ "--files-from=-", "--log-file=&lt;&lt;.DataDir&gt;&gt;/rsync.&lt;&lt;.Host&gt;&gt;.log", "--no-relative", "--times", "--delay-updates", ".", "rsync://&lt;&lt;.Host&gt;&gt;/index/vacancy/", ]</code> </pre> <br><p>  Para cada servidor, simplemente sustituimos su nombre en la variable Host y ejecutamos el comando resultante.  Usamos rsync para la descarga, pero en principio cualquier programa o script que acepte una lista de archivos en stdin y descargue estos archivos a la carpeta donde searchd espera ver los archivos de √≠ndice. </p><br></li><li><p>  Paramos la sincronizaci√≥n </p><br><p>  Dejamos de leer el binlog, detuvimos a los gorutinos responsables de la acumulaci√≥n de cambios. </p><br></li><li><p>  Reemplace el √≠ndice antiguo por uno nuevo </p><br><p>  Para cada servidor con b√∫squeda, realizamos consultas secuenciales <code>RELOAD INDEX vacancy_plain</code> , <code>TRUNCATE INDEX vacancy_plain</code> , <code>ATTACH INDEX vacancy_plain TO vacancy</code> .  Si el √≠ndice se divide en partes, entonces ejecutamos estas consultas para cada parte secuencialmente.  Al mismo tiempo, si estamos en un entorno de producci√≥n, antes de ejecutar estas consultas en cualquier servidor, eliminamos la carga a trav√©s del equilibrador (para que nadie haga consultas SELECT a los √≠ndices entre <code>TRUNCATE</code> y <code>ATTACH</code> ), y tan pronto una vez completada la √∫ltima solicitud <code>ATTACH</code> , devolvemos la carga a este servidor. </p><br></li><li><p>  Reanudando la sincronizaci√≥n desde una posici√≥n guardada </p><br><p>  Tan pronto como reemplazamos todos los √≠ndices en tiempo real con los nuevos, reanudamos la lectura del binlog y sincronizamos los eventos del binlog, comenzando desde la posici√≥n que guardamos antes de que comenzara la indexaci√≥n. </p><br></li></ol><br><p>  Aqu√≠ hay un ejemplo de un gr√°fico del retraso del √≠ndice del servidor MariaDB. </p><br><p><img src="https://habrastorage.org/webt/xs/pq/56/xspq56osyygn1fxx6h5x_oczgpy.png" alt="Retraso despu√©s de reindexar"></p><br><p>  Aqu√≠ puede ver que, aunque el estado del √≠ndice despu√©s de la reconstrucci√≥n vuelve a tiempo, esto sucede muy brevemente. </p><br><p>  Ahora que todo est√° m√°s o menos listo, es hora de su lanzamiento.  Lo hicimos gradualmente.  Primero, vertimos un √≠ndice en tiempo real en un par de servidores, y el resto en ese momento funcion√≥ de la misma manera.  Al mismo tiempo, la estructura de los √≠ndices en los servidores "nuevos" no difer√≠a de los antiguos, por lo que nuestra aplicaci√≥n PHP a√∫n pod√≠a conectarse al equilibrador sin preocuparse de si la solicitud se procesar√≠a en un √≠ndice en tiempo real o en un √≠ndice simple. </p><br><p><img src="https://habrastorage.org/webt/sy/xz/lx/syxzlx_tfmg0-mze5vr1ngt3_tg.png" alt="Esquema de distribuci√≥n de actualizaci√≥n de transici√≥n"></p><br><p>  Las actualizaciones de atributos, de las que habl√© antes, tambi√©n se enviaron de acuerdo con el esquema anterior, con la diferencia de que el √≠ndice distribuido en todos los servidores estaba configurado para enviar consultas de ACTUALIZACI√ìN solo a servidores con √≠ndices simples.  Adem√°s, si la solicitud UPDATE de la aplicaci√≥n llega al servidor con √≠ndices en tiempo real, entonces no ejecuta esta solicitud en el hogar, sino que la env√≠a a los servidores configurados de la manera anterior. </p><br><p>  Despu√©s de la publicaci√≥n, como esper√°bamos, result√≥ reducir significativamente la demora entre c√≥mo un curr√≠culum o una vacante cambia en la base de datos y c√≥mo los cambios correspondientes entran en el √≠ndice. </p><br><p>  Despu√©s de cambiar a un √≠ndice en tiempo real, no hab√≠a necesidad de reconstruir el √≠ndice despu√©s de cada cambio en los servidores de prueba.  Y as√≠ fue posible escribir autotests de extremo a extremo con la participaci√≥n de b√∫squeda de manera relativamente econ√≥mica.  Sin embargo, dado que procesamos los cambios del binlog de forma as√≠ncrona (desde el punto de vista de los clientes que escriben en la base de datos), tuvimos que hacer posible esperar hasta que nuestro servicio procesara los cambios relacionados con el documento que participaba en la prueba autom√°tica y los enviara a la b√∫squeda . </p><br><p>  Para hacer esto, creamos un punto final en nuestro servicio, que hace exactamente eso, es decir, espera hasta que se apliquen todos los cambios al n√∫mero de transacci√≥n especificado.  Para hacer esto, inmediatamente despu√©s de realizar los cambios necesarios en la base de datos, solicitamos a MariaDB <code>@@gtid_current_pos</code> y la transferimos al punto final de nuestro servicio.  Si ya hemos aplicado todas las transacciones a esta posici√≥n en este momento, el servicio responde de inmediato que podemos continuar.  De lo contrario, en la rutina que es responsable de aplicar los cambios, creamos una suscripci√≥n a este GTID, y tan pronto como se aplique (o cualquiera que lo siga), tambi√©n permitimos que el cliente contin√∫e con la prueba autom√°tica. </p><br><p>  En el c√≥digo PHP, se ve as√≠: </p><br><pre> <code class="plaintext hljs">&lt;?php declare(strict_types=1); use GuzzleHttp\ClientInterface; use GuzzleHttp\RequestOptions; use PDO; class RiverClient { private const REQUEST_METHOD = 'post'; /** * @var ClientInterface */ private $httpClient; public function __construct(ClientInterface $httpClient) { $this-&gt;httpClient = $httpClient; } public function waitForSync(PDO $mysqlConnection, PDO $sphinxConnection, string $riverAddr): void { $masterGTID = $mysqlConnection-&gt;query('SELECT @@gtid_current_pos')-&gt;fetchColumn(); $this-&gt;httpClient-&gt;request( self::REQUEST_METHOD, "http://{$riverAddr}/wait", [RequestOptions::FORM_PARAMS =&gt; ['gtid' =&gt; $masterGTID]] ); } }</code> </pre> <br><h2 id="rezultaty">  Resultados </h2><br><p>  Como resultado, pudimos reducir significativamente el retraso entre la actualizaci√≥n de MariaDB y Sphinx. </p><br><p><img src="https://habrastorage.org/webt/lc/rs/rl/lcrsrlzpcw8bzhuptg5s42p6wou.png" alt="Retraso de √≠ndice simple de la base de datos"></p><br><p><img src="https://habrastorage.org/webt/7h/ik/ic/7hikichzuaqyszagbenen-9drhk.png" alt="Rt-index retraso de la base de datos"></p><br><p>  Tambi√©n nos hemos vuelto mucho m√°s seguros de que todas las actualizaciones lleguen a tiempo a todos nuestros servidores Sphinx. </p><br><p>  Adem√°s, las pruebas de b√∫squeda (tanto manuales como autom√°ticas) se han vuelto mucho m√°s agradables. </p><br><p>  Desafortunadamente, esto no se nos dio de forma gratuita: el rendimiento del √≠ndice en tiempo real en comparaci√≥n con el √≠ndice simple result√≥ ser ligeramente peor. </p><br><p>  La distribuci√≥n del tiempo de procesamiento de las consultas de b√∫squeda seg√∫n el tiempo para un √≠ndice simple se muestra a continuaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/op/ro/gm/oprogmvvdykt244nlbmzeldufhu.png" alt="Cronolog√≠a de ejecuci√≥n de consultas - simple"></p><br><p>  Y aqu√≠ est√° el mismo gr√°fico para el √≠ndice en tiempo real. </p><br><p><img src="https://habrastorage.org/webt/07/ii/ce/07iicewkxbb0qvsrrob6dbwoa2i.png" alt="Cronolog√≠a de ejecuci√≥n de consultas: en tiempo real"></p><br><p>  Puede ver que el porcentaje de solicitudes "r√°pidas" ha disminuido ligeramente, mientras que el porcentaje de solicitudes "lentas" ha aumentado. </p><br><h2 id="vmesto-zaklyucheniya">  En lugar de una conclusi√≥n </h2><br><p>  Queda por decir que el c√≥digo del servicio descrito en este art√≠culo, lo publicamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el dominio p√∫blico</a> .  Desafortunadamente, todav√≠a no hay documentaci√≥n detallada, pero si lo desea, puede ejecutar un ejemplo de uso de este servicio a trav√©s de <code>docker-compose</code> . </p><br><h2 id="ssylki">  Referencias </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Video</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">diapositivas de</a> informes </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Informe en video de Andrey Smirnov y Vyacheslav Kryukov en Highload ++</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Biblioteca go-mysql</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥digo de servicio con ejemplo de uso</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/447526/">https://habr.com/ru/post/447526/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447512/index.html">¬øY qui√©n hizo esto? Automatizar la auditor√≠a de seguridad de la informaci√≥n.</a></li>
<li><a href="../447514/index.html">7 startups interesantes en IoT</a></li>
<li><a href="../447516/index.html">C√≥mo overclockeamos CAD COMPASS-3D ‚Üí Parte 2</a></li>
<li><a href="../447520/index.html">Funciones de nivelaci√≥n autom√°tica en el almacenamiento Qsan XCubeSAN</a></li>
<li><a href="../447522/index.html">Qu√© cosas √∫tiles se pueden extraer de los registros de una estaci√≥n de trabajo basada en Windows</a></li>
<li><a href="../447528/index.html">¬øQui√©n es responsable de la calidad?</a></li>
<li><a href="../447530/index.html">OceanLotus: actualizaci√≥n de Malvari para macOS</a></li>
<li><a href="../447532/index.html">Splunk Universal Forwarder en el Docker como recopilador de registros del sistema</a></li>
<li><a href="../447534/index.html">El cosmonauta Aleksandr Laveykin sobre la mejor pel√≠cula espacial, fuerza G de 20 g y aterrizaje suave</a></li>
<li><a href="../447536/index.html">Implementar IdM. Preparaci√≥n para la implementaci√≥n por parte del cliente.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>