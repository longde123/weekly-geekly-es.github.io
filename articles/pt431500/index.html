<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïµÔ∏è üöß ‚òÄÔ∏è Bancos de dados e Kubernetes (revis√£o e reportagem em v√≠deo) üõéÔ∏è üëãüèº üè†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 8 de novembro, no sal√£o principal da confer√™ncia HighLoad ++ 2018 , no √¢mbito da se√ß√£o DevOps and Operations, foi elaborado um relat√≥rio intitulado...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bancos de dados e Kubernetes (revis√£o e reportagem em v√≠deo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/431500/">  Em 8 de novembro, no sal√£o principal da confer√™ncia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HighLoad ++ 2018</a> , no √¢mbito da se√ß√£o DevOps and Operations, foi elaborado um relat√≥rio intitulado Databases and Kubernetes.  Ele fala sobre a alta disponibilidade de bancos de dados e abordagens para toler√¢ncia a falhas no Kubernetes e com ele, al√©m de op√ß√µes pr√°ticas para colocar o DBMS em clusters do Kubernetes e solu√ß√µes existentes para isso (incluindo Stolon para PostgreSQL). <br><br><img src="https://habrastorage.org/webt/oq/mh/kp/oqmhkpy4pxg-olk9yybf_julwvu.jpeg"><br><br>  Por tradi√ß√£o, temos o prazer de apresentar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b>v√≠deo com um relat√≥rio</b></a> (cerca de uma hora, <b>muito mais</b> informativo <b>que o</b> artigo) e o aperto principal em forma de texto.  Vamos l√°! <a name="habracut"></a><br><br><h2>  Teoria </h2><br>  Este relat√≥rio apareceu como resposta a uma das perguntas mais populares que nos √∫ltimos anos nos foram feitas incansavelmente em diferentes lugares: coment√°rios no hub ou no YouTube, redes sociais etc.  Parece simples: ‚Äú√â poss√≠vel executar o banco de dados no Kubernetes?‚Äù, E se geralmente respondemos ‚Äúgeralmente sim, mas ...‚Äù, claramente n√£o havia explica√ß√£o suficiente para esses ‚Äúem geral‚Äù e ‚Äúmas‚Äù, mas para ajust√°-los em uma mensagem curta n√£o teve sucesso. <br><br>  No entanto, para come√ßar, eu resumi o problema do "banco de dados [dados]" para o estado como um todo.  Um DBMS √© apenas um caso especial de decis√µes com estado, cuja lista mais completa pode ser representada da seguinte maneira: <br><br><img src="https://habrastorage.org/webt/px/ps/2_/pxps2_ff80ru5qfth8bduiu_kdw.png"><br><br>  Antes de analisar casos espec√≠ficos, falarei sobre tr√™s caracter√≠sticas importantes do trabalho / uso do Kubernetes. <br><br><h3>  1. Filosofia de Alta Disponibilidade Kubernetes </h3><br>  Todo mundo conhece a analogia "animais de estima√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">versus gado</a> " e entende que, se Kubernetes √© uma hist√≥ria do mundo dos rebanhos, os DBMSs cl√°ssicos s√£o apenas animais de estima√ß√£o. <br><br>  E como era a arquitetura dos ‚Äúanimais de estima√ß√£o‚Äù na vers√£o ‚Äútradicional‚Äù?  Um exemplo cl√°ssico de instala√ß√£o do MySQL √© a replica√ß√£o em dois servidores de ferro com energia redundante, um disco, uma rede ... e tudo o mais (incluindo um engenheiro e v√°rias ferramentas auxiliares), que nos ajudar√£o a ter certeza de que o processo do MySQL n√£o falhar√° e se houver algum problema cr√≠tico. para seus componentes, a toler√¢ncia a falhas ser√° respeitada: <br><br><img src="https://habrastorage.org/webt/0m/qg/s3/0mqgs3e3cco1zukmlah1jg2ubjs.png"><br><br>  Como ser√° a mesma apar√™ncia no Kubernetes?  Aqui, geralmente h√° muito mais servidores de ferro, eles s√£o mais simples e n√£o possuem energia e rede redundantes (no sentido de que a perda de uma m√°quina n√£o afeta nada) - tudo isso √© combinado em um cluster.  Sua toler√¢ncia a falhas √© fornecida pelo software: se algo acontecer com o n√≥, o Kubernetes detecta e inicia as inst√¢ncias necess√°rias no outro n√≥. <br><br>  Quais s√£o os mecanismos de alta disponibilidade nos K8s? <br><br><img src="https://habrastorage.org/webt/n2/gw/mh/n2gwmhiogm3uzv1m5igrzqpyifq.png"><br><br><ol><li>  Controladores  Existem muitos, mas dois principais: <code>Deployment</code> (para aplicativos sem estado) e <code>StatefulSet</code> (para aplicativos com estado).  Eles armazenam toda a l√≥gica das a√ß√µes executadas no caso de uma falha no n√≥ (inacessibilidade do pod). </li><li>  <code>PodAntiAffinity</code> - a capacidade de especificar pods espec√≠ficos para que n√£o fiquem no mesmo n√≥. </li><li>  <code>PodDisruptionBudgets</code> - limite no n√∫mero de inst√¢ncias de pod que podem ser desativadas ao mesmo tempo em caso de trabalho agendado. </li></ol><br><h3>  2. Garantias de consist√™ncia do Kubernetes </h3><br>  Como o esquema familiar de toler√¢ncia a falhas de mestre √∫nico funciona?  Dois servidores (mestre e em espera), um dos quais √© constantemente acessado pelo aplicativo, que por sua vez √© usado atrav√©s do balanceador de carga.  O que acontece no caso de um problema de rede? <br><br><img src="https://habrastorage.org/webt/6p/1k/ve/6p1kvelnrzyrphtu6sb_agftgaa.gif"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><i>C√©rebro dividido</i></a> cl√°ssico: o aplicativo come√ßa a acessar as duas inst√¢ncias do DBMS, cada uma das quais se considera a principal.  Para evitar isso, keepalived foi substitu√≠do por corosync por tr√™s inst√¢ncias para obter um quorum ao votar no mestre.  No entanto, mesmo neste caso, h√° problemas: se uma inst√¢ncia do DBMS ca√≠da tentar "se matar" de todas as formas poss√≠veis (remover o endere√ßo IP, converter o banco de dados em somente leitura ...), a outra parte do cluster n√£o saber√° o que aconteceu com o mestre - isso pode acontecer, que esse n√≥ ainda funcione e as solicita√ß√µes cheguem a ele, o que significa que ainda n√£o podemos mudar o assistente. <br><br>  Para resolver essa situa√ß√£o, existe um mecanismo para isolar o n√≥ para proteger todo o cluster contra opera√ß√£o incorreta - esse processo √© chamado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><i>cerca</i></a> .  A ess√™ncia pr√°tica se resume ao fato de que estamos tentando, por alguns meios externos, "matar" o carro ca√≠do.  As abordagens podem ser diferentes: desde desligar a m√°quina via IPMI e bloquear a porta no comutador at√© acessar a API do provedor de nuvem, etc.  E somente ap√≥s esta opera√ß√£o voc√™ pode mudar o assistente.  Isso garante uma garantia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><i>no m√°ximo,</i></a> que nos garante <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">consist√™ncia</a></i> . <br><br><img src="https://habrastorage.org/webt/sl/xb/9r/slxb9rwf-lk8mcdaeqiuankz3z8.png"><br><br>  Como conseguir o mesmo no Kubernetes?  Para fazer isso, j√° existem controladores mencionados, cujo comportamento no caso de inacessibilidade de um n√≥ √© diferente: <br><br><ol><li>  <code>Deployment</code> : "Disseram-me que deveria haver 3 pods e agora existem apenas 2 pods - vou criar um novo"; </li><li>  <code>StatefulSet</code> : "Pod foi?"  Vou esperar: esse n√≥ retornar√° ou eles nos dir√£o para mat√°-lo "  os pr√≥prios cont√™ineres (sem a√ß√£o do operador) n√£o s√£o recriados.  √â assim que a mesma garantia m√°xima √© alcan√ßada. </li></ol><br>  No entanto, aqui, no √∫ltimo caso, √© necess√°rio cercar: precisamos de um mecanismo que confirme que esse n√≥ definitivamente desapareceu.  Torn√°-lo autom√°tico √©, em primeiro lugar, muito dif√≠cil (muitas implementa√ß√µes s√£o necess√°rias) e, em segundo lugar, ainda pior, geralmente mata n√≥s lentamente (o acesso ao IPMI pode levar segundos ou dezenas de segundos ou at√© minutos).  Poucas pessoas est√£o satisfeitas com a espera por minuto para mudar a base para o novo mestre.  Mas h√° outra abordagem que n√£o requer um mecanismo de esgrima ... <br><br>  Vou come√ßar a descri√ß√£o dele fora do Kubernetes.  Ele usa um balanceador de carga especial por meio do qual os back-ends acessam o DBMS.  Sua especificidade reside no fato de ter a propriedade de consist√™ncia, ou seja,  prote√ß√£o contra falhas de rede e c√©rebro dividido, pois permite remover todas as conex√µes com o mestre atual, aguardar a sincroniza√ß√£o (r√©plica) em outro n√≥ e alternar para ele.  N√£o encontrei um termo estabelecido para essa abordagem e a chamei de <i>altern√¢ncia consistente</i> . <br><br><img src="https://habrastorage.org/webt/ct/oq/lk/ctoqlkp3efyctjlvsyiheesk2s4.gif"><br><br>  A principal quest√£o com ele √© como torn√°-lo universal, fornecendo suporte para provedores de nuvem e instala√ß√µes privadas.  Para isso, servidores proxy s√£o adicionados aos aplicativos.  Cada um deles aceitar√° solicita√ß√µes de seu aplicativo (e as encaminhar√° ao DBMS), e um quorum ser√° coletado de todas elas.  Assim que uma parte do cluster falhar, os proxies que perderam o quorum remover√£o imediatamente suas conex√µes com o DBMS. <br><br><img src="https://habrastorage.org/webt/nj/y0/-t/njy0-tahnwp8uxeyevxot_tlntq.png"><br><br><h3>  3. Armazenamento de Dados e Kubernetes </h3><br>  O mecanismo principal √© a unidade de rede <i>Dispositivo de Bloco de Rede</i> (aka) em v√°rias implementa√ß√µes para as op√ß√µes de nuvem desejadas ou bare metal.  No entanto, colocar um banco de dados carregado (por exemplo, MySQL, que requer 50 mil IOPS) na nuvem (AWS EBS) n√£o funcionar√° devido √† <i>lat√™ncia</i> . <br><br><img src="https://habrastorage.org/webt/qq/wp/r7/qqwpr7fae-nbek0y2o6ex9lbzsa.png"><br><br>  O Kubernetes nesses casos tem a capacidade de conectar um disco r√≠gido <i>local</i> - <i>armazenamento local</i> .  Se ocorrer uma falha (o disco n√£o est√° mais dispon√≠vel no pod), somos for√ßados a reparar esta m√°quina - semelhante ao esquema cl√°ssico no caso de uma falha de um servidor confi√°vel. <br><br>  As duas op√ß√µes ( <i>Dispositivo de Bloco de Rede</i> e <i>Armazenamento Local</i> ) pertencem √† categoria <i>ReadWriteOnce</i> : o armazenamento n√£o pode ser montado em dois locais (pods) - para esse dimensionamento, voc√™ precisar√° criar um novo disco e conect√°-lo a um novo pod (existe um mecanismo K8s interno para isso) e preencha com os dados necess√°rios (j√° realizados por nossas for√ßas). <br><br>  Se precisarmos do modo <i>ReadWriteMany</i> , as implementa√ß√µes do <i>Network File System</i> (ou NAS) estar√£o dispon√≠veis: para a nuvem p√∫blica, essas s√£o <code>AzureFile</code> e <code>AWSElasticFileSystem</code> , e para suas instala√ß√µes CephFS e Glusterfs para f√£s de sistemas distribu√≠dos, bem como NFS. <br><br><img src="https://habrastorage.org/webt/eq/y6/gp/eqy6gpf2duz9ljtzc342bj9upg4.png"><br><br><h2>  Pr√°tica </h2><br><h3>  1. Independente </h3><br>  Essa op√ß√£o ocorre quando nada impede que voc√™ inicie o DBMS no modo de servidor separado com armazenamento local.  N√£o h√° d√∫vida de alta disponibilidade ... embora possa ser at√© certo ponto (isto √©, suficiente para esta aplica√ß√£o) implementado no n√≠vel do ferro.  Existem muitos casos para esta aplica√ß√£o.  Antes de tudo, esses s√£o todos os tipos de ambientes de teste e desenvolvimento, mas n√£o apenas: os servi√ßos secund√°rios tamb√©m caem aqui, desativ√°-los por 15 minutos n√£o √© cr√≠tico.  No Kubernetes, isso √© implementado pelo <code>StatefulSet</code> com um pod: <br><br><img src="https://habrastorage.org/webt/hl/xk/ha/hlxkhaodepe50imvuobtoilrz6o.png"><br><br>  Em geral, essa √© uma op√ß√£o vi√°vel, que, do meu ponto de vista, n√£o possui desvantagens em compara√ß√£o com a instala√ß√£o de um DBMS em uma m√°quina virtual separada. <br><br><h3>  2. Par replicado com comuta√ß√£o manual </h3><br>  <code>StatefulSet</code> usado novamente, mas o esquema geral se parece com isso: <br><br><img src="https://habrastorage.org/webt/vq/_i/to/vq_itonyvigrh0uezqek_lwtlt4.png"><br><br>  Se um dos n√≥s trava ( <code>mysql-a-0</code> ), um milagre n√£o ocorre, mas temos uma r√©plica ( <code>mysql-b-0</code> ) para a qual podemos mudar o tr√°fego.  Nesse caso, mesmo antes de mudar o tr√°fego, √© importante n√£o apenas esquecer as solicita√ß√µes de DBMS do servi√ßo <code>mysql</code> , mas tamb√©m efetuar login manualmente no DBMS e garantir que todas as conex√µes sejam conclu√≠das (elimin√°-las), al√©m de ir para o segundo n√≥ do DBMS e reconfigurar a r√©plica. na dire√ß√£o oposta. <br><br>  Se voc√™ est√° atualmente usando a vers√£o cl√°ssica com dois servidores (mestre + espera) sem <i>failover</i> autom√°tico, essa solu√ß√£o √© equivalente no Kubernetes.  Adequado para MySQL, PostgreSQL, Redis e outros produtos. <br><br><h3>  3. Escalando a carga de leitura </h3><br>  De fato, este caso n√£o √© est√°vel, porque estamos falando apenas de leitura.  Aqui, o servidor DBMS principal est√° fora do esquema considerado e, dentro da estrutura do Kubernetes, √© criado um "farm de servidores escravos", que s√£o somente leitura.  O mecanismo geral - o uso de cont√™ineres init para preencher dados do DBMS em cada novo pod deste farm (usando um despejo quente ou o usual com a√ß√µes adicionais etc. - depende do DBMS usado).  Para garantir que cada inst√¢ncia n√£o fique muito longe do mestre, voc√™ pode usar testes de anima√ß√£o. <br><br><img src="https://habrastorage.org/webt/nz/pd/uk/nzpdukumat3zbax7vnkcs5jtsvs.png"><br><br><h3>  4. Cliente Inteligente </h3><br>  Se voc√™ criar um <code>StatefulSet</code> de tr√™s memcached, o Kubernetes ter√° um servi√ßo especial que n√£o equilibrar√° solicita√ß√µes, mas criar√° cada pod para seu pr√≥prio dom√≠nio.  O cliente poder√° trabalhar com eles se ele pr√≥prio for capaz de fragmentar e replicar. <br><br>  Voc√™ n√£o precisa ir muito longe por exemplo: √© assim que o armazenamento de sess√µes funciona no PHP imediatamente.  Para cada solicita√ß√£o de sess√£o, as solicita√ß√µes s√£o feitas simultaneamente a todos os servidores, ap√≥s o qual a resposta mais relevante √© selecionada a partir deles (da mesma forma que um registro). <br><br><img src="https://habrastorage.org/webt/t8/iz/26/t8iz261adru0mbdw7cd2o5i3y8o.png"><br><br><h3>  5. Solu√ß√µes nativas em nuvem </h3><br>  Existem muitas solu√ß√µes inicialmente focadas na falha dos n√≥s, ou seja,  eles mesmos podem <i>executar failover</i> e recupera√ß√£o de n√≥s, fornecer garantias de <i>consist√™ncia</i> .  Esta n√£o √© uma lista completa deles, mas apenas parte de exemplos populares: <br><br><img src="https://habrastorage.org/webt/9u/ah/qz/9uahqzayfbdsokgyod153jwfjfs.png"><br><br>  Todos eles s√£o simplesmente colocados no <code>StatefulSet</code> , ap√≥s o qual os n√≥s se encontram e formam um cluster.  Os pr√≥prios produtos diferem na maneira como implementam tr√™s coisas: <br><br><ol><li>  Como os n√≥s aprendem um sobre o outro?  Existem m√©todos como a API do Kubernetes, registros DNS, configura√ß√£o est√°tica, n√≥s especializados (semente), descoberta de servi√ßos de terceiros ... </li><li>  Como o cliente se conecta?  Por meio de um balanceador de carga que distribui para hosts, ou o cliente precisa saber sobre todos os hosts e ele decide como proceder. </li><li>  Como √© feito o dimensionamento horizontal?  De jeito nenhum, completo ou dif√≠cil / com restri√ß√µes. </li></ol><br>  Independentemente das solu√ß√µes escolhidas para esses problemas, todos esses produtos funcionam bem com o Kubernetes, porque foram originalmente criados como um "rebanho" <i>(gado)</i> . <br><br><h3>  6. Stolon PostgreSQL </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stolon</a> realmente permite que voc√™ transforme o PostgreSQL, criado como <i>animal de estima√ß√£o</i> , em <i>gado</i> .  Como isso √© alcan√ßado? <br><br><img src="https://habrastorage.org/webt/g_/z4/pc/g_z4pcehfw6p985duphcgrbukzo.png"><br><br><ul><li>  Primeiramente, precisamos de uma descoberta de servi√ßo, cuja fun√ß√£o possa ser <b>etcd</b> (outras op√ß√µes est√£o dispon√≠veis) - um cluster delas √© colocado em um <code>StatefulSet</code> . </li><li>  Outra parte da infraestrutura √© <code>StatefulSet</code> com inst√¢ncias do PostgreSQL.  Al√©m do DBMS propriamente dito, ao lado de cada instala√ß√£o tamb√©m h√° um componente chamado <b>guardi√£o</b> , que executa a configura√ß√£o do DBMS. </li><li>  Outro componente, o <b>sentinel,</b> √© implantado como uma <code>Deployment</code> e monitora a configura√ß√£o do cluster.  √â ele quem decide quem ser√° o mestre e o modo de espera, grava essas informa√ß√µes no etcd.  E o guardi√£o l√™ os dados do etcd e executa a√ß√µes correspondentes ao status atual com uma inst√¢ncia do PostgreSQL. </li><li>  Outro componente implantado no <code>Deployment</code> e enfrentando inst√¢ncias do PostgreSQL, <b>proxy,</b> √© uma implementa√ß√£o do padr√£o de <i>Altern√¢ncia Consistente</i> j√° mencionado.  Esses componentes est√£o conectados ao etcd e, se essa conex√£o for perdida, o proxy eliminar√° imediatamente as conex√µes de sa√≠da, porque a partir deste momento ele n√£o sabe o papel do servidor (agora √© mestre ou em espera?). </li><li>  Por fim, as inst√¢ncias de proxy enfrentam o habitual <code>LoadBalancer</code> LoadBalancer. </li></ul><br><h2>  Conclus√µes </h2><br>  Ent√£o, √© poss√≠vel basear-se no Kubernetes?  Sim, √© claro, √© poss√≠vel, em alguns casos ... E se for apropriado, √© feito assim (consulte o fluxo de trabalho Stolon) ... <br><br>  Todo mundo sabe que a tecnologia est√° evoluindo em ondas.  Inicialmente, qualquer novo dispositivo pode ser muito dif√≠cil de usar, mas com o tempo tudo muda: a tecnologia se torna dispon√≠vel.  Para onde vamos?  Sim, permanecer√° assim por dentro, mas n√£o saberemos como isso funcionar√°.  Kubernetes est√° desenvolvendo ativamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">operadores</a> .  At√© agora, n√£o existem muitos deles e eles n√£o s√£o t√£o bons, mas h√° movimento nessa dire√ß√£o. <br><br><h2>  V√≠deos e slides </h2><br>  V√≠deo da apresenta√ß√£o (cerca de uma hora): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BnegHj53pW4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Apresenta√ß√£o do relat√≥rio: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  PS Tamb√©m encontramos na rede um <a href="">pequeno aperto textual</a> neste relat√≥rio - obrigado por Nikolai Volynkin. <br><br><h2>  PPS </h2><br>  Outros relat√≥rios em nosso blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitoramento e Kubernetes</a> ";  <i>(Dmitry Stolyarov; 28 de maio de 2018 na RootConf)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores pr√°ticas de CI / CD com Kubernetes e GitLab</a> ‚Äù;  <i>(Dmitry Stolyarov; 7 de novembro de 2017 no HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nossa experi√™ncia com o Kubernetes em pequenos projetos</a> ‚Äù;  <i>(Dmitry Stolyarov; 6 de junho de 2017 na RootConf)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Coletamos imagens do Docker para CI / CD de maneira r√°pida e conveniente com o dapp</a> ‚Äù <i>(Dmitry Stolyarov; 8 de novembro de 2016 no HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pr√°ticas de entrega cont√≠nua com Docker</a> ‚Äù <i>(Dmitry Stolyarov; 31 de maio de 2016 na RootConf)</i> . </li></ul><br>  Voc√™ tamb√©m pode estar interessado nas seguintes publica√ß√µes: <br><br><ul><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dicas e truques do Kubernetes: acelerando a inicializa√ß√£o de grandes bancos de dados</a> ‚Äù; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Orquestra√ß√£o de DBMS de barata no Kubernetes</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt431500/">https://habr.com/ru/post/pt431500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt431488/index.html">Modula√ß√£o de som</a></li>
<li><a href="../pt431490/index.html">Externo - GUI para Golang</a></li>
<li><a href="../pt431492/index.html">Reagir o question√°rio do concurso ao analisar o estande do HeadHunter no HolyJs 2018</a></li>
<li><a href="../pt431496/index.html">Como a tecnologia ajuda professores de turmas especiais</a></li>
<li><a href="../pt431498/index.html">O WebP assumir√° a Web em breve, mas n√£o demorar√° muito</a></li>
<li><a href="../pt431502/index.html">Confer√™ncia para desenvolvedores de iOS Kolesa Mobile 3.0. Relat√≥rio de v√≠deo</a></li>
<li><a href="../pt431504/index.html">Phishing - funciona. Cr√¥nica de roubo do iPhone XS seguido por roubo de dados do iCloud</a></li>
<li><a href="../pt431506/index.html">Xcode e depura√ß√£o avan√ßada no LLDB: parte 1</a></li>
<li><a href="../pt431508/index.html">Gerenciamento eficiente de transa√ß√µes na primavera</a></li>
<li><a href="../pt431510/index.html">Como coletar informa√ß√µes do contorno.Comprar com Selenium</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>