<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå∫ üò´ üôéüèø Construyendo una IA segura: especificaciones, confiabilidad y garant√≠as üìß üÖ±Ô∏è üêÖ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entre los autores del art√≠culo se encuentran empleados del equipo de seguridad de inteligencia artificial (equipo de seguridad) de la empresa DeepMind...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Construyendo una IA segura: especificaciones, confiabilidad y garant√≠as</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425387/">  <font color="gray">Entre los autores del art√≠culo se encuentran empleados del equipo de seguridad de inteligencia artificial (equipo de seguridad) de la empresa DeepMind.</font> <br><br>  Construir un cohete es dif√≠cil.  Cada componente requiere un estudio y prueba cuidadosos, mientras que la seguridad y la confiabilidad son fundamentales.  Los cient√≠ficos e ingenieros de cohetes se unen para dise√±ar todos los sistemas: desde navegaci√≥n hasta control, motores y chasis.  Una vez que se ensamblen todas las partes y se verifiquen los sistemas, solo entonces podremos llevar a los astronautas a bordo con la confianza de que todo estar√° bien. <br><br>  Si la inteligencia artificial (IA) es un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cohete</a> , alg√∫n d√≠a todos tendremos boletos a bordo.  Y, como los cohetes, la seguridad es una parte importante de la creaci√≥n de sistemas de inteligencia artificial.  La seguridad requiere un dise√±o cuidadoso del sistema desde cero para garantizar que los diversos componentes trabajen juntos seg√∫n lo previsto, al mismo tiempo que crean todas las herramientas para monitorear la operaci√≥n exitosa del sistema despu√©s de su puesta en servicio. <br><br>  A un alto nivel, la investigaci√≥n de seguridad en DeepMind se enfoca en dise√±ar sistemas confiables mientras detecta y mitiga posibles riesgos a corto y largo plazo.  <b>La seguridad t√©cnica de la IA</b> es un campo relativamente nuevo pero en r√°pido desarrollo, cuyo contenido var√≠a desde un alto nivel te√≥rico hasta la investigaci√≥n emp√≠rica y espec√≠fica.  El prop√≥sito de este blog es contribuir al desarrollo del campo y fomentar una conversaci√≥n sustantiva sobre ideas t√©cnicas, promoviendo as√≠ nuestra comprensi√≥n colectiva de la seguridad de la IA. <br><a name="habracut"></a><br>  En el primer art√≠culo, discutiremos tres √°reas de seguridad t√©cnica de IA: <b>especificaciones</b> , <b>confiabilidad</b> y <b>garant√≠as</b> .  Los art√≠culos futuros generalmente corresponder√°n a los l√≠mites descritos aqu√≠.  Aunque nuestros puntos de vista cambian inevitablemente con el tiempo, creemos que estas tres √°reas cubren un espectro lo suficientemente amplio como para proporcionar una categorizaci√≥n √∫til para la investigaci√≥n actual y futura. <br><br><img src="https://habrastorage.org/webt/sv/8c/se/sv8cseuw2rlofm85zszbk6nhv6k.png"><br>  <i><font color="gray">Tres √°reas problem√°ticas de la seguridad de la IA.</font></i>  <i><font color="gray">Cada bloque enumera algunos temas y enfoques relevantes.</font></i>  <i><font color="gray">Estas tres √°reas no est√°n aisladas, sino que interact√∫an entre s√≠.</font></i>  <i><font color="gray">En particular, un problema de seguridad particular puede incluir m√∫ltiples problemas de bloque.</font></i> <br><br><h1>  Especificaciones: definici√≥n de tareas del sistema </h1><br><h4>  Las especificaciones aseguran que el comportamiento del sistema de IA sea consistente con las verdaderas intenciones del operador </h4><br>  Quiz√°s conozcas el mito del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">rey Midas</a> y el toque dorado.  En una de las opciones, el dios griego Dioniso le prometi√≥ a Midas cualquier recompensa que deseara, en agradecimiento por el hecho de que el rey hizo todo lo posible para mostrar hospitalidad y misericordia al amigo de Dioniso.  Entonces <b>Midas pidi√≥ que todo lo que toca se convierta en oro</b> .  Estaba fuera de s√≠ con alegr√≠a por este nuevo poder: una rama de roble, una piedra y rosas en el jard√≠n, todo se convirti√≥ en oro por su toque.  Pero pronto descubri√≥ la estupidez de su deseo: incluso la comida y la bebida se convirtieron en oro en sus manos.  En algunas versiones de la historia, incluso su hija fue v√≠ctima de una bendici√≥n que result√≥ ser una maldici√≥n. <br><br>  Esta historia ilustra el problema de las especificaciones: ¬øc√≥mo formular correctamente nuestros deseos?  Las especificaciones deben garantizar que el sistema de IA se esfuerza por actuar de acuerdo con los verdaderos deseos del creador, y no sintoniza con un objetivo mal definido o incluso incorrecto.  Se distinguen formalmente tres tipos de especificaciones: <br><br><ul><li>  <b>especificaci√≥n ideal</b> (" <b>deseos</b> "), correspondiente a una descripci√≥n hipot√©tica (pero dif√≠cil de formular) de un sistema de IA ideal, totalmente coherente con los deseos del operador humano; </li><li>  <b>especificaci√≥n de proyecto</b> (" <b>blueprint</b> "), la especificaci√≥n correspondiente que <i>realmente utilizamos</i> para crear un sistema de IA, por ejemplo, una funci√≥n de remuneraci√≥n espec√≠fica, para maximizar la programaci√≥n de un sistema de aprendizaje de refuerzo; </li><li>  <b>especificaci√≥n identificada</b> (" <b>comportamiento</b> "), que describe mejor el <i>comportamiento real del</i> sistema.  Por ejemplo, la funci√≥n de recompensa identificada como resultado de la ingenier√≠a inversa despu√©s de observar el comportamiento del sistema (aprendizaje de refuerzo inverso).  Esta funci√≥n y especificaci√≥n de recompensa generalmente son diferentes de las programadas por el operador porque los sistemas de inteligencia artificial no son optimizadores ideales o por otras consecuencias imprevistas de usar la especificaci√≥n de dise√±o. </li></ul><br>  <b>El problema de la especificaci√≥n</b> surge cuando existe una discrepancia entre la <b>especificaci√≥n ideal</b> y la <b>especificada identificada</b> , es decir, cuando el sistema de IA no hace lo que queremos de √©l.  Estudiar el problema desde el punto de vista de la seguridad t√©cnica de la IA significa: ¬øc√≥mo dise√±ar funciones de objetivos m√°s fundamentales y generales y ayudar a los agentes a determinar si los objetivos no est√°n definidos?  Si los problemas dan lugar a un desajuste entre el ideal y las especificaciones de dise√±o, entonces caen en la subcategor√≠a "Dise√±o", y si est√°n entre dise√±o y los identificados, entonces en la subcategor√≠a "Emergencia". <br><br>  Por ejemplo, en nuestro art√≠culo cient√≠fico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Safety Gridworlds</a> (donde se presentan otras definiciones de especificaciones y problemas de confiabilidad en comparaci√≥n con este art√≠culo) les damos a los agentes una funci√≥n de recompensa por la optimizaci√≥n, pero luego evaluamos su desempe√±o real mediante la "funci√≥n de desempe√±o de seguridad", que est√° oculto a los agentes.  Tal sistema modela las diferencias indicadas: la funci√≥n de seguridad es una especificaci√≥n ideal que se formula incorrectamente como una funci√≥n de recompensa (especificaci√≥n de proyecto), y luego implementada por agentes que crean una especificaci√≥n que se revela impl√≠citamente a trav√©s de su pol√≠tica resultante. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/vd/cm/pwvdcm0ra3_bo4qzpu9gdc_nfco.gif"></div><br>  <i><font color="gray">De <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las funciones de recompensa defectuosas de</a> OpenAI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en la naturaleza</a> : un agente de aprendizaje de refuerzo encontr√≥ una estrategia aleatoria para obtener m√°s puntos</font></i> <br><br>  Como otro ejemplo, considere el juego CoastRunners, que fue analizado por nuestros colegas de OpenAI (vea la animaci√≥n anterior de "Funciones de recompensa de vida silvestre defectuosas").  Para la mayor√≠a de nosotros, el objetivo del juego es terminar r√°pidamente la pista y adelantarnos a otros jugadores; esta es nuestra especificaci√≥n ideal.  Sin embargo, traducir este objetivo en una funci√≥n de recompensa exacta es dif√≠cil, por lo que CoastRunners recompensa a los jugadores (especificaci√≥n de dise√±o) por alcanzar el objetivo a lo largo de la ruta.  Entrenar a un agente para que juegue con entrenamiento de refuerzo conduce a un comportamiento sorprendente: el agente controla el bote en un c√≠rculo para capturar objetivos que reaparecen, chocando y prendiendo fuego repetidamente, en lugar de terminar la carrera.  De este comportamiento, concluimos (especificaci√≥n identificada) que en el juego se rompe el equilibrio entre la recompensa instant√°nea y la recompensa de c√≠rculo completo.  Hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">muchos m√°s ejemplos similares</a> donde los sistemas de IA encuentran lagunas en su especificaci√≥n objetiva. <br><br><h1>  Fiabilidad: dise√±o de sistemas que resistan las infracciones </h1><br><h4>  La confiabilidad asegura que el sistema de IA contin√∫e operando de manera segura en caso de interferencia </h4><br>  En condiciones reales, donde funcionan los sistemas de IA, siempre hay un cierto nivel de riesgo, imprevisibilidad y volatilidad.  Los sistemas de inteligencia artificial deben ser resistentes a eventos imprevistos y ataques hostiles que pueden da√±ar o manipular estos sistemas.  <b>Los</b> estudios de <b>confiabilidad</b> de los sistemas de inteligencia artificial tienen como objetivo garantizar que nuestros agentes permanezcan dentro de l√≠mites seguros, independientemente de las condiciones emergentes.  Esto se puede lograr evitando riesgos ( <b>prevenci√≥n</b> ) o mediante autoestabilizaci√≥n y degradaci√≥n suave ( <b>recuperaci√≥n</b> ).  Los problemas de seguridad derivados del <b>cambio de distribuci√≥n</b> , <b>las entradas hostiles</b> ( <b>entradas adversas</b> ) y la <b>exploraci√≥n</b> insegura (exploraci√≥n insegura) se pueden clasificar como problemas de confiabilidad. <br><br>  Para ilustrar la soluci√≥n al problema del <b>cambio distributivo</b> , considere un robot de limpieza del hogar que generalmente limpia habitaciones sin mascotas.  Luego, el robot fue lanzado a la casa con la mascota, y la inteligencia artificial choc√≥ con √©l durante la limpieza.  Un robot que nunca antes haya visto gatos y perros lo lavar√° con jab√≥n, lo que conducir√° a resultados indeseables ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Amodei y Olah et al., 2016</a> ).  Este es un ejemplo de un problema de confiabilidad que puede surgir cuando la distribuci√≥n de datos durante las pruebas difiere de la distribuci√≥n durante el entrenamiento. <br><br><img src="https://habrastorage.org/webt/oi/k0/lc/oik0lc_srvx7tovbrec-dmbzqsa.gif"><br>  <i><font color="gray">Del trabajo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AI Safety Gridworlds</a> .</font></i>  <i><font color="gray">El agente aprende a evitar la lava, pero cuando realiza pruebas en una nueva situaci√≥n, cuando la ubicaci√≥n de la lava ha cambiado, no puede generalizar el conocimiento y corre directamente hacia la lava.</font></i> <br><br>  La entrada hostil es un caso espec√≠fico de un cambio de distribuci√≥n donde los datos de entrada est√°n especialmente dise√±ados para enga√±ar al sistema de IA. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/550/89a/6d5/55089a6d587e1745783257a0c898b046.png"><br>  <i><font color="gray">Una entrada hostil superpuesta a im√°genes comunes puede hacer que el clasificador reconozca al perezoso como un auto de carreras.</font></i>  <i><font color="gray">Las dos im√°genes difieren en un m√°ximo de 0.0078 en cada p√≠xel.</font></i>  <i><font color="gray">El primero se clasifica como un perezoso de tres dedos con una probabilidad de m√°s del 99%.</font></i>  <i><font color="gray">El segundo, como un auto de carrera con una probabilidad de m√°s del 99%</font></i> <br><br>  <b>La investigaci√≥n insegura</b> puede demostrarse mediante un sistema que busca maximizar su rendimiento y objetivos sin garantizar que la seguridad no se vea comprometida durante el estudio, ya que aprende y examina en su entorno.  Un ejemplo es un robot de limpieza que introduce un trapeador h√∫medo en un tomacorriente mientras estudia estrategias de limpieza √≥ptimas ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Garc√≠a y Fern√°ndez, 2015</a> ; <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Amodei y Olah et al., 2016</a> ). <br><br><h1>  Garant√≠as: seguimiento y control de la actividad del sistema. </h1><br><h4>  La garant√≠a da la confianza de que somos capaces de comprender y controlar los sistemas de IA durante la operaci√≥n </h4><br>  Aunque las precauciones de seguridad cuidadosamente pensadas pueden eliminar muchos riesgos, es dif√≠cil hacer todo desde el principio.  Despu√©s de la puesta en marcha de los sistemas de IA, necesitamos herramientas para su monitoreo y configuraci√≥n constantes.  Nuestra √∫ltima categor√≠a, aseguramiento, aborda estos problemas desde dos perspectivas: <b>monitoreo</b> y aplicaci√≥n. <br><br>  <b>El monitoreo</b> incluye todos los m√©todos de verificaci√≥n de sistemas para analizar y predecir su comportamiento, tanto mediante inspecci√≥n humana (resumen de estad√≠sticas) como mediante inspecci√≥n autom√°tica (para analizar una gran cantidad de registros).  Por otro lado, la <b>sumisi√≥n</b> implica el desarrollo de mecanismos de control y restricciones sobre el comportamiento de los sistemas.  Problemas como la <b>interpretabilidad</b> y la <b>discontinuidad</b> pertenecen a subcategor√≠as de control y sumisi√≥n, respectivamente. <br><br>  Los sistemas de inteligencia artificial no son similares a nosotros ni en su apariencia ni en la forma en que procesan los datos.  Esto crea problemas de <b>interpretabilidad</b> .  Las herramientas y protocolos de medici√≥n bien dise√±ados le permiten evaluar la calidad de las decisiones tomadas por el sistema de inteligencia artificial ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Doshi-Velez y Kim, 2017</a> ).  Por ejemplo, un sistema de inteligencia artificial m√©dica idealmente har√≠a un diagn√≥stico junto con una explicaci√≥n de c√≥mo lleg√≥ a esta conclusi√≥n, para que los m√©dicos puedan verificar el proceso de razonamiento de principio a fin ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">De Fauw et al., 2018</a> ).  Adem√°s, para comprender sistemas de inteligencia artificial m√°s complejos, incluso podr√≠amos utilizar m√©todos automatizados para construir modelos de comportamiento utilizando <b>la teor√≠a de la m√°quina de la mente</b> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Rabinowitz et al., 2018</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sp/ff/ei/spffeiaxzptaap4ghzl_xmcao1o.png"></div><br>  <i><font color="gray">ToMNet detecta dos subespecies de agentes y predice su comportamiento (de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"Teor√≠a de la mente</a> de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√°quina"</a> )</font></i> <br><br>  Finalmente, queremos poder deshabilitar el sistema AI si es necesario.  Este es un problema de <b>discontinuidad</b> .  Dise√±ar un interruptor confiable es muy dif√≠cil: por ejemplo, porque un sistema de IA con maximizaci√≥n de recompensas generalmente tiene fuertes incentivos para prevenir esto ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hadfield-Menell et al., 2017</a> );  y debido a que tales interrupciones, especialmente las frecuentes, en √∫ltima instancia cambian la tarea original, obligando al sistema de IA a sacar conclusiones incorrectas de la experiencia ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Orseau y Armstrong, 2016</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/6d/dn/fl6ddnu5joy9i6jnya2wdrzpyeq.png"></div><br>  <i><font color="gray">El problema con las interrupciones: la intervenci√≥n humana (es decir, presionar el bot√≥n de parada) puede cambiar la tarea.</font></i>  <i><font color="gray">En la figura, la interrupci√≥n agrega una transici√≥n (en rojo) al proceso de toma de decisiones de Markov, que cambia la tarea original (en negro).</font></i>  <i><font color="gray">Ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Orseau y Armstrong, 2016</a></font></i> <br><br><h1>  Mirando hacia el futuro </h1><br>  Estamos construyendo la base de la tecnolog√≠a que se utilizar√° para muchas aplicaciones importantes en el futuro.  Debe tenerse en cuenta que algunas soluciones que no son cr√≠ticas para la seguridad al iniciar el sistema pueden serlo cuando la tecnolog√≠a se generalice.  Aunque en alg√∫n momento estos m√≥dulos se integraron en el sistema por conveniencia, los problemas que surgieron ser√°n dif√≠ciles de solucionar sin una reconstrucci√≥n completa. <br><br>  Se pueden citar dos ejemplos de la historia de la inform√°tica: este es el puntero nulo, que Tony Hoar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">llam√≥ su "error de mil millones de d√≥lares"</a> , y el procedimiento gets () en C. Si los primeros lenguajes de programaci√≥n se dise√±aron teniendo en cuenta la seguridad, el progreso se ralentizar√≠a, pero es probable que Esto tendr√≠a un efecto muy positivo en la seguridad moderna de la informaci√≥n. <br><br>  Ahora, despu√©s de haber pensado y planeado cuidadosamente todo, podemos evitar problemas y vulnerabilidades similares.  Esperamos que la categorizaci√≥n de los problemas de este art√≠culo sirva como una base √∫til para dicha planificaci√≥n metodol√≥gica.  Nos esforzamos por garantizar que en el futuro los sistemas de inteligencia artificial no solo funcionen seg√∫n el principio de "ojal√° sea seguro", sino tambi√©n realmente confiables y verificables, ¬°porque los construimos de esa manera! <br><br>  Esperamos continuar con un progreso emocionante en estas √°reas, en estrecha colaboraci√≥n con la comunidad de investigaci√≥n de IA m√°s amplia, y alentar a las personas de diversas disciplinas a considerar contribuir a la investigaci√≥n de seguridad de IA. <br><br><h1>  Recursos </h1><br>  Para leer sobre este tema, a continuaci√≥n hay una selecci√≥n de otros art√≠culos, programas y taxonom√≠as que nos han ayudado a compilar nuestra categorizaci√≥n o proporcionar una mirada alternativa √∫til a los problemas de seguridad t√©cnica de AI: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Bibliograf√≠a anotada de materiales recomendados</a> (Centro de IA compatible con humanos, 2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Seguridad y control para la inteligencia general artificial</a> (UC Berkeley, 2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Recursos de seguridad de AI</a> (Victoria Krakovna, 2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Revisi√≥n de literatura de seguridad de AGI</a> (Everitt et al., 2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Prepar√°ndose para usos maliciosos de la IA</a> (2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Especificaci√≥n de ejemplos de juegos en IA</a> (Victoria Krakovna, 2018) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Instrucciones y desiderata para la alineaci√≥n de AI</a> (Paul Christiano, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Financiaci√≥n para la investigaci√≥n de alineaci√≥n</a> (Paul Christiano, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Fundamentos de agentes para alinear la inteligencia artificial con los intereses humanos: una agenda de investigaci√≥n t√©cnica</a> (Machine Intelligence Research Institute, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">AI Safety Gridworlds</a> (Leike et al., 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Interacciones entre el problema de control de IA y el problema de gobernanza</a> (Nick Bostrom, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Alineaci√≥n para sistemas avanzados de aprendizaje autom√°tico</a> (Machine Intelligence Research Institute, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Seguridad de IA: tres problemas humanos y un problema de IA</a> (Stuart Armstrong, 2017) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="nofollow noopener">Problemas concretos en la seguridad de la IA</a> (Dario Amodei et al, 2016) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">El problema del aprendizaje del valor</a> (Machine Intelligence Research Institute, 2016) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Una encuesta de preguntas de investigaci√≥n para una IA robusta y beneficiosa</a> (Future of Life Institute, 2015) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="noopener nofollow">Prioridades de investigaci√≥n para la inteligencia artificial robusta y beneficiosa</a> (Future of Life Institute, 2015) </li></ul><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Inteligencia Artificial</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje autom√°tico</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mente profunda</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ai seguridad</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es425387/">https://habr.com/ru/post/es425387/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es425375/index.html">Otra forma de ver las comunicaciones de la aplicaci√≥n.</a></li>
<li><a href="../es425377/index.html">Si los dise√±adores de productos digitales crearan cosas reales</a></li>
<li><a href="../es425379/index.html">Charles Nutter. ¬øC√≥mo transferir un antiguo proyecto monol√≠tico a JRuby y vale la pena?</a></li>
<li><a href="../es425383/index.html">Jet Infosystems, Rosreestr, NLMK y Utkonos lanzan hackathon de IA</a></li>
<li><a href="../es425385/index.html">La cabeza del programador: c√≥mo la codificaci√≥n afecta el pensamiento</a></li>
<li><a href="../es425389/index.html">FadeObjects: oculta objetos entre la c√°mara y el personaje</a></li>
<li><a href="../es425393/index.html">QIWI server party 3.0: informe + videos completos de todos los informes</a></li>
<li><a href="../es425395/index.html">10 datos f√≠sicos que deber√≠as haber sabido en la escuela pero que quiz√°s no sab√≠as</a></li>
<li><a href="../es425397/index.html">10 bibliotecas que todo desarrollador de Android deber√≠a conocer</a></li>
<li><a href="../es425401/index.html">Informe del Club de Roma 2018, Cap√≠tulo 1.11: Tecnolog√≠a disruptiva y la revoluci√≥n digital</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>