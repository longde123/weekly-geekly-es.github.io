<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§òüèΩ ü§π üè≥Ô∏è Una introducci√≥n simple a ALU para redes neuronales: explicaci√≥n, significado f√≠sico e implementaci√≥n ü§∂üèæ üìå üå∂Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recientemente, investigadores de Google DeepMind, incluido un reconocido cient√≠fico de inteligencia artificial, autor del libro " Comprender el aprend...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Una introducci√≥n simple a ALU para redes neuronales: explicaci√≥n, significado f√≠sico e implementaci√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422777/">  Recientemente, investigadores de Google DeepMind, incluido un reconocido cient√≠fico de inteligencia artificial, autor del libro " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Comprender el aprendizaje profundo</a> " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">,</a> Andrew Trask, public√≥ un art√≠culo impresionante que describe un modelo de red neuronal para extrapolar los valores de funciones num√©ricas simples y complejas con un alto grado de precisi√≥n. <br><br>  En esta publicaci√≥n explicar√© la arquitectura de <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dispositivos de l√≥gica aritm√©tica</a> neural, NALU), sus componentes y las diferencias significativas de las redes neuronales tradicionales.  El objetivo principal de este art√≠culo es explicar de manera simple e intuitiva <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> (tanto la implementaci√≥n como la idea) para cient√≠ficos, programadores y estudiantes que son nuevos en las redes neuronales y el aprendizaje profundo. <br><br>  <b>Nota del autor</b> : Tambi√©n recomiendo leer el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo original</a> para un estudio m√°s detallado del tema. <br><a name="habracut"></a><br><h2>  ¬øCu√°ndo est√°n mal las redes neuronales? </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/260/98c/6ba26098c22a26f200ac6c7c9abce91d.jpg" alt="Red neuronal cl√°sica"><br>  <i>Imagen tomada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo.</a></i> <br><br>  En teor√≠a, las redes neuronales deber√≠an aproximarse bien a las funciones.  Casi siempre pueden identificar correspondencias significativas entre los datos de entrada (factores o caracter√≠sticas) y la salida (etiquetas u objetivos).  Es por eso que las redes neuronales se usan en muchos campos, desde el reconocimiento de objetos y su clasificaci√≥n hasta la traducci√≥n del habla en texto y la implementaci√≥n de algoritmos de juego que pueden vencer a los campeones mundiales.  Ya se han creado muchos modelos diferentes: redes neuronales convolucionales y recurrentes, autocodificadores, etc. El √©xito en la creaci√≥n de nuevos modelos de redes neuronales y aprendizaje profundo es un gran tema en s√≠ mismo. <br><br>  Sin embargo, seg√∫n los autores del art√≠culo, ¬°las redes neuronales no siempre hacen frente a tareas que parecen obvias para las personas e incluso para las <abbr title="Enlace [7] del art√≠culo original: C. Randy Gallistel. Encontrar n√∫meros en el cerebro. Transacciones filos√≥ficas de la Royal Society B, 373, 2017.">abejas</abbr> !  Por ejemplo, esta es una cuenta oral u operaciones con n√∫meros, as√≠ como la capacidad de identificar la dependencia de las relaciones.  El art√≠culo mostr√≥ que los modelos est√°ndar de redes neuronales ni siquiera pueden hacer frente al <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mapeo id√©ntico</a> (una funci√≥n que traduce un argumento en s√≠ mismo, <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.846ex" height="2.66ex" viewBox="0 -832 3808.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="3236" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> f (x) = x </script>  ) Es la relaci√≥n num√©rica m√°s obvia.  La siguiente figura muestra el <abbr title="error cuadr√°tico medio">MSE de</abbr> varios modelos de redes neuronales cuando se aprende sobre los valores de esta funci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/e09/8e7/f1ee098e705e22745970e40e13782ef0.png" alt="Error cuadrado medio para redes neuronales est√°ndar"><br>  <i>La figura muestra el error cuadrado medio para redes neuronales est√°ndar que usan la misma arquitectura y diferentes funciones de activaci√≥n (no lineales) en las capas internas</i> <br><br><h2>  ¬øPor qu√© est√°n mal las redes neuronales? </h2><br>  Como se puede ver en la figura, la raz√≥n principal de los errores es la <b>no linealidad de las funciones de activaci√≥n</b> en las capas internas de la red neuronal.  Este enfoque funciona muy bien para determinar las relaciones no lineales entre los datos de entrada y las respuestas, pero es terriblemente incorrecto ir m√°s all√° de los datos en los que la red aprendi√≥.  Por lo tanto, las redes neuronales hacen un excelente trabajo al <b>recordar una</b> dependencia num√©rica de los datos de entrenamiento, pero no pueden extrapolarla. <br><br>  Esto es como escribir una respuesta o un tema antes de un examen sin comprender el tema.  Es f√°cil aprobar el examen si las preguntas son similares a la tarea, pero si se trata de comprender el tema que se est√° evaluando y no la capacidad de recordar, fallaremos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/888/a65/e3b/888a65e3b7a49c678be14c5e2b16d6d4.jpg" alt="Harry potter"><br>  <i>¬°Esto no estaba en el programa del curso!</i> <br><br>  El grado de error est√° directamente relacionado con el nivel de no linealidad de la funci√≥n de activaci√≥n seleccionada.  El diagrama anterior muestra claramente que las funciones no lineales con restricciones duras, como una tangente sigmoidea o hiperb√≥lica ( <b>Tanh</b> ), hacen frente a la tarea de generalizar dependencias mucho peor que las funciones con restricciones suaves, como una transformaci√≥n lineal truncada ( <b>ELU</b> , <b>PReLU</b> ). <br><br><h2>  Soluci√≥n: bater√≠a neuronal (NAC) </h2><br>  Una bater√≠a neural ( <abbr title="Bater√≠a neuronal">NAC</abbr> ) est√° en el coraz√≥n del modelo <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> .  Esta es una parte simple pero efectiva de una red neuronal que hace frente a la <b>suma y la resta</b> , que es necesaria para el c√°lculo eficiente de las relaciones lineales. <br><br>  <abbr title="Bater√≠a neuronal">NAC</abbr> es una capa lineal especial de una red neuronal, en cuyo peso se impone una condici√≥n simple: solo pueden tomar 3 valores: <b>1, 0 o -1</b> .  Dichas restricciones no permiten que la bater√≠a cambie el rango de datos de entrada, y permanece constante en todas las capas de la red, independientemente de su n√∫mero y conexiones.  Por lo tanto, la salida es una <b>combinaci√≥n lineal de los</b> valores del vector de entrada, que puede ser f√°cilmente una operaci√≥n de suma y resta. <br><br>  <b>Reflexiones en voz alta</b> : para una mejor comprensi√≥n de esta declaraci√≥n, veamos un ejemplo de construcci√≥n de capas de una red neuronal que realizan operaciones aritm√©ticas lineales en los datos de entrada. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/851/d6a/ac8/851d6aac8016eae4df4c594cfa2fb517.jpg" alt="Extrapolaci√≥n lineal en la red neuronal."><br>  <i>La figura ilustra c√≥mo las capas de una red neuronal sin agregar una constante y con posibles valores de pesos -1, 0 o 1, pueden realizar una extrapolaci√≥n lineal</i> <br><br>  Como se muestra arriba en la imagen de las capas, la red neuronal puede aprender a extrapolar los valores de funciones aritm√©ticas tan simples como la suma y la resta ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-2B" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> y = x_1 + x_2 </script>  y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-2212" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> y = x_1 - x_2 </script>  ), utilizando las restricciones de los pesos con posibles valores de 1, 0 y -1. <br><br>  <b>Nota: la capa NAC en este caso no contiene un t√©rmino libre (constante) y no aplica transformaciones no lineales a los datos.</b> <br><br>  Dado que las redes neuronales est√°ndar no pueden hacer frente a la soluci√≥n del problema bajo restricciones similares, los autores del art√≠culo ofrecen una f√≥rmula muy √∫til para calcular dichos par√°metros a trav√©s de par√°metros cl√°sicos (ilimitados) <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ hat {W} </script>  y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ hat {M} </script>  .  Los datos de peso, como todos los par√°metros de las redes neuronales, se pueden inicializar y seleccionar aleatoriamente en el proceso de entrenamiento de la red.  F√≥rmula para calcular vector <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> W </script>  a trav√©s de <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7"> \ hat {W} </script>  y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ hat {M} </script>  se ve as√≠: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.562ex" height="2.66ex" viewBox="0 -832 16603.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="1326" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="2382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="2744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6E" x="3273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="3874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="4450" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="5090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="5666" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="6196" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="6557" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="7606" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6F" x="8245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-64" x="8731" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6F" x="9254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="9740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-73" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-69" x="10821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-67" x="11166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6D" x="11647" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="12525" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="13055" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="13694" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="14271" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="14800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-4D" x="15162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="16213" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> W = tanh (\ hat {W}) \ odot \ sigma (\ hat {M}) </script></p>  <i>La <a href="">f√≥rmula</a> usa el producto de matriz de elementos</i> <br><br>  El uso de esta f√≥rmula <b>garantiza el</b> rango limitado de valores W por el intervalo [-1, 1], que est√° m√°s cerca del conjunto -1, 0, 1. Adem√°s, las funciones de esta ecuaci√≥n son <b>diferenciables</b> por par√°metros de peso.  Por lo tanto, ser√° m√°s f√°cil para nuestra capa <abbr title="Bater√≠a neuronal">NAC</abbr> aprender valores <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-10"> W </script>  utilizando el <b>gradiente de descenso y la propagaci√≥n hacia atr√°s del error</b> .  El siguiente es un diagrama de la arquitectura de la capa <abbr title="Bater√≠a neuronal">NAC</abbr> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eea/018/509/eea018509b6580a9364a7e7e91bff537.png" alt="Arquitectura de bater√≠a neuronal"><br>  <i>La arquitectura de una bater√≠a neural para el entrenamiento en funciones aritm√©ticas elementales (lineales)</i> <br><br><h2>  Implementaci√≥n de Python NAC usando Tensorflow </h2><br>  Como ya entendimos, <abbr title="Bater√≠a neuronal">NAC</abbr> es una red neuronal bastante simple (capa de red) con peque√±as caracter√≠sticas.  La siguiente es una implementaci√≥n de una sola capa <abbr title="Bater√≠a neuronal">NAC</abbr> en Python usando las bibliotecas Tensoflow y NumPy. <br><br><div class="spoiler">  <b class="spoiler_title">C√≥digo de Python</b> <div class="spoiler_text"><pre><code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf #    (<span class="hljs-type"><span class="hljs-type">NAC</span></span>)  / # -&gt;     / def nac_simple_single_layer(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>): '''  : x_in -&gt;   X out_units -&gt;     : y_out -&gt;     W -&gt;     ''' #       in_features = x_in.shape[1] #  W_hat  M_hat W_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='W_hat') M_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='M_hat') #  W   W = tf.nn.tanh(<span class="hljs-type"><span class="hljs-type">W_hat</span></span>) * tf.nn.sigmoid(<span class="hljs-type"><span class="hljs-type">M_hat</span></span>) y_out = tf.matmul(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-type"><span class="hljs-type">W</span></span>) return y_out, W</code> </pre> </div></div><br>  En el c√≥digo anterior <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11"> \ hat {W} </script>  y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-12"> \ hat {M} </script>  se inicializan usando una distribuci√≥n uniforme, pero puede usar <b>cualquier</b> m√©todo recomendado para generar una aproximaci√≥n inicial para estos par√°metros.  Puede ver la versi√≥n completa del c√≥digo en mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio de GitHub</a> (el enlace est√° duplicado al final de la publicaci√≥n). <br><br><h2>  Continuando: de la suma y la resta al NAC para expresiones aritm√©ticas complejas </h2><br>  Si bien el modelo de una red neuronal simple descrito anteriormente hace frente a las operaciones m√°s simples, como la suma y la resta, debemos ser capaces de aprender de los muchos significados de funciones m√°s complejas, como la multiplicaci√≥n, la divisi√≥n y la exponenciaci√≥n. <br><br>  A continuaci√≥n se muestra la arquitectura <abbr title="Bater√≠a neuronal">NAC</abbr> modificada, que est√° adaptada para la selecci√≥n de <b>operaciones aritm√©ticas</b> m√°s <b>complejas a</b> trav√©s del <b>logaritmo y tomar el exponente</b> dentro del modelo.  Tenga en cuenta las diferencias entre esta implementaci√≥n de <abbr title="Bater√≠a neuronal">NAC</abbr> y la ya discutida anteriormente. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/2aa/629/d752aa629eae69110dc33e828cd98430.png" alt="imagen"><br>  <i>Arquitectura <abbr title="Bater√≠a neuronal">NAC</abbr> para operaciones aritm√©ticas m√°s complejas.</i> <br><br>  Como se puede ver en la figura, logaritmos los datos de entrada antes de multiplicarlos por la matriz de pesos y luego calculamos el exponente del resultado.  La f√≥rmula para los c√°lculos es la siguiente: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.878ex" height="2.66ex" viewBox="0 -832 16739 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-59" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="1041" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-65" x="2097" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="2564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-70" x="3136" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="3640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="4029" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-62" x="5328" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-75" x="5757" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6C" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6C" x="6628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-65" x="6927" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="7393" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="7755" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6C" x="8144" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6F" x="8443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-67" x="8928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-28" x="9409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-7C" x="9798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-78" x="10077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-7C" x="10649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-2B" x="11150" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-65" x="12401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-70" x="12867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-73" x="13371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-69" x="13840" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6C" x="14186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6F" x="14484" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6E" x="14970" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="15570" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="15960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-29" x="16349" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>W</mi><mtext>&nbsp;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> Y = exp (W \ bullet (log (| x | + \ epsilon))) </script></p>  <i><a href="">La f√≥rmula de salida</a> para la segunda versi√≥n de <abbr title="Bater√≠a neuronal">NAC</abbr> .</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.942ex" height="2.419ex" viewBox="0 -780.1 3419.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-70" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-73" x="1220" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-69" x="1689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6C" x="2035" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6F" x="2333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-6E" x="2819" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-14"> \ epsilon </script></i>   <i>Aqu√≠ hay un n√∫mero muy peque√±o para evitar situaciones como log (0) durante el entrenamiento</i> <br><br>  Por lo tanto, para ambos modelos <abbr title="Bater√≠a neuronal">NAC</abbr> , el principio de funcionamiento, incluido el c√°lculo de la matriz de peso con restricciones <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-15"> W </script>  a trav√©s de <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16"> \ hat {W} </script>  y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17"> \ hat {M} </script>  no cambia  La √∫nica diferencia es el uso de operaciones logar√≠tmicas en la entrada y salida en el segundo caso. <br><br><h2>  Segunda versi√≥n de NAC en Python usando Tensorflow </h2><br>  El c√≥digo, como la arquitectura, dif√≠cilmente cambiar√°, excepto por las mejoras indicadas en el c√°lculo del tensor de los valores de salida. <br><br><div class="spoiler">  <b class="spoiler_title">C√≥digo de Python</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">#    (NAC)     # -&gt;      ,   ,      def nac_complex_single_layer(x_in, out_units, epsilon=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>): <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :return m:     :return W:     '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] W_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="W_hat") M_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="M_hat") #  W   W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) #          x_modified = tf.log(tf.abs(x_in) + epsilon) m = tf.exp(tf.matmul(x_modified, W)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> m, W</code> </pre></div></div><br><br>  Le recuerdo nuevamente que la versi√≥n completa del c√≥digo se puede encontrar en mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio de GitHub</a> (el enlace est√° duplicado al final de la publicaci√≥n). <br><br><h2>  Poniendo todo junto: una unidad l√≥gica aritm√©tica neural (NALU) </h2><br>  Como muchos ya han adivinado, podemos aprender de casi cualquier operaci√≥n aritm√©tica, combinando los dos modelos discutidos anteriormente.  Esta es la <b>idea principal de</b> <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> , que incluye una <b>combinaci√≥n ponderada de</b> <abbr title="Bater√≠a neuronal">NAC</abbr> elemental y complejo, controlada a trav√©s de una se√±al de entrenamiento.  Por lo tanto, los <abbr title="Bater√≠a neuronal">NAC</abbr> son los componentes b√°sicos para construir <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> , y si comprende su dise√±o, crear <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> ser√° f√°cil.  Si todav√≠a tiene preguntas, intente leer las explicaciones para ambos modelos <abbr title="Bater√≠a neuronal">NAC</abbr> nuevamente.  A continuaci√≥n se muestra un diagrama con la arquitectura <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad9/8e3/2ae/ad98e32aea75b4bb4f020338e9b87bf8.png" alt="imagen"><br>  <i><abbr title="Dispositivo L√≥gico Aritm√©tico Neural">Diagrama de</abbr> arquitectura <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> con explicaciones</i> <br><br>  Como se puede ver en la figura anterior, ambas unidades <abbr title="Bater√≠a neuronal">NAC</abbr> (bloques morados) dentro de la <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU se</abbr> interpolan (combinan) a trav√©s de la se√±al de entrenamiento sigmoide (bloque naranja).  Esto le permite (des) activar la salida de cualquiera de ellos, dependiendo de la funci√≥n aritm√©tica, cuyos valores estamos tratando de encontrar. <br><br>  Como se mencion√≥ anteriormente, la unidad elemental <abbr title="Bater√≠a neuronal">NAC</abbr> es una funci√≥n acumulativa, que permite a la <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> realizar operaciones lineales elementales (suma y resta), mientras que la unidad NAC compleja es responsable de la multiplicaci√≥n, divisi√≥n y exponenciaci√≥n.  <b>La salida</b> en <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> se puede representar como una f√≥rmula: <br><br><div class="spoiler">  <b class="spoiler_title">Pseudoc√≥digo</b> <div class="spoiler_text"><pre> <code class="hljs perl">Simple NAC : a = WX Complex NAC: <span class="hljs-keyword"><span class="hljs-keyword">m</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(W <span class="hljs-keyword"><span class="hljs-keyword">log</span></span>(|X| + e))  W = tanh(W_hat) * sigmoid(M_hat) <span class="hljs-comment"><span class="hljs-comment">#  G -       : g = sigmoid(GX) # , ,   NALU #  *      NALU: y = g * a + (1 - g) * m</span></span></code> </pre></div></div><br>  De la f√≥rmula <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> anterior, podemos concluir que con <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>0</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-30" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-18"> g = 0 </script>  la red neuronal seleccionar√° solo valores para operaciones aritm√©ticas complejas, pero no para operaciones elementales;  y viceversa, en el caso de <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMAIN-31" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-19"> g = 1 </script>  .  Por lo tanto, en general, <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> puede aprender cualquier operaci√≥n aritm√©tica que consista en sumar, restar, multiplicar, dividir y elevar a una potencia y extrapolar con √©xito el resultado m√°s all√° de los l√≠mites de los intervalos de los valores de los datos de origen. <br><br><h2>  Implementaci√≥n de Python NALU usando Tensorflow </h2><br>  En la implementaci√≥n de <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU,</abbr> utilizaremos el <abbr title="Bater√≠a neuronal">NAC</abbr> elemental y complejo, que ya hemos definido. <br><br><div class="spoiler">  <b class="spoiler_title">C√≥digo de Python</b> <div class="spoiler_text"><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nalu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x_in, out_units, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.000001</span></span></span></span><span class="hljs-function"><span class="hljs-params">, get_weights=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :param get_weights:   True      :return y_out:     :return G: o   :return W_simple:    NAC1 ( NAC) :return W_complex:    NAC2 ( NAC) '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#     NAC a, W_simple = nac_simple_single_layer(x_in, out_units) #     NAC m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon=epsilon) #    G = tf.get_variable(shape=[in_shape, out_units], initializer=tf.random_normal_initializer(stddev=1.0), trainable=True, name="Gate_weights") g = tf.nn.sigmoid(tf.matmul(x_in, G)) y_out = g * a + (1 - g) * m if(get_weights): return y_out, G, W_simple, W_complex else: return y_out</span></span></code> </pre></div></div><br>  Una vez m√°s, noto que en el c√≥digo anterior, nuevamente inicialic√© la matriz de par√°metros <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.827ex" height="2.057ex" viewBox="0 -780.1 786.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/422777/&amp;usg=ALkJrhjMb5nUCy9Gc7z3zARkO7lWsmCWKg#MJMATHI-47" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-20"> G </script>  usando una distribuci√≥n uniforme, pero puede usar <b>cualquier</b> forma recomendada para generar una aproximaci√≥n inicial. <br><hr><br><h2>  Resumen </h2><br>  Para m√≠ personalmente, la idea de <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> es un gran avance en el campo de la IA, especialmente en las redes neuronales, y parece prometedor.  Este enfoque puede abrir la puerta a aquellas √°reas de aplicaci√≥n donde las redes neuronales est√°ndar no podr√≠an hacer frente. <br><br>  Los autores del art√≠culo hablan sobre varios experimentos usando <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> : desde seleccionar valores de funciones aritm√©ticas elementales hasta contar el n√∫mero de d√≠gitos escritos a mano en una serie dada de im√°genes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MNIST</a> , lo que permite que las redes neuronales verifiquen programas de computadora. <br><br>  Los resultados causan una impresi√≥n sorprendente y demuestran que <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> hace frente a <b>casi cualquier tarea</b> relacionada con la representaci√≥n num√©rica, mejor que los modelos est√°ndar de redes neuronales.  Animo a los lectores a familiarizarse con los resultados de los experimentos para comprender mejor c√≥mo y d√≥nde puede ser √∫til el modelo <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> . <br><br>  Sin embargo, debe recordarse que ni <abbr title="Bater√≠a neuronal">NAC</abbr> ni <abbr title="Dispositivo L√≥gico Aritm√©tico Neural">NALU</abbr> son la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">soluci√≥n ideal</a> para cualquier tarea.  M√°s bien, representan la idea general de c√≥mo crear modelos para una clase particular de operaciones aritm√©ticas. <br><hr><br>  A continuaci√≥n hay un enlace a mi repositorio de GitHub, que contiene la implementaci√≥n completa del c√≥digo del art√≠culo. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">github.com/faizan2786/nalu_implementation</a> <br><br>  Puede verificar independientemente el funcionamiento de mi modelo en varias funciones seleccionando hiperpar√°metros para una red neuronal.  Haga preguntas y comparta sus pensamientos en los comentarios de esta publicaci√≥n, y har√© todo lo posible para responderle. <br><br>  <b>PD (del autor): esta es mi primera publicaci√≥n escrita, as√≠ que si tienes alg√∫n consejo, sugerencia y recomendaci√≥n para el futuro (tanto t√©cnico como general), escr√≠beme.</b> <br><br>  PPS (del traductor): si tiene comentarios sobre la traducci√≥n o el texto, escr√≠bame un mensaje personal.  Estoy especialmente interesado en la redacci√≥n de la se√±al de puerta aprendida: no estoy seguro de poder traducir este t√©rmino con precisi√≥n. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es422777/">https://habr.com/ru/post/es422777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es422767/index.html">Conferencia DEFCON 16. Fedor, InSecure.org Hacker. Escaneo NMAP en l√≠nea</a></li>
<li><a href="../es422769/index.html">Ganadores de Startup Battlefield TechCrunch Disrupt San Francisco 2018</a></li>
<li><a href="../es422771/index.html">Las reglas del dise√±o, alcanzando un nuevo nivel y pensamiento de dise√±o.</a></li>
<li><a href="../es422773/index.html">NVIDIA Revelando los secretos de la arquitectura de GPU Turing de pr√≥xima generaci√≥n: seguimiento de doble rayo, GDDR6 y m√°s</a></li>
<li><a href="../es422775/index.html">Conferencia DEFCON 22. Andrew "Zoz" Brooks. ¬°No lo arruines! Parte 1</a></li>
<li><a href="../es422781/index.html">Fintech digest: SWIFT continuar√° trabajando en la Federaci√≥n de Rusia, VISA le permitir√° transferir fondos por n√∫mero de tel√©fono, costosos datos biom√©tricos</a></li>
<li><a href="../es422783/index.html">Mejor, m√°s r√°pido, m√°s potente: componentes con estilo v4</a></li>
<li><a href="../es422785/index.html">Digitalizaci√≥n de f√°brica: una mirada al frente</a></li>
<li><a href="../es422787/index.html">Grandes cambios en las principales arquitecturas de chips</a></li>
<li><a href="../es422789/index.html">@Pythonetc Aug 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>