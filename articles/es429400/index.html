<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👊🏿 🎌 👎🏿 Seals vs red neuronal 2. O ejecuta SqueezeNet v.1.1 en Raspberry Zero en tiempo real (casi) 🛂 🥉 🚃</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! 

 Después de escribir la primera parte, que no era muy grave ni particularmente útil en términos prácticos, mi conciencia me tragó un p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Seals vs red neuronal 2. O ejecuta SqueezeNet v.1.1 en Raspberry Zero en tiempo real (casi)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/429400/">  Hola a todos! <br><br>  Después de escribir la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera parte, que</a> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">era</a> muy grave ni particularmente útil en términos prácticos, mi conciencia me tragó un poco.  Y decidí terminar lo que comencé.  Es decir, elegir la misma implementación de una red neuronal para que se ejecute en Rasperry Pi Zero W en tiempo real (por supuesto, tanto como sea posible en dicho hardware).  Para expulsarla de los datos de la vida real e iluminar los resultados en Habré. <br><br>  Precaución  Hay un código viable y algunos gatos más debajo del corte que en la primera parte.  En la imagen, cuna y bacalao, respectivamente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_n/hp/-p/_nhp-pj5btxq5w0yvmwdnd-kguc.jpeg" alt="imagen"></div><a name="habracut"></a><br><h2>  ¿Qué red elegir? </h2><br>  Recuerdo que debido a la debilidad del hierro de frambuesa, la elección de las realizaciones de la red neuronal es pequeña.  A saber: <br><br>  1. SqueezeNet. <br>  2. YOLOv3 Tiny. <br>  3. MobileNet. <br>  4. ShuffleNet. <br><br>  ¿Cuán correcta fue la elección a favor de SqueezeNet en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera parte</a> ? ... Ejecutar cada una de las redes neuronales mencionadas anteriormente en su hardware es un evento bastante largo.  Por lo tanto, atormentado por vagas dudas, decidí buscar en Google si alguien había hecho esa pregunta antes que yo.  Resultó que se lo preguntó y lo investigó en detalle.  Los que lo deseen pueden consultar la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fuente</a> .  Me limitaré a una sola imagen: <br><br><img src="https://habrastorage.org/webt/gh/qq/_r/ghqq_rgrl9wjl13hsf7mtrbsyj4.png" alt="imagen"><br><br>  De la imagen se deduce que el tiempo de procesamiento para una imagen para diferentes modelos entrenados en el conjunto de datos ImageNet es el menor con SqueezeNet v.1.1.  Tomaremos esto como una guía de acción.  YOLOv3 no se incluyó en la comparación, pero, por lo que recuerdo, YOLO es más caro que MobileNet.  Es decir  También debería ser inferior en velocidad a SqueezeNet. <br><br><h2>  Implementación de la red seleccionada. </h2><br>  Los pesos y la topología de SqueezeNet entrenados en el conjunto de datos ImageNet (marco Caffe) se pueden encontrar en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> .  Por si acaso, descargué ambas versiones para que luego se puedan comparar.  ¿Por qué ImageNet?  Este conjunto de todos los disponibles tiene el número máximo de clases (1000 piezas), por lo que los resultados de la red neuronal prometen ser bastante interesantes. <br><br>  Esta vez veremos cómo el Raspberry Zero trata con el reconocimiento de fotogramas de la cámara.  Aquí está, nuestro humilde trabajador del post de hoy: <br><br><img src="https://habrastorage.org/webt/uu/vw/4a/uuvw4axgowtwqs1l-7q6wq94ej4.jpeg" alt="imagen"><br><br>  Tomé el código fuente del blog de Adrian Rosebrock mencionado en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primera parte</a> como la base del código, es decir, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de aquí</a> .  Pero tuve que ararlo significativamente: <br><br>  1. Reemplace su modelo con MobileNetSSD en SqueezeNet. <br>  2. La implementación de la Cláusula 1 ha llevado a la expansión del número de clases a 1000. Pero al mismo tiempo, la función de resaltar objetos con marcos multicolores (SSD funcional), por desgracia, se había eliminado. <br>  3. Eliminar la recepción de argumentos a través de la línea de comando (por alguna razón, esa entrada de parámetros me molesta). <br>  4. Elimine el método VideoStream y con él la biblioteca imutils que Adrian ama.  Inicialmente, el método se utilizó para obtener la transmisión de video de la cámara.  Pero con mi cámara conectada al Raspberry Zero, estúpidamente no funcionó, dando algo así como "instrucción ilegal". <br>  5. Agregue la velocidad de fotogramas (FPS) a la imagen reconocida, vuelva a escribir el cálculo de FPS. <br>  6. Haga marcos de guardado para escribir esta publicación. <br><br>  En la frambuesa con Rapbian Stretch OS, Python 3.5.3 e instalado a través de pip3 install OpenCV 3.4.1, resultó lo siguiente y comenzó: <br><br><div class="spoiler">  <b class="spoiler_title">Codigo aqui</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> picamera <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> picamera.array <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PiRGBArray <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-comment"><span class="hljs-comment">#    prototxt = 'models/squeezenet_v1.1.prototxt' model = 'models/squeezenet_v1.1.caffemodel' labels = 'models/synset_words.txt' #    rows = open(labels).read().strip().split("\n") classes = [r[r.find(" ") + 1:].split(",")[0] for r in rows] #    print("[INFO] loading model...") net = cv2.dnn.readNetFromCaffe(prototxt, model) print("[INFO] starting video stream...") #   camera = picamera.PiCamera() camera.resolution = (640, 480) camera.framerate = 25 #   camera.start_preview() sleep(1) camera.stop_preview() #     raw rawCapture = PiRGBArray(camera) #   FPS t0 = time.time() #     for frame in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True): #    blob frame = rawCapture.array blob = cv2.dnn.blobFromImage(frame, 1, (224, 224), (104, 117, 124)) #    blob,     net.setInput(blob) preds = net.forward() preds = preds.reshape((1, len(classes))) idxs = int(np.argsort(preds[0])[::-1][:1]) #  FPS FPS = 1/(time.time() - t0) t0 = time.time() #    ,   FPS,    text = "Label: {}, p = {:.2f}%, fps = {:.2f}".format(classes[idxs], preds[0][idxs] * 100, FPS) cv2.putText(frame, text, (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2) print(text) cv2.imshow("Frame", frame) #     Raspberry fname = 'pic_' + dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.jpg' cv2.imwrite(fname, frame) #    SD  key = cv2.waitKey(1) &amp; 0xFF #    `q`    if key == ord("q"): break #   raw       rawCapture.truncate(0) print("[INFO] video stream is terminated") #    cv2.destroyAllWindows() camera.close()</span></span></code> </pre> <br></div></div><br><h2>  Resultados </h2><br>  El código se muestra en la pantalla del monitor conectado a la Frambuesa, el siguiente marco reconocido en este formulario.  En la parte superior del cuadro, solo se muestra la clase más probable. <br><br><img src="https://habrastorage.org/webt/io/65/7s/io657sevsvfzlgnq803buqvv-mo.jpeg" alt="imagen"><br><br>  Entonces, un mouse de computadora fue identificado como un mouse con una probabilidad muy alta.  Al mismo tiempo, las imágenes se actualizan a una frecuencia de 0,34 FPS (es decir, aproximadamente cada tres segundos).  Es un poco molesto sostener la cámara y esperar a que se procese el siguiente cuadro, pero puedes vivir.  Por cierto, si elimina el marco de guardado en la tarjeta SD, la velocidad de procesamiento aumentará a 0.37 ... 0.38 FPS.  Seguramente, hay otras formas de dispersarse.  Esperaremos y veremos, en cualquier caso, dejaremos esta pregunta para las próximas publicaciones. <br><br>  Por separado, me disculpo por el balance de blancos.  El hecho es que la cámara IR con la luz de fondo encendida estaba conectada a Rapberry, por lo que la mayoría de los cuadros se ven bastante extraños.  Pero cuanto más valioso es cada golpe de la red neuronal.  Obviamente, el balance de blancos en el conjunto de entrenamiento fue más correcto.  Además, decidí insertar solo los cuadros sin formato, para que el lector los vea de la misma manera que ven la red neuronal. <br><br>  Primero, comparemos el trabajo de SqueezeNet versiones 1.0 (en el marco izquierdo) y 1.1 (en el derecho): <br><br><img src="https://habrastorage.org/webt/zc/rs/ao/zcrsaocvvwb7nmdhi3zg6v-fuwe.jpeg" alt="imagen"><br><br>  Se puede ver que la versión 1.1 funciona dos veces y cuarto más rápido que 1.0 (0.34 FPS versus 0.15).  La ganancia de velocidad es palpable.  No vale la pena sacar conclusiones sobre la precisión del reconocimiento en este ejemplo, ya que la precisión depende en gran medida de la posición de la cámara con respecto al objeto, la iluminación, el resplandor, las sombras, etc. <br><br>  En vista de una ventaja de velocidad tan importante v1.1 sobre v.1.0 en el futuro, solo se utilizó SqueezeNet v.1.1.  Para evaluar el rendimiento del modelo, apunté la cámara a varios objetos que <s>vinieron a la mano</s> y recibí los siguientes cuadros en la salida: <br><br><img src="https://habrastorage.org/webt/lo/nq/6v/lonq6vrvcdqtyld8eowva01yn6a.jpeg" alt="imagen"><br><br>  Un teclado es peor que un mouse.  Quizás en el conjunto de entrenamiento, la mayoría de los teclados eran blancos. <br><br><img src="https://habrastorage.org/webt/ds/2j/et/ds2jete8bvrzzvq1wtlvhvxad_a.jpeg" alt="imagen"><br><br>  Un teléfono celular está bastante bien definido si enciende la pantalla.  Una celda con una pantalla apagada no cuenta una red neuronal como una celda. <br><br><img src="https://habrastorage.org/webt/ke/zy/hs/kezyhs6o4dutrhqyjys0b7axyf0.jpeg" alt="imagen"><br><br>  Una taza vacía se define bastante tolerablemente como una taza de café.  Hasta ahora, todo va bastante bien. <br><br><img src="https://habrastorage.org/webt/co/xk/fi/coxkfidvlt1vgcec0rgvcop4yq4.jpeg" alt="imagen"><br><br>  Las tijeras están peor; la red las define obstinadamente como una pinza para el cabello.  Sin embargo, entrar en el manzano si no en la diana) <br><br><h2>  Vamos a complicar la tarea. </h2><br>  Tratemos de poner algo complicado en la red neuronal del <s>cerdo</s> .  Acabo de encontrar un juguete casero para niños.  Creo que la mayoría de los lectores lo reconocen como un gato de juguete.  Me pregunto qué será considerado por nuestra rudimentaria inteligencia artificial. <br><br><img src="https://habrastorage.org/webt/ye/vr/1b/yevr1bf2unalpdxfuta9txi8g3y.jpeg" alt="imagen"><br><br>  En el marco de la izquierda, la luz IR borró todas las tiras de la tela.  Como resultado, el juguete se definió como una máscara de oxígeno con una probabilidad bastante decente.  Por que no  La forma del juguete realmente se parece a una máscara de oxígeno. <br><br>  En el marco de la derecha, cubrí mis dedos con un reflector IR, por lo que las rayas aparecieron en el juguete y el balance de blancos se hizo más creíble.  En realidad, este es el único marco que se ve más o menos normal en esta publicación.  Pero la red neuronal tiene tanta abundancia de detalles en la imagen confundida.  Ella identificó el juguete como una sudadera.  Debo decir que esto tampoco parece un "dedo en el cielo".  Golpea si no está en el "manzano", luego al menos en el huerto de manzanas). <br><br>  Bueno, nos acercamos sin problemas al clímax de nuestra acción.  El ganador destacado de la batalla, consagrado en detalle en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primer puesto,</a> entra al ring.  Y saca fácilmente el cerebro de nuestra red neuronal desde los primeros fotogramas. <br><br><img src="https://habrastorage.org/webt/8l/po/hz/8lpohzb3ijzpmvljyczip7w8uzq.jpeg" alt="imagen"><br><br>  Es curioso que un gato prácticamente no cambie su posición, pero cada vez se determina de manera diferente.  Y desde esta perspectiva, es más similar a una mofeta.  En segundo lugar, se parece a un hámster.  Intentemos cambiar el ángulo. <br><br><img src="https://habrastorage.org/webt/u_/hj/jc/u_hjjc5yj3fdnnm4msadfl-pzys.jpeg" alt="imagen"><br><br>  Sí, si fotografias al gato desde arriba, se determina correctamente, pero si cambias un poco la posición del cuerpo del gato en el marco, para la red neuronal se convierte en un perro, del husky siberiano y malamute (perro de trineo esquimal), respectivamente. <br><br><img src="https://habrastorage.org/webt/cd/up/oy/cdupoyl2mai8a6ctxorqtdw12mi.jpeg" alt="imagen"><br><br>  Y esta selección es hermosa ya que un perro de diferentes razas se define en cada marco separado de un gato.  Y las razas no se repiten) <br><br><img src="https://habrastorage.org/webt/nx/nj/c4/nxnjc45ke4ieyvyreptl2i0nyzw.jpeg" alt="imagen"><br><br>  Por cierto, hay posturas en las que las redes neuronales se hacen obvias de que todavía es un gato, no un perro.  Es decir, SqueezeNet v.1.1 todavía logró probarse incluso en un objeto tan difícil de analizar.  Dado el éxito de la red neuronal al reconocer objetos al comienzo de la prueba y al reconocer a un gato como un gato al final, esta vez declaramos un sólido empate de combate) <br><br>  Bueno eso es todo.  Invito a todos a probar el código propuesto en su frambuesa y cualquier objeto que haya aparecido en la vista de objetos animados e inanimados.  Estaré especialmente agradecido con aquellos que miden el FPS en el Rapberry Pi B +.  Prometo incluir los resultados en esta publicación con referencia a la persona que envió los datos.  ¡Creo que debería resultar significativamente más de 1 FPS! <br><br>  Espero que parte de la información de esta publicación sea útil para fines de entretenimiento o educativos, y que alguien incluso pueda tener nuevas ideas. <br><br>  ¡Que tengas una buena semana laboral!  Y hasta pronto) <br><br><img src="https://habrastorage.org/webt/a9/-_/gx/a9-_gxtb2qzdzz5nbtbbet66wlg.jpeg" alt="imagen"><br><br>  UPD1: en el Raspberry Pi 3B +, el script anterior funciona a una frecuencia de 2 con un pequeño FPS. <br><br>  UPD2: en RPi 3B + con Movidius NCS, el script se ejecuta a 6 FPS. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es429400/">https://habr.com/ru/post/es429400/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es429388/index.html">Cómo hacer un rediseño de un sitio web y no crear problemas: 4 pasos importantes</a></li>
<li><a href="../es429390/index.html">Curso MIT "Seguridad de sistemas informáticos". Lección 16: "Ataques de canal lateral", Parte 1</a></li>
<li><a href="../es429392/index.html">Curso MIT "Seguridad de sistemas informáticos". Lección 16: Ataques a través del canal lateral, Parte 2</a></li>
<li><a href="../es429394/index.html">Curso MIT "Seguridad de sistemas informáticos". Lección 16: "Ataques de canal lateral", Parte 3</a></li>
<li><a href="../es429396/index.html">Cómo probar una aplicación cuando interactúa con la API usando SoapUI</a></li>
<li><a href="../es429402/index.html">ML.NET 0.7 (Machine Learning .NET)</a></li>
<li><a href="../es429404/index.html">8 segundos y medio para priorizar la funcionalidad</a></li>
<li><a href="../es429406/index.html">"Monstruos en los juegos o cómo crear miedo"</a></li>
<li><a href="../es429410/index.html">22 puertos SSH para transportar o no</a></li>
<li><a href="../es429414/index.html">El futuro del video VR: el VR180 de Google</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>