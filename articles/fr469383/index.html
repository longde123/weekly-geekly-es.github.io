<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéÄ üïí üë©‚Äç‚öïÔ∏è Solution hyperconverg√©e AERODISK vAIR. Base - Syst√®me de fichiers ARDFS üôÜüèø ü§∑üèº üë®üèΩ‚Äçü§ù‚Äçüë®üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Salut, lecteurs de Habr. Avec cet article, nous ouvrons un cycle qui parlera du syst√®me hyperconverg√© AERODISK vAIR que nous avons d√©velopp√©. Au d√©par...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Solution hyperconverg√©e AERODISK vAIR. Base - Syst√®me de fichiers ARDFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/469383/"><p><img src="https://habrastorage.org/webt/sk/n3/zh/skn3zhr0fgbcuqfar5ozlg0v2tk.jpeg"></p><br><p>  Salut, lecteurs de Habr.  Avec cet article, nous ouvrons un cycle qui parlera du syst√®me hyperconverg√© AERODISK vAIR que nous avons d√©velopp√©.  Au d√©part, nous voulions que le premier article raconte tout sur tout, mais le syst√®me est assez complexe, nous allons donc manger un √©l√©phant en plusieurs parties. </p><br><p>  Commen√ßons l'histoire par l'histoire du syst√®me, approfondissons le syst√®me de fichiers ARDFS, qui est le fondement de vAIR, et parlons √©galement un peu du positionnement de cette solution sur le march√© russe. </p><br><p>  Dans les prochains articles, nous parlerons davantage des diff√©rents composants architecturaux (cluster, hyperviseur, √©quilibreur de charge, syst√®me de surveillance, etc.), du processus de configuration, nous soul√®verons des probl√®mes de licence, afficherons s√©par√©ment les tests de plantage et, bien s√ªr, √©crirons sur les tests de charge et dimensionnement.  Nous consacrerons √©galement un article s√©par√© √† la version communautaire de vAIR. </p><a name="habracut"></a><br><h2 id="aerodisk---eto-vrode-istoriya-pro-shd-ili-zachem-my-voobsche-nachali-zanimatsya-giperkonvergentom">  Un airdisc est-il une histoire de stockage?  Ou pourquoi avons-nous m√™me commenc√© √† hyperconverger? </h2><br><p>  Au d√©part, l'id√©e de cr√©er notre propre hyperconvergent nous est venue vers 2010.  Ensuite, il n'y avait pas d'Aerodisk et de solutions similaires (syst√®mes commerciaux hyperconverg√©s en bo√Æte) sur le march√©.  Notre t√¢che √©tait la suivante: √† partir d'un ensemble de serveurs avec des disques locaux connect√©s par une interconnexion via Ethernet, nous devions faire un stockage √©tendu et ex√©cuter des machines virtuelles et un r√©seau logiciel au m√™me endroit.  Tout cela devait √™tre mis en ≈ìuvre sans syst√®mes de stockage (car il n'y avait tout simplement pas d'argent pour le stockage et son regroupement, et nous n'avions pas encore invent√© notre propre syst√®me de stockage). </p><br><p>  Nous avons essay√© de nombreuses solutions open source et avons toujours r√©solu ce probl√®me, mais la solution √©tait tr√®s compliqu√©e et difficile √† r√©p√©ter.  De plus, cette d√©cision appartenait √† la cat√©gorie ¬´Works?  Ne touchez pas! "  Par cons√©quent, apr√®s avoir r√©solu ce probl√®me, nous n'avons pas d√©velopp√© l'id√©e de transformer le r√©sultat de notre travail en un produit √† part enti√®re. </p><br><p> Apr√®s cet incident, nous nous sommes √©loign√©s de cette id√©e, mais nous avions toujours le sentiment que cette t√¢che √©tait compl√®tement r√©soluble, et les avantages d'une telle solution √©taient plus qu'√©vidents.  Par la suite, les produits HCI des soci√©t√©s √©trang√®res qui ont √©t√© lib√©r√©s n'ont fait que confirmer ce sentiment. </p><br><p>  Ainsi, mi-2016, nous sommes revenus sur cette t√¢che dans le cadre de la cr√©ation d'un produit √† part enti√®re.  Ensuite, nous n'avions pas encore de relations avec les investisseurs, nous avons donc d√ª acheter un stand de d√©veloppement pour notre argent pas tr√®s gros.  Apr√®s avoir tap√© sur les serveurs et commutateurs Avito BU-shyh, nous nous sommes mis au travail. </p><br><p><img src="https://habrastorage.org/webt/wy/ir/jr/wyirjr50guvzpnolcvdvniyn2mo.jpeg"></p><br><p>  La principale t√¢che initiale √©tait de cr√©er votre propre, bien que simple, mais votre propre syst√®me de fichiers, qui serait capable de distribuer automatiquement et uniform√©ment les donn√©es sous forme de blocs virtuels sur le ni√®me nombre de n≈ìuds de cluster interconnect√©s via Ethernet.  Dans ce cas, le FS doit √™tre bien et facilement mis √† l'√©chelle et ind√©pendant des syst√®mes adjacents, c'est-√†-dire  √™tre ali√©n√© de vAIR sous forme de ¬´stockage juste¬ª. </p><br><p><img src="https://habrastorage.org/webt/il/vk/zs/ilvkzsyjkyr6pkcgoqs_ibumyus.jpeg"></p><br><p>  VAIR First Concept </p><br><p><img src="https://habrastorage.org/webt/h1/e0/pd/h1e0pda3j1ebbxls_cn_gmzm5ug.jpeg"></p><br><p>  Nous avons intentionnellement refus√© d'utiliser des solutions open source pr√™tes √† l'emploi pour organiser un stockage √©tendu (ceph, gluster, lustre et similaires) en faveur de notre d√©veloppement, car nous avions d√©j√† une grande exp√©rience de projet avec eux.  Bien s√ªr, ces solutions elles-m√™mes sont merveilleuses et avant de travailler sur Aerodisk, nous avons impl√©ment√© plus d'un projet d'int√©gration avec elles.  Mais c'est une chose de r√©aliser la t√¢che sp√©cifique d'un client, de former le personnel et, √©ventuellement, d'acheter du support pour un grand fournisseur, et c'est tout autre chose de cr√©er un produit facilement reproductible qui sera utilis√© pour diverses t√¢ches, que nous, en tant que fournisseur, pouvons m√™me nous conna√Ætre. nous ne le ferons pas.  Pour le deuxi√®me objectif, les produits open source existants ne nous convenaient pas, nous avons donc d√©cid√© de voir nous-m√™mes le syst√®me de fichiers distribu√©. <br>  Deux ans plus tard, plusieurs d√©veloppeurs (qui combinaient le travail sur vAIR avec le travail sur le moteur de stockage classique) ont atteint un certain r√©sultat. </p><br><p>  En 2018, nous avions √©crit le syst√®me de fichiers le plus simple et l'avons compl√©t√© avec la liaison n√©cessaire.  Le syst√®me a int√©gr√© des disques physiques (locaux) de diff√©rents serveurs dans un pool plat via une interconnexion interne et les a coup√©s en blocs virtuels, puis des p√©riph√©riques de bloc avec diff√©rents degr√©s de tol√©rance aux pannes ont √©t√© cr√©√©s √† partir de blocs virtuels, sur lesquels des hyperviseurs KVM virtuels ont √©t√© cr√©√©s et ex√©cut√©s voitures. </p><br><p>  Nous ne nous sommes pas souci√©s du nom du syst√®me de fichiers et l'avons appel√© succinctement ARDFS (devinez comment il d√©chiffre)) </p><br><p>  Ce prototype avait l'air bien (pas visuellement, bien s√ªr, il n'y avait pas de conception visuelle √† l'√©poque) et a montr√© de bons r√©sultats en termes de performances et de mise √† l'√©chelle.  Apr√®s le premier r√©sultat r√©el, nous avons fix√© le cap pour ce projet, apr√®s avoir organis√© un environnement de d√©veloppement √† part enti√®re et une √©quipe distincte qui n'√©tait engag√©e que dans vAIR. </p><br><p>  √Ä ce moment-l√†, l'architecture g√©n√©rale de la solution √©tait arriv√©e √† maturit√© et n'avait jusqu'√† pr√©sent pas subi de changements majeurs. </p><br><h2 id="pogruzhaemsya-v-faylovuyu-sistemu-ardfs">  Plonger dans le syst√®me de fichiers ARDFS </h2><br><p>  ARDFS est la base de vAIR, qui fournit un stockage de basculement distribu√© de l'ensemble du cluster.  Une (mais pas la seule) caract√©ristique distinctive d'ARDFS est qu'il n'utilise pas de serveurs d√©di√©s suppl√©mentaires pour la m√©ta et la gestion.  Il s'agissait √† l'origine de simplifier la configuration de la solution et de sa fiabilit√©. </p><br><h3 id="struktura-hraneniya">  Structure de stockage </h3><br><p>  Dans tous les n≈ìuds de cluster, ARDFS organise un pool logique √† partir de tout l'espace disque disponible.  Il est important de comprendre qu'un pool n'est pas encore des donn√©es et pas de l'espace format√©, mais simplement du balisage, c'est-√†-dire  tous les n≈ìuds sur lesquels vAIR est install√© lorsqu'ils sont ajout√©s au cluster sont automatiquement ajout√©s au pool ARDFS partag√© et les ressources de disque sont automatiquement partag√©es sur l'ensemble du cluster (et disponibles pour le stockage futur des donn√©es).  Cette approche vous permet d'ajouter et de supprimer des n≈ìuds √† la vol√©e sans aucun impact s√©rieux sur un syst√®me d√©j√† en cours d'ex√©cution.  C'est-√†-dire  le syst√®me est tr√®s facile √† mettre √† l'√©chelle avec des ¬´briques¬ª, en ajoutant ou supprimant des n≈ìuds dans le cluster si n√©cessaire. </p><br><p>  Des disques virtuels (objets de stockage pour les machines virtuelles) sont ajout√©s au-dessus du pool ARDFS, qui sont construits √† partir de blocs virtuels de 4 m√©gaoctets.  Les disques virtuels stockent directement les donn√©es.  Au niveau du disque virtuel, un sch√©ma de tol√©rance aux pannes est √©galement d√©fini. </p><br><p>  Comme vous l'avez peut-√™tre devin√©, pour la tol√©rance aux pannes du sous-syst√®me de disques, nous n'utilisons pas le concept de RAID (tableau redondant de disques ind√©pendants), mais utilisons RAIN (tableau redondant de n≈ìuds ind√©pendants).  C'est-√†-dire  la tol√©rance aux pannes est mesur√©e, automatis√©e et g√©r√©e en fonction des n≈ìuds et non des disques.  Les disques, bien s√ªr, sont √©galement un objet de stockage, ils sont, comme tout le reste, surveill√©s, vous pouvez effectuer toutes les op√©rations standard avec eux, y compris la cr√©ation de RAID mat√©riel local, mais le cluster fonctionne avec des n≈ìuds. </p><br><p>  Dans une situation o√π vous voulez vraiment du RAID (par exemple, un sc√©nario qui prend en charge plusieurs √©checs sur de petits clusters), rien ne vous emp√™che d'utiliser des contr√¥leurs RAID locaux et de faire du stockage √©tendu et une architecture RAIN par-dessus.  Ce sc√©nario est assez vivant et est pris en charge par nous, nous en parlerons donc dans un article sur les sc√©narios typiques d'utilisation de vAIR. </p><br><h3 id="shemy-otkazoustoychivosti-hranilischa">  Sch√©mas de basculement du stockage </h3><br><p>  Il peut y avoir deux sch√©mas de r√©silience de disque virtuel vAIR: </p><br><p>  1) Facteur de r√©plication ou simplement r√©plication - cette m√©thode de tol√©rance aux pannes est simple ¬´comme un b√¢ton et une corde¬ª.  La r√©plication synchrone entre les n≈ìuds avec un facteur 2 (2 copies par cluster) ou 3 (3 copies, respectivement) est effectu√©e.  RF-2 permet √† un disque virtuel de r√©sister √† une d√©faillance d'un n≈ìud dans un cluster, mais ¬´mange¬ª la moiti√© du volume utilisable, et RF-3 r√©sistera √† une d√©faillance de 2 n≈ìuds dans un cluster, mais il r√©servera 2/3 du volume utilisable √† ses besoins.  Ce sch√©ma est tr√®s similaire √† RAID-1, c'est-√†-dire qu'un disque virtuel configur√© dans RF-2 est r√©sistant √† la d√©faillance de l'un des n≈ìuds du cluster.  Dans ce cas, les donn√©es seront correctes et m√™me les E / S ne s'arr√™teront pas.  Lorsqu'un n≈ìud tomb√© redevient op√©rationnel, la r√©cup√©ration / synchronisation automatique des donn√©es commence. </p><br><p>  Les exemples suivants illustrent la distribution des donn√©es RF-2 et RF-3 en mode normal et en situation de panne. </p><br><p>  Nous avons une machine virtuelle d'une capacit√© de 8 Mo de donn√©es uniques (utiles) qui s'ex√©cute sur 4 n≈ìuds vAIR.  Il est clair qu'en r√©alit√©, il est peu probable qu'il y ait une si petite quantit√©, mais pour un sch√©ma qui refl√®te la logique des ARDFS, cet exemple est le plus compr√©hensible.  AB sont des blocs virtuels de 4 Mo contenant des donn√©es de machine virtuelle uniques.  Avec RF-2, deux copies de ces blocs A1 + A2 et B1 + B2 sont cr√©√©es, respectivement.  Ces blocs sont ¬´dispos√©s¬ª par des n≈ìuds, en √©vitant l'intersection des m√™mes donn√©es sur le m√™me n≈ìud, c'est-√†-dire que la copie A1 ne sera pas sur la m√™me note que la copie A2.  Avec B1 et B2, c'est similaire. </p><br><p><img src="https://habrastorage.org/webt/ho/xm/oh/hoxmohhemj_whmr38pvgfyfousm.png"></p><br><p>  En cas de d√©faillance de l'un des n≈ìuds (par exemple, le n≈ìud 3, qui contient une copie de B1), cette copie est automatiquement activ√©e sur le n≈ìud o√π il n'y a pas de copie de sa copie (c'est-√†-dire la copie B2). </p><br><p><img src="https://habrastorage.org/webt/ex/xl/o4/exxlo4frqr3crhwbh_orvlcwl9g.png"></p><br><p>  Ainsi, le disque virtuel (et les VM, respectivement) survivra facilement √† la d√©faillance d'un n≈ìud dans le sch√©ma RF-2. </p><br><p>  Un circuit avec r√©plication, avec sa simplicit√© et sa fiabilit√©, souffre de la m√™me plaie que RAID1 - il y a peu d'espace utilisable. </p><br><p>  2) Le codage d'effacement ou de suppression (√©galement connu sous le nom de ¬´codage redondant¬ª, ¬´codage d'effacement¬ª ou ¬´code de redondance¬ª) n'existe que pour r√©soudre le probl√®me ci-dessus.  EC est un sch√©ma de redondance qui offre une haute disponibilit√© des donn√©es avec moins de surcharge de disque par rapport √† la r√©plication.  Le principe de fonctionnement de ce m√©canisme est similaire √† RAID 5, 6, 6P. </p><br><p>  Lors de l'encodage, le processus EC divise le bloc virtuel (4 Mo par d√©faut) en plusieurs ¬´morceaux de donn√©es¬ª plus petits selon le sch√©ma EC (par exemple, un sch√©ma 2 + 1 divise chaque bloc de 4 Mo en 2 morceaux de 2 Mo chacun).  En outre, ce processus g√©n√®re des ¬´morceaux de parit√©¬ª pour des ¬´√©l√©ments de donn√©es¬ª d'au plus une des parties pr√©c√©demment s√©par√©es.  Lors du d√©codage, l'EC g√©n√®re les pi√®ces manquantes, lisant les donn√©es ¬´survivantes¬ª sur l'ensemble du cluster. </p><br><p>  Par exemple, un disque virtuel avec un sch√©ma EC 2 + 1, impl√©ment√© sur 4 n≈ìuds d'un cluster, peut facilement r√©sister √† une d√©faillance de n≈ìud unique dans un cluster de la m√™me mani√®re que RF-2.  Dans le m√™me temps, les frais g√©n√©raux seront inf√©rieurs, en particulier, le facteur de capacit√© avec RF-2 est de 2 et avec EC 2 + 1, il sera de 1,5. </p><br><p>  S'il est plus facile √† d√©crire, le r√©sultat inf√©rieur est que le bloc virtuel est divis√© en 2-8 (pourquoi de 2 √† 8 voir ci-dessous) "morceaux", et pour ces morceaux les "morceaux" de parit√© du m√™me volume sont calcul√©s. </p><br><p>  Par cons√©quent, les donn√©es et la parit√© sont r√©parties uniform√©ment sur tous les n≈ìuds du cluster.  En m√™me temps, comme pour la r√©plication, ARDFS r√©partit automatiquement les donn√©es entre les n≈ìuds de mani√®re √† emp√™cher le stockage des m√™mes donn√©es (copies des donn√©es et leur parit√©) sur un n≈ìud afin d'√©liminer le risque de perte de donn√©es du fait que les donn√©es et leur la parit√© se retrouvera soudainement sur le m√™me n≈ìud de stockage, ce qui √©chouera. </p><br><p>  Voici un exemple, avec la m√™me machine virtuelle √† 8 Mo et 4 n≈ìuds, mais d√©j√† avec le sch√©ma EC 2 + 1. </p><br><p>  Les blocs A et B sont divis√©s en deux morceaux de 2 Mo chacun (deux parce que 2 + 1), c'est-√†-dire A1 + A2 et B1 + B2.  Contrairement √† la r√©plique, A1 n'est pas une copie de A2, c'est un bloc virtuel A, divis√© en deux parties, √©galement avec le bloc B. Au total, nous obtenons deux ensembles de 4 Mo, chacun contenant deux pi√®ces de deux m√©gaoctets.  De plus, pour chacun de ces ensembles, la parit√© est calcul√©e avec un volume ne d√©passant pas une pi√®ce (c'est-√†-dire 2 Mo), nous obtenons un + 2 pi√®ces de parit√© suppl√©mentaires (AP et BP).  Au total, nous avons des donn√©es 4x2 + parit√© 2x2. </p><br><p>  Ensuite, les pi√®ces sont ¬´dispos√©es¬ª par des n≈ìuds afin que les donn√©es ne se chevauchent pas avec leur parit√©.  C'est-√†-dire  A1 et A2 ne se trouveront pas sur le m√™me n≈ìud avec AP. </p><br><p><img src="https://habrastorage.org/webt/s9/xi/om/s9xiombqm4ep-bos8lntf8kjq4s.png"></p><br><p>  En cas de d√©faillance d'un n≈ìud (par exemple, √©galement le troisi√®me), le bloc B1 tomb√© sera automatiquement restaur√© √† partir de la parit√© BP, qui est stock√©e sur le n≈ìud n ¬∞ 2, et sera activ√© sur le n≈ìud o√π il n'y a pas de parit√© B, c'est-√†-dire  morceaux de BP.  Dans cet exemple, il s'agit du n≈ìud n ¬∞ 1 </p><br><p><img src="https://habrastorage.org/webt/wu/gk/dr/wugkdrhgfund89fswagb7wc4iba.png"></p><br><p>  Je suis s√ªr que le lecteur a une question: </p><br><blockquote>  "Tout ce que vous avez d√©crit a longtemps √©t√© mis en ≈ìuvre par les concurrents et les solutions open source, quelle est la diff√©rence entre votre impl√©mentation d'EC dans ARDFS?" </blockquote><p>  Et puis il y aura des caract√©ristiques int√©ressantes du travail d'ARDFS. </p><br><h3 id="erasure-coding-s-uporom-na-gibkost">  Codage d'effacement en mettant l'accent sur la flexibilit√© </h3><br><p>  Initialement, nous avons fourni un sch√©ma EC X + Y plut√¥t flexible, o√π X est √©gal √† un nombre de 2 √† 8, et Y est √©gal √† un nombre de 1 √† 8, mais toujours inf√©rieur ou √©gal √† X. Un tel sch√©ma est fourni pour la flexibilit√©.  L'augmentation du nombre de donn√©es (X) dans lesquelles l'unit√© virtuelle est divis√©e permet de r√©duire la surcharge, c'est-√†-dire d'augmenter l'espace utilisable. <br>  Une augmentation du nombre de morceaux de parit√© (Y) augmente la fiabilit√© du disque virtuel.  Plus la valeur Y est √©lev√©e, plus les n≈ìuds du cluster peuvent tomber en panne.  Bien s√ªr, l'augmentation de la parit√© r√©duit la quantit√© de capacit√© utilisable, mais c'est une charge pour la fiabilit√©. </p><br><p>  La d√©pendance des performances vis-√†-vis des circuits EC est presque directe: plus il y a de ¬´pi√®ces¬ª, plus les performances sont faibles, ici, bien s√ªr, vous avez besoin d'un look √©quilibr√©. </p><br><p>  Cette approche offre aux administrateurs le moyen le plus flexible de configurer un stockage √©tendu.  Au sein du pool ARDFS, vous pouvez utiliser tous les sch√©mas de tol√©rance aux pannes et leurs combinaisons, ce qui est √©galement, √† notre avis, tr√®s utile. </p><br><p>  Le tableau ci-dessous compare plusieurs (pas tous possibles) circuits RF et EC. </p><br><p><img src="https://habrastorage.org/webt/g0/vj/oj/g0vjojekzkzjv2xbxbdhyw1fjy0.png"></p><br><p>  Le tableau montre que m√™me la combinaison la plus ¬´terry¬ª d'EC 8 + 7, qui permet de perdre jusqu'√† 7 n≈ìuds √† la fois dans un cluster, ¬´consomme¬ª moins d'espace utilisable (1 875 contre 2) que la r√©plication standard, et prot√®ge 7 fois mieux, ce qui rend ce m√©canisme de protection, bien que plus complexe, mais beaucoup plus attractif dans les situations o√π vous avez besoin d'assurer une fiabilit√© maximale dans les conditions de manque d'espace disque.  Dans le m√™me temps, vous devez comprendre que chaque ¬´plus¬ª √† X ou Y sera un surco√ªt suppl√©mentaire pour la productivit√©, vous devez donc choisir tr√®s soigneusement dans le triangle entre fiabilit√©, √©conomie et performances.  Pour cette raison, nous consacrerons un article s√©par√© au codage de suppression de dimensionnement. </p><br><p><img src="https://habrastorage.org/webt/5v/2o/-j/5v2o-jpchgy8rqib7bugk35vmj4.png"></p><br><h3 id="nadezhnost-i-avtonomnost-faylovoy-sistemy">  Fiabilit√© et autonomie du syst√®me de fichiers </h3><br><p>  ARDFS s'ex√©cute localement sur tous les n≈ìuds du cluster et les synchronise par ses propres moyens via des interfaces Ethernet d√©di√©es.  Un point important est que ARDFS synchronise ind√©pendamment non seulement les donn√©es, mais aussi les m√©tadonn√©es li√©es au stockage.  Tout en travaillant sur ARDFS, nous avons √©tudi√© simultan√©ment un certain nombre de solutions existantes et nous avons constat√© que beaucoup effectuent la m√©ta-synchronisation du syst√®me de fichiers en utilisant un SGBD distribu√© externe, que nous utilisons √©galement pour synchroniser, mais uniquement les configurations, pas les m√©tadonn√©es FS (√† propos de cela et d'autres sous-syst√®mes connexes dans le prochain article). </p><br><p>  La synchronisation des m√©tadonn√©es FS √† l'aide d'un SGBD externe est, bien s√ªr, une solution de travail, mais la coh√©rence des donn√©es stock√©es sur ARDFS d√©pendrait du SGBD externe et de son comportement (et elle, franchement, est une dame capricieuse), ce qui est mauvais √† notre avis.  Pourquoi?  Si les m√©tadonn√©es FS sont endommag√©es, les donn√©es FS elles-m√™mes peuvent √©galement √™tre dites ¬´au revoir¬ª, nous avons donc d√©cid√© d'emprunter un chemin plus compliqu√© mais fiable. </p><br><p>  Nous avons cr√©√© le sous-syst√®me de synchronisation des m√©tadonn√©es pour ARDFS ind√©pendamment, et il vit compl√®tement ind√©pendamment des sous-syst√®mes adjacents.  C'est-√†-dire  aucun autre sous-syst√®me ne peut corrompre les donn√©es ARDFS.  √Ä notre avis, c'est le moyen le plus fiable et le plus correct, et c'est vraiment le cas - le temps nous le dira.  De plus, avec cette approche, un avantage suppl√©mentaire appara√Æt.  ARDFS peut √™tre utilis√© ind√©pendamment de vAIR, tout comme le stockage √©tendu, que nous utiliserons certainement dans les futurs produits. </p><br><p>  En cons√©quence, apr√®s avoir d√©velopp√© ARDFS, nous avons obtenu un syst√®me de fichiers flexible et fiable qui vous donne le choix o√π vous pouvez √©conomiser sur la capacit√© ou tout donner sur les performances, ou rendre le stockage tr√®s fiable pour un prix mod√©r√©, mais en r√©duisant les exigences de performances. </p><br><p>  Associ√© √† une politique de licence simple et √† un mod√®le de livraison flexible (√† l'avenir, il est autoris√© par vAIR par des n≈ìuds et est livr√© soit par logiciel soit en tant que PAC), cela vous permet d'adapter tr√®s pr√©cis√©ment la solution aux exigences les plus diff√©rentes des clients et, √† l'avenir, il sera facile de maintenir cet √©quilibre. </p><br><h2 id="komu-eto-chudo-nuzhno">  Qui a besoin de ce miracle? </h2><br><p>  D'une part, on peut dire qu'il y a d√©j√† des acteurs sur le march√© qui ont des d√©cisions s√©rieuses dans le domaine de l'hyperconvergence, et o√π nous allons r√©ellement.  Cette affirmation semble √™tre vraie, MAIS ... </p><br><p>  En revanche, en allant sur le terrain et en communiquant avec les clients, nous et nos partenaires constatons que ce n'est pas du tout le cas.  Il y a beaucoup de probl√®mes pour les hyper convergents, quelque part les gens ne savaient tout simplement pas qu'il y avait de telles solutions, quelque part cela semblait cher, quelque part il y avait des tests infructueux de solutions alternatives, mais quelque part ils interdisaient g√©n√©ralement d'acheter, √† cause des sanctions.  En g√©n√©ral, le champ n'√©tait pas labour√©, nous sommes donc all√©s √©lever les terres vierges))). </p><br><h3 id="kogda-shd-luchshe-chem-gks">  Quand le stockage est-il meilleur que GCS? </h3><br><p>  En travaillant avec le march√©, on nous demande souvent quand il vaut mieux utiliser le sch√©ma classique avec des syst√®mes de stockage, et quand est hyperconvergent?  De nombreuses entreprises - fabricants de GCS (en particulier celles qui n'ont pas de stockage dans leur portefeuille) disent: "Le stockage est d√©pass√©, seulement hyperconvergent!"  C'est une d√©claration audacieuse, mais elle ne refl√®te pas tout √† fait la r√©alit√©. </p><br><p>  En v√©rit√©, le march√© du stockage nage en effet vers des solutions hyperconvergentes et similaires, mais il y a toujours un ¬´mais¬ª. </p><br><p>  Premi√®rement, les centres de donn√©es et les infrastructures informatiques construits selon le sch√©ma classique avec des syst√®mes de stockage ne peuvent pas √™tre facilement reconstruits comme cela, de sorte que la modernisation et l'ach√®vement de ces infrastructures sont encore un h√©ritage de 5-7 ans. </p><br><p>  Deuxi√®mement, les infrastructures qui sont en train d'√™tre construites en grande partie (c'est-√†-dire la F√©d√©ration de Russie) sont construites selon le sch√©ma classique en utilisant des syst√®mes de stockage et non pas parce que les gens ne connaissent pas l'hyperconvergent, mais parce que le march√© hyperconvergent est nouveau, les solutions et les normes n'ont pas encore √©t√© √©tablies , Les employ√©s des TI n'ont pas encore √©t√© form√©s, il y a peu d'exp√©rience et nous devons construire des centres de donn√©es ici et maintenant.  Et cette tendance se poursuit pendant 3 √† 5 ans (puis un autre h√©ritage, voir le paragraphe 1). </p><br><p>  Troisi√®mement, il y a une limitation purement technique dans les petits retards suppl√©mentaires de 2 millisecondes par √©criture (√† l'exclusion du cache local, bien s√ªr), qui sont des frais pour le stockage distribu√©. </p><br><p>  Eh bien, n'oublions pas d'utiliser de grands serveurs physiques qui aiment la mise √† l'√©chelle verticale du sous-syst√®me de disque. </p><br><p>  Il existe de nombreuses t√¢ches n√©cessaires et populaires o√π le syst√®me de stockage se comporte mieux que le GCS.  Ici, bien s√ªr, les fabricants qui n'ont pas de syst√®mes de stockage dans leur portefeuille de produits seront en d√©saccord avec nous, mais nous sommes pr√™ts √† discuter raisonnablement.  Bien s√ªr, nous, en tant que d√©veloppeurs des deux produits dans l'une des futures publications, ferons certainement une comparaison des syst√®mes de stockage et de GCS, o√π nous d√©montrerons clairement ce qui est mieux dans quelles conditions. </p><br><h3 id="a-gde-giperkonvergentnye-resheniya-budut-rabotat-luchshe-shd">  Et o√π les solutions hyperconverg√©es fonctionneront-elles mieux que les syst√®mes de stockage? </h3><br><p>  Sur la base des th√®ses ci-dessus, il y a trois conclusions √©videntes: </p><br><ol><li>  L√† o√π 2 millisecondes suppl√©mentaires de retards d'enregistrement qui se produisent de mani√®re stable dans n'importe quel produit (maintenant nous ne parlons pas de synth√©tiques, vous pouvez montrer des nanosecondes sur des synth√©tiques) ne sont pas critiques, hyper convergentes feront l'affaire. </li><li>  L√† o√π la charge de grands serveurs physiques peut √™tre transform√©e en de nombreux petits serveurs virtuels et distribu√©e par des n≈ìuds, l'hyperconvergent fonctionnera √©galement bien l√†-bas. </li><li>  L√† o√π la mise √† l'√©chelle horizontale est plus importante que la mise √† l'√©chelle verticale, GCS fonctionnera √©galement correctement. </li></ol><br><h3 id="kakie-eto-resheniya">  Quelles sont ces solutions? </h3><br><ol><li>  Tous les services d'infrastructure standard (service d'annuaire, courrier, EDS, serveurs de fichiers, petits ou moyens syst√®mes ERP et BI, etc.).  Nous appelons cela ¬´l'informatique g√©n√©rale¬ª. </li><li>  L'infrastructure des fournisseurs de cloud, o√π il est n√©cessaire de d√©velopper rapidement et standardiser horizontalement et de ¬´d√©couper¬ª facilement un grand nombre de machines virtuelles pour les clients. </li><li>  Virtual Desktop Infrastructure (VDI), o√π de nombreuses petites machines virtuelles d'utilisateurs sont lanc√©es et ¬´flottent¬ª silencieusement √† l'int√©rieur d'un cluster uniforme. </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> R√©seaux de succursales o√π, dans chaque succursale, vous avez besoin d'une infrastructure standard, tol√©rante aux pannes, mais en m√™me temps √©conomique de 15 √† 20 machines virtuelles. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tout type d'informatique distribu√©e (services de Big Data, par exemple). </font><font style="vertical-align: inherit;">O√π la charge ne va pas ¬´en profondeur¬ª, mais ¬´en largeur¬ª.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Environnements de test o√π de petits retards suppl√©mentaires sont acceptables, mais il y a des contraintes budg√©taires, car ce sont des tests. </font></font></li></ol><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Actuellement, c'est pour ces t√¢ches que nous avons r√©alis√© AERODISK vAIR et que nous nous concentrons sur elles (jusqu'√† pr√©sent avec succ√®s). </font><font style="vertical-align: inherit;">Peut-√™tre que cela changera bient√¥t. </font><font style="vertical-align: inherit;">le monde ne reste pas immobile.</font></font></p><br><h3 id="itak"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alors ... </font></font></h3><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ceci termine la premi√®re partie d'une grande s√©rie d'articles; dans le prochain article, nous parlerons de l'architecture de la solution et des composants utilis√©s. </font></font></p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nous accueillons volontiers les questions, suggestions et diff√©rends constructifs. </font></font></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr469383/">https://habr.com/ru/post/fr469383/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr469371/index.html">Description de l'approche pour organiser et tester le code √† l'aide de Redux Thunk</a></li>
<li><a href="../fr469373/index.html">Les r√©sultats du projet de cr√©ation d'une interface neuronale pour des patients compl√®tement paralys√©s remettent en cause</a></li>
<li><a href="../fr469375/index.html">Pourquoi Mozilla, Coil et Creative Commons allouent 100 millions de dollars pour des projets open source?</a></li>
<li><a href="../fr469379/index.html">Application des m√©thodes formelles de validation des mod√®les pour l'interface utilisateur</a></li>
<li><a href="../fr469381/index.html">Agones, cr√©ez un serveur de jeu multi-utilisateurs. Architecture et installation</a></li>
<li><a href="../fr469387/index.html">L'histoire d'un "d√©veloppeur" ou comment un nouveau venu pour √©crire une application pour iOS</a></li>
<li><a href="../fr469389/index.html">Param√©trage par un r√©seau neuronal d'un mod√®le physique pour r√©soudre un probl√®me d'optimisation topologique</a></li>
<li><a href="../fr469391/index.html">Interfaces audio: le son comme source d'information sur la route, au bureau et dans le ciel</a></li>
<li><a href="../fr469393/index.html">R√©daction sur Flare-On 2019</a></li>
<li><a href="../fr469395/index.html">O√π et comment utiliser les multicolonnes (colonnes CSS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>