<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôåüèø üõåüèª üö≤ CephFS vs GlusterFS üè¥ üö∂üèΩ üëú</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Als Infrastrukturingenieur im Entwicklungsteam der Cloud-Plattform hatte ich die M√∂glichkeit, mit vielen verteilten Speichersystemen zu arbeiten, eins...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CephFS vs GlusterFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/430474/"><p>  Als Infrastrukturingenieur im Entwicklungsteam der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cloud-Plattform</a> hatte ich die M√∂glichkeit, mit vielen verteilten Speichersystemen zu arbeiten, einschlie√ülich der im Header angegebenen.  Es scheint, dass es ein Verst√§ndnis f√ºr ihre St√§rken und Schw√§chen gibt, und ich werde versuchen, meine Gedanken zu diesem Thema mit Ihnen zu teilen.  Mal sehen, wer die Hash-Funktion l√§nger hat. </p><br><p><img src="https://habrastorage.org/webt/fb/qn/vj/fbqnvjedf1ujf2hknyuxqbv82qy.png"></p><a name="habracut"></a><br><p>  Haftungsausschluss: Fr√ºher in diesem Blog konnten Sie Artikel √ºber GlusterFS sehen.  Ich habe nichts mit diesen Artikeln zu tun.  Dies ist der Blog des Autors des Projektteams unserer Cloud, und jedes seiner Mitglieder kann seine Geschichte erz√§hlen.  Der Autor dieser Artikel ist Ingenieur unserer Betriebsgruppe und hat seine eigenen Aufgaben und Erfahrungen, die er geteilt hat.  Bitte ber√ºcksichtigen Sie dies, wenn Sie pl√∂tzlich Meinungsverschiedenheiten feststellen.  Ich nutze diese Gelegenheit, um dem Autor dieser Artikel meine Gr√º√üe zu √ºbermitteln! </p><br><h2 id="o-chem-poydet-rech">  Was wird diskutiert </h2><br><p> Lassen Sie uns √ºber Dateisysteme sprechen, die auf der Basis von GlusterFS und CephFS erstellt werden k√∂nnen.  Wir werden die Architektur dieser beiden Systeme diskutieren, sie aus verschiedenen Blickwinkeln betrachten und am Ende sogar riskieren, Schlussfolgerungen zu ziehen.  Andere Ceph-Funktionen wie RBD und RGW sind davon nicht betroffen. </p><br><h2 id="terminologiya">  Terminologie </h2><br><p>  Um den Artikel f√ºr alle vollst√§ndig und verst√§ndlich zu machen, schauen wir uns die grundlegende Terminologie beider Systeme an: </p><br><p>  Ceph-Terminologie: </p><br><p>  <strong>RADOS</strong> (Reliable Autonomic Distributed Object Store) ist ein in sich geschlossener Objektspeicher, der die Grundlage des Ceph-Projekts bildet. <br>  <strong>CephFS</strong> , <strong>RBD</strong> (RADOS Block Device), <strong>RGW</strong> (RADOS Gateway) - <strong>√ºbergeordnete</strong> Gadgets f√ºr RADOS, die Endbenutzern verschiedene Schnittstellen zu RADOS bieten. <br>  Insbesondere bietet CephFS eine POSIX-kompatible Dateisystemschnittstelle.  Tats√§chlich werden CephFS-Daten in RADOS gespeichert. <br>  <strong>OSD</strong> (Object Storage Daemon) ist ein Prozess, der einen separaten Festplatten- / Objektspeicher in einem RADOS-Cluster bereitstellt. <br>  <strong>RADOS-Pool</strong> (Pool) - mehrere <strong>OSDs</strong> , die durch ein gemeinsames Regelwerk vereint sind, z. B. eine Replikationsrichtlinie.  Aus Sicht der Datenhierarchie ist ein Pool ein Verzeichnis oder ein separater Namespace (flach, keine Unterverzeichnisse) f√ºr Objekte. <br>  <strong>PG</strong> (Placement Group) - Ich werde das Konzept von PG etwas sp√§ter im Kontext vorstellen, um es besser zu verstehen. </p><br><p>  Da RADOS die Grundlage ist, auf der CephFS basiert, werde ich oft dar√ºber sprechen, und dies gilt automatisch f√ºr CephFS. </p><br><p>  GlusterFS-Terminologie (im Folgenden gl): </p><br><p>  <strong>Brick</strong> ist ein Prozess, der eine einzelne Festplatte bedient, ein Analogon von OSD in der RADOS-Terminologie. <br>  <strong>Volumen</strong> - Volumen, in dem Steine ‚Äã‚Äãvereint sind.  Tom ist ein Pool-Analogon in RADOS und verf√ºgt √ºber eine spezifische Replikationstopologie zwischen Bausteinen. </p><br><h2 id="raspredelenie-dannyh">  Datenverteilung </h2><br><p>  Betrachten Sie zur Verdeutlichung ein einfaches Beispiel, das von beiden Systemen implementiert werden kann. </p><br><p>  Das als Beispiel zu verwendende Setup: </p><br><ul><li>  2 Server (S1, S2) mit jeweils 3 Festplatten gleichen Volumens (sda, sdb, sdc); </li><li>  Volume / Pool mit Replikation 2. </li></ul><br><p>  Beide Systeme ben√∂tigen f√ºr den normalen Betrieb mindestens 3 Server.  Aber wir machen ein Auge zu, da dies nur ein Beispiel f√ºr einen Artikel ist. </p><br><p>  Im Fall von gl ist dies ein <strong>verteiltes repliziertes</strong> Volume, das aus 3 Replikationsgruppen besteht: </p><br><p><img src="https://habrastorage.org/webt/ai/k_/pg/aik_pgd6mwqy1wlfyhjx-mdsf6u.png"></p><br><p>  Jede Replikationsgruppe besteht aus zwei Bausteinen auf verschiedenen Servern. <br>  Tats√§chlich stellt sich heraus, welches Volume die drei RAID-1 kombiniert. <br>  Wenn Sie es bereitstellen, das gew√ºnschte Dateisystem herunterladen und mit dem Schreiben von Dateien beginnen, werden Sie feststellen, dass jede von Ihnen geschriebene Datei in eine dieser Replikationsgruppen als Ganzes f√§llt. <br>  Die Verteilung der Dateien zwischen diesen verteilten Gruppen erfolgt √ºber <strong>DHT</strong> (Distributed Hash Tables), eine Hash-Funktion (wir werden sp√§ter darauf zur√ºckkommen). </p><br><p>  Auf dem "Diagramm" sieht es so aus: </p><br><p><img src="https://habrastorage.org/webt/d1/-8/uk/d1-8ukcptl3owiyuqw11v0plxqw.png"></p><br><p>  Als ob sich die ersten architektonischen Merkmale bereits manifestiert h√§tten: </p><br><ul><li>  Platz in Gruppen ist ungleichm√§√üig entsorgt, es h√§ngt von der Dateigr√∂√üe ab; </li><li>  Beim Schreiben einer Datei geht IO nur an eine Gruppe, der Rest ist inaktiv. </li><li>  Sie k√∂nnen nicht die E / A des gesamten Volumes abrufen, wenn Sie eine einzelne Datei schreiben. </li><li>  Wenn in der Gruppe nicht gen√ºgend Speicherplatz zum Schreiben der Datei vorhanden ist, wird eine Fehlermeldung angezeigt. Die Datei wird nicht geschrieben und nicht an eine andere Gruppe weitergegeben. </li></ul><br><p>  Wenn Sie andere Datentr√§gertypen verwenden, z. B. Distributed-Striped-Replicated oder sogar Dispersed (Erasure Coding), √§ndert sich nur die Mechanik der Datenverteilung innerhalb einer Gruppe grundlegend.  DHT wird auch Dateien vollst√§ndig in diese Gruppen zerlegen, und am Ende werden wir alle die gleichen Probleme bekommen.  Ja, wenn das Volume nur aus einer Gruppe besteht oder wenn Sie alle Dateien ungef√§hr gleich gro√ü haben, gibt es kein Problem.  Wir sprechen jedoch von normalen Systemen mit Hunderten von Terabyte Daten, einschlie√ülich Dateien unterschiedlicher Gr√∂√üe. Daher glauben wir, dass es ein Problem gibt. </p><br><p>  Schauen wir uns nun CephFS an.  Das oben erw√§hnte RADOS betritt die Szene.  In RADOS wird jede Festplatte von einem separaten Prozess bedient - OSD.  Basierend auf unserem Setup erhalten wir nur 6 davon, 3 auf jedem Server.  Als n√§chstes m√ºssen wir einen Pool f√ºr die Daten erstellen und die Anzahl der PGs und den Datenreplikationsfaktor in diesem Pool festlegen - in unserem Fall 2. <br>  Angenommen, wir haben einen Pool mit 8 PG erstellt.  Diese PGs werden ungef√§hr gleichm√§√üig √ºber das OSD verteilt: </p><br><p><img src="https://habrastorage.org/webt/cn/ea/hs/cneahsczaws7dzuqtu1syubb4cy.png"></p><br><p>  Es ist Zeit zu kl√§ren, dass PG eine logische Gruppe ist, die eine Reihe von Objekten kombiniert.  Da wir Replikationsfaktor 2 festgelegt haben, verf√ºgt jedes PG √ºber ein Replikat auf einem anderen OSD auf einem anderen Server (standardm√§√üig).  Zum Beispiel hat PG1, das sich auf OSD-1 auf Server S1 befindet, einen Zwilling auf S2 auf OSD-6.  In jedem PG-Paar (oder Triple, wenn Replikation 3) befindet sich PRIMARY PG, das aufgezeichnet wird.  Zum Beispiel ist PRIMARY f√ºr PG4 auf S1, aber PRIMARY f√ºr PG3 ist auf S2. </p><br><p>  Nachdem Sie nun wissen, wie RADOS funktioniert, k√∂nnen wir Dateien in unseren brandneuen Pool schreiben.  Obwohl RADOS ein vollwertiger Speicher ist, ist es nicht m√∂glich, ihn als Dateisystem bereitzustellen oder als Blockger√§t zu verwenden.  Um Daten direkt darauf zu schreiben, m√ºssen Sie ein spezielles Dienstprogramm oder eine Bibliothek verwenden. </p><br><p>  Wir schreiben die gleichen drei Dateien wie im obigen Beispiel: </p><br><p><img src="https://habrastorage.org/webt/ut/0z/zd/ut0zzd20fmocwke70q9rj-0snog.png"></p><br><p>  Im Fall von RADOS ist alles irgendwie komplizierter geworden, stimme zu. </p><br><p>  Dann erschien CRUSH (Controlled Replication Under Scalable Hashing) in der Kette.  CRUSH ist der Algorithmus, auf dem RADOS beruht (wir werden sp√§ter darauf zur√ºckkommen).  In diesem speziellen Fall wird unter Verwendung dieses Algorithmus bestimmt, wo die Datei in welches PG geschrieben werden soll.  Hier hat CRUSH die gleiche Funktion wie DHT in gl.  Als Ergebnis dieser pseudozuf√§lligen Verteilung von Dateien auf PG haben wir alle die gleichen Probleme wie gl, nur bei einem komplexeren Schema. </p><br><p>  Aber ich habe absichtlich √ºber einen wichtigen Punkt geschwiegen.  Fast niemand benutzt RADOS in seiner reinen Form.  F√ºr eine bequeme Arbeit mit RADOS wurden die folgenden Schichten entwickelt: RBD, CephFS, RGW, die ich bereits erw√§hnt habe. </p><br><p>  Alle diese √úbersetzer (RADOS-Clients) bieten eine andere Client-Oberfl√§che, √§hneln sich jedoch in ihrer Arbeit mit RADOS.  Die wichtigste √Ñhnlichkeit besteht darin, dass alle Daten, die sie durchlaufen, in St√ºcke geschnitten und als separate RADOS-Objekte in RADOS abgelegt werden.  Standardm√§√üig schneiden offizielle Clients den Eingabestream in 4 MB gro√üe Teile.  Bei RBD kann die Streifengr√∂√üe beim Erstellen des Volumes festgelegt werden.  Bei CephFS ist dies das Attribut (xattr) der Datei und kann auf der Ebene einzelner Dateien oder f√ºr alle Katalogdateien verwaltet werden.  Nun, RGW hat auch einen entsprechenden Parameter. </p><br><p>  Nehmen wir nun an, wir haben CephFS auf den RADOS-Pool gestapelt, der im vorherigen Beispiel vorgestellt wurde.  Jetzt sind die fraglichen Systeme v√∂llig gleichberechtigt und bieten eine identische Dateizugriffsschnittstelle. </p><br><p>  Wenn wir unsere Testdateien zur√ºck in das brandneue CephFS schreiben, werden wir eine v√∂llig andere, fast gleichm√§√üige Verteilung der Daten auf dem OSD finden.  Zum Beispiel wird Datei2 mit einer Gr√∂√üe von 2 GB in 512 Teile unterteilt, die auf verschiedene PGs und infolgedessen nahezu gleichm√§√üig auf verschiedene OSDs verteilt werden, und dies l√∂st praktisch die oben beschriebenen Probleme mit der Datenverteilung. </p><br><p>  In unserem Beispiel werden nur 8 PG verwendet, obwohl empfohlen wird, ~ 100 PG auf einem OSD zu haben.  Und Sie ben√∂tigen 2 Pools, damit CephFS funktioniert. Sie ben√∂tigen auch einige Service-Daemons, damit RADOS im Prinzip funktioniert.  Denken Sie nicht, dass alles so einfach ist, ich lasse ausdr√ºcklich viel weg, um die Essenz nicht zu verlassen. </p><br><p>  Jetzt scheint CephFS interessanter zu sein, oder?  Aber ich habe keinen weiteren wichtigen Punkt erw√§hnt, diesmal √ºber gl.  Gl hat auch einen Mechanismus zum Schneiden von Dateien in Bl√∂cke und zum Ausf√ºhren dieser Bl√∂cke durch DHT.  Das sogenannte <strong>Sharding</strong> ( <strong>Sharding</strong> ). </p><br><p>  F√ºnf Minuten Geschichte </p><br><blockquote>  Am 21. April 2016 ver√∂ffentlichte das Ceph-Entwicklungsteam "Jewel", die erste Ceph-Version, in der CephFS als stabil gilt. </blockquote><p>  Das ist jetzt alles links und rechts, was √ºber CephFS schreit!  Und vor 3-4 Jahren w√§re es zumindest eine zweifelhafte Entscheidung, es zu benutzen.  Wir haben nach anderen L√∂sungen gesucht, und die oben beschriebene Architektur war nicht gut.  Aber wir glaubten mehr daran als an CephFS und warteten auf die Scherbe, die sich auf die Ver√∂ffentlichung vorbereitete. </p><br><p>  Und hier ist X Tag: </p><br><blockquote>  4. Juni 2015 - Die Gluster Community gab heute die allgemeine Verf√ºgbarkeit der offenen softwaredefinierten Speichersoftware GlusterFS 3.7 bekannt. </blockquote><p>  3.7 - die erste Version von gl, in der Sharding als experimentelle Gelegenheit angek√ºndigt wurde.  Sie hatten fast ein Jahr vor der stabilen Ver√∂ffentlichung von CephFS Zeit, um auf dem Podium Fu√ü zu fassen ... </p><br><p>  Scherben hei√üt also.  Wie alles in gl wird dies in einem separaten √úbersetzer implementiert, der √ºber dem DHT (auch √úbersetzer) auf dem Stapel stand.  Da es h√∂her als DHT ist, empf√§ngt DHT vorgefertigte Shards an der Eingabe und verteilt sie als regul√§re Dateien auf die Replikationsgruppen.  Das Sharding ist auf der Ebene der einzelnen Lautst√§rken aktiviert.  Die Gr√∂√üe des Shards kann standardm√§√üig auf 4 MB eingestellt werden, wie bei Ceph-Lotionen. </p><br><p>  Als ich die ersten Tests durchf√ºhrte, war ich begeistert!  Ich habe allen gesagt, dass gl jetzt das Beste ist und wir jetzt leben werden!  Wenn Sharding aktiviert ist, erfolgt die Aufzeichnung einer Datei parallel zu verschiedenen Replikationsgruppen.  Die Dekomprimierung nach der "On-Write" -Komprimierung kann schrittweise auf die Shard-Ebene erfolgen.  Auch hier wird beim Cache-Shooting alles gut und separate Shards werden in den Cache verschoben und nicht in die gesamten Dateien.  Im Allgemeinen freute ich mich, weil  es schien, als h√§tte er ein sehr cooles Instrument in die H√§nde bekommen. </p><br><p>  Es blieb auf die ersten Bugfixes und den Status "Ready for Production" zu warten.  Aber alles ist nicht so rosig geworden ... Um den Artikel nicht mit einer Liste kritischer Fehler im Zusammenhang mit Sharding zu erweitern, die ab und zu in den n√§chsten Versionen auftreten, kann ich nur sagen, dass das letzte "Hauptproblem" mit der folgenden Beschreibung: </p><br><blockquote>  Das Erweitern eines Shuster-Volumes, das gespalten ist, kann zu einer Besch√§digung der Datei f√ºhren.  Sharded-Volumes werden normalerweise f√ºr VM-Images verwendet. Wenn solche Volumes erweitert oder m√∂glicherweise verkleinert werden (d. H. Bricks hinzuf√ºgen / entfernen und neu ausgleichen), wird berichtet, dass VM-Images besch√§digt werden. </blockquote><p>  wurde in Release 3.13.2 am 20. Januar 2018 geschlossen ... vielleicht ist dies nicht der letzte? </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kommentar</a> zu einem unserer Artikel dazu sozusagen aus erster Hand. </p><br><p>  RedHat stellt in seiner Dokumentation zum aktuellen RedHat Gluster Storage 3.4 fest, dass der einzige von ihnen unterst√ºtzte Sharding-Fall der Speicher f√ºr VM-Festplatten ist. </p><br><blockquote>  Sharding hat einen unterst√ºtzten Anwendungsfall: Im Zusammenhang mit der Bereitstellung von Red Hat Gluster Storage als Speicherdom√§ne f√ºr Red Hat Enterprise Virtualization wird Speicher f√ºr Live-Images virtueller Maschinen bereitgestellt.  Beachten Sie, dass Sharding auch f√ºr diesen Anwendungsfall erforderlich ist, da es gegen√ºber fr√ºheren Implementierungen erhebliche Leistungsverbesserungen bietet. </blockquote><p>  Ich wei√ü nicht, warum eine solche Einschr√§nkung vorliegt, aber Sie m√ºssen zugeben, dass sie alarmierend ist. </p><br><h2 id="seychas-ya-tebe-tut-vse-zaheshiruyu">  Jetzt habe ich alles f√ºr dich da </h2><br><p>  Beide Systeme verwenden eine Hash-Funktion, um Daten pseudozuf√§llig auf Festplatten zu verteilen. </p><br><p>  F√ºr RADOS sieht es ungef√§hr so ‚Äã‚Äãaus: </p><br><pre><code class="plaintext hljs">PG = pool_id + "." + jenkins_hash(object_name) % pg_coun # eg pool id=5 =&gt; pg = 5.1f OSD = crush_hash_based_on_jenkins(PG) # eg pg=5.1f =&gt; OSD = 12</code> </pre> <br><p>  Gl verwendet das sogenannte <strong>konsistente Hashing</strong> .  Jeder Baustein erh√§lt einen "Bereich innerhalb eines 32-Bit-Hash-Bereichs".  Das hei√üt, alle Bausteine ‚Äã‚Äãteilen sich den gesamten linearen Adress-Hash-Raum, ohne Bereiche oder L√∂cher zu schneiden.  Der Client f√ºhrt den Dateinamen √ºber die Hash-Funktion aus und bestimmt dann, in welchen Hash-Bereich der empfangene Hash f√§llt.  Somit wird Ziegel ausgew√§hlt.  Wenn die Replikationsgruppe mehrere Bausteine ‚Äã‚Äãenth√§lt, haben alle denselben Hash-Bereich.  Ungef√§hr so: </p><br><p><img src="https://habrastorage.org/webt/o_/y5/ye/o_y5yeby9vn5enx7r-5xa3zfwuq.png"></p><br><p>  Wenn wir die Arbeit zweier Systeme auf eine bestimmte logische Form bringen, wird sich ungef√§hr so ‚Äã‚Äãherausstellen: </p><br><pre> <code class="plaintext hljs">file -&gt; HASH -&gt; placement_unit</code> </pre> <br><p>  Dabei ist placation_unit im Fall von RADOS PG und im Fall von gl eine Replikationsgruppe aus mehreren Bausteinen. </p><br><p>  Also, eine Hash-Funktion, dann verteilt diese, verteilt Dateien und pl√∂tzlich stellt sich heraus, dass eine Placement_unit mehr als die andere verwendet wird.  Dies ist das grundlegende Merkmal von Hash-Verteilungssystemen.  Und wir stehen vor einer sehr h√§ufigen Aufgabe - das Ungleichgewicht der Daten. </p><br><p>  Gl kann neu erstellt werden. Aufgrund der oben beschriebenen Architektur mit Hash-Bereichen k√∂nnen Sie die Neuerstellung so oft ausf√ºhren, wie Sie m√∂chten, aber kein Hash-Bereich (und infolgedessen Daten) r√ºhrt sich nicht.  Das einzige Kriterium f√ºr die Umverteilung von Hash-Bereichen ist eine √Ñnderung der Volumenkapazit√§t.  Und Sie haben noch eine Option - das Hinzuf√ºgen von Steinen.  Wenn es sich um ein Volume mit Replikation handelt, m√ºssen wir eine ganze Replikationsgruppe hinzuf√ºgen, dh zwei neue Bausteine ‚Äã‚Äãin unserem Setup.  Nach dem Erweitern des Volumes k√∂nnen Sie mit der Neuerstellung beginnen. Die Hash-Bereiche werden unter Ber√ºcksichtigung der neuen Gruppe neu verteilt und die Daten werden verteilt.  Wenn eine Replikationsgruppe gel√∂scht wird, werden Hash-Bereiche automatisch zugewiesen. </p><br><p>  RADOS hat eine ganze Reihe von M√∂glichkeiten.  In einem Ceph- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel habe</a> ich mich viel √ºber das Konzept von PG beschwert, aber hier im Vergleich zu gl nat√ºrlich RADOS zu Pferd.  Jedes OSD hat sein eigenes Gewicht. Normalerweise wird es basierend auf der Gr√∂√üe der Festplatte festgelegt.  PGs werden wiederum von OSD abh√§ngig von dessen Gewicht verteilt.  Alles, dann √§ndern wir einfach das Gewicht des OSD nach oben oder unten und das PG (zusammen mit den Daten) beginnt sich auf andere OSDs zu verschieben.  Au√üerdem verf√ºgt jedes OSD √ºber ein zus√§tzliches Anpassungsgewicht, mit dem Sie Daten zwischen den Festplatten eines Servers ausgleichen k√∂nnen.  All dies ist CRUSH inh√§rent.  Der Hauptgewinn besteht darin, dass die Poolkapazit√§t nicht erweitert werden muss, um die Daten besser aus dem Gleichgewicht zu bringen.  Es ist nicht erforderlich, Festplatten in Gruppen hinzuzuf√ºgen. Sie k√∂nnen nur ein OSD hinzuf√ºgen, und ein Teil von PG wird darauf √ºbertragen. </p><br><p>  Ja, es ist m√∂glich, dass beim Erstellen eines Pools nicht gen√ºgend PG erstellt wurde und sich herausstellte, dass jedes der PGs ein ziemlich gro√ües Volumen aufweist und das Ungleichgewicht √ºberall dort bleibt, wo sie sich bewegen.  In diesem Fall k√∂nnen Sie die Anzahl der PG erh√∂hen, und diese werden in kleinere aufgeteilt.  Ja, wenn der Cluster voller Daten ist, tut es weh, aber die Hauptsache in unserem Vergleich ist, dass es eine solche M√∂glichkeit gibt.  Jetzt ist nur eine Erh√∂hung der Anzahl der PGs zul√§ssig, und dabei m√ºssen Sie vorsichtiger sein. In der n√§chsten Version von Ceph - Nautilus wird jedoch die Reduzierung der Anzahl der PGs unterst√ºtzt (pg fusioning). </p><br><h2 id="replikaciya-dannyh">  Datenreplikation </h2><br><p>  Unsere Testpools und Volumes haben einen Replikationsfaktor von 2. Interessanterweise verwenden die betreffenden Systeme unterschiedliche Ans√§tze, um diese Anzahl von Replikaten zu erreichen. </p><br><p>  Im Fall von RADOS sieht das Aufzeichnungsschema ungef√§hr so ‚Äã‚Äãaus: </p><br><p><img src="https://habrastorage.org/webt/lx/vb/q-/lxvbq-niuingzw76ad7aqanp2pg.png"></p><br><p>  Der Client kennt die Topologie des gesamten Clusters, w√§hlt mit CRUSH (Schritt 0) ein bestimmtes PG zum Schreiben aus, schreibt unter OSD-0 in PRIMARY PG (Schritt 1), repliziert dann OSD-0 synchron Daten nach SECONDARY PG (Schritt 2) und erst danach erfolgreicher / erfolgloser Schritt 2, OSD best√§tigt / best√§tigt dem Client den Vorgang nicht (Schritt 3).  Die Datenreplikation zwischen zwei OSDs ist f√ºr den Client transparent.  OSDs k√∂nnen im Allgemeinen ein separates "Cluster" -Netzwerk f√ºr die Datenreplikation verwenden. </p><br><p>  Wenn die dreifache Replikation konfiguriert ist, wird sie auch synchron mit PRIMARY OSD auf zwei SECONDARY ausgef√ºhrt, die f√ºr den Client transparent sind. Nun, nur diese Letancy ist h√∂her. </p><br><p>  Gl funktioniert anders: </p><br><p><img src="https://habrastorage.org/webt/ll/zz/q-/llzzq-m2jhfaf83-dtk_fokavw0.png"></p><br><p>  Der Client kennt die Topologie des Volumes, verwendet DHT (Schritt 0), um den gew√ºnschten Baustein zu bestimmen, und schreibt dann darauf (Schritt 1).  Alles ist einfach und klar.  Hier erinnern wir uns jedoch daran, dass alle Bausteine ‚Äã‚Äãin der Replikationsgruppe den gleichen Hash-Bereich haben.  Und dieses kleine Merkmal macht den ganzen Urlaub.  Der Client schreibt parallel zu allen Bausteinen, die einen geeigneten Hash-Bereich haben. </p><br><p>  In unserem Fall f√ºhrt der Client bei doppelter Replikation eine doppelte Aufzeichnung parallel auf zwei verschiedenen Bausteinen durch.  W√§hrend der dreifachen Replikation wird jeweils eine dreifache Aufzeichnung durchgef√ºhrt, und 1 MB Daten werden ungef√§hr zu 3 MB Netzwerkverkehr vom Client zur Seite der gl-Server.  Stimmen Sie zu, die Konzepte von Systemen sind senkrecht. </p><br><p>  In einem solchen Schema wird dem gl-Client mehr Arbeit zugewiesen, und als Ergebnis ben√∂tigt er mehr CPU. Nun, ich habe bereits √ºber das Netzwerk gesprochen. </p><br><p>  Die Replikation erfolgt durch den AFP-√úbersetzer (Automatic File Replication) - Ein clientseitiger Xlator, der eine synchrone Replikation durchf√ºhrt.  Repliziert Schreibvorg√§nge auf alle Bausteine ‚Äã‚Äãdes Replikats ‚Üí Verwendet ein Transaktionsmodell. </p><br><p>  Synchronisieren Sie bei Bedarf die Replikate in der Gruppe (Heilung), z. B. nach einer vor√ºbergehenden Nichtverf√ºgbarkeit eines Bausteins tun die Gl-D√§monen dies selbstst√§ndig mithilfe des integrierten AFP, das f√ºr Kunden transparent und ohne deren Teilnahme ist. </p><br><p>  Es ist interessant, dass wir das gleiche Verhalten wie RADOS erhalten, wenn Sie nicht √ºber den nativen gl-Client arbeiten, sondern √ºber den in NF integrierten NFS-Server schreiben.  In diesem Fall wird AFP in Gl-Daemons verwendet, um Daten ohne Client-Intervention zu replizieren.  Das integrierte NFS ist jedoch in gl v4 gesichert. Wenn Sie dieses Verhalten w√ºnschen, wird empfohlen, NFS-Ganesha zu verwenden. </p><br><p>  √úbrigens k√∂nnen Sie aufgrund des unterschiedlichen Verhaltens bei der Verwendung von NFS und des nativen Clients v√∂llig unterschiedliche Leistungsindikatoren sehen. </p><br><h2 id="a-u-vas-est-takoy-zhe-klaster-tolko-na-kolenke">  Haben Sie den gleichen Cluster, nur "am Knie"? </h2><br><p>  Ich sehe im Internet oft Diskussionen √ºber alle Arten von Kniescheiben-Setups, bei denen ein Datencluster aus dem Vorhandenen aufgebaut wird.  In diesem Fall bietet Ihnen eine RADOS-basierte L√∂sung mehr Freiheit bei der Auswahl Ihrer Laufwerke.  In RADOS k√∂nnen Sie Laufwerke fast jeder Gr√∂√üe hinzuf√ºgen.  Jede Festplatte hat (normalerweise) ein Gewicht, das ihrer Gr√∂√üe entspricht, und die Daten werden fast proportional zu ihrem Gewicht auf die Festplatten verteilt.  Im Fall von gl gibt es kein Konzept f√ºr "separate Festplatten" in Volumes mit Replikation.  Festplatten werden paarweise bei doppelter Replikation oder dreifach bei dreifacher Replikation hinzugef√ºgt.  Wenn sich in einer Replikationsgruppe Festplatten unterschiedlicher Gr√∂√üe befinden, werden Sie auf eine Stelle auf der kleinsten Festplatte in der Gruppe sto√üen und die Kapazit√§t gro√üer Festplatten aufheben.  In einem solchen Schema geht gl davon aus, dass die Kapazit√§t einer Replikationsgruppe gleich der Kapazit√§t der kleinsten Festplatte in der Gruppe ist, was logisch ist.  Gleichzeitig d√ºrfen Replikationsgruppen vorhanden sein, die aus Festplatten unterschiedlicher Gr√∂√üe bestehen - Gruppen unterschiedlicher Gr√∂√üe.  Gr√∂√üere Gruppen k√∂nnen im Vergleich zu anderen Gruppen einen gr√∂√üeren Hash-Bereich erhalten und erhalten daher mehr Daten. </p><br><p>  Wir leben seit dem f√ºnften Jahr bei Ceph.  Wir haben mit Festplatten mit dem gleichen Volumen begonnen, jetzt f√ºhren wir gr√∂√üere ein.  Mit Ceph k√∂nnen Sie die Festplatte entfernen und ohne architektonische Schwierigkeiten durch eine andere gr√∂√üere oder etwas kleinere ersetzen.  Mit gl ist alles komplizierter - nehmen Sie eine 2-TB-Festplatte heraus - legen Sie bitte dieselbe ein.  Nun, oder ziehen Sie die ganze Gruppe als Ganzes zur√ºck, was nicht sehr gut ist, stimmen Sie zu. </p><br><h2 id="obrabotka-otkazov">  Failover </h2><br><p>  Wir haben uns bereits ein wenig mit der Architektur der beiden L√∂sungen vertraut gemacht und k√∂nnen nun dar√ºber sprechen, wie wir damit leben sollen und welche Funktionen bei der Wartung zur Verf√ºgung stehen. </p><br><p>  Angenommen, wir haben sda auf s1 abgelehnt - eine h√§ufige Sache. </p><br><p>  Im Fall von gl: </p><br><ul><li>  Eine Kopie der Daten auf der in der Gruppe verbleibenden Live-Festplatte wird nicht automatisch an andere Gruppen weitergegeben. </li><li>  Bis zum Ersetzen der Festplatte verbleibt nur eine Kopie der Daten. </li><li>  Wenn eine ausgefallene Festplatte durch eine neue ersetzt wird, wird die Replikation von einer funktionierenden Festplatte auf eine neue (1 zu 1) durchgef√ºhrt. </li></ul><br><p>  Dies ist wie das Servieren eines Regals mit mehreren RAID-1.  Ja, bei dreifacher Replikation bleibt bei einem Ausfall eines Laufwerks nicht eine Kopie √ºbrig, sondern zwei, aber dieser Ansatz weist schwerwiegende Nachteile auf, und ich werde sie anhand eines guten Beispiels mit RADOS zeigen. </p><br><p>  Angenommen, wir haben sda auf S1 (OSD-0) fehlgeschlagen - eine h√§ufige Sache: </p><br><ul><li>  PGs, die sich auf OSD-0 befanden, werden nach 10 Minuten automatisch anderen OSDs zugeordnet (Standard).  In unserem Beispiel auf OSD 1 und 2. Wenn mehr Server vorhanden waren, dann auf einer gr√∂√üeren Anzahl von OSD. </li><li>  PGs, die die zweite √ºberlebende Kopie der Daten speichern, replizieren sie automatisch auf die OSDs, auf denen die wiederhergestellten PGs √ºbertragen werden.  Es stellt sich heraus, dass viele zu viele repliziert werden, nicht wie eins zu eins wie gl. </li><li>  Wenn eine neue Festplatte anstelle einer defekten Festplatte eingef√ºhrt wird, werden einige PGs entsprechend ihrem Gewicht im neuen OSD akkumuliert und Daten von anderen OSDs werden neu verteilt. </li></ul><br><p>  Ich denke, es macht keinen Sinn, die architektonischen Vorteile von RADOS zu erkl√§ren.  Sie k√∂nnen nicht zucken, wenn Sie einen Brief erhalten, der besagt, dass das Laufwerk ausgefallen ist.  Und wenn Sie morgens zur Arbeit kommen, stellen Sie fest, dass alle fehlenden Kopien bereits auf Dutzenden anderer OSDs oder in diesem Prozess wiederhergestellt wurden.  In gro√üen Clustern, in denen Hunderte von PGs auf mehrere Festplatten verteilt sind, kann die Datenwiederherstellung eines OSD mit einer Geschwindigkeit erfolgen, die viel h√∂her ist als die Geschwindigkeit einer Festplatte, da Dutzende von OSDs beteiligt sind (Lesen und Schreiben).  Nun, Sie sollten auch den Lastausgleich nicht vergessen. </p><br><h2 id="masshtabirovanie">  Skalieren </h2><br><p>  In diesem Zusammenhang werde ich wahrscheinlich den Sockel gl geben.  In einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> √ºber Ceph habe ich bereits √ºber einige der Komplexit√§ten der RADOS-Skalierung geschrieben, die mit dem PG-Konzept verbunden sind.  Wenn der Anstieg der PG mit dem Wachstum des Clusters immer noch zu beobachten ist, ist was mit Ceph MDS nicht klar.  CephFS l√§uft auf RADOS und verwendet einen separaten Pool f√ºr Metadaten und einen speziellen Prozess, den Ceph-Metadatenserver (MDS), um Dateisystemmetadaten zu warten und alle Vorg√§nge mit dem FS zu koordinieren.  Ich sage nicht, dass MDS die Skalierbarkeit von CephFS beeintr√§chtigt, nein, zumal Sie mehrere MDS im Aktiv-Aktiv-Modus ausf√ºhren k√∂nnen.  Ich m√∂chte nur erw√§hnen, dass gl architektonisch ohne all dies ist.  Es hat kein PG-Gegenst√ºck, nichts wie MDS.  Gl skaliert wirklich perfekt, indem einfach Replikationsgruppen fast linear hinzugef√ºgt werden. </p><br><p>  In den Tagen vor CephFS haben wir die L√∂sung f√ºr Daten-Petabyte entwickelt und uns gl angesehen.  Dann hatten wir Zweifel an der Skalierbarkeit von gl und fanden es durch die Mailingliste heraus.  Hier ist eine der Antworten (F: meine Frage): </p><br><blockquote>  Ich verwende 60 Server mit jeweils 26 x 8 TB Festplatten, insgesamt 1560 Festplatten, 16 + 4 EC-Datentr√§ger mit 9 TB nutzbarem Speicherplatz. <br><br>  F: Verwenden Sie libgfapi oder FUSE oder NFS auf der Clientseite? <br><br>  Ich benutze FUSE und habe fast 1000 Kunden. <br><br>  F: Wie viele Dateien hat Ihr Volume? <br>  F: Dateien sind gr√∂√üer oder kleiner? <br><br>  Ich habe √ºber 1 Million Dateien und% 13 des Clusters wird verwendet, was eine durchschnittliche Dateigr√∂√üe von 1 GB ergibt. <br>  Die minimale / maximale Dateigr√∂√üe betr√§gt 100 MB / 2 GB.  Jeden Tag werden 10 bis 20 TB neue Daten in das Volume eingegeben. <br><br>  F: Wie schnell funktioniert "ls"? <br><br>  Metadatenvorg√§nge sind erwartungsgem√§√ü langsam.  Ich versuche nicht mehr als 2-3K Dateien in ein Verzeichnis zu stellen.  Mein Anwendungsfall ist das Sichern / Archivieren, daher f√ºhre ich selten Metadatenoperationen durch. </blockquote><br><h2 id="pereimenovanie-faylov">  Dateien umbenennen </h2><br><p>  Zur√ºck zu den Hash-Funktionen.  Wir haben herausgefunden, wie bestimmte Dateien auf bestimmte Festplatten weitergeleitet werden, und jetzt wird die Frage relevant. Aber was passiert beim Umbenennen von Dateien? </p><br><p>  Wenn wir den Namen der Datei √§ndern, √§ndert sich schlie√ülich auch der Hash in ihrem Namen, dh der Speicherort dieser Datei auf einer anderen Festplatte (in einem anderen Hash-Bereich) oder bei RADOS auf einem anderen PG / OSD.  Ja, wir denken richtig und hier bei zwei Systemen ist alles wieder senkrecht. </p><br><p>  Im Fall von gl wird beim Umbenennen einer Datei der neue Name √ºber eine Hash-Funktion ausgef√ºhrt, ein neuer Baustein definiert und ein spezieller Link zum alten Baustein erstellt, in dem die Datei wie zuvor verbleibt.  Topovka, richtig?  Damit die Daten wirklich an einen neuen Ort verschoben werden und der Client nicht unn√∂tig auf den Link geklickt hat, m√ºssen Sie eine Rebellion durchf√ºhren. </p><br><p>  RADOS verf√ºgt jedoch im Allgemeinen nicht √ºber eine Methode zum Umbenennen von Objekten, da diese sp√§ter verschoben werden m√ºssen.  Es wird vorgeschlagen, zum Umbenennen ein faires Kopieren zu verwenden, was zu einer synchronen Bewegung des Objekts f√ºhrt.  Und CephFS, das auf RADOS l√§uft, hat einen Trumpf in Form eines Pools mit Metadaten und MDS im √Ñrmel.  Das √Ñndern des Dateinamens wirkt sich nicht auf den Inhalt der Datei im Datenpool aus. </p><br><h2 id="replikaciya-25">  Replikation 2.5 </h2><br><p>  Gl hat eine sehr coole Funktion, die ich separat erw√§hnen m√∂chte.  Jeder versteht, dass Replikation 2 keine zuverl√§ssige Konfiguration ist, sie findet jedoch regelm√§√üig statt, um durchaus gerechtfertigt zu sein.  Zum Schutz vor Split-Brain in solchen Schemata und zur Gew√§hrleistung der Datenkonsistenz k√∂nnen Sie mit gl Volumes mit Replikat 2 und einem zus√§tzlichen Arbiter erstellen.  Der Arbiter ist f√ºr die Replikation von 3 oder mehr anwendbar.  Dies ist derselbe Baustein in der Gruppe wie die beiden anderen, nur dass nur eine Dateistruktur aus Dateien und Verzeichnissen erstellt wird.  Dateien auf einem solchen Baustein haben die Gr√∂√üe Null, aber ihre erweiterten Attribute des Dateisystems (erweiterte Attribute) werden mit Dateien voller Gr√∂√üe in derselben Replik synchronisiert.  Ich denke, die Idee ist klar.  Ich denke, das ist eine coole Gelegenheit. </p><br><p>  Der einzige Moment ... die Gr√∂√üe des Ortes in der Replikationsgruppe wird durch die Gr√∂√üe des kleinsten Bausteins bestimmt. Dies bedeutet, dass der Arbiter eine Festplatte mit mindestens der gleichen Gr√∂√üe wie die anderen in der Gruppe verschieben muss.  Zu diesem Zweck wird empfohlen, d√ºnne (d√ºnne) fiktive, gro√üe LV-Gr√∂√üen zu erstellen, um keine echte Festplatte zu verwenden. </p><br><h2 id="a-che-po-klientam">  Und was ist mit Kunden? </h2><br><p>  Die native API der beiden Systeme wird in Form der Bibliotheken libgfapi (gl) und libcephfs (CephFS) implementiert.  Bindungen f√ºr beliebte Sprachen sind ebenfalls verf√ºgbar.  Im Allgemeinen ist bei Bibliotheken alles ungef√§hr gleich gut.  Das allgegenw√§rtige NFS-Ganesha unterst√ºtzt beide Bibliotheken als FSAL, was ebenfalls die Norm ist.  Qemu unterst√ºtzt auch die native gl-API √ºber libgfapi. </p><br><p>  Fio (Flexible I / O Tester) unterst√ºtzt libgfapi jedoch schon lange und erfolgreich, libcephfs jedoch nicht.  Das ist ein Plus, weil  mit fio ist es wirklich sch√∂n, gl direkt zu testen.  Nur wenn Sie vom Userspace √ºber libgfapi arbeiten, erhalten Sie alles, was gl von gl kann. </p><br><p>  Wenn wir jedoch √ºber das POSIX-Dateisystem und dessen Bereitstellung sprechen, kann gl nur den FUSE-Client und die CephFS-Implementierung im Upstream-Kernel anbieten.  Es ist klar, dass Sie im Kernelmodul einen solchen Trick ausf√ºhren k√∂nnen, dass FUSE eine bessere Leistung zeigt.  In der Praxis ist FUSE jedoch immer ein Overhead beim Kontextwechsel.  Ich habe mehr als einmal pers√∂nlich gesehen, wie FUSE einen Dual-Socket-Server nur mit CSs verbog. <br>  Irgendwie sagte Linus: </p><br><blockquote>  Userspace-Dateisystem?  Das Problem ist genau dort.  War schon immer so.  Leute, die denken, dass Userspace-Dateisysteme f√ºr alles andere als Spielzeug realistisch sind, sind einfach irregef√ºhrt. </blockquote><p>  Im Gegensatz dazu finden Gl-Entwickler FUSE cool.  Dies soll mehr Flexibilit√§t bieten und sich von Kernelversionen l√∂sen.  Ich benutze FUSE, weil es bei gl nicht um Geschwindigkeit geht.  Irgendwie ist es geschrieben - nun, es ist normal und es ist wirklich seltsam, sich mit der Implementierung im Kernel zu besch√§ftigen. </p><br><h2 id="proizvoditelnost">  Leistung </h2><br><p>  Es wird keine Vergleiche geben). </p><br><p>  Das ist zu kompliziert.  Selbst bei identischem Aufbau ist es zu schwierig, objektive Tests durchzuf√ºhren.  Wie auch immer, es wird jemanden in den Kommentaren geben, der 100500 Parameter angibt, die eines der Systeme ‚Äûbeschleunigen‚Äú und sagen, dass die Tests Bullshit sind.  Testen Sie sich daher bei Interesse bitte. </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  Insbesondere RADOS und CephFS sind eine komplexere L√∂sung sowohl f√ºr das Verst√§ndnis, die Einstellung als auch f√ºr die Wartung. </p><br><p>  Aber ich pers√∂nlich mag die Architektur von RADOS und die Ausf√ºhrung auf CephFS mehr als auf GlusterFS.  CephFS-Metadaten mit mehr Handles (PG, OSD-Gewicht, CRUSH-Hierarchie usw.) erh√∂hen die Komplexit√§t, bieten jedoch mehr Flexibilit√§t und machen diese L√∂sung meiner Meinung nach effektiver. </p><br><p>  Ceph ist viel besser f√ºr aktuelle SDB-Kriterien geeignet und scheint mir vielversprechender.  Aber das ist meine Meinung, was denkst du? </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de430474/">https://habr.com/ru/post/de430474/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de430462/index.html">In Russland wurde eine Gesetzesvorlage zur Bereitstellung von Daten von Nutzern sozialer Netzwerke f√ºr einen unbegrenzten Personenkreis ver√∂ffentlicht. Soziale Netzwerke gegen</a></li>
<li><a href="../de430466/index.html">Mini AI Cup # 3: Einen Top Bot schreiben</a></li>
<li><a href="../de430468/index.html">Sensibilisierung der B√ºrger</a></li>
<li><a href="../de430470/index.html">Warum den Kontext auf dem Kundenkonto pflegen - ehrlich und profitabel</a></li>
<li><a href="../de430472/index.html">DIY nahtloses DECT-Netzwerk</a></li>
<li><a href="../de430476/index.html">NCBI Genome Workbench: Gef√§hrdete Forschung</a></li>
<li><a href="../de430478/index.html">Trading Bots f√ºr den Kryptow√§hrungsmarkt. Wo soll ich anfangen?</a></li>
<li><a href="../de430480/index.html">Wie wir die Anwendung beim NASA Space Apps Challenge Hackathon geschrieben haben</a></li>
<li><a href="../de430482/index.html">Das Thema der Panzerplatten in der Kultur von Ost und West</a></li>
<li><a href="../de430484/index.html">Typische NGFW-Implementierungsszenarien</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>