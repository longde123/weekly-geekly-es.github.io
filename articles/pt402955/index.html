<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîØ ü§ûüèº üë©üèæ‚Äçüé® O ASIC especializado do Google para aprendizado de m√°quina √© dez vezes mais r√°pido que o GPU ü§¢ ‚õπüèæ üßòüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="H√° quatro anos, o Google percebeu o real potencial do uso de redes neurais em suas aplica√ß√µes. Ent√£o ela come√ßou a apresent√°-los em todos os lugares -...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O ASIC especializado do Google para aprendizado de m√°quina √© dez vezes mais r√°pido que o GPU</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/402955/"><img src="https://habrastorage.org/files/265/7b9/cb4/2657b9cb49834f6ebc249ddcb70c9136.jpg"><br><br>  H√° quatro anos, o Google percebeu o real potencial do uso de redes neurais em suas aplica√ß√µes.  Ent√£o ela come√ßou a apresent√°-los em todos os lugares - tradu√ß√£o de texto, pesquisa por voz com reconhecimento de fala, etc.  Grosso modo, se todos realizassem uma pesquisa por voz no Android (ou ditassem texto com reconhecimento de fala) por apenas tr√™s minutos por dia, o Google teria que dobrar o n√∫mero de datacenters (!) Para que as redes neurais processassem uma quantidade t√£o grande de tr√°fego de voz. <br><br>  Algo precisava ser feito - e o Google encontrou uma solu√ß√£o.  Em 2015, ela desenvolveu sua pr√≥pria arquitetura de hardware para aprendizado de m√°quina (TPU), que √© at√© 70 vezes mais r√°pida que as GPUs e CPUs tradicionais em termos de desempenho e at√© 196 vezes mais em termos de n√∫mero de c√°lculos por watt.  GPUs / CPUs tradicionais referem-se aos processadores de uso geral Xeon E5 v3 (Haswell) e GPUs Nvidia Tesla K80. <br><a name="habracut"></a><br>  A arquitetura do TPU foi descrita pela primeira vez nesta semana em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo cient√≠fico (pdf)</a> que ser√° apresentado no 44¬∫ Simp√≥sio Internacional de Arquitetura de Computadores (ISCA), 26 de junho de 2017 em Toronto.  Um dos principais autores de mais de 70 autores deste trabalho cient√≠fico, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um destacado engenheiro Norman</a> Jouppi, conhecido como um dos criadores do processador MIPS, em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entrevista</a> √† <i>The Next Platform,</i> explicou em suas pr√≥prias palavras os recursos da arquitetura TPU exclusiva, que na verdade √© um ASIC especializado. circuito integrado para fins especiais. <br><br>  Ao contr√°rio dos FPGAs convencionais ou ASICs altamente especializados, os m√≥dulos TPU s√£o programados da mesma maneira que uma GPU ou CPU; n√£o √© um equipamento de alcance estreito para uma √∫nica rede neural.  Norman Yuppy diz que o TPU suporta instru√ß√µes CISC para diferentes tipos de redes neurais: redes neurais convolucionais, modelos LSTM e modelos grandes e totalmente conectados.  Para que ele permane√ßa program√°vel, use apenas a matriz como primitiva, e n√£o como vetor ou escalar. <br><br>  O Google enfatiza que, enquanto outros desenvolvedores est√£o otimizando seus microchips para redes neurais convolucionais, essas redes neurais fornecem apenas 5% da carga nos data centers do Google.  A maioria dos aplicativos do Google usa os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">perceptrons Rumelhart multicamadas</a> , por isso era t√£o importante criar uma arquitetura mais universal que n√£o fosse "aprimorada" apenas para redes neurais convolucionais. <br><br><img src="https://habrastorage.org/files/99f/df0/d93/99fdf0d93e494088a3f1fd86791e0b23.png"><br>  <i>Um dos elementos da arquitetura √© o mecanismo de fluxo de dados sist√≥lico, uma matriz de 256 √ó 256, que recebe ativa√ß√£o (pesos) dos neur√¥nios √† esquerda e depois tudo muda passo a passo, multiplicado pelos pesos na c√©lula.</i>  <i>Acontece que a matriz sist√≥lica realiza 65 536 c√°lculos por ciclo.</i>  <i>Essa arquitetura √© ideal para redes neurais.</i> <br><br>  De acordo com Yuppy, a arquitetura das TPUs √© mais um coprocessador FPU do que uma GPU comum, embora numerosas matrizes para multiplica√ß√£o n√£o armazenem nenhum programa em si, elas simplesmente executam instru√ß√µes recebidas do host. <br><br><img src="https://habrastorage.org/files/9ba/2a1/2d1/9ba2a12d11204b74888b70e29ecdf876.png"><br>  <i>Toda a arquitetura TPU, com exce√ß√£o da mem√≥ria DDR3.</i>  <i>As instru√ß√µes s√£o enviadas do host (esquerda) para a fila.</i>  <i>Ent√£o, a l√≥gica de controle, dependendo da instru√ß√£o, pode executar cada um deles repetidamente</i> <br><br>  Ainda n√£o se sabe o qu√£o escal√°vel √© essa arquitetura.  Yuppy diz que em um sistema com esse tipo de host sempre haver√° algum tipo de gargalo. <br><br><img src="https://habrastorage.org/files/6c0/87d/3c7/6c087d3c75b744628a716e65a6f0ae5e.png"><br><br>  Comparado √†s CPUs e GPUs convencionais, a arquitetura de m√°quinas do Google as supera em dez vezes.  Por exemplo, um processador Haswell Xeon E5-2699 v3 com 18 n√∫cleos a uma freq√º√™ncia de 2,3 GHz com um ponto flutuante de 64 bits executa 1,3 tera-opera√ß√µes por segundo (TOPS) e mostra uma taxa de transfer√™ncia de dados de 51 GB / s.  Nesse caso, o pr√≥prio chip consome 145 watts e todo o sistema possui 256 GB de mem√≥ria - 455 watts. <br><br>  Para compara√ß√£o, o TPU em opera√ß√µes de 8 bits com 256 GB de mem√≥ria externa e 32 GB de mem√≥ria interna demonstra a velocidade de troca com mem√≥ria de 34 GB / s, mas ao mesmo tempo o cart√£o executa 92 TOPS, ou seja, aproximadamente 71 vezes mais que o processador Haswell.  O consumo de energia do servidor na TPU √© de 384 watts. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/fbb/21b/e41/fbb21be41722273b1e79febcd8b6f9ab.jpg"><br><br>  O gr√°fico a seguir compara o desempenho relativo por watt de um servidor com uma GPU (coluna azul), um servidor em TPU (vermelho) em rela√ß√£o a um servidor na CPU.  Ele tamb√©m compara o desempenho relativo por watt do servidor com o TPU em rela√ß√£o ao servidor na GPU (laranja) e a vers√£o aprimorada do TPU em rela√ß√£o ao servidor na CPU (verde) e o servidor na GPU (roxo). <br><br><img src="https://habrastorage.org/files/d85/52a/445/d8552a4455ed436c9daca5bdba5c1c2e.png"><br><br>  Note-se que o Google fez compara√ß√µes em testes de aplicativos no TensorFlow com a vers√£o antiga relativa do Haswell Xeon, enquanto na vers√£o mais recente do Broadwell Xeon E5 v4, o n√∫mero de instru√ß√µes por ciclo aumentou em 5% devido a melhorias arquiteturais e na vers√£o do Skylake Xeon E5 v5 , que √© esperado no ver√£o, o n√∫mero de instru√ß√µes por ciclo pode aumentar em mais 9 a 10%.  E com o aumento do n√∫mero de n√∫cleos de 18 para 28 no Skylake, o desempenho geral dos processadores Intel nos testes do Google pode melhorar em 80%.  Mas, mesmo assim, haver√° uma enorme diferen√ßa de desempenho com o TPU.  Na vers√£o do teste com ponto flutuante de 32 bits, a diferen√ßa entre TPUs e CPUs √© reduzida para aproximadamente 3,5 vezes.  Mas a maioria dos modelos quantiza perfeitamente para 8 bits. <br><br>  O Google pensou em como usar GPU, FPGA e ASIC em seus data centers desde 2006, mas n√£o os encontrou at√© a √∫ltima vez em que introduziu o aprendizado de m√°quina para v√°rias tarefas pr√°ticas, e a carga nessas redes neurais come√ßou a crescer com bilh√µes de solicita√ß√µes de usu√°rios.  Agora a empresa n√£o tem escolha a n√£o ser afastar-se das CPUs tradicionais. <br><br>  A empresa n√£o planeja vender seus processadores para ningu√©m, mas espera que o trabalho cient√≠fico com o ASIC 2015 permita que outras pessoas melhorem a arquitetura e criem vers√µes aprimoradas do ASIC que "elevem ainda mais a fasquia".  O pr√≥prio Google provavelmente j√° est√° trabalhando em uma nova vers√£o do ASIC. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt402955/">https://habr.com/ru/post/pt402955/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt402941/index.html">O Pewble Smartwatch desencadeia a sincroniza√ß√£o em nuvem</a></li>
<li><a href="../pt402943/index.html">Um mundo sem pens√µes</a></li>
<li><a href="../pt402947/index.html">Jeff Bezos gasta US $ 1 bilh√£o por ano em desenvolvimento Blue Origin</a></li>
<li><a href="../pt402951/index.html">Revis√£o ONYX BOOX MAX Carta: A4 intransigente</a></li>
<li><a href="../pt402953/index.html">Inventou um esquema para sacar dinheiro de cart√µes de cr√©dito atrav√©s de casas de penhores</a></li>
<li><a href="../pt402957/index.html">A RAM de tr√™s bits foi impressa em uma impressora a jato de tinta</a></li>
<li><a href="../pt402961/index.html">‚ÄúUma e a mesma coisa de novo‚Äù: como uma lista de reprodu√ß√£o √© formada nas esta√ß√µes de r√°dio</a></li>
<li><a href="../pt402963/index.html">Se voc√™ ficar na escada rolante do metr√¥ em duas linhas, sua taxa de transfer√™ncia aumentar√° em 31%</a></li>
<li><a href="../pt402965/index.html">Geek Keykeeper, Parte II: SmartPoket para as grandes chaves que criamos gra√ßas a voc√™</a></li>
<li><a href="../pt402967/index.html">Um pouco mais de levita√ß√£o para um geek: gadgets capazes de permanecer no ar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>