<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧗🏼 👦🏼 🍟 Parsim 25 To avec AWK et R 🤰🏽 📢 👩🏼‍💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Comment lire cet article : Je m'excuse du fait que le texte s'est avéré si long et chaotique. Pour vous faire gagner du temps, je commence chaque chap...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25 To avec AWK et R</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>Comment lire cet article</b> : Je m'excuse du fait que le texte s'est avéré si long et chaotique.</i>  <i>Pour vous faire gagner du temps, je commence chaque chapitre par l'introduction de «Ce que j'ai appris», dans lequel j'explique l'essence du chapitre en une ou deux phrases.</i> <i><br><br></i>  <i><b>"Montrez simplement la solution!"</b></i>  <i>Si vous voulez simplement voir où je suis arrivé, alors allez au chapitre "Devenez plus inventif", mais je pense que c'est plus intéressant et utile de lire sur les échecs.</i> <br><br>  Récemment, j'ai été chargé de mettre en place un processus de traitement d'un grand volume des séquences d'ADN d'origine (techniquement, il s'agit d'une puce SNP).  Il était nécessaire d'obtenir rapidement des données sur un emplacement génétique donné (appelé SNP) pour la modélisation ultérieure et d'autres tâches.  Avec l'aide de R et AWK, j'ai pu nettoyer et organiser les données de manière naturelle, accélérant considérablement le traitement des demandes.  Cela n'a pas été facile pour moi et a nécessité de nombreuses itérations.  Cet article vous aidera à éviter certaines de mes erreurs et à montrer ce que j'ai fait au final. <br><a name="habracut"></a><br>  Tout d'abord, quelques explications introductives. <br><br><h2>  Les données </h2><br>  Notre centre de traitement des informations génétiques de l'université nous a fourni 25 To de données TSV.  Je les ai divisés en 5 paquets compressés par Gzip, chacun contenant environ 240 fichiers de quatre gigaoctets.  Chaque ligne contenait des données pour un SNP d'une personne.  Au total, des données sur environ 2,5 millions de SNP et environ 60 000 personnes ont été transmises.  En plus des informations SNP, il y avait de nombreuses colonnes dans les fichiers avec des nombres reflétant diverses caractéristiques, telles que l'intensité de lecture, la fréquence des différents allèles, etc.  Il y avait environ 30 colonnes avec des valeurs uniques. <br><br><h4>  But </h4><br>  Comme pour tout projet de gestion des données, le plus important était de déterminer comment les données seraient utilisées.  Dans ce cas, <b>pour la plupart, nous sélectionnerons des modèles et des workflows pour SNP basés sur SNP</b> .  Autrement dit, nous aurons besoin en même temps de données pour un seul SNP.  J'ai dû apprendre à extraire tous les enregistrements liés à l'un des 2,5 millions de SNP aussi simplement que possible, plus rapidement et moins cher. <br><br><h1>  Comment ne pas le faire </h1><br>  Je citerai un cliché approprié: <br><br><blockquote>  Je n'ai pas échoué mille fois, je viens de découvrir mille façons de ne pas analyser un tas de données dans un format pratique pour les requêtes. </blockquote><br>
<h2>  Première tentative </h2><br>  <b>Ce que j'ai appris</b> : il n'y a pas de moyen bon marché d'analyser 25 To à la fois. <br><br>  Après avoir écouté le sujet «Advanced Big Data Processing Methods» à l'Université Vanderbilt, j'étais sûr que c'était un chapeau.  Il faudra peut-être une heure ou deux pour configurer le serveur Hive pour qu'il exécute toutes les données et rende compte du résultat.  Étant donné que nos données sont stockées dans AWS S3, j'ai utilisé le service <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Athena</a> , qui vous permet d'appliquer des requêtes Hive SQL aux données S3.  Pas besoin de configurer / augmenter le cluster Hive, et même de payer uniquement pour les données que vous recherchez. <br><br>  Après avoir montré à Athena mes données et leur format, j'ai effectué quelques tests avec des requêtes similaires: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  Et a rapidement obtenu des résultats bien structurés.  C'est fait. <br><br>  Jusqu'à ce que nous essayions d'utiliser les données dans le travail ... <br><br>  On m'a demandé de retirer toutes les informations SNP afin de tester le modèle dessus.  J'ai lancé une requête: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... et attendu.  Après huit minutes et plus de 4 To des données demandées, j'ai obtenu le résultat.  Athena facture des frais pour la quantité de données trouvées, à 5 $ par téraoctet.  Cette seule demande a donc coûté 20 $ et huit minutes d'attente.  Pour exécuter le modèle selon toutes les données, il a fallu attendre 38 ans et payer 50 millions de dollars, ce qui ne nous convenait évidemment pas. <br><br><h2>  Il fallait utiliser Parquet ... </h2><br>  <b>Ce que j'ai appris</b> : soyez prudent avec la taille de vos dossiers de parquet et leur organisation. <br><br>  Au début, j'ai essayé de corriger la situation en convertissant tous les TSV en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fichiers Parquet</a> .  Ils sont pratiques pour travailler avec de grands ensembles de données, car les informations qu'ils contiennent sont stockées sous forme de colonnes: chaque colonne se trouve dans son propre segment mémoire / disque, contrairement aux fichiers texte dans lesquels les lignes contiennent des éléments de chaque colonne.  Et si vous avez besoin de trouver quelque chose, alors lisez simplement la colonne nécessaire.  De plus, une plage de valeurs est stockée dans chaque fichier d'une colonne, donc si la valeur souhaitée ne se trouve pas dans la plage de colonnes, Spark ne perdra pas de temps à analyser le fichier entier. <br><br>  J'ai exécuté une tâche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AWS Glue</a> simple pour convertir nos TSV en Parquet et j'ai déposé de nouveaux fichiers dans Athena.  Cela a pris environ 5 heures.  Mais lorsque j'ai lancé la demande, il a fallu environ le même temps et un peu moins d'argent pour la compléter.  Le fait est que Spark, essayant d'optimiser la tâche, a simplement déballé un bloc TSV et l'a placé dans son propre bloc Parquet.  Et comme chaque bloc était suffisamment grand et contenait les enregistrements complets de nombreuses personnes, tous les SNP étaient stockés dans chaque fichier, donc Spark a dû ouvrir tous les fichiers pour extraire les informations nécessaires. <br><br>  Curieusement, le type de compression par défaut (et recommandé) dans Parquet - snappy - n'est pas séparable.  Par conséquent, chaque exécuteur s'est attaché à la tâche de décompresser et de télécharger l'ensemble de données complet de 3,5 Go. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Nous comprenons le problème </h2><br>  <b>Ce que j'ai appris</b> : le tri est difficile, surtout si les données sont distribuées. <br><br>  Il me semblait que maintenant je comprenais l'essence du problème.  Tout ce que j'avais à faire était de trier les données par colonne SNP, pas par personnes.  Ensuite, plusieurs SNP seront stockés dans un bloc de données séparé, puis la fonction intelligente Parquet "ouverte uniquement si la valeur est dans la plage" apparaîtra dans toute sa splendeur.  Malheureusement, trier des milliards de lignes dispersées sur un cluster s'est avéré être une tâche ardue. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS ne veut certainement pas rendre l'argent à cause de "Je suis un étudiant distrait."  Après avoir commencé à trier sur Amazon Glue, cela a fonctionné pendant 2 jours et s'est écrasé. <br><br><h2>  Et le partitionnement? </h2><br>  <b>Ce que j'ai appris</b> : les partitions dans Spark doivent être équilibrées. <br><br>  Puis l'idée m'est venue de partitionner les données sur les chromosomes.  Il y en a 23 (et quelques autres, étant donné l'ADN mitochondrial et les zones non cartographiées). <br>  Cela vous permettra de diviser les données en portions plus petites.  Si vous ajoutez une seule ligne <code>partition_by = "chr"</code> à la fonction d'exportation Spark dans le script Glue, les données doivent être triées dans des compartiments. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>Le génome se compose de nombreux fragments appelés chromosomes.</i> <br><br>  Malheureusement, cela n'a pas fonctionné.  Les chromosomes ont des tailles différentes, et donc une quantité d'informations différente.  Cela signifie que les tâches envoyées par Spark aux travailleurs n'étaient pas équilibrées et exécutées lentement, car certains nœuds se terminaient plus tôt et étaient inactifs.  Cependant, les tâches ont été achevées.  Mais lors de la demande d'un SNP, le déséquilibre a de nouveau causé des problèmes.  Le coût du traitement des SNP sur des chromosomes plus gros (c'est-à-dire d'où nous voulons obtenir les données) n'a diminué que d'environ 10 fois.  Beaucoup, mais pas assez. <br><br><h2>  Et si vous vous divisez en partitions encore plus petites? </h2><br>  <b>Ce que j'ai appris</b> : n'essayez jamais de faire 2,5 millions de partitions. <br><br>  J'ai décidé de me promener et de partitionner chaque SNP.  Cela garantissait la même taille de partitions.  <b>MAUVAIS ÉTAIT UNE IDÉE</b> .  J'ai profité de Glue et ajouté la ligne innocente <code>partition_by = 'snp'</code> .  La tâche a commencé et a commencé à s'exécuter.  Un jour plus tard, j'ai vérifié et vu que rien n'était écrit en S3 jusqu'à présent, alors j'ai tué la tâche.  Il semble que Glue écrivait des fichiers intermédiaires dans un endroit caché dans S3, et beaucoup de fichiers, peut-être quelques millions.  En conséquence, mon erreur a coûté plus de mille dollars et n'a pas plu à mon mentor. <br><br><h2>  Partitionnement + tri </h2><br>  <b>Ce que j'ai appris</b> : le tri est toujours difficile, tout comme la configuration de Spark. <br><br>  La dernière tentative de partitionnement a consisté à partitionner les chromosomes puis à trier chaque partition.  En théorie, cela accélérerait chaque demande, car les données SNP souhaitées devraient se trouver dans plusieurs morceaux de parquet dans une plage donnée.  Hélas, le tri des données même partitionnées s'est avéré être une tâche difficile.  En conséquence, je suis passé à EMR pour un cluster personnalisé et j'ai utilisé huit instances puissantes (C5.4xl) et Sparklyr pour créer un flux de travail plus flexible ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... cependant, la tâche n'était toujours pas terminée.  J'ai réglé chaque chose: j'ai augmenté l'allocation de mémoire pour chaque exécuteur de requête, utilisé des nœuds avec une grande quantité de mémoire, utilisé des variables de diffusion, mais à chaque fois cela s'est avéré être des demi-mesures, et progressivement les artistes ont commencé à échouer, jusqu'à ce que tout s'arrête. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Je deviens plus inventif </h1><br>  <b>Ce que j'ai appris</b> : parfois, les données spéciales nécessitent des solutions spéciales. <br><br>  Chaque SNP a une valeur de position.  Il s'agit du nombre correspondant au nombre de bases situées le long de son chromosome.  Il s'agit d'un moyen bon et naturel d'organiser nos données.  Au début, je voulais partitionner par région de chaque chromosome.  Par exemple, positions 1 - 2000, 2001 - 4000, etc.  Mais le problème est que les SNP ne sont pas distribués également entre les chromosomes, c'est pourquoi la taille des groupes variera considérablement. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  En conséquence, je suis venu à être divisé en catégories (grade) postes.  Selon les données déjà téléchargées, j'ai lancé une demande de liste de SNP uniques, de leurs positions et de leurs chromosomes.  Il a ensuite trié les données à l'intérieur de chaque chromosome et collecté le SNP en groupes (bac) d'une taille donnée.  Dites 1000 SNP chacun.  Cela m'a donné une relation SNP avec un groupe en chromosome. <br><br>  Au final, j'ai fait des groupes (bin) sur 75 SNP, j'expliquerai la raison ci-dessous. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Essayez d'abord avec Spark </h2><br>  <b>Ce que j'ai appris</b> : l'intégration de Spark est rapide, mais le partitionnement reste cher. <br><br>  Je voulais lire ce petit bloc de données (2,5 millions de lignes) dans Spark, le combiner avec des données brutes, puis partitionner par la colonne <code>bin</code> nouvellement ajoutée. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  J'ai utilisé <code>sdf_broadcast()</code> , donc Spark découvre qu'il devrait envoyer une trame de données à tous les nœuds.  Ceci est utile si les données sont petites et requises pour toutes les tâches.  Sinon, Spark essaie d'être intelligent et distribue les données selon les besoins, ce qui peut provoquer des freins. <br><br>  Et encore une fois, mon idée n'a pas fonctionné: les tâches ont fonctionné pendant un certain temps, ont terminé la fusion, puis, comme les exécuteurs exécutés par partitionnement, elles ont commencé à échouer. <br><br><h2>  Ajouter AWK </h2><br>  <b>Ce que j'ai appris</b> : ne dormez pas lorsque les bases vous apprennent.  Certes, quelqu'un a déjà résolu votre problème dans les années 1980. <br><br>  Jusqu'à présent, la cause de tous mes échecs avec Spark était la confusion des données dans le cluster.  Peut-être que la situation peut être améliorée par le prétraitement.  J'ai décidé d'essayer de diviser les données brutes du texte en colonnes chromosomiques, alors j'espérais fournir à Spark des données «pré-partitionnées». <br><br>  J'ai cherché sur StackOverflow pour savoir comment décomposer les valeurs des colonnes et j'ai trouvé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une excellente réponse.</a>  À l'aide d'AWK, vous pouvez diviser un fichier texte en valeurs de colonne en écrivant dans le script, plutôt qu'en envoyant les résultats à <code>stdout</code> . <br><br>  Pour les tests, j'ai écrit un script Bash.  J'ai téléchargé l'un des TSV emballés, puis l'ai déballé avec <code>gzip</code> et l'ai envoyé à <code>awk</code> . <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Ça a marché! <br><br><h2>  Remplissage de noyau </h2><br>  <b>Ce que j'ai appris</b> : le <code>gnu parallel</code> est une chose magique, tout le monde devrait l'utiliser. <br><br>  La séparation a été plutôt lente, et quand j'ai commencé <code>htop</code> pour tester l'utilisation d'une instance EC2 puissante (et coûteuse), il s'est avéré que j'utilisais un seul cœur et environ 200 Mo de mémoire.  Afin de résoudre le problème et de ne pas perdre beaucoup d'argent, il a fallu trouver comment paralléliser le travail.  Heureusement, dans l'étonnant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Data Science de</a> Jeron Janssens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le</a> livre de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ligne de commande</a> , j'ai trouvé un chapitre sur la parallélisation.  De là, j'ai appris sur <code>gnu parallel</code> , une méthode très flexible pour implémenter le multithreading sur Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Quand j'ai commencé la partition en utilisant un nouveau processus, tout allait bien, mais il y avait un goulot d'étranglement - le téléchargement d'objets S3 sur le disque n'était pas trop rapide et pas complètement parallélisé.  Pour résoudre ce problème, j'ai fait ceci: <br><br><ol><li>  J'ai découvert qu'il est possible d'implémenter l'étape de téléchargement S3 directement dans le pipeline, éliminant complètement le stockage intermédiaire sur disque.  Cela signifie que je peux éviter d'écrire des données brutes sur le disque et utiliser un stockage encore plus petit et donc moins cher sur AWS. <br></li><li>  La commande <code>aws configure set default.s3.max_concurrent_requests 50</code> considérablement augmenté le nombre de threads que l'AWS CLI utilise (il y en a 10 par défaut). <br></li><li>  Je suis passé à l'instance EC2 optimisée pour la vitesse du réseau, avec la lettre n dans le nom.  J'ai constaté que la perte de puissance de calcul lors de l'utilisation de n instances est plus que compensée par une augmentation de la vitesse de téléchargement.  Pour la plupart des tâches, j'ai utilisé c5n.4xl. <br></li><li>  J'ai changé <code>gzip</code> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><code>pigz</code></a> , c'est un outil gzip qui peut faire des choses sympas pour paralléliser la tâche initialement inégalée de décompresser des fichiers (cela a le moins aidé). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Ces étapes sont combinées entre elles pour que tout fonctionne très rapidement.  Grâce à la vitesse de téléchargement accrue et au refus d'écrire sur le disque, je pouvais désormais traiter un paquet de 5 téraoctets en quelques heures seulement. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Ce tweet était censé mentionner «TSV».  Hélas. <br><br><h2>  Utilisation de données analysées à nouveau </h2><br>  <b>Ce que j'ai appris</b> : Spark aime les données non compressées et n'aime pas combiner les partitions. <br><br>  Maintenant, les données étaient en S3 dans un format décompressé (lu, partagé) et semi-ordonné, et je pouvais revenir à Spark à nouveau.  Une surprise m'attendait: je n'ai de nouveau pas réussi à atteindre le résultat souhaité!  Il était très difficile de dire à Spark exactement comment les données étaient partitionnées.  Et même quand je l'ai fait, il s'est avéré qu'il y avait trop de partitions (95 000), et quand j'ai réduit leur nombre à des limites cohérentes avec <code>coalesce</code> , cela a ruiné mon partitionnement.  Je suis sûr que cela peut être corrigé, mais en quelques jours de recherche, je n'ai pas pu trouver de solution.  Au final, j'ai terminé toutes les tâches dans Spark, même si cela a pris du temps, et mes fichiers de parquet fractionnés n'étaient pas très petits (~ 200 Ko).  Cependant, les données étaient là où elles étaient nécessaires. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Trop petit et différent, merveilleux!</i> <br><br><h2>  Test des demandes Spark locales </h2><br>  <b>Ce que j'ai appris</b> : Spark a trop de temps pour résoudre des problèmes simples. <br><br>  En téléchargeant les données dans un format intelligent, j'ai pu tester la vitesse.  J'ai configuré un script sur R pour démarrer le serveur Spark local, puis j'ai chargé la trame de données Spark à partir du référentiel spécifié des groupes de parquet (bin).  J'ai essayé de charger toutes les données, mais je n'ai pas pu faire en sorte que Sparklyr reconnaisse le partitionnement. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  L'exécution a duré 29,415 secondes.  Beaucoup mieux, mais pas trop bon pour tester quoi que ce soit en masse.  De plus, je n'ai pas pu accélérer le travail avec la mise en cache, car lorsque j'essayais de mettre en cache la trame de données en mémoire, Spark se bloquait toujours, même lorsque j'allouais plus de 50 Go de mémoire pour un ensemble de données pesant moins de 15. <br><br><h2>  Retour à AWK </h2><br>  <b>Ce que j'ai appris</b> : les tableaux associatifs AWK sont très efficaces. <br><br>  J'ai compris que je pouvais atteindre une vitesse plus élevée.  Je me suis rappelé que dans l’excellent guide AWK de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bruce Barnett</a> , j’ai lu sur une fonctionnalité intéressante appelée « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tableaux associatifs</a> ».  En fait, ce sont des paires clé-valeur, qui pour une raison quelconque étaient appelées différemment dans AWK, et donc je ne les ai pas particulièrement mentionnées.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Roman Cheplyaka a</a> rappelé que le terme «tableaux associatifs» est beaucoup plus ancien que le terme «paire clé-valeur».  Même si vous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherchez une valeur-clé dans Google Ngram</a> , vous ne verrez pas ce terme là, mais vous trouverez des tableaux associatifs!  De plus, la paire clé-valeur est le plus souvent associée aux bases de données, il est donc beaucoup plus logique de comparer avec hashmap.  J'ai réalisé que je pouvais utiliser ces tableaux associatifs pour connecter mes SNP à la table bin et aux données brutes sans utiliser Spark. <br><br>  Pour cela, dans le script AWK, j'ai utilisé le bloc <code>BEGIN</code> .  Il s'agit d'un morceau de code qui est exécuté avant le transfert de la première ligne de données vers le corps principal du script. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  La commande <code>while(getline...)</code> chargé toutes les lignes du groupe CSV (bin), définissez la première colonne (nom SNP) comme clé pour le tableau associatif <code>bin</code> et la deuxième valeur (group) comme valeur.  Ensuite, dans le bloc <code>{</code> <code>}</code> , qui est appliqué à toutes les lignes du fichier principal, chaque ligne est envoyée au fichier de sortie, qui obtient un nom unique en fonction de son groupe (bin): <code>..._bin_"bin[$1]"_...</code> <br><br>  Les <code>chunk_id</code> <code>batch_num</code> et <code>chunk_id</code> correspondaient aux données fournies par le pipeline, ce qui évitait l'état de course, et chaque thread d'exécution lancé en <code>parallel</code> écrivait dans son propre fichier unique. <br><br>  Depuis que j'ai dispersé toutes les données brutes dans des dossiers sur les chromosomes laissés après ma précédente expérience avec AWK, je pouvais maintenant écrire un autre script Bash à traiter sur le chromosome à la fois et donner des données partitionnées plus profondes à S3. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  Le script a deux sections <code>parallel</code> . <br><br>  Dans la première section, les données de tous les fichiers contenant des informations sur le chromosome souhaité sont lues, puis ces données sont distribuées en flux qui dispersent les fichiers dans les groupes correspondants (bin).  Pour éviter que des conditions de <code>chr_10_bin_52_batch_2_aa.csv</code> ne se produisent lorsque plusieurs flux sont écrits dans le même fichier, AWK transfère les noms de fichier pour écrire des données à différents endroits, par exemple, <code>chr_10_bin_52_batch_2_aa.csv</code> .  En conséquence, de nombreux petits fichiers sont créés sur le disque (pour cela, j'ai utilisé des volumes EBS de téraoctets). <br><br>  Le pipeline de la deuxième section <code>parallel</code> passe par les groupes (bin) et combine leurs fichiers individuels en CSV communs avec <code>cat</code> , puis les envoie pour exportation. <br><br><h2>  Diffuser vers R? </h2><br>  <b>Ce que j'ai appris</b> : vous pouvez accéder à <code>stdin</code> et <code>stdout</code> partir d'un script R, et donc l'utiliser dans le pipeline. <br><br>  Dans le script Bash, vous remarquerez peut-être cette ligne: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Il traduit tous les fichiers de groupe concaténés (bin) dans le script R ci-dessous.  <code>{}</code> est une technique <code>parallel</code> spéciale qui insère toutes les données envoyées par elle dans le flux spécifié directement dans la commande elle-même.  L'option <code>{#}</code> fournit un ID de thread unique et <code>{%}</code> représente le numéro de l'emplacement de travail (répété, mais jamais en même temps).  Une liste de toutes les options se trouve dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Lorsque la variable de <code>file("stdin")</code> est transmise à <code>readr::read_csv</code> , les données traduites dans le script R sont chargées dans le cadre, qui est ensuite écrit directement dans S3 en tant que fichier <code>.rds</code> à l'aide de <code>aws.s3</code> . <br><br>  RDS est un peu comme une version plus récente de Parquet, sans les fioritures du stockage sur colonne. <br><br>  Après avoir terminé le script Bash, j'ai reçu un <code>.rds</code> fichiers <code>.rds</code> trouvant dans S3, ce qui m'a permis d'utiliser une compression efficace et des types intégrés. <br><br>  Malgré l'utilisation du frein R, tout a fonctionné très rapidement.  Il n'est pas surprenant que les fragments sur R qui sont responsables de la lecture et de l'écriture des données soient bien optimisés.  Après avoir testé sur un chromosome de taille moyenne, la tâche s'est terminée sur l'instance C5n.4xl en environ deux heures. <br><br><h2>  Limitations de S3 </h2><br>  <b>Ce que j'ai appris</b> : grâce à l'implémentation intelligente des chemins, S3 peut traiter de nombreux fichiers. <br><br>  J'étais inquiet si S3 pouvait gérer un grand nombre de fichiers qui y étaient transférés.  Je pourrais donner un sens aux noms de fichiers, mais comment S3 les recherchera-t-il? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   —    . <br><br>          Amazon, ,    «-----»  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     —     . <br><br>       : «    ?»      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # … with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 × 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> …   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Résultats </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     … <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Conclusion </h2><br>   —   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  «»  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> —   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     —     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456392/">https://habr.com/ru/post/fr456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456376/index.html">Not One Spring Boot: un aperçu des alternatives</a></li>
<li><a href="../fr456380/index.html">Journée portes ouvertes de la Faculté de Programmation en Netologie</a></li>
<li><a href="../fr456384/index.html">PVS-Studio Graphique de développement des capacités de diagnostic</a></li>
<li><a href="../fr456386/index.html">Bibliothèques ouvertes pour la visualisation du contenu audio</a></li>
<li><a href="../fr456388/index.html">Tableau de développement de diagnostic dans PVS-Studio</a></li>
<li><a href="../fr456394/index.html">Création de l'écran de démarrage omniprésent sur iOS</a></li>
<li><a href="../fr456398/index.html">Plugins Vue-cli, fonctionnant avec des données complexes et des tests basés sur les propriétés - Annonce Panda-Meetup Frontend</a></li>
<li><a href="../fr456400/index.html">Pourquoi la compétition est meilleure que le bourrage: notre expérience d'apprentissage de la gamification</a></li>
<li><a href="../fr456402/index.html">Dents de sagesse: Pull-Pull</a></li>
<li><a href="../fr456404/index.html">Looper - Plugin pour Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>