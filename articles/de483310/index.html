<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😭 🐳 🏚️ Erstellen Sie mit pix2pix Linsen für Snapchat 👵🏿 💝 👩🏻‍🎤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mein vorheriger Artikel hat fast den gleichen Titel, mit dem einzigen Unterschied, dass ich mithilfe von dlib und openCV Objektive für Snapchat algori...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen Sie mit pix2pix Linsen für Snapchat</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/483310/"><p>  Mein <a href="https://hackernoon.com/how-to-make-snapchat-lenses-f9eae861b5db">vorheriger Artikel hat</a> fast den gleichen Titel, mit dem einzigen Unterschied, dass ich mithilfe von dlib und openCV Objektive für Snapchat algorithmisch erstellt habe. Heute möchte ich zeigen, wie Sie das Ergebnis mithilfe von maschinellem Lernen erzielen können.  Dieser Ansatz ermöglicht es, den Algorithmus nicht manuell zu entwerfen, sondern das endgültige Bild direkt vom neuronalen Netzwerk zu erhalten. </p><br><p>  Folgendes bekommen wir: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/902/f22/0ab902f229dcfe1f01c1847320de0d67.gif" width="512" height="256"></div><a name="habracut"></a><br>
<h2>  Was ist pix2pix? </h2><br><p>  Auf diese Weise können Sie <a href="https://arxiv.org/abs/1611.07004">ein Bild mithilfe von Konkurrenznetzwerken</a> (im Allgemeinen als pix2pix bezeichnet) <a href="https://arxiv.org/abs/1611.07004">in ein Bild</a> konvertieren. </p><br><p>  Der Name "pix2pix" bedeutet, dass das Netzwerk darauf trainiert ist, das Eingabebild in das entsprechende Ausgabebild zu konvertieren.  Hier sind Beispiele für solche Transformationen: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26b/a03/964/26ba03964ee43872130e7b2f3cb045d6.jpg" width="1132" height="543"></div><br><p>  Das coolste Merkmal von pix2pix ist seine <strong>Vielseitigkeit</strong> .  Anstatt für jede der oben genannten Aufgaben einen neuen Algorithmus oder ein neues Modell zu erstellen, reicht es aus, nur verschiedene Datensätze zum Trainieren des Netzwerks zu verwenden. </p><br><p>  Im Gegensatz zu den früher verwendeten Ansätzen lernt pix2pix, Probleme viel schneller und mit einem kleineren Trainingssatz zu lösen.  Die folgenden Ergebnisse wurden beispielsweise beim Training mit der Pascal Titan X-GPU mit einem Datensatz von 400 Bildpaaren in weniger als zwei Stunden erzielt. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/811/84d/5af/81184d5af932f466916b4eec3fc3da97.png" width="1426" height="1044"></div><br><h2>  Wie funktioniert pix2pix? </h2><br><p>  pix2pix verwendet zwei neuronale Netze, die gleichzeitig lernen: </p><br><ol><li>  Generator </li><li>  Diskriminator </li></ol><br><p>  Der Generator versucht, ein Ausgabebild aus den eingegebenen Trainingsdaten zu erzeugen, und der Diskriminator versucht zu bestimmen, ob das Ergebnis real ist oder erzeugt wird. </p><br><p>  Wenn der Generator Bilder erzeugt, die nicht von den realen zu unterscheiden sind (Diskriminator), beginnen wir, den Diskriminator auf sie und die realen Bilder zu trainieren.  Wenn es dem Diskriminator gelingt, echte Bilder von den erzeugten zu unterscheiden, beginnen wir erneut, den Generator so zu trainieren, dass er wieder lernt, den Diskriminator zu täuschen. </p><br><p>  Ein solches „Wettrüsten“ führt dazu, dass es für eine Person schwierig wird, echte Bilder von generierten zu unterscheiden. </p><br><h2>  Übe </h2><br><p>  Wir werden unseren Filtergenerator für Snapchat mit 256x256 Bildern trainieren (für große Formate wird mehr Videospeicher benötigt).  Verwenden Sie zum Erstellen eines Datensatzes den <a href="https://github.com/smitshilu/SnapChatFilterExample">Code aus dem vorherigen Lernprogramm</a> . </p><br><p>  Ich habe viele Gesichtsbilder heruntergeladen und auf jedes einen <strong>„Thug Life Glasses“</strong> -Filter angewendet.  Es wird sich so etwas wie diese Paare ergeben: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d32/b6d/db6/d32b6ddb611c333f64e055b37ea5547b.jpg" width="1193" height="600"></div><br><p>  Um das Modell zu erstellen, verwenden Sie <a href="https://github.com/affinelayer/pix2pix-tensorflow">das</a> auf TensorFlow basierende pix2pix- <a href="https://github.com/affinelayer/pix2pix-tensorflow">Repository</a> .  Klonen Sie es und <a href="https://www.tensorflow.org/install">installieren Sie</a> Tensorflow. </p><br><p>  Der Befehl zum Starten des Trainings lautet wie folgt: </p><br><pre><code class="plaintext hljs">python pix2pix.py --mode train --output_dir dir_to_save_checkpoint --max_epochs 200 --input_dir dir_with_training_data --which_direction AtoB</code> </pre> <br><p>  Der Parameter <strong>which_direction</strong> legt die Trainingsrichtung fest.  <strong>AtoB</strong> bedeutet, dass wir Bild <strong>A</strong> (links, ohne Brille) in Bild <strong>B</strong> (rechts, mit Brille) <strong>umwandeln</strong> möchten.  Beachte übrigens, dass pix2pix erfolgreich lernen kann, wie man das Originalbild aus einem Bild mit einem Filter wiederherstellt. Ändere einfach die Trainingsrichtung. </p><br><p>  Sie können den Trainingsfortschritt mit Tensorboard überwachen, für das Sie den Befehl ausführen müssen: </p><br><pre> <code class="plaintext hljs">tensorboard --logdir=dir_to_save_checkpoint</code> </pre> <br><p>  Sobald Sie feststellen, dass die Ergebnisse der Trainingsdaten recht gut sind, können Sie das Training beenden und das Modell mit beliebigen Daten testen.  Sie können das Training ab dem letzten Kontrollpunkt wie folgt fortsetzen: </p><br><pre> <code class="plaintext hljs">python pix2pix.py --mode train --output_dir dir_to_save_checkpoint --max_epochs 200 --input_dir dir_with_training_data --which_direction AtoB --checkpoint dir_of_saved_checkpoint</code> </pre> <br><h2>  Fazit </h2><br><p>  Das Aufkommen von generativen Netzwerken vom Typ pix2pix eröffnet große Perspektiven für eine universelle und einfache Lösung für alle Arten von Bildverarbeitungsaufgaben. </p><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SJKHhLI31O8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de483310/">https://habr.com/ru/post/de483310/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de483290/index.html">Design Thinking im Projektmanagement oder warum braucht ein Projektmanager kreative Techniken?</a></li>
<li><a href="../de483294/index.html">Wir schreiben einen "Taschenrechner". Teil II Gleichungen lösen, in LaTeX rendern, Funktionen auf Superlight beschleunigen</a></li>
<li><a href="../de483298/index.html">Wie man richtig aufhört, damit später ...</a></li>
<li><a href="../de483302/index.html">Erste Schritte mit Google Sheets in Python. Von der Registrierung bis zum Lesen von Daten</a></li>
<li><a href="../de483308/index.html">VonmoTrade-Experiment. Teil 4: Trading Charts</a></li>
<li><a href="../de483312/index.html">Die große Schneeflockentheorie</a></li>
<li><a href="../de483314/index.html">So führen Sie asynchrone Redux-Aktionen mit Redux-Thunk durch</a></li>
<li><a href="../de483316/index.html">Schnelle Einführung in SwiftUI</a></li>
<li><a href="../de483318/index.html">Bot für die Überwachung von Webdiensten in einer halben Stunde: Telegramm + Bash + Cron</a></li>
<li><a href="../de483320/index.html">Schaffung eines mittelalterlichen VR-Museums</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>