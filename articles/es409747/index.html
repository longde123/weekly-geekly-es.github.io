<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüëß‚Äçüëß üëµüèΩ üçñ La red neuronal AttnGAN dibuja objetos en partes, utilizando el espacio vectorial de no solo oraciones, sino tambi√©n palabras üîÇ üíáüèΩ üôãüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ejemplo de operaci√≥n de AttnGAN. En la fila superior hay varias im√°genes de diferentes resoluciones generadas por una red neuronal. La segunda y terce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La red neuronal AttnGAN dibuja objetos en partes, utilizando el espacio vectorial de no solo oraciones, sino tambi√©n palabras</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/409747/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/gs/w1/lkgsw1oaf838l4x7vrtlww4dcam.jpeg"></div><br>  <i><font color="gray">Ejemplo de operaci√≥n de AttnGAN.</font></i>  <i><font color="gray">En la fila superior hay varias im√°genes de diferentes resoluciones generadas por una red neuronal.</font></i>  <i><font color="gray">La segunda y tercera fila muestran el procesamiento de las cinco palabras m√°s adecuadas por dos modelos de atenci√≥n de la red neuronal para dibujar las secciones m√°s relevantes.</font></i> <br><br>  La creaci√≥n autom√°tica de im√°genes a partir de descripciones de texto en un lenguaje natural es un problema fundamental para muchas aplicaciones, como la generaci√≥n de arte y el dise√±o de computadoras.  Este problema tambi√©n estimula el progreso en el campo del entrenamiento multimodal de IA con una relaci√≥n entre visi√≥n y lenguaje. <br><br>  Investigaciones recientes de investigadores en esta √°rea se basan en redes de confrontaci√≥n generativa (GAN).  El enfoque general es traducir la descripci√≥n del texto completo al vector de oraci√≥n global.  Este enfoque demuestra una serie de resultados impresionantes, pero tiene las principales desventajas: la falta de detalles claros a nivel de palabra y la incapacidad para generar im√°genes de alta resoluci√≥n.  Un equipo de desarrolladores de la Universidad de Lichai, la Universidad de Rutgers, la Universidad de Duke (todos - EE. UU.) Y Microsoft propusieron su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">propia</a> soluci√≥n al problema: la nueva red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Attenional Generative Adversarial Network (AttnGAN)</a> representa una mejora en el enfoque tradicional y permite el cambio en varias etapas de la imagen generada, cambiando palabras individuales en el texto descripci√≥n. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/2j/dv/5i/2jdv5ilcqwd6s4v5ymkjov4eq1i.png"><br><br>  <i><font color="gray">AttnGAN arquitectura de redes neuronales.</font></i>  <i><font color="gray">Cada modelo de atenci√≥n recibe autom√°ticamente condiciones (es decir, vectores de vocabulario correspondientes) para generar diferentes √°reas de la imagen.</font></i>  <i><font color="gray">El m√≥dulo DAMSM proporciona granularidad adicional para la funci√≥n de p√©rdida de conformidad en la traducci√≥n de imagen a texto en la red generativa</font></i> <br><br>  Como puede ver en la ilustraci√≥n que representa la arquitectura de la red neuronal, el modelo AttnGAN tiene dos innovaciones en comparaci√≥n con los enfoques tradicionales. <br><br>  En primer lugar, es una red de confrontaci√≥n, que se refiere a la atenci√≥n como un factor de aprendizaje (Red atencional generativa de confrontaci√≥n).  Es decir, implementa el mecanismo de atenci√≥n, que determina las palabras m√°s adecuadas para generar las partes correspondientes de la imagen.  En otras palabras, adem√°s de codificar la descripci√≥n del texto completo en el espacio vectorial global de las oraciones, cada palabra individual tambi√©n se codifica como un vector de texto.  En la primera etapa, la red neuronal generativa utiliza el espacio vectorial global de las oraciones para representar una imagen de baja resoluci√≥n.  En los siguientes pasos, ella usa el vector de imagen en cada regi√≥n para consultar los vectores del diccionario, usando la capa de atenci√≥n para formar el vector de contexto de la palabra.  Luego, el vector de imagen regional se combina con el vector de contexto de palabra correspondiente para formar un vector de contexto multimodal, en base al cual el modelo genera nuevas caracter√≠sticas de imagen en las regiones respectivas.  Esto le permite aumentar efectivamente la resoluci√≥n de toda la imagen como un todo, ya que en cada etapa hay m√°s y m√°s detalles. <br><br>  La segunda innovaci√≥n de red neuronal de Microsoft es el m√≥dulo Modelo de similitud multimodal de atenci√≥n profunda (DAMSM).  Usando el mecanismo de atenci√≥n, este m√≥dulo calcula el grado de similitud entre la imagen generada y la oraci√≥n de texto, utilizando tanto la informaci√≥n del nivel del espacio vectorial de las oraciones como un nivel bien detallado de los vectores del diccionario.  Por lo tanto, DAMSM proporciona una granularidad adicional para la funci√≥n de p√©rdida de ajuste en la traducci√≥n de imagen a texto cuando se entrena al generador. <br><br>  Gracias a estas dos innovaciones, la red neuronal AttnGAN muestra resultados significativamente mejores que los mejores sistemas GAN tradicionales, escriben los desarrolladores.  En particular, la puntuaci√≥n m√°xima de inicio conocida para las redes neuronales existentes mejor√≥ en un 14.14% (de 3.82 a 4.36) en el conjunto de datos CUB y mejor√≥ hasta en un 170.25% (de 9.58 a 25.89) en el conjunto de datos COCO m√°s sofisticado. <br><br>  La importancia de este desarrollo es dif√≠cil de sobreestimar.  La red neuronal AttnGAN por primera vez mostr√≥ que una red generativa-adversaria multicapa, que se refiere a la atenci√≥n como un factor de aprendizaje, puede determinar autom√°ticamente las condiciones de nivel de palabra para generar partes individuales de una imagen. <br><br>  El art√≠culo cient√≠fico fue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publicado</a> el 28 de noviembre de 2017 en el sitio de preimpresi√≥n arXiv.org (arXiv: 1711.10485v1). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es409747/">https://habr.com/ru/post/es409747/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es409735/index.html">Ilusi√≥n de nivel de consenso</a></li>
<li><a href="../es409739/index.html">Bloomberg: Telegram planea ganar m√°s de $ 1 mil millones durante el ICO</a></li>
<li><a href="../es409741/index.html">La Casa Blanca est√° interesada en lanzar Falcon Heavy</a></li>
<li><a href="../es409743/index.html">Controlador de bricolaje de panel LED en CPLD con modulaci√≥n BAM</a></li>
<li><a href="../es409745/index.html">El especialista en IA afirma que logr√≥ entender en qu√© idioma est√° escrito el manuscrito Voynich</a></li>
<li><a href="../es409749/index.html">Caldera de pir√≥lisis en el hogar, o cuando el precio del gas no importa</a></li>
<li><a href="../es409751/index.html">Letra de AudioFilkina: m√∫sica blue tooth, no para exagerar, sino para bien</a></li>
<li><a href="../es409753/index.html">La ONU puede completar un experimento aleatorio a gran escala para reducir el calentamiento global</a></li>
<li><a href="../es409757/index.html">El an√°lisis cient√≠fico de la infraestructura de Bitcoin y Ethereum muestra una mayor centralizaci√≥n de las redes.</a></li>
<li><a href="../es409759/index.html">Intel advirti√≥ a los proveedores chinos de vulnerabilidades de fusi√≥n y espectro ante el gobierno de EE. UU.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>