<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òØÔ∏è ‚úåüèæ üìó Ich sehe, es bedeutet, dass ich existiere: eine √úberpr√ºfung von Deep Learning in Computer Vision (Teil 1) ü§úüèΩ üç° üêî</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Computer Vision. Jetzt reden sie viel dar√ºber, wo es viel angewendet und implementiert wird. Und irgendwie gab es vor einiger Zeit keine √úbersichtsart...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ich sehe, es bedeutet, dass ich existiere: eine √úberpr√ºfung von Deep Learning in Computer Vision (Teil 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Computer Vision.  Jetzt reden sie viel dar√ºber, wo es viel angewendet und implementiert wird.  Und irgendwie gab es vor einiger Zeit keine √úbersichtsartikel √ºber Habr√© im Lebenslauf mit Beispielen f√ºr Architekturen und moderne Aufgaben.  Aber es gibt viele von ihnen und sie sind wirklich cool!  Wenn Sie daran interessiert sind, was jetzt in Computer Vision passiert, nicht nur aus Sicht der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Forschung und Artikel</a> , sondern auch aus Sicht der angewandten Probleme, dann sind Sie bei cat willkommen.  Der Artikel kann auch eine gute Einf√ºhrung f√ºr diejenigen sein, die schon lange anfangen wollten, all dies zu verstehen, aber etwas war im Weg;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="Bild"><br><a name="habracut"></a><br>  Heute gibt es bei PhysTech eine aktive Zusammenarbeit zwischen der "Akademie" und Industriepartnern.  Insbesondere gibt es viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interessante Labors</a> von Unternehmen wie Sberbank, Biocad, 1C, Tinkoff, MTS und Huawei an der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PhysTech School f√ºr Angewandte Mathematik und Informatik</a> . <br><br>  Ich wurde inspiriert, diesen Artikel zu schreiben, indem ich im Labor f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hybride intelligente Systeme arbeitete</a> , das von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VkusVill</a> er√∂ffnet wurde.  Das Labor hat eine ehrgeizige Aufgabe - ein Gesch√§ft zu bauen, das ohne Geldschalter funktioniert, haupts√§chlich mit Hilfe von Computer Vision.  Fast ein Jahr lang hatte ich die Gelegenheit, an vielen Aufgaben des Sehens zu arbeiten, die in diesen beiden Teilen er√∂rtert werden. <br><br><div class="spoiler">  <b class="spoiler_title">Ohne Kasse einkaufen?</b>  <b class="spoiler_title">Irgendwo habe ich es schon geh√∂rt ..</b> <div class="spoiler_text">  Wahrscheinlich, lieber Leser, haben Sie an <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Go</a> gedacht.  In gewissem Sinne besteht die Aufgabe darin, ihren Erfolg zu wiederholen, aber bei unserer Entscheidung geht es mehr um die Implementierung als darum, ein solches Gesch√§ft f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viel Geld</a> von Grund auf neu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufzubauen</a> . <br></div></div><br>  Wir werden nach Plan umziehen: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Motivation und was los ist</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einstufung als Lebensstil</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungsarchitekturen f√ºr neuronale Netze: 1000 Wege, um ein Ziel zu erreichen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Visualisierung von Faltungs-Neuronalen Netzen: Zeigen Sie mir Leidenschaft</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ich selbst bin eine Art Chirurg: Wir extrahieren Merkmale aus neuronalen Netzen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bleiben Sie nah dran: Repr√§sentationslernen f√ºr Menschen und Einzelpersonen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: <s>Erkennen, Bewerten der Haltung und Erkennen von Handlungen</s> ohne Spoiler</a> </li></ol><br><a name="1"></a><h2>  Motivation und was los ist </h2><br><div class="spoiler">  <b class="spoiler_title">F√ºr wen ist der Artikel?</b> <div class="spoiler_text">  Der Artikel konzentriert sich mehr auf Menschen, die bereits mit maschinellem Lernen und neuronalen Netzen vertraut sind.  Ich rate Ihnen jedoch, mindestens die ersten beiden Abschnitte zu lesen - pl√∂tzlich wird alles klar :) <br></div></div><br>  Im Jahr 2019 sprechen alle √ºber k√ºnstliche Intelligenz, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vierte industrielle Revolution</a> und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ann√§herung der Menschheit an eine Singularit√§t</a> .  Cool, cool, aber ich m√∂chte Einzelheiten.  Schlie√ülich sind wir neugierige Technikfreaks, die nicht an M√§rchen √ºber KI glauben, sondern an formale Aufgabenstellung, Mathematik und Programmierung.  In diesem Artikel werden wir √ºber spezifische F√§lle der Verwendung der sehr modernen KI sprechen - die Verwendung von Deep Learning (n√§mlich Faltungs-Neuronale Netze) in einer Vielzahl von Computer-Vision-Aufgaben. <br><br>  Ja, wir werden speziell √ºber Gitter sprechen und manchmal einige Ideen aus einer ‚Äûklassischen‚Äú Sicht erw√§hnen (wir werden die Methoden in der Vision nennen, die vor neuronalen Netzen verwendet wurden, aber dies bedeutet keineswegs, dass sie jetzt nicht verwendet werden). <br><br><div class="spoiler">  <b class="spoiler_title">Ich m√∂chte Computer Vision von Grund auf lernen</b> <div class="spoiler_text">  Ich empfehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anton Konushins Kurs "Einf√ºhrung in Computer Vision"</a> .  Pers√∂nlich habe ich das Gegenst√ºck in SHAD durchgesehen, das eine solide Grundlage f√ºr das Verst√§ndnis der Bild- und Videoverarbeitung gelegt hat. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="Bild" width="300"></div><br>  Meiner Meinung nach ist die erste wirklich interessante Anwendung neuronaler Netze in der Vision, √ºber die 1993 in den Medien berichtet wurde, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Handschrifterkennung durch</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jan LeCun</a> .  Jetzt ist er einer der Haupt-KI in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Facebook-KI-Forschung</a> . Ihr Team hat bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele n√ºtzliche Open Source-Inhalte ver√∂ffentlicht</a> . <br><br>  Vision wird heute in vielen Bereichen eingesetzt.  Ich werde nur einige bemerkenswerte Beispiele nennen: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="Bild" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="Bild" width="400" height="300"></div><br><br>  <i>Unbemannte Fahrzeuge von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tesla</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="Bild" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Medizinische Bildgebungsanalyse</a> und <a href="">Krebsvorhersage</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="Bild" width="500"></div><br><br>  <i>Spielekonsolen: Kinect 2.0 (obwohl immer noch Tiefeninformationen verwendet werden, dh RGB-D-Bilder)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="Bild" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Gesichtserkennung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apple FaceID</a> (mit mehreren Sensoren)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="Bild" width="400"></div><br><br>  <i>Gesichtspunktbewertung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Snapchat-Masken</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="Bild" width="400"></div><br><br>  <i>Biometrie der Gesichts- und Augenbewegungen (ein Beispiel aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projekt von FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="Bild" width="400"></div><br><br>  <i>Suche nach Bild: Yandex und Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="Bild" width="500"></div><br><br>  <i>Erkennung des Bildtextes ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">optische Zeichenerkennung</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="Bild" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="Bild" width="400"></div><br><br>  <i>Drohnen und Roboter: Empfangen und Verarbeiten von Informationen durch Sehen</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="Bild" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kilometerz√§hler</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen</a> einer Karte und Planen beim Bewegen von Robotern</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="Bild" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbesserung von Grafiken und Texturen in Videospielen</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="Bild" width="200" height="300"></div><br><br>  <i>Bild√ºbersetzung: Yandex und Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="Bild" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="Bild" width="500"></div><br><br>  <i>Augmented Reality: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprungbewegung</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Projekt</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nordstern</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">)</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="Bild" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Stil- und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Textur√ºbertragung</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PicsArt</a></i> <br><br>  Ganz zu schweigen von den zahlreichen Anwendungen in verschiedenen internen Aufgaben von Unternehmen.  Facebook verwendet beispielsweise auch Vision, um Medieninhalte zu filtern.  Computer Vision-Methoden werden auch bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Qualit√§ts- / Schadenspr√ºfungen in der Industrie eingesetzt</a> . <br><br>  Augmented Reality muss hier in der Tat besondere Aufmerksamkeit gewidmet werden, da <s>es</s> in naher Zukunft <s>nicht funktioniert</s> , kann dies einer der Hauptanwendungsbereiche des Sehens werden. <br><br>  Motiviert.  Aufgeladen.  Lass uns gehen: <br><br><a name="2"></a><h2>  Einstufung als Lebensstil </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="Bild"><br><br>  Wie gesagt, in den 90ern wurden die Netze in Sichtweite abgefeuert.  Und sie haben in einer bestimmten Aufgabe gedreht - der Aufgabe, Bilder von handgeschriebenen Zahlen zu klassifizieren (der ber√ºhmte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST-Datensatz</a> ).  Historisch gesehen war es die Aufgabe, Bilder zu klassifizieren, die zur Grundlage f√ºr die L√∂sung fast aller nachfolgenden Aufgaben in der Vision wurde.  Betrachten Sie ein bestimmtes Beispiel: <br><br>  <b>Aufgabe</b> : Am Eingang befindet sich ein Ordner mit Fotos. Jedes Foto hat ein bestimmtes Objekt: entweder eine Katze, einen Hund oder eine Person (auch wenn es keine "M√ºll" -Fotos gibt, ist dies eine nicht unbedingt wichtige Aufgabe, aber Sie m√ºssen irgendwo anfangen).  Sie m√ºssen die Bilder in drei Ordner zerlegen: <code>/cats</code> , <code>/dogs</code> und <s><code>/leather_bags</code></s> <code>/humans</code> , wobei nur Fotos mit den entsprechenden Objekten in jedem Ordner <s><code>/leather_bags</code></s> . <br><br><div class="spoiler">  <b class="spoiler_title">Was ist ein Bild / Foto?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="Bild"><br>  Fast √ºberall in der Vision ist es √ºblich, mit Bildern im RGB-Format zu arbeiten.  Jedes Bild hat eine H√∂he (H), eine Breite (B) und eine Tiefe von 3 (Farben).  Somit kann ein Bild als Tensor der Dimension HxBx3 dargestellt werden (jedes Pixel ist ein Satz von drei Zahlen - Intensit√§tswerte in den Kan√§len). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Stellen Sie sich vor, wir sind noch nicht mit Computer Vision vertraut, aber wir kennen uns mit maschinellem Lernen aus.  Bilder sind einfach numerische Tensoren im Speicher des Computers.  Wir formalisieren die Aufgabe in Bezug auf maschinelles Lernen: Objekte sind Bilder, ihre Zeichen sind Werte in Pixel, die Antwort f√ºr jedes der Objekte ist eine Klassenbezeichnung (Katze, Hund oder Person).  Dies ist eine reine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierungsaufgabe</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Wenn es jetzt schwierig geworden ist ..</b> <div class="spoiler_text">  ... dann ist es besser, zuerst die ersten 4 Artikel aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem OpenDataScience ML Open Course zu</a> lesen und einen einf√ºhrenden Artikel √ºber Vision zu lesen, zum Beispiel einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">guten Vortrag in Small ShAD</a> . <br></div></div><br>  Sie k√∂nnen einige Methoden aus der ‚Äûklassischen‚Äú Sicht oder dem ‚Äûklassischen‚Äú maschinellen Lernen verwenden, dh nicht aus einem neuronalen Netzwerk.  Grunds√§tzlich bestehen diese Methoden darin, die Bilder bestimmter Merkmale (Sonderpunkte) oder lokaler Regionen hervorzuheben, die das Bild charakterisieren (‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tasche mit visuellen W√∂rtern</a> ‚Äú).  Normalerweise l√§uft alles auf etwas wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SVM</a> √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SIFT hinaus</a> . <br><br>  Wir haben uns hier versammelt, um √ºber neuronale Netze zu sprechen. Wir m√∂chten also nicht die von uns erfundenen Zeichen verwenden, sondern dass das Netzwerk alles f√ºr uns tut.  Unser Klassifikator nimmt die Vorzeichen eines Objekts als Eingabe und gibt eine Vorhersage (Klassenbezeichnung) zur√ºck.  Hier wirken die Intensit√§tswerte in Pixel als Vorzeichen (siehe Bildmodell in <br>  Spoiler oben).  Denken Sie daran, dass ein Bild ein Gr√∂√üentensor (H√∂he, Breite, 3) ist (wenn es Farbe ist).  Wenn Sie lernen, in das Raster einzutreten, wird dies normalerweise nicht von einem Bild und nicht von einem ganzen Datensatz, sondern von Stapeln, d. H.  in kleinen Teilen von Objekten (z. B. 64 Bilder im Stapel). <br><br>  Somit empf√§ngt das Netzwerk einen Eingangstensor der Gr√∂√üe (BATCH_SIZE, H, W, 3).  Sie k√∂nnen jedes Bild in eine Vektorlinie aus H * W * 3-Zahlen ‚Äûerweitern‚Äú und mit den Werten in Pixeln arbeiten, genau wie mit Zeichen beim maschinellen Lernen. Ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">normales Multilayer-Perceptron (MLP)</a> w√ºrde genau das tun, aber ehrlich gesagt ist es so Basislinie, da beim Arbeiten mit Pixeln als Vektorzeile beispielsweise die translatorische Invarianz von Objekten im Bild nicht ber√ºcksichtigt wird.  Dieselbe Katze kann sich in der Mitte des Fotos befinden, und in der Ecke lernt MLP dieses Muster nicht. <br><br>  Sie brauchen also etwas Kl√ºgeres, zum Beispiel eine Faltungsoperation.  Und hier geht es um moderne Vision, um <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungs-Neuronale Netze</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">Der Trainingscode f√ºr das Faltungsnetzwerk sieht m√∂glicherweise ungef√§hr so ‚Äã‚Äãaus (im PyTorch-Framework).</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Da wir jetzt √ºber das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Training mit einem Lehrer</a> sprechen, ben√∂tigen wir mehrere Komponenten f√ºr das Training eines neuronalen Netzwerks: <br><br><ul><li>  Daten (bereits vorhanden) </li><li>  Netzwerkarchitektur (Highlight) </li><li>  Eine Verlustfunktion, die angibt, wie das neuronale Netzwerk zu lernen ist (hier ist es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kreuzentropie</a> ). </li><li>  Optimierungsmethode (√§ndert das Netzwerkgewicht in die richtige Richtung) </li><li>  Definieren Sie Architektur- und Optimierer-Hyperparameter (z. B. Schrittgr√∂√üe des Optimierers, Anzahl der Neuronen in Schichten, Regularisierungskoeffizienten) </li></ul><br>  Dies ist genau das, was im Code implementiert ist: Das Faltungs-Neuronale Netzwerk selbst wird in der Klasse Net () beschrieben. <br><br>  Wenn Sie langsam und von Anfang an etwas √ºber Faltung und Faltungsnetzwerke lernen m√∂chten, empfehle ich einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vortrag an der Deep Learning School (MIPT MIPT) (auf Russisch)</a> zu diesem Thema und nat√ºrlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanfords Kurs cs231n (auf Englisch)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - was ist das?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Learning School</a> im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Labor f√ºr Innovation FPMI MIPT</a> ist eine Organisation, die sich aktiv an der Entwicklung eines offenen russischsprachigen Kurses √ºber neuronale Netze beteiligt.  In diesem Artikel werde ich mehrmals auf diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video-Tutorials</a> verweisen. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="Bild" width="650"></div><br>  Kurz gesagt, mit der Faltungsoperation k√∂nnen Sie Muster auf Bildern anhand ihrer Variabilit√§t finden.  Wenn wir Faltungs-Neuronale Netze trainieren (dt.: Faltungs-Neuronale Netze), finden wir tats√§chlich Faltungsfilter (Neuronengewichte), die Bilder gut beschreiben und so gut, dass wir die Klasse daraus genau bestimmen k√∂nnen.  Es wurden viele Wege erfunden, um ein solches Netzwerk aufzubauen.  Mehr als Sie denken ... <br><br><a name="3"></a><h3>  Faltungsarchitekturen f√ºr neuronale Netze: 1000 Wege, um ein Ziel zu erreichen </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="Bild" width="500"></div><br><br>  Ja, ja, noch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine architektonische √úberpr√ºfung</a> .  Aber hier werde ich versuchen, es so relevant wie m√∂glich zu machen! <br><br>  Zuerst gab es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LeNet</a> , es half Jan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LeCun</a> 1998, Zahlen zu erkennen.  Dies war das erste Faltungsnetzwerk zur Klassifizierung.  Ihr Hauptmerkmal war, dass sie im Grunde genommen anfing, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungs- und Pooling-</a> Operationen durchzuf√ºhren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="Bild"><br><br>  Dann gab es eine Pause bei der Entwicklung von Grids, aber die Hardware stand nicht still, und es wurden effektive Berechnungen f√ºr GPU und <abbr title="Beschleunigte lineare Algebra">XLA entwickelt</abbr> .  Im Jahr 2012 erschien AlexNet, sie schoss im ILSVRC-Wettbewerb ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet Large-Scale Visual Recognition Challenge</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Ein kleiner Exkurs √ºber ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet</a> wurde bis 2012 zusammengestellt und eine Teilmenge von Tausenden von Bildern und 1000 Klassen wurde f√ºr den ILSVRC-Wettbewerb verwendet.  ImageNet hat derzeit ~ 14 Millionen Bilder und 21.841 Klassen (von der offiziellen Website √ºbernommen), aber f√ºr den Wettbewerb w√§hlen sie normalerweise nur eine Teilmenge aus.  ILSVRC wurde dann zum gr√∂√üten j√§hrlichen Bildklassifizierungswettbewerb.  √úbrigens haben wir k√ºrzlich herausgefunden, wie man <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in wenigen Minuten auf ImageNet trainiert</a> . <br><br>  Von 2010 bis 2018 erhielten sie im ImageNet (in ILSVRC) <abbr title="Stand der Technik">SOTA-</abbr> Netzwerke f√ºr die Klassifizierung von Bildern.  Zwar sind seit 2016 eher Wettbewerbe zur Lokalisierung, Erkennung und zum Verst√§ndnis der Szene als zur Klassifizierung relevanter. <br></div></div><br>  In der Regel werfen verschiedene <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Architekturpr√ºfungen</a> Licht auf diejenigen, die von 2010 bis 2016 die ersten beim ILSVRC waren, sowie auf einige einzelne Netzwerke.  Um die Geschichte nicht zu √ºberladen, habe ich sie unter den Spoiler gelegt und versucht, die Hauptideen hervorzuheben: <br><br><div class="spoiler">  <b class="spoiler_title">Architektur von 2012 bis 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Gewicht </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alexnet</a> </td><td>  Verwenden Sie zwei B√ºndel hintereinander.  Teilen Sie das Netzwerktraining in zwei parallele Zweige auf </td><td>  240 MB </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zfnet</a> </td><td>  Filtergr√∂√üe, Anzahl der Filter in Schichten </td><td>  - - </td></tr><tr><td>  2013 </td><td>  <a href="">√úberfeature</a> </td><td>  einer der ersten Detektoren f√ºr neuronale Netze </td><td>  - - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  Netzwerktiefe (13-19 Schichten), Verwendung mehrerer Conv-Conv-Pool-Bl√∂cke mit kleinerer Faltungsgr√∂√üe (3x3) </td><td>  549 MB (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Inception (v1) (auch bekannt als GoogLeNet)</a> </td><td>  1x1-Faltung (Idee aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-in-Netzwerk</a> ), Hilfsverluste (oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tiefe √úberwachung</a> ), Stapeln der Ausg√§nge mehrerer Faltungen (Inception-Block) </td><td>  - - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Restverbindungen</a> , sehr tief (152 Schichten ..) </td><td>  98 MB (ResNet-50), 232 MB (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  Die Ideen all dieser Architekturen (mit Ausnahme von ZFNet, es wird normalerweise wenig erw√§hnt) waren zu einer Zeit ein neues Wort in neuronalen Netzen f√ºr Vision.  Nach 2015 gab es jedoch viele weitere wichtige Verbesserungen, beispielsweise Inception-ResNet, Xception, DenseNet, SENet.  Unten habe ich versucht, sie an einem Ort zu sammeln. <br><br><div class="spoiler">  <b class="spoiler_title">Architektur von 2015 bis 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Gewicht </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inception v2 und v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zerlegung von Paketen in Pakete 1xN und Nx1</a> </td><td>  92 MB </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inception v4 und Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kombination von Inception und ResNet</a> </td><td>  215 MB </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Resnext</a> </td><td>  2. Platz ILSVRC, die Verwendung vieler Zweige ("generalisierter" Inception-Block) </td><td>  - - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tief durchtrennbare Faltung</a> , wiegt weniger mit vergleichbarer Genauigkeit wie Inception </td><td>  88 MB </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dichter Block</a>  leicht aber genau </td><td>  33 MB (DenseNet-121), 80 MB (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quetsch- und Erregungsblock</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">46 MB (SENet-Inception), 440 MB (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  Die meisten dieser Modelle f√ºr PyTorch finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> , und es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">so eine coole Sache</a> . <br><br>  Sie haben vielleicht bemerkt, dass das Ganze ziemlich viel wiegt (ich m√∂chte maximal 20 MB oder sogar weniger), w√§hrend heutzutage mobile Ger√§te √ºberall verwendet werden und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Internet</a> der Dinge immer beliebter wird, was bedeutet, dass Sie dort auch Gitter verwenden m√∂chten. <br><br><div class="spoiler">  <b class="spoiler_title">Beziehung zwischen Modellgewicht und Geschwindigkeit</b> <div class="spoiler_text">  Da die neuronalen Netze in sich nur Tensoren multiplizieren, wirkt sich die Anzahl der Multiplikationsoperationen (sprich: die Anzahl der Gewichte) direkt auf die Arbeitsgeschwindigkeit aus (wenn keine arbeitsintensive Nach- oder Vorverarbeitung verwendet wird).  Die Geschwindigkeit des Netzwerks selbst h√§ngt von der Implementierung (Framework), der Hardware, auf der es ausgef√ºhrt wird, und der Gr√∂√üe des Eingabebilds ab. <br></div></div><br>  Die Autoren vieler Artikel gingen den Weg, schnelle Architekturen zu erfinden. Ich sammelte ihre Methoden unter dem folgenden Spoiler: <br><br><div class="spoiler">  <b class="spoiler_title">CNN Lightweight-Architektur</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schl√ºsselidee </th><th>  Gewicht </th><th>  Implementierungsbeispiel </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FireModule-Komprimierung</a> </td><td>  0,5 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dies ist ein Netzwerk aus der Kategorie AutoML, das durch eine neuronale Suche nach Architekturen erhalten wurde</a> </td><td>  23 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Punktweise Gruppenkonv., Kanalmischung</a> </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tief trennbare Windungen und viele andere Tricks</a> </td><td>  16 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ich empfehle diesen Artikel √ºber Habr√©</a> </td><td>  14 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Squeezenext</a> </td><td>  Siehe Bilder im Original-Repository </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Suche nach neuronalen Architekturen speziell f√ºr mobile Ger√§te mit RL</a> </td><td>  ~ 2 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v3)</a> </td><td>  Sie kam heraus, w√§hrend ich einen Artikel schrieb :) </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Die Nummern in allen Tabellen <s>stammen aus der Obergrenze</s> der Repositorys, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Keras-Anwendungstabelle</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> . <br><br>  Sie fragen: ‚ÄûWarum haben Sie √ºber diesen ganzen‚Äû Zoo ‚Äúvon Modellen geschrieben?  Und warum ist die Aufgabe der Klassifizierung?  Aber wir wollen Maschinen das Sehen beibringen, und Klassifizierung ist nur eine enge Aufgabe. ‚Äú  Tatsache ist, dass neuronale Netze zum Erkennen von Objekten, zum Bewerten von K√∂rperhaltungen / Punkten, zum erneuten Identifizieren und Suchen in einem Bild pr√§zise Modelle zur Klassifizierung als <b><abbr title="Basis, buchst√§blich - die Wirbels√§ule">R√ºckgrat verwenden</abbr></b> und 80% des Erfolgs von ihnen abh√§ngen. <br><br>  Aber ich m√∂chte CNN irgendwie mehr vertrauen, oder sie haben sich Black Boxes ausgedacht, aber was ‚Äûdrinnen‚Äú ist, ist nicht offensichtlich.  Um den Funktionsmechanismus von Faltungsnetzwerken besser zu verstehen, entwickelten die Forscher die Verwendung von Visualisierung. <br><br><a name="4"></a><h3>  Visualisierung von Faltungs-Neuronalen Netzen: Zeigen Sie mir Leidenschaft </h3><br>  Ein wichtiger Schritt, um zu verstehen, was in Faltungsnetzwerken geschieht, ist der Artikel <a href="">‚ÄûVisualisieren und Verstehen von Faltungsnetzwerken‚Äú</a> .  Darin schlugen die Autoren verschiedene M√∂glichkeiten vor, um genau zu visualisieren, worauf (auf welchen Teilen des Bildes) Neuronen in verschiedenen CNN-Schichten reagieren (ich empfehle auch, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Stanford-Vortrag zu diesem Thema</a> anzuschauen).  Die Ergebnisse waren sehr beeindruckend: Die Autoren zeigten, dass die ersten Schichten des Faltungsnetzwerks auf einige ‚ÄûDinge auf niedriger Ebene‚Äú durch die Art der Kanten / Winkel / Linien reagieren, und die letzten Schichten bereits auf ganze Teile der Bilder (siehe Bild unten), dh sie tragen bereits an sich einige Semantik. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="Bild"></div><br><br>  Dar√ºber hinaus hat das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projekt zur Tiefenvisualisierung von der Cornell University und das Unternehmen</a> die Visualisierung noch weiter vorangetrieben, w√§hrend der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ber√ºhmte DeepDream</a> gelernt hat, in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">s√ºchtig machenden</a> interessanten Stil zu verzerren (unten ein Bild von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="Bild" width="500"></div><br><br>  2017 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde auf Distill</a> ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehr guter Artikel ver√∂ffentlicht</a> , in dem eine detaillierte Analyse dessen durchgef√ºhrt wurde, was jede Ebene ‚Äûsieht‚Äú. Zuletzt (im M√§rz 2019) erfand Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aktivierungsatlanten</a> : einzigartige Karten, die f√ºr jede Netzwerkschicht erstellt werden k√∂nnen. Dies bringt uns dem Verst√§ndnis des Gesamtbildes der Arbeit von CNN n√§her. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Wenn Sie selbst mit Visualisierung spielen m√∂chten, w√ºrde ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lucid</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorSpace</a> empfehlen. <br><br>  Okay, CNN scheint bis zu einem gewissen Grad wahr zu sein.  Wir m√ºssen lernen, wie man dies bei anderen Aufgaben verwendet und nicht nur bei der Klassifizierung.  Dies wird uns helfen, Embedding'ov-Bilder zu extrahieren und Lernen zu √ºbertragen. <br><br><a name="5"></a><h2>  Ich selbst bin eine Art Chirurg: Wir extrahieren Merkmale aus neuronalen Netzen </h2><br>  Stellen Sie sich vor, es gibt ein Bild, und wir m√∂chten diejenigen finden, die visuell so aussehen (dies ist beispielsweise die Suche in einem Bild in Yandex.Pictures).  Fr√ºher (vor neuronalen Netzen) haben Ingenieure Funktionen daf√ºr manuell extrahiert, z. B. etwas erfunden, das das Bild gut beschreibt und es mit anderen vergleichen kann.  Grunds√§tzlich arbeiten diese Methoden ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SIFT</a> ) mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bildverl√§ufen</a> , normalerweise werden diese Dinge als "klassische" Bilddeskriptoren bezeichnet.  Von besonderem Interesse beziehe ich mich auf den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> und den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurs von Anton Konushin</a> (dies ist keine Werbung, nur ein guter Kurs :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="Bild" width="500"></div><br><br>  Mithilfe neuronaler Netze k√∂nnen wir diese Merkmale und Heuristiken nicht selbst erfinden, sondern das Modell richtig trainieren und dann <b>die Ausgabe einer oder mehrerer Schichten des Netzes als Zeichen des Bildes verwenden</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="Bild" width="650"></div><br>  Ein genauerer Blick auf alle Architekturen macht deutlich, dass die Klassifizierung in CNN zwei Schritte umfasst: <br>  1).  <b>Feature-Extraktor-</b> Ebenen zum Extrahieren informativer Features aus Bildern mithilfe von Faltungsebenen <br>  2).  Lernen √ºber diese Funktionen hinaus <b>Fully Connected (FC)</b> -Klassifiziererebenen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="Bild" width="500"></div><br><br>  <b>Beim Einbetten von Bildern (Features)</b> geht es nur darum, dass Sie ihre Zeichen nach dem Feature-Extraktor eines Faltungs-Neuronalen Netzwerks (obwohl sie auf unterschiedliche Weise aggregiert werden k√∂nnen) als informative Beschreibung von Bildern verwenden k√∂nnen.  Das hei√üt, wir haben das Netzwerk f√ºr die Klassifizierung geschult und nehmen dann einfach den Ausgang vor den Klassifizierungsebenen.  Diese Zeichen werden als <i>Merkmale</i> , <i>neuronale Netzwerkdeskriptoren</i> oder <i>Bildeinbettungen bezeichnet</i> (obwohl Einbettungen im NLP normalerweise akzeptiert werden, da dies eine Vision ist, spreche ich h√§ufig <i>Merkmale</i> ).  Normalerweise ist dies eine Art numerischer Vektor, zum Beispiel 128 Zahlen, mit denen Sie bereits arbeiten k√∂nnen. <br><br><div class="spoiler">  <b class="spoiler_title">Aber was ist mit Auto-Encodern?</b> <div class="spoiler_text">  Ja, tats√§chlich k√∂nnen Funktionen von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auto-Encodern erhalten werden</a> .  In meiner Praxis haben sie dies auf unterschiedliche Weise getan, aber zum Beispiel in Artikeln zur Neuidentifizierung (auf die sp√§ter noch eingegangen wird) nehmen sie immer noch Funktionen nach dem Extraktor an, anstatt den Auto-Encoder daf√ºr zu trainieren.  Es scheint mir, dass es sich lohnt, Experimente in beide Richtungen durchzuf√ºhren, wenn die Frage ist, was besser funktioniert. <br></div></div><br>  Somit kann die Pipeline zur L√∂sung <b>des Suchproblems in einem Bild</b> einfach angeordnet werden: Wir f√ºhren die Bilder durch CNN, nehmen Zeichen aus den gew√ºnschten Ebenen und vergleichen diese Merkmale aus verschiedenen Bildern miteinander.  Zum Beispiel betrachten wir einfach den euklidischen Abstand dieser Vektoren. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="Bild" width="500"></div><br><br>  <b>Transfer Learning</b> ist eine bekannte Technik f√ºr das effektive Training neuronaler Netze, die bereits f√ºr ihre Aufgabe an einem bestimmten Datensatz trainiert wurden.  Oft hei√üt es auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feinabstimmung</a> anstelle von Transferlernen. In den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanford-Kursnotizen cs231 werden</a> diese Konzepte geteilt. Transferlernen ist eine allgemeine Idee, und Feinabstimmung ist eine der Implementierungen der Technik.  Dies ist f√ºr uns in Zukunft nicht so wichtig. Die Hauptsache ist zu verstehen, dass wir das Netzwerk nur so trainieren k√∂nnen, dass es den neuen Datensatz gut vorhersagt, und zwar nicht ausgehend von zuf√§lligen Gewichten, sondern von solchen, die auf einem gro√üen ImageNet-Typ trainiert wurden.  Dies gilt insbesondere dann, wenn nur wenige Daten vorhanden sind und Sie das Problem qualitativ l√∂sen m√∂chten. <br><br><div class="spoiler">  <b class="spoiler_title">Erfahren Sie mehr √ºber Transfer Learning</b> <div class="spoiler_text">  <a href="">Originalartikel</a> , aber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">warum viel Text lesen, wenn man sich das Video ansehen kann</a> <br></div></div><br>  Es reicht jedoch m√∂glicherweise nicht aus, nur die erforderlichen Funktionen zu √ºbernehmen und zus√§tzliche Schulungen vom Datensatz zum Datensatz durchzuf√ºhren, um beispielsweise nach √§hnlichen Personen / Personen / etwas Bestimmtem zu suchen.  Fotos derselben Person k√∂nnen manchmal visuell noch un√§hnlicher sein als Fotos verschiedener Personen.  Es ist notwendig, dass das Netzwerk genau die Zeichen hervorhebt, die einer Person / einem Objekt innewohnen, auch wenn es f√ºr uns schwierig ist, dies mit unseren Augen zu tun.  Willkommen in der Welt des <b>Repr√§sentationslernens</b> . <br><br><a name="6"></a><h2>  Bleiben Sie nah dran: Repr√§sentationslernen f√ºr Menschen und Einzelpersonen </h2><br><div class="spoiler">  <b class="spoiler_title">Terminologie Hinweis</b> <div class="spoiler_text">  Wenn Sie wissenschaftliche Artikel lesen, scheinen einige Autoren den Ausdruck <b>metrisches Lernen</b> manchmal anders zu verstehen, und es besteht kein Konsens dar√ºber, welche Methoden als metrisches Lernen bezeichnet werden sollen und welche nicht.  Aus diesem Grund habe ich mich in diesem Artikel dazu entschlossen, diesen bestimmten Satz zu vermeiden und ein logischeres <b>Darstellungslernen zu verwenden</b> . Einige Leser stimmen dem m√∂glicherweise nicht zu. Ich werde dies gerne in den Kommentaren diskutieren. <br></div></div><br>  Wir stellen die Aufgaben: <br><br><ul><li>  <b>Aufgabe 1</b> : Es gibt eine Galerie (Set) mit Fotos von Gesichtern von Personen. Wir m√∂chten, dass das Netzwerk gem√§√ü einem neuen Foto entweder mit dem Namen einer Person aus der Galerie antworten kann (angeblich ist es das) oder dass es keine solche Person in der Galerie gibt (und wir f√ºgen sie m√∂glicherweise hinzu neue Person) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Aufgabe 2</b> : Das Gleiche, aber wir arbeiten nicht mit Fotos von Gesichtern, sondern mit Menschen in voller L√§nge <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  Die erste Aufgabe wird normalerweise als <b>Gesichtserkennung bezeichnet</b> , die zweite als <i>Neuidentifizierung</i> (abgek√ºrzt als <i>Reid</i> ).  Ich habe sie in einem Block zusammengefasst, weil ihre L√∂sungen heute √§hnliche Ideen verwenden: Um effektive Bildeinbettungen zu erlernen, die mit ziemlich schwierigen Situationen umgehen k√∂nnen, verwenden sie heute verschiedene Arten von Verlusten, wie zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Triplettverlust</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vierfachverlust</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontrastmittelverlust</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kosinusverlust</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Es gibt immer noch wunderbare <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">siamesische</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerke</a> , aber ich habe sie ehrlich gesagt nicht selbst benutzt.  √úbrigens ‚Äûentscheidet‚Äú nicht nur der Verlust selbst, sondern auch, wie Paare von Positiven und Negativen daf√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abgetastet werden,</a> betonen die Autoren des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sampling, die beim tiefgreifenden Einbetten des Lernens eine</a> Rolle spielen. <br><br>  Das Wesentliche all dieser Verluste und siamesischen Netzwerke ist einfach: Wir m√∂chten, dass die Bilder einer Klasse (Person) im latenten Raum der Merkmale (Einbettungen) ‚Äûnah‚Äú und verschiedener Klassen (Menschen) ‚Äûfern‚Äú sind.  Die N√§he wird normalerweise wie folgt gemessen: Es werden Einbettungen von Bildern aus einem neuronalen Netzwerk vorgenommen (z. B. ein Vektor mit 128 Zahlen), und wir betrachten entweder den √ºblichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">euklidischen Abstand</a> zwischen diesen Vektoren oder die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kosinusn√§he.</a>  Wie Sie messen k√∂nnen, ist besser, wenn Sie einen Datensatz / eine Aufgabe ausw√§hlen. <br><br>  Eine schematische Darstellung einer Probleml√∂sungspipeline zum Lernen von Darstellungen sieht ungef√§hr so ‚Äã‚Äãaus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Genauer gesagt, so</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        ‚Äî   .   ,     ‚Äî     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="Bild" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="menschliche Erkennung aus einem gro√üen Foto geschnitten"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> Reid ‚Äî    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 2016 </a> . ,     ,    ‚Äî   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="Bild" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   ‚Äî    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TorchReid</a> .      ‚Äî   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  ,   </a> ).   ?  ‚Ä¶ <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="Bild" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de450732/">https://habr.com/ru/post/de450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de450720/index.html">So entwickeln Sie eine benutzerfreundliche Anwendung</a></li>
<li><a href="../de450724/index.html">Einf√ºhrung von Python f√ºr Kameraden, die der ‚ÄûA vs. V-Sprache‚Äú entwachsen sind Sprache B "und andere Vorurteile</a></li>
<li><a href="../de450726/index.html">Erstellen eines Tools zum schnellen und effizienten Schreiben von Autotests auf Selen</a></li>
<li><a href="../de450728/index.html">NLog: Regeln und Filter</a></li>
<li><a href="../de450730/index.html">ok.tech: Frontend-Treffen</a></li>
<li><a href="../de450734/index.html">Fuzzing ist ein wichtiger Schritt f√ºr eine sichere Entwicklung</a></li>
<li><a href="../de450736/index.html">"Das Internet zu isolieren ist viel einfacher und billiger als das externe Blockieren."</a></li>
<li><a href="../de450738/index.html">Roboter im Rechenzentrum: Wie kann k√ºnstliche Intelligenz n√ºtzlich sein?</a></li>
<li><a href="../de450740/index.html">Smart Lamp Base REDMOND - Erg√§nzen Sie das Smart Home</a></li>
<li><a href="../de450744/index.html">Fahrradinfrastruktur in Minsk f√ºr einen IT-Expat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>