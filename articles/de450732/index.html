<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☯️ ✌🏾 📗 Ich sehe, es bedeutet, dass ich existiere: eine Überprüfung von Deep Learning in Computer Vision (Teil 1) 🤜🏽 🍡 🐔</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Computer Vision. Jetzt reden sie viel darüber, wo es viel angewendet und implementiert wird. Und irgendwie gab es vor einiger Zeit keine Übersichtsart...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ich sehe, es bedeutet, dass ich existiere: eine Überprüfung von Deep Learning in Computer Vision (Teil 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Computer Vision.  Jetzt reden sie viel darüber, wo es viel angewendet und implementiert wird.  Und irgendwie gab es vor einiger Zeit keine Übersichtsartikel über Habré im Lebenslauf mit Beispielen für Architekturen und moderne Aufgaben.  Aber es gibt viele von ihnen und sie sind wirklich cool!  Wenn Sie daran interessiert sind, was jetzt in Computer Vision passiert, nicht nur aus Sicht der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Forschung und Artikel</a> , sondern auch aus Sicht der angewandten Probleme, dann sind Sie bei cat willkommen.  Der Artikel kann auch eine gute Einführung für diejenigen sein, die schon lange anfangen wollten, all dies zu verstehen, aber etwas war im Weg;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="Bild"><br><a name="habracut"></a><br>  Heute gibt es bei PhysTech eine aktive Zusammenarbeit zwischen der "Akademie" und Industriepartnern.  Insbesondere gibt es viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">interessante Labors</a> von Unternehmen wie Sberbank, Biocad, 1C, Tinkoff, MTS und Huawei an der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PhysTech School für Angewandte Mathematik und Informatik</a> . <br><br>  Ich wurde inspiriert, diesen Artikel zu schreiben, indem ich im Labor für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hybride intelligente Systeme arbeitete</a> , das von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VkusVill</a> eröffnet wurde.  Das Labor hat eine ehrgeizige Aufgabe - ein Geschäft zu bauen, das ohne Geldschalter funktioniert, hauptsächlich mit Hilfe von Computer Vision.  Fast ein Jahr lang hatte ich die Gelegenheit, an vielen Aufgaben des Sehens zu arbeiten, die in diesen beiden Teilen erörtert werden. <br><br><div class="spoiler">  <b class="spoiler_title">Ohne Kasse einkaufen?</b>  <b class="spoiler_title">Irgendwo habe ich es schon gehört ..</b> <div class="spoiler_text">  Wahrscheinlich, lieber Leser, haben Sie an <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Go</a> gedacht.  In gewissem Sinne besteht die Aufgabe darin, ihren Erfolg zu wiederholen, aber bei unserer Entscheidung geht es mehr um die Implementierung als darum, ein solches Geschäft für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viel Geld</a> von Grund auf neu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufzubauen</a> . <br></div></div><br>  Wir werden nach Plan umziehen: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Motivation und was los ist</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einstufung als Lebensstil</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungsarchitekturen für neuronale Netze: 1000 Wege, um ein Ziel zu erreichen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Visualisierung von Faltungs-Neuronalen Netzen: Zeigen Sie mir Leidenschaft</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ich selbst bin eine Art Chirurg: Wir extrahieren Merkmale aus neuronalen Netzen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bleiben Sie nah dran: Repräsentationslernen für Menschen und Einzelpersonen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: <s>Erkennen, Bewerten der Haltung und Erkennen von Handlungen</s> ohne Spoiler</a> </li></ol><br><a name="1"></a><h2>  Motivation und was los ist </h2><br><div class="spoiler">  <b class="spoiler_title">Für wen ist der Artikel?</b> <div class="spoiler_text">  Der Artikel konzentriert sich mehr auf Menschen, die bereits mit maschinellem Lernen und neuronalen Netzen vertraut sind.  Ich rate Ihnen jedoch, mindestens die ersten beiden Abschnitte zu lesen - plötzlich wird alles klar :) <br></div></div><br>  Im Jahr 2019 sprechen alle über künstliche Intelligenz, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vierte industrielle Revolution</a> und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Annäherung der Menschheit an eine Singularität</a> .  Cool, cool, aber ich möchte Einzelheiten.  Schließlich sind wir neugierige Technikfreaks, die nicht an Märchen über KI glauben, sondern an formale Aufgabenstellung, Mathematik und Programmierung.  In diesem Artikel werden wir über spezifische Fälle der Verwendung der sehr modernen KI sprechen - die Verwendung von Deep Learning (nämlich Faltungs-Neuronale Netze) in einer Vielzahl von Computer-Vision-Aufgaben. <br><br>  Ja, wir werden speziell über Gitter sprechen und manchmal einige Ideen aus einer „klassischen“ Sicht erwähnen (wir werden die Methoden in der Vision nennen, die vor neuronalen Netzen verwendet wurden, aber dies bedeutet keineswegs, dass sie jetzt nicht verwendet werden). <br><br><div class="spoiler">  <b class="spoiler_title">Ich möchte Computer Vision von Grund auf lernen</b> <div class="spoiler_text">  Ich empfehle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anton Konushins Kurs "Einführung in Computer Vision"</a> .  Persönlich habe ich das Gegenstück in SHAD durchgesehen, das eine solide Grundlage für das Verständnis der Bild- und Videoverarbeitung gelegt hat. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="Bild" width="300"></div><br>  Meiner Meinung nach ist die erste wirklich interessante Anwendung neuronaler Netze in der Vision, über die 1993 in den Medien berichtet wurde, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Handschrifterkennung durch</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jan LeCun</a> .  Jetzt ist er einer der Haupt-KI in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Facebook-KI-Forschung</a> . Ihr Team hat bereits <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele nützliche Open Source-Inhalte veröffentlicht</a> . <br><br>  Vision wird heute in vielen Bereichen eingesetzt.  Ich werde nur einige bemerkenswerte Beispiele nennen: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="Bild" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="Bild" width="400" height="300"></div><br><br>  <i>Unbemannte Fahrzeuge von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tesla</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="Bild" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Medizinische Bildgebungsanalyse</a> und <a href="">Krebsvorhersage</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="Bild" width="500"></div><br><br>  <i>Spielekonsolen: Kinect 2.0 (obwohl immer noch Tiefeninformationen verwendet werden, dh RGB-D-Bilder)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="Bild" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Gesichtserkennung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apple FaceID</a> (mit mehreren Sensoren)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="Bild" width="400"></div><br><br>  <i>Gesichtspunktbewertung: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Snapchat-Masken</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="Bild" width="400"></div><br><br>  <i>Biometrie der Gesichts- und Augenbewegungen (ein Beispiel aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projekt von FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="Bild" width="400"></div><br><br>  <i>Suche nach Bild: Yandex und Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="Bild" width="500"></div><br><br>  <i>Erkennung des Bildtextes ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">optische Zeichenerkennung</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="Bild" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="Bild" width="400"></div><br><br>  <i>Drohnen und Roboter: Empfangen und Verarbeiten von Informationen durch Sehen</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="Bild" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kilometerzähler</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen</a> einer Karte und Planen beim Bewegen von Robotern</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="Bild" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verbesserung von Grafiken und Texturen in Videospielen</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="Bild" width="200" height="300"></div><br><br>  <i>Bildübersetzung: Yandex und Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="Bild" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="Bild" width="500"></div><br><br>  <i>Augmented Reality: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprungbewegung</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Projekt</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nordstern</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">)</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="Bild" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Stil- und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Texturübertragung</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PicsArt</a></i> <br><br>  Ganz zu schweigen von den zahlreichen Anwendungen in verschiedenen internen Aufgaben von Unternehmen.  Facebook verwendet beispielsweise auch Vision, um Medieninhalte zu filtern.  Computer Vision-Methoden werden auch bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Qualitäts- / Schadensprüfungen in der Industrie eingesetzt</a> . <br><br>  Augmented Reality muss hier in der Tat besondere Aufmerksamkeit gewidmet werden, da <s>es</s> in naher Zukunft <s>nicht funktioniert</s> , kann dies einer der Hauptanwendungsbereiche des Sehens werden. <br><br>  Motiviert.  Aufgeladen.  Lass uns gehen: <br><br><a name="2"></a><h2>  Einstufung als Lebensstil </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="Bild"><br><br>  Wie gesagt, in den 90ern wurden die Netze in Sichtweite abgefeuert.  Und sie haben in einer bestimmten Aufgabe gedreht - der Aufgabe, Bilder von handgeschriebenen Zahlen zu klassifizieren (der berühmte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST-Datensatz</a> ).  Historisch gesehen war es die Aufgabe, Bilder zu klassifizieren, die zur Grundlage für die Lösung fast aller nachfolgenden Aufgaben in der Vision wurde.  Betrachten Sie ein bestimmtes Beispiel: <br><br>  <b>Aufgabe</b> : Am Eingang befindet sich ein Ordner mit Fotos. Jedes Foto hat ein bestimmtes Objekt: entweder eine Katze, einen Hund oder eine Person (auch wenn es keine "Müll" -Fotos gibt, ist dies eine nicht unbedingt wichtige Aufgabe, aber Sie müssen irgendwo anfangen).  Sie müssen die Bilder in drei Ordner zerlegen: <code>/cats</code> , <code>/dogs</code> und <s><code>/leather_bags</code></s> <code>/humans</code> , wobei nur Fotos mit den entsprechenden Objekten in jedem Ordner <s><code>/leather_bags</code></s> . <br><br><div class="spoiler">  <b class="spoiler_title">Was ist ein Bild / Foto?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="Bild"><br>  Fast überall in der Vision ist es üblich, mit Bildern im RGB-Format zu arbeiten.  Jedes Bild hat eine Höhe (H), eine Breite (B) und eine Tiefe von 3 (Farben).  Somit kann ein Bild als Tensor der Dimension HxBx3 dargestellt werden (jedes Pixel ist ein Satz von drei Zahlen - Intensitätswerte in den Kanälen). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Stellen Sie sich vor, wir sind noch nicht mit Computer Vision vertraut, aber wir kennen uns mit maschinellem Lernen aus.  Bilder sind einfach numerische Tensoren im Speicher des Computers.  Wir formalisieren die Aufgabe in Bezug auf maschinelles Lernen: Objekte sind Bilder, ihre Zeichen sind Werte in Pixel, die Antwort für jedes der Objekte ist eine Klassenbezeichnung (Katze, Hund oder Person).  Dies ist eine reine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klassifizierungsaufgabe</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Wenn es jetzt schwierig geworden ist ..</b> <div class="spoiler_text">  ... dann ist es besser, zuerst die ersten 4 Artikel aus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem OpenDataScience ML Open Course zu</a> lesen und einen einführenden Artikel über Vision zu lesen, zum Beispiel einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">guten Vortrag in Small ShAD</a> . <br></div></div><br>  Sie können einige Methoden aus der „klassischen“ Sicht oder dem „klassischen“ maschinellen Lernen verwenden, dh nicht aus einem neuronalen Netzwerk.  Grundsätzlich bestehen diese Methoden darin, die Bilder bestimmter Merkmale (Sonderpunkte) oder lokaler Regionen hervorzuheben, die das Bild charakterisieren („ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tasche mit visuellen Wörtern</a> “).  Normalerweise läuft alles auf etwas wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SVM</a> über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SIFT hinaus</a> . <br><br>  Wir haben uns hier versammelt, um über neuronale Netze zu sprechen. Wir möchten also nicht die von uns erfundenen Zeichen verwenden, sondern dass das Netzwerk alles für uns tut.  Unser Klassifikator nimmt die Vorzeichen eines Objekts als Eingabe und gibt eine Vorhersage (Klassenbezeichnung) zurück.  Hier wirken die Intensitätswerte in Pixel als Vorzeichen (siehe Bildmodell in <br>  Spoiler oben).  Denken Sie daran, dass ein Bild ein Größentensor (Höhe, Breite, 3) ist (wenn es Farbe ist).  Wenn Sie lernen, in das Raster einzutreten, wird dies normalerweise nicht von einem Bild und nicht von einem ganzen Datensatz, sondern von Stapeln, d. H.  in kleinen Teilen von Objekten (z. B. 64 Bilder im Stapel). <br><br>  Somit empfängt das Netzwerk einen Eingangstensor der Größe (BATCH_SIZE, H, W, 3).  Sie können jedes Bild in eine Vektorlinie aus H * W * 3-Zahlen „erweitern“ und mit den Werten in Pixeln arbeiten, genau wie mit Zeichen beim maschinellen Lernen. Ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">normales Multilayer-Perceptron (MLP)</a> würde genau das tun, aber ehrlich gesagt ist es so Basislinie, da beim Arbeiten mit Pixeln als Vektorzeile beispielsweise die translatorische Invarianz von Objekten im Bild nicht berücksichtigt wird.  Dieselbe Katze kann sich in der Mitte des Fotos befinden, und in der Ecke lernt MLP dieses Muster nicht. <br><br>  Sie brauchen also etwas Klügeres, zum Beispiel eine Faltungsoperation.  Und hier geht es um moderne Vision, um <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungs-Neuronale Netze</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">Der Trainingscode für das Faltungsnetzwerk sieht möglicherweise ungefähr so ​​aus (im PyTorch-Framework).</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Da wir jetzt über das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Training mit einem Lehrer</a> sprechen, benötigen wir mehrere Komponenten für das Training eines neuronalen Netzwerks: <br><br><ul><li>  Daten (bereits vorhanden) </li><li>  Netzwerkarchitektur (Highlight) </li><li>  Eine Verlustfunktion, die angibt, wie das neuronale Netzwerk zu lernen ist (hier ist es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kreuzentropie</a> ). </li><li>  Optimierungsmethode (ändert das Netzwerkgewicht in die richtige Richtung) </li><li>  Definieren Sie Architektur- und Optimierer-Hyperparameter (z. B. Schrittgröße des Optimierers, Anzahl der Neuronen in Schichten, Regularisierungskoeffizienten) </li></ul><br>  Dies ist genau das, was im Code implementiert ist: Das Faltungs-Neuronale Netzwerk selbst wird in der Klasse Net () beschrieben. <br><br>  Wenn Sie langsam und von Anfang an etwas über Faltung und Faltungsnetzwerke lernen möchten, empfehle ich einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vortrag an der Deep Learning School (MIPT MIPT) (auf Russisch)</a> zu diesem Thema und natürlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanfords Kurs cs231n (auf Englisch)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - was ist das?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Learning School</a> im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Labor für Innovation FPMI MIPT</a> ist eine Organisation, die sich aktiv an der Entwicklung eines offenen russischsprachigen Kurses über neuronale Netze beteiligt.  In diesem Artikel werde ich mehrmals auf diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Video-Tutorials</a> verweisen. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="Bild" width="650"></div><br>  Kurz gesagt, mit der Faltungsoperation können Sie Muster auf Bildern anhand ihrer Variabilität finden.  Wenn wir Faltungs-Neuronale Netze trainieren (dt.: Faltungs-Neuronale Netze), finden wir tatsächlich Faltungsfilter (Neuronengewichte), die Bilder gut beschreiben und so gut, dass wir die Klasse daraus genau bestimmen können.  Es wurden viele Wege erfunden, um ein solches Netzwerk aufzubauen.  Mehr als Sie denken ... <br><br><a name="3"></a><h3>  Faltungsarchitekturen für neuronale Netze: 1000 Wege, um ein Ziel zu erreichen </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="Bild" width="500"></div><br><br>  Ja, ja, noch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eine architektonische Überprüfung</a> .  Aber hier werde ich versuchen, es so relevant wie möglich zu machen! <br><br>  Zuerst gab es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LeNet</a> , es half Jan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LeCun</a> 1998, Zahlen zu erkennen.  Dies war das erste Faltungsnetzwerk zur Klassifizierung.  Ihr Hauptmerkmal war, dass sie im Grunde genommen anfing, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungs- und Pooling-</a> Operationen durchzuführen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="Bild"><br><br>  Dann gab es eine Pause bei der Entwicklung von Grids, aber die Hardware stand nicht still, und es wurden effektive Berechnungen für GPU und <abbr title="Beschleunigte lineare Algebra">XLA entwickelt</abbr> .  Im Jahr 2012 erschien AlexNet, sie schoss im ILSVRC-Wettbewerb ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet Large-Scale Visual Recognition Challenge</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Ein kleiner Exkurs über ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet</a> wurde bis 2012 zusammengestellt und eine Teilmenge von Tausenden von Bildern und 1000 Klassen wurde für den ILSVRC-Wettbewerb verwendet.  ImageNet hat derzeit ~ 14 Millionen Bilder und 21.841 Klassen (von der offiziellen Website übernommen), aber für den Wettbewerb wählen sie normalerweise nur eine Teilmenge aus.  ILSVRC wurde dann zum größten jährlichen Bildklassifizierungswettbewerb.  Übrigens haben wir kürzlich herausgefunden, wie man <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in wenigen Minuten auf ImageNet trainiert</a> . <br><br>  Von 2010 bis 2018 erhielten sie im ImageNet (in ILSVRC) <abbr title="Stand der Technik">SOTA-</abbr> Netzwerke für die Klassifizierung von Bildern.  Zwar sind seit 2016 eher Wettbewerbe zur Lokalisierung, Erkennung und zum Verständnis der Szene als zur Klassifizierung relevanter. <br></div></div><br>  In der Regel werfen verschiedene <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Architekturprüfungen</a> Licht auf diejenigen, die von 2010 bis 2016 die ersten beim ILSVRC waren, sowie auf einige einzelne Netzwerke.  Um die Geschichte nicht zu überladen, habe ich sie unter den Spoiler gelegt und versucht, die Hauptideen hervorzuheben: <br><br><div class="spoiler">  <b class="spoiler_title">Architektur von 2012 bis 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Gewicht </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alexnet</a> </td><td>  Verwenden Sie zwei Bündel hintereinander.  Teilen Sie das Netzwerktraining in zwei parallele Zweige auf </td><td>  240 MB </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zfnet</a> </td><td>  Filtergröße, Anzahl der Filter in Schichten </td><td>  - - </td></tr><tr><td>  2013 </td><td>  <a href="">Überfeature</a> </td><td>  einer der ersten Detektoren für neuronale Netze </td><td>  - - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  Netzwerktiefe (13-19 Schichten), Verwendung mehrerer Conv-Conv-Pool-Blöcke mit kleinerer Faltungsgröße (3x3) </td><td>  549 MB (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Inception (v1) (auch bekannt als GoogLeNet)</a> </td><td>  1x1-Faltung (Idee aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerk-in-Netzwerk</a> ), Hilfsverluste (oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tiefe Überwachung</a> ), Stapeln der Ausgänge mehrerer Faltungen (Inception-Block) </td><td>  - - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Restverbindungen</a> , sehr tief (152 Schichten ..) </td><td>  98 MB (ResNet-50), 232 MB (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  Die Ideen all dieser Architekturen (mit Ausnahme von ZFNet, es wird normalerweise wenig erwähnt) waren zu einer Zeit ein neues Wort in neuronalen Netzen für Vision.  Nach 2015 gab es jedoch viele weitere wichtige Verbesserungen, beispielsweise Inception-ResNet, Xception, DenseNet, SENet.  Unten habe ich versucht, sie an einem Ort zu sammeln. <br><br><div class="spoiler">  <b class="spoiler_title">Architektur von 2015 bis 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Gewicht </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inception v2 und v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zerlegung von Paketen in Pakete 1xN und Nx1</a> </td><td>  92 MB </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inception v4 und Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kombination von Inception und ResNet</a> </td><td>  215 MB </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Resnext</a> </td><td>  2. Platz ILSVRC, die Verwendung vieler Zweige ("generalisierter" Inception-Block) </td><td>  - - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tief durchtrennbare Faltung</a> , wiegt weniger mit vergleichbarer Genauigkeit wie Inception </td><td>  88 MB </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dichter Block</a>  leicht aber genau </td><td>  33 MB (DenseNet-121), 80 MB (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quetsch- und Erregungsblock</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">46 MB (SENet-Inception), 440 MB (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  Die meisten dieser Modelle für PyTorch finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> , und es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">so eine coole Sache</a> . <br><br>  Sie haben vielleicht bemerkt, dass das Ganze ziemlich viel wiegt (ich möchte maximal 20 MB oder sogar weniger), während heutzutage mobile Geräte überall verwendet werden und das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Internet</a> der Dinge immer beliebter wird, was bedeutet, dass Sie dort auch Gitter verwenden möchten. <br><br><div class="spoiler">  <b class="spoiler_title">Beziehung zwischen Modellgewicht und Geschwindigkeit</b> <div class="spoiler_text">  Da die neuronalen Netze in sich nur Tensoren multiplizieren, wirkt sich die Anzahl der Multiplikationsoperationen (sprich: die Anzahl der Gewichte) direkt auf die Arbeitsgeschwindigkeit aus (wenn keine arbeitsintensive Nach- oder Vorverarbeitung verwendet wird).  Die Geschwindigkeit des Netzwerks selbst hängt von der Implementierung (Framework), der Hardware, auf der es ausgeführt wird, und der Größe des Eingabebilds ab. <br></div></div><br>  Die Autoren vieler Artikel gingen den Weg, schnelle Architekturen zu erfinden. Ich sammelte ihre Methoden unter dem folgenden Spoiler: <br><br><div class="spoiler">  <b class="spoiler_title">CNN Lightweight-Architektur</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Jahr </th><th>  Artikel </th><th>  Schlüsselidee </th><th>  Gewicht </th><th>  Implementierungsbeispiel </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FireModule-Komprimierung</a> </td><td>  0,5 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dies ist ein Netzwerk aus der Kategorie AutoML, das durch eine neuronale Suche nach Architekturen erhalten wurde</a> </td><td>  23 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Punktweise Gruppenkonv., Kanalmischung</a> </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tief trennbare Windungen und viele andere Tricks</a> </td><td>  16 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ich empfehle diesen Artikel über Habré</a> </td><td>  14 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Squeezenext</a> </td><td>  Siehe Bilder im Original-Repository </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Suche nach neuronalen Architekturen speziell für mobile Geräte mit RL</a> </td><td>  ~ 2 MB </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet (v3)</a> </td><td>  Sie kam heraus, während ich einen Artikel schrieb :) </td><td>  - - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Die Nummern in allen Tabellen <s>stammen aus der Obergrenze</s> der Repositorys, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Keras-Anwendungstabelle</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Artikel</a> . <br><br>  Sie fragen: „Warum haben Sie über diesen ganzen„ Zoo “von Modellen geschrieben?  Und warum ist die Aufgabe der Klassifizierung?  Aber wir wollen Maschinen das Sehen beibringen, und Klassifizierung ist nur eine enge Aufgabe. “  Tatsache ist, dass neuronale Netze zum Erkennen von Objekten, zum Bewerten von Körperhaltungen / Punkten, zum erneuten Identifizieren und Suchen in einem Bild präzise Modelle zur Klassifizierung als <b><abbr title="Basis, buchstäblich - die Wirbelsäule">Rückgrat verwenden</abbr></b> und 80% des Erfolgs von ihnen abhängen. <br><br>  Aber ich möchte CNN irgendwie mehr vertrauen, oder sie haben sich Black Boxes ausgedacht, aber was „drinnen“ ist, ist nicht offensichtlich.  Um den Funktionsmechanismus von Faltungsnetzwerken besser zu verstehen, entwickelten die Forscher die Verwendung von Visualisierung. <br><br><a name="4"></a><h3>  Visualisierung von Faltungs-Neuronalen Netzen: Zeigen Sie mir Leidenschaft </h3><br>  Ein wichtiger Schritt, um zu verstehen, was in Faltungsnetzwerken geschieht, ist der Artikel <a href="">„Visualisieren und Verstehen von Faltungsnetzwerken“</a> .  Darin schlugen die Autoren verschiedene Möglichkeiten vor, um genau zu visualisieren, worauf (auf welchen Teilen des Bildes) Neuronen in verschiedenen CNN-Schichten reagieren (ich empfehle auch, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Stanford-Vortrag zu diesem Thema</a> anzuschauen).  Die Ergebnisse waren sehr beeindruckend: Die Autoren zeigten, dass die ersten Schichten des Faltungsnetzwerks auf einige „Dinge auf niedriger Ebene“ durch die Art der Kanten / Winkel / Linien reagieren, und die letzten Schichten bereits auf ganze Teile der Bilder (siehe Bild unten), dh sie tragen bereits an sich einige Semantik. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="Bild"></div><br><br>  Darüber hinaus hat das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projekt zur Tiefenvisualisierung von der Cornell University und das Unternehmen</a> die Visualisierung noch weiter vorangetrieben, während der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">berühmte DeepDream</a> gelernt hat, in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">süchtig machenden</a> interessanten Stil zu verzerren (unten ein Bild von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="Bild" width="500"></div><br><br>  2017 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde auf Distill</a> ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehr guter Artikel veröffentlicht</a> , in dem eine detaillierte Analyse dessen durchgeführt wurde, was jede Ebene „sieht“. Zuletzt (im März 2019) erfand Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aktivierungsatlanten</a> : einzigartige Karten, die für jede Netzwerkschicht erstellt werden können. Dies bringt uns dem Verständnis des Gesamtbildes der Arbeit von CNN näher. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Wenn Sie selbst mit Visualisierung spielen möchten, würde ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lucid</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorSpace</a> empfehlen. <br><br>  Okay, CNN scheint bis zu einem gewissen Grad wahr zu sein.  Wir müssen lernen, wie man dies bei anderen Aufgaben verwendet und nicht nur bei der Klassifizierung.  Dies wird uns helfen, Embedding'ov-Bilder zu extrahieren und Lernen zu übertragen. <br><br><a name="5"></a><h2>  Ich selbst bin eine Art Chirurg: Wir extrahieren Merkmale aus neuronalen Netzen </h2><br>  Stellen Sie sich vor, es gibt ein Bild, und wir möchten diejenigen finden, die visuell so aussehen (dies ist beispielsweise die Suche in einem Bild in Yandex.Pictures).  Früher (vor neuronalen Netzen) haben Ingenieure Funktionen dafür manuell extrahiert, z. B. etwas erfunden, das das Bild gut beschreibt und es mit anderen vergleichen kann.  Grundsätzlich arbeiten diese Methoden ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SIFT</a> ) mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bildverläufen</a> , normalerweise werden diese Dinge als "klassische" Bilddeskriptoren bezeichnet.  Von besonderem Interesse beziehe ich mich auf den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> und den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurs von Anton Konushin</a> (dies ist keine Werbung, nur ein guter Kurs :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="Bild" width="500"></div><br><br>  Mithilfe neuronaler Netze können wir diese Merkmale und Heuristiken nicht selbst erfinden, sondern das Modell richtig trainieren und dann <b>die Ausgabe einer oder mehrerer Schichten des Netzes als Zeichen des Bildes verwenden</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="Bild" width="650"></div><br>  Ein genauerer Blick auf alle Architekturen macht deutlich, dass die Klassifizierung in CNN zwei Schritte umfasst: <br>  1).  <b>Feature-Extraktor-</b> Ebenen zum Extrahieren informativer Features aus Bildern mithilfe von Faltungsebenen <br>  2).  Lernen über diese Funktionen hinaus <b>Fully Connected (FC)</b> -Klassifiziererebenen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="Bild" width="500"></div><br><br>  <b>Beim Einbetten von Bildern (Features)</b> geht es nur darum, dass Sie ihre Zeichen nach dem Feature-Extraktor eines Faltungs-Neuronalen Netzwerks (obwohl sie auf unterschiedliche Weise aggregiert werden können) als informative Beschreibung von Bildern verwenden können.  Das heißt, wir haben das Netzwerk für die Klassifizierung geschult und nehmen dann einfach den Ausgang vor den Klassifizierungsebenen.  Diese Zeichen werden als <i>Merkmale</i> , <i>neuronale Netzwerkdeskriptoren</i> oder <i>Bildeinbettungen bezeichnet</i> (obwohl Einbettungen im NLP normalerweise akzeptiert werden, da dies eine Vision ist, spreche ich häufig <i>Merkmale</i> ).  Normalerweise ist dies eine Art numerischer Vektor, zum Beispiel 128 Zahlen, mit denen Sie bereits arbeiten können. <br><br><div class="spoiler">  <b class="spoiler_title">Aber was ist mit Auto-Encodern?</b> <div class="spoiler_text">  Ja, tatsächlich können Funktionen von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auto-Encodern erhalten werden</a> .  In meiner Praxis haben sie dies auf unterschiedliche Weise getan, aber zum Beispiel in Artikeln zur Neuidentifizierung (auf die später noch eingegangen wird) nehmen sie immer noch Funktionen nach dem Extraktor an, anstatt den Auto-Encoder dafür zu trainieren.  Es scheint mir, dass es sich lohnt, Experimente in beide Richtungen durchzuführen, wenn die Frage ist, was besser funktioniert. <br></div></div><br>  Somit kann die Pipeline zur Lösung <b>des Suchproblems in einem Bild</b> einfach angeordnet werden: Wir führen die Bilder durch CNN, nehmen Zeichen aus den gewünschten Ebenen und vergleichen diese Merkmale aus verschiedenen Bildern miteinander.  Zum Beispiel betrachten wir einfach den euklidischen Abstand dieser Vektoren. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="Bild" width="500"></div><br><br>  <b>Transfer Learning</b> ist eine bekannte Technik für das effektive Training neuronaler Netze, die bereits für ihre Aufgabe an einem bestimmten Datensatz trainiert wurden.  Oft heißt es auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feinabstimmung</a> anstelle von Transferlernen. In den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stanford-Kursnotizen cs231 werden</a> diese Konzepte geteilt. Transferlernen ist eine allgemeine Idee, und Feinabstimmung ist eine der Implementierungen der Technik.  Dies ist für uns in Zukunft nicht so wichtig. Die Hauptsache ist zu verstehen, dass wir das Netzwerk nur so trainieren können, dass es den neuen Datensatz gut vorhersagt, und zwar nicht ausgehend von zufälligen Gewichten, sondern von solchen, die auf einem großen ImageNet-Typ trainiert wurden.  Dies gilt insbesondere dann, wenn nur wenige Daten vorhanden sind und Sie das Problem qualitativ lösen möchten. <br><br><div class="spoiler">  <b class="spoiler_title">Erfahren Sie mehr über Transfer Learning</b> <div class="spoiler_text">  <a href="">Originalartikel</a> , aber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">warum viel Text lesen, wenn man sich das Video ansehen kann</a> <br></div></div><br>  Es reicht jedoch möglicherweise nicht aus, nur die erforderlichen Funktionen zu übernehmen und zusätzliche Schulungen vom Datensatz zum Datensatz durchzuführen, um beispielsweise nach ähnlichen Personen / Personen / etwas Bestimmtem zu suchen.  Fotos derselben Person können manchmal visuell noch unähnlicher sein als Fotos verschiedener Personen.  Es ist notwendig, dass das Netzwerk genau die Zeichen hervorhebt, die einer Person / einem Objekt innewohnen, auch wenn es für uns schwierig ist, dies mit unseren Augen zu tun.  Willkommen in der Welt des <b>Repräsentationslernens</b> . <br><br><a name="6"></a><h2>  Bleiben Sie nah dran: Repräsentationslernen für Menschen und Einzelpersonen </h2><br><div class="spoiler">  <b class="spoiler_title">Terminologie Hinweis</b> <div class="spoiler_text">  Wenn Sie wissenschaftliche Artikel lesen, scheinen einige Autoren den Ausdruck <b>metrisches Lernen</b> manchmal anders zu verstehen, und es besteht kein Konsens darüber, welche Methoden als metrisches Lernen bezeichnet werden sollen und welche nicht.  Aus diesem Grund habe ich mich in diesem Artikel dazu entschlossen, diesen bestimmten Satz zu vermeiden und ein logischeres <b>Darstellungslernen zu verwenden</b> . Einige Leser stimmen dem möglicherweise nicht zu. Ich werde dies gerne in den Kommentaren diskutieren. <br></div></div><br>  Wir stellen die Aufgaben: <br><br><ul><li>  <b>Aufgabe 1</b> : Es gibt eine Galerie (Set) mit Fotos von Gesichtern von Personen. Wir möchten, dass das Netzwerk gemäß einem neuen Foto entweder mit dem Namen einer Person aus der Galerie antworten kann (angeblich ist es das) oder dass es keine solche Person in der Galerie gibt (und wir fügen sie möglicherweise hinzu neue Person) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Aufgabe 2</b> : Das Gleiche, aber wir arbeiten nicht mit Fotos von Gesichtern, sondern mit Menschen in voller Länge <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  Die erste Aufgabe wird normalerweise als <b>Gesichtserkennung bezeichnet</b> , die zweite als <i>Neuidentifizierung</i> (abgekürzt als <i>Reid</i> ).  Ich habe sie in einem Block zusammengefasst, weil ihre Lösungen heute ähnliche Ideen verwenden: Um effektive Bildeinbettungen zu erlernen, die mit ziemlich schwierigen Situationen umgehen können, verwenden sie heute verschiedene Arten von Verlusten, wie zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Triplettverlust</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vierfachverlust</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kontrastmittelverlust</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kosinusverlust</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Es gibt immer noch wunderbare <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">siamesische</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerke</a> , aber ich habe sie ehrlich gesagt nicht selbst benutzt.  Übrigens „entscheidet“ nicht nur der Verlust selbst, sondern auch, wie Paare von Positiven und Negativen dafür <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abgetastet werden,</a> betonen die Autoren des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sampling, die beim tiefgreifenden Einbetten des Lernens eine</a> Rolle spielen. <br><br>  Das Wesentliche all dieser Verluste und siamesischen Netzwerke ist einfach: Wir möchten, dass die Bilder einer Klasse (Person) im latenten Raum der Merkmale (Einbettungen) „nah“ und verschiedener Klassen (Menschen) „fern“ sind.  Die Nähe wird normalerweise wie folgt gemessen: Es werden Einbettungen von Bildern aus einem neuronalen Netzwerk vorgenommen (z. B. ein Vektor mit 128 Zahlen), und wir betrachten entweder den üblichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">euklidischen Abstand</a> zwischen diesen Vektoren oder die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kosinusnähe.</a>  Wie Sie messen können, ist besser, wenn Sie einen Datensatz / eine Aufgabe auswählen. <br><br>  Eine schematische Darstellung einer Problemlösungspipeline zum Lernen von Darstellungen sieht ungefähr so ​​aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Genauer gesagt, so</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        —   .   ,     —     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="Bild" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="menschliche Erkennung aus einem großen Foto geschnitten"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> Reid —    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 2016 </a> . ,     ,    —   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="Bild" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   —    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TorchReid</a> .      —   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  ,   </a> ).   ?  … <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="Bild" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de450732/">https://habr.com/ru/post/de450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de450720/index.html">So entwickeln Sie eine benutzerfreundliche Anwendung</a></li>
<li><a href="../de450724/index.html">Einführung von Python für Kameraden, die der „A vs. V-Sprache“ entwachsen sind Sprache B "und andere Vorurteile</a></li>
<li><a href="../de450726/index.html">Erstellen eines Tools zum schnellen und effizienten Schreiben von Autotests auf Selen</a></li>
<li><a href="../de450728/index.html">NLog: Regeln und Filter</a></li>
<li><a href="../de450730/index.html">ok.tech: Frontend-Treffen</a></li>
<li><a href="../de450734/index.html">Fuzzing ist ein wichtiger Schritt für eine sichere Entwicklung</a></li>
<li><a href="../de450736/index.html">"Das Internet zu isolieren ist viel einfacher und billiger als das externe Blockieren."</a></li>
<li><a href="../de450738/index.html">Roboter im Rechenzentrum: Wie kann künstliche Intelligenz nützlich sein?</a></li>
<li><a href="../de450740/index.html">Smart Lamp Base REDMOND - Ergänzen Sie das Smart Home</a></li>
<li><a href="../de450744/index.html">Fahrradinfrastruktur in Minsk für einen IT-Expat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>