<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª üó∫Ô∏è üí• Escala de banco de dados em sistemas altamente carregados üëΩ üåë üà¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No √∫ltimo com√≠cio interno da Pyrus, conversamos sobre armazenamento distribu√≠do moderno, e Maxim Nalsky, CEO e fundador da Pyrus, compartilhou sua pri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Escala de banco de dados em sistemas altamente carregados</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440306/"> No √∫ltimo com√≠cio interno da Pyrus, conversamos sobre armazenamento distribu√≠do moderno, e Maxim Nalsky, CEO e fundador da Pyrus, compartilhou sua primeira impress√£o do FoundationDB.  Neste artigo, falamos sobre as nuances t√©cnicas que voc√™ enfrenta ao escolher uma tecnologia para dimensionar o armazenamento de dados estruturados. <br><br>  Quando o servi√ßo n√£o est√° dispon√≠vel para os usu√°rios h√° algum tempo, √© extremamente desagrad√°vel, mas ainda n√£o √© mortal.  Mas perder dados de clientes √© absolutamente inaceit√°vel.  Portanto, avaliamos escrupulosamente qualquer tecnologia para armazenar dados em duas a tr√™s d√∫zias de par√¢metros. <a name="habracut"></a>  Alguns deles determinam a carga atual no servi√ßo. <br><br><img src="https://habrastorage.org/webt/1c/p2/gm/1cp2gmh6pjentlkkocm1k2msgay.png">  <font color="#777777">Carga atual.</font>  <font color="#777777">Selecionamos a tecnologia levando em considera√ß√£o o crescimento desses indicadores.</font> <br><br><h2>  Arquitetura do servidor cliente </h2><br>  O modelo cliente-servidor cl√°ssico √© o exemplo mais simples de um sistema distribu√≠do.  Um servidor √© um ponto de sincroniza√ß√£o, pois permite que v√°rios clientes fa√ßam algo juntos de maneira coordenada. <br><br><img src="https://habrastorage.org/webt/jq/hy/ee/jqhyeeqxfebgrvhzvkwsrz58jhy.png">  <font color="#777777">Um esquema muito simplificado de intera√ß√£o cliente-servidor.</font> <br><br>  O que n√£o √© confi√°vel na arquitetura cliente-servidor?  Obviamente, o servidor pode falhar.  E quando o servidor falha, todos os clientes n√£o podem funcionar.  Para evitar isso, as pessoas criaram uma conex√£o mestre-escravo (que agora √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">politicamente correta chamada seguidor de l√≠der</a> ).  A conclus√£o √© que existem dois servidores, todos os clientes se comunicam com o principal e, no segundo, todos os dados s√£o simplesmente replicados. <br><br><img src="https://habrastorage.org/webt/tk/bo/vu/tkbovuwxni03qjfpkx7m5iwmfie.png">  <font color="#777777">Arquitetura cliente-servidor com replica√ß√£o de dados para seguidores.</font> <br><br>  Est√° claro que este √© um sistema mais confi√°vel: se o servidor principal travar, uma c√≥pia de todos os dados estar√° no seguidor e poder√° ser aumentada rapidamente. <br><br>  √â importante entender como a replica√ß√£o funciona.  Se for s√≠ncrona, a transa√ß√£o deve ser armazenada simultaneamente no l√≠der e no seguidor, e isso pode ser lento.  Se a replica√ß√£o for ass√≠ncrona, voc√™ poder√° perder alguns dados ap√≥s um failover. <br><br>  E o que acontecer√° se o l√≠der cair √† noite quando todos estiverem dormindo?  Existem dados sobre o seguidor, mas ningu√©m lhe disse que ele agora √© um l√≠der, e os clientes n√£o se conectam a ele.  OK, vamos dotar o seguidor com a l√≥gica de que ele come√ßa a se considerar a coisa principal quando a conex√£o com o l√≠der √© perdida.  Ent√£o podemos facilmente ter um c√©rebro dividido - um conflito quando a conex√£o entre o l√≠der e o seguidor √© interrompida, e ambos pensam que eles s√£o os principais.  Isso realmente acontece em muitos sistemas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">como o RabbitMQ</a> , a tecnologia de filas mais popular da atualidade. <br><br>  Para resolver esses problemas, organize o failover autom√°tico - adicione um terceiro servidor (testemunha, testemunha).  Isso garante que temos apenas um l√≠der.  E se o l√≠der cair, o seguidor liga automaticamente com um tempo de inatividade m√≠nimo, que pode ser reduzido para alguns segundos.  Obviamente, os clientes nesse esquema devem conhecer antecipadamente os endere√ßos do l√≠der e seguidor e implementar a l√≥gica da reconex√£o autom√°tica entre eles. <br><br><img src="https://habrastorage.org/webt/n1/ve/aq/n1veaqz67xiw-ozn0cnsudwx7cw.png">  <font color="#777777">A testemunha garante que existe apenas um l√≠der.</font>  <font color="#777777">Se o l√≠der cair, o seguidor liga automaticamente.</font> <br><br>  Esse sistema agora funciona conosco.  H√° um banco de dados principal, um banco de dados sobressalente, h√° uma testemunha e sim - √†s vezes chegamos de manh√£ e vemos que a troca aconteceu √† noite. <br><br>  Mas esse esquema tamb√©m tem desvantagens.  Imagine que voc√™ est√° instalando service packs ou atualizando o sistema operacional em um servidor l√≠der.  Antes disso, voc√™ alternava manualmente a carga no seguidor e depois ... ela cai!  Desastre, seu servi√ßo est√° indispon√≠vel.  O que fazer para se proteger disso?  Adicione um terceiro servidor de backup - outro seguidor.  Tr√™s √© um tipo de n√∫mero m√°gico.  Se voc√™ deseja que o sistema funcione de maneira confi√°vel, dois servidores n√£o s√£o suficientes, s√£o necess√°rios tr√™s.  Um para manuten√ß√£o, o segundo cai, o terceiro permanece. <br><br><img src="https://habrastorage.org/webt/57/et/ao/57etao8c03-skbgh4_evsna3vwu.png">  <font color="#777777">O terceiro servidor fornece opera√ß√£o confi√°vel se os dois primeiros n√£o estiverem dispon√≠veis.</font> <br><br>  Para resumir, a redund√¢ncia deve ser igual a dois.  Uma redund√¢ncia de um n√£o √© suficiente.  Por esse motivo, em matrizes de disco, as pessoas come√ßaram a usar o esquema RAID6 em vez do RAID5, sobrevivendo √† queda de dois discos ao mesmo tempo. <br><br><h2>  Transa√ß√µes </h2><br>  S√£o conhecidos quatro requisitos b√°sicos para transa√ß√µes: atomicidade, consist√™ncia, isolamento e durabilidade (Atomicidade, Consist√™ncia, Isolamento, Durabilidade - ACID). <br><br>  Quando falamos de bancos de dados distribu√≠dos, queremos dizer que os dados devem ser escalados.  A leitura √© muito boa - milhares de transa√ß√µes podem ler dados em paralelo sem problemas.  Mas quando outras transa√ß√µes gravam dados ao mesmo tempo que a leitura, v√°rios efeitos indesej√°veis ‚Äã‚Äãs√£o poss√≠veis.  √â muito f√°cil obter uma situa√ß√£o em que uma transa√ß√£o leia valores diferentes dos mesmos registros.  Aqui est√£o alguns exemplos. <br><br>  <b>Leituras sujas.</b>  Na primeira transa√ß√£o, enviamos a mesma solicita√ß√£o duas vezes: pegue todos os usu√°rios cujo ID = 1. Se a segunda transa√ß√£o alterar essa linha e depois reverter, o banco de dados n√£o ver√° nenhuma altera√ß√£o por um lado, mas por outro a primeira transa√ß√£o ler√° diferentes valores de idade para Joe. <br><br><img src="https://habrastorage.org/webt/qj/zl/d6/qjzld6frtj0ogu9lpfoss5wva2k.png"><br><br>  <b>Leituras n√£o repet√≠veis.</b>  Outro caso √© se a transa√ß√£o de grava√ß√£o foi conclu√≠da com √™xito e a transa√ß√£o de leitura recebeu dados diferentes durante a execu√ß√£o da mesma solicita√ß√£o. <br><br><img src="https://habrastorage.org/webt/85/wd/ug/85wdugy_ypmk0cfxpxl1hy5pf-s.png"><br><br>  No primeiro caso, o cliente leu dados que geralmente estavam ausentes no banco de dados.  No segundo caso, o cliente l√™ os dados duas vezes no banco de dados, mas eles s√£o diferentes, embora a leitura ocorra na mesma transa√ß√£o. <br><br>  <b>A leitura fantasma √©</b> quando relemos um intervalo dentro da mesma transa√ß√£o e obtemos um conjunto diferente de linhas.  Em algum lugar no meio, outra transa√ß√£o entrou e inseriu ou excluiu registros. <br><br><img src="https://habrastorage.org/webt/k4/65/yq/k465yqehsgbjd0ertvg7poziaie.png"><br><br>  Para evitar esses efeitos indesej√°veis, os DBMSs modernos implementam mecanismos de bloqueio (uma transa√ß√£o restringe o acesso de outras transa√ß√µes aos dados com os quais est√£o trabalhando atualmente) ou o controle de vers√µes em v√°rias vers√µes, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MVCC</a> (uma transa√ß√£o nunca altera os dados gravados anteriormente e sempre cria uma nova vers√£o). <br><br>  O padr√£o ANSI / ISO SQL define 4 n√≠veis de isolamento para transa√ß√µes que afetam seu grau de bloqueio m√∫tuo.  Quanto maior o n√≠vel de isolamento, menos efeitos indesej√°veis.  O pre√ßo para isso √© desacelerar o aplicativo (j√° que as transa√ß√µes aguardam com mais frequ√™ncia para desbloquear os dados necess√°rios) e aumentar a probabilidade de conflitos. <br><br><img src="https://habrastorage.org/webt/0-/iy/4h/0-iy4hwem9b4-mkupvpetxgc9so.png"><br><br>  O mais agrad√°vel para um programador de aplicativos √© o n√≠vel Serializable - n√£o h√° efeitos indesej√°veis ‚Äã‚Äãe toda a complexidade de garantir a integridade dos dados √© transferida para o DBMS. <br><br>  Vamos pensar na implementa√ß√£o ing√™nua do n√≠vel Serializable - com cada transa√ß√£o, apenas bloqueamos todos os outros.  Cada transa√ß√£o de grava√ß√£o pode teoricamente ser executada em 50 ¬µs (o tempo de uma opera√ß√£o de grava√ß√£o em discos SSD modernos).  E queremos salvar dados em tr√™s m√°quinas, lembra?  Se eles estiverem no mesmo data center, a grava√ß√£o levar√° de 1 a 3 ms.  E, se eles, por confiabilidade, estiverem em cidades diferentes, a grava√ß√£o poder√° demorar de 10 a 12ms (o tempo de viagem de um pacote de rede de Moscou a S√£o Petersburgo e vice-versa).  Ou seja, com uma implementa√ß√£o ing√™nua do n√≠vel Serializable por grava√ß√£o sequencial, n√£o podemos realizar mais de 100 transa√ß√µes por segundo.  Enquanto um SSD separado permite executar cerca de 20.000 opera√ß√µes de grava√ß√£o por segundo! <br><br>  Conclus√£o: as transa√ß√µes de grava√ß√£o devem ser executadas em paralelo e, para escal√°-las, √© necess√°rio um bom mecanismo de resolu√ß√£o de conflitos. <br><br><h2>  Sharding </h2><br>  O que fazer quando os dados param de entrar em um servidor?  Existem dois mecanismos de zoom padr√£o: <br><br><ul><li>  <b>Vertical</b> quando adicionamos mem√≥ria e discos a este servidor.  Isso tem seus limites - em termos de n√∫mero de n√∫cleos por processador, n√∫mero de processadores e quantidade de mem√≥ria. <br></li><li>  <b>Horizontal,</b> quando usamos muitas m√°quinas e distribu√≠mos dados entre elas.  Os conjuntos dessas m√°quinas s√£o chamados de clusters.  Para colocar dados em um cluster, eles precisam ser fragmentados - ou seja, para cada registro, determine em qual servidor ele estar√° localizado. <br></li></ul><br>  Uma chave de fragmenta√ß√£o √© um par√¢metro pelo qual os dados s√£o distribu√≠dos entre servidores, por exemplo, um identificador de cliente ou organiza√ß√£o. <br><br>  Imagine que voc√™ precisa registrar dados sobre todos os habitantes da Terra em um cluster.  Como chave de fragmento, voc√™ pode usar, por exemplo, o ano de nascimento da pessoa.  Ent√£o, 116 servidores ser√£o suficientes (e a cada ano ser√° necess√°rio adicionar um novo servidor).  Ou voc√™ pode tomar como chave o pa√≠s em que a pessoa mora, e precisar√° de aproximadamente 250 servidores.  Ainda, a primeira op√ß√£o √© prefer√≠vel, porque a data de nascimento da pessoa n√£o muda e voc√™ nunca precisar√° transferir dados sobre ela entre os servidores. <br><br><img src="https://habrastorage.org/webt/zr/8h/sz/zr8hszqcct-xf5q07gcow3cf7lu.png"><br><br>  No Pyrus, voc√™ pode considerar uma organiza√ß√£o como uma chave de fragmenta√ß√£o.  Mas eles s√£o muito diferentes em tamanho: existe um enorme Sovcombank (mais de 15 mil usu√°rios) e milhares de pequenas empresas.  Quando voc√™ atribui a uma organiza√ß√£o um servidor espec√≠fico, n√£o sabe com anteced√™ncia como ela crescer√°.  Se a organiza√ß√£o for grande e usar o servi√ßo ativamente, mais cedo ou mais tarde seus dados deixar√£o de ser colocados em um servidor e voc√™ precisar√° compartilhar novamente.  E isso n√£o √© f√°cil se os dados tiverem terabytes.  Imagine: um sistema carregado, as transa√ß√µes acontecem a cada segundo e, nessas condi√ß√µes, voc√™ precisa mover dados de um lugar para outro.  Voc√™ n√£o pode interromper o sistema, esse volume pode ser bombeado por v√°rias horas e os clientes comerciais n√£o sobreviver√£o a um per√≠odo de inatividade t√£o longo. <br><br>  Como chave de compartilhamento, √© melhor escolher dados que raramente mudam.  No entanto, longe de sempre uma tarefa aplicada, √© f√°cil fazer isso. <br><br><h2>  Consenso no cluster </h2><br>  Quando h√° muitas m√°quinas no cluster e algumas delas perdem contato com as outras, como decidir quem armazena a vers√£o mais recente dos dados?  Apenas atribuir um servidor testemunha n√£o √© suficiente, pois tamb√©m pode perder o contato com todo o cluster.  Al√©m disso, em uma situa√ß√£o de c√©rebro dividido, v√°rias m√°quinas podem gravar vers√µes diferentes dos mesmos dados - e voc√™ precisa determinar de alguma forma qual √© a mais relevante.  Para resolver esse problema, as pessoas criaram algoritmos de consenso.  Eles permitem que v√°rias m√°quinas id√™nticas cheguem a um √∫nico resultado em qualquer quest√£o, votando.  Em 1989, o primeiro algoritmo desse tipo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Paxos</a> , foi publicado e, em 2014, os caras de Stanford criaram um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Raft</a> mais simples de implementar.  Estritamente falando, para que um cluster de servidores (2N + 1) alcance consenso, basta que ele tenha ao mesmo tempo n√£o mais que N falhas.  Para sobreviver a 2 falhas, o cluster deve ter pelo menos 5 servidores. <br><br><h2>  Escala relacional do DBMS </h2><br>  A maioria dos bancos de dados que os desenvolvedores est√£o acostumados a trabalhar com √°lgebra relacional de suporte.  Os dados s√£o armazenados em tabelas e, √†s vezes, voc√™ precisa unir os dados de diferentes tabelas usando a opera√ß√£o JOIN.  Considere um exemplo de banco de dados e uma consulta simples. <br><br><img src="https://habrastorage.org/webt/hv/fo/d-/hvfod-guiaz4-52ji4epjhizpaq.png"><br><br>  Suponha que A.id seja uma chave prim√°ria com um √≠ndice em cluster.  Em seguida, o otimizador criar√° um plano que provavelmente primeiro selecionar√° os registros necess√°rios da tabela A e, em seguida, obter√° os links apropriados para os registros na tabela B a partir de um √≠ndice adequado (A, B). O tempo de execu√ß√£o dessa consulta aumenta logaritmicamente a partir do n√∫mero de registros nas tabelas. <br><br>  Agora imagine que os dados est√£o distribu√≠dos por quatro servidores no cluster e voc√™ precisa executar a mesma consulta: <br><br><img src="https://habrastorage.org/webt/kr/yw/45/kryw45daubuflf-r14zqswson_o.png"><br><br>  Se o DBMS n√£o quiser exibir todos os registros de todo o cluster, provavelmente tentar√° encontrar registros com A.id iguais a 128, 129 ou 130 e encontrar√° os registros apropriados para eles na tabela B. Mas se A.id n√£o for uma chave de fragmento, o DBMS antecipadamente n√£o consigo saber em qual servidor est√£o os dados da tabela A. Precisarei entrar em contato com todos os servidores para descobrir se h√° registros A.id adequados para nossa condi√ß√£o.  Cada servidor pode criar um JOIN dentro de si, mas isso n√£o √© suficiente.  Veja bem, precisamos do registro no n√≥ 2 da amostra, mas n√£o h√° registro com A.id = 128?  Se os n√≥s 1 e 2 fizerem JOIN independentemente, o resultado da consulta estar√° incompleto - n√£o receberemos parte dos dados. <br><br>  Portanto, para atender a essa solicita√ß√£o, cada servidor deve recorrer a todos os outros.  O tempo de execu√ß√£o cresce quadraticamente com o n√∫mero de servidores.  (Voc√™ tem sorte se puder dividir todas as tabelas com a mesma chave e n√£o precisar percorrer todos os servidores. No entanto, na pr√°tica isso n√£o √© realista - sempre haver√° consultas em que a busca n√£o se baseia na chave do fragmento). <br><br>  Portanto, as opera√ß√µes do JOIN s√£o muito escassas e esse √© um problema fundamental da abordagem relacional. <br><br><h2>  Abordagem NoSQL </h2><br>  As dificuldades com o dimensionamento de DBMSs cl√°ssicos levaram as pessoas a criar bancos de dados NoSQL que n√£o possuem opera√ß√µes JOIN.  Sem jun√ß√µes - sem problemas.  Mas n√£o h√° propriedades ACID, mas elas n√£o mencionaram isso nos materiais de marketing.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Artes√£os</a> rapidamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">encontrados</a> que testam a for√ßa de v√°rios sistemas distribu√≠dos e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">publicam os resultados publicamente</a> .  Verificou-se que existem cen√°rios em que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cluster Redis perde 45% dos dados armazenados, o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cluster RabbitMQ - 35% das mensagens</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MongoDB - 9% dos registros</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cassandra - at√© 5%</a> .  E estamos falando <b>sobre a perda depois que o cluster informou o cliente sobre o salvamento bem-sucedido.</b>  Normalmente, voc√™ espera um n√≠vel mais alto de confiabilidade da tecnologia escolhida. <br><br>  O Google desenvolveu o banco de dados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Spanner</a> , que opera globalmente em todo o mundo.  A chave inglesa garante propriedades ACID, serializa√ß√£o e muito mais.  Eles possuem rel√≥gios at√¥micos nos datacenters que fornecem tempo preciso, e isso permite que voc√™ crie uma ordem global de transa√ß√µes sem precisar encaminhar pacotes de rede entre continentes.  A id√©ia do Spanner √© que √© melhor para os programadores lidar com problemas de desempenho que surgem com um grande n√∫mero de transa√ß√µes do que muletas em torno da falta de transa√ß√µes.  No entanto, o Spanner √© uma tecnologia fechada, n√£o lhe conv√©m se, por algum motivo, voc√™ n√£o quiser depender de um fornecedor. <br><br>  Os nativos do Google desenvolveram um an√°logo de c√≥digo aberto da Spanner e o chamaram CockroachDB ("barata" em ingl√™s "barata", que deve simbolizar a capacidade de sobreviv√™ncia do banco de dados).  Em Habr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">j√° escreveu</a> sobre a indisponibilidade do produto para produ√ß√£o, porque o cluster estava perdendo dados.  Decidimos verificar a vers√£o mais recente 2.0 e chegamos a uma conclus√£o semelhante.  N√£o perdemos os dados, mas algumas das consultas mais simples foram executadas de maneira irracionalmente longa. <br><br><hr><br>  Como resultado, hoje existem bancos de dados relacionais que escalam bem apenas verticalmente, o que √© caro.  E existem solu√ß√µes NoSQL sem transa√ß√µes e sem garantias ACID (se voc√™ quiser ACID, escreva muletas). <br><br>  Como criar aplicativos de miss√£o cr√≠tica nos quais os dados n√£o cabem em um servidor?  Novas solu√ß√µes aparecem no mercado e, sobre uma delas - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">FoundationDB</a> -, contaremos mais no pr√≥ximo artigo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt440306/">https://habr.com/ru/post/pt440306/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt440296/index.html">6 Aplica√ß√µes para a Internet das coisas industrial</a></li>
<li><a href="../pt440298/index.html">Aplicativo do dia dos namorados no Libgdx</a></li>
<li><a href="../pt440300/index.html">10 comandos do console para ajudar a debater o c√≥digo JavaScript como um PRO</a></li>
<li><a href="../pt440302/index.html">CRM - custo de sucesso, custo de erro, custo de propriedade</a></li>
<li><a href="../pt440304/index.html">Interrompe dispositivos externos em um sistema x86. Parte 3. Configurando o roteamento de interrup√ß√£o no chipset usando o exemplo coreboot</a></li>
<li><a href="../pt440308/index.html">Divida e conquiste ou escreva devagar - leia rapidamente</a></li>
<li><a href="../pt440310/index.html">Como ensinar uma m√°quina a entender faturas e extrair dados delas</a></li>
<li><a href="../pt440312/index.html">Hackquest 2018. Resultados e revis√µes. Dia 4-7</a></li>
<li><a href="../pt440314/index.html">Candidato de libera√ß√£o do JDK 12: Shenandoah, G1, JMH, Arm64. Bugs no Swing contra-atacam</a></li>
<li><a href="../pt440316/index.html">Distribui√ß√£o uniforme de pontos em um tri√¢ngulo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>