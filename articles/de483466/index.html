<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßóüèΩ üë©‚Äçüíª üßì Einf√ºhrung in die Backpropagation-Methode üö¥üèº ‚Ü©Ô∏è üë∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Die Neujahrsfeiertage sind zu Ende, sodass wir wieder bereit sind, n√ºtzliches Material mit Ihnen zu teilen. Eine √úbersetzung dieses ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einf√ºhrung in die Backpropagation-Methode</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>Hallo allerseits!</i>  <i>Die Neujahrsfeiertage sind zu Ende, sodass wir wieder bereit sind, n√ºtzliches Material mit Ihnen zu teilen.</i>  <i>Eine √úbersetzung dieses Artikels wurde in Erwartung des Starts eines neuen Streams f√ºr den Kurs <a href="https://otus.pw/h0mh/">"Algorithmen f√ºr Entwickler" erstellt</a> .</i> <i><br><br></i>  <i>Lass uns gehen!</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br>  Die Methode der R√ºck√ºbertragung von Fehlern ist wahrscheinlich die grundlegendste Komponente eines neuronalen Netzwerks.  Es wurde zum ersten Mal in den 1960er Jahren beschrieben und fast 30 Jahre sp√§ter von Rumelhart, Hinton und Williams in einem Artikel mit dem Titel <a href="https://www.nature.com/articles/323533a0">‚ÄûRepr√§sentationen durch Fehler</a> in der R√ºck√ºbertragung <a href="https://www.nature.com/articles/323533a0">lernen‚Äú</a> popul√§r gemacht. <a name="habracut"></a><br><br>  Das Verfahren wird verwendet, um ein neuronales Netzwerk unter Verwendung der sogenannten Kettenregel (der Differenzierungsregel einer komplexen Funktion) effektiv zu trainieren.  Einfach ausgedr√ºckt, nach jedem Durchlauf durch das Netzwerk f√ºhrt die R√ºckw√§rtsausbreitung einen Durchlauf in die entgegengesetzte Richtung durch und passt die Modellparameter (Gewichte und Verschiebungen) an. <br><br>  In diesem Artikel m√∂chte ich den Prozess des Lernens und Optimierens eines einfachen neuronalen 4-Schicht-Netzwerks aus mathematischer Sicht detailliert betrachten.  Ich glaube, dass dies dem Leser helfen wird, die Funktionsweise von Backpropagation zu verstehen und ihre Bedeutung zu erkennen. <br><br><h3>  Definieren eines neuronalen Netzwerkmodells </h3><br>  Das vierschichtige neuronale Netzwerk besteht aus vier Neuronen in der Eingangsschicht, vier Neuronen in den versteckten Schichten und einem Neuron in der Ausgangsschicht. <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>Ein einfaches Bild eines vierschichtigen neuronalen Netzwerks.</i> <br><br><h3>  Eingabeebene </h3><br>  In der Abbildung stellen violette Neuronen die Eingabe dar.  Dies k√∂nnen einfache skalare Gr√∂√üen oder komplexere Vektoren oder mehrdimensionale Matrizen sein. <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>Gleichung, die die Eing√§nge xi beschreibt.</i> <br><br>  Der erste Aktivierungssatz (a) entspricht den Eingabewerten.  "Aktivierung" ist der Wert eines Neurons nach Anwendung der Aktivierungsfunktion.  Siehe unten f√ºr weitere Details. <br><br><h3>  Versteckte Schichten </h3><br>  Die Endwerte in verborgenen Neuronen (in der gr√ºnen Abbildung) werden unter Verwendung von zl-gewichteten Eingaben in Schicht I und einer <sup>I-</sup> Aktivierung in Schicht L berechnet. F√ºr die Schichten 2 und 3 lauten die Gleichungen wie folgt: <br><br>  F√ºr l = 2: <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br>  F√ºr l = 3: <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup> und W <sup>3</sup> sind die Gewichte auf den Schichten 2 und 3 und b <sup>2</sup> und b <sup>3</sup> sind die Offsets auf diesen Schichten. <br><br>  Die Aktivierungen a <sup>2</sup> und a <sup>3</sup> werden mit der Aktivierungsfunktion f berechnet.  Zum Beispiel ist diese Funktion f nicht linear (wie <a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> , <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> und <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">hyperbolischer Tangens</a> ) und erm√∂glicht es dem Netzwerk, komplexe Muster in den Daten zu untersuchen.  Wir werden nicht n√§her darauf eingehen, wie Aktivierungsfunktionen funktionieren, aber wenn Sie interessiert sind, empfehle ich Ihnen dringend, diesen wunderbaren <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">Artikel zu</a> lesen. <br><br>  Wenn Sie genau hinsehen, werden Sie feststellen, dass alle x, z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> , a <sup>3</sup> , W <sup>1</sup> , W <sup>2</sup> , b <sup>1</sup> und b <sup>2</sup> keine in der Figur eines neuronalen Netzwerks mit vier Schichten gezeigten Indizes haben.  Tatsache ist, dass wir alle Parameterwerte in Matrizen zusammengefasst haben, die nach Ebenen gruppiert sind.  Dies ist eine Standardmethode f√ºr die Arbeit mit neuronalen Netzen und sehr komfortabel.  Ich werde jedoch die Gleichungen durchgehen, damit es keine Verwirrung gibt. <br><br>  Nehmen wir als Beispiel Schicht 2 und ihre Parameter.  Dieselben Operationen k√∂nnen auf jede Schicht des neuronalen Netzwerks angewendet werden. <br>  W <sup>1</sup> ist die Matrix der Gewichte der Dimension <i>(n, m)</i> , wobei <i>n</i> die Anzahl der Ausgangsneuronen (Neuronen in der n√§chsten Schicht) und <i>m</i> die Anzahl der Eingangsneuronen (Neuronen in der vorherigen Schicht) ist.  In unserem Fall ist <i>n = 2</i> und <i>m = 4</i> . <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br>  Hier entspricht die erste Zahl im Index einer der Gewichte dem Neuronenindex in der n√§chsten Schicht (in unserem Fall ist dies die zweite verborgene Schicht), und die zweite Zahl entspricht dem Neuronenindex in der vorherigen Schicht (in unserem Fall ist dies die Eingabeschicht). <br><br>  <i>x</i> ist der Eingangsvektor der Dimension ( <i>m</i> , 1), wobei <i>m</i> die Anzahl der Eingangsneuronen ist.  In unserem Fall ist <i>m</i> = 4. <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup> ist der Verschiebungsvektor der Dimension ( <i>n</i> , 1), wobei <i>n</i> die Anzahl der Neuronen in der aktuellen Schicht ist.  In unserem Fall ist <i>n</i> = 2. <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br>  Nach der Gleichung f√ºr z <sup>2 k√∂nnen</sup> wir die obigen Definitionen von W <sup>1</sup> , x und b <sup>1 verwenden</sup> , um die Gleichung z <sup>2 zu erhalten</sup> : <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br>  Schauen Sie sich nun die Abbildung des obigen neuronalen Netzwerks genau an: <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br>  Wie Sie sehen k√∂nnen, kann z <sup>2</sup> als z <sub>1</sub> <sup>2</sup> und z <sub>2</sub> <sup>2</sup> ausgedr√ºckt werden, wobei z <sub>1</sub> <sup>2</sup> und z <sub>2</sub> <sup>2</sup> die Summen der Produkte jedes Eingabewerts x <sup>i</sup> durch das entsprechende Gewicht W <sub>ij</sub> <sup>1 sind</sup> . <br><br>  Dies f√ºhrt zu der gleichen Gleichung f√ºr z <sup>2</sup> und beweist, dass die Matrixdarstellungen z <sup>2</sup> , a <sup>2</sup> , z <sup>3</sup> und a <sup>3</sup> wahr sind. <br><br><h3>  Ausgabeschicht </h3><br>  Der letzte Teil des neuronalen Netzwerks ist die Ausgabeschicht, die den vorhergesagten Wert angibt.  In unserem einfachen Beispiel wird es in Form eines einzelnen blau gef√§rbten Neurons dargestellt und wie folgt berechnet: <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br>  Wieder verwenden wir die Matrixdarstellung, um die Gleichung zu vereinfachen.  Sie k√∂nnen die obigen Methoden verwenden, um die zugrunde liegende Logik zu verstehen. <br><br><h3>  Direktvertrieb und Auswertung </h3><br>  Die obigen Gleichungen bilden eine direkte Verteilung durch das neuronale Netz.  Hier ist eine kurze √úbersicht: <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>(1) - Eingabeschicht</i> <i><br></i>  <i>(2) - der Wert des Neurons in der ersten verborgenen Schicht</i> <i><br></i>  <i>(3) - Aktivierungswert auf der ersten verborgenen Ebene</i> <i><br></i>  <i>(4) - der Wert des Neurons in der zweiten verborgenen Schicht</i> <i><br></i>  <i>(5) - Aktivierungswert auf der zweiten verborgenen Ebene</i> <i><br></i>  <i>(6) - Ausgangsschicht</i> <br><br>  Der letzte Schritt im direkten Durchlauf besteht darin, den vorhergesagten Ausgabewert <i>s</i> relativ zum erwarteten Ausgabewert <i>y</i> auszuwerten. <br><br>  Die Ausgabe y ist Teil des Trainingsdatensatzes (x, y), wobei <i>x</i> die Eingabe ist (wie wir uns aus dem vorherigen Abschnitt erinnern). <br><br>  Die Sch√§tzung zwischen <i>s</i> und <i>y</i> erfolgt √ºber die Verlustfunktion.  Es kann einfach als <a href="https://en.wikipedia.org/wiki/Mean_squared_error">Standardfehler</a> oder komplexer als <a href="http://neuralnetworksanddeeplearning.com/chap3.html">Kreuzentropie sein</a> . <br><br>  Wir nennen diese Verlustfunktion C und bezeichnen sie wie folgt: <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br>  Wo die <i>Kosten</i> gleich dem Standardfehler, der Querentropie oder einer anderen Verlustfunktion sein k√∂nnen. <br><br>  Basierend auf dem Wert von C ‚Äûwei√ü‚Äú das Modell, wie sehr seine Parameter angepasst werden m√ºssen, um sich dem erwarteten Ausgabewert von <i>y anzun√§hern</i> .  Dies geschieht mit der Backpropagation-Methode. <br><br><h3>  R√ºckausbreitung von Fehlern und Berechnung von Gradienten </h3><br>  Basierend auf einem Artikel von 1989 wurde die Backpropagation-Methode: <br><br>  <i>Passt die Gewichtung der Verbindungen im Netzwerk st√§ndig an, um die Differenz zwischen dem tats√§chlichen Ausgangsvektor des Netzwerks und dem gew√ºnschten Ausgangsvektor zu minimieren</i> . <br>  und <br>  <i>... erm√∂glicht es, n√ºtzliche neue Funktionen zu erstellen, die die R√ºck√ºbertragung von fr√ºheren und einfacheren Methoden unterscheiden ...</i> <br><br>  Mit anderen Worten, Backpropagation zielt darauf ab, die Verlustfunktion durch Anpassen der Gewichte und Offsets des Netzwerks zu minimieren.  Der Grad der Anpassung wird durch die Gradienten der Verlustfunktion in Bezug auf diese Parameter bestimmt. <br><br>  Eine Frage stellt sich: <i>Warum Steigungen berechnen</i> ? <br><br>  Um diese Frage zu beantworten, m√ºssen wir zun√§chst einige Computerkonzepte √ºberarbeiten: <br><br>  Der Gradient der Funktion C (x <sup>1</sup> , x <sup>2</sup> , ..., x <sup>m</sup> ) bei x ist der <a href="https://en.wikipedia.org/wiki/Partial_derivative">Vektor der partiellen Ableitungen von</a> C in <i>Bezug</i> auf <i>x</i> . <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br>  Die Ableitung der Funktion C spiegelt die Empfindlichkeit gegen√ºber einer √Ñnderung des Wertes der Funktion (Ausgabewert) im Verh√§ltnis zu der √Ñnderung ihres Arguments <i>x</i> ( <a href="https://en.wikipedia.org/wiki/Derivative">Eingabewert</a> ) wider.  Mit anderen Worten, die Ableitung sagt uns, in welche Richtung sich C. bewegt. <br><br>  Der Gradient zeigt an, wie stark der Parameter <i>x</i> (in positiver oder negativer Richtung) ge√§ndert werden muss, um C zu minimieren. <br><br>  Diese Gradienten werden mit einer Methode berechnet, die als Kettenregel bezeichnet wird. <br>  F√ºr ein Gewicht (w <sup>jk</sup> ) <sub>l ist der</sub> Gradient: <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>(1) Kettenregel</i> <i><br></i>  <i>(2) Definitionsgem√§√ü ist m die Anzahl der Neuronen pro 1-Schicht</i> <i><br></i>  <i>(3) Ableitungsberechnung</i> <i><br></i>  <i>(4) Endwert</i> <i><br></i>  <i>Ein √§hnlicher Satz von Gleichungen kann auf (b <sup>j</sup> ) <sub>l</sub> angewendet werden</i> : <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>(1) Kettenregel</i> <i><br></i>  <i>(2) Ableitungsberechnung</i> <i><br></i>  <i>(3) Endwert</i> <br>  Der gemeinsame Teil in beiden Gleichungen wird oft als "lokaler Gradient" bezeichnet und wie folgt ausgedr√ºckt: <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br>  Ein "lokaler Gradient" kann leicht mit einer Kettenregel bestimmt werden.  Ich werde diesen Prozess jetzt nicht malen. <br><br>  Mit Farbverl√§ufen k√∂nnen Modellparameter optimiert werden: <br><br>  Bis das Stoppkriterium erreicht ist, wird Folgendes ausgef√ºhrt: <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>Algorithmus zur Optimierung von Gewichten und Offsets</i> (auch Gradientenabstieg genannt) <br><ul><li>  Die Anfangswerte von <i>w</i> und <i>b</i> werden zuf√§llig ausgew√§hlt. </li><li>  Epsilon (e) ist die Lerngeschwindigkeit.  Es bestimmt die Wirkung des Verlaufs. </li><li>  <i>w</i> und <i>b</i> sind Matrixdarstellungen von Gewichten und Offsets. </li><li>  Die Ableitung von C in Bezug auf <i>w</i> oder <i>b</i> kann unter Verwendung von partiellen Ableitungen von C in Bezug auf einzelne Gewichte oder Offsets berechnet werden. </li><li>  Die Abbruchbedingung ist erf√ºllt, sobald die Verlustfunktion minimiert ist. </li></ul><br><br>  Ich m√∂chte den letzten Teil dieses Abschnitts einem einfachen Beispiel widmen, in dem wir den Gradienten C f√ºr ein Gewicht (w <sup>22</sup> ) <sub>2</sub> berechnen. <br><br>  Vergr√∂√üern wir den unteren Rand des oben genannten neuronalen Netzwerks: <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>Visuelle Darstellung der Backpropagation in einem neuronalen Netzwerk</i> <br>  Das Gewicht (w <sup>22</sup> ) <sub>2</sub> verbindet (a <sup>2</sup> ) <sub>2</sub> und (z <sup>2</sup> ) <sub>2</sub> , daher erfordert die Berechnung des Gradienten die Anwendung der Kettenregel auf (z <sup>2</sup> ) <sub>3</sub> und (a <sup>2</sup> ) <sub>3</sub> : <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br>  Die Berechnung des Endwertes der Ableitung von C aus (a <sup>2</sup> ) <sub>3</sub> erfordert Kenntnis der Funktion C. Da C von (a <sup>2</sup> ) <sub>3</sub> abh√§ngt, sollte die Berechnung der Ableitung einfach sein. <br><br>  Ich hoffe, dieses Beispiel hat es geschafft, etwas Licht in die Mathematik f√ºr die Berechnung von Verl√§ufen zu bringen.  Wenn Sie mehr wissen m√∂chten, empfehle ich Ihnen dringend, die Artikelreihe Stanford NLP zu lesen, in der Richard Socher 4 gro√üartige Erkl√§rungen f√ºr die Backpropagation liefert. <br><br><h3>  Schlussbemerkung </h3><br>  In diesem Artikel habe ich ausf√ºhrlich erkl√§rt, wie die R√ºck√ºbertragung eines Fehlers unter der Haube mit mathematischen Methoden wie der Berechnung von Gradienten, Kettenregeln usw. funktioniert.  Wenn Sie die Mechanismen dieses Algorithmus kennen, werden Sie Ihre Kenntnisse √ºber neuronale Netze vertiefen und sich bei der Arbeit mit komplexeren Modellen wohlf√ºhlen.  Viel Gl√ºck auf deiner tiefen Lernreise! <br><br>  <b><i>Das ist alles.</i></b>  <b><i>Wir laden alle zu einem kostenlosen Webinar zum Thema <a href="https://otus.pw/h0mh/">"Baum der Segmente: einfach und schnell" ein.</a></i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de483466/">https://habr.com/ru/post/de483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de483448/index.html">Disney - die gr√∂√üte Zwei-Wege in der Geschichte der Menschheit</a></li>
<li><a href="../de483454/index.html">Wechseln von Mercurial zu GIT in Atlassian Bitbucket mit Speichern von Dateien in kyrillischer Sprache</a></li>
<li><a href="../de483458/index.html">GreenPig-Datenbankassistent</a></li>
<li><a href="../de483460/index.html">SQL HowTo: Erstellen von Ketten mit Fensterfunktionen</a></li>
<li><a href="../de483462/index.html">Halt die Klappe und nimm mein Geld</a></li>
<li><a href="../de483468/index.html">Flatterintegrationstests - ganz einfach</a></li>
<li><a href="../de483470/index.html">Fliesen effizient verlegen (Pro CSS, SVG, Muster und mehr)</a></li>
<li><a href="../de483472/index.html">Alles l√∂schen: So l√∂schen Sie Daten und setzen die NVMe-SSD auf die Werkseinstellungen zur√ºck</a></li>
<li><a href="../de483476/index.html">Die Moral des Robotertransports: Das Problem des Wagens, Risiken und Konsequenzen</a></li>
<li><a href="../de483478/index.html">Sonne, Wind und Wasser ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>