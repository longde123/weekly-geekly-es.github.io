<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôãÔ∏è üóÑÔ∏è üë©üèø‚Äçüíª Das Buch "Grok Deep Learning" üêç ü•î üèà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habrozhiteli! Das Buch legt den Grundstein f√ºr die weitere Beherrschung der Deep-Learning-Technologie. Es beginnt mit einer Beschreibung der Gru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Das Buch "Grok Deep Learning"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/464509/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/go/gm/1s/gogm1solwetphsozljuyzuizcbs.jpeg" align="left" alt="Bild"></a>  Hallo habrozhiteli!  Das Buch legt den Grundstein f√ºr die weitere Beherrschung der Deep-Learning-Technologie.  Es beginnt mit einer Beschreibung der Grundlagen neuronaler Netze und untersucht anschlie√üend die zus√§tzlichen Architekturschichten im Detail. <br><br>  Das Buch wurde speziell mit der Absicht geschrieben, die niedrigstm√∂gliche Eintrittsschwelle bereitzustellen.  Sie ben√∂tigen keine Kenntnisse √ºber lineare Algebra, numerische Methoden, konvexe Optimierungen und sogar maschinelles Lernen.  Alles, was erforderlich ist, um tiefes Lernen zu verstehen, wird im Laufe der Zeit gekl√§rt. <br><br>  Wir bieten Ihnen an, sich mit der Passage "Was ist ein Deep-Learning-Rahmen?" <br><a name="habracut"></a><br>  <b>Gute Tools reduzieren Fehler, beschleunigen die Entwicklung und erh√∂hen die Ausf√ºhrungsgeschwindigkeit.</b> <br><br>  Wenn Sie viel √ºber Deep Learning gelesen haben, sind Sie wahrscheinlich auf so bekannte Frameworks wie PyTorch, TensorFlow, Theano (k√ºrzlich f√ºr veraltet erkl√§rt), Keras, Lasagne und DyNet gesto√üen.  In den letzten Jahren haben sich Frameworks sehr schnell entwickelt, und trotz der Tatsache, dass alle diese Frameworks kostenlos und Open Source verteilt werden, hat jedes von ihnen einen Geist des Wettbewerbs und der Kameradschaft. <br><br>  Bisher habe ich es vermieden, Frameworks zu diskutieren, da es f√ºr Sie zun√§chst √§u√üerst wichtig war, zu verstehen, was sich hinter den Kulissen abspielte, und die Algorithmen manuell zu implementieren (nur mit der NumPy-Bibliothek).  Aber jetzt werden wir solche Frameworks verwenden, da die Netzwerke, die wir trainieren werden, Netzwerke mit Langzeit-Kurzzeitged√§chtnis (LSTM), sehr komplex sind und der Code, der sie mit NumPy implementiert, schwer zu lesen, zu verwenden und zu debuggen ist (Gradienten in diesem Code) sind √ºberall zu finden). <br><br>  Es ist diese Komplexit√§t, auf die Deep-Learning-Frameworks abzielen.  Das Deep-Learning-Framework kann die Komplexit√§t des Codes erheblich reduzieren (sowie die Anzahl der Fehler verringern und die Entwicklungsgeschwindigkeit erh√∂hen) und die Ausf√ºhrungsgeschwindigkeit erh√∂hen, insbesondere wenn Sie einen Grafikprozessor (GPU) zum Trainieren des neuronalen Netzwerks verwenden, wodurch der Prozess um das 10-100-fache beschleunigt werden kann.  Aus diesen Gr√ºnden werden die Frameworks fast √ºberall in der Forschungsgemeinschaft verwendet, und das Verst√§ndnis der Merkmale ihrer Arbeit wird Ihnen in Ihrer Karriere als Benutzer und Deep-Learning-Forscher von Nutzen sein. <br><br>  Wir werden uns jedoch nicht auf das Framework eines bestimmten Frameworks beschr√§nken, da dies Sie daran hindert, zu lernen, wie all diese komplexen Modelle (wie LSTM) funktionieren.  Stattdessen werden wir unser eigenes leichtes Framework erstellen, das den neuesten Trends bei der Entwicklung von Frameworks folgt.  Wenn Sie diesem Pfad folgen, wissen Sie genau, was Frameworks tun, wenn mit ihrer Hilfe komplexe Architekturen erstellt werden.  Wenn Sie versuchen, Ihr eigenes kleines Framework selbst zu erstellen, k√∂nnen Sie problemlos auf echte Deep-Learning-Frameworks umsteigen, da Sie die Prinzipien der Organisation einer Programmschnittstelle (API) und deren Funktionalit√§t bereits kennen.  Diese √úbung war f√ºr mich sehr n√ºtzlich, und die beim Erstellen meines eigenen Frameworks gewonnenen Erkenntnisse erwiesen sich beim Debuggen von Problemmodellen als sehr hilfreich. <br><br>  Wie vereinfacht das Framework Code?  Wenn Sie abstrakt sprechen, m√ºssen Sie nicht immer wieder denselben Code schreiben.  Das bequemste Merkmal des Deep-Learning-Frameworks ist insbesondere die Unterst√ºtzung der automatischen Backpropagation und der automatischen Optimierung.  Auf diese Weise k√∂nnen Sie nur direkten Verteilungscode schreiben, und das Framework k√ºmmert sich automatisch um die R√ºckverteilung und Korrektur von Gewichten.  Die meisten modernen Frameworks vereinfachen sogar Code, der die direkte Verteilung implementiert, indem sie √ºbergeordnete Schnittstellen zum Definieren typischer Ebenen und Verlustfunktionen bieten. <br><br><h3>  Einf√ºhrung in Tensoren </h3><br>  <b>Tensoren sind eine abstrakte Form von Vektoren und Matrizen</b> <br><br>  Bis zu diesem Moment verwendeten wir Vektoren und Matrizen als Hauptstrukturen.  Ich m√∂chte Sie daran erinnern, dass eine Matrix eine Liste von Vektoren und ein Vektor eine Liste von Skalaren (einzelne Zahlen) ist.  Ein Tensor ist eine abstrakte Form zur Darstellung verschachtelter Zahlenlisten.  Der Vektor ist ein eindimensionaler Tensor.  Die Matrix ist ein zweidimensionaler Tensor, und Strukturen mit einer gro√üen Anzahl von Dimensionen werden als n-dimensionale Tensoren bezeichnet.  Beginnen wir also mit der Erstellung eines neuen Deep-Learning-Frameworks, indem wir einen Basistyp definieren, den wir Tensor nennen: <br><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class"> </span></span>= np.array(data) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) print(x) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>] y = x + x print(y) [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>]</code> </pre> <br>  Dies ist die erste Version unserer Basisdatenstruktur.  Beachten Sie, dass alle numerischen Informationen im NumPy-Array (self.data) gespeichert werden und eine einzelne Tensoroperation (Addition) unterst√ºtzt wird.  Das Hinzuf√ºgen zus√§tzlicher Operationen ist √ºberhaupt nicht schwierig. F√ºgen Sie der Tensor-Klasse einfach zus√§tzliche Funktionen mit der entsprechenden Funktionalit√§t hinzu. <br><br><h3>  Einf√ºhrung in die automatische Gradientenberechnung (autograd) </h3><br>  <b>Zuvor haben wir eine manuelle R√ºckausbreitung durchgef√ºhrt.</b>  <b>Jetzt machen wir es automatisch!</b> <br><br>  In Kapitel 4 haben wir Derivate eingef√ºhrt.  Seitdem haben wir diese Ableitungen in jedem neuen neuronalen Netzwerk manuell berechnet.  Ich m√∂chte Sie daran erinnern, dass dies durch umgekehrte Bewegung durch das neuronale Netzwerk erreicht wird: Zuerst wird der Gradient am Ausgang des Netzwerks berechnet, dann wird dieses Ergebnis verwendet, um die Ableitung in der vorherigen Komponente zu berechnen, und so weiter, bis die richtigen Gradienten f√ºr alle Gewichte in der Architektur bestimmt sind.  Diese Logik zur Berechnung von Gradienten kann auch der Tensorklasse hinzugef√ºgt werden.  Das Folgende zeigt, was ich vorhatte. <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Tensor</span></span></span><span class="hljs-class"> (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">object</span></span></span><span class="hljs-class">): </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">def</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">__init__</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">creators</span></span></span></span>=None, creation_op=None): self.data = np.array(data) self.creation_op = creation_op self.creators = creators self.grad = None def backward(self, grad): self.grad = grad <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(self.creation_op == <span class="hljs-string"><span class="hljs-string">"add"</span></span>): self.creators[<span class="hljs-number"><span class="hljs-number">0</span></span>].backward(grad) self.creators[<span class="hljs-number"><span class="hljs-number">1</span></span>].backward(grad) def __add__(self, other): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Tensor(self.data + other.data, creators=[self,other], creation_op=<span class="hljs-string"><span class="hljs-string">"add"</span></span>) def __repr__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__repr__()) def __str__(self): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(self.data.__str__()) x = Tensor([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>]) y = Tensor([<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>]) z = x + y z.backward(Tensor(np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre> <br>  Diese Methode f√ºhrt zwei Neuerungen ein.  Zun√§chst erh√§lt jeder Tensor zwei neue Attribute.  Ersteller ist eine Liste aller Tensoren, die zum Erstellen des aktuellen Tensors verwendet werden (standardm√§√üig Keine).  Das hei√üt, wenn der Tensor z durch Addieren der beiden anderen Tensoren x und y erhalten wird, enth√§lt das Erstellerattribut des Tensors z die Tensoren x und y.  kreation_op ist ein begleitendes Attribut, das die Operationen speichert, die beim Erstellen dieses Tensors verwendet werden.  Das hei√üt, der Befehl z = x + y erzeugt einen Berechnungsgraphen mit drei Knoten (x, y und z) und zwei Kanten (z -&gt; x und z -&gt; y).  Jede Kante wird durch die Operation von creation_op signiert, dh add.  Dieses Diagramm hilft bei der Organisation der rekursiven R√ºckw√§rtsausbreitung von Verl√§ufen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/4d/qz/fx4dqzrh6y62rtttfy2vrn1jley.png" alt="Bild"></div><br>  Die erste Neuerung bei dieser Implementierung ist die automatische Erstellung eines Diagramms w√§hrend jeder mathematischen Operation.  Wenn wir z nehmen und eine andere Operation ausf√ºhren, wird das Diagramm in einer neuen Variablen fortgesetzt, die auf z verweist. <br><br>  Die zweite Neuerung in dieser Version der Tensor-Klasse ist die M√∂glichkeit, mithilfe eines Diagramms Gradienten zu berechnen.  Wenn Sie die z.backward () -Methode aufrufen, wird der Gradient f√ºr x und y √ºbergeben, wobei die Funktion ber√ºcksichtigt wird, mit der der z (add) -Tensor erstellt wurde.  Wie im obigen Beispiel gezeigt, √ºbergeben wir den Gradientenvektor (np.array ([1,1,1,1,1]]) an z, und dieser wendet ihn auf seine Eltern an.  Wie Sie sich wahrscheinlich aus Kapitel 4 erinnern, bedeutet R√ºckw√§rtsausbreitung durch Addition die Anwendung der R√ºckw√§rtsausbreitung.  In diesem Fall m√ºssen wir nur einen Farbverlauf zu x und y hinzuf√ºgen, also kopieren wir ihn von z nach x und y: <br><br><pre> <code class="javascript hljs">print(x.grad) print(y.grad) print(z.creators) print(z.creation_op) [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>] [array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])] add</code> </pre> <br>  Das bemerkenswerteste Merkmal dieser Form der automatischen Gradientenberechnung ist, dass sie rekursiv funktioniert - jeder Vektor ruft die .backward () -Methode aller seiner Eltern aus der Liste self.creators auf: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bt/ye/if/btyeiflumrhlbprlzqtzjxwipcs.png" alt="Bild"></div><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  25% Rabatt auf den Gutschein f√ºr Stra√üenh√§ndler - <b>Deep Learning</b> <br>  Nach Bezahlung der Papierversion des Buches wird ein elektronisches Buch per E-Mail verschickt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de464509/">https://habr.com/ru/post/de464509/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de464491/index.html">Vivaldi 2.7 - Intensives Leben in der Stille</a></li>
<li><a href="../de464495/index.html">Teamentwicklung und Reflexion als Managementkommunikation des Teamleiters</a></li>
<li><a href="../de464497/index.html">JIRA als Heilmittel gegen Schlaflosigkeit und Nervenzusammenbr√ºche</a></li>
<li><a href="../de464499/index.html">"Mat. Wall Street-Modell ‚Äúoder ein Versuch, die Kosten der Cloud-IT-Infrastruktur zu optimieren</a></li>
<li><a href="../de464503/index.html">Wi-Fi Passwort passend zu aircrack-ng</a></li>
<li><a href="../de464511/index.html">So sammeln Sie Benutzerkohorten in Form von Grafiken in Grafana [+ Docker-Bild mit einem Beispiel]</a></li>
<li><a href="../de464513/index.html">Duffle: Transformator von XD Design</a></li>
<li><a href="../de464515/index.html">Wie man E-Mails erstellt und nicht durcheinander bringt: Praktische Tipps</a></li>
<li><a href="../de464517/index.html">Neue KUBA-Karten</a></li>
<li><a href="../de464523/index.html">Zahlungssysteme (PSP) f√ºr das IT-Gesch√§ft: Wir spielen gro√ü</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>