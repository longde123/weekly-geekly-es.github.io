<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóØÔ∏è üê∞ üë®üèº‚Äçüöí Fermeture de trous dans un cluster Kubernetes. Rapport et transcription avec DevOpsConf üÄÑÔ∏è ‚è™ üèÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pavel Selivanov, Southbridge Solution Architect et Slurm Lecturer, a fait une pr√©sentation √† DevOpsConf 2019. Ce rapport fait partie du cours approfon...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Fermeture de trous dans un cluster Kubernetes. Rapport et transcription avec DevOpsConf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/472484/"><p>  Pavel Selivanov, Southbridge Solution Architect et Slurm Lecturer, a fait une pr√©sentation √† DevOpsConf 2019. Ce rapport fait partie du cours approfondi de Kubernetes Slur Mega. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Slurm Basic: une introduction √† Kubernetes</a> a lieu √† Moscou du 18 au 20 novembre. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Slurm Mega: Nous regardons sous le capot de Kubernetes</a> - Moscou, 22-24 novembre. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Slurm Online: les deux cours Kubernetes sont toujours</a> disponibles. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Gt4Q1du5FXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Sous la coupe - transcription du rapport. </p><a name="habracut"></a><br><p>  Bonjour, coll√®gues et sympathisants.  Aujourd'hui, je vais parler de s√©curit√©. </p><br><p>  Je vois qu'il y a beaucoup de gardes de s√©curit√© dans le hall aujourd'hui.  Je m'excuse √† l'avance si je n'utilise pas les termes du monde de la s√©curit√© de la mani√®re que vous avez accept√©e. </p><br><p>  Il se trouve qu'il y a environ six mois, je suis tomb√© entre les mains d'un groupe public de Kubernetes.  Public - signifie qu'il existe un ni√®me nombre d'espaces de noms, dans ces espaces de noms, il y a des utilisateurs isol√©s dans leur espace de noms.  Tous ces utilisateurs appartiennent √† diff√©rentes soci√©t√©s.  Eh bien, il √©tait suppos√© que ce cluster devait √™tre utilis√© comme CDN.  Autrement dit, ils vous donnent un cluster, ils donnent l'utilisateur l√†-bas, vous allez l√†-bas dans votre espace de noms, d√©ployez vos fronts. </p><br><p>  Ils ont essay√© de vendre un tel service √† ma pr√©c√©dente entreprise.  Et on m'a demand√© de pousser un cluster sur le sujet - cette solution est-elle appropri√©e ou non? </p><br><p>  Je suis venu dans ce cluster.  On m'a donn√© des droits limit√©s, un espace de noms limit√©.  L√†, les gars ont compris ce qu'√©tait la s√©curit√©.  Ils ont lu ce que Kubernetes avait le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC) - et ils l'ont tordu pour que je ne puisse pas ex√©cuter les pods s√©par√©ment du d√©ploiement.  Je ne me souviens pas de la t√¢che que j'essayais de r√©soudre en ex√©cutant sous sans d√©ploiement, mais je voulais vraiment ex√©cuter juste en dessous.  J‚Äôai d√©cid√© pour la bonne chance de voir quels droits j‚Äôai dans le cluster, ce que je peux, ce que je ne peux pas, ce qu‚Äôils ont rat√©.  Dans le m√™me temps, je vais vous dire ce qu'ils ont configur√© de mani√®re incorrecte dans le RBAC. </p><br><p>  Il se trouve que deux minutes plus tard, j'ai trouv√© un administrateur dans leur cluster, j'ai regard√© tous les espaces de noms voisins, j'ai vu les fronts de production des entreprises qui avaient d√©j√† achet√© le service et je suis rest√© bloqu√©.  Je me suis √† peine arr√™t√©, pour ne pas venir voir quelqu'un en avant et ne mettre aucun mot obsc√®ne sur la page principale. </p><br><p>  Je vais vous dire avec des exemples comment j'ai fait cela et comment m'en prot√©ger. </p><br><p>  Mais d'abord, je me pr√©sente.  Je m'appelle Pavel Selivanov.  Je suis architecte √† Southbridge.  Je comprends Kubernetes, DevOps et toutes sortes de choses fantaisistes.  Les ing√©nieurs de Southbridge et moi construisons tout cela, et je vous conseille. </p><br><p>  En plus de notre c≈ìur de m√©tier, nous avons r√©cemment lanc√© des projets appel√©s Slory.  Nous essayons d'apporter notre capacit√© √† travailler avec Kubernetes aux masses, √† enseigner aux autres comment travailler avec les K8 aussi. </p><br><p> De quoi je vais parler aujourd'hui.  Le sujet du rapport est √©vident - sur la s√©curit√© du cluster Kubernetes.  Mais je veux dire tout de suite que ce sujet est tr√®s vaste - et je veux donc pr√©ciser imm√©diatement ce que je ne dirai pas avec certitude.  Je ne parlerai pas de termes farfelus qui sont d√©j√† cent fois trop broy√©s sur Internet.  Tout RBAC et certificats. </p><br><p>  Je vais parler de la fa√ßon dont mes coll√®gues et moi en avons marre de la s√©curit√© dans le cluster Kubernetes.  Nous voyons ces probl√®mes √† la fois avec les fournisseurs qui fournissent des clusters Kubernetes et avec les clients qui viennent chez nous.  Et m√™me avec des clients qui nous viennent d'autres soci√©t√©s de conseil en administration.  Autrement dit, l'ampleur de la trag√©die est tr√®s grande en fait. </p><br><p>  Litt√©ralement trois points, dont je parlerai aujourd'hui: </p><br><ol><li>  Droits utilisateur vs droits pod.  Les droits d'utilisateur et les droits de foyer ne sont pas la m√™me chose. </li><li>  Collecte d'informations sur les clusters.  Je montrerai qu'√† partir du cluster, vous pouvez collecter toutes les informations dont vous avez besoin sans avoir de droits sp√©ciaux sur ce cluster. </li><li>  Attaque DoS sur le cluster.  Si nous ne pouvons pas collecter d'informations, nous pouvons dans tous les cas mettre le cluster.  Je vais parler des attaques DoS sur les contr√¥les de cluster. </li></ol><br><p>  Une autre chose courante que je mentionnerai est l'endroit o√π j'ai tout test√©, ce que je peux dire avec certitude que tout fonctionne. </p><br><p>  Comme base, nous prenons l'installation d'un cluster Kubernetes √† l'aide de Kubespray.  Si quelqu'un ne le sait pas, il s'agit en fait d'un ensemble de r√¥les pour Ansible.  Nous l'utilisons constamment dans notre travail.  La bonne chose est que vous pouvez rouler n'importe o√π - vous pouvez rouler sur les glandes et quelque part dans le nuage.  Une m√©thode d'installation convient en principe √† tout. </p><br><p>  Dans ce cluster, j'aurai Kubernetes v1.14.5.  L'ensemble du cluster de Cuba, que nous examinerons, est divis√© en espaces de noms, chaque espace de noms appartient √† une √©quipe distincte, les membres de cette √©quipe ont acc√®s √† chaque espace de noms.  Ils ne peuvent pas acc√©der √† des espaces de noms diff√©rents, mais uniquement aux leurs.  Mais il existe un compte administrateur qui a des droits sur l'ensemble du cluster. </p><br><p><img src="https://habrastorage.org/webt/xm/rj/rx/xmrjrxw6toktjj1ukm-tir_remm.jpeg"></p><br><p>  J'ai promis que la premi√®re chose que nous aurons serait d'obtenir des droits d'administrateur sur le cluster.  Nous avons besoin d'un pod sp√©cialement pr√©par√© qui cassera le cluster Kubernetes.  Il nous suffit de l'appliquer au cluster Kubernetes. </p><br><pre><code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Ce pod arrivera chez l'un des ma√Ætres du cluster Kubernetes.  Et apr√®s cela, le cluster nous retournera avec plaisir un fichier appel√© admin.conf.  √Ä Cuba, tous les certificats d'administrateur sont stock√©s dans ce fichier, et en m√™me temps, l'API de cluster est configur√©e.  C'est juste comment vous pouvez obtenir un acc√®s administrateur, je pense, √† 98% des clusters Kubernetes. </p><br><p>  Je r√©p√®te, ce pod a √©t√© cr√©√© par un d√©veloppeur de votre cluster qui a acc√®s pour d√©ployer ses propositions dans un petit espace de noms, il est tout serr√© par RBAC.  Il n'avait aucun droit.  N√©anmoins, le certificat est revenu. </p><br><p>  Et maintenant sur le foyer sp√©cialement pr√©par√©.  Ex√©cutez sur n'importe quelle image.  Par exemple, prenez debian: jessie. </p><br><p>  Nous avons une telle chose: </p><br><pre> <code class="plaintext hljs">tolerations: - effect: NoSchedule operator: Exists nodeSelector: node-role.kubernetes.io/master: ""</code> </pre> <br><p>  Qu'est-ce que la tol√©rance?  Les ma√Ætres du cluster Kubernetes sont g√©n√©ralement marqu√©s d'une chose appel√©e taint ("infection" en anglais).  Et l'essence de cette "infection" - elle dit que les pods ne peuvent pas √™tre assign√©s aux n≈ìuds ma√Ætres.  Mais personne ne se soucie d'indiquer en aucune fa√ßon qu'il est tol√©rant √† "l'infection".  La section Tol√©rance dit simplement que si NoSchedule est sur un n≈ìud, alors notre sous-infection est tol√©rante - et aucun probl√®me. </p><br><p>  De plus, nous disons que notre sous n'est pas seulement tol√©rant, mais veut aussi tomber sp√©cifiquement sur le ma√Ætre.  Parce que les ma√Ætres sont les plus d√©licieux dont nous avons besoin - tous les certificats.  Par cons√©quent, nous disons nodeSelector - et nous avons une √©tiquette standard sur les assistants, ce qui nous permet de s√©lectionner exactement les n≈ìuds qui sont des assistants de tous les n≈ìuds du cluster. </p><br><p>  Avec ces deux sections, il viendra d√©finitivement au ma√Ætre.  Et il sera autoris√© √† y vivre. </p><br><p>  Mais venir au ma√Ætre ne nous suffit pas.  Cela ne nous donnera rien.  Par cons√©quent, nous avons en outre ces deux choses: </p><br><pre> <code class="plaintext hljs">hostNetwork: true hostPID: true</code> </pre> <br><p>  Nous indiquons que notre sous, que nous lan√ßons, vivra dans l'espace de noms du noyau, dans l'espace de noms du r√©seau et dans l'espace de noms PID.  D√®s qu'il d√©marre sur l'assistant, il pourra voir toutes les interfaces r√©elles et en direct de ce n≈ìud, √©couter tout le trafic et voir le PID de tous les processus. </p><br><p>  Ensuite, c'est petit.  Prenez etcd et lisez ce que vous voulez. </p><br><p>  La plus int√©ressante est cette fonctionnalit√© Kubernetes, qui y est pr√©sente par d√©faut. </p><br><pre> <code class="plaintext hljs">volumeMounts: - mountPath: /host name: host volumes: - hostPath: path: / type: Directory name: host</code> </pre> <br><p>  Et son essence est que nous pouvons dire que nous voulons cr√©er un volume de type hostPath dans le pod que nous ex√©cutons, m√™me sans les droits sur ce cluster.  Cela signifie prendre le chemin de l'h√¥te sur lequel nous allons commencer - et le prendre comme volume.  Et puis appelez-le nom: h√¥te.  Tout ce hostPath que nous montons √† l'int√©rieur du foyer.  Dans cet exemple, dans le r√©pertoire / host. </p><br><p>  Je r√©p√®te encore une fois.  Nous avons dit au pod de venir vers le ma√Ætre, d'y obtenir hostNetwork et hostPID - et de monter la racine enti√®re du ma√Ætre √† l'int√©rieur de ce pod. </p><br><p>  Vous comprenez que dans debian, nous avons bash en cours d'ex√©cution, et ce bash fonctionne sous notre racine.  Autrement dit, nous venons d'obtenir la racine du ma√Ætre, sans avoir de droits sur le cluster Kubernetes. </p><br><p>  De plus, toute la t√¢che consiste √† aller dans le sous-r√©pertoire / host / etc / kubernetes / pki, si je ne me trompe pas, r√©cup√©rez tous les certificats ma√Ætres du cluster l√†-bas et, en cons√©quence, devenez l'administrateur du cluster. </p><br><p>  Si vous regardez de cette fa√ßon, ce sont certains des droits les plus dangereux dans les pods - malgr√© les droits de l'utilisateur: <br><img src="https://habrastorage.org/webt/ax/07/cj/ax07cjhm7y0dwpueikrj-qwqv2k.jpeg"></p><br><p>  Si j'ai des droits d'ex√©cution dans un espace de noms de cluster, ce sous-marin a ces droits par d√©faut.  Je peux ex√©cuter des pods privil√©gi√©s, et ce sont g√©n√©ralement tous les droits, pratiquement root sur le n≈ìud. </p><br><p>  Mon pr√©f√©r√© est l'utilisateur root.  Et Kubernetes a une telle option Run As Non-Root.  Il s'agit d'un type de protection contre les pirates.  Savez-vous ce qu'est le ¬´virus moldave¬ª?  Si vous √™tes un pirate informatique et que vous venez sur mon cluster Kubernetes, alors nous, pauvres administrateurs, demandons: ¬´Veuillez indiquer dans vos pods avec lesquels vous allez pirater mon cluster, ex√©cut√© en tant que non root.  Et il se trouve que vous d√©marrez le processus dans votre foyer sous la racine, et il vous sera tr√®s facile de me pirater.  Veuillez vous prot√©ger de vous-m√™me. ¬ª </p><br><p>  Volume du chemin de l'h√¥te - √† mon avis, le moyen le plus rapide d'obtenir le r√©sultat souhait√© √† partir du cluster Kubernetes. </p><br><p>  Mais que faire de tout √ßa? </p><br><p>  R√©flexions qui devraient venir √† tout administrateur normal qui rencontre Kubernetes: ¬´Oui, je vous l'ai dit, Kubernetes ne fonctionne pas.  Il y a des trous dedans.  Et le cube entier est des conneries. "  En fait, la documentation existe, et si vous y regardez, il y a une section <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Politique de s√©curit√© des pods</a> . </p><br><p>  Il s'agit d'un tel objet yaml - nous pouvons le cr√©er dans le cluster Kubernetes - qui contr√¥le les aspects de s√©curit√© dans la description des foyers.  Autrement dit, il contr√¥le ces droits pour utiliser toutes sortes de hostNetwork, hostPID, certains types de volume, qui sont dans les pods au d√©marrage.  Avec la politique de s√©curit√© des pods, tout cela peut √™tre d√©crit. </p><br><p>  La chose la plus int√©ressante dans la politique de s√©curit√© des pods est que dans le cluster Kubernetes, tous les installateurs PSP ne sont tout simplement pas d√©crits en aucune fa√ßon, ils sont simplement d√©sactiv√©s par d√©faut.  La politique de s√©curit√© des pods est activ√©e √† l'aide du plug-in d'admission. </p><br><p>  D'accord, finissons dans une politique de s√©curit√© des pods de cluster, disons que nous avons une sorte de pods de service dans l'espace de noms, auxquels seuls les administrateurs ont acc√®s.  Disons que dans tous les autres pods, ils ont des droits limit√©s.  Parce que tr√®s probablement, les d√©veloppeurs n'ont pas besoin d'ex√©cuter des modules privil√©gi√©s dans votre cluster. </p><br><p>  Et tout semble aller bien avec nous.  Et notre cluster Kubernetes ne peut pas √™tre pirat√© en deux minutes. </p><br><p>  Il y a un probl√®me.  Tr√®s probablement, si vous avez un cluster Kubernetes, la surveillance est install√©e dans votre cluster.  Je pr√©sume m√™me de pr√©dire que s'il y a une surveillance dans votre cluster, alors cela s'appelle Prom√©th√©e. </p><br><p>  Ce que je vais vous dire maintenant sera valable √† la fois pour l'op√©rateur Prometheus et pour le Prometheus livr√© sous sa forme pure.  La question est que si je ne parviens pas √† obtenir l‚Äôadministrateur aussi rapidement dans le cluster, cela signifie que j‚Äôai besoin de chercher plus.  Et je peux rechercher en utilisant votre surveillance. </p><br><p>  Probablement, tout le monde a lu les m√™mes articles sur Habr√©, et la surveillance est en surveillance.  Le diagramme de barre est appel√© approximativement le m√™me pour tout le monde.  Je suppose que si vous installez helm stable / prometheus, vous obtiendrez approximativement les m√™mes noms.  Et m√™me tr√®s probablement, je n'aurai pas √† deviner le nom DNS de votre cluster.  Parce que c'est standard. </p><br><p><img src="https://habrastorage.org/webt/o6/rv/fw/o6rvfw0idykw0wyxivpfmwkd_vy.jpeg"></p><br><p>  En outre, nous avons un certain dev ns, il est possible de lancer un certain sous.  Et plus loin de ce foyer, il est tr√®s facile de faire comme √ßa: </p><br><pre> <code class="plaintext hljs">$ curl http://prometheus-kube-state-metrics.monitoring</code> </pre> <br><p>  prometheus-kube-state-metrics est l'un des exportateurs prometheus qui collecte des m√©triques √† partir de l'API Kubernetes.  Il y a beaucoup de donn√©es qui s'ex√©cutent dans votre cluster, ce que c'est, quels probl√®mes vous avez avec. </p><br><p>  Comme exemple simple: </p><br><p>  kube_pod_container_info {namespace = "kube-system", pod = "kube-apiserver-k8s-1", container = "kube-apiserver", image = </p><br><p>  <strong>"gcr.io/google-containers/kube-apiserver:v1.14.5"</strong> </p><br><p>  , Image_id = "docker-pullable: //gcr.io/google-containers/kube- apiserver @ SHA256: e29561119a52adad9edc72bfe0e7fcab308501313b09bf99df4a96 38ee634989", container_id = "docker: // 7cbe7b1fea33f811fdd8f7e0e079191110268f2 853397d7daf08e72c22d3cf8b"} 1 </p><br><p>  Apr√®s avoir fait une simple demande de boucle √† partir d'un fichier non privil√©gi√©, vous pouvez obtenir ces informations.  Si vous ne savez pas dans quelle version de Kubernetes vous utilisez, il vous le dira facilement. </p><br><p>  Et le plus int√©ressant est que, outre le fait que vous vous tourniez vers les m√©triques d'√©tat du kube, vous pouvez tout aussi bien appliquer directement √† Prom√©th√©e lui-m√™me.  Vous pouvez collecter des m√©triques √† partir de l√†.  Vous pouvez m√™me cr√©er des m√©triques √† partir de l√†.  M√™me th√©oriquement, vous pouvez cr√©er une telle demande √† partir d'un cluster dans Prometheus, ce qui la d√©sactive simplement.  Et votre surveillance cesse g√©n√©ralement de fonctionner √† partir du cluster. </p><br><p>  Et ici, la question se pose d√©j√† de savoir si une surveillance externe surveille votre surveillance.  Je viens d'avoir l'opportunit√© d'agir dans le cluster Kubernetes sans aucune cons√©quence pour moi.  Vous ne saurez m√™me pas que j'agis l√†-bas, car il n'y a plus de surveillance. </p><br><p>  Tout comme avec PSP, le probl√®me est que toutes ces technologies √† la mode - Kubernetes, Prometheus - ne fonctionnent tout simplement pas et sont pleines de trous.  Pas vraiment. </p><br><p>  Il y a une telle chose - la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">politique de r√©seau</a> . </p><br><p>  Si vous √™tes un administrateur normal, alors tr√®s probablement √† propos de la strat√©gie r√©seau, vous savez qu'il s'agit d'un autre yaml, qui dans le cluster est d√©j√† dofig.  Et certaines politiques r√©seau ne sont absolument pas n√©cessaires.  Et m√™me si vous lisez ce qu'est la strat√©gie r√©seau, qu'est-ce que le pare-feu Kubernetes yaml, il vous permet de restreindre les droits d'acc√®s entre les espaces de noms, entre les pods, puis vous avez certainement d√©cid√© que le pare-feu de type yaml dans Kubernetes est sur les abstractions suivantes ... Non, non .  Ce n'est certainement pas n√©cessaire. </p><br><p>  M√™me si vos sp√©cialistes de la s√©curit√© n‚Äôont pas √©t√© inform√©s qu‚Äôen utilisant votre Kubernetes, vous pouvez cr√©er un pare-feu tr√®s facilement et simplement, et il est tr√®s granulaire.  S'ils ne le savent toujours pas et ne vous tirent pas: ¬´Eh bien, donnez, donnez ...¬ª Dans tous les cas, vous avez besoin d'une strat√©gie r√©seau pour bloquer l'acc√®s √† certains emplacements de service que vous pouvez retirer de votre cluster sans aucune autorisation. </p><br><p>  Comme dans l'exemple que j'ai cit√©, vous pouvez extraire les m√©triques d'√©tat du kube depuis n'importe quel espace de noms du cluster Kubernetes sans y avoir de droits.  Les strat√©gies r√©seau ont ferm√© l'acc√®s de tous les autres espaces de noms √† la surveillance des espaces de noms et, pour ainsi dire, √† tout: aucun acc√®s, aucun probl√®me.  Dans tous les graphiques qui existent, √† la fois le prometeus standard et ce prometeus qui est dans l'op√©rateur, il y a simplement dans les valeurs de la barre il y a une option pour simplement activer les politiques de r√©seau pour eux.  Il vous suffit de l'activer et ils fonctionneront. </p><br><p>  Il y a vraiment un probl√®me ici.  En tant qu'administrateur barbu normal, vous avez probablement d√©cid√© que les strat√©gies r√©seau ne sont pas n√©cessaires.  Et apr√®s avoir lu toutes sortes d'articles sur des ressources telles que Habr, vous avez d√©cid√© que la flanelle, en particulier avec le mode passerelle h√¥te, √©tait la meilleure chose que vous puissiez choisir. </p><br><p>  Que faire </p><br><p>  Vous pouvez essayer de red√©ployer la solution r√©seau qui se trouve dans votre cluster Kubernetes, essayez de la remplacer par quelque chose de plus fonctionnel.  Sur le m√™me Calico, par exemple.  Mais imm√©diatement, je veux dire que la t√¢che de changer la solution r√©seau dans le cluster de travail de Kubernetes n'est pas anodine.  Je l'ai r√©solu deux fois (les deux fois, cependant, th√©oriquement), mais nous avons m√™me montr√© comment faire cela sur les Slurms.  Pour nos √©tudiants, nous avons montr√© comment changer la solution r√©seau dans le cluster Kubernetes.  En principe, vous pouvez essayer de vous assurer qu'il n'y a pas de temps d'arr√™t sur le cluster de production.  Mais vous ne r√©ussirez probablement pas. </p><br><p>  Et le probl√®me est en fait r√©solu tr√®s simplement.  Il y a des certificats dans le cluster et vous savez que vos certificats vont mal tourner dans un an.  Eh bien, et g√©n√©ralement une solution normale avec des certificats dans le cluster - pourquoi allons-nous cr√©er un nouveau cluster √† c√¥t√© de lui, le laisser pourrir dans l'ancien, et nous allons tout refaire.  Certes, quand √ßa va mal, tout se couchera de nos jours, mais alors un nouveau cluster. </p><br><p>  Lorsque vous soulevez un nouveau cluster, ins√©rez en m√™me temps Calico au lieu de flanelle. </p><br><p>  Que faire si vous avez des certificats d√©livr√©s depuis cent ans et que vous n'allez pas recluster le cluster?  Il y a une telle chose Kube-RBAC-Proxy.  C'est un d√©veloppement tr√®s cool, il vous permet de vous int√©grer comme conteneur de sidecar √† n'importe quel foyer du cluster Kubernetes.  Et elle ajoute en fait une autorisation via Kubernetes RBAC √† ce pod. </p><br><p>  Il y a un probl√®me.  Auparavant, Kube-RBAC-Proxy √©tait int√©gr√© au prometheus de l'op√©rateur.  Mais il √©tait parti.  D√©sormais, les versions modernes reposent sur le fait que vous avez une strat√©gie r√©seau et que vous les fermez.  Et donc vous devez r√©√©crire un peu le graphique.  En fait, si vous acc√©dez √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce r√©f√©rentiel</a> , il existe des exemples de la fa√ßon de l'utiliser comme side-cars, et vous devrez r√©√©crire les graphiques de mani√®re minimale. </p><br><p>  Il y a un autre petit probl√®me.  Non seulement Prometheus donne ses m√©triques √† toute personne qui les obtient.  Nous avons √©galement tous les composants du cluster Kubernetes, ils peuvent donner leurs m√©triques. </p><br><p>  Mais comme je l'ai dit, si vous ne pouvez pas acc√©der au cluster et collecter des informations, vous pouvez au moins faire du mal. </p><br><p>  Je vais donc vous montrer rapidement deux fa√ßons de g√¢cher votre cluster Kubernetes. </p><br><p>  Vous allez rire quand je vous le dis, ce sont deux cas de la vraie vie. </p><br><p>  La premi√®re fa√ßon.  Manque de ressources. </p><br><p>  Nous lan√ßons un autre sous sp√©cial.  Il aura une telle section. </p><br><pre> <code class="plaintext hljs">resources: requests: cpu: 4 memory: 4Gi</code> </pre> <br><p>  Comme vous le savez, les requ√™tes sont la quantit√© de CPU et de m√©moire r√©serv√©e sur l'h√¥te pour des pods sp√©cifiques avec des requ√™tes.  Si nous avons un h√¥te √† quatre c≈ìurs dans le cluster Kubernetes et que quatre CPU y arrivent avec des demandes, cela signifie qu'aucun pod suppl√©mentaire avec des demandes √† cet h√¥te ne peut arriver. </p><br><p>  Si je lance ceci sous, alors je ferai une commande: </p><br><pre> <code class="plaintext hljs">$ kubectl scale special-pod --replicas=...</code> </pre> <br><p>  Personne d'autre ne pourra alors se d√©ployer sur le cluster Kubernetes.  Parce que dans tous les n≈ìuds, les demandes se termineront.  Et donc j'arr√™te votre cluster Kubernetes.  Si je le fais le soir, je peux arr√™ter le d√©ploiement pendant un certain temps. </p><br><p>  Si nous regardons √† nouveau la documentation de Kubernetes, nous verrons une chose appel√©e Limit Range.  Il d√©finit des ressources pour les objets de cluster.  Vous pouvez √©crire un objet Limit Range dans yaml, l'appliquer √† certains espaces de noms - et plus loin dans cet espace de noms, vous pouvez dire que vous avez des ressources pour les pods par d√©faut, maximum et minimum. </p><br><p>  Avec l'aide d'une telle chose, nous pouvons limiter les utilisateurs dans des espaces de noms de produits sp√©cifiques d'√©quipes dans la capacit√© d'indiquer des choses d√©sagr√©ables sur leurs pods.  Mais malheureusement, m√™me si vous dites √† l'utilisateur qu'il est impossible d'ex√©cuter des pods avec des demandes de plusieurs processeurs, il existe une merveilleuse commande de mise √† l'√©chelle, ou via le tableau de bord, ils peuvent faire de la mise √† l'√©chelle. </p><br><p>  Et d'ici vient la m√©thode num√©ro deux.  Nous lan√ßons 11 111 111 111 111 111 foyers.  C'est onze milliards.  Ce n'est pas parce que j'ai trouv√© un tel num√©ro, mais parce que je l'ai vu moi-m√™me. </p><br><p>  La vraie histoire.  Tard dans la soir√©e, j'allais quitter le bureau.  Je regarde, un groupe de d√©veloppeurs est assis dans le coin et fait quelque chose fr√©n√©tiquement avec les ordinateurs portables.  Je vais voir les gars et je leur demande: "Qu'est-ce qui vous est arriv√©?" </p><br><p>  Un peu plus t√¥t, √† neuf heures du soir, l'un des d√©veloppeurs rentrait chez lui.  Et il a d√©cid√©: "Je vais sauter ma candidature jusqu'√† maintenant."  J'ai cliqu√© un peu et Internet un peu terne.  Il a de nouveau cliqu√© sur l'unit√©, il a appuy√© sur l'unit√©, a cliqu√© sur Entr√©e.  Pouss√© sur tout ce qu'il pouvait.  Puis Internet a pris vie - et tout a commenc√© √† √©voluer jusqu'√† cette date. </p><br><p>  Certes, cette histoire n'a pas eu lieu sur Kubernetes, √† l'√©poque c'√©tait Nomad.  Cela s'est termin√© par le fait qu'apr√®s une heure de nos tentatives pour emp√™cher Nomad de tenter de rester ensemble, Nomad a r√©pondu qu'il n'arr√™terait pas de coller et ne ferait rien d'autre.  "Je suis fatigu√©, je pars."  Et recroquevill√©. </p><br><p>  J'ai naturellement essay√© de faire de m√™me sur Kubernetes.  Les onze milliards de pods de Kubernetes n'√©taient pas satisfaits, a-t-il d√©clar√©: ¬´Je ne peux pas.  D√©passe les prot√®ge-dents internes. "  Mais 1 000 000 000 de foyers le pouvaient. </p><br><p>  En r√©ponse √† un milliard, le Cube n'est pas entr√© √† l'int√©rieur.  Il a vraiment commenc√© √† √©voluer.  Plus le processus avan√ßait, plus il lui fallait de temps pour cr√©er de nouveaux foyers.  Mais le processus continuait.  Le seul probl√®me est que si je peux ex√©cuter des pods ind√©finiment dans mon espace de noms, m√™me sans demandes et limites, je peux d√©marrer un tel nombre de pods avec certaines t√¢ches qu'avec ces t√¢ches, les n≈ìuds commenceront √† s'additionner √† partir de la m√©moire, √† partir du CPU.  Lorsque j'ex√©cute autant de foyers, les informations qu'ils contiennent doivent aller dans le r√©f√©rentiel, c'est-√†-dire, etcd.  Et quand trop d'informations arrivent l√†-bas, l'entrep√¥t commence √† trahir trop lentement - et √† Kubernetes les choses ternes commencent. </p><br><p>  Et encore un probl√®me ... Comme vous le savez, les √©l√©ments de contr√¥le de Kubernetes ne sont pas seulement une chose centrale, mais plusieurs composants.  L√†, en particulier, il y a un gestionnaire de contr√¥leur, un planificateur, etc.  Tous ces gars-l√† commenceront √† faire un travail stupide inutile en m√™me temps, ce qui avec le temps commencera √† prendre de plus en plus de temps.  Le gestionnaire de contr√¥leur va cr√©er de nouveaux pods.  Le planificateur va essayer de leur trouver un nouveau n≈ìud.  Les nouveaux n≈ìuds de votre cluster prendront probablement fin bient√¥t.  Le cluster Kubernetes commencera √† fonctionner plus lentement et plus lentement. </p><br><p>  Mais j'ai d√©cid√© d'aller encore plus loin.  Comme vous le savez, √† Kubernetes, il existe une chose appel√©e service.  Eh bien, et par d√©faut dans vos clusters, le service fonctionne probablement √† l'aide de tables IP. </p><br><p>  Si vous ex√©cutez un milliard de foyers, par exemple, puis utilisez un script pour forcer Kubernetis √† cr√©er de nouveaux services: </p><br><pre> <code class="plaintext hljs">for i in {1..1111111}; do kubectl expose deployment test --port 80 \ --overrides="{\"apiVersion\": \"v1\", \"metadata\": {\"name\": \"nginx$i\"}}"; done</code> </pre> <br><p>  Sur tous les n≈ìuds du cluster, approximativement de nouvelles r√®gles iptables seront g√©n√©r√©es approximativement simultan√©ment.  De plus, pour chaque service, un milliard de r√®gles iptables seront g√©n√©r√©es. </p><br><p>  J'ai v√©rifi√© tout cela sur plusieurs milliers, jusqu'√† une douzaine.  Et le probl√®me est que d√©j√† √† ce seuil ssh sur le n≈ìud est assez probl√©matique √† faire.  Parce que les paquets, passant un tel nombre de cha√Ænes, commencent √† ne pas se sentir tr√®s bien. </p><br><p>  Et tout cela est √©galement r√©solu avec l'aide de Kubernetes.  Il existe un tel objet quota de ressources.  D√©finit le nombre de ressources et d'objets disponibles pour un espace de noms dans un cluster.  Nous pouvons cr√©er un objet yaml dans chaque espace de noms du cluster Kubernetes.  En utilisant cet objet, nous pouvons dire que nous avons allou√© un certain nombre de demandes, des limites pour cet espace de noms, puis nous pouvons dire que dans cet espace de noms, il est possible de cr√©er 10 services et 10 pods.  Et un seul d√©veloppeur peut au moins se faufiler le soir.  Kubernetes lui dira: "Vous ne pouvez pas tenir vos pods √† un tel montant car il d√©passe le quota de ressources."  Tout, le probl√®me est r√©solu.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La documentation est ici</a> . </p><br><p>  Un point probl√©matique se pose √† cet √©gard.  Vous sentez √† quel point il devient difficile de cr√©er un espace de noms dans Kubernetes.  Pour le cr√©er, nous devons consid√©rer un tas de choses. </p><br><p>  Quota de ressources + plage limite + RBAC <br>  ‚Ä¢ Cr√©er un espace de noms <br>  ‚Ä¢ Cr√©er une plage limite int√©rieure <br>  ‚Ä¢ Cr√©er un quota de ressources interne <br>  ‚Ä¢ Cr√©er un compte de service pour CI <br>  ‚Ä¢ Cr√©er une liaison de r√¥les pour CI et les utilisateurs <br>  ‚Ä¢ Ex√©cutez √©ventuellement les modules de service n√©cessaires </p><br><p>  Par cons√©quent, saisissant cette occasion, je voudrais partager mes d√©veloppements.  Il existe une telle chose, appel√©e l'op√©rateur SDK.  C'est un moyen dans le cluster Kubernetes d'√©crire des op√©rateurs pour lui.  Vous pouvez √©crire des instructions √† l'aide d'Ansible. </p><br><p>  Tout d'abord, il a √©t√© √©crit en Ansible, puis j'ai regard√© qu'il y avait un op√©rateur SDK et j'ai r√©√©crit le r√¥le Ansible dans l'op√©rateur.  Cet op√©rateur vous permet de cr√©er un objet dans le cluster Kubernetes appel√© une √©quipe.       yaml    .       ,    - . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">    </a> . </p><br><p>   .     ? <br> . Pod Security Policy ‚Äî  .     ,            , -      . </p><br><p> Network Policy ‚Äî   -    .  ,     . </p><br><p> LimitRange/ResourceQuota ‚Äî   .     ,     ,     . ,   . </p><br><p>  ,      ,   ,    .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> . </p><br><p>      .  ,           warlocks ,   . </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  ,   ,   .      ,  ResourceQuota, Pod Security Policy .     . </p><br><p>  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr472484/">https://habr.com/ru/post/fr472484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr472468/index.html">ClusterJ - travailler avec MySQL NDB Cluster de Java</a></li>
<li><a href="../fr472470/index.html">Souris transg√©niques et anti-√¢ge</a></li>
<li><a href="../fr472472/index.html">Chalet en hiver: √™tre ou ne pas √™tre?</a></li>
<li><a href="../fr472474/index.html">Bug cosm√©tique dr√¥le dans Google Chrome</a></li>
<li><a href="../fr472482/index.html">Accident radioactif: d√©couverte d'une phase solide solide de plutonium</a></li>
<li><a href="../fr472486/index.html">Stockage de donn√©es √† long terme. (Article - discussion)</a></li>
<li><a href="../fr472488/index.html">Trente reportages de DevOops 2019: Tim Lister, Hadi Hariri, Roman Shaposhnik et autres stars du DevOps international</a></li>
<li><a href="../fr472490/index.html">Comment j'ai recherch√© une norme de beaut√© √† l'aide du traitement du langage naturel (et je ne l'ai pas trouv√©e)</a></li>
<li><a href="../fr472492/index.html">Analyse du code de ROOT, cadre d'analyse des donn√©es scientifiques</a></li>
<li><a href="../fr472494/index.html">Analyse de code ROOT - cadre d'analyse des donn√©es de recherche</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>