<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥅 🤹🏻 🤸 Keingintahuan dan penundaan dalam pembelajaran mesin 🈺 🌑 🐉</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforced Learning (RL) adalah salah satu teknik pembelajaran mesin yang paling menjanjikan yang sedang dikembangkan secara aktif. Di sini, agen AI m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Keingintahuan dan penundaan dalam pembelajaran mesin</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427847/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Reinforced Learning</a> (RL) adalah salah satu teknik pembelajaran mesin yang paling menjanjikan yang sedang dikembangkan secara aktif.  Di sini, agen AI menerima hadiah positif untuk tindakan yang benar, dan hadiah negatif untuk yang salah.  Metode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">wortel dan tongkat</a> ini sederhana dan universal.  Dengan itu, DeepMind mengajarkan algoritma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DQN</a> untuk memainkan video game Atari lama, dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AlphaGoZero</a> untuk memainkan game Go kuno.  Jadi OpenAI mengajarkan algoritma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">OpenAI-Five</a> untuk memainkan video game Dota modern, dan Google mengajarkan tangan robot untuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menangkap objek baru</a> .  Terlepas dari keberhasilan RL, masih ada banyak masalah yang mengurangi efektivitas teknik ini. <br><br>  Algoritma RL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">merasa sulit untuk bekerja</a> di lingkungan di mana agen jarang menerima umpan balik.  Tapi ini tipikal dari dunia nyata.  Sebagai contoh, bayangkan mencari keju favorit Anda di labirin besar, seperti supermarket.  Anda mencari dan mencari departemen dengan keju, tetapi Anda tidak dapat menemukannya.  Jika pada setiap langkah Anda tidak mendapatkan "tongkat" atau "wortel", maka tidak mungkin untuk mengatakan apakah Anda bergerak ke arah yang benar.  Dengan tidak adanya hadiah, apa yang mencegah Anda berkeliaran selamanya?  Tidak ada yang lain kecuali mungkin keingintahuan Anda.  Itu memotivasi pindah ke departemen kelontong, yang tampak asing. <br><a name="habracut"></a><br>  Karya ilmiah, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Keingintahuan Episodik melalui Reachability,"</a> adalah hasil kolaborasi antara <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tim Google Brain</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DeepMind,</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Sekolah Tinggi Teknik Swiss Zurich</a> .  Kami menawarkan model hadiah RL berbasis memori episodik baru.  Dia terlihat seperti keingintahuan yang memungkinkan Anda menjelajahi lingkungan.  Karena agen tidak hanya harus mempelajari lingkungan, tetapi juga menyelesaikan masalah awal, model kami menambahkan bonus ke hadiah yang awalnya jarang.  Hadiah gabungan tidak lagi jarang, yang memungkinkan algoritma RL standar untuk belajar darinya.  Dengan demikian, metode keingintahuan kami memperluas berbagai tugas yang dapat diselesaikan menggunakan RL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">Rasa ingin tahu sesekali melalui keterjangkauan: data observasi ditambahkan ke memori, hadiah dihitung berdasarkan seberapa jauh pengamatan saat ini dari pengamatan serupa di memori.</font></i>  <i><font color="gray">Agen menerima hadiah yang lebih besar untuk pengamatan yang belum disajikan dalam memori.</font></i> <br><br>  Gagasan utama dari metode ini adalah untuk menyimpan pengamatan agen terhadap lingkungan dalam memori episodik, serta menghargai agen untuk melihat pengamatan yang belum disajikan dalam memori.  "Kurangnya memori" adalah definisi yang baru dalam metode kami.  Pencarian pengamatan seperti itu berarti pencarian orang asing.  Keinginan untuk mencari orang asing seperti itu akan membawa agen AI ke lokasi baru, sehingga mencegah berkeliaran dalam lingkaran, dan akhirnya membantunya tersandung pada target.  Seperti yang akan kita diskusikan nanti, kata-kata kita dapat menghalangi agen dari perilaku yang tidak diinginkan yang dikenakan oleh beberapa kata lain.  Yang mengejutkan kami, perilaku ini memiliki beberapa kesamaan dengan apa yang oleh orang awam disebut "penundaan." <br><br><h4>  Keingintahuan sebelumnya </h4><br>  Meskipun ada banyak upaya untuk merumuskan keingintahuan di masa lalu <sup>[1] [2] [3] [4]</sup> , dalam artikel ini kita akan fokus pada satu pendekatan alami dan sangat populer: keingintahuan melalui kejutan berdasarkan perkiraan.  Teknik ini dijelaskan dalam artikel baru-baru ini, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Investigasi Lingkungan yang Menggunakan Keingintahuan dengan Memprediksi Di Bawah Kendali Sendiri"</a> (biasanya disebut sebagai ICM).  Untuk menggambarkan hubungan antara kejutan dan rasa ingin tahu, kami kembali menggunakan analogi menemukan keju di supermarket. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Ilustrasi oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Indira Pasko</a> , dilisensikan di bawah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CC BY-NC-ND 4.0</a></font></i> <br><br>  Berkeliaran di sekitar toko, Anda mencoba untuk memprediksi masa depan ( <i>"Sekarang saya di departemen daging, jadi saya berpikir bahwa departemen di sudut adalah departemen ikan, mereka biasanya di dekatnya dalam rantai supermarket ini"</i> ).  Jika ramalannya salah, Anda akan terkejut ( <i>"Sebenarnya, ada departemen sayuran. Saya tidak mengharapkan ini!"</i> ) - dan dengan cara ini Anda mendapatkan hadiah.  Ini meningkatkan motivasi di masa depan untuk melihat ke sudut lagi, menjelajahi tempat-tempat baru hanya untuk memverifikasi bahwa harapan Anda benar (dan mungkin tersandung pada keju). <br><br>  Demikian pula, metode ICM membangun model prediksi dinamika dunia dan memberikan agen hadiah jika model gagal membuat prediksi yang baik - penanda kejutan atau hal baru.  Harap dicatat bahwa menjelajahi tempat-tempat baru tidak secara langsung diartikulasikan dalam keingintahuan ICM.  Untuk metode ICM, menghadiri mereka hanyalah cara untuk mendapatkan lebih banyak "kejutan" dan dengan demikian memaksimalkan hadiah keseluruhan Anda.  Ternyata, di beberapa lingkungan mungkin ada cara lain untuk mengejutkan diri sendiri, yang mengarah pada hasil yang tidak terduga. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">Seorang agen dengan sistem rasa ingin tahu berdasarkan kejutan membeku ketika bertemu dengan TV.</font></i>  <i><font color="gray">Animasi dari video <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deepak Patak</a> , dilisensikan di bawah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CC BY 2.0</a></font></i> <br><br><h4>  Bahaya penundaan </h4><br>  Dalam artikel tersebut, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Studi Skala Besar Pembelajaran Berbasis Curiosity,"</a> penulis metode ICM, bersama dengan para peneliti OpenAI, menunjukkan bahaya tersembunyi untuk memaksimalkan kejutan: agen dapat belajar untuk melakukan penundaan daripada melakukan sesuatu yang berguna untuk tugas tersebut.  Untuk memahami mengapa ini terjadi, pertimbangkan eksperimen pikiran yang oleh penulis disebut sebagai "masalah kebisingan televisi."  Di sini agen ditempatkan di labirin dengan tugas menemukan barang yang sangat berguna (seperti "keju" dalam contoh kita).  Lingkungan memiliki TV, dan agen memiliki remote control.  Ada sejumlah saluran yang terbatas (setiap saluran memiliki transmisi terpisah), dan setiap tekan pada remote control mengalihkan TV ke saluran acak.  Bagaimana agen bertindak dalam lingkungan seperti itu? <br><br>  Jika rasa ingin tahu terbentuk berdasarkan kejutan, maka perubahan saluran akan memberikan lebih banyak hadiah, karena setiap perubahan tidak dapat diprediksi dan tidak terduga.  Penting untuk dicatat bahwa bahkan setelah pemindaian siklus semua saluran yang tersedia, pemilihan acak saluran memastikan bahwa setiap perubahan baru akan tetap tidak terduga - agen membuat prediksi bahwa ia akan menampilkan TV setelah mengganti saluran, dan kemungkinan besar ramalan itu akan berubah menjadi tidak benar, yang akan menyebabkan kejutan.  Penting untuk dicatat bahwa meskipun agen sudah melihat setiap transmisi pada setiap saluran, perubahannya masih tidak dapat diprediksi.  Karena itu, agen, bukannya mencari barang yang sangat berguna, pada akhirnya akan tetap berada di depan TV - mirip dengan penundaan.  Bagaimana mengubah kata-kata ingin tahu untuk mencegah perilaku ini? <br><br><h4>  Keingintahuan episodik </h4><br>  Dalam artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Keingintahuan Episodik melalui Reachability",</a> kami memeriksa model rasa ingin tahu berbasis memori episodik yang kurang rentan terhadap kesenangan instan.  Kenapa begitu  Jika kita mengambil contoh di atas, maka setelah beberapa waktu berpindah saluran, semua transmisi pada akhirnya akan berakhir di memori.  Dengan demikian, TV akan kehilangan daya tariknya: bahkan jika urutan program muncul di layar adalah acak dan tidak dapat diprediksi, semuanya ada dalam memori!  Ini adalah perbedaan utama dari metode berdasarkan kejutan: metode kami bahkan tidak mencoba untuk memprediksi masa depan, sulit untuk diprediksi (atau bahkan tidak mungkin).  Sebaliknya, agen memeriksa masa lalu dan memeriksa untuk melihat apakah ada pengamatan dalam memori <i>seperti yang</i> sekarang.  Dengan demikian, agen kami tidak rentan terhadap kesenangan instan, yang memberikan "suara televisi".  Agen harus pergi dan menjelajahi dunia di luar TV untuk mendapatkan hadiah lebih banyak. <br><br>  Tetapi bagaimana kita memutuskan apakah agen melihat hal yang sama yang disimpan dalam memori?  Pemeriksaan kecocokan yang tepat tidak ada gunanya: dalam lingkungan nyata, agen jarang melihat hal yang sama dua kali.  Misalnya, bahkan jika agen kembali ke ruangan yang sama, dia masih akan melihat ruangan ini dari sudut yang berbeda. <br><br>  Alih-alih memeriksa kecocokan yang tepat, kami menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">jaringan saraf yang dalam</a> yang dilatih untuk mengukur seberapa mirip dua pengalaman itu.  Untuk melatih jaringan ini, kita harus menebak seberapa dekat pengamatan terjadi dalam waktu.  Kedekatan waktu adalah indikator yang baik apakah dua pengamatan harus dianggap bagian dari yang sama.  Pembelajaran semacam itu mengarah pada konsep umum kebaruan melalui jangkauan, yang diilustrasikan di bawah ini. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">Grafik reachability mendefinisikan kebaruan.</font></i>  <i><font color="gray">Dalam praktiknya, grafik ini tidak tersedia - oleh karena itu, kami melatih aproksimasi jaringan saraf untuk memperkirakan jumlah langkah di antara pengamatan</font></i> <br><br><h4>  Hasil percobaan </h4><br>  Untuk membandingkan kinerja berbagai pendekatan dalam menggambarkan keingintahuan, kami mengujinya dalam dua lingkungan 3D yang kaya visual: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ViZDoom</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DMLab</a> .  Dalam kondisi ini, agen diberi berbagai tugas, seperti menemukan target di labirin, mengumpulkan benda-benda baik dan menghindari yang buruk.  Di lingkungan DMLab, agen dilengkapi secara default dengan gadget fantastis seperti laser, tetapi jika gadget tidak diperlukan untuk tugas tertentu, agen tidak dapat menggunakannya secara bebas.  Menariknya, berdasarkan kejutan, agen ICM benar-benar menggunakan laser sangat sering, bahkan jika itu sia-sia untuk menyelesaikan tugas!  Seperti halnya TV, daripada mencari barang berharga di labirin, ia lebih suka menghabiskan waktu memotret di dinding, karena memberi banyak hadiah dalam bentuk kejutan.  Secara teoritis, hasil dari tembakan dinding harus dapat diprediksi, tetapi dalam praktiknya terlalu sulit untuk diprediksi.  Ini mungkin membutuhkan pengetahuan fisika yang lebih dalam daripada yang tersedia untuk agen AI standar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">Agen ICM yang terkejut terus-menerus menembak ke dinding bukannya menjelajahi labirin</font></i> <br><br>  Tidak seperti dia, agen kami telah menguasai perilaku yang masuk akal untuk mempelajari lingkungan.  Ini terjadi karena dia tidak mencoba untuk memprediksi hasil dari tindakannya, tetapi lebih mencari pengamatan yang "lebih jauh" dari yang ada dalam memori episodik.  Dengan kata lain, agen secara implisit mengejar tujuan yang membutuhkan lebih banyak upaya daripada tembakan sederhana di dinding. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Metode kami menunjukkan perilaku eksplorasi lingkungan yang cerdas.</font></i> <br><br>  Sangat menarik untuk mengamati bagaimana pendekatan kami untuk hadiah menghukum agen yang berjalan dalam lingkaran, karena setelah menyelesaikan lingkaran pertama, agen tidak menemukan pengamatan baru dan, dengan demikian, tidak menerima hadiah apa pun: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">Visualisasi hadiah: merah sesuai dengan hadiah negatif, Hijau ke positif.</font></i>  <i><font color="gray">Dari kiri ke kanan: kartu penghargaan, peta dengan lokasi dalam memori, tampilan orang pertama</font></i> <br><br>  Pada saat yang sama, metode kami berkontribusi pada studi lingkungan yang baik: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">Visualisasi hadiah: merah sesuai dengan hadiah negatif, Hijau ke positif.</font></i>  <i><font color="gray">Dari kiri ke kanan: kartu penghargaan, peta dengan lokasi dalam memori, tampilan orang pertama</font></i> <br><br>  Kami berharap bahwa pekerjaan kami berkontribusi pada gelombang penelitian baru yang melampaui lingkup teknik kejutan untuk mendidik agen tentang perilaku yang lebih cerdas.  Untuk analisis mendalam tentang metode kami, silakan lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pracetak karya ilmiah</a> . <br><br><h4>  Ucapan Terima Kasih: </h4><br>  Proyek ini adalah hasil kolaborasi antara tim Google Brain, DeepMind, dan Sekolah Tinggi Teknik Swiss di Zurich.  Kelompok penelitian utama: Nikolay Savinov, Anton Raichuk, Rafael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillirap dan Sylvain Zheli.  Kami ingin mengucapkan terima kasih kepada Olivier Pietkin, Carlos Riquelme, Charles Blundell dan Sergey Levine karena telah membahas dokumen ini.  Kami berterima kasih kepada Indira Pasco untuk bantuan dengan ilustrasinya. <br><br><h4>  Referensi literatur: </h4><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">“Studi lingkungan berdasarkan penghitungan dengan model kepadatan saraf”</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remy Munoz <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">“</a> Menghitung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">lingkungan belajar berbasis pembelajaran dalam dengan penguatan”</a> , Khaoran Tan, Rain Huthuft, Davis Foot, Adam Knock, Xi Chen, Yan Duan, John Schulman, Philip de Turk, Peter de Bel, Peter Abbel <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">“Belajar tanpa guru untuk menemukan sasaran untuk penelitian yang bermotivasi internal,”</a> Alexander Pere, Sebastien Forestier, Olivier Sigot, Pierre-Yves Udaye <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"VIME: Kecerdasan untuk Memaksimalkan Perubahan Informasi,"</a> Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id427847/">https://habr.com/ru/post/id427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id427837/index.html">Hari-hari pertama di tim pengembangan - seperti yang terjadi pada kami</a></li>
<li><a href="../id427839/index.html">Otorisasi pengguna di Django melalui GSSAPI dan pendelegasian hak pengguna ke server</a></li>
<li><a href="../id427841/index.html">Magic Leap Scam</a></li>
<li><a href="../id427843/index.html">Cara tidur benar dan salah</a></li>
<li><a href="../id427845/index.html">Cara memuat jutaan bintang di iPhone</a></li>
<li><a href="../id427849/index.html">Garis lurus dengan TM. v3.0</a></li>
<li><a href="../id427853/index.html">Refleksi pada TDD. Mengapa metodologi ini tidak dikenal secara luas</a></li>
<li><a href="../id427855/index.html">Mitos MOSDROID di FunCorp</a></li>
<li><a href="../id427857/index.html">Masalah pajak dan hukum untuk freelancer pemula</a></li>
<li><a href="../id427859/index.html">Mengapa keterampilan teknis untuk manajer proyek: jelaskan tentang kasus-kasus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>