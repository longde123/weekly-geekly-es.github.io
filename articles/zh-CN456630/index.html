<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏿‍🤝‍🧑🏼 👵🏻 👨‍🌾 引入Airflow在ivi中管理Spark作业：希望与拐杖 👨🏻‍💻 ☂️ 📮</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在生产中部署机器学习模型的任务总是很痛苦，因为要离开舒适的Jupyter笔记本进入监视和容错的世界是非常不舒服的。 

 我们已经写过关于重构在线电影院ivi推荐系统的第一次迭代的文章 。 在过去的一年中，我们几乎没有最终确定应用程序体系结构（从全局-仅从过时的python 2.7和python 3...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>引入Airflow在ivi中管理Spark作业：希望与拐杖</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/"> 在生产中部署机器学习模型的任务总是很痛苦，因为要离开舒适的Jupyter笔记本进入监视和容错的世界是非常不舒服的。 <br><br> 我们已经写过关于<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">重构</a>在线电影院ivi推荐系统的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">第一次迭代的文章</a> 。 在过去的一年中，我们几乎没有最终确定应用程序体系结构（从全局-仅从过时的python 2.7和python 3.4过渡到“新鲜的” python 3.6），但是我们添加了一些新的ML模型，并立即遇到了在生产中推出新算法的问题。 在本文中，我将介绍我们在实现诸如Apache Airflow之类的任务流管理工具方面的经验：为什么团队有此需求，不适合现有解决方案的东西，必须解决的关键问题以及随之而来的事情。 <br><br>  →可以在YouTube上观看报告的视频版本（从03:00:00开始）。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div><br><a name="habracut"></a><br><br><h2>  <font color="#fd004c">九头蛇队</font> </h2><br> 我将稍微介绍一下该项目：ivi是数以万计的内容单元，我们拥有RuNet中最大的法律目录之一。  ivi网络版本的主页是从目录中进行的个性化剪裁，旨在根据用户的反馈（视图，等级等）为用户提供最丰富，最相关的内容。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br> 推荐系统的在线部分是Flask后端应用程序，负载高达600 RPS。 在离线状态下，该模型每月接受超过2.5亿次内容观看的培训。 在Hive存储库顶部运行的Spark上实现了用于培训的数据准备管道。 <br><br> 该团队现在有7个开发人员，他们既要创建模型又要把它们推广到生产中-这是一个相当大的团队，需要方便的工具来管理任务流。 <br><br><h2>  <font color="#fd004c">离线架构</font> </h2><br> 在下面，您可以看到推荐系统的数据流基础结构图。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br> 这里描述了两个数据存储-Hive用于用户反馈（视图，评级）和Postgres用于各种业务信息（内容获利的类型等），而从Postgres到Hive的传输已进行了调整。 一包Spark应用程序从Hive吸取数据：并在此数据上训练我们的模型（用于个人推荐的ALS，各种内容相似性的协作模型）。 <br><br> 传统上，Spark应用程序是通过专用虚拟机进行管理的，我们使用一堆cron + shell脚本将其称为hydra-updater。 这个捆绑包是在ivi运营部门创建的，而且时间紧迫。  Shell脚本是启动Spark应用程序的单个入口点-也就是说，每个新模型只有在管理员完成此脚本后才开始在产品中运行。 <br><br> 模型训练的某些工件存储在HDFS中以进行永久存储（并等待有人从那里下载它们并传输到在线部分正在旋转的服务器中），而另一些则直接从Spark驱动程序写入Redis快速存储中，我们通常将其使用在线部分的数十个python进程的内存。 <br><br> 随着时间的流逝，这种架构累积了许多缺点： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div><br> 该图显示数据流具有相当复杂的结构-如果没有简单明了的工具来管理这种产品，开发和操作将变成恐怖，衰败和痛苦。 <br><br> 除了管理Spark应用程序外，管理脚本还执行许多有用的操作：在战斗中重新启动服务，Redis转储和其他系统功能。 显然，在长期的运行过程中，该脚本具有许多功能，因为我们的每个新模型都在其中生成了几十行。 在功能方面，该脚本开始显得过于繁琐，因此，作为推荐系统的团队，我们希望在某些地方删除与启动和管理Spark应用程序有关的功能。 为此，我们决定使用Airflow。 <br><br><h2>  <font color="#fd004c">拐杖</font> </h2><br> 当然，除了解决所有这些问题外，在我们为自己创建新问题的方式上，部署Airflow来启动和监视Spark应用程序也很困难。 <br><br> 主要的困难是没有人会为我们重塑整个基础架构，因为 开发资源是一件稀缺的事情。 因此，我们不仅必须实施Airflow，还必须将其集成到现有系统中，这从头开始很难看。 <br><br> 我想谈谈我们在实施过程中遇到的痛苦，以及为了获得Airflow而必须付出的艰辛。 <br><br>  <b>第一个也是主要的难题</b> ：如何将Airflow集成到运营部门的大型Shell脚本中。 <br><br> 在这里，解决方案是最明显的-我们开始使用气流二进制文件和trigger_dag键直接从Shell脚本触发图形。 通过这种方法，我们不使用Airflow sheduler，实际上，Spark应用程序是用相同的表冠启动的-从宗教上讲这不是很正确。 但是我们与现有解决方案实现了无缝集成。 这是从我们主要的Spark应用程序的shell脚本开始的样子，该脚本以前称为hydramatrices。 <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$FUNCNAME</span></span></span><span class="hljs-string"> started"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> RETVAL=0 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg AIRFLOW_API=api/dag_last_run/hydramatrices/all <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span></span> /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable"><span class="hljs-variable">$LOGFILE</span></span></code> </pre> <br>  <b>痛苦：</b>运营部门<b>的</b>外壳脚本必须以某种方式确定气流图的状态，以便控制其自身的执行流程。 <br><br> 拐杖：我们在外壳程序脚本中使用了DAG监视端点来扩展了Airflow REST API。 现在，每个图形都具有三种状态：运行，成功，失败。 <br><br> 实际上，在Airflow中开始计算之后，我们只需定期轮询运行中的图形：我们将GET请求设为项目符号，以确定DAG是否已完成。 当监视端点对图的成功执行作出响应时，shell脚本将继续执行其流程。 <br> 我想说的是，Airflow REST API只是一个火热的事情，它允许您灵活地配置管道-例如，您可以将POST参数转发到图形。 <br><br>  Airflow API扩展仅仅是一个Python类，看起来像这样： <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> settings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DagBag, DagRun <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Blueprint, request, Response airflow_api_blueprint = Blueprint(<span class="hljs-string"><span class="hljs-string">'airflow_api'</span></span>, __name__, url_prefix=<span class="hljs-string"><span class="hljs-string">'/api'</span></span>) AIRFLOW_DAGS = <span class="hljs-string"><span class="hljs-string">'{}/dags'</span></span>.format( os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ApiResponse</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    GET """</span></span> STATUS_OK = <span class="hljs-number"><span class="hljs-number">200</span></span> STATUS_NOT_FOUND = <span class="hljs-number"><span class="hljs-number">404</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">standard_response</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(status: int, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> json_data = json.dumps(payload) resp = Response(json_data, status=status, mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> resp <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">success</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(self.STATUS_OK, payload) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, status: int, message: str)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(status, {<span class="hljs-string"><span class="hljs-string">'error'</span></span>: message}) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">not_found</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, message: str = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Resource not found'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.error(self.STATUS_NOT_FOUND, message)</code> </pre><br> 我们在外壳程序脚本中使用API​​-我们每10分钟轮询一次端点： <br><br><pre> <code class="bash hljs"> TRIGGER=$? [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TRIGGER</span></span></span><span class="hljs-string">"</span></span> -eq <span class="hljs-string"><span class="hljs-string">"0"</span></span> ] &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG succeeded"</span></span> || { <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG failed"</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> 1; } CMD=<span class="hljs-string"><span class="hljs-string">"curl -s http://</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HYDRA_SERVER</span></span></span><span class="hljs-string">/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$AIRFLOW_API</span></span></span><span class="hljs-string"> | jq .dag_last_run.state"</span></span> STATE=$(<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> <span class="hljs-variable"><span class="hljs-variable">$CMD</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> [ <span class="hljs-variable"><span class="hljs-variable">$STATE</span></span> == \<span class="hljs-string"><span class="hljs-string">"running\" ]; do log "</span></span>Generating matrices <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> progress...<span class="hljs-string"><span class="hljs-string">" sleep 600 STATE=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(eval $CMD)</span></span></span><span class="hljs-string"> done [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$STATE</span></span></span><span class="hljs-string"> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1 [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span><span class="hljs-string"> -eq 0 ] &amp;&amp; log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> succeeded<span class="hljs-string"><span class="hljs-string">" || log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> failed<span class="hljs-string"><span class="hljs-string">" return </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span></span></code> </pre><br>  <b>痛苦</b> ：如果您曾经在集群模式下使用spark-submit运行Spark作业，那么您知道STDOUT中的日志是一个没有信息的工作表，其行为“ SPARK APPLICATION_ID IS RUNNING”。 例如，可以使用yarn yarn命令查看Spark应用程序本身的日志。 在shell脚本中，此问题得以简单解决：向其中一台群集计算机打开了SSH隧道，并在客户端模式下为此计算机执行了spark-submit。 在这种情况下，STDOUT将具有可读且可理解的日志。 在Airflow中，我们决定始终使用cluster-decide，而这样的数字将不起作用。 <br><br> 拐杖：提交火花后，我们通过application_id从HDFS中获取驱动程序日志，并通过Python print（）运算符将其显示在Airflow界面中。 唯一的缺点-在Airflow界面中，日志仅在执行火花提交后才显示，您必须在其他地方监视实时-例如，YARN网络枪口。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_logs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(config: BaseConfig, app_id: str)</span></span></span><span class="hljs-function"> -&gt; </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">None</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   :param config: :param app_id: """</span></span> hdfs = HDFSInteractor(config) logs_path = <span class="hljs-string"><span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span></span>.format(username=config.CURRENT_USERNAME, app_id=app_id) logs_files = hdfs.files_in_folder(logs_path) logs_files = [file <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file[<span class="hljs-number"><span class="hljs-number">-4</span></span>:] != <span class="hljs-string"><span class="hljs-string">'.tmp'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, delimiter=<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> reader: print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> reader: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stdout'</span></span>, line) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(line) &gt; <span class="hljs-number"><span class="hljs-number">30</span></span>: print_line = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stderr'</span></span>, line): print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> print_line: print(line)</code> </pre><br>  <b>痛苦</b> ：对于测试人员和开发人员来说，拥有一个Airflow测试平台会很好，但是我们节省了devops资源，因此很长一段时间以来，我们一直在考虑如何部署测试环境。 <br><br> 拐杖：我们将Airflow打包在一个Docker容器中，而Dockerfile将它放置在带有Spark作业的存储库中。 因此，每个开发人员或测试人员都可以在本地计算机上提高自己的气流。 由于应用程序以集群模式运行，因此几乎不需要docker的本地资源。 <br><br>  spark的本地安装通过环境变量隐藏在docker容器及其整个配置中-您不再需要花费几个小时来设置环境。 下面我给出了一个示例，其中包含带有Airflow的容器的docker文件片段，您可以在其中看到如何使用环境变量配置Airflow： <br><br><pre> <code class="bash hljs">FROM ubuntu:16.04 ARG AIRFLOW_VERSION=1.9.0 ARG AIRFLOW_HOME ARG USERNAME=airflow ARG USER_ID ARG GROUP_ID ARG LOCALHOST ARG AIRFLOW_PORT ARG PIPENV_PATH ARG PROJECT_HYDRAMATRICES_DOCKER_PATH RUN apt-get update \ &amp;&amp; apt-get install -y \ python3.6 \ python3.6-dev \ &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \ &amp;&amp; apt-get -y install python3-pip RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg RUN pip3 install pandas==0.20.3 \ apache-airflow==<span class="hljs-variable"><span class="hljs-variable">$AIRFLOW_VERSION</span></span> \ psycopg2==2.7.5 \ ldap3==2.5.1 \ cryptography <span class="hljs-comment"><span class="hljs-comment">#   ,       ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=${PROJECT_HYDRAMATRICES_DOCKER_PATH} ENV PIPENV_PATH=${PIPENV_PATH} ENV SPARK_HOME=/usr/lib/spark2 ENV HADOOP_CONF_DIR=$PROJECT_HYDRAMATRICES_DOCKER_PATH/etc/hadoop-conf-preprod ENV PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib ENV PIP_NO_BINARY=numpy ENV AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW_DAGS=${AIRFLOW_HOME}/dags ENV AIRFLOW_LOGS=${AIRFLOW_HOME}/logs ENV AIRFLOW_PLUGINS=${AIRFLOW_HOME}/plugins #      Airflow (log url) BASE_URL="http://${AIRFLOW_CURRENT_HOST}:${AIRFLOW_PORT}" ; #   Airflow ENV AIRFLOW__WEBSERVER__BASE_URL=${BASE_URL} ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=${BASE_URL} ENV AIRFLOW__CORE__AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_DAGS} ENV AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_LOGS} ENV AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_PLUGINS} ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_LOGS}/scheduler</span></span></code> </pre><br> 通过实施Airflow，我们获得了以下结果： <br><br><ul><li> 缩短了发布周期：推出新模型（或数据准备管道）现在可以编写新的Airflow图，这些图本身存储在存储库中并与代码一起部署。 此过程完全在开发人员手中。 管理员很高兴，我们不再为琐事而烦恼。 </li><li> 曾经直奔地狱的Spark应用程序日志现在通过方便的访问界面存储在Aiflow中。 您可以查看任何一天的日志，而无需选择HDFS目录。 </li><li> 可以通过界面中的一个按钮重新启动失败的计算，这非常方便，即使June也可以处理它。 </li><li> 您可以从界面中执行Spark作业，而不必在本地计算机上运行Spark设置。 测试人员很高兴-已在Dockerfile中进行了火花提交正常工作的所有设置 </li><li>  Aiflow标准包子-计划，重新启动失败的作业，精美的图表（例如，应用程序执行时间，启动成功与失败的统计信息）。 </li></ul><br> 接下来要去哪里？ 现在，我们拥有大量的数据源和接收器，并且其数量将不断增长。 任何水文计量库类中的更改都可能在另一个管道（甚至在在线部分）中崩溃： <br><br><ul><li>  Clickhouse溢出→Hive </li><li> 数据预处理：Hive→Hive </li><li> 部署c2c模型：Hive→Redis </li><li> 准备目录（例如内容货币化的类型）：Postgres→Redis </li><li> 模型准备：本地FS→HDFS </li></ul><br> 在这种情况下，我们确实需要一个在数据准备中自动测试管道的支架。 这将大大降低测试存储库中更改的成本，加快在生产中推出新模型的速度，并显着提高测试人员中内啡肽的水平。 但是如果没有气流，就不可能部署这种自动测试的支架！ <br><br> 我写这篇文章是为了介绍我们在实施Airflow方面的经验，这对于处于类似情况的其他团队可能很有用-您已经拥有庞大的工作系统，并且想要尝试一些新颖，时尚和年轻的事物。 无需担心会对工作系统进行任何更新，您需要尝试一下-这种实验通常为进一步的开发打开了新的视野。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN456630/">https://habr.com/ru/post/zh-CN456630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN456614/index.html">瘟疫的故事：无罪的帧渲染</a></li>
<li><a href="../zh-CN456616/index.html">可以编码的人三百万卢布</a></li>
<li><a href="../zh-CN456618/index.html">莫斯科拉腊贝尔-6月21日</a></li>
<li><a href="../zh-CN456622/index.html">如何创建根据I类保护认证的OS</a></li>
<li><a href="../zh-CN456624/index.html">有用的Python工具</a></li>
<li><a href="../zh-CN456632/index.html">我们正在RESTinio中构建C ++模板的第四层。 为什么以及如何？</a></li>
<li><a href="../zh-CN456634/index.html">Nginx食谱：CAS（中央授权服务）</a></li>
<li><a href="../zh-CN456638/index.html">在Rust，Haskell，C ++，Python，Scala和OCaml中比较同一项目</a></li>
<li><a href="../zh-CN456640/index.html">PHDays 9的竞争情报竞赛分析</a></li>
<li><a href="../zh-CN456642/index.html">JetBrains企业硕士课程和ITMO大学的首届毕业</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>