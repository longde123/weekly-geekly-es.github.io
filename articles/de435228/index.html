<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äç‚úàÔ∏è ‚ùå ü§¥üèø Kubernetes Cluster f√ºr 20 US-Dollar pro Monat üìú üë©üèº‚Äçüéì üå§Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 Wir erh√∂hen den Cluster, um zustandslose Webanwendungen mit Ingress , Letsencrypt, zu bedienen, ohne Automatisierungstools wie Kubespray, Ku...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes Cluster f√ºr 20 US-Dollar pro Monat</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  Wir erh√∂hen den Cluster, um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zustandslose</a> Webanwendungen mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ingress</a> , Letsencrypt, zu bedienen, ohne Automatisierungstools wie Kubespray, Kubeadm und andere zu verwenden. <br>  Lesezeit: ~ 45-60 Minuten, Wiedergabezeit: ab 3 Stunden. </p><br><h1 id="preambula">  Pr√§ambel </h1><br><p>  Ich wurde aufgefordert, einen Artikel zu schreiben, da ich einen eigenen Cluster von Kubernetes zum Experimentieren ben√∂tigte.  Die automatisierten Open Source-Installations- und Konfigurationsl√∂sungen funktionierten in meinem Fall nicht, da ich Nicht-Mainstream-Linux-Distributionen verwendete.  Die intensive Arbeit mit Kubernetes in IPONWEB ermutigt Sie zu einer solchen Plattform, die Ihre Aufgaben auf komfortable Weise l√∂st, auch f√ºr Heimprojekte. </p><br><h1 id="komponenty">  Komponenten </h1><br><p>  Die folgenden Komponenten werden im Artikel angezeigt: </p><br><p>  - <em>Ihr Lieblings-</em> Linux - Ich habe Gentoo (Knoten-1: systemd / Knoten-2: openrc), Ubuntu 18.04.1 verwendet. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Server</a> - Kube-Apiserver, Kube-Controller-Manager, Kube-Scheduler, Kubelet, Kube-Proxy. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Containerd</a> + <a href="">CNI-Plugins (0.7.4)</a> - <a href="">Um</a> die Containerisierung zu organisieren, verwenden wir Containerd + CNI anstelle von Docker (obwohl urspr√ºnglich die gesamte Konfiguration auf Docker hochgeladen wurde, sodass nichts die Verwendung bei Bedarf verhindert). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CoreDNS</a> - zum Organisieren der Serviceerkennung von Komponenten, die im kubernetes-Cluster arbeiten.  Eine Version, die nicht niedriger als 1.2.5 ist, wird empfohlen, da mit dieser Version eine angemessene Unterst√ºtzung f√ºr Coredns besteht, die als Prozess au√üerhalb des Clusters ausgef√ºhrt werden. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flanell</a> - zum Organisieren eines Netzwerkstapels, zum Kommunizieren von Herden und Containern untereinander. <br>  - <em>Deine Lieblings-</em> Datenbank. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="F√ºr alle"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Einschr√§nkungen und Annahmen </h1><br><ul><li>  Der Artikel ber√ºcksichtigt nicht die Kosten f√ºr vps / vds-L√∂sungen auf dem Markt sowie die M√∂glichkeit, Maschinen f√ºr diese Dienste bereitzustellen.  Es wird davon ausgegangen, dass Sie bereits etwas erweitert haben oder es selbst tun k√∂nnen.  Au√üerdem wird die Installation / Konfiguration Ihrer bevorzugten Datenbank und Ihres privaten Docker-Repositorys, falls erforderlich, nicht behandelt. </li><li>  Wir k√∂nnen sowohl Containerd + CNI-Plugins als auch Docker verwenden.  In diesem Artikel wird die Verwendung von Docker als Containerisierungstool nicht ber√ºcksichtigt.  Wenn Sie Docker verwenden m√∂chten, k√∂nnen Sie <a href="">Flanell selbst entsprechend</a> konfigurieren. Au√üerdem m√ºssen Sie Kubelet konfigurieren, dh alle Optionen in Bezug auf Containerd entfernen.  Wie meine Experimente gezeigt haben, funktionieren Docker und Containerd auf verschiedenen Knoten als Container korrekt. </li><li> Wir k√∂nnen das <code>host-gw</code> Backend nicht f√ºr Flanell verwenden. Weitere Informationen finden Sie im Abschnitt Flanellkonfiguration </li><li>  Wir werden nichts zum √úberwachen, Sichern, Speichern von Benutzerdateien (Status), Speichern von Konfigurationsdateien und Anwendungscode (git / hg / svn / etc) verwenden. </li></ul><br><h1 id="vvedenie">  Einf√ºhrung </h1><br><p>  Im Laufe der Arbeit habe ich eine gro√üe Anzahl von Quellen verwendet, aber ich m√∂chte separat einen ziemlich detaillierten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-</a> Leitfaden f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den harten Weg</a> erw√§hnen, der ungef√§hr 90% der Grundkonfiguration seines eigenen Clusters abdeckt.  Wenn Sie dieses Handbuch bereits gelesen haben, k√∂nnen Sie sicher direkt zum Abschnitt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flanellkonfiguration √ºbergehen</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Bezeichnungen</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Liste der Begriffe / Glossar </h2><br><ul><li>  api-server - eine physische oder virtuelle Maschine, auf der sich eine Reihe von Anwendungen zum Ausf√ºhren und zur korrekten Funktion von kubernetes kube-apiserver befindet.  F√ºr die Zwecke dieses Artikels ist es etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - eine dedizierte Workstation- oder VPS-Installation, ein Synonym f√ºr API-Server. </li><li>  Node-X - Eine dedizierte Workstation oder VPS-Installation. <code>X</code> gibt die Seriennummer der Station an.  In diesem Artikel sind alle Zahlen eindeutig und der Schl√ºssel zum Verst√§ndnis: <br><ul><li>  Knoten-1 - Maschinennummer 1 </li><li>  Knoten 2 - Maschinennummer 2 </li></ul></li><li>  vCPU - virtuelle CPU, Prozessorkern.  Die Anzahl entspricht der Anzahl der Kerne: 1vCPU - ein Kern, 2vCPU - zwei usw. </li><li>  Benutzer - Benutzer oder Benutzerbereich.  Bei Verwendung von <code>user$</code> in Befehlszeilenanweisungen bezieht sich der Begriff auf einen beliebigen Clientcomputer. </li><li>  Arbeiter - der Arbeitsknoten, an dem direkte Berechnungen durchgef√ºhrt werden, synonym mit <code>node-X</code> </li><li>  Ressource ist die Entit√§t, mit der der Kubernetes-Cluster arbeitet.  Zu den Ressourcen von Kubernetes geh√∂rt eine gro√üe Anzahl <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verwandter Entit√§ten</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  L√∂sungen f√ºr die Netzwerkarchitektur </h1><br><p>  Bei der Erh√∂hung des Clusters habe ich mir nicht die Aufgabe gestellt, die Eisenressourcen so zu optimieren, dass sie in das Budget von 20 USD pro Monat passen.  Es war lediglich erforderlich, einen Arbeitscluster mit mindestens zwei Arbeitsknoten (Knoten) zusammenzustellen.  Daher sah der Cluster anfangs folgenderma√üen aus: </p><br><ul><li>  Maschine mit 2 vCPU / 4G RAM: API-Server + Node-1 [20 $] </li><li>  Maschine mit 2 vCPU / 4G RAM: Knoten-2 [$ 20] </li></ul><br><p>  Nachdem die erste Version des Clusters funktioniert hatte, entschied ich mich, sie neu zu erstellen, um zwischen den Knoten, die f√ºr die Ausf√ºhrung von Anwendungen innerhalb des Clusters verantwortlich sind (Arbeitsknoten, sie sind auch Worker), und der API des Master-Servers zu unterscheiden. </p><br><p>  Als Ergebnis erhielt ich die Antwort auf die Frage: "Wie bekomme ich einen mehr oder weniger kosteng√ºnstigen, aber funktionierenden Cluster, wenn ich dort nicht die dicksten Anwendungen platzieren m√∂chte?" </p><br><div class="spoiler">  <b class="spoiler_title">20 $ Entscheidung</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Design"><br>  (Geplant, so zu sein) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Allgemeine Informationen zur Kubernetes-Architektur</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Design"><br>  (Aus dem Internet gestohlen, wenn jemand pl√∂tzlich noch nichts wei√ü oder nicht gesehen hat) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Komponenten und ihre Leistung </h2><br><p>  Der erste Schritt bestand darin, zu verstehen, wie viele Ressourcen ich ben√∂tige, um Softwarepakete auszuf√ºhren, die in direktem Zusammenhang mit dem Cluster stehen.  Die Suche nach "Hardwareanforderungen" ergab keine spezifischen Ergebnisse, so dass ich mich der Aufgabe aus praktischer Sicht n√§hern musste.  Als Messung von MEM und CPU habe ich Statistiken von systemd genommen - wir k√∂nnen davon ausgehen, dass die Messungen sehr amateurhaft durchgef√ºhrt wurden, aber ich hatte nicht die Aufgabe, genaue Werte zu erhalten, da ich immer noch keine billigeren Optionen als 5 USD pro Instanz finden konnte. </p><br><div class="spoiler">  <b class="spoiler_title">Warum genau 5 Dollar?</b> <div class="spoiler_text"><p>  Es war m√∂glich, VPS / VDS billiger zu finden, wenn Server in Russland oder der GUS gehostet wurden, aber die traurigen Geschichten, die mit ILV und seinen Aktionen verbunden sind, bergen bestimmte Risiken und lassen den nat√ºrlichen Wunsch aufkommen, sie zu vermeiden. </p></div></div><br><p>  Also: </p><br><ul><li>  Master Server / Server-Konfiguration (Master Nodes): <br><ul><li>  etcd (3.2.17): 80 - 100M, Metriken wurden zu zuf√§llig ausgew√§hlten Zeiten genommen.  Der durchschnittliche Speicherverbrauch von Etcd √ºberschritt 300 MB nicht. </li><li>  Kube-Apiserver (1.12.x - 1.13.0): 237,6 M ~ 300 M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): ungef√§hr 90M, stieg nicht √ºber 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): ca. 20M, Verbrauch √ºber 30-50M ist nicht festgelegt. </li></ul></li><li>  Worker Server-Konfiguration (Worker Nodes): <br><ul><li>  Kubelet (1.12.3 - 1.13.1): ca. 35 MB, Verbrauch √ºber 50 MB ist nicht festgelegt; </li><li>  kube-proxy (1.12.3 - 1.13.1): ungef√§hr 7,5 - 10 M; </li><li>  Flanell (0,10,0): ungef√§hr 15-20 M; </li><li>  Kerne (1.3.0): ungef√§hr 25 M; </li><li>  Containerd (1.2.1): Der Containerd-Verbrauch ist gering, aber Statistiken zeigen auch Containerprozesse, die vom Daemon gestartet wurden. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">Wird Containerd / Docker auf Masterknoten ben√∂tigt?</b> <div class="spoiler_text"><p>  <strong>Nein, nicht ben√∂tigt</strong> .  Der Masterknoten ben√∂tigt kein Docker oder Containerd an sich, obwohl es im Internet eine gro√üe Anzahl von Handb√ºchern gibt, die f√ºr den einen oder anderen Zweck die Verwendung der Umgebung f√ºr die Containerisierung umfassen.  In der fraglichen Konfiguration wurde Containerd absichtlich aus der Liste der Abh√§ngigkeiten deaktiviert. Ich m√∂chte jedoch keine offensichtlichen Vorteile dieses Ansatzes hervorheben. </p><br><p>  Die oben angegebene Konfiguration ist minimal und ausreichend, um den Cluster zu starten.  Es ist keine zus√§tzliche Aktion / Komponente erforderlich, es sei denn, Sie m√∂chten etwas hinzuf√ºgen, wie Sie m√∂chten. </p></div></div><br><p>  Um einen Testcluster oder Cluster f√ºr Heimprojekte zu erstellen, reicht 1vCPU / 1G RAM aus, damit der Masterknoten funktioniert.  Die Belastung des Masterknotens h√§ngt nat√ºrlich von der Anzahl der beteiligten Mitarbeiter sowie von der Verf√ºgbarkeit und dem Volumen der Anforderungen von Drittanbietern an den API-Server ab. </p><br><p>  Ich habe die Master- und Worker-Konfigurationen wie folgt aufgel√∂st: </p><br><ul><li>  1x Master mit installierten Komponenten: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Arbeiter mit installierten Komponenten: Containerd, Coredns, Flanell, Kubelet, Kube-Proxy </li></ul><br><h1 id="konfiguraciya">  Konfiguration </h1><br><p>  Zum Konfigurieren des Assistenten sind die folgenden Komponenten erforderlich: </p><br><ul><li><p>  etcd - zum Speichern von Daten f√ºr API-Server sowie f√ºr Flanell; </p><br></li><li><p>  kube-apiserver - eigentlich api-server; </p><br></li><li><p>  kube-controller-manager - zum Generieren und Verarbeiten von Ereignissen; </p><br></li><li><p>  kube-scheduler - zur verteilung von ressourcen, die √ºber den api-server registriert sind - zum beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herd</a> . <br>  F√ºr die Konfiguration von Arbeitspferden sind folgende Komponenten erforderlich: </p><br></li><li><p>  kubelet - um die Herde zu betreiben, um die Netzwerkeinstellungen zu konfigurieren; </p><br></li><li><p>  kube-proxy - zum Organisieren des Routings / Ausgleichs von kubernetes- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diensten</a> ; </p><br></li><li><p>  coredns - zur Serviceerkennung in laufenden Containern; </p><br></li><li><p>  Flanell - zur Organisation des Netzwerkzugriffs von Containern, die auf verschiedenen Knoten ausgef√ºhrt werden, sowie zur dynamischen Verteilung von Netzwerken zwischen Clusterknoten (Kubernetes-Knoten). </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Hier sollte ein kleiner Exkurs gemacht werden: Coredns k√∂nnen auch auf dem Master-Server gestartet werden.  Es gibt keine Einschr√§nkungen, die die Ausf√ºhrung von Coredns auf Arbeitsknoten erzwingen w√ºrden, mit Ausnahme der Konfigurationsnuance coredns.service, die aufgrund eines Konflikts mit dem vom System aufgel√∂sten Dienst auf einem standardm√§√üigen / nicht modifizierten Ubuntu-Server einfach nicht gestartet wird.  Ich habe nicht versucht, dieses Problem zu l√∂sen, da 2-ns-Server auf den Arbeitsknoten sehr zufrieden mit mir waren. </p></div></div><br><p>  Um keine Zeit damit zu verschwenden, sich jetzt mit allen Details des Komponentenkonfigurationsprozesses vertraut zu machen, empfehle ich Ihnen, sich in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-</a> Anleitung auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die harte Tour</a> mit ihnen vertraut zu machen.  Ich werde mich auf die Unterscheidungsmerkmale meiner Konfigurationsoption konzentrieren. </p><br><h2 id="fayly">  Dateien </h2><br><p>  Alle Dateien f√ºr die Funktionsweise der Clusterkomponenten f√ºr den Assistenten und die Arbeitsknoten werden der <strong>Einfachheit halber</strong> in <strong>/ var / lib / kubernetes / abgelegt</strong> .  Bei Bedarf k√∂nnen Sie sie auch anders platzieren. </p><br><h2 id="sertifikaty">  Zertifizierungen </h2><br><p>  Die Basis f√ºr die Erstellung von Zertifikaten ist immer noch die gleiche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes auf die harte Tour</a> , es gibt praktisch keine signifikanten Unterschiede.  Um untergeordnete Zertifikate neu zu <a href="">generieren</a> , wurden einfache Bash-Skripte um <a href="">cfssl-</a> Anwendungen geschrieben - dies war beim Debugging-Prozess sehr n√ºtzlich. </p><br><p>  Mit den folgenden Skripten, Rezepten von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes auf die harte Tour</a> oder anderen geeigneten Tools k√∂nnen Sie Zertifikate f√ºr Ihre Anforderungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erstellen</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Zertifikatserstellung mit Bash-Skripten</b> <div class="spoiler_text"><p>  Sie k√∂nnen Skripte hier erhalten: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes bootstrap</a> .  Bearbeiten Sie vor dem Start die Datei <a href="">certs / env.sh</a> unter Angabe Ihrer Einstellungen.  Ein Beispiel: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Wenn Sie <code>env.sh</code> und alle Parameter korrekt angegeben haben, m√ºssen Sie die generierten Zertifikate nicht ber√ºhren.  Wenn Sie irgendwann einen Fehler gemacht haben, k√∂nnen die Zertifikate in Teilen neu generiert werden.  Die oben genannten Bash-Skripte sind trivial, das Aussortieren ist nicht schwierig. </p><br><p>  Ein wichtiger Hinweis: Sie sollten die <code>ca.pem</code> und <code>ca-key.pem</code> nicht h√§ufig neu erstellen, da sie die <code>ca.pem</code> f√ºr alle nachfolgenden Zertifikate sind. Mit anderen Worten, Sie m√ºssen alle zugeh√∂rigen Zertifikate neu erstellen und an alle Computer und an alle erforderlichen Verzeichnisse <code>ca-key.pem</code> . </p></div></div><br><h3 id="master">  Der Meister </h3><br><p>  Die erforderlichen Zertifikate zum Starten von Diensten auf dem <code>/var/lib/kubernetes/</code> sollten in <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - Dieses Zertifikat wird √ºberall verwendet. Es kann nur einmal generiert und dann ohne √Ñnderungen verwendet werden. Seien Sie also vorsichtig.  Wenn Sie es neu generieren, m√ºssen Sie es auf alle Knoten kopieren und die kubeconfig-Dateien damit aktualisieren (auch auf allen Computern). </li><li>  ca-key.pem entspricht dem Kopieren √ºber Knoten. </li><li>  kube-controller-manager.pem - wird nur f√ºr kube-controller-manager ben√∂tigt. </li><li>  kube-controller-manager-key.pem - wird nur f√ºr kube-controller-manager ben√∂tigt. </li><li><p>  kubernetes.pem - erforderlich f√ºr Flanell, Coredns beim Verbinden mit etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Theoretischer R√ºckzug</b> <div class="spoiler_text"><p>  Diese Funktion basiert auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes-Hard-Way-</a> Konfigurationslogik. <br>  Auf dieser Grundlage wird diese Datei √ºberall ben√∂tigt - sowohl auf dem Assistenten als auch auf den Arbeitsknoten.  Ich habe den Ansatz des urspr√ºnglichen Handbuchs nicht ge√§ndert, da es mit seiner Hilfe m√∂glich ist, den Cluster-Betrieb schneller und klarer zu organisieren und die gesamte Reihe von Abh√§ngigkeiten zu verstehen. </p><br><p>  Meine pers√∂nliche Meinung ist, dass Sie f√ºr etcd separate Zertifikate ben√∂tigen, die sich nicht mit den f√ºr kubernetes verwendeten Zertifikaten √ºberschneiden. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - bleibt auf Master-Servern. </li><li>  service-account.pem - wird nur f√ºr Kube-Controller-Manager-Daemons ben√∂tigt. </li><li>  service-account-key.pem - √§hnlich. </li></ul><br><h3 id="rabochie-uzly">  Arbeitseinheiten </h3><br><ul><li>  ca.pem - wird f√ºr alle Dienste ben√∂tigt, die an Arbeitsknoten (kubelet, kube-proxy) beteiligt sind, sowie f√ºr Flanell-Coredns.  Der Inhalt ist unter anderem in kubeconfig-Dateien enthalten, wenn sie mit kubectl generiert werden. </li><li>  kubernetes-key.pem - wird nur ben√∂tigt, damit Flanell und Coredns eine Verbindung zu etcd herstellen, das sich auf dem API-Masterknoten befindet. </li><li>  kubernetes.pem - √§hnlich dem vorherigen, wird nur f√ºr Flanell und Coredns ben√∂tigt. </li><li>  kubelet / node-1.pem - Schl√ºssel f√ºr den Autorisierungsknoten-1. </li><li>  kubelet / node-1-key.pem - Schl√ºssel f√ºr den Autorisierungsknoten-1. </li></ul><br><p>  <strong>Wichtig!</strong>  Wenn Sie mehr als einen Knoten haben, enth√§lt jeder Knoten die Dateien <code>node-X-key.pem</code> , <code>node-X.pem</code> und <code>node-X.kubeconfig</code> in kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Zertifikat-Debugging</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Zertifikat-Debugging </h4><br><p>  Manchmal m√ºssen Sie sich m√∂glicherweise ansehen, wie das Zertifikat konfiguriert ist, um herauszufinden, welche IP / DNS-Hosts zum Generieren verwendet wurden.  Der <code>cfssl-certinfo -cert &lt;cert&gt;</code> hilft uns dabei.  Zum Beispiel lernen wir diese Informationen f√ºr <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Alle anderen Zertifikate f√ºr kubelet und kube-proxy sind direkt in die entsprechende kubeconfig eingebettet. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Alle notwendigen kubeconfig k√∂nnen mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes auf die harte</a> Tour gemacht werden, hier beginnen jedoch einige Unterschiede.  Das Handbuch verwendet <code>kubedns</code> und <code>cni bridge</code> und <code>cni bridge</code> auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Coredns</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flanell</a> .  Diese beiden Dienste verwenden wiederum <code>kubeconfig</code> , <code>kubeconfig</code> sich beim Cluster <code>kubeconfig</code> . </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  Der Meister </h3><br><p>  F√ºr den Assistenten werden die folgenden kubeconfig-Dateien ben√∂tigt (wie oben erw√§hnt, k√∂nnen sie nach der Generierung in <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  Diese Dateien werden ben√∂tigt, um jede der Servicekomponenten auszuf√ºhren. </p><br><h3 id="rabochie-uzly-1">  Arbeitseinheiten </h3><br><p>  F√ºr Arbeitsknoten sind die folgenden kubeconfig-Dateien erforderlich: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¬¶  L-- coredns.kubeconfig +-- flanneld ¬¶  L-- flanneld.kubeconfig +-- kubelet ¬¶  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Servicestart </h2><br><div class="spoiler">  <b class="spoiler_title">Dienstleistungen</b> <div class="spoiler_text"><p>  Trotz der Tatsache, dass meine Arbeitsknoten unterschiedliche Initialisierungssysteme verwenden, bieten die Beispiele und das Repository Optionen mit systemd.  Mit ihrer Hilfe ist es am einfachsten zu verstehen, welchen Prozess und mit welchen Parametern Sie starten m√ºssen. Au√üerdem sollten sie beim Studium von Diensten mit Zielflags keine gro√üen Probleme verursachen. </p></div></div><br><p>  Um Dienste zu starten, m√ºssen Sie <code>service-name.service</code> in <code>/lib/systemd/system/</code> oder in ein anderes Verzeichnis <code>service-name.service</code> in dem sich die Dienste f√ºr systemd befinden. Anschlie√üend m√ºssen Sie den <code>service-name.service</code> und starten.  Beispiel f√ºr kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Nat√ºrlich m√ºssen alle Dienste <em>gr√ºn sein</em> (dh ausgef√ºhrt werden und funktionieren).  Wenn Sie auf einen Fehler <code>journal -f -t kube-apiserver</code> , helfen Ihnen die <code>journal -f -t kube-apiserver</code> <code>journalct -xe</code> oder <code>journal -f -t kube-apiserver</code> , zu verstehen, was genau schief gelaufen ist. </p><br><p>  Beeilen Sie sich nicht, alle Server gleichzeitig zu starten. F√ºr einen Start reicht es aus, etcd und kube-apiserver zu aktivieren.  Wenn alles gut gelaufen ist und Sie sofort alle vier Dienste des Assistenten erhalten haben, kann der Start des Assistenten als erfolgreich angesehen werden. </p><br><h3 id="master-2">  Der Meister </h3><br><p>  Sie k√∂nnen die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd-</a> Einstellungen verwenden oder Init-Skripte f√ºr die von Ihnen verwendete Konfiguration generieren.  Wie bereits erw√§hnt, ben√∂tigen Sie f√ºr den Master: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Arbeitseinheiten </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systemd / Containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">System / Flanell</a> </p><br><h3 id="klient">  Kunde </h3><br><p>  Damit der Client funktioniert, kopieren <code>certs/kubeconfig/admin.kubeconfig</code> einfach <code>certs/kubeconfig/admin.kubeconfig</code> (nachdem Sie es generiert oder selbst geschrieben haben) in <code>${HOME}/.kube/config</code> </p><br><p>  Laden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubectl</a> herunter und √ºberpr√ºfen Sie die Funktion von kube-apiserver.  Ich m√∂chte Sie noch einmal daran erinnern, dass in diesem Stadium, damit kube-apiserver funktioniert, nur etcd funktionieren sollte.  Die restlichen Komponenten werden etwas sp√§ter f√ºr den vollst√§ndigen Betrieb des Clusters ben√∂tigt. </p><br><p>  √úberpr√ºfen Sie, ob kube-apiserver und kubectl funktionieren: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Flanellkonfiguration </h1><br><p>  Als <code>vxlan</code> ich mich f√ºr das <code>vxlan</code> Backend entschieden.  Lesen Sie hier mehr √ºber Backends. </p><br><div class="spoiler">  <b class="spoiler_title">host-gw und warum es nicht funktioniert</b> <div class="spoiler_text"><p>  Ich muss sofort sagen, dass das Ausf√ºhren eines Kubernetes-Clusters auf einem VPS Sie wahrscheinlich auf die Verwendung des <code>host-gw</code> Backends beschr√§nken wird.  Da ich kein erfahrener Netzwerktechniker bin, habe ich ungef√§hr zwei Tage lang debuggt, um zu verstehen, was das Problem bei der Verwendung bei beliebten VDS / VPS-Anbietern war. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Linode.com</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Digitalocean</a> wurden getestet.  Das Wesentliche des Problems ist, dass Anbieter kein ehrliches L2 f√ºr ein privates Netzwerk bereitstellen.  Dies macht es wiederum unm√∂glich, den Netzwerkverkehr zwischen Knoten in dieser Konfiguration zu verschieben: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Verkehr"></p><br><p>  Damit der Netzwerkverkehr zwischen den Knoten funktioniert, reicht normales Routing aus.  Vergessen Sie nicht, dass net.ipv4.ip_forward auf 1 gesetzt werden sollte und die FORWARD-Kette in der Filtertabelle keine Verbotsregeln f√ºr Knoten enthalten sollte. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  Dies ist genau das, was auf dem angegebenen (und h√∂chstwahrscheinlich allgemein auf allen) VPS / VDS nicht funktioniert. </p><br><p>  Wenn Ihnen daher die Konfiguration einer L√∂sung mit hoher Netzwerkleistung zwischen den Knoten <strong>wichtig ist</strong> , m√ºssen Sie immer noch mehr als 20 US-Dollar f√ºr die Organisation des Clusters ausgeben. </p></div></div><br><p>  Sie k√∂nnen <code>set-flannel-config.sh</code> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etc /</a> <code>set-flannel-config.sh</code> , um die gew√ºnschte <code>set-flannel-config.sh</code> .  <strong>Es ist wichtig zu beachten</strong> : Wenn Sie das Backend √§ndern m√∂chten, m√ºssen Sie die Konfiguration in etcd l√∂schen und alle Flanell-Daemons auf allen Knoten neu starten. W√§hlen Sie sie daher mit Bedacht aus.  Der Standardwert ist vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Nachdem Sie die gew√ºnschte Konfiguration in etcd registriert haben, m√ºssen Sie den Dienst so konfigurieren, dass er auf jedem der Arbeitsknoten ausgef√ºhrt wird. </p><br><h2 id="flannelservice">  flannel.service </h2><br><p>  Ein Beispiel f√ºr den Dienst kann hier genommen werden: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">flannel.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Anpassung </h2><br><p>  Wie bereits beschrieben, ben√∂tigen wir die Dateien ca.pem, kubernetes.pem und kubernetes-key.pem f√ºr die Autorisierung in etcd.  Alle anderen Parameter haben keine heilige Bedeutung.  Das einzige, was wirklich wichtig ist, ist die Konfiguration der globalen IP-Adresse, √ºber die Netzwerkpakete zwischen Netzwerken √ºbertragen werden: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Flanellvernetzung"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Multi-Host-Netzwerk-Overlay mit Flanell</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Nachdem Flanell erfolgreich gestartet wurde, sollten Sie die Netzwerkschnittstelle flannel.N auf Ihrem System finden: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Es ist ganz einfach zu √ºberpr√ºfen, ob Ihre Schnittstellen auf allen Knoten ordnungsgem√§√ü funktionieren.  In meinem Fall haben Knoten-1 und Knoten-2 10.200.8.0/24 bzw. 10.200.12.0/24 Netzwerke. Bei einer regul√§ren icmp-Anfrage √ºberpr√ºfen wir daher deren Verf√ºgbarkeit: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  Bei Problemen wird empfohlen, zu √ºberpr√ºfen, ob in iptables √ºber UDP Schnittregeln zwischen Hosts vorhanden sind. </p><br><h1 id="konfiguraciya-containerd">  Containerd-Konfiguration </h1><br><p>  Platzieren Sie <a href="">etc / Containerd / config.toml</a> in <code>/etc/containerd/config.toml</code> oder wo immer es Ihnen <a href="">passt.</a> <code>/etc/containerd/config.toml</code> Sie daran, den Pfad zur Konfigurationsdatei im Dienst (containerd.service, siehe unten) zu √§ndern. </p><br><p>  Konfiguration mit einigen √Ñnderungen des Standards.  <strong>Es ist wichtig,</strong> <code>enable_tls_streaming = true</code> <strong>nicht zu setzen</strong> <code>enable_tls_streaming = true</code> wenn Sie nicht verstehen, warum Sie dies tun.  Andernfalls <code>kubectl exec</code> nicht mehr und gibt den Fehler aus, dass das Zertifikat von einer unbekannten Partei signiert wurde. </p><br><h2 id="containerdservice">  containerd.service </h2><br><div class="spoiler">  <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) ‚Äî    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               ‚Äî     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> ‚Äî <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> ‚Äî  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://kubernetes-example.w40k.net/</a> ‚Äî   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Referenzen </h1><br><p> ,     ,   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes the hard way</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intro to Podman</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stateless Applications</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ) <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            ‚Äî    . </p><br><p>              . , ,   , ‚Äî      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de435228/">https://habr.com/ru/post/de435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de435214/index.html">Linux 4.20 ver√∂ffentlicht - was sich in der neuen Kernel-Version ge√§ndert hat</a></li>
<li><a href="../de435216/index.html">Wie man aus zwei Codezeilen 200 macht und warum Sie dies tun m√ºssen</a></li>
<li><a href="../de435220/index.html">Kotlin Native: Verfolgen Sie die Dateien</a></li>
<li><a href="../de435224/index.html">So kommunizieren Sie in einem englischsprachigen B√ºro: 14 n√ºtzliche Redewendungen</a></li>
<li><a href="../de435226/index.html">Stellen Sie die Daten von Grund auf neu her</a></li>
<li><a href="../de435234/index.html">Kl√ºger, genauer gesagt: Wie KI Fl√ºge in den Weltraum ver√§ndert</a></li>
<li><a href="../de435236/index.html">Byte-Maschine f√ºr das Fort (und nicht nur) in Native American (Teil 3)</a></li>
<li><a href="../de435240/index.html">Unreal Engine4 - PostProcess-Scan-Effekt</a></li>
<li><a href="../de435242/index.html">Warum habe ich Angst, ein "gepumpter Mann" zu werden?</a></li>
<li><a href="../de435244/index.html">ITER-Projekt im Jahr 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>