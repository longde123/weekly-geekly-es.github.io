<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⌨️ 🔹 🧑🏼‍🤝‍🧑🏻 12 neue künstliche Intelligenz für Azure Media Services 🧜🏼 🖥️ 🧟</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Mission von Microsoft ist es, jeder Person und Organisation auf dem Planeten die Möglichkeit zu geben, mehr zu erreichen. Die Medienbranche ist ei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>12 neue künstliche Intelligenz für Azure Media Services</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/microsoft/blog/469031/">  Die Mission von Microsoft ist es, jeder Person und Organisation auf dem Planeten die Möglichkeit zu geben, mehr zu erreichen.  Die Medienbranche ist ein hervorragendes Beispiel für die Umsetzung dieser Mission in die Realität.  Wir leben in einer Zeit, in der immer mehr Inhalte erstellt und konsumiert werden, alles auf großartige Weise und auf mehr Geräten.  Auf der IBC 2019 haben wir die neuesten Innovationen vorgestellt, an denen wir gerade arbeiten, und darüber gesprochen, wie sie zur Transformation Ihres Medienprozesses beitragen können. <br><img src="https://habrastorage.org/getpro/habr/post_images/d33/662/fc1/d33662fc1b9cb7b3a7310711f087a51d.jpg"><br>  Details unter dem Schnitt! <a name="habracut"></a><br><br>  Diese Seite befindet sich auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserer Website.</a> <br><br><h2>  Video Indexer bietet Unterstützung für Animationen und mehrsprachige Inhalte </h2><br>  Letztes Jahr haben wir auf der IBC unseren preisgekrönten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Media Services-Videoindexer veröffentlicht</a> , und dieses Jahr wurde er noch besser.  Der Video-Indexer extrahiert automatisch Informationen und Metadaten aus Mediendateien wie gesprochenen Wörtern, Gesichtern, Emotionen, Themen und Marken, und Sie müssen kein Experte für maschinelles Lernen sein, um sie zu verwenden. <br><br>  Unsere neuesten Angebote umfassen vorläufige Versionen von zwei sehr beliebten und differenzierten Funktionen - Erkennung von animierten Zeichen und Transkription mehrsprachiger Sprache sowie mehrere Ergänzungen zu bestehenden Modellen, die heute im Video Indexer verfügbar sind. <br><br><h2>  Erkennung von animierten Charakteren </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/01f/1ab/900/01f1ab900cce076468f55a62bc103b2f.png"><br>  Zeichentrickfilme sind eine der beliebtesten Arten von Inhalten, aber Standard-Bildverarbeitungsmodelle, die zum Erkennen menschlicher Gesichter erstellt wurden, funktionieren nicht sehr gut damit, insbesondere wenn der Inhalt Zeichen ohne menschliche Merkmale enthält.  In der neuen Vorschau-Version ist Video Indexer in den Azure Custom Vision-Dienst von Microsoft integriert. Dadurch werden neue Modelle erstellt, die animierte Zeichen automatisch erkennen und gruppieren und das Kennzeichnen und Erkennen mithilfe integrierter benutzerdefinierter Bildverarbeitungsmodelle vereinfachen. <br><br>  Die Modelle sind in einem einzigen Förderband integriert, sodass jeder diesen Service ohne Kenntnisse im Bereich des maschinellen Lernens nutzen kann.  Die Ergebnisse sind über das Video Indexer-Portal verfügbar, für das kein Code erforderlich ist, oder über die REST-API für die schnelle Integration in Ihre eigenen Anwendungen. <br><br>  Wir haben diese Modelle für die Arbeit mit animierten Charakteren zusammen mit einigen Verbrauchern erstellt, die echte animierte Inhalte für Schulungen und Tests bereitgestellt haben.  Der Wert der neuen Funktionalität wurde von Andy Gutteridge, Senior Director für Studiotechnologie und Postproduktion von Viacom International Media Networks, der einer der Datenanbieter war, gut beschrieben: „Durch Hinzufügen einer robusten AI-basierten Funktion zur Erkennung animierter Inhalte können wir schnell und effizient Zeichenmetadaten aus unserer Bibliothek finden und katalogisieren Inhalt. <br><br>  Vor allem wird es unseren Kreativteams die Möglichkeit geben, sofort die richtigen Inhalte zu finden, den Zeitaufwand für die Verwaltung der Medien zu minimieren und uns auf die Kreativität zu konzentrieren. “ <br><br>  Sie können die Erkennung animierter Zeichen auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentationsseite untersuchen</a> . <br><br><h2>  Identifizierung und Transkription von Inhalten in mehreren Sprachen </h2><br>  Einige Medienressourcen wie Nachrichten, Ereignischroniken und Interviews enthalten Aufzeichnungen von Personen, die verschiedene Sprachen sprechen.  Die meisten der vorhandenen Optionen zum Übersetzen von Sprache in Text erfordern eine vorläufige Angabe der Tonerkennungssprache, was das Transkribieren mehrsprachiger Videos erschwert. <br><br>  Unsere neue Funktion zur automatischen Identifizierung einer gesprochenen Sprache für verschiedene Arten von Inhalten verwendet die Technologie des maschinellen Lernens, um Sprachen in Medienressourcen zu identifizieren.  Nach der Erkennung durchläuft jedes Sprachsegment automatisch den Transkriptionsprozess in der entsprechenden Sprache. Anschließend werden alle Segmente zu einer einzigen Transkriptionsdatei zusammengefasst, die aus mehreren Sprachen besteht. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/593/b6c/37c/593b6c37c928baa329de15eb80f0a8af.png"><br><br>  Die resultierende Entschlüsselung ist als Teil der Ausgabe des JSON Video Indexer und in Form von Dateien mit Untertiteln verfügbar.  Die Ausgabeentschlüsselung ist auch in Azure Search integriert, sodass Sie sofort nach verschiedenen Sprachsegmenten in Videos suchen können.  Darüber hinaus ist bei der Arbeit mit dem Video Indexer-Portal eine mehrsprachige Transkription verfügbar, sodass Sie das Transkript und die identifizierte Sprache nach Zeit anzeigen oder für jede Sprache zu bestimmten Stellen im Video gehen und während der Videowiedergabe eine mehrsprachige Transkription in Form von Signaturen anzeigen können.  Sie können den resultierenden Text auch über das Portal und die API in eine der 54 verfügbaren Sprachen übersetzen. <br><br>  Weitere Informationen zur neuen Funktion zur Erkennung mehrsprachiger Inhalte und ihrer Verwendung im Video-Indexer finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Dokumentation</a> . <br><br><h2>  Zusätzliche aktualisierte und verbesserte Modelle </h2><br>  Wir fügen dem Video-Indexer auch neue Modelle hinzu und verbessern vorhandene, einschließlich der unten beschriebenen. <br><br><h4>  Abrufen von mit Personen und Orten verknüpften Entitäten </h4><br>  Wir haben unsere bestehenden Möglichkeiten zur Markenentdeckung um bekannte Namen und Standorte wie den Eiffelturm in Paris und den Big Ben in London erweitert.  Wenn sie in der generierten Entschlüsselung oder mithilfe der optischen Zeichenerkennung (OCR) auf dem Bildschirm angezeigt werden, werden die entsprechenden Informationen hinzugefügt.  Mit dieser neuen Funktion können Sie alle Personen, Orte und Marken durchsuchen, die im Video angezeigt werden, und Informationen zu diesen anzeigen, einschließlich Zeitintervallen, Beschreibungen und Links zur Bing-Suchmaschine, um weitere Informationen zu erhalten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/750/c43/023/750c43023844290c35eefa61f2867890.png"><br><br><h4>  Editor-Erkennungsmodell </h4><br>  Diese neue Funktion fügt den Metadaten, die einzelnen Frames in den JSON-Details zugeordnet sind, eine Reihe von „Tags“ hinzu, um deren redaktionellen Typ darzustellen (z. B. Weitwinkel, mittlerer Frame, Nahaufnahme, sehr Nahaufnahme, zwei Aufnahmen, mehrere Personen, im Freien, drinnen usw.).  Diese Rahmentypmerkmale sind nützlich, wenn Sie Videos für Clips und Trailer bearbeiten oder nach einem bestimmten Bildstil für künstlerische Zwecke suchen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2e8/634/2b4/2e86342b4eb18285316689503e59ac63.png"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weitere</a> Informationen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zur</a> Frame-Typ-Erkennung finden Sie im Video-Indexer. <br><br><h4>  Erweiterte IPTC-Zuordnungsdetails </h4><br>  Unser Themenerkennungsmodell identifiziert ein Videothema basierend auf Transkription, optischer Zeichenerkennung (OCR) und entdeckten Prominenten, auch wenn das Thema nicht explizit angegeben ist.  Wir ordnen diese entdeckten Themen vier Klassifizierungsbereichen zu: Wikipedia, Bing, IPTC und IAB.  Diese Verbesserung ermöglicht es uns, eine IPTC-Klassifizierung der zweiten Ebene aufzunehmen. <br>  Die Nutzung dieser Verbesserungen ist so einfach wie die Neuindizierung Ihrer aktuellen Video Indexer-Bibliothek. <br><br><h4>  Neue Live-Streaming-Funktionalität </h4><br>  In der Vorschau-Version von Azure Media Services bieten wir außerdem zwei neue Funktionen für das Live-Streaming. <br><br><h4>  Die Echtzeit-Transkription von KI bringt Live-Übertragungen auf die nächste Stufe </h4><br>  Wenn Sie Azure Media Services für das Live-Streaming verwenden, können Sie jetzt einen Ausgabestream abrufen, der neben Audio- und Videoinhalten auch eine automatisch generierte Textspur enthält.  Text wird durch Transkribieren von Echtzeit-Audio basierend auf künstlicher Intelligenz erstellt.  Benutzerdefinierte Methoden werden vor und nach der Konvertierung von Sprache in Text angewendet, um die Ergebnisse zu verbessern.  Die Textspur ist in IMSC1, TTML oder WebVTT gepackt, je nachdem, ob sie in DASH, HLS CMAF oder HLS TS vorliegt. <br><br><h4>  Lineare Echtzeitcodierung für 24/7-OTT-Kanäle </h4><br>  Mit unserer API v3 können Sie Kanäle mithilfe der OTT-Technologie (Over-the-Top) erstellen, Live-Übertragungen verwalten und verwalten sowie alle anderen Azure Media Services-Funktionen wie Live-Video-on-Demand (VOD) verwenden. Video on Demand), Verpackung und Digital Rights Management (DRM). <br>  Eine Vorschau dieser Funktionen finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Community-</a> Seite von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Media Services</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/176/eab/e8a/176eabe8a1e9c0cf847b80d0a122f3a5.png"><br><br><h4>  Neue Funktionen zur Paketgenerierung </h4><br><h4>  Sound Track Beschreibung Unterstützung </h4><br>  Inhalte, die auf Sendekanälen ausgestrahlt werden, enthalten häufig eine Audiospur mit verbalen Erklärungen darüber, was zusätzlich zum normalen Audiosignal auf dem Bildschirm geschieht.  Dies macht Programme für sehbehinderte Zuschauer zugänglicher, insbesondere wenn der Inhalt hauptsächlich visuell ist.  Mit der neuen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Audiobeschreibungsfunktion</a> können Sie eine der Audiospuren als Audiobeschreibungsspur (AD, Audiobeschreibung) mit Anmerkungen versehen, damit die Spieler die AD-Spur für die Zuschauer zugänglich machen können. <br><br><h4>  Fügen Sie ID3-Metadaten ein </h4><br>  Rundfunkunternehmen verwenden häufig im Video eingebettete zeitbasierte Metadaten, um ein Signal über das Einfügen von Werbung oder benutzergenerierten Metadatenereignissen auf den Player des Kunden zu übertragen.  Zusätzlich zu den SCTE-35-Signalisierungsmodi unterstützen wir jetzt auch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ID3v2 und andere</a> vom Anwendungsentwickler definierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benutzerschemata</a> zur Verwendung durch die Clientanwendung. <br><br><h2>  Microsoft Azure-Partner präsentieren End-to-End-Lösungen </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bitmovin</a> führt die Bitmovin-Videokodierung und den Bitmovin-Video-Player für Microsoft Azure ein.  Kunden können diese Codierungs- und Wiedergabelösungen jetzt in Azure verwenden und erweiterte Funktionen wie dreistufige Codierung, Unterstützung für AV1 / VC-Codecs, mehrsprachige Untertitel und vorintegrierte Videoanalysen für QoS, Werbung und Video-Tracking verwenden. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Evergent</a> präsentiert seine User Lifecycle Management Platform in Azure.  Als führender Anbieter von Managementlösungen für Umsatz und Kundenlebenszyklus nutzt Evergent Azure AI, um Premium-Unterhaltungsanbietern dabei zu helfen, die Kundenbindung und -bindung zu verbessern, indem gezielte Servicepakete und Angebote zu kritischen Zeiten in ihrem Lebenszyklus erstellt werden. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Haivision</a> wird seinen intelligenten Cloud-basierten Multimedia-Routing-Service SRT Hub <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorstellen</a> , mit dem Kunden Workflows mithilfe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Azure Data Box Edge</a> von Anfang bis Ende transformieren und Workflows mithilfe von Hublets von Avid, Telestream, Wowza, Cinegy und Make.tv transformieren können. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SES hat</a> die Azure-basierte Broadcast Class Media Services Suite für seine satellitenbasierten und verwalteten Mediendienstkunden entwickelt.  SES zeigt Lösungen für vollständig verwaltete Wiedergabedienste, einschließlich Master-Wiedergabe, lokalisierter Wiedergabe, Anzeigenerkennung und -ersetzung sowie hochwertiger 24-Stunden-Echtzeit-Mehrkanalcodierung in Azure. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SyncWords</a> stellt in Azure praktische Cloud-Tools und -Technologien zum Erstellen von Signaturen zur Verfügung.  Diese Angebote erleichtern Medienunternehmen das automatische Hinzufügen von Untertiteln, auch in einer Fremdsprache, zu den Workflows der Videoverarbeitung in Echtzeit und offline in Azure. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tata Elxsi</a> , ein internationales Technologiedienstleistungsunternehmen, hat seine OTT SaaS TEPlay-Plattform in Azure Media Services integriert, um OTT-Inhalte aus der Cloud bereitzustellen.  Tata Elxsi hat auch die QoE-Lösung von Falcon Eye, die Analyse- und Entscheidungsmetriken bereitstellt, auf Microsoft Azure migriert. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verizon Media</a> stellt seine Streaming-Plattform als Beta auf Azure zur Verfügung.  Verizon Media Platform ist eine OTT-Lösung für Unternehmen, die DRM, Anzeigeneinfügung, personalisierte personalisierte Sitzungen, dynamischen Inhaltsaustausch und Videobereitstellung umfasst.  Die Integration vereinfacht Workflows, globale Unterstützung und Skalierbarkeit und bietet Ihnen Zugriff auf eine Reihe einzigartiger Funktionen in Azure. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de469031/">https://habr.com/ru/post/de469031/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de469019/index.html">SPA Meetup 5: Jest-Integration mit QA, leistungsstarkes UIKit, Komponentenbibliotheken, DI für die Skalierung, Plattformbefehle</a></li>
<li><a href="../de469021/index.html">Entwicklung in einem Monorepository. Yandex-Bericht</a></li>
<li><a href="../de469023/index.html">So finden Sie einen Job mit Umzug nach Europa: ein praktischer Leitfaden für IT-Experten</a></li>
<li><a href="../de469025/index.html">Kühlen Sie den Wein schnell ab! Russische Erfindung</a></li>
<li><a href="../de469027/index.html">Ivanovo! Mitap: Wie baue ich eine Karriere in Digital auf?</a></li>
<li><a href="../de469033/index.html">Starten der Elbrus-Plattform für neuronale PuzzleLib-Netzwerke</a></li>
<li><a href="../de469035/index.html">Die neuen AI-basierten Innovationen von Azure Media Services</a></li>
<li><a href="../de469037/index.html">Industrielle Steuerung. Datenerfassungssystem. ACS</a></li>
<li><a href="../de469039/index.html">Mehr als ein Spiel: Mahjong mit KI und maschinellem Lernen meistern</a></li>
<li><a href="../de469041/index.html">Wie schützen Sie Ihr ERP-System?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>