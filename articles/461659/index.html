<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçüíº üßìüèø üë∞ Redes neuronales y aprendizaje profundo, Cap√≠tulo 4: Prueba visual de que las redes neuronales pueden calcular cualquier funci√≥n üë®üèº‚Äçüè≠ üàµ üõ∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En este cap√≠tulo, doy una explicaci√≥n simple y principalmente visual del teorema de universalidad. Para seguir el material de este cap√≠tulo, no tiene ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales y aprendizaje profundo, Cap√≠tulo 4: Prueba visual de que las redes neuronales pueden calcular cualquier funci√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  En este cap√≠tulo, doy una explicaci√≥n simple y principalmente visual del teorema de universalidad.  Para seguir el material de este cap√≠tulo, no tiene que leer los anteriores.  Est√° estructurado como un ensayo independiente.  Si tiene la comprensi√≥n m√°s b√°sica de NS, deber√≠a poder entender las explicaciones. <br><br><div class="spoiler">  <b class="spoiler_title">Contenido</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 1: uso de redes neuronales para reconocer n√∫meros escritos a mano</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 2: c√≥mo funciona el algoritmo de retropropagaci√≥n</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: mejorar el m√©todo de entrenamiento de redes neuronales</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: ¬øPor qu√© la regularizaci√≥n ayuda a reducir el reciclaje?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 3: ¬øc√≥mo elegir hiperpar√°metros de red neuronal?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 4: prueba visual de que las redes neuronales son capaces de calcular cualquier funci√≥n</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 5: ¬øpor qu√© las redes neuronales profundas son tan dif√≠ciles de entrenar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 1: aprendizaje profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Parte 2: progreso reciente en el reconocimiento de im√°genes</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ep√≠logo: ¬øexiste un algoritmo simple para crear inteligencia?</a> </li></ul></div></div><br>  Uno de los hechos m√°s sorprendentes sobre las redes neuronales es que pueden calcular cualquier funci√≥n.  Es decir, digamos que alguien le da alg√∫n tipo de funci√≥n compleja y sinuosa f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  E independientemente de esta funci√≥n, se garantiza una red neuronal tal que para cualquier entrada x, el valor f (x) (o alguna aproximaci√≥n cercana a √©l) ser√° la salida de esta red, es decir: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Esto funciona incluso si es una funci√≥n de muchas variables f = f (x <sub>1</sub> , ..., x <sub>m</sub> ), y con muchos valores.  Por ejemplo, aqu√≠ hay una red que calcula una funci√≥n con m = 3 entradas yn = 2 salidas: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Este resultado sugiere que las redes neuronales tienen una cierta universalidad.  No importa qu√© funci√≥n queramos calcular, sabemos que hay una red neuronal que puede hacer esto. <br><br>  Adem√°s, el teorema de universalidad se mantiene incluso si restringimos la red a una sola capa entre las neuronas entrantes y salientes, la llamada  en una capa oculta  Entonces, incluso las redes con una arquitectura muy simple pueden ser extremadamente poderosas. <br><br>  El teorema de universalidad es bien conocido por las personas que usan redes neuronales.  Pero aunque esto es as√≠, la comprensi√≥n de este hecho no est√° tan extendida.  Y la mayor√≠a de las explicaciones para esto son demasiado complejas t√©cnicamente.  Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">uno de los primeros documentos que</a> prueban este resultado utiliza el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">teorema de Hahn - Banach</a> , el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">teorema de representaci√≥n de Riesz</a> y algunos an√°lisis de Fourier.  Si usted es matem√°tico, es f√°cil para usted comprender esta evidencia, pero para la mayor√≠a de las personas no es tan f√°cil.  Es una pena, porque las razones b√°sicas para la universalidad son simples y hermosas. <br><br>  En este cap√≠tulo, doy una explicaci√≥n simple y principalmente visual del teorema de universalidad.  Iremos paso a paso a trav√©s de las ideas subyacentes.  Comprender√° por qu√© las redes neuronales realmente pueden calcular cualquier funci√≥n.  Comprender√° algunas de las limitaciones de este resultado.  Y comprender√° c√≥mo se asocia el resultado con NS profunda. <br><br>  Para seguir el material de este cap√≠tulo, no tiene que leer los anteriores.  Est√° estructurado como un ensayo independiente.  Si tiene la comprensi√≥n m√°s b√°sica de NS, deber√≠a poder entender las explicaciones.  Pero a veces proporcionar√© enlaces a material anterior para ayudar a llenar los vac√≠os de conocimiento. <br><br>  Los teoremas de universalidad a menudo se encuentran en la inform√°tica, por lo que a veces incluso olvidamos lo incre√≠bles que son.  Pero vale la pena recordarse: la capacidad de calcular cualquier funci√≥n arbitraria es realmente sorprendente.  Casi cualquier proceso que pueda imaginar puede reducirse al c√°lculo de una funci√≥n.  Considere la tarea de encontrar el nombre de una composici√≥n musical basada en un breve pasaje.  Esto puede considerarse un c√°lculo de funci√≥n.  O considere la tarea de traducir un texto chino al ingl√©s.  Y esto puede considerarse un c√°lculo de funci√≥n (de hecho, muchas funciones, ya que hay muchas opciones aceptables para traducir un solo texto).  O considere la tarea de generar una descripci√≥n de la trama de la pel√≠cula y la calidad de la actuaci√≥n basada en el archivo mp4.  Esto tambi√©n puede considerarse como el c√°lculo de una determinada funci√≥n (la observaci√≥n hecha sobre las opciones de traducci√≥n de texto tambi√©n es correcta aqu√≠).  La universalidad significa que, en principio, los NS pueden realizar todas estas tareas y muchas otras. <br><br>  Por supuesto, solo por el hecho de que sabemos que hay NS capaces de, por ejemplo, traducir del chino al ingl√©s, no se deduce que tengamos buenas t√©cnicas para crear o incluso reconocer dicha red.  Esta restricci√≥n tambi√©n se aplica a los teoremas de universalidad tradicionales para modelos como los esquemas booleanos.  Pero, como ya hemos visto en este libro, el NS tiene algoritmos poderosos para las funciones de aprendizaje.  La combinaci√≥n de algoritmos de aprendizaje y versatilidad es una combinaci√≥n atractiva.  Hasta ahora, en el libro, nos hemos concentrado en algoritmos de entrenamiento.  En este cap√≠tulo, nos centraremos en la versatilidad y lo que significa. <br><br><h2>  Dos trucos </h2><br>  Antes de explicar por qu√© el teorema de universalidad es verdadero, quiero mencionar dos trucos contenidos en la declaraci√≥n informal "una red neuronal puede calcular cualquier funci√≥n". <br><br>  En primer lugar, esto no significa que la red pueda usarse para calcular con precisi√≥n cualquier funci√≥n.  Solo podemos obtener una aproximaci√≥n tan buena como la que necesitamos.  Al aumentar el n√∫mero de neuronas ocultas, mejoramos la aproximaci√≥n.  Por ejemplo, anteriormente ilustr√© una red que computa una determinada funci√≥n f (x) usando tres neuronas ocultas.  Para la mayor√≠a de las funciones, usando tres neuronas, solo se puede obtener una aproximaci√≥n de baja calidad.  Al aumentar el n√∫mero de neuronas ocultas (por ejemplo, hasta cinco), generalmente podemos obtener una aproximaci√≥n mejorada: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Y para mejorar la situaci√≥n aumentando a√∫n m√°s el n√∫mero de neuronas ocultas. <br><br>  Para aclarar esta afirmaci√≥n, digamos que se nos dio una funci√≥n f (x), que queremos calcular con cierta precisi√≥n necesaria Œµ&gt; 0.  Hay una garant√≠a de que cuando se usa un n√∫mero suficiente de neuronas ocultas, siempre podemos encontrar un NS cuya salida g (x) satisfaga la ecuaci√≥n | g (x) ‚àíf (x) | &lt;Œµ para cualquier x.  En otras palabras, la aproximaci√≥n se lograr√° con la precisi√≥n deseada para cualquier posible valor de entrada. <br><br>  El segundo problema es que las funciones que pueden ser aproximadas por el m√©todo descrito pertenecen a una clase continua.  Si la funci√≥n se interrumpe, es decir, realiza saltos bruscos repentinos, entonces, en el caso general, ser√° imposible aproximarse con la ayuda de NS.  Y esto no es sorprendente, ya que nuestros NS calculan funciones continuas de datos de entrada.  Sin embargo, incluso si la funci√≥n que realmente necesitamos calcular es discontinua, la aproximaci√≥n es a menudo bastante continua.  Si es as√≠, entonces podemos usar NS.  En la pr√°ctica, esta limitaci√≥n generalmente no es importante. <br><br>  Como resultado, una declaraci√≥n m√°s precisa del teorema de universalidad ser√° que NS con una capa oculta puede usarse para aproximar cualquier funci√≥n continua con la precisi√≥n deseada.  En este cap√≠tulo, demostramos una versi√≥n un poco menos rigurosa de este teorema, utilizando dos capas ocultas en lugar de una.  En las tareas, describir√© brevemente c√≥mo se puede adaptar esta explicaci√≥n, con cambios menores, a una prueba que use solo una capa oculta. <br><br><h2>  Versatilidad con un valor de entrada y uno de salida. </h2><br>  Para entender por qu√© el teorema de universalidad es verdadero, comenzamos por comprender c√≥mo crear una funci√≥n de aproximaci√≥n NS con solo un valor de entrada y un valor de salida: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Resulta que esta es la esencia de la tarea de la universalidad.  Una vez que comprendamos este caso especial, ser√° bastante f√°cil extenderlo a funciones con muchos valores de entrada y salida. <br><br>  Para comprender c√≥mo construir una red para contar f, comenzamos con una red que contiene una sola capa oculta con dos neuronas ocultas, y con una capa de salida que contiene una neurona de salida: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Para imaginar c√≥mo funcionan los componentes de la red, nos enfocamos en la neurona oculta superior.  En el diagrama del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo original,</a> puede cambiar interactivamente el peso con el mouse haciendo clic en "w" e inmediatamente ver c√≥mo cambia la funci√≥n calculada por la neurona oculta superior: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Como aprendimos anteriormente en el libro, una neurona oculta cuenta œÉ (wx + b), donde œÉ (z) ‚â° 1 / (1 + e <sup>‚àíz</sup> ) es un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sigmoide</a> .  Hasta ahora, hemos usado esta forma algebraica con bastante frecuencia.  Sin embargo, para probar la universalidad ser√≠a mejor si ignoramos por completo este √°lgebra y, en su lugar, manipulamos y observamos la forma en el gr√°fico.  Esto no solo lo ayudar√° a sentir mejor lo que est√° sucediendo, sino que tambi√©n nos dar√° una prueba de universalidad aplicable a otras funciones de activaci√≥n adem√°s de sigmoide. <br><br>  Estrictamente hablando, el enfoque visual que he elegido tradicionalmente no se considera evidencia.  Pero creo que el enfoque visual proporciona m√°s informaci√≥n sobre la verdad del resultado final que la prueba tradicional.  Y, por supuesto, tal comprensi√≥n es el verdadero prop√≥sito de la prueba.  En la evidencia que propongo, las brechas ocasionalmente se encontrar√°n;  Dar√© evidencia visual razonable, pero no siempre rigurosa.  Si esto te molesta, entonces considera que es tu tarea llenar estos vac√≠os.  Sin embargo, no pierda de vista el objetivo principal: comprender por qu√© el teorema de universalidad es verdadero. <br><br>  Para comenzar con esta prueba, haga clic en el desplazamiento b en el diagrama original y arrastre hacia la derecha para agrandarlo.  Ver√° que con un aumento en el desplazamiento, el gr√°fico se mueve hacia la izquierda, pero no cambia de forma. <br><br>  Luego arr√°strelo hacia la izquierda para reducir el desplazamiento.  Ver√° que el gr√°fico se mueve hacia la derecha sin cambiar de forma. <br><br>  Reduce el peso a 2-3.  Ver√° que a medida que disminuye el peso, la curva se endereza.  Para que la curva no se salga del gr√°fico, es posible que deba corregir el desplazamiento. <br><br>  Finalmente, aumente el peso a valores superiores a 100. La curva se har√° m√°s empinada y eventualmente se acercar√° al paso.  Intente ajustar el desplazamiento para que su √°ngulo est√© en la regi√≥n del punto x = 0.3.  El siguiente video muestra lo que deber√≠a suceder: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Podemos simplificar en gran medida nuestro an√°lisis aumentando el peso para que la salida sea realmente una buena aproximaci√≥n de la funci√≥n de paso.  A continuaci√≥n constru√≠ la salida de la neurona oculta superior para el peso w = 999.  Esta es una imagen est√°tica: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Usar funciones escalonadas es un poco m√°s f√°cil que con un sigmoide t√≠pico.  La raz√≥n es que las contribuciones de todas las neuronas ocultas se suman en la capa de salida.  La suma de un mont√≥n de funciones de pasos es f√°cil de analizar, pero es m√°s dif√≠cil hablar sobre lo que sucede cuando se agregan un mont√≥n de curvas en forma de sigmoide.  Por lo tanto, ser√° mucho m√°s simple suponer que nuestras neuronas ocultas producen funciones escalonadas.  M√°s precisamente, hacemos esto fijando el peso w en un valor muy grande y luego asignando la posici√≥n del paso a trav√©s del desplazamiento.  Por supuesto, trabajar con una salida como una funci√≥n de paso es una aproximaci√≥n, pero es muy bueno, y hasta ahora trataremos la funci√≥n como una verdadera funci√≥n de paso.  M√°s tarde, volver√© a discutir el efecto de las desviaciones de esta aproximaci√≥n. <br><br>  ¬øQu√© valor de x es el paso?  En otras palabras, ¬øc√≥mo depende la posici√≥n del escal√≥n del peso y el desplazamiento? <br><br>  Para responder la pregunta, intente cambiar el peso y el desplazamiento en el gr√°fico interactivo.  ¬øPuedes entender c√≥mo la posici√≥n del paso depende de w y b?  Al practicar un poco, puedes convencerte de que su posici√≥n es proporcional a b e inversamente proporcional a w. <br><br>  De hecho, el paso est√° en s = ‚àíb / w, como se ver√° si ajustamos el peso y el desplazamiento a los siguientes valores: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Nuestras vidas se simplificar√°n enormemente si describimos las neuronas ocultas con un solo par√°metro, s, es decir, por la posici√≥n del paso, s = ‚àíb / w.  En el siguiente diagrama interactivo, simplemente puede cambiar s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Como se se√±al√≥ anteriormente, asignamos especialmente un peso w en la entrada a un valor muy grande, lo suficientemente grande como para que la funci√≥n de paso se convierta en una buena aproximaci√≥n.  Y podemos volver f√°cilmente la neurona parametrizada de esta manera a su forma habitual eligiendo el sesgo b = ‚àíws. <br><br>  Hasta ahora, nos hemos concentrado en la salida de solo la neurona oculta superior.  Veamos el comportamiento de toda la red.  Suponga que las neuronas ocultas calculan las funciones de paso definidas por los par√°metros de los pasos s <sub>1</sub> (neurona superior) y s <sub>2</sub> (neurona inferior).  Sus respectivos pesos de salida son w <sub>1</sub> yw <sub>2</sub> .  Aqu√≠ est√° nuestra red: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  A la derecha hay un gr√°fico de la salida ponderada w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 de la</sub> capa oculta.  Aqu√≠ un <sub>1</sub> y un <sub>2</sub> son las salidas de las neuronas ocultas superior e inferior, respectivamente.  Se denotan con "a", ya que a menudo se llaman activaciones neuronales. <br><br>  Por cierto, notamos que la salida de toda la red es œÉ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), donde b es el sesgo de la neurona de salida.  Esto, obviamente, no es lo mismo que la salida ponderada de la capa oculta, cuyo gr√°fico estamos construyendo.  Pero por ahora, nos concentraremos en la salida equilibrada de la capa oculta, y solo m√°s adelante pensaremos en c√≥mo se relaciona con la salida de toda la red. <br><br>  Intente aumentar y disminuir el paso s <sub>1 de la</sub> neurona oculta superior en el diagrama interactivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">del art√≠culo original</a> .  Vea c√≥mo esto cambia la salida ponderada de la capa oculta.  Es especialmente √∫til comprender qu√© sucede cuando s <sub>1</sub> excede s <sub>2</sub> .  Ver√° que el gr√°fico en estos casos cambia de forma, a medida que pasamos de una situaci√≥n en la que la neurona oculta superior se activa primero a una situaci√≥n en la que la neurona oculta inferior se activa primero. <br><br>  Del mismo modo, intente manipular el paso s <sub>2 de</sub> la neurona oculta inferior y vea c√≥mo esto cambia la producci√≥n general de las neuronas ocultas. <br><br>  Intente reducir y aumentar los pesos de salida.  Observe c√≥mo esto escala la contribuci√≥n de las neuronas ocultas correspondientes.  ¬øQu√© sucede si uno de los pesos es igual a 0? <br><br>  Finalmente, intente configurar w <sub>1</sub> a 0.8 y w <sub>2</sub> a -0.8.  El resultado es una funci√≥n de "protrusi√≥n", con un inicio en s <sub>1</sub> , un final en s <sub>2</sub> y una altura de 0.8.  Por ejemplo, una salida ponderada podr√≠a verse as√≠: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Por supuesto, la protuberancia se puede escalar a cualquier altura.  Usemos un par√°metro, h, que denota altura.  Adem√°s, para simplificar, eliminar√© la notaci√≥n "s <sub>1</sub> = ..." y "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Intente aumentar y disminuir el valor h para ver c√≥mo cambia la altura de la protuberancia.  Intenta hacer h negativo.  Intente cambiar los puntos de los pasos para observar c√≥mo esto cambia la forma de la protuberancia. <br><br>  Ver√° que usamos nuestras neuronas no solo como primitivas gr√°ficas, sino tambi√©n como unidades m√°s familiares para los programadores, algo as√≠ como una instrucci√≥n if-then-else en programaci√≥n: <br><br>  si input&gt; = inicio del paso: <br>  agregue 1 a la salida ponderada <br>  m√°s: <br>  agregue 0 a la salida ponderada <br><br>  En su mayor parte, me atendr√© a la notaci√≥n gr√°fica.  Sin embargo, a veces ser√° √∫til que cambie a la vista if-then-else y reflexione sobre lo que est√° sucediendo en estos t√©rminos. <br><br>  Podemos usar nuestro truco de protrusi√≥n pegando dos partes de neuronas ocultas en la misma red: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Aqu√≠ baj√© los pesos simplemente escribiendo los valores h para cada par de neuronas ocultas.  Intente jugar con ambos valores h y vea c√≥mo cambia el gr√°fico.  Mueva las pesta√±as, cambiando los puntos de los pasos. <br><br>  En un caso m√°s general, esta idea puede usarse para obtener cualquier n√∫mero deseado de picos de cualquier altura.  En particular, podemos dividir el intervalo [0,1] en un gran n√∫mero de subintervalos (N), y usar N pares de neuronas ocultas para obtener picos de cualquier altura deseada.  Veamos c√≥mo funciona esto para N = 5.  Esto ya es un mont√≥n de neuronas, as√≠ que soy una presentaci√≥n un poco m√°s estrecha.  Perd√≥n por el diagrama complejo: podr√≠a ocultar la complejidad detr√°s de abstracciones adicionales, pero me parece que vale la pena un poco de tormento con la complejidad para sentir mejor c√≥mo funcionan las redes neuronales. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Ver√°s, tenemos cinco pares de neuronas ocultas.  Los puntos de los pasos de los pares correspondientes se encuentran en 0.1 / 5, luego 1 / 5.2 / 5, y as√≠ sucesivamente, hasta 4 / 5.5 / 5.  Estos valores son fijos: obtenemos cinco protuberancias de igual ancho en el gr√°fico. <br><br>  Cada par de neuronas tiene un valor h asociado a √©l.  Recuerde que las conexiones neuronales de salida tienen pesos h y ‚Äìh.  En el art√≠culo original en el gr√°fico, puede hacer clic en los valores h y moverlos de izquierda a derecha.  Con un cambio de altura, el horario tambi√©n cambia.  ¬°Al cambiar los pesos de salida, construimos la funci√≥n final! <br><br>  En el diagrama, a√∫n puede hacer clic en el gr√°fico y arrastrar la altura de los pasos hacia arriba o hacia abajo.  Cuando cambia su altura, ve c√≥mo cambia la altura de la h correspondiente.  Los pesos de salida + hy ‚Äìh cambian en consecuencia.  En otras palabras, manipulamos directamente una funci√≥n cuyo gr√°fico se muestra a la derecha y vemos estos cambios en los valores de h a la izquierda.  Tambi√©n puede mantener presionado el bot√≥n del mouse sobre una de las protuberancias, y luego arrastrar el mouse hacia la izquierda o hacia la derecha, y las protuberancias se ajustar√°n a la altura actual. <br><br>  Es hora de hacer el trabajo. <br><br>  Recordemos la funci√≥n que dibuj√© al comienzo del cap√≠tulo: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Entonces no mencion√© esto, pero de hecho se ve as√≠: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.2</mn><mo>+</mo><mn>0.4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0.3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0.05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="58.655ex" height="3.021ex" viewBox="0 -987.6 25254.3 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-3D" x="2179" y="0"></use><g transform="translate(3236,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-32" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2B" x="4737" y="0"></use><g transform="translate(5738,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-34" x="779" y="0"></use></g><g transform="translate(7018,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2B" x="8266" y="0"></use><g transform="translate(9267,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-33" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-78" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-73" x="11369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-69" x="11838" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-6E" x="12184" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-28" x="12784" y="0"></use><g transform="translate(13174,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-78" x="14175" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-29" x="14747" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2B" x="15359" y="0"></use><g transform="translate(16360,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-35" x="1279" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-63" x="18390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-6F" x="18823" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-73" x="19309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-28" x="19778" y="0"></use><g transform="translate(20168,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-78" x="21169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-29" x="21741" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-74" x="22381" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-61" x="22742" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMATHI-67" x="23272" y="0"></use><g transform="translate(23752,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhhYeQQO5Wa1cudJBX0x5h6TBAS06g#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.2 0.2</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.4 0.4</font></font></mn><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn></msup><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0,3</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0,05</font></font></mn><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">50</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">un</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">113</font></font></mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0.2 + 0.4 x ^ 2 + 0.3x \ sin (15 x) + 0.05 \ cos (50 x) \ tag {113} </script></p><br><br>  Se construye para valores x de 0 a 1, y los valores a lo largo del eje y var√≠an de 0 a 1. <br><br>  Obviamente, esta funci√≥n no es trivial.  Y tienes que descubrir c√≥mo calcularlo usando redes neuronales. <br><br>  En nuestras redes neuronales anteriores, analizamos una combinaci√≥n ponderada ‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> de salida neuronal oculta.  Sabemos c√≥mo obtener un control significativo sobre este valor.  Pero, como se√±al√© anteriormente, este valor no es igual a la salida de la red.  La salida de la red es œÉ (‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), donde b es el desplazamiento de la neurona de salida.  ¬øPodemos obtener el control directamente sobre la salida de la red? <br><br>  La soluci√≥n es desarrollar una red neuronal en la que la salida ponderada de la capa oculta est√© dada por la ecuaci√≥n œÉ <sup>‚àí1</sup> ‚ãÖf (x), donde œÉ <sup>‚àí1</sup> es la funci√≥n inversa de œÉ.  Es decir, queremos que la salida ponderada de la capa oculta sea as√≠: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si esto tiene √©xito, entonces la salida de toda la red ser√° una buena aproximaci√≥n de f (x) (configur√© el desplazamiento de la neurona de salida a 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entonces su tarea es desarrollar un NS que se aproxime a la funci√≥n objetivo que se muestra arriba. Para comprender mejor lo que est√° sucediendo, le recomiendo que resuelva este problema dos veces. Por primera vez en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">art√≠culo original,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> haga clic en el gr√°fico y ajuste directamente las alturas de las diferentes protuberancias. Ser√° bastante f√°cil para usted obtener una buena aproximaci√≥n a la funci√≥n objetivo. El grado de aproximaci√≥n se estima por la desviaci√≥n promedio, la diferencia entre la funci√≥n objetivo y la funci√≥n que calcula la red. Su tarea es llevar la desviaci√≥n promedio a un valor m√≠nimo. La tarea se considera completada cuando la desviaci√≥n promedio no excede 0.40.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Despu√©s de lograr el √©xito, presione el bot√≥n Restablecer, que cambia aleatoriamente las pesta√±as. La segunda vez, no toque el gr√°fico, pero cambie los valores h en el lado izquierdo del diagrama, tratando de llevar la desviaci√≥n promedio a un valor de 0.40 o menos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Entonces, ha encontrado todos los elementos necesarios para que la red calcule aproximadamente la funci√≥n f (x)! La aproximaci√≥n result√≥ ser aproximada, pero podemos mejorar f√°cilmente el resultado simplemente aumentando el n√∫mero de pares de neuronas ocultas, lo que aumentar√° el n√∫mero de protuberancias. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En particular, es f√°cil convertir todos los datos encontrados en la vista est√°ndar con la parametrizaci√≥n utilizada para NS. D√©jame recordarte r√°pidamente c√≥mo funciona esto. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En la primera capa, todos los pesos tienen un valor constante grande, por ejemplo, w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Los desplazamientos de las neuronas ocultas se calculan a trav√©s de b = ‚àíws. Entonces, por ejemplo, para la segunda neurona oculta, s = 0.2 se convierte en b = ‚àí1000 √ó 0.2 = ‚àí200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La √∫ltima capa de la escala est√° determinada por los valores de h. Entonces, por ejemplo, el valor que seleccion√≥ para la primera h, h = -0.2, significa que los pesos de salida de las dos neuronas ocultas superiores son -0.2 y 0.2, respectivamente. Y as√≠ sucesivamente, para toda la capa de pesos de salida. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, el desplazamiento de la neurona de salida es 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Y eso es todo: obtuvimos una descripci√≥n completa del NS, que calcula bien la funci√≥n objetivo inicial. Y entendemos c√≥mo mejorar la calidad de la aproximaci√≥n mejorando la cantidad de neuronas ocultas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adem√°s, en nuestra funci√≥n objetivo original f (x) = 0.2 + 0.4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0.3sin (15x) + 0.05cos (50x) no es nada especial. </font><font style="vertical-align: inherit;">Se podr√≠a utilizar un procedimiento similar para cualquier funci√≥n continua en los intervalos de [0,1] a [0,1]. </font><font style="vertical-align: inherit;">De hecho, usamos nuestro NS de una sola capa para construir una tabla de b√∫squeda para una funci√≥n. </font><font style="vertical-align: inherit;">Y podemos tomar esta idea como base para obtener una prueba generalizada de universalidad.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Funci√≥n de muchos par√°metros </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Extendemos nuestros resultados al caso de un conjunto de variables de entrada. Suena complicado, pero todas las ideas que necesitamos ya se pueden entender para el caso con solo dos variables entrantes. Por lo tanto, consideramos el caso con dos variables entrantes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comencemos mirando lo que sucede cuando una neurona tiene dos entradas: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tenemos entradas x e y, con los pesos correspondientes w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y compensaci√≥n b de la neurona. Establecemos el peso de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en 0 y jugamos con el primero, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , y compensamos b para ver c√≥mo afectan la salida de la neurona: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como puede ver, con w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0, la entrada y no afecta la salida de la neurona. Todo sucede como si x es la √∫nica entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dado esto, ¬øqu√© crees que suceder√° cuando aumentemos el peso de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 y w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> deje 0? Si esto no le resulta claro de inmediato, piense un poco sobre este tema. Luego mire el siguiente video, que muestra lo que suceder√°:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como antes, con un aumento en el peso de entrada, la salida se acerca a la forma del paso. La diferencia es que nuestra funci√≥n de paso ahora se encuentra en tres dimensiones. Como antes, podemos mover la ubicaci√≥n de los pasos cambiando el desplazamiento. El √°ngulo estar√° en el punto s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos a rehacer el diagrama para que el par√°metro sea la ubicaci√≥n del paso: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">suponemos que el peso de entrada de x es de gran importancia: utilic√© w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 y el peso w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. El n√∫mero en la neurona es la posici√≥n del paso, y la x de arriba nos recuerda que movemos el paso a lo largo del eje x. Naturalmente, es bastante posible obtener una funci√≥n de paso a lo largo del eje y, haciendo que el peso entrante para y sea grande (por ejemplo, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), y el peso para x es 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">el n√∫mero en la neurona, nuevamente, indica la posici√≥n del paso, yy encima nos recuerda que movemos el paso a lo largo del eje y. Pude designar directamente los pesos para x e y, pero no lo hice, porque eso ensuciar√≠a el gr√°fico. Pero tenga en cuenta que el marcador y indica que el peso para y es grande y para x es 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Podemos usar las funciones de pasos que acabamos de dise√±ar para calcular la funci√≥n de protrusi√≥n tridimensional. Para hacer esto, tomamos dos neuronas, cada una de las cuales calcular√° una funci√≥n de paso a lo largo del eje x. Luego combinamos estas funciones de paso con los pesos h y ‚Äìh, donde h es la altura de protuberancia deseada. Todo esto se puede ver en el siguiente diagrama:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intenta cambiar el valor de h. Vea c√≥mo se relaciona con los pesos de la red. Y c√≥mo cambia la altura de la funci√≥n de protrusi√≥n a la derecha. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n intente cambiar el punto del paso, cuyo valor se establece en 0,30 en la neurona oculta superior. Vea c√≥mo cambia la forma de la protuberancia. ¬øQu√© sucede si lo mueve m√°s all√° del punto 0.70 asociado con la neurona oculta inferior? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendimos a construir la funci√≥n de protrusi√≥n a lo largo del eje x. Naturalmente, podemos hacer que la protuberancia funcione f√°cilmente a lo largo del eje y, utilizando funciones de dos pasos a lo largo del eje y. Recuerde que podemos hacer esto haciendo grandes pesos en la entrada y, y configurando el peso 0 en la entrada x. Y entonces, qu√© pasa:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Se ve casi id√©ntico a la red anterior! El √∫nico cambio visible son peque√±os marcadores y en neuronas ocultas. Nos recuerdan que producen funciones escalonadas para y, y no para x, por lo que el peso en la entrada y es muy grande, y en la entrada x es cero, y no al rev√©s. Como antes, decid√≠ no mostrarlo directamente, para no saturar la imagen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Veamos qu√© sucede si agregamos dos funciones de protrusi√≥n, una a lo largo del eje x, la otra a lo largo del eje y, ambas de altura h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">para simplificar el diagrama de conexi√≥n con peso cero, omit√≠. Hasta ahora, he dejado peque√±os marcadores x e y en neuronas ocultas para recordar en qu√© direcciones se calculan las funciones de protrusi√≥n. M√°s tarde los rechazaremos, ya que est√°n implicados por la variable entrante.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intente cambiar el par√°metro h. </font><font style="vertical-align: inherit;">Como puede ver, debido a esto, los pesos de salida cambian, as√≠ como los pesos de ambas funciones de protrusi√≥n, x e y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nuestra </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">creaci√≥n es un </font><font style="vertical-align: inherit;">poco como una "funci√≥n de torre": </font><font style="vertical-align: inherit;">si podemos crear tales funciones de torre, podemos usarlas para aproximar funciones arbitrarias simplemente agregando torres de varias alturas en diferentes lugares: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por supuesto, a√∫n no hemos llegado a la creaci√≥n de una funci√≥n de torre arbitraria. </font><font style="vertical-align: inherit;">Hasta ahora, hemos construido algo as√≠ como una torre central de altura 2h con una meseta de altura h que la rodea. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero podemos hacer que una torre funcione. </font><font style="vertical-align: inherit;">Recuerde que previamente mostramos c√≥mo se pueden usar las neuronas para implementar la declaraci√≥n if-then-else:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Era una neurona de una entrada.  Y necesitamos aplicar una idea similar a la salida combinada de neuronas ocultas: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Si elegimos el umbral correcto, por ejemplo, 3h / 2, apretado entre la altura de la meseta y la altura de la torre central, podemos aplastar la meseta a cero y dejar solo una torre. <br><br>  Imagina c√≥mo hacer esto?  Intenta experimentar con la siguiente red.  Ahora estamos trazando la salida de toda la red, y no solo la salida ponderada de la capa oculta.  Esto significa que agregamos el t√©rmino de compensaci√≥n a la salida ponderada de la capa oculta y aplicamos el sigmoide.  ¬øPuedes encontrar los valores para h y b para los que obtienes una torre?  Si se atasca en este punto, aqu√≠ hay dos consejos: (1) para que la neurona saliente demuestre el comportamiento correcto en el estilo if-then-else, necesitamos que los pesos entrantes (todos h o ‚Äìh) sean grandes;  (2) el valor de b determina la escala del umbral if-then-else. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Con los par√°metros predeterminados, la salida es similar a una versi√≥n aplanada del diagrama anterior, con una torre y una meseta.  Para obtener el comportamiento deseado, debe aumentar el valor de h.  Esto nos dar√° el comportamiento umbral de if-then-else.  En segundo lugar, para establecer correctamente el umbral, uno debe elegir b ‚âà ‚àí3h / 2. <br><br>  Esto es lo que parece para h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Su navegador no admite video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Incluso para valores relativamente modestos de h, obtenemos una buena funci√≥n de torre.  Y, por supuesto, podemos obtener un resultado arbitrariamente hermoso aumentando h a√∫n m√°s y manteniendo el sesgo en el nivel b = ‚àí3h / 2. <br><br>  Intentemos unir dos redes para contar dos funciones de torre diferentes.  Para aclarar las funciones respectivas de las dos subredes, las coloco en rect√°ngulos separados: cada una de ellas calcula la funci√≥n de la torre utilizando la t√©cnica descrita anteriormente.  El gr√°fico de la derecha muestra la salida ponderada de la segunda capa oculta, es decir, la combinaci√≥n ponderada de las funciones de la torre. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  En particular, se puede ver que al cambiar el peso en la √∫ltima capa, puede cambiar la altura de las torres de salida. <br><br>  La misma idea le permite calcular tantas torres como desee.  Podemos hacerlos arbitrariamente delgados y altos.  Como resultado, garantizamos que la salida ponderada de la segunda capa oculta se aproxima a cualquier funci√≥n deseada de dos variables: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  En particular, al hacer que la salida ponderada de la segunda capa oculta se aproxime bien a œÉ <sup>‚àí1</sup> ‚ãÖf, garantizamos que la salida de nuestra red ser√° una buena aproximaci√≥n de la funci√≥n deseada f. <br><br>  ¬øQu√© pasa con las funciones de muchas variables? <br><br>  Tratemos de tomar tres variables, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  ¬øSe puede usar la siguiente red para calcular la funci√≥n de la torre en cuatro dimensiones? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Aqu√≠ x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> denotan la entrada de red.  s <sub>1</sub> , t <sub>1,</sub> y as√≠ sucesivamente: puntos de paso para las neuronas, es decir, todos los pesos en la primera capa son grandes, y los desplazamientos se asignan de modo que los puntos de los pasos sean s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... Los pesos en la segunda capa se alternan, + h, ‚àíh, donde h es un n√∫mero muy grande.  El desplazamiento de salida es ‚àí5h / 2. <br><br>  La red calcula una funci√≥n igual a 1 en tres condiciones: x <sub>1</sub> est√° entre s <sub>1</sub> y t <sub>1</sub> ;  x <sub>2</sub> est√° entre s <sub>2</sub> y t <sub>2</sub> ;  x <sub>3</sub> est√° entre s <sub>3</sub> y t <sub>3</sub> .  La red es 0 en todos los dem√°s lugares.  Esta es una torre en la que 1 es una peque√±a porci√≥n del espacio de entrada, y 0 es todo lo dem√°s. <br><br>  Al pegar muchas de esas redes, podemos obtener tantas torres como queramos y aproximar una funci√≥n arbitraria de tres variables.  La misma idea funciona en m dimensiones.  Solo el desplazamiento de salida (‚àím + 1/2) h se cambia para exprimir adecuadamente los valores deseados y eliminar la meseta. <br><br>  Bueno, ahora sabemos c√≥mo usar NS para aproximar la funci√≥n real de muchas variables.  ¬øQu√© pasa con las funciones vectoriales f (x <sub>1</sub> , ..., x <sub>m</sub> ) ‚àà R <sup>n</sup> ?  Por supuesto, dicha funci√≥n puede considerarse simplemente como n funciones reales separadas f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ), y as√≠ sucesivamente.  Y luego simplemente pegamos todas las redes juntas.  As√≠ que es f√°cil resolverlo. <br><br><h3>  Desaf√≠o </h3><br><ul><li>  Vimos c√≥mo usar redes neuronales con dos capas ocultas para aproximar una funci√≥n arbitraria.  ¬øPuedes demostrar que esto es posible con una capa oculta?  Sugerencia: intente trabajar con solo dos variables de salida y demuestre que: (a) es posible obtener las funciones de los pasos no solo a lo largo de los ejes x o y, sino tambi√©n en una direcci√≥n arbitraria;  (b) sumando muchas construcciones del paso (a), es posible aproximar la funci√≥n de una torre redonda en lugar de una torre rectangular;  ¬© utilizando torres redondas, es posible aproximar una funci√≥n arbitraria.  El paso ¬© ser√° m√°s f√°cil de hacer usando el material presentado en este cap√≠tulo un poco m√°s abajo. </li></ul><br><h2>  Ir m√°s all√° de las neuronas sigmoideas </h2><br>  Hemos demostrado que una red de neuronas sigmoideas puede calcular cualquier funci√≥n.  Recuerde que en una neurona sigmoidea, las entradas x <sub>1</sub> , x <sub>2</sub> , ... se convierten en la salida en œÉ (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b), donde w <sub>j</sub> son los pesos, b es el sesgo y œÉ es el sigmoide. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  ¬øQu√© sucede si observamos otro tipo de neurona usando una funci√≥n de activaci√≥n diferente, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Es decir, suponemos que si una neurona tiene x <sub>1</sub> , x <sub>2</sub> , ... pesos w <sub>1</sub> , w <sub>2</sub> , ... y sesgo b, entonces se generar√° s (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b). <br><br>  Podemos usar esta funci√≥n de activaci√≥n para avanzar, al igual que en el caso del sigmoide.  Pruebe (en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo original</a> ) en el diagrama para levantar el peso, por ejemplo, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Como en el caso del sigmoide, debido a esto, la funci√≥n de activaci√≥n se comprime y, como resultado, se convierte en una muy buena aproximaci√≥n de la funci√≥n de paso.  Intente cambiar el desplazamiento, y ver√° que podemos cambiar la ubicaci√≥n del paso a cualquiera.  Por lo tanto, podemos usar los mismos trucos que antes para calcular cualquier funci√≥n deseada. <br><br>  ¬øQu√© propiedades debe tener s (z) para que esto funcione?  Debemos suponer que s (z) est√° bien definido como z ‚Üí ‚àí‚àû y z ‚Üí ‚àû.  Estos l√≠mites son dos valores aceptados por nuestra funci√≥n de paso.  Tambi√©n debemos suponer que estos l√≠mites son diferentes.  Si no fueran diferentes, los pasos no funcionar√≠an; ¬°simplemente habr√≠a un horario fijo!  Pero si la funci√≥n de activaci√≥n s (z) satisface estas propiedades, las neuronas basadas en ella son universalmente adecuadas para los c√°lculos. <br><br><h3>  Las tareas </h3><br><ul><li>  Anteriormente en el libro, conocimos un tipo diferente de neurona: una neurona lineal enderezada o una unidad lineal rectificada, ReLU.  Explica por qu√© tales neuronas no satisfacen las condiciones necesarias para la universalidad.  Encuentre evidencia de versatilidad que demuestre que las ReLU son universalmente adecuadas para la inform√°tica. </li><li>  Supongamos que estamos considerando neuronas lineales, con la funci√≥n de activaci√≥n s (z) = z.  Explica por qu√© las neuronas lineales no satisfacen las condiciones de universalidad.  Muestre que tales neuronas no pueden usarse para la computaci√≥n universal. </li></ul><br><h2>  Fix step function </h2><br>  Por el momento, asumimos que nuestras neuronas producen funciones escalonadas precisas.  Esta es una buena aproximaci√≥n, pero solo una aproximaci√≥n.  De hecho, hay una brecha estrecha de falla, que se muestra en el siguiente gr√°fico, donde las funciones no se comportan en absoluto como una funci√≥n de paso: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  En este per√≠odo de fracaso, mi explicaci√≥n de la universalidad no funciona. <br><br>  El fracaso no es tan aterrador.  Al establecer pesos de entrada suficientemente grandes, podemos hacer que estas brechas sean arbitrariamente peque√±as.  Podemos hacerlos mucho m√°s peque√±os que en el gr√°fico, invisibles a la vista.  Entonces quiz√°s no tengamos que preocuparnos por este problema. <br><br>  Sin embargo, me gustar√≠a tener alguna forma de resolverlo. <br><br>  Resulta que es f√°cil de resolver.  Veamos esta soluci√≥n para calcular funciones NS con solo una entrada y salida.  Las mismas ideas funcionar√°n para resolver el problema con una gran cantidad de entradas y salidas. <br><br>  En particular, supongamos que queremos que nuestra red calcule alguna funci√≥n f.  Como antes, intentamos hacer esto dise√±ando la red de manera que la salida ponderada de la capa oculta de neuronas sea œÉ <sup>‚àí1</sup> ‚ãÖf (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Si hacemos esto usando la t√©cnica descrita anteriormente, forzaremos a las neuronas ocultas a producir una secuencia de funciones de protrusi√≥n: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Por supuesto, exager√© el tama√±o de los intervalos de falla, para que fuera m√°s f√°cil de ver.  Debe quedar claro que si sumamos todas estas funciones de las protuberancias, obtenemos una aproximaci√≥n bastante buena de œÉ <sup>‚àí1</sup> ‚ãÖf (x) en todas partes, excepto los intervalos de falla. <br><br>  Pero, supongamos que en lugar de usar la aproximaci√≥n que acabamos de describir, usamos un conjunto de neuronas ocultas para calcular la aproximaci√≥n de la mitad de nuestra funci√≥n objetivo original, es decir, œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2.  Por supuesto, se ver√° como una versi√≥n a escala del √∫ltimo gr√°fico: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Y, supongamos que hacemos que un conjunto m√°s de neuronas ocultas calcule la aproximaci√≥n a œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2, sin embargo, en su base las protuberancias se desplazar√°n a la mitad de su ancho: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Ahora tenemos dos aproximaciones diferentes para œÉ - 1‚ãÖf (x) / 2.  Si sumamos estas dos aproximaciones, obtenemos una aproximaci√≥n general a œÉ - 1‚ãÖf (x).  Esta aproximaci√≥n general todav√≠a tendr√° imprecisiones en peque√±os intervalos.  Pero el problema ser√° menor que antes, porque los puntos que caen en los intervalos de falla de la primera aproximaci√≥n no caer√°n en los intervalos de falla de la segunda aproximaci√≥n.  Por lo tanto, la aproximaci√≥n en estos intervalos ser√° aproximadamente 2 veces mejor. <br><br>  Podemos mejorar la situaci√≥n agregando un gran n√∫mero, M, de aproximaciones superpuestas de la funci√≥n œÉ - 1‚ãÖf (x) / M.  Si todos sus intervalos de falla son lo suficientemente estrechos, cualquier corriente estar√° en solo uno de ellos.  Si utiliza un n√∫mero suficientemente grande de aproximaciones superpuestas de M, el resultado es una aproximaci√≥n general excelente. <br><br><h2>  Conclusi√≥n </h2><br>  ¬°La explicaci√≥n de universalidad discutida aqu√≠ definitivamente no puede llamarse una descripci√≥n pr√°ctica de c√≥mo contar funciones usando redes neuronales!  En este sentido, es m√°s como una prueba de la versatilidad de las puertas l√≥gicas NAND y m√°s.  Por lo tanto, b√°sicamente intent√© hacer que este dise√±o sea claro y f√°cil de seguir sin optimizar sus detalles.  Sin embargo, tratar de optimizar este dise√±o puede ser un ejercicio interesante e instructivo para usted. <br><br>  Aunque el resultado obtenido no se puede usar directamente para crear NS, es importante porque elimina la cuesti√≥n de la computabilidad de cualquier funci√≥n particular que use NS.  La respuesta a esa pregunta siempre ser√° positiva.  Por lo tanto, es correcto preguntar si alguna funci√≥n es computable, pero cu√°l es la forma correcta de calcularla. <br><br>  Nuestro dise√±o universal usa solo dos capas ocultas para calcular una funci√≥n arbitraria.  Como discutimos, es posible obtener el mismo resultado con una sola capa oculta.  Dado esto, puede preguntarse por qu√© necesitamos redes profundas, es decir, redes con una gran cantidad de capas ocultas.  ¬øNo podemos simplemente reemplazar estas redes por otras poco profundas que tienen una capa oculta? <br><br>  Aunque, en principio, es posible, existen buenas razones pr√°cticas para utilizar redes neuronales profundas.  Como se describe en el Cap√≠tulo 1, los NS profundos tienen una estructura jer√°rquica que les permite adaptarse bien para estudiar el conocimiento jer√°rquico, que son √∫tiles para resolver problemas reales.  M√°s espec√≠ficamente, cuando se resuelven problemas como el reconocimiento de patrones, es √∫til usar un sistema que comprenda no solo p√≠xeles individuales, sino tambi√©n conceptos cada vez m√°s complejos: desde bordes hasta formas geom√©tricas simples, y m√°s all√°, hasta escenas complejas que involucran varios objetos.  En cap√≠tulos posteriores veremos evidencia a favor del hecho de que las NS profundas ser√°n m√°s capaces de hacer frente al estudio de tales jerarqu√≠as de conocimiento que las superficiales.  Para resumir: la universalidad nos dice que NS puede calcular cualquier funci√≥n;  La evidencia emp√≠rica sugiere que los NS profundos se adaptan mejor al estudio de funciones √∫tiles para resolver muchos problemas del mundo real. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/461659/">https://habr.com/ru/post/461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461649/index.html">¬øC√≥mo salvar√© el mundo?</a></li>
<li><a href="../461651/index.html">Frontend Weekly Digest (22-28 de julio de 2019)</a></li>
<li><a href="../461653/index.html">Radio definida por software: ¬øc√≥mo funciona? Parte 10</a></li>
<li><a href="../461655/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 373 (22-28 de julio de 2019)</a></li>
<li><a href="../461657/index.html">Comprar Red Hat: ¬øayudar√° a la lucha del gigante azul por el liderazgo de la nube h√≠brida?</a></li>
<li><a href="../461661/index.html">Gu√≠a de desarrollo basada en componentes</a></li>
<li><a href="../461663/index.html">La historia de c√≥mo Linux trajo a Windows</a></li>
<li><a href="../461665/index.html">Zen2. La evoluci√≥n de la plataforma AM4 en el ejemplo de Ryzen 7 3700x</a></li>
<li><a href="../461669/index.html">PHP Digest No. 161 (15-29 de julio de 2019)</a></li>
<li><a href="../461673/index.html">8 consejos para programadores novatos o una retrospectiva de mi carrera</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>