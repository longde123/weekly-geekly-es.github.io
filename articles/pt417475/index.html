<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêê üßô üëò Codifica√ß√£o Glusterfs + apagamento: quando voc√™ precisar de muito, barato e confi√°vel üßóüèæ ‚õπÔ∏è üíú</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Poucas pessoas t√™m um Glaster na R√∫ssia, e qualquer experi√™ncia √© interessante. Temos grande e industrial e, a julgar pela discuss√£o no √∫ltimo post , ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Codifica√ß√£o Glusterfs + apagamento: quando voc√™ precisar de muito, barato e confi√°vel</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Poucas pessoas t√™m um Glaster na R√∫ssia, e qualquer experi√™ncia √© interessante.  Temos grande e industrial e, a julgar pela discuss√£o no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√∫ltimo post</a> , em demanda.  Falei sobre o in√≠cio da experi√™ncia de migra√ß√£o de backups do armazenamento corporativo para o Glusterfs. <br><br>  Isso n√£o √© suficientemente expl√≠cito.  N√£o paramos e decidimos coletar algo mais s√©rio.  Portanto, aqui falaremos sobre coisas como codifica√ß√£o de apagamento, sharding, reequil√≠brio e sua otimiza√ß√£o, teste de estresse e assim por diante. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  Mais teoria volum / subwolum </li><li>  reposi√ß√£o quente </li><li>  curar / curar completamente / reequilibrar </li><li>  Conclus√µes ap√≥s a reinicializa√ß√£o de 3 n√≥s (nunca fa√ßa isso) </li><li>  Como a grava√ß√£o em diferentes velocidades de diferentes VMs e on / off do shard afeta a carga do subvolume? </li><li>  reequilibrar ap√≥s a partida do disco </li><li>  reequil√≠brio r√°pido </li></ul><br><a name="habracut"></a><h3>  O que voce queria </h3><br>  <b>A tarefa √© simples:</b> coletar uma loja barata, mas confi√°vel.  O mais barato poss√≠vel, confi√°vel - para que n√£o seja assustador armazenar nossos pr√≥prios arquivos para venda.  Tchau.  Depois, ap√≥s longos testes e backups em outro sistema de armazenamento - tamb√©m no cliente. <br><br>  <b>Aplicativo (IO sequencial)</b> : <br><br>  - Backups <br>  - Infraestruturas de teste <br>  - Teste de armazenamento para arquivos de m√≠dia pesados. <br>  N√≥s estamos aqui. <br>  - Arquivo de batalha e infraestrutura de teste s√©ria <br>  - Armazenamento de dados importantes. <br><br>  Como na √∫ltima vez, o principal requisito √© a velocidade da rede entre inst√¢ncias do Glaster.  10G no come√ßo est√° bem. <br><br><h3>  Teoria: o que √© volume disperso? </h3><br>  O volume disperso √© baseado na tecnologia de apagamento de codifica√ß√£o (EC), que fornece prote√ß√£o bastante eficaz contra falhas de disco ou servidor.  √â como RAID 5 ou 6, mas n√£o realmente.  Ele armazena o fragmento codificado do arquivo para cada bloco de forma que apenas um subconjunto dos fragmentos armazenados nos briks restantes seja necess√°rio para restaurar o arquivo.  O n√∫mero de tijolos que podem estar indispon√≠veis sem perda de acesso aos dados √© configurado pelo administrador durante a cria√ß√£o do volume. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  O que √© um subvolume? </h3><br>  A ess√™ncia do subvolume na terminologia do GlusterFS √© manifestada juntamente com os volumes distribu√≠dos.  No apagamento com dispers√£o distribu√≠da, a codifica√ß√£o funcionar√° apenas na estrutura do subwoofer.  E no caso, por exemplo, com dados replicados distribu√≠dos ser√£o replicados dentro da estrutura do subwoofer. <br>  Cada um deles √© distribu√≠do em servidores diferentes, o que permite que eles percam ou saiam livremente para sincronizar.  Na figura, os servidores (f√≠sicos) s√£o marcados em verde, os sub-lobos s√£o pontilhados.  Cada um deles √© apresentado como um disco (volume) para o servidor de aplicativos: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Decidiu-se que a configura√ß√£o 4 + 2 distribu√≠da em 6 n√≥s parece bastante confi√°vel, podemos perder 2 servidores ou 2 discos em cada subwoofer, enquanto continuamos a ter acesso aos dados. <br><br>  T√≠nhamos √† disposi√ß√£o 6 DELL PowerEdge R510 antigos com 12 slots de disco e unidades SATA de 48x2TB 3.5.  Em princ√≠pio, se houver um servidor com 12 slots de disco e com unidades de at√© 12 TB no mercado, podemos coletar armazenamento de at√© 576 TB de espa√ßo √∫til.  Mas n√£o esque√ßa que, embora os tamanhos m√°ximos de HDD continuem a crescer de ano para ano, o desempenho deles permanece parado e a reconstru√ß√£o de um disco de 10 a 12 TB pode levar uma semana. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Cria√ß√£o de volume:</b> <br>  Uma descri√ß√£o detalhada de como preparar tijolos, voc√™ pode ler no meu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">post anterior</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Criamos, mas n√£o temos pressa de lan√ßar e montar, pois ainda precisamos aplicar v√°rios par√¢metros importantes. <br><br>  <b>O que temos:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Tudo parece bastante normal, mas h√° uma ressalva. <br><br>  <b>Consiste em gravar esse volume nos tijolos:</b> <br>  Os arquivos s√£o colocados um a um nos sub-lobos e n√£o s√£o distribu√≠dos uniformemente por eles; portanto, mais cedo ou mais tarde, veremos o tamanho e n√£o o tamanho do volume inteiro.  O tamanho m√°ximo do arquivo que podemos colocar neste reposit√≥rio √© o tamanho utiliz√°vel do subwoofer menos o espa√ßo j√° ocupado nele.  No meu caso, √© &lt;8 Tb. <br><br>  <b>O que fazer?</b>  <b>Como ser</b> <br>  Esse problema √© resolvido com o sharding ou o volume da faixa, mas, como a pr√°tica demonstrou, a faixa funciona muito mal. <br><br>  Portanto, tentaremos sharding. <br><br>  <b>O que √© sharding, em detalhes</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br>  <b>O que √© sharding, em suma</b> : <br>  Cada arquivo que voc√™ coloca em um volume ser√° dividido em partes (fragmentos), que s√£o organizados de maneira relativamente uniforme em sub-lobos.  O tamanho do shard √© especificado pelo administrador, o valor padr√£o √© 4 MB. <br><br>  <b>Ative o sharding ap√≥s criar um volume, mas antes de iniciar</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Expor tamanho caco (alguns dos melhores Dudes oVirt recomendar 512?):</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Empiricamente, verifica-se que o tamanho real do fragmento nos tijolos ao usar o volume disperso 4 + 2 √© igual ao tamanho do bloco de fragmento / 4, no nosso caso 512M / 4 = 128M. <br><br>  Cada fragmento, de acordo com a l√≥gica de codifica√ß√£o do apagamento, √© decomposto de acordo com os tijolos da estrutura do submundo com estas pe√ßas: 4 * 128M + 2 * 128M <br><br>  <b>Desenhe os casos de falha que o gluster sobrevive com esta configura√ß√£o:</b> <br>  Nesta configura√ß√£o, podemos sobreviver √† queda de 2 n√≥s ou 2 de qualquer disco no mesmo subvolume. <br><br>  Para os testes, decidimos colocar o armazenamento resultante em nossa nuvem e executar o fio a partir de m√°quinas virtuais. <br><br>  Ativamos a grava√ß√£o sequencial de 15 VMs e fazemos o seguinte. <br><br>  <b>Reinicializa√ß√£o do 1¬∫ n√≥:</b> <br>  17:09 <br>  Parece n√£o cr√≠tico (~ 5 segundos de indisponibilidade pelo par√¢metro ping.timeout). <br><br>  17:19 <br>  Lan√ßado curar completamente. <br>  O n√∫mero de entradas de recupera√ß√£o est√° apenas aumentando, provavelmente devido ao alto n√≠vel de grava√ß√£o no cluster. <br><br>  17:32 <br>  Foi decidido desativar a grava√ß√£o da VM. <br>  O n√∫mero de entradas de cura come√ßou a diminuir. <br><br>  17:50 <br>  curar feito. <br><br>  <b>Reinicialize 2 n√≥s:</b> <br><br>  <i>Os mesmos resultados s√£o observados como no 1¬∫ n√≥.</i> <br><br>  <b>Reinicialize 3 n√≥s:</b> <br>  <i>Ponto de montagem emitido O terminal de transporte n√£o est√° conectado, as VMs receberam ioerror.</i> <i><br></i>  <i>Depois de ligar os n√≥s, o Glaster se restaurou, sem interfer√™ncia do nosso lado, e o processo de tratamento come√ßou.</i> <br><br>  Mas 4 de 15 VMs n√£o puderam subir.  Vi erros no hypervisor: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Pagamento dif√≠cil com 3 n√≥s com sharding desativado</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Tamb√©m perdemos dados, n√£o √© poss√≠vel restaurar. <br><br>  <b>Pague gentilmente 3 n√≥s com sharding, haver√° corrup√ß√£o de dados?</b> <br>  H√°, mas muito menos (coincid√™ncia?), Perdi 3 de 30 unidades. <br><br>  <b>Conclus√µes:</b> <br><br><ol><li>  A cura desses arquivos trava indefinidamente, o reequil√≠brio n√£o ajuda.  Conclu√≠mos que os arquivos para os quais a grava√ß√£o ativa estava em andamento quando o terceiro n√≥ foi desativado s√£o perdidos para sempre. </li><li>  Nunca recarregue mais de 2 n√≥s em uma configura√ß√£o 4 + 2 em produ√ß√£o! </li><li>  Como n√£o perder dados se voc√™ realmente deseja reiniciar mais de 3 n√≥s?  P Pare a grava√ß√£o no ponto de montagem e / ou interrompa o volume. </li><li>  N√≥s ou tijolos devem ser substitu√≠dos o mais r√°pido poss√≠vel.  Para isso, √© altamente desej√°vel ter, por exemplo, 1-2 a la tijolos hot-spare em cada n√≥ para substitui√ß√£o r√°pida.  E mais um n√≥ sobressalente com tijolos em caso de despejo de n√≥. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Tamb√©m √© muito importante testar casos de substitui√ß√£o de unidades <br><br>  <b>Partidas de briks (discos):</b> <b><br></b>  <b>17:20</b> <br>  Nocauteamos um tijolo: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Voc√™ pode ver esse rebaixamento no momento da substitui√ß√£o do bloco (registro de 1 fonte): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  O processo de substitui√ß√£o √© bastante longo, com um pequeno n√≠vel de grava√ß√£o por cluster e configura√ß√µes padr√£o de 1 TB, leva cerca de um dia para se recuperar. <br><br>  <b>Par√¢metros ajust√°veis ‚Äã‚Äãpara tratamento:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Op√ß√£o: disperse.background-heals</i> <i><br></i>  <i>Valor padr√£o: 8</i> <i><br></i>  <i>Descri√ß√£o: Esta op√ß√£o pode ser usada para controlar o n√∫mero de curas paralelas</i> <i><br><br></i>  <i>Op√ß√£o: disperse.heal-wait-qlength</i> <i><br></i>  <i>Valor padr√£o: 128</i> <i><br></i>  <i>Descri√ß√£o: esta op√ß√£o pode ser usada para controlar o n√∫mero de curas que podem esperar</i> <i><br><br></i>  <i>Op√ß√£o: disperse.shd-max-threads</i> <i><br></i>  <i>Valor padr√£o: 1</i> <i><br></i>  <i>Descri√ß√£o: n√∫mero m√°ximo de curas paralelas que o SHD pode realizar por tijolo local.</i>  <i>Isso pode reduzir substancialmente os tempos de reparo, mas tamb√©m pode danificar seus tijolos se voc√™ n√£o tiver o hardware de armazenamento para suportar isso.</i> <i><br><br></i>  <i>Op√ß√£o: disperse.shd-wait-qlength</i> <i><br></i>  <i>Valor padr√£o: 1024</i> <i><br></i>  <i>Descri√ß√£o: esta op√ß√£o pode ser usada para controlar o n√∫mero de curas que podem esperar em SHD por subvolume</i> <i><br><br></i>  <i>Op√ß√£o: disperse.cpu-extensions</i> <i><br></i>  <i>Valor padr√£o: auto</i> <i><br></i>  <i>Descri√ß√£o: for√ßa as extens√µes da CPU a serem usadas para acelerar os c√°lculos do campo de Galois.</i> <i><br><br></i>  <i>Op√ß√£o: disperse.self-heal-window-size</i> <i><br></i>  <i>Valor padr√£o: 1</i> <i><br></i>  <i>Descri√ß√£o: n√∫mero m√°ximo de blocos (128 KB) por arquivo para o qual o processo de recupera√ß√£o autom√°tica seria aplicado simultaneamente.</i> <br><br>  Parou: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Com novos par√¢metros, 1 TB de dados foi conclu√≠do em 8 horas (3 vezes mais r√°pido!) <br><br>  <b>O momento desagrad√°vel √© que o resultado √© um brik maior do que era</b> <br><br>  <b>foi:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>tornou-se:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  √â necess√°rio entender.  Provavelmente a coisa est√° inflando discos finos.  Com a substitui√ß√£o subsequente do tijolo aumentado, o tamanho permaneceu o mesmo. <br><br>  <b>Rebalanceamento:</b> <br>  <i>Depois de expandir ou reduzir (sem migrar dados) um volume (usando os comandos add-brick e remove-brick respectivamente), √© necess√°rio reequilibrar os dados entre os servidores.</i>  <i>Em um volume n√£o replicado, todos os tijolos devem estar ativos para executar a opera√ß√£o de substitui√ß√£o de tijolos (op√ß√£o de in√≠cio).</i>  <i>Em um volume replicado, pelo menos um dos tijolos na r√©plica deve estar ativo.</i> <br><br>  <b>Modelando o reequil√≠brio:</b> <br><br>  <i>Op√ß√£o: cluster.rebal-throttle</i> <i><br></i>  <i>Valor padr√£o: normal</i> <i><br></i>  <i>Descri√ß√£o: define o n√∫mero m√°ximo de migra√ß√µes de arquivos paralelos permitidos em um n√≥ durante a opera√ß√£o de reequil√≠brio.</i>  <i>O valor padr√£o √© normal e permite um m√°ximo de [($ (unidades de processamento) - 4) / 2), 2] arquivos para b</i> <i><br></i>  <i>n√≥s migramos de cada vez.</i>  <i>O Lazy permitir√° que apenas um arquivo seja migrado por vez e o agressivo permitir√° no m√°ximo [[$ (unidades de processamento) - 4) / 2), 4]</i> <br><br>  <i>Op√ß√£o: cluster.lock-migration</i> <i><br></i>  <i>Valor padr√£o: desativado</i> <i><br></i>  <i>Descri√ß√£o: se ativado, esse recurso migrar√° os bloqueios posix associados a um arquivo durante o reequil√≠brio</i> <br><br>  <i>Op√ß√£o: cluster.weighted-rebalance</i> <i><br></i>  <i>Valor padr√£o: ativado</i> <i><br></i>  <i>Descri√ß√£o: quando ativado, os arquivos ser√£o alocados aos tijolos com uma probabilidade proporcional ao seu tamanho.</i>  <i>Caso contr√°rio, todos os tijolos ter√£o a mesma probabilidade (comportamento herdado).</i> <br><br>  <b>Compara√ß√£o da escrita e da leitura dos mesmos par√¢metros do fio (resultados mais detalhados dos testes de desempenho - no PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Se for interessante, compare a velocidade do rsync com o tr√°fego com os n√≥s do Glaster:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>Pode-se observar que aproximadamente 170 MB / s / tr√°fego a 110 MB / s / carga √∫til.</i>  <i>Acontece que isso representa 33% do tr√°fego adicional, al√©m de 1/3 da redund√¢ncia de Erasure Coding.</i> <br><br>  <b>O consumo de mem√≥ria no servidor com e sem carga n√£o muda:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>A carga no cluster hospeda com a carga m√°xima no volume:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417475/">https://habr.com/ru/post/pt417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../In146149/index.html">‡§ì‡§°‡•á‡§∏‡§æ ‡§Æ‡•á‡§Ç ‡§ó‡•ç‡§∞‡•Ä‡§∑‡•ç‡§Æ‡§ï‡§æ‡§≤‡•Ä‡§® ‡§¨‡§æ‡§∞‡§ï‡•à‡§Æ‡•ç‡§™</a></li>
<li><a href="../In146150/index.html">‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡•ç‡§≤‡•á‡§Ø‡§∞ ‡§ï‡•ã ‡§®‡•á‡§ü‡§ü‡•â‡§™ ‡§Æ‡•á‡§Ç ‡§ï‡•à‡§∏‡•á ‡§¨‡§¶‡§≤‡•á‡§Ç?</a></li>
<li><a href="../In146151/index.html">‡§è‡§ú‡§æ‡§á‡§≤ ‡§°‡§æ‡§á‡§µ ‡§°‡•á‡§™‡•ç‡§•: ‡§è‡§ú‡§æ‡§á‡§≤ ‡§á‡§µ‡•à‡§≤‡•ç‡§Ø‡•Ç‡§è‡§∂‡§® ‡§´‡•ç‡§∞‡•á‡§Æ‡§µ‡§∞‡•ç‡§ï</a></li>
<li><a href="../In146152/index.html">‡§ü‡•à‡§≤‡•á‡§Ç‡§ü ‡§Æ‡•à‡§™ ‡§∞‡§ø‡§ú‡•ç‡§Ø‡•Ç‡§Æ‡•á ‡§µ‡§ø‡§ú‡§º‡•Å‡§Ö‡§≤‡§æ‡§á‡§ú‡§º‡§∞ - ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§∞‡§æ‡§∏‡•ç‡§§‡•á ‡§™‡§∞ ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞‡•ç‡§∏</a></li>
<li><a href="../pt417473/index.html">Armazenamento confi√°vel com DRBD9 e Proxmox (parte 1: NFS)</a></li>
<li><a href="../pt417477/index.html">Secret√°ria quente</a></li>
<li><a href="../pt417479/index.html">Concatena√ß√£o de string mais r√°pida do tipo fa√ßa voc√™ mesmo no Go</a></li>
<li><a href="../pt417481/index.html">Sobre geradores no JavaScript ES6 e por que √© opcional estud√°-los</a></li>
<li><a href="../pt417483/index.html">Compara√ß√£o de estruturas JS: React, Vue e Hyperapp</a></li>
<li><a href="../pt417485/index.html">[marcador] Folha de dicas do administrador do sistema para ferramentas de rede Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>