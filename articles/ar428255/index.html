<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>๐ฎ ๐ป ๐คท๐พ ููู ุงูุชุนูู: ููููุฉ ุชุฏุฑูุจ ุดุจูุฉ ุนุตุจูุฉ ุนูู ุจูุงูุงุชู ุจุณุฑุนุฉ ๐ง ๐ ๐คน๐พ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ุฃุตุจุญ ุงูุชุนูู ุงูุขูู ุฃูุซุฑ ุณูููุฉ ุ ูููุงู ุงููุฒูุฏ ูู ุงููุฑุต ูุชุทุจูู ูุฐู ุงูุชูููููุฌูุง ุจุงุณุชุฎุฏุงู "ุงูููููุงุช ุงูุฌุงูุฒุฉ". ุนูู ุณุจูู ุงููุซุงู ุ ูุชูุญ ูู Transfer Learning ุง...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>ููู ุงูุชุนูู: ููููุฉ ุชุฏุฑูุจ ุดุจูุฉ ุนุตุจูุฉ ุนูู ุจูุงูุงุชู ุจุณุฑุนุฉ</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/binarydistrict/blog/428255/" style=";text-align:right;direction:rtl">  ุฃุตุจุญ ุงูุชุนูู ุงูุขูู ุฃูุซุฑ ุณูููุฉ ุ ูููุงู ุงููุฒูุฏ ูู ุงููุฑุต ูุชุทุจูู ูุฐู ุงูุชูููููุฌูุง ุจุงุณุชุฎุฏุงู "ุงูููููุงุช ุงูุฌุงูุฒุฉ".  ุนูู ุณุจูู ุงููุซุงู ุ ูุชูุญ ูู Transfer Learning ุงุณุชุฎุฏุงู ุงูุฎุจุฑุฉ ุงูููุชุณุจุฉ ูู ุญู ูุดููุฉ ูุงุญุฏุฉ ูุญู ูุดููุฉ ุฃุฎุฑู ููุงุซูุฉ.  ูุชู ุชุฏุฑูุจ ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุฃููุงู ุนูู ูููุฉ ูุจูุฑุฉ ูู ุงูุจูุงูุงุช ุ ุซู ุนูู ุงููุฌููุนุฉ ุงููุณุชูุฏูุฉ. <br><br><img src="https://habrastorage.org/webt/q-/wr/cn/q-wrcns6clfsv1n2k6gki-sdoea.jpeg" alt="ุงูุชุนุฑู ุนูู ุงูุทุนุงู"><br><br>  ูู ูุฐู ุงูููุงูุฉ ุณุฃุฎุจุฑู ุจููููุฉ ุงุณุชุฎุฏุงู ุทุฑููุฉ ููู ุงูุชุนูู ุจุงุณุชุฎุฏุงู ูุซุงู ุงูุชุนุฑู ุนูู ุงูุตูุฑ ูุน ุงูุทุนุงู.  ุณุฃุชุญุฏุซ ุนู ุฃุฏูุงุช ุงูุชุนูู ุงูุขูู ุงูุฃุฎุฑู ูู ูุฑุดุฉ ุนูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ุงูุชุนูู ุงูุขูู ูุงูุดุจูุงุช ุงูุนุตุจูุฉ ูููุทูุฑูู</a> . <br><a name="habracut"></a><br>  ุฅุฐุง ูุงุฌูุชูุง ูููุฉ ุงูุชุนุฑู ุนูู ุงูุตูุฑ ุ ูููููู ุงุณุชุฎุฏุงู ุงูุฎุฏูุฉ ุงูุฌุงูุฒุฉ.  ููุน ุฐูู ุ ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุ ูุณูุชุนูู ุนููู ุงูููุงู ุจุฐูู ุจููุณู. <br><br>  ุจุงููุณุจุฉ ูููุงู ูููุฐุฌูุฉ ูุซู ุชุตููู ุงูุตูุฑ ุ ููููู ุงุณุชุฎุฏุงู ุงูุจููุฉ ุงูุฌุงูุฒุฉ (AlexNet ู VGG ู Inception ู ResNet ููุง ุฅูู ุฐูู) ูุชุฏุฑูุจ ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุนูู ุจูุงูุงุชู.  ุชูุฌุฏ ุจุงููุนู ุชุทุจููุงุช ููุฐู ุงูุดุจูุงุช ุจุงุณุชุฎุฏุงู ุฃุทุฑ ูุฎุชููุฉ ุ ูุฐูู ูู ูุฐู ุงููุฑุญูุฉ ููููู ุงุณุชุฎุฏุงู ุฃุญุฏูุง ููุฑุจุน ุฃุณูุฏ ุ ุฏูู ุงูุฎูุถ ูู ูุจุฏุฃ ุนูููุง ุจุนูู. <br><br>  ููุน ุฐูู ุ ุชุทุงูุจ ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุงูุนูููุฉ ุจูููุงุช ูุจูุฑุฉ ูู ุงูุจูุงูุงุช ูุชูุงุฑุจ ุงูุชุนูู.  ูุบุงูุจูุง ูุง ุชููู ูููุชูุง ุงูุฎุงุตุฉ ูุง ุชูุฌุฏ ุจูุงูุงุช ูุงููุฉ ูุชุฏุฑูุจ ุฌููุน ุทุจูุงุช ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุจุดูู ุตุญูุญ.  ููู ุงูุชุนูู ูุญู ูุฐู ุงููุดููุฉ. <br><br><h1 style=";text-align:right;direction:rtl">  ููู ุงูุชุนูู ูุชุตููู ุงูุตูุฑ </h1><br>  ุนุงุฏุฉ ูุง ุชุญุชูู ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุงููุณุชุฎุฏูุฉ ูู ุงูุชุตููู ุนูู ุงูุฎูุงูุง ุงูุนุตุจูุฉ ุงููุงุชุฌุฉ ูู <code>N</code> ูู ุงูุทุจูุฉ ุงูุฃุฎูุฑุฉ ุ ุญูุซ <code>N</code> ูู ุนุฏุฏ ุงููุฆุงุช.  ูุชู ุงูุชุนุงูู ูุน ูุงููุงุช ุงููุฎุฑุฌุงุช ูุฐู ููุฌููุนุฉ ูู ุงุญุชูุงูุงุช ุงูุงูุชูุงุก ุฅูู ูุฆุฉ.  ูู ูููุชูุง ููุชุนุฑู ุนูู ุตูุฑ ุงูุทุนุงู ุ ูุฏ ูุฎุชูู ุนุฏุฏ ุงููุตูู ุนู ุฐูู ุงูููุฌูุฏ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฃุตููุฉ.  ูู ูุฐู ุงูุญุงูุฉ ุ ุณูุชุนูู ุนูููุง ุงูุชุฎูุต ุชูุงููุง ูู ูุฐู ุงูุทุจูุฉ ุงูุฃุฎูุฑุฉ ููุถุน ุทุจูุฉ ุฌุฏูุฏุฉ ุ ูุน ุงูุนุฏุฏ ุงูุตุญูุญ ูู ุงูุฎูุงูุง ุงูุนุตุจูุฉ ุงููุงุชุฌุฉ <br><br><img src="https://habrastorage.org/webt/u_/n3/k3/u_n3k3qpkps6nw9tjjzwc0njl-y.jpeg" alt="ููู ุงูุชุนูู"><br><br>  ุบุงูุจูุง ูุง ูุชู ุงุณุชุฎุฏุงู ุทุจูุฉ ูุชุตูุฉ ุจุงููุงูู ูู ููุงูุฉ ุดุจูุงุช ุงูุชุตููู.  ูุธุฑูุง ูุฃููุง ุงุณุชุจุฏููุง ูุฐู ุงูุทุจูุฉ ุ ูุฅู ุงุณุชุฎุฏุงู ุงูุฃูุฒุงู ุงููุฏุฑุจุฉ ูุณุจููุง ูู ูุนูู.  ุณูููู ุนููู ุชุฏุฑูุจู ูู ุงูุตูุฑ ุ ูุชููุฆุฉ ุฃูุฒุงูู ุจููู ุนุดูุงุฆูุฉ.  ูููู ุจุชุญููู ุงูุฃูุฒุงู ูุฌููุน ุงูุทุจูุงุช ุงูุฃุฎุฑู ูู ููุทุฉ ุชู ุชุฏุฑูุจูุง ูุณุจููุง. <br><br>  ููุงู ุงุณุชุฑุงุชูุฌูุงุช ูุฎุชููุฉ ููุฒูุฏ ูู ุงูุชุฏุฑูุจ ูููููุฐุฌ.  ุณูุณุชุฎุฏู ูุง ููู: ุณูููู ุจุชุฏุฑูุจ ุงูุดุจูุฉ ุจุงููุงูู ูู ุทุฑู ุฅูู ุขุฎุฑ (ูู <i>ุทุฑู ุฅูู ุทุฑู</i> ) ุ ููู ูููู ุจุฅุตูุงุญ ุงูุฃูุฒุงู ุงููุฏุฑุจุฉ ูุณุจููุง ููุณูุงุญ ููุง ุจุงูุชุนุฏูู ููููุงู ูุงูุชููู ูุน ุจูุงูุงุชูุง.  ูุฐู ุงูุนูููุฉ ุชุณูู <i>ุงูุถุจุท ุงูุฏููู</i> . <br><br><h1 style=";text-align:right;direction:rtl">  ุงูููููุงุช ุงููููููุฉ </h1><br>  ูุญู ุงููุดููุฉ ุ ูุญุชุงุฌ ุฅูู ุงูููููุงุช ุงูุชุงููุฉ: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ูุตู ูููุฐุฌ ุงูุดุจูุฉ ุงูุนุตุจูุฉ </li><li style=";text-align:right;direction:rtl">  ุฎุท ุฃูุงุจูุจ ุงูุชุนูู </li><li style=";text-align:right;direction:rtl">  ุฎุท ุฃูุงุจูุจ ุงูุชุฏุฎู </li><li style=";text-align:right;direction:rtl">  ุฃูุฒุงู ููุฏุฑุจุฉ ูุณุจููุง ููุฐุง ุงููููุฐุฌ </li><li style=";text-align:right;direction:rtl">  ุจูุงูุงุช ููุชุฏุฑูุจ ูุงููุตุงุฏูุฉ </li></ol><br><img src="https://habrastorage.org/webt/tf/xp/2o/tfxp2on4o-rxj6hnt4dij7u8vlk.jpeg" alt="ููููุงุช"><br><br>  ูู ูุซุงููุง ุ ุณุขุฎุฐ ุงูููููุงุช (1) ู (2) ู (3) ูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ุงููุณุชูุฏุน ุงูุฎุงุต ุจู</a> ุ ูุงูุฐู ูุญุชูู ุนูู ุงูููุฏ ุงูุฃูุซุฑ ุฎูุฉ - ููููู ุงูุชุดุงูู ุจุณูููุฉ ุฅุฐุง ููุช ุชุฑุบุจ ูู ุฐูู.  ุณูุชู ุชูููุฐ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ูุซุงููุง ุนูู</a> ุฅุทุงุฑ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">TensorFlow</a> ุงูุดููุฑ.  ูููู ุงูุนุซูุฑ ุนูู ุฃูุฒุงู ูุชุฏุฑุจุฉ ูุณุจููุง (4) ููุงุณุจุฉ ููุฅุทุงุฑ ุงููุญุฏุฏ ุฅุฐุง ูุงูุช ุชุชูุงูู ูุน ุฅุญุฏู ุงูุจููุงุช ุงูููุงุณูููุฉ.  ููุฌููุนุฉ ุจูุงูุงุช (5) ูููุธุงูุฑุฉ ุณูู ุฃุชูุงูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Food-101</a> . <br><br><h1 style=";text-align:right;direction:rtl">  ูููุฐุฌ </h1><br>  ููููุฐุฌ ุ ูุณุชุฎุฏู ุงูุดุจูุฉ ุงูุนุตุจูุฉ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">VGG</a> ุงูููุงุณูููุฉ (ุจุชุนุจูุฑ ุฃุฏู ุ <i>VGG19</i> ).  ุนูู ุงูุฑุบู ูู ุจุนุถ ุงูุนููุจ ุ ููุถุญ ูุฐุง ุงููููุฐุฌ ุฌูุฏุฉ ุนุงููุฉ ุฅูู ุญุฏ ูุง.  ุจุงูุฅุถุงูุฉ ุฅูู ุฐูู ุ ูู ุงูุณูู ุชุญููููุง.  ูู TensorFlow Slim ุ ูุจุฏู ูุตู ุงููููุฐุฌ ูุถุบูุทูุง ุชูุงููุง: <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vgg_19</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inputs, num_classes, is_training, scope=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'vgg_19'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, weight_decay=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0005</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, weights_regularizer=slim.l2_regularizer(weight_decay), biases_initializer=tf.zeros_initializer(), padding=<span class="hljs-string"><span class="hljs-string">'SAME'</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(scope, <span class="hljs-string"><span class="hljs-string">'vgg_19'</span></span>, [inputs]): net = slim.repeat(inputs, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">64</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv1'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool1'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">128</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv2'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool2'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">256</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv3'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool3'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv4'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool4'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv5'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool5'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Use conv2d instead of fully_connected layers net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop6') net = slim.conv2d(net, 4096, [1, 1], scope='fc7') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop7') net = slim.conv2d(net, num_classes, [1, 1], scope='fc8', activation_fn=None) net = tf.squeeze(net, [1, 2], name='fc8/squeezed') return net</span></span></code> </pre><br>  ูุชู ุชูุฒูู ุฃูุฒุงู VGG19 ุ ุงููุฏุฑุจุฉ ุนูู ImageNet ููุชูุงููุฉ ูุน TensorFlow ุ ูู ุงููุณุชูุฏุน ุนูู GitHub ูู ูุณู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ุงูููุงุฐุฌ ุงููุฏุฑุจุฉ ูุณุจููุง</a> . <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs">mkdir data &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz tar -xzf vgg_19_2016_08_28.tar.gz</code> </pre><br><h1 style=";text-align:right;direction:rtl">  ูุฌููุนุฉ ุงูุจูุงูุงุช </h1><br>  ูุนููุฉ ุชุฏุฑูุจ ูุงุนุชูุงุฏ ุ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุจูุงูุงุช <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ุงูุบุฐุงุก 101</a> ุงูุนุงูุฉ ุ ูุงูุชู ุชุญุชูู ุนูู ุฃูุซุฑ ูู 100 ุฃูู ุตูุฑุฉ ุบุฐุงุฆูุฉ ููุณูุฉ ุฅูู 101 ูุฆุฉ. <br><br><img src="https://habrastorage.org/webt/re/oh/pb/reohpbmt76_3qfzplzccsz-hbf8.jpeg" alt="ูุฌููุนุฉ ุจูุงูุงุช ุงูุบุฐุงุก 101"><br><br>  ุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชูุฑูุบูุง: <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz tar -xzf food-101.tar.gz</code> </pre><br>  ุชู ุชุตููู ุฎุท ุงูุจูุงูุงุช ูู ุชุฏุฑูุจูุง ุจุญูุซ ูุญุชุงุฌ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ุชุญููู ูุง ููู: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ูุงุฆูุฉ ุงููุฆุงุช (ุงููุฆุงุช) </li><li style=";text-align:right;direction:rtl">  ุงูุจุฑูุงูุฌ ุงูุชุนูููู: ูุงุฆูุฉ ูุณุงุฑุงุช ุงูุตูุฑ ููุงุฆูุฉ ุจุงูุฅุฌุงุจุงุช ุงูุตุญูุญุฉ </li><li style=";text-align:right;direction:rtl">  ูุฌููุนุฉ ุงูุชุญูู ูู ุงูุตุญุฉ: โโูุงุฆูุฉ ูุณุงุฑุงุช ุงูุตูุฑ ููุงุฆูุฉ ุงูุฅุฌุงุจุงุช ุงูุตุญูุญุฉ </li></ol><br>  ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุ ูุฃูุช ุจุญุงุฌุฉ ุฅูู ูุณุฑ ุงููุฌููุนุงุช ุจููุณู ูู ุฃุฌู <i>ุงูุชุฏุฑูุจ</i> <i>ูุงูุชุญูู ูู ุงูุตุญุฉ</i> .  ูุญุชูู Food-101 ุจุงููุนู ุนูู ูุซู ูุฐุง ุงููุณู ุ ููุชู ุชุฎุฒูู ูุฐู ุงููุนูููุงุช ูู ุฏููู ุงูุชุนุฑูู. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs">DATASET_ROOT = <span class="hljs-string"><span class="hljs-string">'data/food-101/'</span></span> train_data, val_data, classes = data.food101(DATASET_ROOT) num_classes = len(classes)</code> </pre><br>  ูุชู ููู ุฌููุน ุงููุธุงุฆู ุงูุฅุถุงููุฉ ุงููุณุคููุฉ ุนู ูุนุงูุฌุฉ ุงูุจูุงูุงุช ุฅูู ููู <code>data.py</code> ูููุตู: <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">data.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> os.path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> join <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> opj <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_ds_subset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img_root, list_fpath, classes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Parse a meta file with image paths and labels -&gt; img_root: path to the root of image folders -&gt; list_fpath: path to the file with the list (eg train.txt) -&gt; classes: list of class names &lt;- (list_of_img_paths, integer_labels) '''</span></span> fpaths = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: class_name, image_id = line.strip().split(<span class="hljs-string"><span class="hljs-string">'/'</span></span>) fpaths.append(opj(img_root, class_name, image_id+<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) labels.append(classes.index(class_name)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fpaths, labels <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">food101</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset_root)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Get lists of train and validation examples for Food-101 dataset -&gt; dataset_root: root of the Food-101 dataset &lt;- ((train_fpaths, train_labels), (val_fpaths, val_labels), classes) '''</span></span> img_root = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'images'</span></span>) train_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'train.txt'</span></span>) test_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'test.txt'</span></span>) classes_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'classes.txt'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(classes_list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: classes = [line.strip() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f] train_data = parse_ds_subset(img_root, train_list_fpath, classes) val_data = parse_ds_subset(img_root, test_list_fpath, classes) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_data, val_data, classes <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">imread_and_crop</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(fpath, inp_size, margin=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_crop=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Construct TF graph for image preparation: Read the file, crop and resize -&gt; fpath: path to the JPEG image file (TF node) -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin -&gt; random_crop: perform random crop or central crop &lt;- prepared image (TF node) '''</span></span> data = tf.read_file(fpath) img = tf.image.decode_jpeg(data, channels=<span class="hljs-number"><span class="hljs-number">3</span></span>) img = tf.image.convert_image_dtype(img, dtype=tf.float32) shape = tf.shape(img) crop_size = tf.minimum(shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) - <span class="hljs-number"><span class="hljs-number">2</span></span> * margin <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random_crop: img = tf.random_crop(img, (crop_size, crop_size, <span class="hljs-number"><span class="hljs-number">3</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-comment"><span class="hljs-comment"># central crop ho = (shape[0] - crop_size) // 2 wo = (shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = tf.image.resize_images(img, (inp_size, inp_size), method=tf.image.ResizeMethod.AREA) return img def train_dataset(data, batch_size, epochs, inp_size, margin): ''' Prepare training data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: training batch size -&gt; epochs: number of training epochs -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin &lt;- (dataset, number_of_train_iterations) ''' num_examples = len(data[0]) iters = (epochs * num_examples) // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, margin, random_crop=True) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.shuffle(buffer_size=num_examples) dataset = dataset.map(fpath_to_image) dataset = dataset.repeat(epochs) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters def val_dataset(data, batch_size, inp_size): ''' Prepare validation data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: validation batch size -&gt; inp_size: size of the network input (eg 224) &lt;- (dataset, number_of_val_iterations) ''' num_examples = len(data[0]) iters = num_examples // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, 0, random_crop=False) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.map(fpath_to_image) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters</span></span></code> </pre><br></div></div><br><h1 style=";text-align:right;direction:rtl">  ุชุฏุฑูุจ ูููุฐุฌู </h1><br>  ูุชููู ุฑูุฒ ุงูุชุฏุฑูุจ ุงููููุฐุฌู ูู ุงูุฎุทูุงุช ุงูุชุงููุฉ: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ุจูุงุก ุฎุทูุท ุจูุงูุงุช <i>ูุทุงุฑ / ุงูุชุญูู ูู ุงูุตุญุฉ</i> </li><li style=";text-align:right;direction:rtl">  ุจูุงุก ุงูุฑุณูู ุงูุจูุงููุฉ <i>ูููุทุงุฑุงุช / ุงูุชุญูู</i> (ุงูุดุจูุงุช) </li><li style=";text-align:right;direction:rtl">  ุฅุฑูุงู ุฏุงูุฉ ุชุตููู ุงูุฎุณุงุฆุฑ ( <i>ุฎุณุงุฑุฉ ุงูุฅูุชุฑูุจูุง</i> ) ุนุจุฑ ุงูุฑุณู ุงูุจูุงูู <i>ูููุทุงุฑ</i> </li><li style=";text-align:right;direction:rtl">  ุงูููุฏ ุงููุทููุจ ูุญุณุงุจ ุฏูุฉ ุงูุชูุจุคุงุช ุนูู ุนููุฉ ุงูุชุญูู ุฃุซูุงุก ุงูุชุฏุฑูุจ </li><li style=";text-align:right;direction:rtl">  ููุทู ูุชุญููู ุฌุฏุงูู ุชู ุชุฏุฑูุจูุง ูุณุจููุง ูู ููุทุฉ </li><li style=";text-align:right;direction:rtl">  ุฅูุดุงุก ููุงูู ูุฎุชููุฉ ููุชุฏุฑูุจ </li><li style=";text-align:right;direction:rtl">  ุฏูุฑุฉ ุงูุชุนูู ููุณูุง (ุงูุชุญุณูู ุงูุชูุฑุงุฑู) </li></ol><br>  ูุชู ุฅูุดุงุก ุงูุทุจูุฉ ุงูุฃุฎูุฑุฉ ูู ุงูุฑุณู ุงูุจูุงูู ุจุงูุนุฏุฏ ุงููุทููุจ ูู ุงูุฎูุงูุง ุงูุนุตุจูุฉ ููุชู ุงุณุชุจุนุงุฏูุง ูู ูุงุฆูุฉ ุงููุนููุงุช ุงูุชู ุชู ุชุญููููุง ูู ููุทุฉ ุชู ุชุฏุฑูุจูุง ูุณุจููุง. <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ููุฏ ุงูุชุฏุฑูุจ ุงููููุฐุฌู</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim tf.logging.set_verbosity(tf.logging.INFO) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### INPUT_SIZE = 224 RANDOM_CROP_MARGIN = 10 TRAIN_EPOCHS = 20 TRAIN_BATCH_SIZE = 64 VAL_BATCH_SIZE = 128 LR_START = 0.001 LR_END = LR_START / 1e4 MOMENTUM = 0.9 VGG_PRETRAINED_CKPT = 'data/vgg_19.ckpt' CHECKPOINT_DIR = 'checkpoints/vgg19_food' LOG_LOSS_EVERY = 10 CALC_ACC_EVERY = 500 ########################################################### ### Build training and validation data pipelines ########################################################### train_ds, train_iters = data.train_dataset(train_data, TRAIN_BATCH_SIZE, TRAIN_EPOCHS, INPUT_SIZE, RANDOM_CROP_MARGIN) train_ds_iterator = train_ds.make_one_shot_iterator() train_x, train_y = train_ds_iterator.get_next() val_ds, val_iters = data.val_dataset(val_data, VAL_BATCH_SIZE, INPUT_SIZE) val_ds_iterator = val_ds.make_initializable_iterator() val_x, val_y = val_ds_iterator.get_next() ########################################################### ### Construct training and validation graphs ########################################################### with tf.variable_scope('', reuse=tf.AUTO_REUSE): train_logits = model.vgg_19(train_x, num_classes, is_training=True) val_logits = model.vgg_19(val_x, num_classes, is_training=False) ########################################################### ### Construct training loss ########################################################### loss = tf.losses.sparse_softmax_cross_entropy( labels=train_y, logits=train_logits) tf.summary.scalar('loss', loss) ########################################################### ### Construct validation accuracy ### and related functions ########################################################### def calc_accuracy(sess, val_logits, val_y, val_iters): acc_total = 0.0 acc_denom = 0 for i in range(val_iters): logits, y = sess.run((val_logits, val_y)) y_pred = np.argmax(logits, axis=1) correct = np.count_nonzero(y == y_pred) acc_denom += y_pred.shape[0] acc_total += float(correct) tf.logging.info('Validating batch [{} / {}] correct = {}'.format( i, val_iters, correct)) acc_total /= acc_denom return acc_total def accuracy_summary(sess, acc_value, iteration): acc_summary = tf.Summary() acc_summary.value.add(tag="accuracy", simple_value=acc_value) sess._hooks[1]._summary_writer.add_summary(acc_summary, iteration) ########################################################### ### Define set of VGG variables to restore ### Create the Restorer ### Define init callback (used by monitored session) ########################################################### vars_to_restore = tf.contrib.framework.get_variables_to_restore( exclude=['vgg_19/fc8']) vgg_restorer = tf.train.Saver(vars_to_restore) def init_fn(scaffold, sess): vgg_restorer.restore(sess, VGG_PRETRAINED_CKPT) ########################################################### ### Create various training structures ########################################################### global_step = tf.train.get_or_create_global_step() lr = tf.train.polynomial_decay(LR_START, global_step, train_iters, LR_END) tf.summary.scalar('learning_rate', lr) optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=MOMENTUM) training_op = slim.learning.create_train_op( loss, optimizer, global_step=global_step) scaffold = tf.train.Scaffold(init_fn=init_fn) ########################################################### ### Create monitored session ### Run training loop ########################################################### with tf.train.MonitoredTrainingSession(checkpoint_dir=CHECKPOINT_DIR, save_checkpoint_secs=600, save_summaries_steps=30, scaffold=scaffold) as sess: start_iter = sess.run(global_step) for iteration in range(start_iter, train_iters): # Gradient Descent loss_value = sess.run(training_op) # Loss logging if iteration % LOG_LOSS_EVERY == 0: tf.logging.info('[{} / {}] Loss = {}'.format( iteration, train_iters, loss_value)) # Accuracy logging if iteration % CALC_ACC_EVERY == 0: sess.run(val_ds_iterator.initializer) acc_value = calc_accuracy(sess, val_logits, val_y, val_iters) accuracy_summary(sess, acc_value, iteration) tf.logging.info('[{} / {}] Validation accuracy = {}'.format( iteration, train_iters, acc_value))</span></span></code> </pre><br></div></div><br>  ุจุนุฏ ุจุฏุก ุงูุชุฏุฑูุจ ุ ููููู ุฅููุงุก ูุธุฑุฉ ุนูู ุชูุฏูู ุจุงุณุชุฎุฏุงู ุฃุฏุงุฉ TensorBoard ุ ุงูุชู ุชุฃุชู ูุฌูุนุฉ ูุน TensorFlow ูุชุนูู ุนูู ุชุตูุฑ ุงูููุงููุณ ุงููุฎุชููุฉ ูุงููุนููุงุช ุงูุฃุฎุฑู. <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs">tensorboard --logdir checkpoints/</code> </pre><br>  ูู ููุงูุฉ ุงูุชุฏุฑูุจ ูู TensorBoard ุ ูุฑู ุตูุฑุฉ ูุซุงููุฉ ุชูุฑูุจูุง: ุงูุฎูุงุถ ูู <i>ููุฏุงู ุงููุทุงุฑ</i> ูุฒูุงุฏุฉ ูู <i>ุฏูุฉ ุงูุชุญูู</i> <br><br><img src="https://habrastorage.org/webt/bk/hc/ay/bkhcayg7tn4nczu3dx2flgfukrc.jpeg" alt="ููุฏุงู ูุฏูุฉ TensorBoard"><br><br>  ููุชูุฌุฉ ูุฐูู ุ ูุญุตู ุนูู ุงูููุทุฉ ุงููุญููุธุฉ ูู <code>checkpoints/vgg19_food</code> ุ ูุงูุชู <code>checkpoints/vgg19_food</code> ุฃุซูุงุก ุงุฎุชุจุงุฑ ูููุฐุฌูุง ( <i>ุงูุงุณุชุฏูุงู</i> ). <br><br><h1 style=";text-align:right;direction:rtl">  ุงุฎุชุจุงุฑ ุงููููุฐุฌ </h1><br>  ุงุฎุชุจุงุฑ ูููุฐุฌูุง ุงูุขู.  ููููุงู ุจุฐูู: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ูููู ุจุจูุงุก ุฑุณู ุจูุงูู ุฌุฏูุฏ ูุตูู ุฎุตูุตูุง ููุงุณุชุฏูุงู ( <code>is_training=False</code> ) </li><li style=";text-align:right;direction:rtl">  ูู ุจุชุญููู ุฃูุฒุงู ูุฏุฑุจุฉ ูู ููุทุฉ </li><li style=";text-align:right;direction:rtl">  ูู ุจุชูุฒูู ุตูุฑุฉ ุงุฎุชุจุงุฑ ุงูุฅุฏุฎุงู ููุนุงูุฌุชูุง ูุณุจููุง. </li><li style=";text-align:right;direction:rtl">  ุฏุนูุง ูููุฏ ุงูุตูุฑุฉ ูู ุฎูุงู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ููุญุตู ุนูู ุงูุชูุจุค </li></ol><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">inference.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> imageio <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.transform <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> resize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### CLASSES_FPATH = 'data/food-101/meta/labels.txt' INP_SIZE = 224 # Input will be cropped and resized CHECKPOINT_DIR = 'checkpoints/vgg19_food' IMG_FPATH = 'data/food-101/images/bruschetta/3564471.jpg' ########################################################### ### Get all class names ########################################################### with open(CLASSES_FPATH, 'r') as f: classes = [line.strip() for line in f] num_classes = len(classes) ########################################################### ### Construct inference graph ########################################################### x = tf.placeholder(tf.float32, (1, INP_SIZE, INP_SIZE, 3), name='inputs') logits = model.vgg_19(x, num_classes, is_training=False) ########################################################### ### Create TF session and restore from a snapshot ########################################################### sess = tf.Session() snapshot_fpath = tf.train.latest_checkpoint(CHECKPOINT_DIR) restorer = tf.train.Saver() restorer.restore(sess, snapshot_fpath) ########################################################### ### Load and prepare input image ########################################################### def crop_and_resize(img, input_size): crop_size = min(img.shape[0], img.shape[1]) ho = (img.shape[0] - crop_size) // 2 wo = (img.shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = resize(img, (input_size, input_size), order=3, mode='reflect', anti_aliasing=True, preserve_range=True) return img img = imageio.imread(IMG_FPATH) img = img.astype(np.float32) img = crop_and_resize(img, INP_SIZE) img = img[None, ...] ########################################################### ### Run inference ########################################################### out = sess.run(logits, feed_dict={x:img}) pred_class = classes[np.argmax(out)] print('Input: {}'.format(IMG_FPATH)) print('Prediction: {}'.format(pred_class))</span></span></code> </pre><br></div></div><br><img src="https://habrastorage.org/webt/j6/e6/jv/j6e6jv72cuvsl3ztjo_392quidm.jpeg" alt="ุงูุงุณุชุฏูุงู"><br><br>  ุฌููุน ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุ ุจูุง ูู ุฐูู ุงูููุงุฑุฏ ุงููุงุฒูุฉ ูุฅูุดุงุก ุญุงููุฉ Docker ูุชุดุบูููุง ูุน ุฌููุน ุงูุฅุตุฏุงุฑุงุช ุงูุถุฑูุฑูุฉ ูู ุงูููุชุจุงุช ุ ููุฌูุฏุฉ ูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ูุฐุง ุงููุณุชูุฏุน</a> - ูู ููุช ูุฑุงุกุฉ ุงูููุงูุฉ ุ ูุฏ ูููู ุงูุฑูุฒ ูู ุงููุณุชูุฏุน ูุญุชูู ุนูู ุชุญุฏูุซุงุช. <br><br>  ูู ูุฑุดุฉ ุงูุนูู <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">"ุงูุชุนูู ุงูุขูู ูุงูุดุจูุงุช ุงูุนุตุจูุฉ ูููุทูุฑูู" ุ</a> ุณุฃููู ุจุชุญููู ุงูููุงู ุงูุฃุฎุฑู ูุชุนูู ุงูุขูุฉ ุ ูุณููุฏู ุงูุทูุงุจ ูุดุงุฑูุนูู ุจููุงูุฉ ุงูุฌูุณุฉ ุงูููุซูุฉ. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar428255/">https://habr.com/ru/post/ar428255/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar428239/index.html">ูุญุฑูุงุช ุฃูุฑุงุต ููุงุด ุนูู ูุดู ุนุงู 2019 - ูู ุจูุงูุง ุงููุงุถู ุฃู ูุง ุชุฒุงู ุถุฑูุฑุฉุ</a></li>
<li><a href="../ar428243/index.html">ุณูุนูู GeekBrains ูุบุฉ ุจุฑูุฌุฉ C ++</a></li>
<li><a href="../ar428249/index.html">ุชูููุงุช ุฅุฏุงุฑุฉ ุงูุทูุจ ุนูู ุงูููุงู: ุฏูุฌ ูุฑุงูุฒ ุงูุจูุงูุงุช ูู ูุฌููุนุงุช ููุงููุฉ ููููุงุฑุซ</a></li>
<li><a href="../ar428251/index.html">ุซุบุฑุฉ ุบุจูุฉ ูู ุชุทุจูู "My Beeline"</a></li>
<li><a href="../ar428253/index.html">ุงููุบุงุช ุงููุฏูุฌุฉ: ููุงุฐุง ููุงุ</a></li>
<li><a href="../ar428257/index.html">ุงูุจุญุซ: 95ูช ูู ุชุทุจููุงุช ุงูุฃุทูุงู ุจูุง ุฅุนูุงูุงุช</a></li>
<li><a href="../ar428259/index.html">ูุชุงุจ "ููุงุฐุง ูุญู ูุฎุทุฆูู. ุงูุชูููุฑ ูู ุงููุฎุงุฎ ูู ุงูุนูู ". ููุชุทูุงุช ุงูุฌุฒุก 2</a></li>
<li><a href="../ar428261/index.html">ุฃุณุงุจูุน ูุงุจุงููุฉ ูู ุญุฒุงู ุงูููููุจุงุช</a></li>
<li><a href="../ar428263/index.html">"ูุงู ูุฏู ุฃูุฏ ูุญููุฉ ุญููุง": ูุฐูุจ ุงููุงุนุจูู ุงููุญุชุฑููู ุฅูู ุตุงูุงุช ุงูุฃูุนุงุจ ุงูุฑูุงุถูุฉ</a></li>
<li><a href="../ar428265/index.html">ูููููุง ุงููุตูู ุฅูู ุณุทุญ ููุชุจ WinCE ูุชุดุบูู Doom ุนูู ุฑุงุณู ุงูุฐุจุฐุจุงุช Keysight DSOX1102G</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>