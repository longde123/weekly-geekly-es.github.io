<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ® ğŸš» ğŸ¤·ğŸ¾ Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù…: ÙƒÙŠÙÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¨Ø³Ø±Ø¹Ø© ğŸ§ ğŸ–• ğŸ¤¹ğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ø£ØµØ¨Ø­ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø£ÙƒØ«Ø± Ø³Ù‡ÙˆÙ„Ø© ØŒ ÙˆÙ‡Ù†Ø§Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙØ±Øµ Ù„ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©". Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ØŒ ÙŠØªÙŠØ­ Ù„Ùƒ Transfer Learning Ø§...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù…: ÙƒÙŠÙÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¨Ø³Ø±Ø¹Ø©</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/binarydistrict/blog/428255/" style=";text-align:right;direction:rtl">  Ø£ØµØ¨Ø­ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø£ÙƒØ«Ø± Ø³Ù‡ÙˆÙ„Ø© ØŒ ÙˆÙ‡Ù†Ø§Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙØ±Øµ Ù„ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©".  Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ØŒ ÙŠØªÙŠØ­ Ù„Ùƒ Transfer Learning Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ù…ÙƒØªØ³Ø¨Ø© ÙÙŠ Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø£Ø®Ø±Ù‰ Ù…Ù…Ø§Ø«Ù„Ø©.  ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ Ø¹Ù„Ù‰ ÙƒÙ…ÙŠØ© ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØŒ Ø«Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©. <br><br><img src="https://habrastorage.org/webt/q-/wr/cn/q-wrcns6clfsv1n2k6gki-sdoea.jpeg" alt="Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¹Ø§Ù…"><br><br>  ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‚Ø§Ù„Ø© Ø³Ø£Ø®Ø¨Ø±Ùƒ Ø¨ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ù…Ø¹ Ø§Ù„Ø·Ø¹Ø§Ù….  Ø³Ø£ØªØ­Ø¯Ø« Ø¹Ù† Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙÙŠ ÙˆØ±Ø´Ø© Ø¹Ù…Ù„ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„Ù„Ù…Ø·ÙˆØ±ÙŠÙ†</a> . <br><a name="habracut"></a><br>  Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡ØªÙ†Ø§ Ù…Ù‡Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©.  ÙˆÙ…Ø¹ Ø°Ù„Ùƒ ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ØŒ ÙØ³ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¨Ù†ÙØ³Ùƒ. <br><br>  Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ù‡Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ù…Ø«Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ø§Ù‡Ø²Ø© (AlexNet Ùˆ VGG Ùˆ Inception Ùˆ ResNet ÙˆÙ…Ø§ Ø¥Ù„Ù‰ Ø°Ù„Ùƒ) ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ.  ØªÙˆØ¬Ø¯ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø·Ø± Ù…Ø®ØªÙ„ÙØ© ØŒ Ù„Ø°Ù„Ùƒ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø­Ø¯Ù‡Ø§ ÙƒÙ…Ø±Ø¨Ø¹ Ø£Ø³ÙˆØ¯ ØŒ Ø¯ÙˆÙ† Ø§Ù„Ø®ÙˆØ¶ ÙÙŠ Ù…Ø¨Ø¯Ø£ Ø¹Ù…Ù„Ù‡Ø§ Ø¨Ø¹Ù…Ù‚. <br><br>  ÙˆÙ…Ø¹ Ø°Ù„Ùƒ ØŒ ØªØ·Ø§Ù„Ø¨ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ø¨ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„ØªØ¹Ù„Ù….  ÙˆØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ù‡Ù…ØªÙ†Ø§ Ø§Ù„Ø®Ø§ØµØ© Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.  Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙŠØ­Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©. <br><br><h1 style=";text-align:right;direction:rtl">  Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± </h1><br>  Ø¹Ø§Ø¯Ø© Ù…Ø§ ØªØ­ØªÙˆÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù†Ø§ØªØ¬Ø© Ù…Ù† <code>N</code> ÙÙŠ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØŒ Ø­ÙŠØ« <code>N</code> Ù‡ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª.  ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù‡Ø°Ù‡ ÙƒÙ…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ø§Ù†ØªÙ…Ø§Ø¡ Ø¥Ù„Ù‰ ÙØ¦Ø©.  ÙÙŠ Ù…Ù‡Ù…ØªÙ†Ø§ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ØµÙˆØ± Ø§Ù„Ø·Ø¹Ø§Ù… ØŒ Ù‚Ø¯ ÙŠØ®ØªÙ„Ù Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„ Ø¹Ù† Ø°Ù„Ùƒ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©.  ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© ØŒ Ø³ÙŠØªØ¹ÙŠÙ† Ø¹Ù„ÙŠÙ†Ø§ Ø§Ù„ØªØ®Ù„Øµ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ÙˆÙˆØ¶Ø¹ Ø·Ø¨Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø© ØŒ Ù…Ø¹ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØµØ­ÙŠØ­ Ù…Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù†Ø§ØªØ¬Ø© <br><br><img src="https://habrastorage.org/webt/u_/n3/k3/u_n3k3qpkps6nw9tjjzwc0njl-y.jpeg" alt="Ù†Ù‚Ù„ Ø§Ù„ØªØ¹Ù„Ù…"><br><br>  ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø¨Ù‚Ø© Ù…ØªØµÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø´Ø¨ÙƒØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ.  Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù†Ø§ Ø§Ø³ØªØ¨Ø¯Ù„Ù†Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø¨Ù‚Ø© ØŒ ÙØ¥Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù† ÙŠØ¹Ù…Ù„.  Ø³ÙŠÙƒÙˆÙ† Ø¹Ù„ÙŠÙƒ ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ù† Ø§Ù„ØµÙØ± ØŒ ÙˆØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù†Ù‡ Ø¨Ù‚ÙŠÙ… Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©.  Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ù† Ù„Ù‚Ø·Ø© ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§. <br><br>  Ù‡Ù†Ø§Ùƒ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.  Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø§ ÙŠÙ„ÙŠ: Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø·Ø±Ù Ø¥Ù„Ù‰ Ø¢Ø®Ø± (Ù…Ù† <i>Ø·Ø±Ù Ø¥Ù„Ù‰ Ø·Ø±Ù</i> ) ØŒ ÙˆÙ„Ù† Ù†Ù‚ÙˆÙ… Ø¨Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù‡Ø§ Ø¨Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§.  Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ØªØ³Ù…Ù‰ <i>Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚</i> . <br><br><h1 style=";text-align:right;direction:rtl">  Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© </h1><br>  Ù„Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ÙˆØµÙ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© </li><li style=";text-align:right;direction:rtl">  Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… </li><li style=";text-align:right;direction:rtl">  Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªØ¯Ø®Ù„ </li><li style=";text-align:right;direction:rtl">  Ø£ÙˆØ²Ø§Ù† Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ </li><li style=";text-align:right;direction:rtl">  Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø© </li></ol><br><img src="https://habrastorage.org/webt/tf/xp/2o/tfxp2on4o-rxj6hnt4dij7u8vlk.jpeg" alt="Ù…ÙƒÙˆÙ†Ø§Øª"><br><br>  ÙÙŠ Ù…Ø«Ø§Ù„Ù†Ø§ ØŒ Ø³Ø¢Ø®Ø° Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª (1) Ùˆ (2) Ùˆ (3) Ù…Ù† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø®Ø§Øµ Ø¨ÙŠ</a> ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ÙƒØ«Ø± Ø®ÙØ© - ÙŠÙ…ÙƒÙ†Ùƒ Ø§ÙƒØªØ´Ø§ÙÙ‡ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ.  Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ù…Ø«Ø§Ù„Ù†Ø§ Ø¹Ù„Ù‰</a> Ø¥Ø·Ø§Ø± <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">TensorFlow</a> Ø§Ù„Ø´Ù‡ÙŠØ±.  ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† Ù…ØªØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ (4) Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ§Øª Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©.  ÙƒÙ…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª (5) Ù„Ù„Ù…Ø¸Ø§Ù‡Ø±Ø© Ø³ÙˆÙ Ø£ØªÙ†Ø§ÙˆÙ„ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Food-101</a> . <br><br><h1 style=";text-align:right;direction:rtl">  Ù†Ù…ÙˆØ°Ø¬ </h1><br>  ÙƒÙ†Ù…ÙˆØ°Ø¬ ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">VGG</a> Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© (Ø¨ØªØ¹Ø¨ÙŠØ± Ø£Ø¯Ù‚ ØŒ <i>VGG19</i> ).  Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø¹ÙŠÙˆØ¨ ØŒ ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø¯ Ù…Ø§.  Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø°Ù„Ùƒ ØŒ Ù…Ù† Ø§Ù„Ø³Ù‡Ù„ ØªØ­Ù„ÙŠÙ„Ù‡Ø§.  ÙÙŠ TensorFlow Slim ØŒ ÙŠØ¨Ø¯Ùˆ ÙˆØµÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¶ØºÙˆØ·Ù‹Ø§ ØªÙ…Ø§Ù…Ù‹Ø§: <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vgg_19</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inputs, num_classes, is_training, scope=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'vgg_19'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, weight_decay=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0005</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, weights_regularizer=slim.l2_regularizer(weight_decay), biases_initializer=tf.zeros_initializer(), padding=<span class="hljs-string"><span class="hljs-string">'SAME'</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(scope, <span class="hljs-string"><span class="hljs-string">'vgg_19'</span></span>, [inputs]): net = slim.repeat(inputs, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">64</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv1'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool1'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">2</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">128</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv2'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool2'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">256</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv3'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool3'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv4'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool4'</span></span>) net = slim.repeat(net, <span class="hljs-number"><span class="hljs-number">4</span></span>, slim.conv2d, <span class="hljs-number"><span class="hljs-number">512</span></span>, [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'conv5'</span></span>) net = slim.max_pool2d(net, [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], scope=<span class="hljs-string"><span class="hljs-string">'pool5'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Use conv2d instead of fully_connected layers net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop6') net = slim.conv2d(net, 4096, [1, 1], scope='fc7') net = slim.dropout(net, 0.5, is_training=is_training, scope='drop7') net = slim.conv2d(net, num_classes, [1, 1], scope='fc8', activation_fn=None) net = tf.squeeze(net, [1, 2], name='fc8/squeezed') return net</span></span></code> </pre><br>  ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† VGG19 ØŒ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ ImageNet ÙˆÙ…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ TensorFlow ØŒ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¹Ù„Ù‰ GitHub Ù…Ù† Ù‚Ø³Ù… <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§</a> . <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs">mkdir data &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz tar -xzf vgg_19_2016_08_28.tar.gz</code> </pre><br><h1 style=";text-align:right;direction:rtl">  Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª </h1><br>  ÙƒØ¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø¹ØªÙ…Ø§Ø¯ ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„ØºØ°Ø§Ø¡ 101</a> Ø§Ù„Ø¹Ø§Ù…Ø© ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 100 Ø£Ù„Ù ØµÙˆØ±Ø© ØºØ°Ø§Ø¦ÙŠØ© Ù…Ù‚Ø³Ù…Ø© Ø¥Ù„Ù‰ 101 ÙØ¦Ø©. <br><br><img src="https://habrastorage.org/webt/re/oh/pb/reohpbmt76_3qfzplzccsz-hbf8.jpeg" alt="Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØºØ°Ø§Ø¡ 101"><br><br>  ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙØ±ÙŠØºÙ‡Ø§: <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> data wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz tar -xzf food-101.tar.gz</code> </pre><br>  ØªÙ… ØªØµÙ…ÙŠÙ… Ø®Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ØªØ¯Ø±ÙŠØ¨Ù†Ø§ Ø¨Ø­ÙŠØ« Ù†Ø­ØªØ§Ø¬ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…Ø§ ÙŠÙ„ÙŠ: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØ¦Ø§Øª (Ø§Ù„ÙØ¦Ø§Øª) </li><li style=";text-align:right;direction:rtl">  Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© </li><li style=";text-align:right;direction:rtl">  Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©: â€‹â€‹Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© </li></ol><br>  Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ØŒ ÙØ£Ù†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ÙƒØ³Ø± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨Ù†ÙØ³Ùƒ Ù…Ù† Ø£Ø¬Ù„ <i>Ø§Ù„ØªØ¯Ø±ÙŠØ¨</i> <i>ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©</i> .  ÙŠØ­ØªÙˆÙŠ Food-101 Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ù…Ø«Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ØŒ ÙˆÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ø±ÙŠÙ. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs">DATASET_ROOT = <span class="hljs-string"><span class="hljs-string">'data/food-101/'</span></span> train_data, val_data, classes = data.food101(DATASET_ROOT) num_classes = len(classes)</code> </pre><br>  ÙŠØªÙ… Ù†Ù‚Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ù <code>data.py</code> Ù…Ù†ÙØµÙ„: <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">data.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> os.path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> join <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> opj <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_ds_subset</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img_root, list_fpath, classes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Parse a meta file with image paths and labels -&gt; img_root: path to the root of image folders -&gt; list_fpath: path to the file with the list (eg train.txt) -&gt; classes: list of class names &lt;- (list_of_img_paths, integer_labels) '''</span></span> fpaths = [] labels = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: class_name, image_id = line.strip().split(<span class="hljs-string"><span class="hljs-string">'/'</span></span>) fpaths.append(opj(img_root, class_name, image_id+<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) labels.append(classes.index(class_name)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fpaths, labels <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">food101</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset_root)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Get lists of train and validation examples for Food-101 dataset -&gt; dataset_root: root of the Food-101 dataset &lt;- ((train_fpaths, train_labels), (val_fpaths, val_labels), classes) '''</span></span> img_root = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'images'</span></span>) train_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'train.txt'</span></span>) test_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'test.txt'</span></span>) classes_list_fpath = opj(dataset_root, <span class="hljs-string"><span class="hljs-string">'meta'</span></span>, <span class="hljs-string"><span class="hljs-string">'classes.txt'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(classes_list_fpath, <span class="hljs-string"><span class="hljs-string">'r'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: classes = [line.strip() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f] train_data = parse_ds_subset(img_root, train_list_fpath, classes) val_data = parse_ds_subset(img_root, test_list_fpath, classes) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_data, val_data, classes <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">imread_and_crop</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(fpath, inp_size, margin=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_crop=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Construct TF graph for image preparation: Read the file, crop and resize -&gt; fpath: path to the JPEG image file (TF node) -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin -&gt; random_crop: perform random crop or central crop &lt;- prepared image (TF node) '''</span></span> data = tf.read_file(fpath) img = tf.image.decode_jpeg(data, channels=<span class="hljs-number"><span class="hljs-number">3</span></span>) img = tf.image.convert_image_dtype(img, dtype=tf.float32) shape = tf.shape(img) crop_size = tf.minimum(shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) - <span class="hljs-number"><span class="hljs-number">2</span></span> * margin <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random_crop: img = tf.random_crop(img, (crop_size, crop_size, <span class="hljs-number"><span class="hljs-number">3</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-comment"><span class="hljs-comment"># central crop ho = (shape[0] - crop_size) // 2 wo = (shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = tf.image.resize_images(img, (inp_size, inp_size), method=tf.image.ResizeMethod.AREA) return img def train_dataset(data, batch_size, epochs, inp_size, margin): ''' Prepare training data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: training batch size -&gt; epochs: number of training epochs -&gt; inp_size: size of the network input (eg 224) -&gt; margin: cropping margin &lt;- (dataset, number_of_train_iterations) ''' num_examples = len(data[0]) iters = (epochs * num_examples) // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, margin, random_crop=True) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.shuffle(buffer_size=num_examples) dataset = dataset.map(fpath_to_image) dataset = dataset.repeat(epochs) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters def val_dataset(data, batch_size, inp_size): ''' Prepare validation data pipeline -&gt; data: (list_of_img_paths, integer_labels) -&gt; batch_size: validation batch size -&gt; inp_size: size of the network input (eg 224) &lt;- (dataset, number_of_val_iterations) ''' num_examples = len(data[0]) iters = num_examples // batch_size def fpath_to_image(fpath, label): img = imread_and_crop(fpath, inp_size, 0, random_crop=False) return img, label dataset = tf.data.Dataset.from_tensor_slices(data) dataset = dataset.map(fpath_to_image) dataset = dataset.batch(batch_size, drop_remainder=True) return dataset, iters</span></span></code> </pre><br></div></div><br><h1 style=";text-align:right;direction:rtl">  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ÙŠ </h1><br>  ÙŠØªÙƒÙˆÙ† Ø±Ù…Ø² Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ø¨Ù†Ø§Ø¡ Ø®Ø·ÙˆØ· Ø¨ÙŠØ§Ù†Ø§Øª <i>Ù‚Ø·Ø§Ø± / Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©</i> </li><li style=";text-align:right;direction:rtl">  Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© <i>Ù„Ù„Ù‚Ø·Ø§Ø±Ø§Øª / Ø§Ù„ØªØ­Ù‚Ù‚</i> (Ø§Ù„Ø´Ø¨ÙƒØ§Øª) </li><li style=";text-align:right;direction:rtl">  Ø¥Ø±ÙØ§Ù‚ Ø¯Ø§Ù„Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø³Ø§Ø¦Ø± ( <i>Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§</i> ) Ø¹Ø¨Ø± Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ <i>Ù„Ù„Ù‚Ø·Ø§Ø±</i> </li><li style=";text-align:right;direction:rtl">  Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ </li><li style=";text-align:right;direction:rtl">  Ù…Ù†Ø·Ù‚ Ù„ØªØ­Ù…ÙŠÙ„ Ø¬Ø¯Ø§ÙˆÙ„ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ù„Ù‚Ø·Ø© </li><li style=";text-align:right;direction:rtl">  Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠØ§ÙƒÙ„ Ù…Ø®ØªÙ„ÙØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ </li><li style=";text-align:right;direction:rtl">  Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ¹Ù„Ù… Ù†ÙØ³Ù‡Ø§ (Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙƒØ±Ø§Ø±ÙŠ) </li></ol><br>  ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆÙŠØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù„Ù‚Ø·Ø© ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§. <br><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">ÙƒÙˆØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim tf.logging.set_verbosity(tf.logging.INFO) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### INPUT_SIZE = 224 RANDOM_CROP_MARGIN = 10 TRAIN_EPOCHS = 20 TRAIN_BATCH_SIZE = 64 VAL_BATCH_SIZE = 128 LR_START = 0.001 LR_END = LR_START / 1e4 MOMENTUM = 0.9 VGG_PRETRAINED_CKPT = 'data/vgg_19.ckpt' CHECKPOINT_DIR = 'checkpoints/vgg19_food' LOG_LOSS_EVERY = 10 CALC_ACC_EVERY = 500 ########################################################### ### Build training and validation data pipelines ########################################################### train_ds, train_iters = data.train_dataset(train_data, TRAIN_BATCH_SIZE, TRAIN_EPOCHS, INPUT_SIZE, RANDOM_CROP_MARGIN) train_ds_iterator = train_ds.make_one_shot_iterator() train_x, train_y = train_ds_iterator.get_next() val_ds, val_iters = data.val_dataset(val_data, VAL_BATCH_SIZE, INPUT_SIZE) val_ds_iterator = val_ds.make_initializable_iterator() val_x, val_y = val_ds_iterator.get_next() ########################################################### ### Construct training and validation graphs ########################################################### with tf.variable_scope('', reuse=tf.AUTO_REUSE): train_logits = model.vgg_19(train_x, num_classes, is_training=True) val_logits = model.vgg_19(val_x, num_classes, is_training=False) ########################################################### ### Construct training loss ########################################################### loss = tf.losses.sparse_softmax_cross_entropy( labels=train_y, logits=train_logits) tf.summary.scalar('loss', loss) ########################################################### ### Construct validation accuracy ### and related functions ########################################################### def calc_accuracy(sess, val_logits, val_y, val_iters): acc_total = 0.0 acc_denom = 0 for i in range(val_iters): logits, y = sess.run((val_logits, val_y)) y_pred = np.argmax(logits, axis=1) correct = np.count_nonzero(y == y_pred) acc_denom += y_pred.shape[0] acc_total += float(correct) tf.logging.info('Validating batch [{} / {}] correct = {}'.format( i, val_iters, correct)) acc_total /= acc_denom return acc_total def accuracy_summary(sess, acc_value, iteration): acc_summary = tf.Summary() acc_summary.value.add(tag="accuracy", simple_value=acc_value) sess._hooks[1]._summary_writer.add_summary(acc_summary, iteration) ########################################################### ### Define set of VGG variables to restore ### Create the Restorer ### Define init callback (used by monitored session) ########################################################### vars_to_restore = tf.contrib.framework.get_variables_to_restore( exclude=['vgg_19/fc8']) vgg_restorer = tf.train.Saver(vars_to_restore) def init_fn(scaffold, sess): vgg_restorer.restore(sess, VGG_PRETRAINED_CKPT) ########################################################### ### Create various training structures ########################################################### global_step = tf.train.get_or_create_global_step() lr = tf.train.polynomial_decay(LR_START, global_step, train_iters, LR_END) tf.summary.scalar('learning_rate', lr) optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=MOMENTUM) training_op = slim.learning.create_train_op( loss, optimizer, global_step=global_step) scaffold = tf.train.Scaffold(init_fn=init_fn) ########################################################### ### Create monitored session ### Run training loop ########################################################### with tf.train.MonitoredTrainingSession(checkpoint_dir=CHECKPOINT_DIR, save_checkpoint_secs=600, save_summaries_steps=30, scaffold=scaffold) as sess: start_iter = sess.run(global_step) for iteration in range(start_iter, train_iters): # Gradient Descent loss_value = sess.run(training_op) # Loss logging if iteration % LOG_LOSS_EVERY == 0: tf.logging.info('[{} / {}] Loss = {}'.format( iteration, train_iters, loss_value)) # Accuracy logging if iteration % CALC_ACC_EVERY == 0: sess.run(val_ds_iterator.initializer) acc_value = calc_accuracy(sess, val_logits, val_y, val_iters) accuracy_summary(sess, acc_value, iteration) tf.logging.info('[{} / {}] Validation accuracy = {}'.format( iteration, train_iters, acc_value))</span></span></code> </pre><br></div></div><br>  Ø¨Ø¹Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù„Ù‚Ø§Ø¡ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø¯Ù…Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© TensorBoard ØŒ Ø§Ù„ØªÙŠ ØªØ£ØªÙŠ Ù…Ø¬Ù…Ø¹Ø© Ù…Ø¹ TensorFlow ÙˆØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØµÙˆØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰. <br><br><pre style=";text-align:right;direction:rtl"> <code class="bash hljs">tensorboard --logdir checkpoints/</code> </pre><br>  ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ TensorBoard ØŒ Ù†Ø±Ù‰ ØµÙˆØ±Ø© Ù…Ø«Ø§Ù„ÙŠØ© ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§: Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ <i>ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ù‚Ø·Ø§Ø±</i> ÙˆØ²ÙŠØ§Ø¯Ø© ÙÙŠ <i>Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚</i> <br><br><img src="https://habrastorage.org/webt/bk/hc/ay/bkhcayg7tn4nczu3dx2flgfukrc.jpeg" alt="ÙÙ‚Ø¯Ø§Ù† ÙˆØ¯Ù‚Ø© TensorBoard"><br><br>  ÙˆÙ†ØªÙŠØ¬Ø© Ù„Ø°Ù„Ùƒ ØŒ Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø·Ø© Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ <code>checkpoints/vgg19_food</code> ØŒ ÙˆØ§Ù„ØªÙŠ <code>checkpoints/vgg19_food</code> Ø£Ø«Ù†Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ ( <i>Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„</i> ). <br><br><h1 style=";text-align:right;direction:rtl">  Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ </h1><br>  Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Ø§Ù„Ø¢Ù†.  Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ: <br><br><ol style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù†Ù‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø¬Ø¯ÙŠØ¯ Ù…ØµÙ…Ù… Ø®ØµÙŠØµÙ‹Ø§ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ( <code>is_training=False</code> ) </li><li style=";text-align:right;direction:rtl">  Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ù…Ø¯Ø±Ø¨Ø© Ù…Ù† Ù„Ù‚Ø·Ø© </li><li style=";text-align:right;direction:rtl">  Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ ØµÙˆØ±Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§. </li><li style=";text-align:right;direction:rtl">  Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆÙ†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤ </li></ol><br><div class="spoiler" style=";text-align:right;direction:rtl">  <b class="spoiler_title">inference.py</b> <div class="spoiler_text" style=";text-align:right;direction:rtl"><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> imageio <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.transform <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> resize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-comment"><span class="hljs-comment">########################################################### ### Settings ########################################################### CLASSES_FPATH = 'data/food-101/meta/labels.txt' INP_SIZE = 224 # Input will be cropped and resized CHECKPOINT_DIR = 'checkpoints/vgg19_food' IMG_FPATH = 'data/food-101/images/bruschetta/3564471.jpg' ########################################################### ### Get all class names ########################################################### with open(CLASSES_FPATH, 'r') as f: classes = [line.strip() for line in f] num_classes = len(classes) ########################################################### ### Construct inference graph ########################################################### x = tf.placeholder(tf.float32, (1, INP_SIZE, INP_SIZE, 3), name='inputs') logits = model.vgg_19(x, num_classes, is_training=False) ########################################################### ### Create TF session and restore from a snapshot ########################################################### sess = tf.Session() snapshot_fpath = tf.train.latest_checkpoint(CHECKPOINT_DIR) restorer = tf.train.Saver() restorer.restore(sess, snapshot_fpath) ########################################################### ### Load and prepare input image ########################################################### def crop_and_resize(img, input_size): crop_size = min(img.shape[0], img.shape[1]) ho = (img.shape[0] - crop_size) // 2 wo = (img.shape[0] - crop_size) // 2 img = img[ho:ho+crop_size, wo:wo+crop_size, :] img = resize(img, (input_size, input_size), order=3, mode='reflect', anti_aliasing=True, preserve_range=True) return img img = imageio.imread(IMG_FPATH) img = img.astype(np.float32) img = crop_and_resize(img, INP_SIZE) img = img[None, ...] ########################################################### ### Run inference ########################################################### out = sess.run(logits, feed_dict={x:img}) pred_class = classes[np.argmax(out)] print('Input: {}'.format(IMG_FPATH)) print('Prediction: {}'.format(pred_class))</span></span></code> </pre><br></div></div><br><img src="https://habrastorage.org/webt/j6/e6/jv/j6e6jv72cuvsl3ztjo_392quidm.jpeg" alt="Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"><br><br>  Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø§ÙˆÙŠØ© Docker ÙˆØªØ´ØºÙŠÙ„Ù‡Ø§ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ØŒ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹</a> - ÙÙŠ ÙˆÙ‚Øª Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø© ØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ù…Ø² ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ«Ø§Øª. <br><br>  ÙÙŠ ÙˆØ±Ø´Ø© Ø§Ù„Ø¹Ù…Ù„ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">"Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„Ù„Ù…Ø·ÙˆØ±ÙŠÙ†" ØŒ</a> Ø³Ø£Ù‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø£Ø®Ø±Ù‰ Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© ØŒ ÙˆØ³ÙŠÙ‚Ø¯Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ Ù…Ø´Ø§Ø±ÙŠØ¹Ù‡Ù… Ø¨Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ù…ÙƒØ«ÙØ©. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar428255/">https://habr.com/ru/post/ar428255/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar428239/index.html">Ù…Ø­Ø±ÙƒØ§Øª Ø£Ù‚Ø±Ø§Øµ ÙÙ„Ø§Ø´ Ø¹Ù„Ù‰ ÙˆØ´Ùƒ Ø¹Ø§Ù… 2019 - Ù…Ù† Ø¨Ù‚Ø§ÙŠØ§ Ø§Ù„Ù…Ø§Ø¶ÙŠ Ø£Ùˆ Ù„Ø§ ØªØ²Ø§Ù„ Ø¶Ø±ÙˆØ±Ø©ØŸ</a></li>
<li><a href="../ar428243/index.html">Ø³ÙŠØ¹Ù„Ù… GeekBrains Ù„ØºØ© Ø¨Ø±Ù…Ø¬Ø© C ++</a></li>
<li><a href="../ar428249/index.html">ØªÙ‚Ù†ÙŠØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ§Ù‡: Ø¯Ù…Ø¬ Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù‚Ø§ÙˆÙ…Ø© Ù„Ù„ÙƒÙˆØ§Ø±Ø«</a></li>
<li><a href="../ar428251/index.html">Ø«ØºØ±Ø© ØºØ¨ÙŠØ© ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ "My Beeline"</a></li>
<li><a href="../ar428253/index.html">Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©: Ù„Ù…Ø§Ø°Ø§ Ù„ÙˆØ§ØŸ</a></li>
<li><a href="../ar428257/index.html">Ø§Ù„Ø¨Ø­Ø«: 95Ùª Ù…Ù† ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø¨Ù‡Ø§ Ø¥Ø¹Ù„Ø§Ù†Ø§Øª</a></li>
<li><a href="../ar428259/index.html">ÙƒØªØ§Ø¨ "Ù„Ù…Ø§Ø°Ø§ Ù†Ø­Ù† Ù…Ø®Ø·Ø¦ÙˆÙ†. Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ù„ÙØ®Ø§Ø® ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ ". Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„Ø¬Ø²Ø¡ 2</a></li>
<li><a href="../ar428261/index.html">Ø£Ø³Ø§Ø¨ÙŠØ¹ ÙŠØ§Ø¨Ø§Ù†ÙŠØ© ÙÙŠ Ø­Ø²Ø§Ù… Ø§Ù„ÙƒÙˆÙŠÙƒØ¨Ø§Øª</a></li>
<li><a href="../ar428263/index.html">"ÙƒØ§Ù† Ù„Ø¯ÙŠ Ø£ÙŠØ¯ Ù†Ø­ÙŠÙØ© Ø­Ù‚Ù‹Ø§": ÙŠØ°Ù‡Ø¨ Ø§Ù„Ù„Ø§Ø¹Ø¨ÙˆÙ† Ø§Ù„Ù…Ø­ØªØ±ÙÙˆÙ† Ø¥Ù„Ù‰ ØµØ§Ù„Ø§Øª Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©</a></li>
<li><a href="../ar428265/index.html">ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø³Ø·Ø­ Ù…ÙƒØªØ¨ WinCE ÙˆØªØ´ØºÙŠÙ„ Doom Ø¹Ù„Ù‰ Ø±Ø§Ø³Ù… Ø§Ù„Ø°Ø¨Ø°Ø¨Ø§Øª Keysight DSOX1102G</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>