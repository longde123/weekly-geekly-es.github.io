<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßõüèø üòí üöí Mit Bart, in dunkler Brille und im Profil: schwierige Situationen f√ºr Computer Vision üöß üì¢ üëó</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Technologien und Modelle f√ºr unser zuk√ºnftiges Computer-Vision-System wurden in verschiedenen Projekten unseres Unternehmens schrittweise entwickelt u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mit Bart, in dunkler Brille und im Profil: schwierige Situationen f√ºr Computer Vision</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/449120/"><img src="https://habrastorage.org/getpro/habr/post_images/027/06d/ef1/02706def16ee17a808ab04bef455cb83.jpg"><br><br>  Technologien und Modelle f√ºr unser zuk√ºnftiges Computer-Vision-System wurden in verschiedenen Projekten unseres Unternehmens schrittweise entwickelt und verbessert - in den Bereichen Mail, Cloud und Suche.  Gereift wie guter K√§se oder Cognac.  Als wir feststellten, dass unsere neuronalen Netze hervorragende Erkennungsergebnisse zeigten, beschlossen wir, sie in ein einziges B2B-Produkt - Vision - zu integrieren, das wir jetzt selbst verwenden und anbieten, Sie zu verwenden. <br><br>  Heute funktioniert unsere Computer-Vision-Technologie auf der Mail.Ru Cloud Solutions-Plattform erfolgreich und l√∂st sehr komplexe praktische Probleme.  Es basiert auf einer Reihe von neuronalen Netzen, die auf unseren Datens√§tzen trainiert sind und sich auf die L√∂sung angewandter Probleme spezialisiert haben.  Alle Dienste drehen sich um unsere Serverkapazit√§ten.  Sie k√∂nnen die √∂ffentliche Vision-API in Ihre Anwendungen integrieren, √ºber die alle Funktionen des Dienstes verf√ºgbar sind.  Die API ist schnell - dank Server-GPUs betr√§gt die durchschnittliche Antwortzeit in unserem Netzwerk 100 ms. <br><br>  Kommen Sie unter den Schnitt, es gibt eine detaillierte Geschichte und viele Beispiele f√ºr Vision. <br><a name="habracut"></a><br>  Als Beispiel f√ºr einen Dienst, bei dem wir selbst die oben genannten Gesichtserkennungstechnologien verwenden, k√∂nnen wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ereignisse</a> anf√ºhren.  Eine seiner Komponenten sind die Vision-Fotost√§nder, die wir auf verschiedenen Konferenzen installieren.  Wenn Sie zu einem solchen Fotostand gehen, ein Bild mit der eingebauten Kamera aufnehmen und Ihre E-Mail-Adresse eingeben, findet das System sofort unter den Fotos diejenigen, von denen die regul√§ren Konferenzfotografen Sie aufgenommen haben, und sendet Ihnen die gefundenen Fotos auf Wunsch per Post.  Und es geht nicht um inszenierte Portr√§taufnahmen - Vision erkennt Sie auch im Hintergrund in der Menge der Besucher.  Nat√ºrlich werden sie von den Fotost√§nden selbst nicht erkannt, sondern sind nur Tablets in sch√∂nen Untersetzern, die G√§ste einfach mit ihren eingebauten Kameras fotografieren und Informationen an Server √ºbertragen, auf denen die ganze Magie der Erkennung stattfindet.  Und wir haben wiederholt beobachtet, wie √ºberraschend die Effektivit√§t der Technologie selbst unter Spezialisten f√ºr Bilderkennung ist.  Im Folgenden werden einige Beispiele aufgef√ºhrt. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/3gE-OeSmoKo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  1. Unser Gesichtserkennungsmodell </h1><br><h3>  1.1.  Neuronales Netz und Verarbeitungsgeschwindigkeit </h3><br>  Zur Erkennung verwenden wir eine Modifikation des neuronalen Netzwerkmodells ResNet 101. Das durchschnittliche Pooling am Ende wird durch eine vollst√§ndig verbundene Ebene ersetzt, √§hnlich wie in ArcFace.  Die Gr√∂√üe der Vektordarstellungen betr√§gt jedoch 128 und nicht 512. Unser Trainingsset enth√§lt etwa 10 Millionen Fotos von 273.593 Personen. <br><br>  Das Modell arbeitet dank einer sorgf√§ltig ausgew√§hlten Serverkonfigurationsarchitektur und GPU-Computing sehr schnell.  Es dauert 100 ms, um eine Antwort von der API in unseren internen Netzwerken zu erhalten. Dies umfasst die Gesichtserkennung (Gesichtserkennung auf dem Foto), die Erkennung und die R√ºckgabe der PersonID in der API-Antwort.  Bei gro√üen Mengen eingehender Daten - Fotos und Videos - dauert es viel l√§nger, Daten an den Dienst zu √ºbertragen und eine Antwort zu erhalten. <br><br><h3>  1.2.  Bewertung der Wirksamkeit des Modells </h3><br>  Die Bestimmung der Effizienz neuronaler Netze ist jedoch eine sehr gemischte Aufgabe.  Die Qualit√§t ihrer Arbeit h√§ngt davon ab, auf welchen Datens√§tzen die Modelle trainiert wurden und ob sie f√ºr die Arbeit mit bestimmten Daten optimiert wurden. <br><br>  Wir haben begonnen, die Genauigkeit unseres Modells mit dem beliebten LFW-Verifikationstest zu bewerten, aber es ist zu klein und einfach.  Nach Erreichen einer Genauigkeit von 99,8% ist es nicht mehr n√ºtzlich.  Es gibt einen guten Wettbewerb f√ºr die Bewertung von Erkennungsmodellen - Megaface erreichte allm√§hlich 82% Rang 1. Der Megaface-Test besteht aus einer Million Fotos - Distraktoren - und das Modell sollte in der Lage sein, mehrere tausend Fotos von Prominenten aus dem Facescrub-Datensatz gut von Distraktoren zu unterscheiden.  Nachdem wir den Megaface-Test auf Fehler √ºberpr√ºft hatten, stellten wir fest, dass wir bei der bereinigten Version eine Genauigkeit von 98% Rang 1 erreichen (Promi-Fotos sind im Allgemeinen recht spezifisch).  Aus diesem Grund haben sie einen separaten Identifikationstest erstellt, √§hnlich wie Megaface, jedoch mit Fotos von ‚Äûnormalen‚Äú Menschen.  Die Erkennungsgenauigkeit ihrer Datens√§tze wurde weiter verbessert und ging weit voran.  Zus√§tzlich verwenden wir den Clustering-Qualit√§tstest, der aus mehreren tausend Fotos besteht.  Es simuliert das Markup von Gesichtern in der Cloud des Benutzers.  In diesem Fall sind Cluster Gruppen √§hnlicher Personen, eine Gruppe f√ºr jede erkennbare Person.  Wir haben die Qualit√§t der Arbeit an realen Gruppen √ºberpr√ºft (wahr). <br><br>  Nat√ºrlich hat jedes Modell Erkennungsfehler.  Solche Situationen werden jedoch h√§ufig durch Feinabstimmung der Schwellenwerte f√ºr bestimmte Bedingungen gel√∂st (f√ºr alle Konferenzen verwenden wir dieselben Schwellenwerte, und beispielsweise m√ºssen wir f√ºr ACS die Schwellenwerte erheblich erh√∂hen, damit weniger Fehlalarme auftreten).  Die √ºberwiegende Mehrheit der Konferenzteilnehmer wurde von unseren Vision-Fotost√§nden korrekt erkannt.  Manchmal schaute jemand auf die zugeschnittene Vorschau und sagte: "Ihr System war falsch, ich bin es nicht."  Dann haben wir das gesamte Foto ge√∂ffnet und es stellte sich heraus, dass dieser Besucher wirklich auf dem Foto war, nur dass er es nicht aufgenommen hat, sondern dass jemand anderes, nur ein Mann, versehentlich im Hintergrund in der Unsch√§rfezone aufgetaucht ist.  Dar√ºber hinaus erkennt das neuronale Netzwerk h√§ufig korrekt, selbst wenn ein Teil des Gesichts nicht sichtbar ist oder eine Person im Profil oder sogar im Halbgesicht steht.  Das System kann eine Person erkennen, selbst wenn die Person beispielsweise beim Aufnehmen mit einem Weitwinkelobjektiv in das Feld der optischen Verzerrung gefallen ist. <br><br><h3>  1.3.  Testbeispiele in schwierigen Situationen </h3><br>  Nachfolgend finden Sie Beispiele f√ºr den Betrieb unseres neuronalen Netzwerks.  Am Eingang werden Fotos eingereicht, die sie mit PersonID markieren muss - eine eindeutige Kennung f√ºr die Person.  Wenn zwei oder mehr Bilder dieselbe Kennung haben, zeigen diese Fotos je nach Modell eine Person. <br><br>  Wir stellen sofort fest, dass wir w√§hrend des Testens Zugriff auf verschiedene Parameter und Schwellenwerte von Modellen haben, die wir konfigurieren k√∂nnen, um ein bestimmtes Ergebnis zu erzielen.  Die √∂ffentliche API ist f√ºr maximale Genauigkeit in h√§ufigen F√§llen optimiert. <br><br>  Beginnen wir mit dem Einfachsten, mit der Gesichtserkennung im Gesicht. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/973/ba9/21e973ba959f23ab2b8592e795fb8bcb.png"><br><br>  Das war zu einfach.  Wir erschweren die Aufgabe, f√ºgen einen Bart und eine Handvoll Jahre hinzu. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/24e/a74/347/24ea7434773548d5dc23cdff03887b0f.png"><br><br>  Jemand wird sagen, dass dies nicht zu schwierig war, da in beiden F√§llen das Gesicht in seiner Gesamtheit sichtbar ist und der Algorithmus viele Informationen √ºber das Gesicht enth√§lt.  Okay, dreh Tom Hardy im Profil.  Diese Aufgabe ist viel komplizierter, und wir haben viel Aufwand f√ºr die erfolgreiche L√∂sung bei gleichzeitig geringem Fehleraufwand betrieben: Wir haben ein Trainingsmuster ausgew√§hlt, √ºber die Architektur des neuronalen Netzwerks nachgedacht, die Verlustfunktionen verbessert und die vorl√§ufige Verarbeitung von Fotos verbessert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df6/191/445/df619144560539d45c09176174349b9d.png"><br><br>  Setzen wir einen Hut auf ihn: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/40c/2e5/48b/40c2e548b16ea8a2015b0b7f0ff53b62.png"><br><br>  Dies ist √ºbrigens ein Beispiel f√ºr eine besonders schwierige Situation, da das Gesicht hier sehr bedeckt ist und im unteren Bild auch ein tiefer Schatten die Augen verbirgt.  Im wirklichen Leben √§ndern Menschen ihr Aussehen sehr oft mit Hilfe einer dunklen Brille.  Mach dasselbe mit Tom. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/27e/792/5db/27e7925dbb402d2ba1957e812279de7c.png"><br><br>  Lassen Sie uns versuchen, Fotos aus verschiedenen Altersgruppen hochzuladen, und dieses Mal werden wir Erfahrungen mit einem anderen Schauspieler machen.  Nehmen wir ein viel komplexeres Beispiel, wenn altersbedingte Ver√§nderungen besonders ausgepr√§gt sind.  Die Situation ist nicht weit hergeholt, es passiert immer wieder, wenn Sie ein Foto in Ihrem Reisepass mit dem Gesicht des Inhabers vergleichen m√ºssen.  Immerhin steckt das erste Foto im Pass, wenn der Besitzer 20 Jahre alt ist, und von 45 Personen kann sich sehr viel √§ndern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/59f/d37/109/59fd37109369ee642fa75ff107be9726.png"><br><br>  Denken Sie, dass sich das Hauptspecial f√ºr unm√∂gliche Missionen mit dem Alter nicht wesentlich ge√§ndert hat?  Ich denke, dass sogar ein paar Leute die oberen und unteren Fotos kombinieren w√ºrden, der Junge hat sich im Laufe der Jahre so sehr ver√§ndert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f3c/f57/20c/f3cf5720c2aeced56746035af7687531.png"><br><br>  Neuronale Netze sind viel h√§ufiger mit Ver√§nderungen im Erscheinungsbild konfrontiert.  Zum Beispiel k√∂nnen Frauen manchmal ihr Image mithilfe von Kosmetika stark ver√§ndern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e07/47e/c03/e0747ec033917573a028028e04a0cf24.png"><br><br>  Lassen Sie uns die Aufgabe jetzt noch komplizierter machen: Lassen Sie verschiedene Teile des Gesichts mit verschiedenen Fotos bedecken.  In solchen F√§llen kann der Algorithmus nicht die gesamten Stichproben vergleichen.  Vision geht jedoch gut mit solchen Situationen um. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f99/14b/600/f9914b600d274f8f29ecbd229e937e9e.png"><br><br>  √úbrigens sind auf Fotos viele Gesichter zu sehen, zum Beispiel k√∂nnen mehr als 100 Personen in ein gemeinsames Bild der Halle passen.  Dies ist eine schwierige Situation f√ºr neuronale Netze, da viele Gesichter unterschiedlich beleuchtet werden k√∂nnen, jemand au√üerhalb der Sch√§rfezone.  Wenn das Foto jedoch mit ausreichender Aufl√∂sung und Qualit√§t aufgenommen wurde (mindestens 75 Pixel pro Quadrat, das das Gesicht bedeckt), kann Vision es identifizieren und erkennen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/362/3a9/b8e/3623a9b8e23fd68b025f6bc9c81f22d2.png"><br><br>  Die Besonderheit bei der Meldung von Fotos und Bildern von √úberwachungskameras besteht darin, dass Menschen h√§ufig verschwommen sind, weil sie sich au√üerhalb des Sch√§rfebereichs befanden oder sich in diesem Moment bewegten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/819/db8/c4e/819db8c4e234677690cfc86c7be73a02.png"><br><br>  Auch die Beleuchtungsst√§rke kann von Bild zu Bild stark variieren.  Dies wird auch oft zu einem Stolperstein. Viele Algorithmen haben gro√üe Schwierigkeiten, Bilder, die zu dunkel und zu hell sind, korrekt zu verarbeiten, ganz zu schweigen vom genauen Vergleich.  Ich m√∂chte Sie daran erinnern, dass Sie, um ein solches Ergebnis zu erzielen, Schwellenwerte auf eine bestimmte Weise festlegen m√ºssen. Diese M√∂glichkeit ist noch nicht √∂ffentlich verf√ºgbar.  F√ºr alle Clients verwenden wir dasselbe neuronale Netzwerk mit Schwellenwerten, die f√ºr die meisten praktischen Aufgaben geeignet sind. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db0/423/6a3/db04236a312641cc7ed888bc279cbf66.png"><br><br>  K√ºrzlich haben wir eine neue Version des Modells eingef√ºhrt, die asiatische Gesichter mit hoher Genauigkeit erkennt.  Zuvor war dies ein gro√ües Problem, das sogar als "Rassismus beim maschinellen Lernen" (oder "neuronale Netze") bezeichnet wurde.  Europ√§ische und amerikanische neuronale Netze erkannten europoidische Gesichter gut, und bei mongoloiden und negroiden war es viel schlimmer.  Wahrscheinlich im selben China war die Situation genau umgekehrt.  Es geht um Trainingsdatens√§tze, die die dominierenden Arten von Menschen in einem bestimmten Land widerspiegeln.  Die Situation √§ndert sich jedoch, heute ist dieses Problem alles andere als akut.  Das Sehen hat keine Schwierigkeiten mit Vertretern verschiedener Rassen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/be8/750/da9/be8750da9407b7ca4c2cd11dac314ef3.png"><br><br>  Gesichtserkennung ist nur eine von vielen Anwendungen unserer Technologie. Vision kann gelehrt werden, alles zu erkennen.  Zum Beispiel Fahrzeugnummern, auch unter schwierigen Bedingungen f√ºr Algorithmen: in scharfen Winkeln, schmutzige und schwer lesbare Zahlen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b22/ddb/39d/b22ddb39d2fe119db6eb50db37df829b.png"><br><br><h1>  2. Praktische Anwendungsf√§lle </h1><br><h3>  2.1.  Physische Zugangskontrolle: Wenn zwei den gleichen Pass haben </h3><br>  Mit Hilfe von Vision ist es m√∂glich, Buchhaltungssysteme f√ºr die An- und Abreise von Mitarbeitern zu implementieren.  Ein traditionelles System, das auf elektronischen Ausweisen basiert, hat offensichtliche Nachteile. Sie k√∂nnen beispielsweise zwei Ausweise zusammen durchlaufen.  Wenn das Zugangskontrollsystem (ACS) durch Vision erg√§nzt wird, wird ehrlich aufgezeichnet, wer wann kam und ging. <br><br><h3>  2.2.  Zeiterfassung </h3><br>  Dieser Anwendungsfall f√ºr Vision ist eng mit dem vorherigen verwandt.  Wenn wir das Zugangskontrollsystem durch unseren Gesichtserkennungsdienst erg√§nzen, kann es nicht nur Verst√∂√üe gegen die Zugangskontrolle feststellen, sondern auch den tats√§chlichen Aufenthalt der Mitarbeiter im Geb√§ude oder in der Einrichtung aufzeichnen.  Mit anderen Worten, Vision wird helfen, ehrlich zu √ºberlegen, wer und wie viel zur Arbeit gekommen ist und mit ihr gegangen ist und wer sogar √ºbersprungen hat, selbst wenn seine Kollegen ihn vor seinen Vorgesetzten gedeckt haben. <br><br><h3>  2.3.  Videoanalyse: Personenverfolgung und Sicherheit </h3><br>  Indem Sie Personen mit Vision verfolgen, k√∂nnen Sie die tats√§chliche Durchg√§ngigkeit von Einkaufsbereichen, Bahnh√∂fen, Kreuzungen, Stra√üen und vielen anderen √∂ffentlichen Orten genau einsch√§tzen.  Unsere Nachverfolgung kann auch eine gro√üe Hilfe bei der Kontrolle des Zugriffs sein, beispielsweise auf ein Lager oder andere wichtige B√ºror√§ume.  Das Verfolgen von Personen und Gesichtern hilft nat√ºrlich bei der L√∂sung von Sicherheitsproblemen.  Jemanden erwischt, der aus Ihrem Gesch√§ft gestohlen hat?  F√ºgen Sie es in die PersonID, die Vision zur√ºckgegeben hat, in die schwarze Liste Ihrer Videoanalyse-Software ein, und das n√§chste Mal benachrichtigt das System die Sicherheit sofort, wenn dieser Typ erneut angezeigt wird. <br><br><h3>  2.4.  Im Handel </h3><br>  Einzelhandel und verschiedene Dienstleistungsunternehmen sind an der Erkennung von Warteschlangen interessiert.  Mit Vision k√∂nnen Sie erkennen, dass es sich nicht um eine zuf√§llige Personenmenge handelt, sondern um eine Warteschlange, und deren L√§nge bestimmen.  Und dann informiert das System die Verantwortlichen der Warteschlange, um die Situation zu verstehen: Entweder handelt es sich um einen Zustrom von Besuchern, und es m√ºssen zus√§tzliche Mitarbeiter angerufen werden, oder jemand hackt mit seiner Arbeitsverantwortung. <br><br>  Eine weitere interessante Aufgabe ist die Trennung der Mitarbeiter des Unternehmens in der Halle von den Besuchern.  In der Regel lernt das System, Objekte in bestimmten Kleidungsst√ºcken (Kleiderordnung) oder mit bestimmten Unterscheidungsmerkmalen (Unterschriftenschal, Abzeichen auf der Brust usw.) zu trennen.  Dies hilft, die Anwesenheit genauer zu bewerten (damit die Mitarbeiter allein die Statistiken der Personen in der Halle nicht ‚Äûaufwickeln‚Äú). <br><br>  Mithilfe der Gesichtserkennung k√∂nnen Sie Ihr Publikum bewerten: Wie hoch ist die Loyalit√§t der Besucher, dh wie viele Personen kehren mit welcher H√§ufigkeit zu Ihrer Einrichtung zur√ºck?  Berechnen Sie, wie viele eindeutige Besucher pro Monat zu Ihnen kommen.  Um die Kosten f√ºr die Gewinnung und Bindung zu optimieren, k√∂nnen Sie die Besucherzahlen je nach Wochentag und sogar Tageszeit ermitteln und √§ndern. <br><br>  Franchisegeber und Netzwerkunternehmen k√∂nnen anhand von Fotos eine Bewertung der Markenqualit√§t verschiedener Einzelhandelsgesch√§fte anordnen: das Vorhandensein von Logos, Schildern, Postern, Bannern usw. <br><br><h3>  2.5.  Beim Transport </h3><br>  Ein weiteres Beispiel f√ºr Sicherheit durch Videoanalyse ist die Identifizierung von Gegenst√§nden, die in Flughafen- oder Bahnhofshallen zur√ºckgelassen wurden.  Das Sehen kann trainiert werden, um Objekte aus Hunderten von Klassen zu erkennen: M√∂bel, Taschen, Koffer, Regenschirme, verschiedene Arten von Kleidung, Flaschen und so weiter.  Wenn Ihr Videoanalysesystem ein Objekt ohne Eigent√ºmer erkennt und es mit Vision erkennt, sendet es ein Signal an den Sicherheitsdienst.  Eine √§hnliche Aufgabe betrifft die automatische Erkennung von nicht standardm√§√üigen Situationen an √∂ffentlichen Orten: Jemand wurde krank oder jemand rauchte am falschen Ort oder die Person fiel auf die Schienen und so weiter - all diese Muster des Videoanalysesystems k√∂nnen durch die API Vision erkannt werden. <br><br><h3>  2.6.  Workflow </h3><br>  Eine weitere interessante zuk√ºnftige Anwendung von Vision, die wir derzeit entwickeln, ist die Erkennung von Dokumenten und deren automatische Analyse in Datenbanken.  Anstatt endlose Serien, Nummern, Ausgabedaten, Kontonummern, Bankdaten, Geburtsdaten und -orte sowie viele andere formalisierte Daten manuell einzugeben (oder noch schlimmer einzugeben), k√∂nnen Sie Dokumente scannen und automatisch √ºber einen sicheren Kanal √ºber die API senden In der Cloud, in der das System im laufenden Betrieb ist, werden diese Dokumente erkannt, analysiert und eine Antwort mit Daten im gew√ºnschten Format f√ºr den automatischen Eintrag in die Datenbank zur√ºckgegeben.  Vision wei√ü bereits heute, wie man Dokumente klassifiziert (auch als PDF) - es unterscheidet P√§sse, SNILS, TIN, Geburtsurkunden, Heiratsurkunden und andere. <br><br>  Nat√ºrlich kann das neuronale Netzwerk all diese Situationen nicht sofort bew√§ltigen.  In jedem Fall wird ein neues Modell f√ºr einen bestimmten Kunden erstellt, viele Faktoren, Nuancen und Anforderungen werden ber√ºcksichtigt, Datens√§tze werden ausgew√§hlt, Einstellungen f√ºr Trainingstests werden wiederholt. <br><br><h1>  3. API-Arbeitsschema </h1><br>  Das ‚ÄûEingangstor‚Äú von Vision f√ºr Benutzer ist die REST-API.  Am Eingang kann er Fotos, Videodateien und Sendungen von Netzwerkkameras (RTSP-Streams) aufnehmen. <br><br>  Um Vision verwenden zu k√∂nnen, m√ºssen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sich</a> bei Mail.ru Cloud Solutions <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">registrieren</a> und Zugriffstoken (client_id + client_secret) erhalten.  Die Benutzerauthentifizierung wird mithilfe des OAuth-Protokolls durchgef√ºhrt.  Die Quelldaten in den Hauptteilen der POST-Anforderungen werden an die API gesendet.  Als Antwort erh√§lt der Client das Erkennungsergebnis von der API im JSON-Format, und die Antwort ist strukturiert: Sie enth√§lt Informationen zu den gefundenen Objekten und ihren Koordinaten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8c/bc3/859/d8cbc3859777f8f0a40712ef8e8a4e17.png"><br><br><div class="spoiler">  <b class="spoiler_title">Antwortbeispiel</b> <div class="spoiler_text"><pre><code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"status"</span></span>:<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-attr"><span class="hljs-attr">"body"</span></span>:{ <span class="hljs-attr"><span class="hljs-attr">"objects"</span></span>:[ { <span class="hljs-attr"><span class="hljs-attr">"status"</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>:<span class="hljs-string"><span class="hljs-string">"file_0"</span></span> }, { <span class="hljs-attr"><span class="hljs-attr">"status"</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>:<span class="hljs-string"><span class="hljs-string">"file_2"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"persons"</span></span>:[ { <span class="hljs-attr"><span class="hljs-attr">"tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"person9"</span></span> <span class="hljs-string"><span class="hljs-string">"coord"</span></span>:[<span class="hljs-number"><span class="hljs-number">149</span></span>,<span class="hljs-number"><span class="hljs-number">60</span></span>,<span class="hljs-number"><span class="hljs-number">234</span></span>,<span class="hljs-number"><span class="hljs-number">181</span></span>], <span class="hljs-attr"><span class="hljs-attr">"confidence"</span></span>:<span class="hljs-number"><span class="hljs-number">0.9999</span></span>, <span class="hljs-attr"><span class="hljs-attr">"awesomeness"</span></span>:<span class="hljs-number"><span class="hljs-number">0.45</span></span> }, { <span class="hljs-attr"><span class="hljs-attr">"tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"person10"</span></span> <span class="hljs-string"><span class="hljs-string">"coord"</span></span>:[<span class="hljs-number"><span class="hljs-number">159</span></span>,<span class="hljs-number"><span class="hljs-number">70</span></span>,<span class="hljs-number"><span class="hljs-number">224</span></span>,<span class="hljs-number"><span class="hljs-number">171</span></span>], <span class="hljs-attr"><span class="hljs-attr">"confidence"</span></span>:<span class="hljs-number"><span class="hljs-number">0.9998</span></span>, <span class="hljs-attr"><span class="hljs-attr">"awesomeness"</span></span>:<span class="hljs-number"><span class="hljs-number">0.32</span></span> } ] } { <span class="hljs-attr"><span class="hljs-attr">"status"</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>:<span class="hljs-string"><span class="hljs-string">"file_3"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"persons"</span></span>:[ { <span class="hljs-attr"><span class="hljs-attr">"tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"person11"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"coord"</span></span>:[<span class="hljs-number"><span class="hljs-number">157</span></span>,<span class="hljs-number"><span class="hljs-number">60</span></span>,<span class="hljs-number"><span class="hljs-number">232</span></span>,<span class="hljs-number"><span class="hljs-number">111</span></span>], <span class="hljs-attr"><span class="hljs-attr">"aliases"</span></span>:[<span class="hljs-string"><span class="hljs-string">"person12"</span></span>, <span class="hljs-string"><span class="hljs-string">"person13"</span></span>] <span class="hljs-string"><span class="hljs-string">"confidence"</span></span>:<span class="hljs-number"><span class="hljs-number">0.9998</span></span>, <span class="hljs-attr"><span class="hljs-attr">"awesomeness"</span></span>:<span class="hljs-number"><span class="hljs-number">0.32</span></span> } ] }, { <span class="hljs-attr"><span class="hljs-attr">"status"</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>:<span class="hljs-string"><span class="hljs-string">"file_4"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"persons"</span></span>:[ { <span class="hljs-attr"><span class="hljs-attr">"tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"undefined"</span></span> <span class="hljs-string"><span class="hljs-string">"coord"</span></span>:[<span class="hljs-number"><span class="hljs-number">147</span></span>,<span class="hljs-number"><span class="hljs-number">50</span></span>,<span class="hljs-number"><span class="hljs-number">222</span></span>,<span class="hljs-number"><span class="hljs-number">121</span></span>], <span class="hljs-attr"><span class="hljs-attr">"confidence"</span></span>:<span class="hljs-number"><span class="hljs-number">0.9997</span></span>, <span class="hljs-attr"><span class="hljs-attr">"awesomeness"</span></span>:<span class="hljs-number"><span class="hljs-number">0.26</span></span> } ] } ], <span class="hljs-attr"><span class="hljs-attr">"aliases_changed"</span></span>:<span class="hljs-literal"><span class="hljs-literal">false</span></span> }, <span class="hljs-attr"><span class="hljs-attr">"htmlencoded"</span></span>:<span class="hljs-literal"><span class="hljs-literal">false</span></span>, <span class="hljs-attr"><span class="hljs-attr">"last_modified"</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span> }</code> </pre> <br></div></div><br>  Die Antwort hat einen interessanten Parameter - dies ist die bedingte ‚ÄûK√ºhle‚Äú des Gesichts auf dem Foto. Damit w√§hlen wir die beste Gesichtsaufnahme aus der Sequenz aus.  Wir haben das neuronale Netzwerk trainiert, um die Wahrscheinlichkeit vorherzusagen, mit der das Bild in sozialen Netzwerken aussehen wird.  Je besser das Bild und je glatter das Gesicht, desto gr√∂√üer die Ehrfurcht. <br><br>  Die Vision-API verwendet ein Konzept wie Space.  Dies ist ein Werkzeug zum Erstellen verschiedener S√§tze von Gesichtern.  Beispiele f√ºr Leerzeichen sind Schwarzwei√ülisten, Listen von Besuchern, Mitarbeitern, Kunden usw. F√ºr jedes Token in Vision k√∂nnen Sie bis zu 10 Leerzeichen erstellen. In jedem Leerzeichen k√∂nnen bis zu 50.000 Personen-IDs vorhanden sein, dh bis zu 500.000 f√ºr ein Token .  Dar√ºber hinaus ist die Anzahl der Token pro Konto nicht begrenzt. <br><br>  Heute unterst√ºtzt die API die folgenden Erkennungs- und Erkennungsmethoden: <br><br><ul><li>  Erkennen / Setzen - Definition und Erkennung von Gesichtern.  Weist jedem eindeutigen Gesicht automatisch eine PersonID zu, gibt die PersonID und die Koordinaten der gefundenen Gesichter zur√ºck. <br></li><li>  L√∂schen - L√∂scht eine bestimmte PersonID aus der Personendatenbank. <br></li><li>  Abschneiden - L√∂scht den gesamten Speicherplatz von PersonID. Dies ist n√ºtzlich, wenn er als Test verwendet wurde und Sie die Basis f√ºr die Produktion zur√ºcksetzen m√ºssen. <br></li><li>  Erkennen - Definition von Objekten, Szenen, Nummernschildern, Attraktionen, Warteschlangen usw. Gibt die Klasse der gefundenen Objekte und ihre Koordinaten zur√ºck <br></li><li>  Nach Dokumenten suchen - Erkennt bestimmte Arten von Dokumenten der Russischen F√∂deration (unterscheidet Pass, Snls, Gasthaus usw.). <br></li></ul><br>  Au√üerdem werden wir in K√ºrze die Arbeit an Methoden f√ºr die OCR beenden, Geschlecht, Alter und Emotionen bestimmen sowie Merchandising-Aufgaben l√∂sen, dh die Anzeige von Waren in Gesch√§ften automatisch steuern.  Die vollst√§ndige API-Dokumentation finden Sie hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://mcs.mail.ru/help/vision-api</a> <br><br><h1>  4. Fazit </h1><br>  √úber die √∂ffentliche API k√∂nnen Sie jetzt auf die Gesichtserkennung in Fotos und Videos zugreifen. Sie unterst√ºtzt die Definition verschiedener Objekte, Fahrzeugnummern, Attraktionen, Dokumente und ganzer Szenen.  Anwendungsszenarien - Meer.  Kommen Sie, testen Sie unseren Service und stellen Sie die schwierigsten Aufgaben daf√ºr.  Die ersten 5.000 Transaktionen sind kostenlos.  Es kann die ‚Äûfehlende Zutat‚Äú f√ºr Ihre Projekte sein. <br><br>  Der Zugriff auf die API kann sofort bei der Registrierung und Verbindung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vision erfolgen</a> .  Alle Habra-Benutzer - ein Aktionscode f√ºr zus√§tzliche Transaktionen.  Schreiben Sie eine pers√∂nliche E-Mail-Adresse, unter der das Konto registriert wurde! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de449120/">https://habr.com/ru/post/de449120/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de449108/index.html">UDB. Was ist das Teil 7. Steuermodul f√ºr Timing und Reset</a></li>
<li><a href="../de449110/index.html">Es wurde ein Fehler behoben, der mit der Unf√§higkeit zusammenh√§ngt, das kyrillische Alphabet in den Namen von IMAP-Ordnern zu verwenden</a></li>
<li><a href="../de449112/index.html">Wir haben uns zur√ºckgezogen - wir diskutieren √ºber einst beliebte Audio-Gadgets, die bereits "veraltet" sind.</a></li>
<li><a href="../de449114/index.html">Reagiere auf Œªambda</a></li>
<li><a href="../de449118/index.html">Kreml-D√§monenpille</a></li>
<li><a href="../de449122/index.html">Bedauern, dass in C ++ keine vollwertige statische Aufladung vorhanden ist, wenn oder ...</a></li>
<li><a href="../de449124/index.html">So schwer zu finden, leicht zu √ºbersehen und unm√∂glich auszugeben</a></li>
<li><a href="../de449128/index.html">Top-Spieleentwicklungsunternehmen der Welt</a></li>
<li><a href="../de449132/index.html">Top 17 Plugins f√ºr Android Studio</a></li>
<li><a href="../de449134/index.html">Zoo afl</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>