<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏔️ 🤦🏻 🖍️ Redes de bastidores no Kubernetes ™️ 📖 👴🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : O autor do artigo original, Nicolas Leiva, é um arquiteto de soluções da Cisco que decidiu compartilhar com seus colegas, engenheiros de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes de bastidores no Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: O autor do artigo original, Nicolas Leiva, é um arquiteto de soluções da Cisco que decidiu compartilhar com seus colegas, engenheiros de rede, como a rede Kubernetes funciona por dentro.</i>  <i>Para fazer isso, ele explora sua configuração mais simples no cluster, usando ativamente o bom senso, seu conhecimento de redes e utilitários padrão do Linux / Kubernetes.</i>  <i>Ficou volumoso, mas muito claramente.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  Além do fato de o guia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way de</a> Kelsey Hightower funcionar ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">até na AWS!</a> ), Eu gostei que a rede fosse mantida limpa e simples;  e esta é uma ótima oportunidade para entender o papel, por exemplo, da <a href="">CNI</a> (Container Network Interface).  Dito isto, acrescentarei que a rede Kubernetes não é realmente muito intuitiva, especialmente para iniciantes ... e também não se esqueça de que “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">simplesmente não existe</a> uma rede de contêineres”. <a name="habracut"></a><br><br>  Embora já existam bons materiais sobre esse tópico (veja os links <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ), não consegui encontrar um exemplo que combinasse tudo o necessário com as conclusões das equipes que os engenheiros de rede amam e odeiam, demonstrando o que realmente está acontecendo nos bastidores.  Portanto, decidi coletar informações de várias fontes - espero que isso ajude e você entenda melhor como tudo está conectado.  Esse conhecimento é importante não apenas para testar a si mesmo, mas também para simplificar o processo de diagnóstico de problemas.  Você pode seguir o exemplo em seu cluster no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> : todos os endereços IP e configurações são obtidos de lá (a partir das confirmações de maio de 2018, antes de usar os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">contêineres Nabla</a> ). <br><br>  E começaremos do final, quando tivermos três controladores e três nós de trabalho: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  Você pode perceber que também há pelo menos três sub-redes privadas aqui!  Um pouco de paciência, e todos serão considerados.  Lembre-se de que, apesar de nos referirmos a prefixos IP muito específicos, eles são simplesmente retirados do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> , portanto, eles têm apenas significado local e você pode escolher qualquer outro bloco de endereço para o seu ambiente, de acordo com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RFC 1918</a> .  Para o caso do IPv6, haverá um artigo de blog separado. <br><br><h2>  Rede do host (10.240.0.0/24) </h2><br>  Essa é uma rede interna da qual todos os nós fazem parte.  Definido pelo <code>--private-network-ip</code> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GCP</a> ou pela opção <code>--private-ip-address</code> na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS</a> ao alocar recursos de computação. <br><br><h3>  Inicializando nós do controlador no GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Inicializando nós do controlador na AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Cada instância terá dois endereços IP: privado da rede host (controladores - <code>10.240.0.1${i}/24</code> , trabalhadores - <code>10.240.0.2${i}/24</code> ) e um público, nomeado pelo provedor de nuvem, sobre o qual falaremos mais adiante como chegar ao <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Todos os nós devem poder executar ping se as <a href="">políticas de segurança estiverem corretas</a> (e se o <code>ping</code> instalado no host). <br><br><h2>  Rede da lareira (10.200.0.0/16) </h2><br>  Essa é a rede na qual os pods vivem.  Cada nó de trabalho usa uma sub-rede desta rede.  No nosso caso, <code>POD_CIDR=10.200.${i}.0/24</code> para o <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Para entender como tudo está configurado, dê um passo para trás e observe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o modelo de rede Kubernetes</a> , que requer o seguinte: <br><br><ul><li>  Todos os contêineres podem se comunicar com outros contêineres sem usar o NAT. </li><li>  Todos os nós podem se comunicar com todos os contêineres (e vice-versa) sem usar o NAT. </li><li>  O IP que o contêiner vê deve ser o mesmo que os outros o veem. </li></ul><br>  Tudo isso pode ser implementado de várias maneiras, e o Kubernetes passa a configuração de rede para o <a href="">plugin CNI</a> . <br><br><blockquote>  “O plugin CNI é responsável por adicionar uma interface de <b>rede ao namespace de rede</b> do contêiner (por exemplo, uma extremidade de um <b>par veth</b> ) e fazer as alterações necessárias no host (por exemplo, conectar a segunda extremidade do veth a uma ponte).  Depois, ele deve atribuir uma interface IP e configurar as rotas de acordo com a seção Gerenciamento de endereço IP, chamando o plug-in IPAM desejado. ”  <i>(da <a href="">especificação de interface de rede de contêiner</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Namespace de rede </h3><br><blockquote>  “O espaço para nome agrupa o recurso global do sistema em uma abstração que é visível aos processos nesse espaço para nome, de forma que eles tenham sua própria instância isolada do recurso global.  Alterações no recurso global são visíveis para outros processos incluídos neste espaço para nome, mas não são visíveis para outros processos. ”  <i>( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na página do manual namespaces</a> )</i> </blockquote><br>  O Linux fornece sete namespaces diferentes ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Os espaços para nome da rede ( <code>CLONE_NEWNET</code> ) definem os recursos de rede disponíveis para o processo: "Cada espaço para nome da rede possui seus próprios dispositivos de rede, endereços IP, tabelas de roteamento IP, diretório <code>/proc/net</code> , números de porta e assim por diante" <i>( do artigo “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Namespaces in operation</a> ”)</i> . <br><br><h3>  Dispositivos Ethernet virtuais (Veth) </h3><br><blockquote>  “Um par de rede virtual (veth) oferece uma abstração na forma de um“ canal ”, que pode ser usado para criar túneis entre namespaces de rede ou para criar uma ponte para um dispositivo de rede físico em outro espaço de rede.  Quando o espaço para nome é liberado, todos os dispositivos veth nele são destruídos. ”  <i>(na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">página do manual namespaces de rede</a> )</i> </blockquote><br>  Vá para o chão e veja como tudo isso se relaciona com o cluster.  Primeiro, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">os plugins de rede</a> no Kubernetes são diversos, e os plugins CNI são um deles ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">por que não o CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Kubelet</a> em cada nó informa ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tempo de execução</a> do contêiner qual <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plug-in de rede</a> usar.  A interface de rede de contêiner ( <a href="">CNI</a> ) está entre o tempo de execução do contêiner e a implementação da rede.  E o plugin CNI já configura a rede. <br><br><blockquote>  “O plugin CNI é selecionado passando a <code>--network-plugin=cni</code> linha de comando <code>--network-plugin=cni</code> cni para o Kubelet.  O Kubelet lê o arquivo em <code>--cni-conf-dir</code> (o padrão é <code>/etc/cni/net.d</code> ) e usa a configuração CNI deste arquivo para configurar a rede para cada arquivo. "  <i>(a partir de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Requisitos de plug-in de rede</a> )</i> </blockquote><br>  Os binários reais do plugin CNI estão em <code>-- cni-bin-dir</code> (o padrão é <code>/opt/cni/bin</code> ). <br><br>  Observe que os <a href=""><code>kubelet.service</code></a> chamada <code>--network-plugin=cni</code> incluem <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  Primeiro de tudo, o Kubernetes cria um espaço para nome de rede para a lareira, mesmo antes de chamar qualquer plug-in.  Isso é implementado usando o contêiner de <code>pause</code> especial, que "serve como o" contêiner pai "para todos os contêineres da lareira" <i>(do artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Contêiner Todo-Poderoso para Pausa</a> ")</i> .  O Kubernetes então executa o plugin CNI para anexar o contêiner de <code>pause</code> à rede.  Todos os contêineres de pod usam o <code>netns</code> desse contêiner de <code>pause</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  A <a href="">configuração CNI usada</a> indica o uso do plug-in de <code>bridge</code> para configurar a ponte do software Linux (L2) no espaço de nomes raiz chamado <code>cnio0</code> (o <a href="">nome padrão</a> é <code>cni0</code> ), que atua como um gateway ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Um par veth também será configurado para conectar a lareira à ponte recém-criada: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Para atribuir informações L3, como endereços IP, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plug</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">in IPAM</a> ( <code>ipam</code> ) é chamado.  Nesse caso, o tipo <code>host-local</code> é usado ", que armazena o estado localmente no sistema de arquivos host, o que garante a exclusividade dos endereços IP em um host" <i>(a partir da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code> host-local</code></a> )</i> .  O plug-in IPAM retorna essas informações para o plug-in anterior ( <code>bridge</code> ), para que todas as rotas especificadas na configuração possam ser configuradas ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Se <code>gw</code> não <code>gw</code> especificado, ele <a href="">será retirado da sub-rede</a> .  A rota padrão também é configurada no espaço para nome da rede da lareira, apontando para a ponte (que é configurada como a primeira sub-rede IP da lareira). <br><br>  E o último detalhe importante: solicitamos o mascaramento ( <code>"ipMasq": true</code> ) para o tráfego proveniente da rede da lareira.  Realmente não precisamos de NAT aqui, mas essa é a configuração do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> .  Portanto, para ser completo, devo mencionar que as entradas nas <code>iptables</code> plug-in de <code>bridge</code> estão configuradas para este exemplo específico.  Todos os pacotes da lareira, cujo destinatário não está no intervalo <code>224.0.0.0/4</code> , <a href="">estarão atrás do NAT</a> , que não atende exatamente ao requisito "todos os contêineres podem se comunicar com outros contêineres sem usar o NAT".  Bem, provaremos por que o NAT não é necessário ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Roteamento da lareira </h3><br>  Agora estamos prontos para personalizar os pods.  Vamos examinar todos os espaços de rede dos nomes de um dos nós de trabalho e analisar um deles depois de criar a implantação do <code>nginx</code> <a href="">partir daqui</a> .  Usaremos <code>lsns</code> com a opção <code>-t</code> para selecionar o tipo de espaço para nome desejado (ou seja, <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  Usando a opção <code>-i</code> para <code>ls</code> , podemos encontrar seus números de inode: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Você também pode listar todos os namespaces de rede usando <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Para ver todos os processos em execução no espaço de rede <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ( <code>4026532426</code> ), é possível executar, por exemplo, o seguinte comando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  Pode-se observar que, além da <code>pause</code> neste pod, lançamos o <code>nginx</code> .  O contêiner de <code>pause</code> compartilha os namespaces <code>net</code> e <code>ipc</code> com todos os outros contêineres de pod.  Lembre-se do PID da <code>pause</code> - 27255;  nós retornaremos a ele. <br><br>  Agora vamos ver o que o <code>kubectl</code> diz sobre este pod: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  Mais detalhes: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Vemos o nome do pod - <code>nginx-65899c769f-wxdx6</code> - e o ID de um de seus contêineres ( <code>nginx</code> ), mas nada foi dito sobre a <code>pause</code> .  Cavar um nó de trabalho mais profundo para corresponder a todos os dados.  Lembre-se de que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> não usa o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Docker</a> , portanto, para obter detalhes sobre o contêiner, consultamos o utilitário console consoleerderd-ctr <i>(consulte também o artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Integração do containererd com o Kubernetes, substituindo o Docker, pronto para produção</a> " - <b>aprox. Transfer</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Conhecendo o <code>k8s.io</code> ( <code>k8s.io</code> ), é possível obter o ID do contêiner <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... e faça uma <code>pause</code> também: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  O ID do contêiner <code>nginx</code> que termina em <code>…983c7</code> corresponde ao que obtivemos do <code>kubectl</code> .  Vamos ver se conseguimos descobrir qual contêiner de <code>pause</code> pertence ao pod do <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  Lembre-se de que os processos com o PID 27331 e 27355 estão em execução no espaço para nome da rede <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... e: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Agora sabemos com certeza quais contêineres estão sendo executados neste pod ( <code>nginx-65899c769f-wxdx6</code> ) e o espaço para nome da rede ( <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pausa (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  Como isso está ( <code>nginx-65899c769f-wxdx6</code> ) conectado à rede?  Usamos o PID 27255 recebido anteriormente da <code>pause</code> para executar comandos em seu namespace de rede ( <code>cni-912bcc63–712d-1c84–89a7–9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Para esses fins, usaremos o <code>nsenter</code> com a opção <code>-t</code> que define o PID de destino e <code>-n</code> sem especificar um arquivo para entrar no espaço de nomes de rede do processo de destino (27255).  Aqui está o que o <code>ip link show</code> dirá: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... e <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Isso confirma que o endereço IP obtido anteriormente pelo <code>kubectl get pod</code> está configurado na interface <code>eth0</code> .  Essa interface faz parte de um <b>par veth</b> , uma extremidade na lareira e a outra no espaço para nome raiz.  Para descobrir a interface do segundo lado, usamos o <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Vemos que o <code>ifindex</code> banquete é 7. Verifique se ele está no espaço para nome raiz.  Isso pode ser feito usando o <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Para ter certeza disso, finalmente, vamos ver: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  Ótimo, agora tudo está claro com o link virtual.  Usando <code>brctl</code> vamos ver quem mais está conectado à ponte Linux: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Então, a imagem é a seguinte: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Verificação de roteamento </h3><br>  Como realmente encaminhamos o tráfego?  Vejamos a tabela de roteamento no pod de namespace da rede: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Pelo menos sabemos como chegar ao espaço para nome raiz ( <code>default via 10.200.0.1</code> ).  Agora vamos ver a tabela de roteamento de host: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Sabemos como encaminhar pacotes para um roteador VPC (o VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">possui um</a> roteador "implícito", que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">geralmente possui um segundo endereço</a> do espaço de endereço IP principal da sub-rede).  Agora: o roteador VPC sabe como acessar a rede de cada lareira?  Não, ele não, portanto, presume-se que as rotas serão configuradas pelo plug-in da CNI ou <a href="">manualmente</a> (como no manual).  Aparentemente, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS CNI-plugin</a> faz exatamente isso para nós na AWS.  Lembre-se de que existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muitos plugins CNI</a> e estamos considerando um exemplo de uma <b>configuração de rede simples</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Imersão profunda no NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> crie dois contêineres idênticos com o Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Temos: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Pings de um contêiner para outro devem ser bem-sucedidos: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Para entender o movimento do tráfego, você pode ver os pacotes usando <code>tcpdump</code> ou <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  O IP de origem do pod 10.200.0.21 é convertido no endereço IP do host 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  No iptables, você pode ver que as contagens estão aumentando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  Por outro lado, se você remover <code>"ipMasq": true</code> da configuração do plugin CNI, poderá ver o seguinte (esta operação é realizada exclusivamente para fins educacionais - não recomendamos alterar a configuração em um cluster ativo!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping ainda deve passar: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  E neste caso - sem usar o NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Portanto, verificamos que "todos os contêineres podem se comunicar com outros contêineres sem usar o NAT". <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Rede de cluster (10.32.0.0/24) </h2><br>  Você deve ter notado no exemplo do <code>busybox</code> que os endereços IP atribuídos ao <code>busybox</code> eram diferentes em cada caso.  E se quiséssemos disponibilizar esses contêineres para comunicação de outros lares?  Pode-se pegar os endereços IP atuais do pod, mas eles mudarão.  Por esse motivo, você precisa configurar o recurso <code>Service</code> , que fará proxy de solicitações para muitos lares de vida curta. <br><br><blockquote>  "O serviço no Kubernetes é uma abstração que define o conjunto lógico de lareiras e as políticas pelas quais elas podem ser acessadas."  <i>(da documentação dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Serviços Kubernetes</a> )</i> </blockquote><br>  Existem várias maneiras de publicar um serviço;  o tipo padrão é <code>ClusterIP</code> , que define o endereço IP do bloco CIDR do cluster (ou seja, acessível apenas a partir do cluster).  Um exemplo é o complemento de cluster DNS configurado no Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> mostra que o <code>Service</code> lembra dos pontos finais e os converte: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  Como exatamente? .. <code>iptables</code> novamente.  Vamos examinar as regras criadas para este exemplo.  Sua lista completa pode ser vista com o comando <code>iptables-save</code> . <br><br>  Assim que os pacotes são criados pelo processo ( <code>OUTPUT</code> ) ou chegam à interface de rede ( <code>PREROUTING</code> ), eles passam pelas seguintes cadeias de <code>iptables</code> : <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Os seguintes destinos correspondem aos pacotes TCP enviados para a 53ª porta em 10.32.0.10 e são transmitidos ao destinatário 10.200.0.27 com a 53ª porta: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  O mesmo para pacotes UDP (destinatário 10.32.0.10:53 → 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Existem outros tipos de <code>Services</code> no Kubernetes.  Em particular, o Kubernetes The Hard Way <code>NodePort</code> sobre o <code>NodePort</code> - consulte <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publica o serviço no endereço IP de cada nó, colocando-o em uma porta estática (chamada <code>NodePort</code> ).  <code>NodePort</code> pode ser acessado de fora do cluster.  Você pode verificar a porta dedicada (neste caso - 31088) usando o <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Agora, Under está disponível na Internet como <code>http://${EXTERNAL_IP}:31088/</code> .  Aqui <code>EXTERNAL_IP</code> é o endereço IP público de <b>qualquer instância de trabalho</b> .  Neste exemplo, usei o endereço IP público do <b>worker-0</b> .  A solicitação é recebida por um host com um endereço IP interno 10.240.0.20 (o provedor de nuvem está envolvido no NAT público); no entanto, o serviço é realmente iniciado em outro host ( <b>worker-1</b> , que pode ser visto pelo endereço IP do terminal - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  O pacote é enviado do <b>trabalhador-0</b> para o <b>trabalhador-1</b> , onde encontra seu destinatário: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Esse circuito é ideal?  Talvez não, mas funciona.  Nesse caso, as regras do <code>iptables</code> programadas são as seguintes: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  Em outras palavras, o endereço para o destinatário dos pacotes com porta 31088 é transmitido em 10.200.1.18.  A porta também está transmitindo, de 31088 a 80. <br><br>  Não tocamos em outro tipo de serviço - o <code>LoadBalancer</code> - que disponibiliza publicamente o serviço usando um balanceador de carga do provedor de nuvem, mas o artigo já era grande. <br><br><h2>  Conclusão </h2><br>  Pode parecer que há muita informação, mas apenas tocamos a ponta do iceberg.  No futuro, vou falar sobre IPv6, IPVS, eBPF e alguns plugins CNI atuais interessantes. <br><br><h2>  PS do tradutor </h2><br>  Leia também em nosso blog: <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Guia Ilustrado de Rede em Kubernetes</a> ”; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Comparação do desempenho da rede para o Kubernetes</a> "; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Experimentos com proxy do kube e inacessibilidade do host no Kubernetes</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhorando a confiabilidade do Kubernetes: como perceber rapidamente que um nó caiu</a> ”; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Play with Kubernetes —      K8s</a> »; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   Kubernetes   </a> » <i>( ,        Kubernetes)</i> ; </li><li> « <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Container Networking Interface (CNI) —      Linux-</a> ». </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt420813/">https://habr.com/ru/post/pt420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt420799/index.html">MPS 2018.2: testes de gerador, plug-in GitHub, aspecto VCS, notificações de migração e muito mais</a></li>
<li><a href="../pt420803/index.html">Aulas de impressão 3D. Economizando plástico ao imprimir modelos não funcionais do 3Dtool</a></li>
<li><a href="../pt420805/index.html">[Translation] Quando usar fluxos paralelos</a></li>
<li><a href="../pt420809/index.html">Semana de segurança 31: cinquenta tons de insegurança no Android</a></li>
<li><a href="../pt420811/index.html">Rede descentralizada de mensagens e telefone da nova geração</a></li>
<li><a href="../pt420815/index.html">Como "decodificar o mundo digital" explodiu no corredor: os 10 principais relatórios do DotNext 2018 Piter</a></li>
<li><a href="../pt420819/index.html">As 10 principais ferramentas Python para aprendizado de máquina e ciência de dados</a></li>
<li><a href="../pt420821/index.html">Regra 10: 1 em programação e redação</a></li>
<li><a href="../pt420825/index.html">Hoje será a primeira partida entre os profissionais OpenAI e Dota 2 (pessoas vencidas). Entendemos como o bot funciona</a></li>
<li><a href="../pt420827/index.html">Crie um projeto simples do maven usando Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>