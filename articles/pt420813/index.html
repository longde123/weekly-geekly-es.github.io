<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèîÔ∏è ü§¶üèª üñçÔ∏è Redes de bastidores no Kubernetes ‚Ñ¢Ô∏è üìñ üë¥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : O autor do artigo original, Nicolas Leiva, √© um arquiteto de solu√ß√µes da Cisco que decidiu compartilhar com seus colegas, engenheiros de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes de bastidores no Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/420813/"> <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: O autor do artigo original, Nicolas Leiva, √© um arquiteto de solu√ß√µes da Cisco que decidiu compartilhar com seus colegas, engenheiros de rede, como a rede Kubernetes funciona por dentro.</i>  <i>Para fazer isso, ele explora sua configura√ß√£o mais simples no cluster, usando ativamente o bom senso, seu conhecimento de redes e utilit√°rios padr√£o do Linux / Kubernetes.</i>  <i>Ficou volumoso, mas muito claramente.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  Al√©m do fato de o guia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way de</a> Kelsey Hightower funcionar ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">at√© na AWS!</a> ), Eu gostei que a rede fosse mantida limpa e simples;  e esta √© uma √≥tima oportunidade para entender o papel, por exemplo, da <a href="">CNI</a> (Container Network Interface).  Dito isto, acrescentarei que a rede Kubernetes n√£o √© realmente muito intuitiva, especialmente para iniciantes ... e tamb√©m n√£o se esque√ßa de que ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">simplesmente n√£o existe</a> uma rede de cont√™ineres‚Äù. <a name="habracut"></a><br><br>  Embora j√° existam bons materiais sobre esse t√≥pico (veja os links <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ), n√£o consegui encontrar um exemplo que combinasse tudo o necess√°rio com as conclus√µes das equipes que os engenheiros de rede amam e odeiam, demonstrando o que realmente est√° acontecendo nos bastidores.  Portanto, decidi coletar informa√ß√µes de v√°rias fontes - espero que isso ajude e voc√™ entenda melhor como tudo est√° conectado.  Esse conhecimento √© importante n√£o apenas para testar a si mesmo, mas tamb√©m para simplificar o processo de diagn√≥stico de problemas.  Voc√™ pode seguir o exemplo em seu cluster no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> : todos os endere√ßos IP e configura√ß√µes s√£o obtidos de l√° (a partir das confirma√ß√µes de maio de 2018, antes de usar os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cont√™ineres Nabla</a> ). <br><br>  E come√ßaremos do final, quando tivermos tr√™s controladores e tr√™s n√≥s de trabalho: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  Voc√™ pode perceber que tamb√©m h√° pelo menos tr√™s sub-redes privadas aqui!  Um pouco de paci√™ncia, e todos ser√£o considerados.  Lembre-se de que, apesar de nos referirmos a prefixos IP muito espec√≠ficos, eles s√£o simplesmente retirados do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> , portanto, eles t√™m apenas significado local e voc√™ pode escolher qualquer outro bloco de endere√ßo para o seu ambiente, de acordo com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RFC 1918</a> .  Para o caso do IPv6, haver√° um artigo de blog separado. <br><br><h2>  Rede do host (10.240.0.0/24) </h2><br>  Essa √© uma rede interna da qual todos os n√≥s fazem parte.  Definido pelo <code>--private-network-ip</code> no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GCP</a> ou pela op√ß√£o <code>--private-ip-address</code> na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS</a> ao alocar recursos de computa√ß√£o. <br><br><h3>  Inicializando n√≥s do controlador no GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Inicializando n√≥s do controlador na AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Cada inst√¢ncia ter√° dois endere√ßos IP: privado da rede host (controladores - <code>10.240.0.1${i}/24</code> , trabalhadores - <code>10.240.0.2${i}/24</code> ) e um p√∫blico, nomeado pelo provedor de nuvem, sobre o qual falaremos mais adiante como chegar ao <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  Aws </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  Todos os n√≥s devem poder executar ping se as <a href="">pol√≠ticas de seguran√ßa estiverem corretas</a> (e se o <code>ping</code> instalado no host). <br><br><h2>  Rede da lareira (10.200.0.0/16) </h2><br>  Essa √© a rede na qual os pods vivem.  Cada n√≥ de trabalho usa uma sub-rede desta rede.  No nosso caso, <code>POD_CIDR=10.200.${i}.0/24</code> para o <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  Para entender como tudo est√° configurado, d√™ um passo para tr√°s e observe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o modelo de rede Kubernetes</a> , que requer o seguinte: <br><br><ul><li>  Todos os cont√™ineres podem se comunicar com outros cont√™ineres sem usar o NAT. </li><li>  Todos os n√≥s podem se comunicar com todos os cont√™ineres (e vice-versa) sem usar o NAT. </li><li>  O IP que o cont√™iner v√™ deve ser o mesmo que os outros o veem. </li></ul><br>  Tudo isso pode ser implementado de v√°rias maneiras, e o Kubernetes passa a configura√ß√£o de rede para o <a href="">plugin CNI</a> . <br><br><blockquote>  ‚ÄúO plugin CNI √© respons√°vel por adicionar uma interface de <b>rede ao namespace de rede</b> do cont√™iner (por exemplo, uma extremidade de um <b>par veth</b> ) e fazer as altera√ß√µes necess√°rias no host (por exemplo, conectar a segunda extremidade do veth a uma ponte).  Depois, ele deve atribuir uma interface IP e configurar as rotas de acordo com a se√ß√£o Gerenciamento de endere√ßo IP, chamando o plug-in IPAM desejado. ‚Äù  <i>(da <a href="">especifica√ß√£o de interface de rede de cont√™iner</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Namespace de rede </h3><br><blockquote>  ‚ÄúO espa√ßo para nome agrupa o recurso global do sistema em uma abstra√ß√£o que √© vis√≠vel aos processos nesse espa√ßo para nome, de forma que eles tenham sua pr√≥pria inst√¢ncia isolada do recurso global.  Altera√ß√µes no recurso global s√£o vis√≠veis para outros processos inclu√≠dos neste espa√ßo para nome, mas n√£o s√£o vis√≠veis para outros processos. ‚Äù  <i>( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na p√°gina do manual namespaces</a> )</i> </blockquote><br>  O Linux fornece sete namespaces diferentes ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Os espa√ßos para nome da rede ( <code>CLONE_NEWNET</code> ) definem os recursos de rede dispon√≠veis para o processo: "Cada espa√ßo para nome da rede possui seus pr√≥prios dispositivos de rede, endere√ßos IP, tabelas de roteamento IP, diret√≥rio <code>/proc/net</code> , n√∫meros de porta e assim por diante" <i>( do artigo ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Namespaces in operation</a> ‚Äù)</i> . <br><br><h3>  Dispositivos Ethernet virtuais (Veth) </h3><br><blockquote>  ‚ÄúUm par de rede virtual (veth) oferece uma abstra√ß√£o na forma de um‚Äú canal ‚Äù, que pode ser usado para criar t√∫neis entre namespaces de rede ou para criar uma ponte para um dispositivo de rede f√≠sico em outro espa√ßo de rede.  Quando o espa√ßo para nome √© liberado, todos os dispositivos veth nele s√£o destru√≠dos. ‚Äù  <i>(na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">p√°gina do manual namespaces de rede</a> )</i> </blockquote><br>  V√° para o ch√£o e veja como tudo isso se relaciona com o cluster.  Primeiro, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">os plugins de rede</a> no Kubernetes s√£o diversos, e os plugins CNI s√£o um deles ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">por que n√£o o CNM?</a> ).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Kubelet</a> em cada n√≥ informa ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tempo de execu√ß√£o</a> do cont√™iner qual <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plug-in de rede</a> usar.  A interface de rede de cont√™iner ( <a href="">CNI</a> ) est√° entre o tempo de execu√ß√£o do cont√™iner e a implementa√ß√£o da rede.  E o plugin CNI j√° configura a rede. <br><br><blockquote>  ‚ÄúO plugin CNI √© selecionado passando a <code>--network-plugin=cni</code> linha de comando <code>--network-plugin=cni</code> cni para o Kubelet.  O Kubelet l√™ o arquivo em <code>--cni-conf-dir</code> (o padr√£o √© <code>/etc/cni/net.d</code> ) e usa a configura√ß√£o CNI deste arquivo para configurar a rede para cada arquivo. "  <i>(a partir de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Requisitos de plug-in de rede</a> )</i> </blockquote><br>  Os bin√°rios reais do plugin CNI est√£o em <code>-- cni-bin-dir</code> (o padr√£o √© <code>/opt/cni/bin</code> ). <br><br>  Observe que os <a href=""><code>kubelet.service</code></a> chamada <code>--network-plugin=cni</code> incluem <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  Primeiro de tudo, o Kubernetes cria um espa√ßo para nome de rede para a lareira, mesmo antes de chamar qualquer plug-in.  Isso √© implementado usando o cont√™iner de <code>pause</code> especial, que "serve como o" cont√™iner pai "para todos os cont√™ineres da lareira" <i>(do artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Cont√™iner Todo-Poderoso para Pausa</a> ")</i> .  O Kubernetes ent√£o executa o plugin CNI para anexar o cont√™iner de <code>pause</code> √† rede.  Todos os cont√™ineres de pod usam o <code>netns</code> desse cont√™iner de <code>pause</code> . <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  A <a href="">configura√ß√£o CNI usada</a> indica o uso do plug-in de <code>bridge</code> para configurar a ponte do software Linux (L2) no espa√ßo de nomes raiz chamado <code>cnio0</code> (o <a href="">nome padr√£o</a> √© <code>cni0</code> ), que atua como um gateway ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  Um par veth tamb√©m ser√° configurado para conectar a lareira √† ponte rec√©m-criada: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  Para atribuir informa√ß√µes L3, como endere√ßos IP, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plug</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">in IPAM</a> ( <code>ipam</code> ) √© chamado.  Nesse caso, o tipo <code>host-local</code> √© usado ", que armazena o estado localmente no sistema de arquivos host, o que garante a exclusividade dos endere√ßos IP em um host" <i>(a partir da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code> host-local</code></a> )</i> .  O plug-in IPAM retorna essas informa√ß√µes para o plug-in anterior ( <code>bridge</code> ), para que todas as rotas especificadas na configura√ß√£o possam ser configuradas ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  Se <code>gw</code> n√£o <code>gw</code> especificado, ele <a href="">ser√° retirado da sub-rede</a> .  A rota padr√£o tamb√©m √© configurada no espa√ßo para nome da rede da lareira, apontando para a ponte (que √© configurada como a primeira sub-rede IP da lareira). <br><br>  E o √∫ltimo detalhe importante: solicitamos o mascaramento ( <code>"ipMasq": true</code> ) para o tr√°fego proveniente da rede da lareira.  Realmente n√£o precisamos de NAT aqui, mas essa √© a configura√ß√£o do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> .  Portanto, para ser completo, devo mencionar que as entradas nas <code>iptables</code> plug-in de <code>bridge</code> est√£o configuradas para este exemplo espec√≠fico.  Todos os pacotes da lareira, cujo destinat√°rio n√£o est√° no intervalo <code>224.0.0.0/4</code> , <a href="">estar√£o atr√°s do NAT</a> , que n√£o atende exatamente ao requisito "todos os cont√™ineres podem se comunicar com outros cont√™ineres sem usar o NAT".  Bem, provaremos por que o NAT n√£o √© necess√°rio ... <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Roteamento da lareira </h3><br>  Agora estamos prontos para personalizar os pods.  Vamos examinar todos os espa√ßos de rede dos nomes de um dos n√≥s de trabalho e analisar um deles depois de criar a implanta√ß√£o do <code>nginx</code> <a href="">partir daqui</a> .  Usaremos <code>lsns</code> com a op√ß√£o <code>-t</code> para selecionar o tipo de espa√ßo para nome desejado (ou seja, <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  Usando a op√ß√£o <code>-i</code> para <code>ls</code> , podemos encontrar seus n√∫meros de inode: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Voc√™ tamb√©m pode listar todos os namespaces de rede usando <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  Para ver todos os processos em execu√ß√£o no espa√ßo de rede <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>4026532426</code> ), √© poss√≠vel executar, por exemplo, o seguinte comando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  Pode-se observar que, al√©m da <code>pause</code> neste pod, lan√ßamos o <code>nginx</code> .  O cont√™iner de <code>pause</code> compartilha os namespaces <code>net</code> e <code>ipc</code> com todos os outros cont√™ineres de pod.  Lembre-se do PID da <code>pause</code> - 27255;  n√≥s retornaremos a ele. <br><br>  Agora vamos ver o que o <code>kubectl</code> diz sobre este pod: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  Mais detalhes: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  Vemos o nome do pod - <code>nginx-65899c769f-wxdx6</code> - e o ID de um de seus cont√™ineres ( <code>nginx</code> ), mas nada foi dito sobre a <code>pause</code> .  Cavar um n√≥ de trabalho mais profundo para corresponder a todos os dados.  Lembre-se de que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes The Hard Way</a> n√£o usa o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Docker</a> , portanto, para obter detalhes sobre o cont√™iner, consultamos o utilit√°rio console consoleerderd-ctr <i>(consulte tamb√©m o artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Integra√ß√£o do containererd com o Kubernetes, substituindo o Docker, pronto para produ√ß√£o</a> " - <b>aprox. Transfer</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Conhecendo o <code>k8s.io</code> ( <code>k8s.io</code> ), √© poss√≠vel obter o ID do cont√™iner <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... e fa√ßa uma <code>pause</code> tamb√©m: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  O ID do cont√™iner <code>nginx</code> que termina em <code>‚Ä¶983c7</code> corresponde ao que obtivemos do <code>kubectl</code> .  Vamos ver se conseguimos descobrir qual cont√™iner de <code>pause</code> pertence ao pod do <code>nginx</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  Lembre-se de que os processos com o PID 27331 e 27355 est√£o em execu√ß√£o no espa√ßo para nome da rede <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... e: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Agora sabemos com certeza quais cont√™ineres est√£o sendo executados neste pod ( <code>nginx-65899c769f-wxdx6</code> ) e o espa√ßo para nome da rede ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pausa (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  Como isso est√° ( <code>nginx-65899c769f-wxdx6</code> ) conectado √† rede?  Usamos o PID 27255 recebido anteriormente da <code>pause</code> para executar comandos em seu namespace de rede ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  Para esses fins, usaremos o <code>nsenter</code> com a op√ß√£o <code>-t</code> que define o PID de destino e <code>-n</code> sem especificar um arquivo para entrar no espa√ßo de nomes de rede do processo de destino (27255).  Aqui est√° o que o <code>ip link show</code> dir√°: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... e <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Isso confirma que o endere√ßo IP obtido anteriormente pelo <code>kubectl get pod</code> est√° configurado na interface <code>eth0</code> .  Essa interface faz parte de um <b>par veth</b> , uma extremidade na lareira e a outra no espa√ßo para nome raiz.  Para descobrir a interface do segundo lado, usamos o <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  Vemos que o <code>ifindex</code> banquete √© 7. Verifique se ele est√° no espa√ßo para nome raiz.  Isso pode ser feito usando o <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  Para ter certeza disso, finalmente, vamos ver: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  √ìtimo, agora tudo est√° claro com o link virtual.  Usando <code>brctl</code> vamos ver quem mais est√° conectado √† ponte Linux: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  Ent√£o, a imagem √© a seguinte: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Verifica√ß√£o de roteamento </h3><br>  Como realmente encaminhamos o tr√°fego?  Vejamos a tabela de roteamento no pod de namespace da rede: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  Pelo menos sabemos como chegar ao espa√ßo para nome raiz ( <code>default via 10.200.0.1</code> ).  Agora vamos ver a tabela de roteamento de host: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  Sabemos como encaminhar pacotes para um roteador VPC (o VPC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">possui um</a> roteador "impl√≠cito", que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">geralmente possui um segundo endere√ßo</a> do espa√ßo de endere√ßo IP principal da sub-rede).  Agora: o roteador VPC sabe como acessar a rede de cada lareira?  N√£o, ele n√£o, portanto, presume-se que as rotas ser√£o configuradas pelo plug-in da CNI ou <a href="">manualmente</a> (como no manual).  Aparentemente, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS CNI-plugin</a> faz exatamente isso para n√≥s na AWS.  Lembre-se de que existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">muitos plugins CNI</a> e estamos considerando um exemplo de uma <b>configura√ß√£o de rede simples</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Imers√£o profunda no NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> crie dois cont√™ineres id√™nticos com o Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>busybox.yaml</code></a> ) <br><br>  Temos: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Pings de um cont√™iner para outro devem ser bem-sucedidos: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  Para entender o movimento do tr√°fego, voc√™ pode ver os pacotes usando <code>tcpdump</code> ou <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  O IP de origem do pod 10.200.0.21 √© convertido no endere√ßo IP do host 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  No iptables, voc√™ pode ver que as contagens est√£o aumentando: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  Por outro lado, se voc√™ remover <code>"ipMasq": true</code> da configura√ß√£o do plugin CNI, poder√° ver o seguinte (esta opera√ß√£o √© realizada exclusivamente para fins educacionais - n√£o recomendamos alterar a configura√ß√£o em um cluster ativo!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping ainda deve passar: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  E neste caso - sem usar o NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  Portanto, verificamos que "todos os cont√™ineres podem se comunicar com outros cont√™ineres sem usar o NAT". <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Rede de cluster (10.32.0.0/24) </h2><br>  Voc√™ deve ter notado no exemplo do <code>busybox</code> que os endere√ßos IP atribu√≠dos ao <code>busybox</code> eram diferentes em cada caso.  E se quis√©ssemos disponibilizar esses cont√™ineres para comunica√ß√£o de outros lares?  Pode-se pegar os endere√ßos IP atuais do pod, mas eles mudar√£o.  Por esse motivo, voc√™ precisa configurar o recurso <code>Service</code> , que far√° proxy de solicita√ß√µes para muitos lares de vida curta. <br><br><blockquote>  "O servi√ßo no Kubernetes √© uma abstra√ß√£o que define o conjunto l√≥gico de lareiras e as pol√≠ticas pelas quais elas podem ser acessadas."  <i>(da documenta√ß√£o dos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Servi√ßos Kubernetes</a> )</i> </blockquote><br>  Existem v√°rias maneiras de publicar um servi√ßo;  o tipo padr√£o √© <code>ClusterIP</code> , que define o endere√ßo IP do bloco CIDR do cluster (ou seja, acess√≠vel apenas a partir do cluster).  Um exemplo √© o complemento de cluster DNS configurado no Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> mostra que o <code>Service</code> lembra dos pontos finais e os converte: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  Como exatamente? .. <code>iptables</code> novamente.  Vamos examinar as regras criadas para este exemplo.  Sua lista completa pode ser vista com o comando <code>iptables-save</code> . <br><br>  Assim que os pacotes s√£o criados pelo processo ( <code>OUTPUT</code> ) ou chegam √† interface de rede ( <code>PREROUTING</code> ), eles passam pelas seguintes cadeias de <code>iptables</code> : <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  Os seguintes destinos correspondem aos pacotes TCP enviados para a 53¬™ porta em 10.32.0.10 e s√£o transmitidos ao destinat√°rio 10.200.0.27 com a 53¬™ porta: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  O mesmo para pacotes UDP (destinat√°rio 10.32.0.10:53 ‚Üí 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  Existem outros tipos de <code>Services</code> no Kubernetes.  Em particular, o Kubernetes The Hard Way <code>NodePort</code> sobre o <code>NodePort</code> - consulte <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publica o servi√ßo no endere√ßo IP de cada n√≥, colocando-o em uma porta est√°tica (chamada <code>NodePort</code> ).  <code>NodePort</code> pode ser acessado de fora do cluster.  Voc√™ pode verificar a porta dedicada (neste caso - 31088) usando o <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Agora, Under est√° dispon√≠vel na Internet como <code>http://${EXTERNAL_IP}:31088/</code> .  Aqui <code>EXTERNAL_IP</code> √© o endere√ßo IP p√∫blico de <b>qualquer inst√¢ncia de trabalho</b> .  Neste exemplo, usei o endere√ßo IP p√∫blico do <b>worker-0</b> .  A solicita√ß√£o √© recebida por um host com um endere√ßo IP interno 10.240.0.20 (o provedor de nuvem est√° envolvido no NAT p√∫blico); no entanto, o servi√ßo √© realmente iniciado em outro host ( <b>worker-1</b> , que pode ser visto pelo endere√ßo IP do terminal - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  O pacote √© enviado do <b>trabalhador-0</b> para o <b>trabalhador-1</b> , onde encontra seu destinat√°rio: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Esse circuito √© ideal?  Talvez n√£o, mas funciona.  Nesse caso, as regras do <code>iptables</code> programadas s√£o as seguintes: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  Em outras palavras, o endere√ßo para o destinat√°rio dos pacotes com porta 31088 √© transmitido em 10.200.1.18.  A porta tamb√©m est√° transmitindo, de 31088 a 80. <br><br>  N√£o tocamos em outro tipo de servi√ßo - o <code>LoadBalancer</code> - que disponibiliza publicamente o servi√ßo usando um balanceador de carga do provedor de nuvem, mas o artigo j√° era grande. <br><br><h2>  Conclus√£o </h2><br>  Pode parecer que h√° muita informa√ß√£o, mas apenas tocamos a ponta do iceberg.  No futuro, vou falar sobre IPv6, IPVS, eBPF e alguns plugins CNI atuais interessantes. <br><br><h2>  PS do tradutor </h2><br>  Leia tamb√©m em nosso blog: <br><br><ul><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Guia Ilustrado de Rede em Kubernetes</a> ‚Äù; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Compara√ß√£o do desempenho da rede para o Kubernetes</a> "; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Experimentos com proxy do kube e inacessibilidade do host no Kubernetes</a> ‚Äù; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhorando a confiabilidade do Kubernetes: como perceber rapidamente que um n√≥ caiu</a> ‚Äù; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Play with Kubernetes ‚Äî      K8s</a> ¬ª; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   Kubernetes   </a> ¬ª <i>( ,        Kubernetes)</i> ; </li><li> ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Container Networking Interface (CNI) ‚Äî      Linux-</a> ¬ª. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt420813/">https://habr.com/ru/post/pt420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt420799/index.html">MPS 2018.2: testes de gerador, plug-in GitHub, aspecto VCS, notifica√ß√µes de migra√ß√£o e muito mais</a></li>
<li><a href="../pt420803/index.html">Aulas de impress√£o 3D. Economizando pl√°stico ao imprimir modelos n√£o funcionais do 3Dtool</a></li>
<li><a href="../pt420805/index.html">[Translation] Quando usar fluxos paralelos</a></li>
<li><a href="../pt420809/index.html">Semana de seguran√ßa 31: cinquenta tons de inseguran√ßa no Android</a></li>
<li><a href="../pt420811/index.html">Rede descentralizada de mensagens e telefone da nova gera√ß√£o</a></li>
<li><a href="../pt420815/index.html">Como "decodificar o mundo digital" explodiu no corredor: os 10 principais relat√≥rios do DotNext 2018 Piter</a></li>
<li><a href="../pt420819/index.html">As 10 principais ferramentas Python para aprendizado de m√°quina e ci√™ncia de dados</a></li>
<li><a href="../pt420821/index.html">Regra 10: 1 em programa√ß√£o e reda√ß√£o</a></li>
<li><a href="../pt420825/index.html">Hoje ser√° a primeira partida entre os profissionais OpenAI e Dota 2 (pessoas vencidas). Entendemos como o bot funciona</a></li>
<li><a href="../pt420827/index.html">Crie um projeto simples do maven usando Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>