<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äç‚öñÔ∏è üì© üôéüèΩ ¬øEstall√≥ la burbuja de aprendizaje autom√°tico o el comienzo de un nuevo amanecer? üèº üî∫ üë®‚Äçüî¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recientemente, se ha publicado un art√≠culo que muestra una buena tendencia en el aprendizaje autom√°tico en los √∫ltimos a√±os. En resumen: el n√∫mero de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¬øEstall√≥ la burbuja de aprendizaje autom√°tico o el comienzo de un nuevo amanecer?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/recognitor/blog/455676/">  Recientemente, se ha publicado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un art√≠culo</a> que muestra una buena tendencia en el aprendizaje autom√°tico en los √∫ltimos a√±os.  En resumen: el n√∫mero de nuevas empresas en el campo del aprendizaje autom√°tico ha disminuido considerablemente en los √∫ltimos dos a√±os. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c6/466/4fc/1c64664fcaf125f2104e67547b533e41.png" alt="imagen"></div><br>  Pues que.  Analicemos "si estall√≥ la burbuja", "c√≥mo vivir" y hablemos de d√≥nde vino ese garabato. <br><a name="habracut"></a><br>  Primero, hablemos sobre cu√°l fue el refuerzo de esta curva.  ¬øDe d√≥nde vino ella?  Probablemente todos recordar√°n la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">victoria del</a> aprendizaje autom√°tico en 2012 en el concurso ImageNet.  Despu√©s de todo, ¬°este es el primer evento mundial!  Pero en realidad esto no es as√≠.  Y el crecimiento de la curva comienza un poco antes.  Lo dividir√≠a en varios puntos. <br><br><ol><li>  2008 es la aparici√≥n del t√©rmino "big data".  Los productos reales comenzaron <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">a aparecer</a> en 2010.  Big data est√° directamente relacionado con el aprendizaje autom√°tico.  Sin big data, el funcionamiento estable de los algoritmos que exist√≠an en ese momento es imposible.  Y estas no son redes neuronales.  Hasta 2012, las redes neuronales son la minor√≠a marginal.  Pero entonces comenzaron a funcionar algoritmos completamente diferentes, que hab√≠an existido durante a√±os, o incluso d√©cadas: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SVM</a> (1963, 1993), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Random Forest</a> (1995), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AdaBoost</a> (2003), ... Las nuevas empresas de esos a√±os se asocian principalmente con el procesamiento autom√°tico de datos estructurados. : taquillas, usuarios, publicidad, mucho m√°s. <br><br>  La derivada de esta primera ola es un conjunto de marcos como XGBoost, CatBoost, LightGBM, etc. <br></li><li>  En 2011-2012, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">las redes neuronales convolucionales</a> ganaron una serie de concursos de reconocimiento de im√°genes.  Su uso real fue algo retrasado.  Dir√≠a que las nuevas empresas y soluciones masivamente significativas comenzaron a aparecer en 2014.  Se necesitaron dos a√±os para digerir que las neuronas a√∫n funcionan, para crear marcos convenientes que se puedan instalar y ejecutar en un per√≠odo de tiempo razonable, para desarrollar m√©todos que estabilicen y aceleren el tiempo de convergencia. <br><br>  Las redes convolucionales permitieron resolver problemas de visi√≥n artificial: clasificaci√≥n de im√°genes y objetos en una imagen, detecci√≥n de objetos, reconocimiento de objetos y personas, mejora de im√°genes, etc., etc. </li><li>  2015-2017 a√±os.  El auge de los algoritmos y proyectos vinculados a las redes de recurrencia o sus an√°logos (LSTM, GRU, TransformerNet, etc.).  Han aparecido algoritmos de voz a texto y sistemas de traducci√≥n autom√°tica que funcionan bien.  En parte, se basan en redes convolucionales para resaltar caracter√≠sticas b√°sicas.  Parcialmente por el hecho de que aprendieron a recopilar conjuntos de datos realmente grandes y buenos. </li></ol><br><img src="https://habrastorage.org/webt/_c/bj/f2/_cbjf2doqjypuqwfwh1d8_vx92a.png"><br><br>  "¬øHa estallado la burbuja?"  ¬øHype se est√° sobrecalentando?  ¬øMurieron como una cadena de bloques? <br>  Pues bien!  Ma√±ana Siri dejar√° de funcionar en su tel√©fono, y pasado ma√±ana Tesla no distinguir√° un giro de un canguro. <br><br>  Las redes neuronales ya est√°n funcionando.  Est√°n en docenas de dispositivos.  Realmente te permiten ganar, cambiar el mercado y el mundo que te rodea.  Hype se ve un poco diferente: <br><br><img src="https://habrastorage.org/webt/zl/7m/ph/zl7mphh3m3rzprgxpbaopha3gwy.png"><br><br>  Es solo que las redes neuronales han dejado de ser algo nuevo.  S√≠, muchas personas tienen altas expectativas.  Pero una gran cantidad de compa√±√≠as han aprendido a usar sus neuronas y a fabricar productos basados ‚Äã‚Äãen ellas.  Las neuronas dan una nueva funcionalidad, pueden reducir trabajos, reducir el precio de los servicios: <br><br><ul><li>  Las empresas manufactureras integran algoritmos para el an√°lisis de rechazos en el transportador. </li><li>  Las granjas ganaderas est√°n comprando sistemas para controlar las vacas. </li><li>  Cosechadoras autom√°ticas. </li><li>  Centros de llamadas automatizados. </li><li>  Filtros en Snapchat.  ( <s>bueno, al menos algo sensato!</s> ) </li></ul><br>  Pero lo principal, y no lo m√°s obvio: "No hay m√°s ideas nuevas, o no traer√°n capital instant√°neo".  Las redes neuronales han resuelto docenas de problemas.  Y ellos decidir√°n a√∫n m√°s.  Todas las ideas obvias que fueron - generaron muchas nuevas empresas.  Pero todo lo que estaba en la superficie ya ha sido recolectado.  En los √∫ltimos dos a√±os, no he encontrado una sola idea nueva para el uso de redes neuronales.  No hay un solo enfoque nuevo (bueno, est√° bien, hay algunos problemas con las GAN). <br><br>  Y cada pr√≥xima puesta en marcha es cada vez m√°s complicada.  Ya no se requieren dos tipos que entrenen a una neurona en datos abiertos.  Requiere programadores, un servidor, un equipo de redactores, soporte complejo, etc. <br><br>  Como resultado, hay menos startups.  Pero la producci√≥n es m√°s.  ¬øNecesita adjuntar el reconocimiento de matr√≠cula?  Hay cientos de profesionales con experiencia relevante en el mercado.  Puede contratar y en un par de meses su empleado crear√° un sistema.  O compre uno terminado.  Pero haciendo una nueva startup? .. Locura! <br><br>  Necesitamos crear un sistema para rastrear a los visitantes: ¬øpor qu√© pagar por un mont√≥n de licencias? Si puede hacer las suyas durante 3-4 meses, agud√≠celas para su negocio. <br><br>  Ahora las redes neuronales siguen el mismo camino que docenas de otras tecnolog√≠as. <br><br>  ¬øRecuerdas c√≥mo ha cambiado el concepto de "desarrollador de sitios" desde 1995?  Si bien el mercado no est√° saturado de especialistas.  Hay muy pocos profesionales.  Pero puedo apostar que en 5-10 a√±os no habr√° mucha diferencia entre un programador de Java y un desarrollador de redes neuronales.  Y esos y esos especialistas ser√°n suficientes en el mercado. <br><br>  Simplemente habr√° una clase de tareas para las que las neuronas resuelven.  Hab√≠a una tarea: contratar a un especialista. <br><br>  <b>‚Äú¬øY luego qu√©?</b>  <b>¬øD√≥nde est√° la inteligencia artificial prometida?</b> <br><br>  Y aqu√≠ hay un peque√±o pero interesante neponyatchka :) <br><br>  La pila de tecnolog√≠a que existe hoy, aparentemente, todav√≠a no nos llevar√° a la inteligencia artificial.  Las ideas, su novedad, se han agotado en gran medida.  Hablemos de lo que mantiene el nivel actual de desarrollo. <br><br><h3>  Limitaciones </h3><br>  Comencemos con los drones autom√°ticos.  Parece entenderse que es posible fabricar autom√≥viles totalmente aut√≥nomos con las tecnolog√≠as actuales.  Pero despu√©s de cu√°ntos a√±os esto suceder√° no est√° claro.  Tesla cree que esto suceder√° en un par de a√±os. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Ucp0TTmvqOE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Hay muchos otros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">especialistas</a> que califican esto como 5-10 a√±os. <br><br>  Lo m√°s probable, en mi opini√≥n, despu√©s de 15 a√±os, la infraestructura de las ciudades en s√≠ misma cambiar√° para que la aparici√≥n de autom√≥viles aut√≥nomos sea inevitable, ser√° su continuaci√≥n.  Pero esto no puede considerarse inteligencia.  Modern Tesla es una tuber√≠a muy compleja para filtrar datos, buscarlos y volver a capacitarlos.  Estas son reglas, reglas, reglas, recopilaci√≥n de datos y filtros sobre ellos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> escrib√≠ un poco m√°s al respecto, o mire desde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este</a> punto). <br><br><h3>  Primer problema </h3><br>  Y es aqu√≠ donde vemos el <b>primer problema fundamental</b> .  Big data  Esto es exactamente lo que gener√≥ la ola actual de redes neuronales y aprendizaje autom√°tico.  Ahora, para hacer algo complejo y autom√°tico, necesita muchos datos.  No solo mucho, sino mucho, mucho.  Necesitamos algoritmos automatizados para su recopilaci√≥n, marcado, uso.  Queremos hacer que el autom√≥vil vea camiones contra el sol; primero debemos recoger un n√∫mero suficiente de ellos.  Queremos que el auto no se vuelva loco con una bicicleta atornillada a la cajuela - m√°s muestras. <br><br>  Adem√°s, un ejemplo no es suficiente.  Cientos?  Miles? <br><br><img src="https://habrastorage.org/webt/hl/tm/ip/hltmipml3fq_m-md4ansomrxjmk.jpeg"><br><br><h3>  Segundo problema </h3><br>  <b>El segundo problema</b> es la visualizaci√≥n de lo que ha entendido nuestra red neuronal.  Esta es una tarea muy no trivial.  Hasta ahora, pocas personas entienden c√≥mo visualizar esto.  Estos art√≠culos son muy recientes, estos son solo algunos ejemplos, incluso remotos: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Visualizaci√≥n de</a> fijaci√≥n en texturas.  Muestra bien lo que la neurona tiende a ir en ciclos + lo que ella percibe como informaci√≥n inicial. <br><br><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="imagen"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Visualizaci√≥n de</a> atenuaci√≥n durante las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducciones</a> .  Realmente, la atenuaci√≥n a menudo se puede usar con precisi√≥n para mostrar qu√© caus√≥ tal reacci√≥n de red.  Conoc√≠ tales cosas para depurar y para soluciones de productos.  Hay muchos art√≠culos sobre este tema.  Pero cuanto m√°s complejos son los datos, m√°s dif√≠cil es comprender c√≥mo lograr una visualizaci√≥n sostenible. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/154/93f/988/15493f988978760639233843c9c28c91.png" alt="imagen"><br><br>  Bueno, s√≠, el viejo conjunto de "mira lo que hay dentro de la rejilla en los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">filtros</a> ".  Estas im√°genes eran populares hace unos 3-4 a√±os, pero todos se dieron cuenta r√°pidamente de que las im√°genes son hermosas, pero no tienen mucho sentido. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f68/78c/a4a/f6878ca4a732ef4890e1ad9d8a369896.jpg" alt="imagen"><br><br>  No mencion√© docenas de otras lociones, m√©todos, hacks, estudios sobre c√≥mo mostrar el interior de la red.  ¬øEstas herramientas funcionan?  ¬øTe ayudan a entender r√°pidamente cu√°l es el problema y a depurar la red? ... ¬øSacar el √∫ltimo porcentaje?  Bueno, algo como esto: <br><br><img width="600" src="https://habrastorage.org/webt/eo/p0/i6/eop0i67mupthvfm86dwn139kkha.jpeg"><br><br>  Puedes ver cualquier concurso en Kaggle.  Y una descripci√≥n de c√≥mo las personas toman las decisiones finales.  ¬°Llegamos al modelo 100-500-800 mulenov y funcion√≥! <br><br>  Por supuesto, exagero.  Pero estos enfoques no dan respuestas r√°pidas y directas. <br><br>  Teniendo suficiente experiencia, despu√©s de haber elegido diferentes opciones, puede emitir un veredicto sobre por qu√© su sistema tom√≥ tal decisi√≥n.  Pero corregir el comportamiento del sistema ser√° dif√≠cil.  Coloque una muleta, mueva el umbral, agregue un conjunto de datos, tome otra red de fondo. <br><br><h3>  Tercer problema </h3><br>  <b>El tercer problema fundamental</b> es que las cuadr√≠culas no ense√±an l√≥gica, sino estad√≠stica.  Estad√≠sticamente esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">persona</a> : <br><br><img src="https://habrastorage.org/webt/wa/lg/6h/walg6hlyvy_cvd7i6oojypaa0lc.png" alt="imagen"><br><br>  L√≥gicamente, no muy similar.  Las redes neuronales no aprenden algo complicado si no son forzadas.  Siempre aprenden los s√≠ntomas m√°s simples.  Tiene ojos, nariz, cabeza?  Entonces esta cara!  O d√© un ejemplo donde los ojos no signifiquen la cara.  Y de nuevo, millones de ejemplos. <br><br><h3>  Hay mucho espacio en la parte inferior </h3><br>  Dir√≠a que son estos tres problemas globales los que hoy limitan el desarrollo de las redes neuronales y el aprendizaje autom√°tico.  Y donde estos problemas no se limitaron ya se usa activamente. <br><br>  <b>¬øEs este el final?</b>  <b>Las redes neuronales se levantaron?</b> <br><br>  Desconocido  Pero, por supuesto, todos esperan que no. <br><br>  Hay muchos enfoques y direcciones para resolver los problemas fundamentales que he cubierto anteriormente.  Pero hasta ahora, ninguno de estos enfoques nos ha permitido hacer algo fundamentalmente nuevo, resolver algo que a√∫n no se ha resuelto.  Hasta ahora, todos los proyectos fundamentales se realizan sobre la base de enfoques estables (Tesla), o siguen siendo proyectos de prueba de institutos o corporaciones (Google Brain, OpenAI). <br><br>  En t√©rminos generales, la direcci√≥n principal es la creaci√≥n de una representaci√≥n de alto nivel de los datos de entrada.  En cierto sentido, "memoria".  El ejemplo m√°s simple de memoria son las diversas representaciones de "incrustaci√≥n" de im√°genes.  Bueno, por ejemplo, todos los sistemas de reconocimiento facial.  La red aprende a obtener de la cara una cierta idea estable que no depende de la rotaci√≥n, la iluminaci√≥n y la resoluci√≥n.  De hecho, la red minimiza la m√©trica de "caras diferentes - lejos" e "id√©ntico - cerca". <br><br><img src="https://habrastorage.org/webt/8z/re/sp/8zrespvq6y2unwlyuaeovq3fj58.png"><br><br>  Tal entrenamiento requiere decenas y cientos de miles de ejemplos.  Pero el resultado trae algunos rudimentos de "Aprendizaje √∫nico".  Ahora no necesitamos cientos de caras para recordar a una persona.  Solo una cara, y eso es todo, ¬°lo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">descubriremos</a> ! <br>  Solo aqu√≠ est√° el problema ... La cuadr√≠cula solo puede aprender objetos bastante simples.  Cuando se trata de distinguir no caras, sino, por ejemplo, "personas vestidas" (la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tarea de redentificaci√≥n</a> ), la calidad falla en muchos √≥rdenes de magnitud.  Y la red ya no puede aprender suficientes cambios de √°ngulo obvios. <br><br>  Y aprender de millones de ejemplos tambi√©n es de alguna manera un entretenimiento regular. <br><br>  Hay trabajo para reducir significativamente las elecciones.  Por ejemplo, puede recuperar de inmediato uno de los primeros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajos de Google OneShot</a> <b>Learning</b> : <br><br><img src="https://habrastorage.org/webt/dv/h5/zt/dvh5zte1ggsunwya3tm40zqwiec.png"><br><br>  Hay muchos de estos trabajos, por ejemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> . <br><br>  Hay una desventaja: por lo general, el entrenamiento funciona bien en algunos "ejemplos MNIST'ovskie" simples.  Y en la transici√≥n a tareas complejas, necesita una base grande, un modelo de objetos o alg√∫n tipo de magia. <br>  En general, el trabajo en el entrenamiento One-Shot es un tema muy interesante.  Encuentras muchas ideas.  Pero en su mayor parte, los dos problemas que he enumerado (capacitaci√≥n previa en un gran conjunto de datos / inestabilidad en datos complejos) est√°n obstaculizando el aprendizaje. <br><br>  Por otro lado, GAN - redes generativamente competitivas - se acerca a la integraci√≥n.  Probablemente ley√≥ un mont√≥n de art√≠culos sobre este tema en Habr√©.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> ) <br>  Una caracter√≠stica de la GAN es la formaci√≥n de un espacio de estado interno (esencialmente la misma incrustaci√≥n), que le permite dibujar una imagen.  Pueden ser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">personas</a> , puede haber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">acciones</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e25/77f/f6d/e2577ff6d435046c44f861ab5d3ce2b3.jpg" alt="imagen"><br><br>  El problema de GAN es que cuanto m√°s complejo es el objeto generado, m√°s dif√≠cil es describirlo en la l√≥gica del "generador-discriminador".  Como resultado, a partir de aplicaciones reales de GAN, que solo se escuchan DeepFake, que, nuevamente, manipula las representaciones de los individuos (para lo cual existe una base enorme). <br><br>  He encontrado muy pocas otras aplicaciones √∫tiles.  Por lo general, una especie de silbato falso con dibujos. <br><br>  Y de nuevo.  Nadie comprende c√≥mo esto nos permitir√° avanzar hacia un futuro m√°s brillante.  Representar la l√≥gica / espacio en una red neuronal es bueno.  Pero necesitamos una gran cantidad de ejemplos, no entendemos c√≥mo esta neurona se representa a s√≠ misma, no entendemos c√≥mo hacer que la neurona recuerde alguna idea realmente complicada. <br><br>  <b>El aprendizaje por refuerzo</b> es un enfoque completamente diferente.  Seguramente recuerdas c√≥mo Google venci√≥ a todos en Go.  Victorias recientes en Starcraft y Dota.  Pero aqu√≠ todo est√° lejos de ser tan optimista y prometedor.  Lo mejor de RL y su complejidad es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este art√≠culo</a> . <br><br>  Para resumir brevemente lo que escribi√≥ el autor: <br><br><ul><li>  Los modelos listos para usar no se ajustan / funcionan mal en la mayor√≠a de los casos </li><li>  Las tareas pr√°cticas son m√°s f√°ciles de resolver de otras maneras.  Boston Dynamics no usa RL debido a su complejidad / imprevisibilidad / complejidad computacional </li><li>  Para que RL funcione, necesita una funci√≥n compleja.  A menudo es dif√≠cil crear / escribir. </li><li>  Es dif√≠cil entrenar modelos.  Tenemos que pasar mucho tiempo para balancearnos y salir de optima local </li><li>  Como resultado, es dif√≠cil repetir el modelo, la inestabilidad del modelo al m√°s m√≠nimo cambio. </li><li>  A menudo se sobrellena en algunos patrones izquierdos, hasta el generador de n√∫meros aleatorios </li></ul><br>  El punto clave es que RL a√∫n no funciona en producci√≥n.  Google tiene alg√∫n tipo de experimentos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> ).  Pero no he visto un solo sistema de abarrotes. <br><br>  <b>Memoria</b>  La desventaja de todo lo que se describe arriba no est√° estructurada.  Un enfoque para tratar de ordenar todo esto es proporcionar a la red neuronal acceso a una memoria separada.  Para que pueda grabar y reescribir los resultados de sus pasos all√≠.  Entonces la red neuronal se puede determinar por el estado actual de la memoria.  Esto es muy similar a los procesadores y computadoras cl√°sicos. <br><br>  El art√≠culo m√°s famoso y popular es de DeepMind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b66/f0d/3a9/b66f0d3a9da550ad89e677eb4453a6bf.png" alt="imagen"><br><br>  Parece que aqu√≠ est√°, ¬øla clave para entender la inteligencia?  Pero m√°s bien, no.  El sistema todav√≠a necesita una gran cantidad de datos para la capacitaci√≥n.  Y funciona principalmente con datos tabulares estructurados.  Al mismo tiempo, cuando Facebook <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">resolvi√≥ un</a> problema similar, siguieron el camino "ven la memoria, solo hacen que la neurona sea m√°s complicada, pero m√°s ejemplos, y se aprender√° por s√≠ misma". <br><br>  <b>Desenredamiento</b> .  Otra forma de crear una memoria significativa es tomar las mismas incrustaciones, pero al aprender a introducir criterios adicionales que les permitan resaltar "significados" en ellos.  Por ejemplo, queremos entrenar una red neuronal para distinguir entre el comportamiento de una persona en una tienda.  Si sigui√©ramos el camino est√°ndar, tendr√≠amos que hacer una docena de redes.  Uno est√° buscando a una persona, el segundo determina lo que est√° haciendo, el tercero es su edad, el cuarto es el g√©nero.  La l√≥gica separada mira la parte de la tienda donde la hace / aprende.  El tercero determina su trayectoria, etc. <br><br>  O, si hubiera una cantidad infinita de datos, entonces ser√≠a posible entrenar una red para todo tipo de resultados (es obvio que tal conjunto de datos no se puede escribir). <br><br>  El enfoque de desinserci√≥n nos dice, y capacitemos a la red para que pueda distinguir entre conceptos.  Para que ella pueda integrarse en el video, donde un √°rea determinar√≠a la acci√≥n, una, la posici√≥n en el piso a tiempo, una, la altura de la persona y otra, su g√©nero.  Al mismo tiempo, durante el entrenamiento, me gustar√≠a sugerir casi nunca tales conceptos clave a la red, sino para que identifique y agrupe las √°reas.  Hay pocos art√≠culos de este tipo (algunos de ellos son <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> ) y en general son bastante te√≥ricos. <br><br>  Pero esta direcci√≥n, al menos te√≥ricamente, deber√≠a cubrir los problemas enumerados al principio. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/314/bd2/7ab/314bd27abc46e22c8c64430c6b6b9211.gif" alt="imagen"><br><br>  Descomposici√≥n de la imagen seg√∫n los par√°metros "color de pared / color de piso / forma de objeto / color de objeto / etc." <br><br><img src="https://habrastorage.org/webt/km/md/fb/kmmdfbtnliixqj3d5szfofcqe7q.jpeg"><br><br>  Descomposici√≥n de la cara seg√∫n los par√°metros "tama√±o, cejas, orientaci√≥n, color de piel, etc." <br><br><h3>  Otros </h3><br>  Hay muchas otras direcciones no tan globales que nos permiten reducir de alguna manera la base, trabajar con datos m√°s heterog√©neos, etc. <br><br>  <b>Atencion</b>  Probablemente no tenga sentido aislar esto como un m√©todo separado.  Solo un enfoque que refuerza a los dem√°s.  Se le han dedicado muchos art√≠culos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">3</a> ).  El significado de Atenci√≥n es fortalecer la respuesta de la red a objetos importantes durante el entrenamiento.  A menudo por alguna designaci√≥n de destino externo, o una peque√±a red externa. <br><br>  <b>Simulaci√≥n 3D</b>  Si haces un buen motor 3D, a menudo puedes cerrar el 90% de los datos de entrenamiento con √©l (incluso vi un ejemplo en el que casi el 99% de los datos se cerr√≥ con un buen motor).  Hay muchas ideas y trucos sobre c√≥mo hacer que una red entrenada en un motor 3D funcione con datos reales (ajuste fino, transferencia de estilo, etc.).  Pero a menudo, hacer un buen motor es varios √≥rdenes de magnitud m√°s dif√≠cil que recopilar datos.  Ejemplos al hacer motores: <br>  Entrenamiento de robots ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">google</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">braingarden</a> ) <br>  Aprendiendo a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">reconocer los</a> productos en una tienda (pero en dos proyectos que hicimos, lo dispensamos con calma). <br>  Entrenamiento en Tesla (nuevamente, el video que estaba arriba). <br><br><h2>  Conclusiones </h2><br>  Todo el art√≠culo es, en cierto sentido, conclusiones.  Probablemente el mensaje principal que quer√≠a hacer era "se acab√≥ el obsequio, las neuronas no dan soluciones m√°s simples".  Ahora tenemos que trabajar duro para construir soluciones complejas.  O trabajar duro haciendo informes cient√≠ficos complejos. <br><br>  En general, el tema es discutible.  ¬øQuiz√°s los lectores tienen ejemplos m√°s interesantes? </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455676/">https://habr.com/ru/post/455676/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455658/index.html">El legendario Intel Core i7-2600K: prueba de Sandy Bridge en 2019 (parte 3)</a></li>
<li><a href="../455662/index.html">Gran pantalla mec√°nica con mecanismo de leva como decodificador.</a></li>
<li><a href="../455666/index.html">Creaci√≥n de ventas salientes en una empresa de servicios de TI</a></li>
<li><a href="../455668/index.html">Escribimos bajo FPGA sin HDL. Comparaci√≥n de herramientas de desarrollo de alto nivel.</a></li>
<li><a href="../455670/index.html">C√≥mo las impresoras 3D imprimen huesos, vasos sangu√≠neos y √≥rganos</a></li>
<li><a href="../455678/index.html">En el camino de Sergey Pavlovich Korolev. Proyecto tripulado ruso moderno. Parte 1. "Federaci√≥n"</a></li>
<li><a href="../455682/index.html">¬øCu√°nto gastas en infraestructura? ¬øY c√≥mo ahorrar en esto?</a></li>
<li><a href="../455684/index.html">¬øPor qu√© realizamos un hackathon para probadores?</a></li>
<li><a href="../455686/index.html">¬øC√≥mo elegir la mejor herramienta de gesti√≥n de proyectos si eres un milenio?</a></li>
<li><a href="../455692/index.html">ASZP: el restyling o el teatro comienzan con una percha</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>