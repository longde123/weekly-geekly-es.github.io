<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ÅâÔ∏è üë®‚Äçüë©‚Äçüëß üíà KVM (unter) VDI mit einmaligen virtuellen Maschinen unter Verwendung von Bash üèáüèø üöî ‚úåüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="F√ºr wen ist dieser Artikel? 
 Dieser Artikel kann f√ºr Systemadministratoren von Interesse sein, die vor der Aufgabe standen, einen "einmaligen" Jobdie...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KVM (unter) VDI mit einmaligen virtuellen Maschinen unter Verwendung von Bash</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462203/"><h4>  F√ºr wen ist dieser Artikel? </h4><br>  Dieser Artikel kann f√ºr Systemadministratoren von Interesse sein, die vor der Aufgabe standen, einen "einmaligen" Jobdienst zu erstellen. <br><br><h4>  Prolog </h4><br>  Die IT-Support-Abteilung eines jungen, sich dynamisch entwickelnden Unternehmens mit einem kleinen regionalen Netzwerk wurde gebeten, "Self-Service-Stationen" f√ºr die Nutzung durch externe Kunden zu organisieren.  Die Stationsdaten sollten f√ºr die Registrierung auf den externen Portalen des Unternehmens, das Herunterladen von Daten von externen Ger√§ten und die Arbeit mit Regierungsportalen verwendet werden. <br><br>  Ein wichtiger Aspekt war die Tatsache, dass der gr√∂√üte Teil der Software unter MS Windows ‚Äûgesch√§rft‚Äú wird (z. B. ‚ÄûDeklaration‚Äú), und trotz der Hinwendung zu offenen Formaten bleibt MS Office der dominierende Standard beim Austausch elektronischer Dokumente.  Daher konnten wir MS Windows bei der L√∂sung dieses Problems nicht ablehnen. <br><a name="habracut"></a><br>  Das Hauptproblem war die M√∂glichkeit, verschiedene Daten aus Benutzersitzungen zu sammeln, die zu deren Weitergabe an Dritte f√ºhren k√∂nnten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diese Situation hat den MFC bereits im Stich gelassen</a> .  Im Gegensatz zur quasi-gerichtlichen (staatlich autonomen Institution) MFC werden nichtstaatliche Organisationen f√ºr solche M√§ngel viel st√§rker bestraft.  Das n√§chste kritische Problem war die Anforderung, mit externen Speichermedien zu arbeiten, auf denen auf jeden Fall eine Menge b√∂sartiger Malware vorhanden sein wird.  Die Wahrscheinlichkeit des Eintritts von Malware aus dem Internet wurde aufgrund der Einschr√§nkung des Zugangs zum Internet √ºber eine wei√üe Adressliste als weniger wahrscheinlich angesehen. Mitarbeiter anderer Abteilungen erarbeiteten gemeinsam die Anforderungen, stellten ihre Anforderungen und W√ºnsche. Die endg√ºltigen Anforderungen sahen wie folgt aus: <br><br>  <b>IS-Anforderungen</b> <br><br><ul><li>  Nach der Verwendung sollten alle Benutzerdaten (einschlie√ülich tempor√§rer Dateien und Registrierungsschl√ºssel) gel√∂scht werden. </li><li>  Alle vom Benutzer gestarteten Prozesse sollten am Ende der Arbeit abgeschlossen sein. </li><li>  Internetzugang √ºber eine wei√üe Liste von Adressen. </li><li>  Einschr√§nkungen bei der Ausf√ºhrung von Code von Drittanbietern. </li><li>  Wenn die Sitzung l√§nger als 5 Minuten inaktiv ist, sollte die Sitzung automatisch beendet werden und die Station sollte eine Bereinigung durchf√ºhren. </li></ul><br>  <b>Kundenanforderungen</b> <br><br><ul><li>  Die Anzahl der Client-Stationen pro Zweigstelle betr√§gt nicht mehr als 4. </li><li>  Die minimale Wartezeit f√ºr die Systembereitschaft, von dem Moment an, als ich mich auf einen Stuhl setzte, bis zum Beginn der Arbeit mit Client-Software. </li><li>  Die M√∂glichkeit, Peripherieger√§te (Scanner, Flash-Laufwerke) direkt vom Installationsort der "Self-Service-Station" aus anzuschlie√üen. </li><li>  Kundenw√ºnsche </li><li>  Vorf√ºhrung von Werbematerialien (Bilder) zum Zeitpunkt der Schlie√üung des Komplexes. </li></ul><cut></cut><br><h4>  Mehl der Kreativit√§t </h4><br>  Nachdem wir genug mit Windows LiveCD gespielt hatten, kamen wir einstimmig zu dem Schluss, dass die resultierende L√∂sung nicht mindestens 3 kritische Punkte erf√ºllt.  Sie sind entweder f√ºr eine lange Zeit geladen oder nicht ganz lebendig, oder ihre Anpassung war mit wilden Schmerzen verbunden.  Vielleicht haben wir schlecht gesucht, und Sie k√∂nnen eine Reihe von Tools empfehlen, ich werde Ihnen dankbar sein. <br><br>  Weiter haben wir uns mit VDI befasst, aber f√ºr diese Aufgabe sind die meisten L√∂sungen entweder zu teuer oder erfordern besondere Aufmerksamkeit.  Und ich wollte ein einfaches Tool mit einem Minimum an Magie, dessen Probleme gr√∂√ütenteils durch einen einfachen Neustart / Neustart des Dienstes gel√∂st werden konnten.  Gl√ºcklicherweise hatten wir Serverausr√ºstung, Low-End-Klasse in den Filialen, aus dem stillgelegten Dienst, die wir f√ºr die technologische Basis verwenden konnten. <br><br>  Was ist das Ergebnis?  Aber ich kann Ihnen nicht sagen, was am Ende passiert ist, weil die NDA, aber w√§hrend der Suche haben wir ein interessantes Schema entwickelt, das sich in Labortests gut gezeigt hat, obwohl es nicht in Serie ging. <br><br>  Einige Haftungsausschl√ºsse: Der Autor behauptet nicht, dass die vorgeschlagene L√∂sung alle Aufgaben vollst√§ndig l√∂st und dies freiwillig und mit dem Lied tut.  Der Autor stimmt im Voraus der Aussage zu, dass Sein Englishe sprache zehr schlecht ist.  Da sich die L√∂sung nicht mehr entwickelt, k√∂nnen Sie nicht mit einer Fehlerbehebung oder einer √Ñnderung der Funktionalit√§t rechnen. Alles liegt in Ihren H√§nden.  Der Autor geht davon aus, dass Sie mit KVM zumindest ein wenig vertraut sind und einen √úbersichtsartikel zum Spice-Protokoll gelesen haben und ein wenig mit Centos oder einer anderen GNU Linux-Distribution gearbeitet haben. <br><br>  In diesem Artikel m√∂chte ich das R√ºckgrat der resultierenden L√∂sung analysieren, n√§mlich die Interaktion von Client und Server und die Essenz der Prozesse im Lebenszyklus virtueller Maschinen im Rahmen der betreffenden L√∂sung.  Wenn der Artikel f√ºr die √ñffentlichkeit interessant sein soll, beschreibe ich die Details der Implementierung von Live-Images zum Erstellen von Thin Clients auf Fedora-Basis und erl√§utere die Details der Optimierung virtueller Maschinen und KVM-Server zur Optimierung von Leistung und Sicherheit. <br><br>  Wenn Sie farbiges Papier nehmen, <br>  Farben, Pinsel und Kleber, <br>  Und ein bisschen mehr Geschicklichkeit ... <br>  Sie k√∂nnen hundert Rubel machen! <br><br><h4>  Schema und Beschreibung des Pr√ºfstands </h4><br><img src="https://habrastorage.org/webt/pu/tu/rk/puturkaiwqcpbp4wld7ezdk_lcw.png"><br><br>  Alle Ger√§te befinden sich im Filialnetz, nur der Internetkanal geht aus.  In der Vergangenheit gab es bereits einen Proxyserver, es ist nichts Au√üergew√∂hnliches.  Daraufhin wird unter anderem der Datenverkehr von virtuellen Maschinen gefiltert (Abk. VM sp√§ter im Text).  Nichts hindert Sie daran, diesen Dienst auf dem KVM-Server zu platzieren. Sie m√ºssen lediglich beobachten, wie sich die Last auf dem Festplattensubsystem √§ndert. <br><br>  Client Station - in der Tat "Self-Service-Stationen", "Front-End" unseres Service.  Sind Nettops von Lenovo IdeaCentre.  Wof√ºr ist dieses Ger√§t gut?  Ja, fast alle, besonders zufrieden mit der gro√üen Anzahl von USB-Anschl√ºssen und Kartenlesern auf der Vorderseite.  In unserem Schema wird eine SD-Karte mit Hardware-Schreibschutz in den Kartenleser eingelegt, auf der das modifizierte Livebild von Fedora 28 aufgezeichnet wird. Nat√ºrlich sind ein Monitor, eine Tastatur und eine Maus mit dem Nettop verbunden. <br><br>  Schalter - ein unauff√§lliger Hardware-Schalter der zweiten Ebene, der sich im Serverraum befindet und mit Lichtern blinkt.  Es ist mit keinem Netzwerk au√üer dem Netzwerk der ‚ÄûSelbstbedienungsstationen‚Äú verbunden. <br><br>  KVM_Server ist der Kern der Schaltung. In den Bench-Tests des Core 2 Quad Q9650 mit 8 GB RAM wurden sicher 3 virtuelle Windows 10-Maschinen auf sich gezogen.  Festplattensubsystem - adaptec 3405 2 Laufwerke Raid 1 + SSD.  In Feldversuchen des Xeon 1220 zog die ernstere LSI 9260 + SSD leicht 5-6 VMs.  Wir w√ºrden den Server vom pensionierten Dienst bekommen, es w√ºrde nicht viel Kapitalkosten geben.  Auf diesen Servern wird das KVM-Virtualisierungssystem mit dem Pool der virtuellen Maschine pool_Vm bereitgestellt. <br><br>  VM ist eine virtuelle Maschine, das Backend unseres Service.  Es ist die Arbeit des Benutzers. <br><br>  Enp5s0 ist eine Netzwerkschnittstelle, die auf das Netzwerk von "Self-Service-Stationen", dhcpd, ntpd, httpd live darauf und xinetd auf den "Signal" -Port blickt. <br><br>  Lo0 ist die Loopback-Pseudo-Schnittstelle.  Standard. <br><br>  Spice_console - Eine sehr interessante Sache ist die Tatsache, dass im Gegensatz zum klassischen RDP beim Einschalten des KVM + Spice-Protokollpakets eine zus√§tzliche Entit√§t angezeigt wird - der Konsolenport der virtuellen Maschine.  Wenn wir eine Verbindung zu diesem TCP-Port herstellen, erhalten wir die VM-Konsole, ohne dass eine Verbindung zu VM √ºber die Netzwerkschnittstelle hergestellt werden muss.  Alle Interaktionen mit Vm zur Signal√ºbertragung √ºbernimmt der Server.  Das n√§chstgelegene Analogon in der Funktion ist IPKVM.  Das hei√üt,  Das Image des VM-Monitors wird an diesen Port √ºbertragen, Daten zur Mausbewegung werden an diesen Port √ºbertragen, und (was am wichtigsten ist) die Interaktion √ºber das Spice-Protokoll erm√∂glicht es Ihnen, USB-Ger√§te nahtlos an die virtuelle Maschine umzuleiten, als ob dieses Ger√§t mit der VM selbst verbunden w√§re.  Getestet f√ºr Flash-Laufwerke, Scanner, Webcams. <br><br>  Vnet0-, virbr0- und virtuelle Netzwerkkarten Vm bilden ein Netzwerk virtueller Maschinen. <br><br><h4>  Wie funktioniert es? </h4><br>  Von der Client Station <br><br>  Die Client-Station startet im grafischen Modus vom modifizierten Live-Image von Fedora 28 und empf√§ngt die IP-Adresse per DHCP aus dem Netzwerkadressraum 169.254.24.0/24.  W√§hrend des Startvorgangs werden Firewall-Regeln erstellt, die Verbindungen zu den Server-Ports "Signal" und "Spice" erm√∂glichen.  Nach Abschluss des Downloads wartet die Station auf die Autorisierung des Client-Benutzers.  Nach der Benutzerautorisierung wird der Desktop-Manager "openbox" gestartet und das Autostart-Autostart-Skript im Namen des autorisierten Benutzers ausgef√ºhrt.  Das Autorun-Skript f√ºhrt unter anderem das Skript remote.sh aus. <br><br><div class="spoiler">  <b class="spoiler_title">$ HOME / .config / openbox / scripts / remote.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh server_ip=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "server_ip" \ |/usr/bin/cut -d "=" -f2) vdi_signal_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_signal_port" \ |/usr/bin/cut -d "=" -f2) vdi_spice_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_spice_port" \ |/usr/bin/cut -d "=" -f2) animation_folder=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "animation_folder" \ |/usr/bin/cut -d "=" -f2) process=/usr/bin/remote-viewer while true do if [ -z `/usr/bin/pidof feh` ] then /usr/bin/echo $animation_folder /usr/bin/feh -N -x -D1 $animation_folder &amp; else /usr/bin/echo fi /usr/bin/nc -i 1 $server_ip $vdi_signal_port |while read line do if /usr/bin/echo "$line" |/usr/bin/grep "RULE ADDED, CONNECT NOW!" then /usr/bin/killall feh pid_process=$($process "spice://$server_ip:$vdi_spice_port" \ "--spice-disable-audio" "--spice-disable-effects=animation" \ "--spice-preferred-compression=auto-glz" "-k" \ "--kiosk-quit=on-disconnect" | /bin/echo $!) /usr/bin/wait $pid_process /usr/bin/killall -u $USER exit else /usr/bin/echo $line &gt;&gt; /var/log/remote.log fi done done</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/client.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">server_ip=169.254.24.1 vdi_signal_port=5905 vdi_spice_port=5906 animation_folder=/usr/share/backgrounds/animation background_folder=/usr/share/backgrounds2/fedora-workstation</code> </pre><br></div></div><br>  Beschreibung der Variablen der Datei client.conf <br>  server_ip - Adresse KVM_Server <br>  vdi_signal_port - Port KVM_Server, auf dem xinetd "sitzt" <br>  vdi_spice_port - Netzwerkport KVM_Server, von dem die Verbindungsanforderung vom Remote-Viewer-Client zum Spice-Port des ausgew√§hlten VM umgeleitet wird (Details unten). <br>  animation_folder - Ordner, aus dem Bilder f√ºr Demonstrations-Bullshit-Animationen entnommen werden <br>  background_folder - Der Ordner, aus dem Bilder f√ºr Standby-Pr√§sentationen entnommen werden.  Mehr zur Animation im n√§chsten Teil des Artikels. <br><br>  Das Skript remote.sh √ºbernimmt die Einstellungen aus der Konfigurationsdatei /etc/client.conf und verwendet nc, um eine Verbindung zum Port "vdi_signal_port" des KVM-Servers herzustellen, und empf√§ngt einen Datenstrom vom Server, unter dem die Zeichenfolge "RULE ADDED, CONNECT NOW" erwartet wird.  Wenn die gew√ºnschte Leitung empfangen wird, startet der Remote-Viewer-Prozess im Kiosk-Modus und stellt eine Verbindung zum Server-Port "vdi_spice_port" her.  Die Ausf√ºhrung des Skripts wird bis zum Ende der Remote-Viewer-Ausf√ºhrung angehalten. <br><br>  Der Remote-Viewer, der aufgrund einer Umleitung auf der Serverseite eine Verbindung zum Port "vdi_spice_port" herstellt, gelangt zum Port "spice_console" der lo0-Schnittstelle, d. H.  an die Konsole der virtuellen Maschine und die Arbeit des Benutzers erfolgt direkt.  W√§hrend des Wartens auf die Verbindung wird dem Benutzer eine Bullshit-Animation in Form einer Diashow mit JPEG-Dateien angezeigt. Der Pfad zum Verzeichnis mit den Bildern wird durch den Wert der Variablen animation_folder aus der Konfigurationsdatei bestimmt. <br><br>  Wenn die Verbindung zum Port "spice_console" der virtuellen Maschine unterbrochen wird, was das Herunterfahren / Neustarten der virtuellen Maschine (dh das tats√§chliche Ende der Benutzersitzung) signalisiert, werden alle Prozesse, die im Auftrag des autorisierten Benutzers ausgef√ºhrt werden, beendet, was zum Neustart von lightdm f√ºhrt und zum Autorisierungsbildschirm zur√ºckkehrt . <br><br><h4>  Von der Seite des KVM-Servers </h4><br>  Auf dem Signalport der Netzwerkkarte wartet enp5s0 auf die xinetd-Verbindung.  Nach dem Herstellen einer Verbindung zum Signal-Port f√ºhrt xinetd das Skript vm_manager.sh aus, ohne Eingabeparameter zu √ºbergeben, und leitet das Ergebnis des Skripts an die NC-Sitzung der Client Station weiter. <br><br><div class="spoiler">  <b class="spoiler_title">/etc/xinetd.d/test-server</b> <div class="spoiler_text"><pre> <code class="bash hljs">service vdi_signal { port = 5905 socket_type = stream protocol = tcp <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> = no user = root server = /home/admin/scripts_vdi_new/vm_manager.sh }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_manager.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" export "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL export SRV_START_PORT_POOL=$SRV_START_PORT_POOL SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR date=$(/usr/bin/date) #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# /usr/bin/echo "# $date START EXECUTE VM_MANAGER.SH #" make_connect_to_vm() { #&lt;READING CLEAR.LIST AND CHECK PORT FOR NETWORK STATE&gt;# /usr/bin/echo "READING CLEAN.LIST AND CHECK PORT STATE" #&lt;CHECK FOR NO ONE PORT IN CLEAR.LIST&gt;# if [ -z `/usr/bin/cat $SRV_TMP_DIR/clear.list` ] then /usr/bin/echo "NO AVALIBLE PORTS IN CLEAN.LIST FOUND" /usr/bin/echo "Will try to make housekeeper, and create new vm" make_housekeeper else #&lt;MINIMUN ONE PORT IN CLEAR.LIST FOUND&gt;# /usr/bin/cat $SRV_TMP_DIR/clear.list |while read line do clear_vm_port=$(($line)) /bin/echo "FOUND PORT $clear_vm_port IN CLEAN.LIST. TRY NETSTAT" \ "CHECK FOR PORT=$clear_vm_port" #&lt;NETSTAT LISTEN CHECK FOR PORT FROM CLEAN.LIST&gt;# if /usr/bin/netstat -lnt |/usr/bin/grep ":$clear_vm_port" &gt; /dev/null then /bin/echo "$clear_vm_port IS LISTEN" #&lt;PORT IS LISTEN. CHECK FOR IS CONNECTED NOW&gt;# if /usr/bin/netstat -nt |/usr/bin/grep ":$clear_vm_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then #&lt;PORT LISTEN AND ALREADY CONNECTED! MOVE PORT FROM CLEAR.LIST # TO WASTE.LIST&gt;# /bin/echo "$clear_vm_port IS ALREADY CONNECTED, MOVE PORT TO WASTE.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list else #&lt;PORT LISTEN AND NO ONE CONNECT NOW. MOVE PORT FROM CLEAR.LIST TO # CONN_WAIT.LIST AND CREATE IPTABLES RULES&gt;## /usr/bin/echo "OK, $clear_vm_port IS NOT ALREADY CONNECTED" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/conn_wait.list $SRV_SCRIPTS_DIR/vm_connect.sh $clear_vm_port #&lt;TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW WM&gt;# /bin/echo "TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW VM" make_housekeeper /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH#" exit fi else #&lt;PORT IS NOT A LISTEN. MOVE PORT FROM CLEAR.LIST TO WASTE.LIST&gt;# /bin/echo " "$clear_vm_port" is NOT LISTEN. REMOVE PORT FROM CLEAR.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list make_housekeeper fi done fi } make_housekeeper() { /usr/bin/echo "=Execute housekeeper=" /usr/bin/cat $SRV_TMP_DIR/waste.list |while read line do /usr/bin/echo "$line" if /usr/bin/netstat -lnt |/usr/bin/grep ":$line" &gt; /dev/null then /bin/echo "port_alive, vm is running" if /usr/bin/netstat -nt |/usr/bin/grep ":$line" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "port_in_use can't delete vm!!!" else /bin/echo "port_not in use. Deleting vm" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh $line fi else /usr/bin/echo "posible vm is already off. Deleting vm" /usr/bin/echo "MOVE VM IN OFF STATE $line FROM WASTE.LIST TO" \ "RECYCLE.LIST AND DELETE VM" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh "$line" fi done create_clear_vm } create_clear_vm() { /usr/bin/echo "=Create new VM=" while [ $SRV_POOL_SIZE -gt 0 ] do new_vm_port=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) /usr/bin/echo "new_vm_port=$new_vm_port" if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/clear.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in clear.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/waste.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in waste.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/recycle.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN RECYCLE LIST" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/conn_wait.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN CONN_WAIT LIST" else /usr/bin/echo "PORT IN NOT DEFINED IN NO ONE LIST WILL CREATE" \ "VM ON PORT $new_vm_port" /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_create.sh $new_vm_port fi fi fi fi SRV_POOL_SIZE=$(($SRV_POOL_SIZE-1)) done /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH #" } make_connect_to_vm |/usr/bin/tee -a /var/log/vm_manager.log</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/vm_manager.conf</b> <div class="spoiler_text">  srv_scripts_dir = / home / admin / scripts_vdi_new <br>  srv_pool_size = 4 <br>  srv_start_port_pool = 5920 <br>  srv_tmp_dir = / tmp / vm_state <br>  base_host = win10_2 <br>  input_iface = enp5s0 <br>  vdi_spice_port = 5906 <br>  count_conn_tryes = 10 <br></div></div><br><br>  Beschreibung der Variablen der Konfigurationsdatei vm_manager.conf <br>  srv_scripts_dir - Skriptspeicherortordner vm_manager.sh, vm_connect.sh, vm_delete.sh, vm_create.sh, vm_clear.sh <br>  srv_pool_size - Vm Poolgr√∂√üe <br>  srv_start_port_pool - der anf√§ngliche Port, nach dem die Spice-Ports der Konsolen der virtuellen Maschine beginnen <br>  srv_tmp_dir - Ordner f√ºr tempor√§re Dateien <br>  base_host - base Vm (goldenes Bild), aus dem Vm-Klone in den Pool erstellt werden <br>  input_iface - Die Netzwerkschnittstelle des Servers mit Blick auf Client-Stationen <br>  vdi_spice_port - Der Netzwerkport des Servers, von dem die Verbindungsanforderung vom Remote-Viewer-Client zum Spice-Port der ausgew√§hlten VM umgeleitet wird <br>  count_conn_tryes - Wartezeit, nach der angenommen wird, dass die Verbindung zu Vm nicht hergestellt wurde (Einzelheiten siehe vm_connect.sh). <br><br>  Das Skript vm_manager.sh liest die Konfigurationsdatei aus der Datei vm_manager.conf und wertet den Status der virtuellen Maschinen im Pool anhand verschiedener Parameter aus, n√§mlich: Wie viele VMs werden bereitgestellt, ob freie saubere VMs vorhanden sind.  Dazu liest er die Datei clear.list, die die Anzahl der "spice_console" -Ports der "neu erstellten" (siehe VM-Erstellungszyklus unten) virtuellen Maschinen enth√§lt, und pr√ºft, ob eine Verbindung zu ihnen hergestellt wurde.  Wenn ein Port mit einer hergestellten Netzwerkverbindung erkannt wird (was auf keinen Fall der Fall sein sollte), wird eine Warnung angezeigt und der Port wird an verschwenderisch √ºbertragen. Wenn der erste Port aus der Datei clear.list gefunden wird, mit der derzeit keine Verbindung besteht, ruft vm_manager.sh das Skript vm_connect.sh auf und √ºbergibt es ihm als Parameter die Nummer dieses Ports. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_connect.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh date=$(/usr/bin/date) /usr/bin/echo "#" "$date" "START EXECUTE VM_CONNECT.SH#" #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# free_port="$1" input_iface=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "input_iface" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "input_iface=$input_iface" vdi_spice_port=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "vdi_spice_port" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "vdi_spice_port=$vdi_spice_port" count_conn_tryes=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "count_conn_tryes" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "count_conn_tryes=$count_conn_tryes" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# #&lt;CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# /usr/bin/echo "create rule for port" $free_port /usr/sbin/iptables -I INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -I OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/sbin/iptables -t nat -I PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/bin/echo "RULE ADDED, CONNECT NOW!" #&lt;/CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# #&lt;WAIT CONNECT ESTABLISHED AND ACTIVATE CONNECT TIMER&gt;# while [ $count_conn_tryes -gt 0 ] do if /usr/bin/netstat -nt |/usr/bin/grep ":$free_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "$free_port NOW in use!!!" /usr/bin/sleep 1s /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/waste.list return else /usr/bin/echo "$free_port NOT IN USE" /usr/bin/echo "RULE ADDED, CONNECT NOW!" /usr/bin/sleep 1s fi count_conn_tryes=$((count_conn_tryes-1)) done #&lt;/WAIT CONNECT ESTABLISED AND ACTIVATE CONNECT TIMER&gt;# #&lt;IF COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT \ # VM TO CLEAR.LIST&gt;# /usr/bin/echo "REVERT IPTABLES RULE AND REVERT VM TO CLEAN \ LIST $free_port" /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport $free_port \ -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/clear.list #&lt;/COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT VM \ #TO CLEAR.LIST&gt;# /usr/bin/echo "#" "$date" "END EXECUTE VM_CONNECT.SH#" # Attention! Must Be! sysctl net.ipv4.conf.all.route_localnet=1</span></span></code> </pre><br></div></div><br>  Das Skript vm_connect.sh f√ºhrt Firewall-Regeln ein, die eine Umleitung "vdi_spice_port" des Server-Ports der enp5s0-Schnittstelle zum "spice console port" der VM auf der lo0-Serverschnittstelle erstellen, die als Startparameter √ºbergeben wird.  Der Port wird an conn_wait.list √ºbertragen. Die VM wird als ausstehende Verbindung betrachtet.  Die Zeile RULE ADDED, CONNECT NOW wird an die Client Station-Sitzung am Signalport des Servers gesendet, was von dem darauf ausgef√ºhrten Skript remote.sh erwartet wird.  Ein Verbindungswartezyklus beginnt mit der Anzahl der Versuche, die durch den Wert der Variablen "count_conn_tryes" aus der Konfigurationsdatei bestimmt werden.  Jede Sekunde in der NC-Sitzung wird die Zeichenfolge "REGEL HINZUGEF√úGT, JETZT VERBINDEN" angegeben und die Verbindung zum Port "spice_console" √ºberpr√ºft. <br><br>  Wenn die Verbindung f√ºr die festgelegte Anzahl von Versuchen fehlgeschlagen ist, wird der spice_console-Port zur√ºck an clear.list √ºbertragen. Die Ausf√ºhrung von vm_connect.sh ist abgeschlossen. Die Ausf√ºhrung von vm_manager.sh wird fortgesetzt, wodurch der Reinigungszyklus gestartet wird. <br><br>  Wenn die Client Station eine Verbindung zum spice_console-Port auf der lo0-Schnittstelle herstellt, werden die Firewall-Regeln, die eine Umleitung zwischen dem Spice Server-Port und dem spice_console-Port erstellen, gel√∂scht, und die Verbindung wird durch einen Mechanismus zum Ermitteln des Status der Firewall weiter aufrechterhalten.  Im Falle einer getrennten Verbindung schl√§gt die erneute Verbindung zum spice_console-Port fehl.  Der spice_console-Port wird in die Datei "iar.list "√ºbertragen. Die VM wird als fehlerhaft betrachtet und kann ohne Reinigung nicht in den Pool sauberer virtueller Maschinen zur√ºckkehren.  Die Ausf√ºhrung von vm_connect.sh ist abgeschlossen, die Ausf√ºhrung von vm_manager.sh wird fortgesetzt, wodurch der Reinigungszyklus gestartet wird. <br><br>  Der Reinigungszyklus beginnt mit einem Blick auf die Datei wash.list, in die die spice_console-Nummern der Ports der virtuellen Maschine √ºbertragen werden, zu denen die Verbindung hergestellt wird.  Das Vorhandensein einer aktiven Verbindung wird an jedem spice_console-Port aus der Liste ermittelt.  Wenn keine Verbindung besteht, wird davon ausgegangen, dass die virtuelle Maschine nicht mehr verwendet wird und der Port an recycle.list √ºbertragen wird. Der Vorgang zum L√∂schen der virtuellen Maschine (siehe unten), zu der dieser Port geh√∂rt, wird gestartet.  Wenn am Port eine aktive Netzwerkverbindung erkannt wird, wird davon ausgegangen, dass die virtuelle Maschine verwendet wird, und es werden keine Ma√ünahmen ergriffen.  Wenn der Port nicht abgegriffen wird, wird davon ausgegangen, dass die VM ausgeschaltet ist und nicht mehr ben√∂tigt wird.  Der Port wird an recycle.list √ºbertragen und der Vorgang zum Entfernen der virtuellen Maschine wird gestartet.  Dazu wird das Skript vm_delete.sh aufgerufen, an das die Nummer "spice_console" als Parameter an den VM-Port √ºbertragen wird, der gel√∂scht werden muss. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_delete.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh #&lt;Set local VARIABLES&gt;# port_to_delete="$1" date=$(/usr/bin/date) #&lt;/Set local VARIABLES&gt;# /usr/bin/echo "# $date START EXECUTE VM_DELETE.SH#" /usr/bin/echo "TRY DELETE VM ON PORT: $vm_port" #&lt;VM NAME SETUP&gt;# vm_name_part1=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep 'base_host' \ |/usr/bin/cut -d'=' -f2) vm_name=$(/usr/bin/echo "$vm_name_part1""-""$port_to_delete") #&lt;/VM NAME SETUP&gt;# #&lt;SHUTDOWN AND DELETE VM&gt;# /usr/bin/virsh destroy $vm_name /usr/bin/virsh undefine $vm_name /usr/bin/rm -f /var/lib/libvirt/images_write/$vm_name.qcow2 /usr/bin/sed -i "/$port_to_delete/d" $SRV_TMP_DIR/recycle.list #&lt;/SHUTDOWN AND DELETE VM&gt;# /usr/bin/echo "VM ON PORT $vm_port HAS BEEN DELETE AND REMOVE" \ "FROM RECYCLE.LIST. EXIT FROM VM_DELETE.SH" /usr/bin/echo "# $date STOP EXECUTE VM_DELETE.SH#" exit</span></span></code> </pre><br></div></div><br>  Das Entfernen einer virtuellen Maschine ist eine ziemlich triviale Operation. Das Skript vm_delete.sh bestimmt den Namen der virtuellen Maschine, der der als Startparameter √ºbergebene Port geh√∂rt.  Die VM muss angehalten werden, die VM wird aus dem Hypervisor entfernt und die virtuelle Festplatte dieser VM wird gel√∂scht.  Der spice_console-Port wird aus der recycle.list entfernt.  Die Ausf√ºhrung von vm_delete.sh wird beendet, die Ausf√ºhrung von vm_manager.sh wird fortgesetzt <br><br>  Das Skript vm_manager.sh startet am Ende der Vorg√§nge zum Bereinigen nicht ben√∂tigter virtueller Maschinen aus der Liste WASTE.list den Zyklus zum Erstellen virtueller Maschinen im Pool. <br><br>  Der Prozess beginnt mit der Bestimmung der f√ºr das Hosting verf√ºgbaren spice_console-Ports.  Basierend auf dem Parameter der Konfigurationsdatei "srv_start_port_pool", der den Startport f√ºr den Pool "spice_console" virtueller Maschinen festlegt, und dem Parameter "srv_pool_size", der die Begrenzung der Anzahl virtueller Maschinen festlegt, werden alle m√∂glichen Portvarianten nacheinander aufgelistet.  F√ºr jeden bestimmten Port wird in clear.list, WASte.list, conn_wait.list, recycle.list gesucht.  Wenn in einer dieser Dateien ein Port gefunden wird, wird der Port als belegt betrachtet und √ºbersprungen.  Wenn der Port in den angegebenen Dateien nicht gefunden wird, wird er in die Datei recycle.list eingegeben und der Vorgang zum Erstellen einer neuen virtuellen Maschine beginnt.  Dazu wird das Skript vm_create.sh aufgerufen, an das die spice_console-Nummer des Ports √ºbergeben wird, f√ºr den Sie eine VM erstellen m√∂chten. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_create.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh /usr/bin/echo "#" "$date" "START RUNNING VM_CREATE.SH#" new_vm_port=$1 date=$(/usr/bin/date) a=0 /usr/bin/echo SRV_TMP_DIR=$SRV_TMP_DIR #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# base_host=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "base_host" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "base_host=$base_host" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# hdd_image_locate() { /bin/echo "Run STEP 1 - hdd_image_locate" hdd_base_image=$(/usr/bin/virsh dumpxml $base_host \ |/usr/bin/grep "source file" |/usr/bin/grep "qcow2" |/usr/bin/head -n 1 \ |/usr/bin/cut -d "'" -f2) if [ -z "$hdd_base_image" ] then /bin/echo "base hdd image not found!" else /usr/bin/echo "hdd_base_image found is a $hdd_base_image. Run next step 2" #&lt; CHECK FOR SNAPSHOT ON BASE HDD &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" | /usr/bin/grep -c "Snapshot"` ] then /usr/bin/echo "base image haven't snapshot, run NEXT STEP 3" else /usr/bin/echo "base hdd image have a snapshot, can't use this image" exit fi #&lt;/ CHECK FOR SNAPSHOT ON BASE HDD &gt;# #&lt; CHECK FOR HDD IMAGE IS LINK CLONE &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" |/usr/bin/grep -c "backing file" then /usr/bin/echo "base image is not a linked clone, NEXT STEP 4" /usr/bin/echo "Base image check complete!" else /usr/bin/echo "base hdd image is a linked clone, can't use this image" exit fi fi #&lt;/ CHECK FOR HDD IMAGE IS LINK CLONE &gt;# cloning } cloning() { # &lt;Step_1 turn the base VM off &gt;# /usr/bin/virsh shutdown $base_host &gt; /dev/null 2&gt;&amp;1 # &lt;/Step_1 turn the base VM off &gt;# #&lt;Create_vm_config&gt;# /usr/bin/echo "Free port for Spice VM is $new_vm_port" #&lt;Setup_name_for_new_VM&gt;# new_vm_name=$(/bin/echo $base_host"-"$new_vm_port) #&lt;/Setup_name_for_new_VM&gt;# #&lt;Make_base_config_as_clone_base_VM&gt;# /usr/bin/virsh dumpxml $base_host &gt; $SRV_TMP_DIR/$new_vm_name.xml #&lt;Make_base_config_as_clone_base_VM&gt;# ##&lt;Setup_New_VM_Name_in_config&gt;## /usr/bin/sed -i "s%&lt;name&gt;$base_host&lt;/name&gt;%&lt;name&gt;$new_vm_name&lt;/name&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Setup_New_VM_Name_in_config&gt;# #&lt;UUID Changing&gt;# old_uuid=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "&lt;uuid&gt;") /usr/bin/echo old UUID $old_uuid new_uuid_part1=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 1,2) new_uuid_part2=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 4,5) new_uuid=$(/bin/echo $new_uuid_part1"-"$new_vm_port"-"$new_uuid_part2) /usr/bin/echo $new_uuid /usr/bin/sed -i "s%$old_uuid%$new_uuid%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/UUID Changing&gt;# #&lt;Spice port replace&gt;# old_spice_port=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml \ |/usr/bin/grep "graphics type='spice' port=") /bin/echo old spice port $old_spice_port new_spice_port=$(/usr/bin/echo "&lt;graphics type='spice' port='$new_vm_port' autoport='no' listen='127.0.0.1'&gt;") /bin/echo $new_spice_port /usr/bin/sed -i "s%$old_spice_port%$new_spice_port%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Spice port replace&gt;# #&lt;MAC_ADDR_GENERATE&gt;# mac_new=$(/usr/bin/hexdump -n6 -e '/1 ":%02X"' /dev/random|/usr/bin/sed s/^://g) /usr/bin/echo New Mac is $mac_new #&lt;/MAC_ADDR_GENERATE&gt;# #&lt;GET OLD MAC AND REPLACE&gt;# mac_old=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "mac address=") /usr/bin/echo old mac is $mac_old /usr/bin/sed -i "s%$mac_old%$mac_new%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;GET OLD MAC AND REPLACE&gt;# #&lt;new_disk_create&gt;# /usr/bin/qemu-img create -f qcow2 -b $hdd_base_image /var/lib/libvirt/images_write/$new_vm_name.qcow2 #&lt;/new_disk_create&gt;# #&lt;attach_new_disk_in_confiig&gt;# /usr/bin/echo hdd base image is $hdd_base_image /usr/bin/sed -i "s%&lt;source file='$hdd_base_image'/&gt;%&lt;source file='/var/lib/libvirt/images_write/$new_vm_name.qcow2'/&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/attach_new_disk_in_confiig&gt;# starting_vm #&lt;/Create_vm config&gt;# } starting_vm() { /usr/bin/virsh define $SRV_TMP_DIR/$new_vm_name.xml /usr/bin/virsh start $new_vm_name while [ $a -ne 1 ] do if /usr/bin/virsh list --all |/usr/bin/grep "$new_vm_name" |/usr/bin/grep "running" &gt; /dev/null 2&gt;&amp;1 then a=1 /usr/bin/sed -i "/$new_vm_port/d" $SRV_TMP_DIR/recycle.list /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/clear.list /usr/bin/echo "#" "$date" "VM $new_vm_name IS STARTED #" else /usr/bin/echo "#VM $new_vm_name is not ready#" a=0 /usr/bin/sleep 2s fi done /usr/bin/echo "#$date EXIT FROM VM_CREATE.SH#" exit } hdd_image_locate</span></span></code> </pre><br></div></div><br>  Der Prozess zum Erstellen einer neuen virtuellen Maschine <br><br>  Das Skript vm_create.sh liest aus der Konfigurationsdatei den Wert der Variablen "base_host", die die virtuelle Beispielmaschine bestimmt, auf deren Grundlage der Klon erstellt wird.  Es entl√§dt die XML-Konfiguration der VM aus der Hypervisor-Datenbank, f√ºhrt eine Reihe von √úberpr√ºfungen des VM-Disk-Images durch und erstellt nach erfolgreichem Abschluss die XML-Konfigurationsdatei f√ºr die neue VM und das Disk-Image des verkn√ºpften Klons der neuen VM.  Danach wird die XML-Konfiguration der neuen VM in die Hypervisor-Datenbank geladen und die VM gestartet.  Der spice_console-Port wird von recycle.list nach clear.list √ºbertragen.  Die Ausf√ºhrung von vm_create.sh endet und die Ausf√ºhrung von vm_manager.sh endet. <br>  Wenn Sie das n√§chste Mal eine Verbindung herstellen, beginnt dies von vorne. <br><br>  F√ºr Notf√§lle enth√§lt das Kit ein Skript vm_clear.sh, das alle VMs aus dem Pool zwangsweise durchl√§uft und sie entfernt, indem die Werte der Listen auf Null gesetzt werden.  Wenn Sie es beim Laden aufrufen, k√∂nnen Sie VDI von Grund auf neu starten. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_clear.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #set VARIABLES# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL #Set VARIABLES# /usr/bin/echo "= Cleanup ALL VM=" /usr/bin/mkdir $SRV_TMP_DIR /usr/sbin/service iptables restart /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/clear.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/waste.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/recycle.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/conn_wait.list port_to_delete=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) while [ "$port_to_delete" -gt "$SRV_START_PORT_POOL" ] do $SRV_SCRIPTS_DIR/vm_delete.sh $port_to_delete port_to_delete=$(($port_to_delete-1)) done /usr/bin/echo "= EXIT FROM VM_CLEAR.SH="</span></span></code> </pre><br></div></div><br>  Damit m√∂chte ich den ersten Teil meiner Geschichte beenden.  Dies sollte f√ºr Systemadministratoren ausreichen, um underVDI im Gesch√§ftsleben zu testen.  Wenn die Community dieses Thema interessant findet, werde ich im zweiten Teil √ºber die Modifikation von livecd Fedora und deren Umwandlung in einen Kiosk sprechen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462203/">https://habr.com/ru/post/de462203/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462181/index.html">Regeln f√ºr eine effektive Kommunikation in Gruppenchats</a></li>
<li><a href="../de462185/index.html">Die Revolution ist vorbei. Gibt es eine Alternative zu einem Lithium-Ionen-Akku?</a></li>
<li><a href="../de462189/index.html">√Ñtzen von Daten mit Travajs</a></li>
<li><a href="../de462191/index.html">DataArt Museum: Eine Tour durch Norditalien</a></li>
<li><a href="../de462197/index.html">Tipps, wie Sie Ihren Geist befreien und Ihre Kreativit√§t steigern k√∂nnen</a></li>
<li><a href="../de462205/index.html">PHDays 9 gewinnen The Standoff: Die Chronik des True0xA3-Teams</a></li>
<li><a href="../de462209/index.html">Polycom VideoConference-L√∂sungen. Erinnerungen 6 Jahre sp√§ter ... Stufe 2. Teil 1. RMX1500</a></li>
<li><a href="../de462213/index.html">Lernen und arbeiten: die Erfahrung von Studenten an der Fakult√§t f√ºr Informationstechnologie und Programmierung</a></li>
<li><a href="../de462221/index.html">Wie entt√§uscht ich von Google Play bin</a></li>
<li><a href="../de462227/index.html">Moskau, 9. August - Backend Stories 4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>