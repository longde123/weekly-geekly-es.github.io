<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⁉️ 👨‍👩‍👧 💈 KVM (unter) VDI mit einmaligen virtuellen Maschinen unter Verwendung von Bash 🏇🏿 🚔 ✌🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Für wen ist dieser Artikel? 
 Dieser Artikel kann für Systemadministratoren von Interesse sein, die vor der Aufgabe standen, einen "einmaligen" Jobdie...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>KVM (unter) VDI mit einmaligen virtuellen Maschinen unter Verwendung von Bash</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462203/"><h4>  Für wen ist dieser Artikel? </h4><br>  Dieser Artikel kann für Systemadministratoren von Interesse sein, die vor der Aufgabe standen, einen "einmaligen" Jobdienst zu erstellen. <br><br><h4>  Prolog </h4><br>  Die IT-Support-Abteilung eines jungen, sich dynamisch entwickelnden Unternehmens mit einem kleinen regionalen Netzwerk wurde gebeten, "Self-Service-Stationen" für die Nutzung durch externe Kunden zu organisieren.  Die Stationsdaten sollten für die Registrierung auf den externen Portalen des Unternehmens, das Herunterladen von Daten von externen Geräten und die Arbeit mit Regierungsportalen verwendet werden. <br><br>  Ein wichtiger Aspekt war die Tatsache, dass der größte Teil der Software unter MS Windows „geschärft“ wird (z. B. „Deklaration“), und trotz der Hinwendung zu offenen Formaten bleibt MS Office der dominierende Standard beim Austausch elektronischer Dokumente.  Daher konnten wir MS Windows bei der Lösung dieses Problems nicht ablehnen. <br><a name="habracut"></a><br>  Das Hauptproblem war die Möglichkeit, verschiedene Daten aus Benutzersitzungen zu sammeln, die zu deren Weitergabe an Dritte führen könnten.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diese Situation hat den MFC bereits im Stich gelassen</a> .  Im Gegensatz zur quasi-gerichtlichen (staatlich autonomen Institution) MFC werden nichtstaatliche Organisationen für solche Mängel viel stärker bestraft.  Das nächste kritische Problem war die Anforderung, mit externen Speichermedien zu arbeiten, auf denen auf jeden Fall eine Menge bösartiger Malware vorhanden sein wird.  Die Wahrscheinlichkeit des Eintritts von Malware aus dem Internet wurde aufgrund der Einschränkung des Zugangs zum Internet über eine weiße Adressliste als weniger wahrscheinlich angesehen. Mitarbeiter anderer Abteilungen erarbeiteten gemeinsam die Anforderungen, stellten ihre Anforderungen und Wünsche. Die endgültigen Anforderungen sahen wie folgt aus: <br><br>  <b>IS-Anforderungen</b> <br><br><ul><li>  Nach der Verwendung sollten alle Benutzerdaten (einschließlich temporärer Dateien und Registrierungsschlüssel) gelöscht werden. </li><li>  Alle vom Benutzer gestarteten Prozesse sollten am Ende der Arbeit abgeschlossen sein. </li><li>  Internetzugang über eine weiße Liste von Adressen. </li><li>  Einschränkungen bei der Ausführung von Code von Drittanbietern. </li><li>  Wenn die Sitzung länger als 5 Minuten inaktiv ist, sollte die Sitzung automatisch beendet werden und die Station sollte eine Bereinigung durchführen. </li></ul><br>  <b>Kundenanforderungen</b> <br><br><ul><li>  Die Anzahl der Client-Stationen pro Zweigstelle beträgt nicht mehr als 4. </li><li>  Die minimale Wartezeit für die Systembereitschaft, von dem Moment an, als ich mich auf einen Stuhl setzte, bis zum Beginn der Arbeit mit Client-Software. </li><li>  Die Möglichkeit, Peripheriegeräte (Scanner, Flash-Laufwerke) direkt vom Installationsort der "Self-Service-Station" aus anzuschließen. </li><li>  Kundenwünsche </li><li>  Vorführung von Werbematerialien (Bilder) zum Zeitpunkt der Schließung des Komplexes. </li></ul><cut></cut><br><h4>  Mehl der Kreativität </h4><br>  Nachdem wir genug mit Windows LiveCD gespielt hatten, kamen wir einstimmig zu dem Schluss, dass die resultierende Lösung nicht mindestens 3 kritische Punkte erfüllt.  Sie sind entweder für eine lange Zeit geladen oder nicht ganz lebendig, oder ihre Anpassung war mit wilden Schmerzen verbunden.  Vielleicht haben wir schlecht gesucht, und Sie können eine Reihe von Tools empfehlen, ich werde Ihnen dankbar sein. <br><br>  Weiter haben wir uns mit VDI befasst, aber für diese Aufgabe sind die meisten Lösungen entweder zu teuer oder erfordern besondere Aufmerksamkeit.  Und ich wollte ein einfaches Tool mit einem Minimum an Magie, dessen Probleme größtenteils durch einen einfachen Neustart / Neustart des Dienstes gelöst werden konnten.  Glücklicherweise hatten wir Serverausrüstung, Low-End-Klasse in den Filialen, aus dem stillgelegten Dienst, die wir für die technologische Basis verwenden konnten. <br><br>  Was ist das Ergebnis?  Aber ich kann Ihnen nicht sagen, was am Ende passiert ist, weil die NDA, aber während der Suche haben wir ein interessantes Schema entwickelt, das sich in Labortests gut gezeigt hat, obwohl es nicht in Serie ging. <br><br>  Einige Haftungsausschlüsse: Der Autor behauptet nicht, dass die vorgeschlagene Lösung alle Aufgaben vollständig löst und dies freiwillig und mit dem Lied tut.  Der Autor stimmt im Voraus der Aussage zu, dass Sein Englishe sprache zehr schlecht ist.  Da sich die Lösung nicht mehr entwickelt, können Sie nicht mit einer Fehlerbehebung oder einer Änderung der Funktionalität rechnen. Alles liegt in Ihren Händen.  Der Autor geht davon aus, dass Sie mit KVM zumindest ein wenig vertraut sind und einen Übersichtsartikel zum Spice-Protokoll gelesen haben und ein wenig mit Centos oder einer anderen GNU Linux-Distribution gearbeitet haben. <br><br>  In diesem Artikel möchte ich das Rückgrat der resultierenden Lösung analysieren, nämlich die Interaktion von Client und Server und die Essenz der Prozesse im Lebenszyklus virtueller Maschinen im Rahmen der betreffenden Lösung.  Wenn der Artikel für die Öffentlichkeit interessant sein soll, beschreibe ich die Details der Implementierung von Live-Images zum Erstellen von Thin Clients auf Fedora-Basis und erläutere die Details der Optimierung virtueller Maschinen und KVM-Server zur Optimierung von Leistung und Sicherheit. <br><br>  Wenn Sie farbiges Papier nehmen, <br>  Farben, Pinsel und Kleber, <br>  Und ein bisschen mehr Geschicklichkeit ... <br>  Sie können hundert Rubel machen! <br><br><h4>  Schema und Beschreibung des Prüfstands </h4><br><img src="https://habrastorage.org/webt/pu/tu/rk/puturkaiwqcpbp4wld7ezdk_lcw.png"><br><br>  Alle Geräte befinden sich im Filialnetz, nur der Internetkanal geht aus.  In der Vergangenheit gab es bereits einen Proxyserver, es ist nichts Außergewöhnliches.  Daraufhin wird unter anderem der Datenverkehr von virtuellen Maschinen gefiltert (Abk. VM später im Text).  Nichts hindert Sie daran, diesen Dienst auf dem KVM-Server zu platzieren. Sie müssen lediglich beobachten, wie sich die Last auf dem Festplattensubsystem ändert. <br><br>  Client Station - in der Tat "Self-Service-Stationen", "Front-End" unseres Service.  Sind Nettops von Lenovo IdeaCentre.  Wofür ist dieses Gerät gut?  Ja, fast alle, besonders zufrieden mit der großen Anzahl von USB-Anschlüssen und Kartenlesern auf der Vorderseite.  In unserem Schema wird eine SD-Karte mit Hardware-Schreibschutz in den Kartenleser eingelegt, auf der das modifizierte Livebild von Fedora 28 aufgezeichnet wird. Natürlich sind ein Monitor, eine Tastatur und eine Maus mit dem Nettop verbunden. <br><br>  Schalter - ein unauffälliger Hardware-Schalter der zweiten Ebene, der sich im Serverraum befindet und mit Lichtern blinkt.  Es ist mit keinem Netzwerk außer dem Netzwerk der „Selbstbedienungsstationen“ verbunden. <br><br>  KVM_Server ist der Kern der Schaltung. In den Bench-Tests des Core 2 Quad Q9650 mit 8 GB RAM wurden sicher 3 virtuelle Windows 10-Maschinen auf sich gezogen.  Festplattensubsystem - adaptec 3405 2 Laufwerke Raid 1 + SSD.  In Feldversuchen des Xeon 1220 zog die ernstere LSI 9260 + SSD leicht 5-6 VMs.  Wir würden den Server vom pensionierten Dienst bekommen, es würde nicht viel Kapitalkosten geben.  Auf diesen Servern wird das KVM-Virtualisierungssystem mit dem Pool der virtuellen Maschine pool_Vm bereitgestellt. <br><br>  VM ist eine virtuelle Maschine, das Backend unseres Service.  Es ist die Arbeit des Benutzers. <br><br>  Enp5s0 ist eine Netzwerkschnittstelle, die auf das Netzwerk von "Self-Service-Stationen", dhcpd, ntpd, httpd live darauf und xinetd auf den "Signal" -Port blickt. <br><br>  Lo0 ist die Loopback-Pseudo-Schnittstelle.  Standard. <br><br>  Spice_console - Eine sehr interessante Sache ist die Tatsache, dass im Gegensatz zum klassischen RDP beim Einschalten des KVM + Spice-Protokollpakets eine zusätzliche Entität angezeigt wird - der Konsolenport der virtuellen Maschine.  Wenn wir eine Verbindung zu diesem TCP-Port herstellen, erhalten wir die VM-Konsole, ohne dass eine Verbindung zu VM über die Netzwerkschnittstelle hergestellt werden muss.  Alle Interaktionen mit Vm zur Signalübertragung übernimmt der Server.  Das nächstgelegene Analogon in der Funktion ist IPKVM.  Das heißt,  Das Image des VM-Monitors wird an diesen Port übertragen, Daten zur Mausbewegung werden an diesen Port übertragen, und (was am wichtigsten ist) die Interaktion über das Spice-Protokoll ermöglicht es Ihnen, USB-Geräte nahtlos an die virtuelle Maschine umzuleiten, als ob dieses Gerät mit der VM selbst verbunden wäre.  Getestet für Flash-Laufwerke, Scanner, Webcams. <br><br>  Vnet0-, virbr0- und virtuelle Netzwerkkarten Vm bilden ein Netzwerk virtueller Maschinen. <br><br><h4>  Wie funktioniert es? </h4><br>  Von der Client Station <br><br>  Die Client-Station startet im grafischen Modus vom modifizierten Live-Image von Fedora 28 und empfängt die IP-Adresse per DHCP aus dem Netzwerkadressraum 169.254.24.0/24.  Während des Startvorgangs werden Firewall-Regeln erstellt, die Verbindungen zu den Server-Ports "Signal" und "Spice" ermöglichen.  Nach Abschluss des Downloads wartet die Station auf die Autorisierung des Client-Benutzers.  Nach der Benutzerautorisierung wird der Desktop-Manager "openbox" gestartet und das Autostart-Autostart-Skript im Namen des autorisierten Benutzers ausgeführt.  Das Autorun-Skript führt unter anderem das Skript remote.sh aus. <br><br><div class="spoiler">  <b class="spoiler_title">$ HOME / .config / openbox / scripts / remote.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh server_ip=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "server_ip" \ |/usr/bin/cut -d "=" -f2) vdi_signal_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_signal_port" \ |/usr/bin/cut -d "=" -f2) vdi_spice_port=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "vdi_spice_port" \ |/usr/bin/cut -d "=" -f2) animation_folder=$(/usr/bin/cat /etc/client.conf |/usr/bin/grep "animation_folder" \ |/usr/bin/cut -d "=" -f2) process=/usr/bin/remote-viewer while true do if [ -z `/usr/bin/pidof feh` ] then /usr/bin/echo $animation_folder /usr/bin/feh -N -x -D1 $animation_folder &amp; else /usr/bin/echo fi /usr/bin/nc -i 1 $server_ip $vdi_signal_port |while read line do if /usr/bin/echo "$line" |/usr/bin/grep "RULE ADDED, CONNECT NOW!" then /usr/bin/killall feh pid_process=$($process "spice://$server_ip:$vdi_spice_port" \ "--spice-disable-audio" "--spice-disable-effects=animation" \ "--spice-preferred-compression=auto-glz" "-k" \ "--kiosk-quit=on-disconnect" | /bin/echo $!) /usr/bin/wait $pid_process /usr/bin/killall -u $USER exit else /usr/bin/echo $line &gt;&gt; /var/log/remote.log fi done done</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/client.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">server_ip=169.254.24.1 vdi_signal_port=5905 vdi_spice_port=5906 animation_folder=/usr/share/backgrounds/animation background_folder=/usr/share/backgrounds2/fedora-workstation</code> </pre><br></div></div><br>  Beschreibung der Variablen der Datei client.conf <br>  server_ip - Adresse KVM_Server <br>  vdi_signal_port - Port KVM_Server, auf dem xinetd "sitzt" <br>  vdi_spice_port - Netzwerkport KVM_Server, von dem die Verbindungsanforderung vom Remote-Viewer-Client zum Spice-Port des ausgewählten VM umgeleitet wird (Details unten). <br>  animation_folder - Ordner, aus dem Bilder für Demonstrations-Bullshit-Animationen entnommen werden <br>  background_folder - Der Ordner, aus dem Bilder für Standby-Präsentationen entnommen werden.  Mehr zur Animation im nächsten Teil des Artikels. <br><br>  Das Skript remote.sh übernimmt die Einstellungen aus der Konfigurationsdatei /etc/client.conf und verwendet nc, um eine Verbindung zum Port "vdi_signal_port" des KVM-Servers herzustellen, und empfängt einen Datenstrom vom Server, unter dem die Zeichenfolge "RULE ADDED, CONNECT NOW" erwartet wird.  Wenn die gewünschte Leitung empfangen wird, startet der Remote-Viewer-Prozess im Kiosk-Modus und stellt eine Verbindung zum Server-Port "vdi_spice_port" her.  Die Ausführung des Skripts wird bis zum Ende der Remote-Viewer-Ausführung angehalten. <br><br>  Der Remote-Viewer, der aufgrund einer Umleitung auf der Serverseite eine Verbindung zum Port "vdi_spice_port" herstellt, gelangt zum Port "spice_console" der lo0-Schnittstelle, d. H.  an die Konsole der virtuellen Maschine und die Arbeit des Benutzers erfolgt direkt.  Während des Wartens auf die Verbindung wird dem Benutzer eine Bullshit-Animation in Form einer Diashow mit JPEG-Dateien angezeigt. Der Pfad zum Verzeichnis mit den Bildern wird durch den Wert der Variablen animation_folder aus der Konfigurationsdatei bestimmt. <br><br>  Wenn die Verbindung zum Port "spice_console" der virtuellen Maschine unterbrochen wird, was das Herunterfahren / Neustarten der virtuellen Maschine (dh das tatsächliche Ende der Benutzersitzung) signalisiert, werden alle Prozesse, die im Auftrag des autorisierten Benutzers ausgeführt werden, beendet, was zum Neustart von lightdm führt und zum Autorisierungsbildschirm zurückkehrt . <br><br><h4>  Von der Seite des KVM-Servers </h4><br>  Auf dem Signalport der Netzwerkkarte wartet enp5s0 auf die xinetd-Verbindung.  Nach dem Herstellen einer Verbindung zum Signal-Port führt xinetd das Skript vm_manager.sh aus, ohne Eingabeparameter zu übergeben, und leitet das Ergebnis des Skripts an die NC-Sitzung der Client Station weiter. <br><br><div class="spoiler">  <b class="spoiler_title">/etc/xinetd.d/test-server</b> <div class="spoiler_text"><pre> <code class="bash hljs">service vdi_signal { port = 5905 socket_type = stream protocol = tcp <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span> = no user = root server = /home/admin/scripts_vdi_new/vm_manager.sh }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_manager.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" export "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL export SRV_START_PORT_POOL=$SRV_START_PORT_POOL SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR date=$(/usr/bin/date) #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# /usr/bin/echo "# $date START EXECUTE VM_MANAGER.SH #" make_connect_to_vm() { #&lt;READING CLEAR.LIST AND CHECK PORT FOR NETWORK STATE&gt;# /usr/bin/echo "READING CLEAN.LIST AND CHECK PORT STATE" #&lt;CHECK FOR NO ONE PORT IN CLEAR.LIST&gt;# if [ -z `/usr/bin/cat $SRV_TMP_DIR/clear.list` ] then /usr/bin/echo "NO AVALIBLE PORTS IN CLEAN.LIST FOUND" /usr/bin/echo "Will try to make housekeeper, and create new vm" make_housekeeper else #&lt;MINIMUN ONE PORT IN CLEAR.LIST FOUND&gt;# /usr/bin/cat $SRV_TMP_DIR/clear.list |while read line do clear_vm_port=$(($line)) /bin/echo "FOUND PORT $clear_vm_port IN CLEAN.LIST. TRY NETSTAT" \ "CHECK FOR PORT=$clear_vm_port" #&lt;NETSTAT LISTEN CHECK FOR PORT FROM CLEAN.LIST&gt;# if /usr/bin/netstat -lnt |/usr/bin/grep ":$clear_vm_port" &gt; /dev/null then /bin/echo "$clear_vm_port IS LISTEN" #&lt;PORT IS LISTEN. CHECK FOR IS CONNECTED NOW&gt;# if /usr/bin/netstat -nt |/usr/bin/grep ":$clear_vm_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then #&lt;PORT LISTEN AND ALREADY CONNECTED! MOVE PORT FROM CLEAR.LIST # TO WASTE.LIST&gt;# /bin/echo "$clear_vm_port IS ALREADY CONNECTED, MOVE PORT TO WASTE.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list else #&lt;PORT LISTEN AND NO ONE CONNECT NOW. MOVE PORT FROM CLEAR.LIST TO # CONN_WAIT.LIST AND CREATE IPTABLES RULES&gt;## /usr/bin/echo "OK, $clear_vm_port IS NOT ALREADY CONNECTED" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/conn_wait.list $SRV_SCRIPTS_DIR/vm_connect.sh $clear_vm_port #&lt;TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW WM&gt;# /bin/echo "TRY TO CLEAN VM IN WASTE.LIST AND CREATE NEW VM" make_housekeeper /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH#" exit fi else #&lt;PORT IS NOT A LISTEN. MOVE PORT FROM CLEAR.LIST TO WASTE.LIST&gt;# /bin/echo " "$clear_vm_port" is NOT LISTEN. REMOVE PORT FROM CLEAR.LIST" /usr/bin/sed -i "/$clear_vm_port/d" $SRV_TMP_DIR/clear.list /usr/bin/echo $clear_vm_port &gt;&gt; $SRV_TMP_DIR/waste.list make_housekeeper fi done fi } make_housekeeper() { /usr/bin/echo "=Execute housekeeper=" /usr/bin/cat $SRV_TMP_DIR/waste.list |while read line do /usr/bin/echo "$line" if /usr/bin/netstat -lnt |/usr/bin/grep ":$line" &gt; /dev/null then /bin/echo "port_alive, vm is running" if /usr/bin/netstat -nt |/usr/bin/grep ":$line" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "port_in_use can't delete vm!!!" else /bin/echo "port_not in use. Deleting vm" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh $line fi else /usr/bin/echo "posible vm is already off. Deleting vm" /usr/bin/echo "MOVE VM IN OFF STATE $line FROM WASTE.LIST TO" \ "RECYCLE.LIST AND DELETE VM" /usr/bin/sed -i "/$line/d" $SRV_TMP_DIR/waste.list /usr/bin/echo $line &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_delete.sh "$line" fi done create_clear_vm } create_clear_vm() { /usr/bin/echo "=Create new VM=" while [ $SRV_POOL_SIZE -gt 0 ] do new_vm_port=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) /usr/bin/echo "new_vm_port=$new_vm_port" if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/clear.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in clear.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/waste.list &gt; /dev/null then /usr/bin/echo "$new_vm_port port is already defined in waste.list" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/recycle.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN RECYCLE LIST" else if /usr/bin/grep "$new_vm_port" $SRV_TMP_DIR/conn_wait.list &gt; /dev/null then /usr/bin/echo "$new_vm_port PORT IS ALREADY DEFINED IN CONN_WAIT LIST" else /usr/bin/echo "PORT IN NOT DEFINED IN NO ONE LIST WILL CREATE" \ "VM ON PORT $new_vm_port" /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/recycle.list $SRV_SCRIPTS_DIR/vm_create.sh $new_vm_port fi fi fi fi SRV_POOL_SIZE=$(($SRV_POOL_SIZE-1)) done /usr/bin/echo "# $date STOP EXECUTE VM_MANAGER.SH #" } make_connect_to_vm |/usr/bin/tee -a /var/log/vm_manager.log</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/vm_manager.conf</b> <div class="spoiler_text">  srv_scripts_dir = / home / admin / scripts_vdi_new <br>  srv_pool_size = 4 <br>  srv_start_port_pool = 5920 <br>  srv_tmp_dir = / tmp / vm_state <br>  base_host = win10_2 <br>  input_iface = enp5s0 <br>  vdi_spice_port = 5906 <br>  count_conn_tryes = 10 <br></div></div><br><br>  Beschreibung der Variablen der Konfigurationsdatei vm_manager.conf <br>  srv_scripts_dir - Skriptspeicherortordner vm_manager.sh, vm_connect.sh, vm_delete.sh, vm_create.sh, vm_clear.sh <br>  srv_pool_size - Vm Poolgröße <br>  srv_start_port_pool - der anfängliche Port, nach dem die Spice-Ports der Konsolen der virtuellen Maschine beginnen <br>  srv_tmp_dir - Ordner für temporäre Dateien <br>  base_host - base Vm (goldenes Bild), aus dem Vm-Klone in den Pool erstellt werden <br>  input_iface - Die Netzwerkschnittstelle des Servers mit Blick auf Client-Stationen <br>  vdi_spice_port - Der Netzwerkport des Servers, von dem die Verbindungsanforderung vom Remote-Viewer-Client zum Spice-Port der ausgewählten VM umgeleitet wird <br>  count_conn_tryes - Wartezeit, nach der angenommen wird, dass die Verbindung zu Vm nicht hergestellt wurde (Einzelheiten siehe vm_connect.sh). <br><br>  Das Skript vm_manager.sh liest die Konfigurationsdatei aus der Datei vm_manager.conf und wertet den Status der virtuellen Maschinen im Pool anhand verschiedener Parameter aus, nämlich: Wie viele VMs werden bereitgestellt, ob freie saubere VMs vorhanden sind.  Dazu liest er die Datei clear.list, die die Anzahl der "spice_console" -Ports der "neu erstellten" (siehe VM-Erstellungszyklus unten) virtuellen Maschinen enthält, und prüft, ob eine Verbindung zu ihnen hergestellt wurde.  Wenn ein Port mit einer hergestellten Netzwerkverbindung erkannt wird (was auf keinen Fall der Fall sein sollte), wird eine Warnung angezeigt und der Port wird an verschwenderisch übertragen. Wenn der erste Port aus der Datei clear.list gefunden wird, mit der derzeit keine Verbindung besteht, ruft vm_manager.sh das Skript vm_connect.sh auf und übergibt es ihm als Parameter die Nummer dieses Ports. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_connect.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh date=$(/usr/bin/date) /usr/bin/echo "#" "$date" "START EXECUTE VM_CONNECT.SH#" #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# free_port="$1" input_iface=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "input_iface" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "input_iface=$input_iface" vdi_spice_port=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "vdi_spice_port" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "vdi_spice_port=$vdi_spice_port" count_conn_tryes=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "count_conn_tryes" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "count_conn_tryes=$count_conn_tryes" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# #&lt;CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# /usr/bin/echo "create rule for port" $free_port /usr/sbin/iptables -I INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -I OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/sbin/iptables -t nat -I PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/bin/echo "RULE ADDED, CONNECT NOW!" #&lt;/CREATE IPTABLES RULES AND SEND SIGNAL TO CONNECT&gt;# #&lt;WAIT CONNECT ESTABLISHED AND ACTIVATE CONNECT TIMER&gt;# while [ $count_conn_tryes -gt 0 ] do if /usr/bin/netstat -nt |/usr/bin/grep ":$free_port" \ |/usr/bin/grep "ESTABLISHED" &gt; /dev/null then /bin/echo "$free_port NOW in use!!!" /usr/bin/sleep 1s /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport \ $free_port -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/waste.list return else /usr/bin/echo "$free_port NOT IN USE" /usr/bin/echo "RULE ADDED, CONNECT NOW!" /usr/bin/sleep 1s fi count_conn_tryes=$((count_conn_tryes-1)) done #&lt;/WAIT CONNECT ESTABLISED AND ACTIVATE CONNECT TIMER&gt;# #&lt;IF COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT \ # VM TO CLEAR.LIST&gt;# /usr/bin/echo "REVERT IPTABLES RULE AND REVERT VM TO CLEAN \ LIST $free_port" /usr/sbin/iptables -t nat -D PREROUTING -p tcp -i $input_iface --dport \ $vdi_spice_port -j DNAT --to-destination 127.0.0.1:$free_port /usr/sbin/iptables -D INPUT -i $input_iface -p tcp -m tcp --dport $free_port \ -j ACCEPT /usr/sbin/iptables -D OUTPUT -o $input_iface -p tcp -m tcp --sport \ $free_port -j ACCEPT /usr/bin/sed -i "/$free_port/d" $SRV_TMP_DIR/conn_wait.list /usr/bin/echo $free_port &gt;&gt; $SRV_TMP_DIR/clear.list #&lt;/COUNT HAS EXPIRED. REMOVE IPTABLES RULE AND REVERT VM \ #TO CLEAR.LIST&gt;# /usr/bin/echo "#" "$date" "END EXECUTE VM_CONNECT.SH#" # Attention! Must Be! sysctl net.ipv4.conf.all.route_localnet=1</span></span></code> </pre><br></div></div><br>  Das Skript vm_connect.sh führt Firewall-Regeln ein, die eine Umleitung "vdi_spice_port" des Server-Ports der enp5s0-Schnittstelle zum "spice console port" der VM auf der lo0-Serverschnittstelle erstellen, die als Startparameter übergeben wird.  Der Port wird an conn_wait.list übertragen. Die VM wird als ausstehende Verbindung betrachtet.  Die Zeile RULE ADDED, CONNECT NOW wird an die Client Station-Sitzung am Signalport des Servers gesendet, was von dem darauf ausgeführten Skript remote.sh erwartet wird.  Ein Verbindungswartezyklus beginnt mit der Anzahl der Versuche, die durch den Wert der Variablen "count_conn_tryes" aus der Konfigurationsdatei bestimmt werden.  Jede Sekunde in der NC-Sitzung wird die Zeichenfolge "REGEL HINZUGEFÜGT, JETZT VERBINDEN" angegeben und die Verbindung zum Port "spice_console" überprüft. <br><br>  Wenn die Verbindung für die festgelegte Anzahl von Versuchen fehlgeschlagen ist, wird der spice_console-Port zurück an clear.list übertragen. Die Ausführung von vm_connect.sh ist abgeschlossen. Die Ausführung von vm_manager.sh wird fortgesetzt, wodurch der Reinigungszyklus gestartet wird. <br><br>  Wenn die Client Station eine Verbindung zum spice_console-Port auf der lo0-Schnittstelle herstellt, werden die Firewall-Regeln, die eine Umleitung zwischen dem Spice Server-Port und dem spice_console-Port erstellen, gelöscht, und die Verbindung wird durch einen Mechanismus zum Ermitteln des Status der Firewall weiter aufrechterhalten.  Im Falle einer getrennten Verbindung schlägt die erneute Verbindung zum spice_console-Port fehl.  Der spice_console-Port wird in die Datei "iar.list "übertragen. Die VM wird als fehlerhaft betrachtet und kann ohne Reinigung nicht in den Pool sauberer virtueller Maschinen zurückkehren.  Die Ausführung von vm_connect.sh ist abgeschlossen, die Ausführung von vm_manager.sh wird fortgesetzt, wodurch der Reinigungszyklus gestartet wird. <br><br>  Der Reinigungszyklus beginnt mit einem Blick auf die Datei wash.list, in die die spice_console-Nummern der Ports der virtuellen Maschine übertragen werden, zu denen die Verbindung hergestellt wird.  Das Vorhandensein einer aktiven Verbindung wird an jedem spice_console-Port aus der Liste ermittelt.  Wenn keine Verbindung besteht, wird davon ausgegangen, dass die virtuelle Maschine nicht mehr verwendet wird und der Port an recycle.list übertragen wird. Der Vorgang zum Löschen der virtuellen Maschine (siehe unten), zu der dieser Port gehört, wird gestartet.  Wenn am Port eine aktive Netzwerkverbindung erkannt wird, wird davon ausgegangen, dass die virtuelle Maschine verwendet wird, und es werden keine Maßnahmen ergriffen.  Wenn der Port nicht abgegriffen wird, wird davon ausgegangen, dass die VM ausgeschaltet ist und nicht mehr benötigt wird.  Der Port wird an recycle.list übertragen und der Vorgang zum Entfernen der virtuellen Maschine wird gestartet.  Dazu wird das Skript vm_delete.sh aufgerufen, an das die Nummer "spice_console" als Parameter an den VM-Port übertragen wird, der gelöscht werden muss. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_delete.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh #&lt;Set local VARIABLES&gt;# port_to_delete="$1" date=$(/usr/bin/date) #&lt;/Set local VARIABLES&gt;# /usr/bin/echo "# $date START EXECUTE VM_DELETE.SH#" /usr/bin/echo "TRY DELETE VM ON PORT: $vm_port" #&lt;VM NAME SETUP&gt;# vm_name_part1=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep 'base_host' \ |/usr/bin/cut -d'=' -f2) vm_name=$(/usr/bin/echo "$vm_name_part1""-""$port_to_delete") #&lt;/VM NAME SETUP&gt;# #&lt;SHUTDOWN AND DELETE VM&gt;# /usr/bin/virsh destroy $vm_name /usr/bin/virsh undefine $vm_name /usr/bin/rm -f /var/lib/libvirt/images_write/$vm_name.qcow2 /usr/bin/sed -i "/$port_to_delete/d" $SRV_TMP_DIR/recycle.list #&lt;/SHUTDOWN AND DELETE VM&gt;# /usr/bin/echo "VM ON PORT $vm_port HAS BEEN DELETE AND REMOVE" \ "FROM RECYCLE.LIST. EXIT FROM VM_DELETE.SH" /usr/bin/echo "# $date STOP EXECUTE VM_DELETE.SH#" exit</span></span></code> </pre><br></div></div><br>  Das Entfernen einer virtuellen Maschine ist eine ziemlich triviale Operation. Das Skript vm_delete.sh bestimmt den Namen der virtuellen Maschine, der der als Startparameter übergebene Port gehört.  Die VM muss angehalten werden, die VM wird aus dem Hypervisor entfernt und die virtuelle Festplatte dieser VM wird gelöscht.  Der spice_console-Port wird aus der recycle.list entfernt.  Die Ausführung von vm_delete.sh wird beendet, die Ausführung von vm_manager.sh wird fortgesetzt <br><br>  Das Skript vm_manager.sh startet am Ende der Vorgänge zum Bereinigen nicht benötigter virtueller Maschinen aus der Liste WASTE.list den Zyklus zum Erstellen virtueller Maschinen im Pool. <br><br>  Der Prozess beginnt mit der Bestimmung der für das Hosting verfügbaren spice_console-Ports.  Basierend auf dem Parameter der Konfigurationsdatei "srv_start_port_pool", der den Startport für den Pool "spice_console" virtueller Maschinen festlegt, und dem Parameter "srv_pool_size", der die Begrenzung der Anzahl virtueller Maschinen festlegt, werden alle möglichen Portvarianten nacheinander aufgelistet.  Für jeden bestimmten Port wird in clear.list, WASte.list, conn_wait.list, recycle.list gesucht.  Wenn in einer dieser Dateien ein Port gefunden wird, wird der Port als belegt betrachtet und übersprungen.  Wenn der Port in den angegebenen Dateien nicht gefunden wird, wird er in die Datei recycle.list eingegeben und der Vorgang zum Erstellen einer neuen virtuellen Maschine beginnt.  Dazu wird das Skript vm_create.sh aufgerufen, an das die spice_console-Nummer des Ports übergeben wird, für den Sie eine VM erstellen möchten. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_create.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh /usr/bin/echo "#" "$date" "START RUNNING VM_CREATE.SH#" new_vm_port=$1 date=$(/usr/bin/date) a=0 /usr/bin/echo SRV_TMP_DIR=$SRV_TMP_DIR #&lt;SET LOCAL VARIABLES FOR SCRIPT&gt;# base_host=$(/usr/bin/cat /etc/vm_manager.conf |/usr/bin/grep "base_host" \ |/usr/bin/cut -d "=" -f2) /usr/bin/echo "base_host=$base_host" #&lt;/SET LOCAL VARIABLES FOR SCRIPT&gt;# hdd_image_locate() { /bin/echo "Run STEP 1 - hdd_image_locate" hdd_base_image=$(/usr/bin/virsh dumpxml $base_host \ |/usr/bin/grep "source file" |/usr/bin/grep "qcow2" |/usr/bin/head -n 1 \ |/usr/bin/cut -d "'" -f2) if [ -z "$hdd_base_image" ] then /bin/echo "base hdd image not found!" else /usr/bin/echo "hdd_base_image found is a $hdd_base_image. Run next step 2" #&lt; CHECK FOR SNAPSHOT ON BASE HDD &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" | /usr/bin/grep -c "Snapshot"` ] then /usr/bin/echo "base image haven't snapshot, run NEXT STEP 3" else /usr/bin/echo "base hdd image have a snapshot, can't use this image" exit fi #&lt;/ CHECK FOR SNAPSHOT ON BASE HDD &gt;# #&lt; CHECK FOR HDD IMAGE IS LINK CLONE &gt;# if [ 0 -eq `/usr/bin/qemu-img info "$hdd_base_image" |/usr/bin/grep -c "backing file" then /usr/bin/echo "base image is not a linked clone, NEXT STEP 4" /usr/bin/echo "Base image check complete!" else /usr/bin/echo "base hdd image is a linked clone, can't use this image" exit fi fi #&lt;/ CHECK FOR HDD IMAGE IS LINK CLONE &gt;# cloning } cloning() { # &lt;Step_1 turn the base VM off &gt;# /usr/bin/virsh shutdown $base_host &gt; /dev/null 2&gt;&amp;1 # &lt;/Step_1 turn the base VM off &gt;# #&lt;Create_vm_config&gt;# /usr/bin/echo "Free port for Spice VM is $new_vm_port" #&lt;Setup_name_for_new_VM&gt;# new_vm_name=$(/bin/echo $base_host"-"$new_vm_port) #&lt;/Setup_name_for_new_VM&gt;# #&lt;Make_base_config_as_clone_base_VM&gt;# /usr/bin/virsh dumpxml $base_host &gt; $SRV_TMP_DIR/$new_vm_name.xml #&lt;Make_base_config_as_clone_base_VM&gt;# ##&lt;Setup_New_VM_Name_in_config&gt;## /usr/bin/sed -i "s%&lt;name&gt;$base_host&lt;/name&gt;%&lt;name&gt;$new_vm_name&lt;/name&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Setup_New_VM_Name_in_config&gt;# #&lt;UUID Changing&gt;# old_uuid=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "&lt;uuid&gt;") /usr/bin/echo old UUID $old_uuid new_uuid_part1=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 1,2) new_uuid_part2=$(/usr/bin/echo "$old_uuid" |/usr/bin/cut -d "-" -f 4,5) new_uuid=$(/bin/echo $new_uuid_part1"-"$new_vm_port"-"$new_uuid_part2) /usr/bin/echo $new_uuid /usr/bin/sed -i "s%$old_uuid%$new_uuid%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/UUID Changing&gt;# #&lt;Spice port replace&gt;# old_spice_port=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml \ |/usr/bin/grep "graphics type='spice' port=") /bin/echo old spice port $old_spice_port new_spice_port=$(/usr/bin/echo "&lt;graphics type='spice' port='$new_vm_port' autoport='no' listen='127.0.0.1'&gt;") /bin/echo $new_spice_port /usr/bin/sed -i "s%$old_spice_port%$new_spice_port%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/Spice port replace&gt;# #&lt;MAC_ADDR_GENERATE&gt;# mac_new=$(/usr/bin/hexdump -n6 -e '/1 ":%02X"' /dev/random|/usr/bin/sed s/^://g) /usr/bin/echo New Mac is $mac_new #&lt;/MAC_ADDR_GENERATE&gt;# #&lt;GET OLD MAC AND REPLACE&gt;# mac_old=$(/usr/bin/cat $SRV_TMP_DIR/$new_vm_name.xml |/usr/bin/grep "mac address=") /usr/bin/echo old mac is $mac_old /usr/bin/sed -i "s%$mac_old%$mac_new%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;GET OLD MAC AND REPLACE&gt;# #&lt;new_disk_create&gt;# /usr/bin/qemu-img create -f qcow2 -b $hdd_base_image /var/lib/libvirt/images_write/$new_vm_name.qcow2 #&lt;/new_disk_create&gt;# #&lt;attach_new_disk_in_confiig&gt;# /usr/bin/echo hdd base image is $hdd_base_image /usr/bin/sed -i "s%&lt;source file='$hdd_base_image'/&gt;%&lt;source file='/var/lib/libvirt/images_write/$new_vm_name.qcow2'/&gt;%g" $SRV_TMP_DIR/$new_vm_name.xml #&lt;/attach_new_disk_in_confiig&gt;# starting_vm #&lt;/Create_vm config&gt;# } starting_vm() { /usr/bin/virsh define $SRV_TMP_DIR/$new_vm_name.xml /usr/bin/virsh start $new_vm_name while [ $a -ne 1 ] do if /usr/bin/virsh list --all |/usr/bin/grep "$new_vm_name" |/usr/bin/grep "running" &gt; /dev/null 2&gt;&amp;1 then a=1 /usr/bin/sed -i "/$new_vm_port/d" $SRV_TMP_DIR/recycle.list /usr/bin/echo $new_vm_port &gt;&gt; $SRV_TMP_DIR/clear.list /usr/bin/echo "#" "$date" "VM $new_vm_name IS STARTED #" else /usr/bin/echo "#VM $new_vm_name is not ready#" a=0 /usr/bin/sleep 2s fi done /usr/bin/echo "#$date EXIT FROM VM_CREATE.SH#" exit } hdd_image_locate</span></span></code> </pre><br></div></div><br>  Der Prozess zum Erstellen einer neuen virtuellen Maschine <br><br>  Das Skript vm_create.sh liest aus der Konfigurationsdatei den Wert der Variablen "base_host", die die virtuelle Beispielmaschine bestimmt, auf deren Grundlage der Klon erstellt wird.  Es entlädt die XML-Konfiguration der VM aus der Hypervisor-Datenbank, führt eine Reihe von Überprüfungen des VM-Disk-Images durch und erstellt nach erfolgreichem Abschluss die XML-Konfigurationsdatei für die neue VM und das Disk-Image des verknüpften Klons der neuen VM.  Danach wird die XML-Konfiguration der neuen VM in die Hypervisor-Datenbank geladen und die VM gestartet.  Der spice_console-Port wird von recycle.list nach clear.list übertragen.  Die Ausführung von vm_create.sh endet und die Ausführung von vm_manager.sh endet. <br>  Wenn Sie das nächste Mal eine Verbindung herstellen, beginnt dies von vorne. <br><br>  Für Notfälle enthält das Kit ein Skript vm_clear.sh, das alle VMs aus dem Pool zwangsweise durchläuft und sie entfernt, indem die Werte der Listen auf Null gesetzt werden.  Wenn Sie es beim Laden aufrufen, können Sie VDI von Grund auf neu starten. <br><br><div class="spoiler">  <b class="spoiler_title">/home/admin/scripts_vdi_new/vm_clear.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/sh #set VARIABLES# SRV_SCRIPTS_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_scripts_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR" export SRV_SCRIPTS_DIR=$SRV_SCRIPTS_DIR SRV_TMP_DIR=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_tmp_dir" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_TMP_DIR=$SRV_TMP_DIR" export SRV_TMP_DIR=$SRV_TMP_DIR SRV_POOL_SIZE=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_pool_size" |/usr/bin/cut -d "=" -f2) /usr/bin/echo "SRV_POOL_SIZE=$SRV_POOL_SIZE" SRV_START_PORT_POOL=$(/usr/bin/cat /etc/vm_manager.conf \ |/usr/bin/grep "srv_start_port_pool" |/usr/bin/cut -d "=" -f2) /usr/bin/echo SRV_START_PORT_POOL=$SRV_START_PORT_POOL #Set VARIABLES# /usr/bin/echo "= Cleanup ALL VM=" /usr/bin/mkdir $SRV_TMP_DIR /usr/sbin/service iptables restart /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/clear.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/waste.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/recycle.list /usr/bin/cat /dev/null &gt; $SRV_TMP_DIR/conn_wait.list port_to_delete=$(($SRV_START_PORT_POOL+$SRV_POOL_SIZE)) while [ "$port_to_delete" -gt "$SRV_START_PORT_POOL" ] do $SRV_SCRIPTS_DIR/vm_delete.sh $port_to_delete port_to_delete=$(($port_to_delete-1)) done /usr/bin/echo "= EXIT FROM VM_CLEAR.SH="</span></span></code> </pre><br></div></div><br>  Damit möchte ich den ersten Teil meiner Geschichte beenden.  Dies sollte für Systemadministratoren ausreichen, um underVDI im Geschäftsleben zu testen.  Wenn die Community dieses Thema interessant findet, werde ich im zweiten Teil über die Modifikation von livecd Fedora und deren Umwandlung in einen Kiosk sprechen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462203/">https://habr.com/ru/post/de462203/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462181/index.html">Regeln für eine effektive Kommunikation in Gruppenchats</a></li>
<li><a href="../de462185/index.html">Die Revolution ist vorbei. Gibt es eine Alternative zu einem Lithium-Ionen-Akku?</a></li>
<li><a href="../de462189/index.html">Ätzen von Daten mit Travajs</a></li>
<li><a href="../de462191/index.html">DataArt Museum: Eine Tour durch Norditalien</a></li>
<li><a href="../de462197/index.html">Tipps, wie Sie Ihren Geist befreien und Ihre Kreativität steigern können</a></li>
<li><a href="../de462205/index.html">PHDays 9 gewinnen The Standoff: Die Chronik des True0xA3-Teams</a></li>
<li><a href="../de462209/index.html">Polycom VideoConference-Lösungen. Erinnerungen 6 Jahre später ... Stufe 2. Teil 1. RMX1500</a></li>
<li><a href="../de462213/index.html">Lernen und arbeiten: die Erfahrung von Studenten an der Fakultät für Informationstechnologie und Programmierung</a></li>
<li><a href="../de462221/index.html">Wie enttäuscht ich von Google Play bin</a></li>
<li><a href="../de462227/index.html">Moskau, 9. August - Backend Stories 4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>