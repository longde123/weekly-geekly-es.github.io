<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛀🏼 🧘🏾 🗽 Audio AI: mengekstraksi vokal dari musik menggunakan jaringan saraf convolutional 👨🏾‍🎤 💇🏽 👸🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Meretas musik untuk mendemokratisasi konten yang diturunkan 

 Penafian: Semua kekayaan intelektual, desain, dan metode yang dijelaskan dalam artikel ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Audio AI: mengekstraksi vokal dari musik menggunakan jaringan saraf convolutional</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441090/">  <i>Meretas musik untuk mendemokratisasi konten yang diturunkan</i> <br><br><blockquote>  <b>Penafian:</b> Semua kekayaan intelektual, desain, dan metode yang dijelaskan dalam artikel ini diungkapkan dalam US10014002B2 dan US9842609B2. </blockquote><br>  Seandainya saya bisa kembali ke 1965, mengetuk pintu depan studio Abby Road dengan sebuah pas, masuk ke dalam - dan dengarkan suara asli Lennon dan McCartney ... Baiklah, mari kita coba.  Input: Beatles kualitas rata-rata MP3 <i>We Can Work Out</i> .  Track atas adalah campuran input, track bawah adalah vokal terisolasi yang disorot oleh jaringan saraf kita. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Secara formal, masalah ini dikenal sebagai <i>pemisahan sumber suara</i> atau <i>pemisahan sinyal</i> (pemisahan sumber audio).  Ini terdiri dalam pemulihan atau rekonstruksi satu atau lebih dari sinyal asli, yang dicampur dengan sinyal lain sebagai hasil dari proses <i>linier atau konvolusional</i> .  Bidang penelitian ini memiliki banyak aplikasi praktis, termasuk meningkatkan kualitas suara (ucapan) dan menghilangkan kebisingan, remix musik, distribusi spasial suara, remastering, dll. Insinyur suara kadang-kadang menyebut teknik ini demixing.  Ada banyak sumber daya pada topik ini, dari pemisahan sinyal buta dengan analisis komponen independen (ICA) ke faktorisasi semi-terkontrol dari matriks non-negatif dan diakhiri dengan pendekatan selanjutnya berdasarkan jaringan saraf.  Anda dapat menemukan informasi yang bagus tentang dua poin pertama dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">panduan mini</a> dari CCRMA ini, yang pada satu waktu sangat berguna bagi saya. <br><br>  <b>Tapi sebelum terjun ke pengembangan ... sedikit filosofi pembelajaran mesin yang diterapkan ...</b> <br><br>  Saya terlibat dalam pemrosesan sinyal dan gambar bahkan sebelum slogan "pembelajaran mendalam memecahkan segalanya" telah menyebar, jadi saya dapat memberikan Anda solusi sebagai perjalanan <i>rekayasa fitur</i> dan menunjukkan <b>mengapa jaringan saraf adalah pendekatan terbaik untuk masalah khusus ini</b> .  Mengapa  Sangat sering, saya melihat orang menulis sesuatu seperti ini: <br><br>  <i>“Dengan pembelajaran yang mendalam, Anda tidak perlu lagi khawatir memilih fitur;</i>  <i>itu akan melakukannya untuk Anda. "</i> <br><br>  atau lebih buruk ... <br><br>  <i>"Perbedaan antara pembelajaran mesin dan pembelajaran mendalam</i> [hei ... pembelajaran mendalam masih pembelajaran mesin!] Apakah <i>itu di ML Anda sendiri mengekstrak atribut, dan dalam pembelajaran mendalam ini terjadi secara otomatis dalam jaringan."</i> <br><br>  Generalisasi ini mungkin berasal dari kenyataan bahwa DNN bisa sangat efektif dalam mengeksplorasi ruang tersembunyi yang baik.  Tapi jadi tidak mungkin untuk menggeneralisasi.  Saya sangat sedih ketika lulusan dan praktisi baru-baru ini menyerah pada kesalahpahaman di atas dan mengadopsi pendekatan "belajar-dalam-itu-semua".  Seperti, itu sudah cukup untuk membuang banyak data mentah (bahkan setelah sedikit proses awal) - dan semuanya akan berfungsi sebagaimana mestinya.  Di dunia nyata, Anda harus mengurus hal-hal seperti kinerja, eksekusi waktu nyata, dan sebagainya. Karena kesalahpahaman seperti itu, Anda akan terjebak dalam mode percobaan untuk waktu yang sangat lama ... <br><br>  <b>Fitur Teknik tetap menjadi disiplin yang sangat penting dalam desain jaringan saraf tiruan.</b>  <b>Seperti dalam teknik ML lainnya, dalam banyak kasus, itu yang membedakan solusi efektif dari tingkat produksi dari percobaan yang gagal atau tidak efektif.</b>  <b>Pemahaman yang mendalam tentang data Anda dan sifatnya masih sangat berarti ...</b> <br><br><h1>  A hingga Z </h1><br>  Ok, saya selesai khotbah.  Sekarang mari kita lihat mengapa kita ada di sini!  Seperti halnya masalah pemrosesan data, pertama mari kita lihat seperti apa bentuknya.  Lihatlah bagian vokal berikutnya dari rekaman studio asli. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Studio vokal 'Satu Kali Terakhir', Ariana Grande</font></i> <br><br>  Tidak terlalu menarik, bukan?  Nah, ini karena kami memvisualisasikan sinyal <i>tepat waktu</i> .  Di sini kita hanya melihat perubahan amplitudo dari waktu ke waktu.  Tetapi Anda dapat mengekstraksi segala macam hal lain, seperti amplop amplop (amplop), nilai rata-rata akar (RMS), laju perubahan dari nilai positif amplitudo menjadi negatif (laju penyilangan nol), dll., Tetapi <i>tanda</i> - <i>tanda ini</i> terlalu <i>primitif</i> dan tidak cukup khas, untuk membantu dalam masalah kita.  Jika kita ingin mengekstraksi vokal dari sinyal audio, pertama-tama kita perlu menentukan struktur bicara manusia.  Untungnya, Window <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Fourier Transform</a> (STFT) hadir untuk menyelamatkan. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Spektrum amplitudo STFT - ukuran jendela = 2048, tumpang tindih = 75%, skala frekuensi logaritmik [Sonic Visualizer]</font></i> <br><br>  Meskipun saya suka pemrosesan pidato dan pasti suka bermain dengan <i>simulasi filter input, cepstrum, sottotami, LPC, MFCC</i> dan sebagainya <i>, kami akan</i> melewati semua omong kosong ini dan fokus pada elemen utama yang terkait dengan masalah kami sehingga artikel tersebut dapat dipahami oleh sebanyak mungkin orang, bukan hanya spesialis pemrosesan sinyal. <br><br>  Jadi apa yang dikatakan oleh struktur bicara manusia kepada kita? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Kita dapat mendefinisikan tiga elemen utama di sini: <br><br><ul><li>  <b>Frekuensi mendasar</b> (f0), yang ditentukan oleh frekuensi getaran pita suara kita.  Dalam hal ini, Ariana bernyanyi di kisaran 300-500 Hz. <br></li><li>  Serangkaian <b>harmonik di</b> atas f0 yang mengikuti bentuk atau pola yang serupa.  Harmonik ini muncul pada frekuensi yang merupakan kelipatan dari f0. <br></li><li>  Pidato <b>tanpa</b> suara, yang mencakup konsonan seperti 't', 'p', 'k', 's' (yang tidak dihasilkan oleh getaran pita suara), pernapasan, dll. Semua ini memanifestasikan dirinya dalam bentuk ledakan singkat di wilayah frekuensi tinggi. </li></ul><br><h1>  Percobaan Pertama dengan Aturan </h1><br>  Mari kita lupakan sejenak apa yang disebut pembelajaran mesin.  Bisakah metode ekstraksi vokal dikembangkan berdasarkan pengetahuan kita tentang sinyal?  Biarkan saya mencoba ... <br><br>  <b>Isolasi vokal <i>naif</i> V1.0:</b> <br><br><ol><li>  Identifikasi area dengan vokal.  Ada banyak hal dalam sinyal aslinya.  Kami ingin fokus pada area-area yang benar-benar berisi konten vokal, dan mengabaikan yang lainnya. <br></li><li>  Bedakan antara ucapan yang disuarakan dan yang tidak disuarakan.  Seperti yang telah kita lihat, mereka sangat berbeda.  Mereka mungkin perlu ditangani secara berbeda. <br></li><li>  Nilai perubahan frekuensi dasar dari waktu ke waktu. <br></li><li>  Berdasarkan pin 3, aplikasikan semacam topeng untuk menangkap harmonik. <br></li><li>  Lakukan sesuatu dengan fragmen pidato yang tidak disuarakan ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  Jika kita bekerja dengan layak, hasilnya akan menjadi <i>lunak</i> atau <i>bitmask</i> , aplikasi yang untuk amplitudo STFT (penggandaan elemen) memberikan perkiraan rekonstruksi amplitudo vokal STFT.  Kemudian kami menggabungkan STFT vokal ini dengan informasi tentang fase sinyal asli, menghitung STFT terbalik, dan mendapatkan sinyal waktu vokal yang direkonstruksi. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Melakukannya dari awal sudah merupakan pekerjaan besar.  Namun demi demonstrasi, penerapan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">algoritma pYIN dapat diterapkan</a> .  Meskipun ini dimaksudkan untuk menyelesaikan langkah 3, tetapi dengan pengaturan yang benar, ia melakukan langkah 1 dan 2 dengan baik, melacak basis vokal bahkan di hadapan musik.  Contoh di bawah ini berisi output setelah memproses algoritma ini, tanpa memproses ucapan yang tidak disuarakan. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Dan apa ...?  Dia tampaknya telah melakukan semua pekerjaan, tetapi tidak ada kualitas yang baik dan dekat.  Mungkin dengan menghabiskan lebih banyak waktu, energi, dan uang, kami akan meningkatkan metode ini ... <br><br>  Tapi saya ingin bertanya ... <br><br>  Apa yang terjadi jika <b>beberapa suara</b> muncul di trek, namun sering ditemukan di setidaknya 50% trek profesional modern? <br><br>  Apa yang terjadi jika vokal diproses oleh <b>reverb, penundaan</b> dan efek lainnya?  Mari kita lihat paduan suara terakhir Ariana Grande dari lagu ini. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Apakah Anda sudah merasakan sakit ...?  Saya <br><br>  Metode seperti itu pada aturan ketat sangat cepat berubah menjadi rumah kartu.  Masalahnya terlalu rumit.  Terlalu banyak aturan, terlalu banyak pengecualian, dan terlalu banyak kondisi berbeda (efek dan pengaturan campuran).  Pendekatan multi-langkah juga menyiratkan bahwa kesalahan dalam satu langkah memperluas masalah ke langkah berikutnya.  Memperbaiki setiap langkah akan menjadi sangat mahal: akan membutuhkan banyak iterasi untuk memperbaikinya.  Dan yang terakhir, namun tidak kalah pentingnya, kemungkinan besar pada akhirnya kita akan mendapatkan konveyor yang sangat intensif sumber daya, yang dengan sendirinya dapat meniadakan semua upaya. <br><br>  <b>Dalam situasi seperti itu, sekarang saatnya untuk mulai berpikir tentang pendekatan <i>yang</i> lebih <i>komprehensif</i> dan biarkan ML mengetahui bagian dari proses dasar dan operasi yang diperlukan untuk menyelesaikan masalah.</b>  <b>Tapi kami masih harus menunjukkan keahlian kami dan terlibat dalam rekayasa fitur, dan Anda akan tahu sebabnya.</b> <br><br><h1>  Hipotesis: gunakan jaringan saraf sebagai fungsi transfer yang menerjemahkan campuran menjadi vokal </h1><br>  Melihat pencapaian jaringan saraf convolutional dalam pemrosesan foto, mengapa tidak menerapkan pendekatan yang sama di sini? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">Jaringan saraf berhasil memecahkan masalah seperti pewarnaan gambar, penajaman dan resolusi.</font></i> <br><br>  Pada akhirnya, Anda bisa membayangkan sinyal suara "sebagai gambar" menggunakan transformasi Fourier jangka pendek, bukan?  Meskipun <i>gambar suara</i> ini tidak sesuai dengan distribusi statistik gambar alami, mereka masih memiliki pola spasial (dalam ruang waktu dan frekuensi) yang digunakan untuk melatih jaringan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">Kiri: drum beat dan baseline di bawah, beberapa suara synthesizer di tengah, semua dicampur dengan vokal.</font></i>  <i><font color="gray">Kanan: hanya vokal</font></i> <br><br>  Melakukan eksperimen semacam itu akan menjadi pekerjaan yang mahal karena sulit untuk mendapatkan atau menghasilkan data pelatihan yang diperlukan.  Tetapi dalam penelitian terapan, saya selalu mencoba menggunakan pendekatan ini: pertama, <b>untuk mengidentifikasi masalah yang lebih sederhana yang menegaskan prinsip yang sama</b> , tetapi tidak memerlukan banyak pekerjaan.  Ini memungkinkan Anda untuk mengevaluasi hipotesis, beralih lebih cepat dan memperbaiki model dengan kerugian minimal jika tidak berfungsi sebagaimana mestinya. <br><br>  Kondisi tersirat adalah bahwa <b>jaringan saraf harus memahami struktur bicara manusia</b> .  Masalah yang lebih sederhana mungkin adalah ini: <i>dapatkah jaringan syaraf menentukan keberadaan ucapan pada fragmen rekaman suara yang berubah-ubah</i> .  Kita berbicara tentang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pendeteksi aktivitas suara (VAD) yang</a> andal, diimplementasikan dalam bentuk classifier biner. <br><br><h3>  Kami merancang ruang tanda </h3><br>  Kita tahu bahwa sinyal suara, seperti musik dan ucapan manusia, didasarkan pada ketergantungan waktu.  Sederhananya, tidak ada yang terjadi dalam isolasi pada titik waktu tertentu.  Jika saya ingin tahu apakah ada suara pada rekaman suara tertentu, maka saya perlu melihat daerah tetangga.  <i>Konteks waktu</i> seperti <i>itu</i> memberikan informasi yang baik tentang apa yang terjadi di bidang yang diminati.  Pada saat yang sama, diinginkan untuk melakukan klasifikasi dengan penambahan waktu yang sangat kecil untuk mengenali suara manusia dengan resolusi waktu setinggi mungkin. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Mari kita hitung sedikit ... <br><br><ul><li>  Frekuensi pengambilan sampel (fs): 22050 Hz (kami menurunkan sampel dari 44100 menjadi 22050) <br></li><li>  Desain STFT: ukuran jendela = 1024, ukuran hop = 256, interpolasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">skala kapur</a> untuk filter pembobotan, dengan mempertimbangkan persepsi akun.  Karena input kami adalah <i>nyata</i> , Anda dapat bekerja dengan setengah STFT (penjelasan di luar ruang lingkup artikel ini ...) sambil mempertahankan komponen DC (opsional), yang memberi kami 513 tempat frekuensi. <br></li><li>  Resolusi klasifikasi target: satu frame STFT (~ 11,6 ms = 256/22050) <br></li><li>  Konteks waktu target: ~ 300 milidetik = 25 frame STFT. <br></li><li>  Jumlah target contoh pelatihan: 500 ribu. <br></li><li>  Dengan asumsi kami menggunakan jendela geser dengan penambahan 1 kerangka waktu STFT untuk menghasilkan data pelatihan, kami membutuhkan sekitar 1,6 jam suara yang ditandai untuk menghasilkan 500 ribu sampel data </li></ul><br>  Dengan persyaratan di atas, input dan output dari classifier biner kami adalah sebagai berikut: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Model </h3><br>  Menggunakan Keras, kami akan membangun model kecil dari jaringan saraf untuk menguji hipotesis kami. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  Dengan membagi 80/20 data ke dalam pelatihan dan pengujian setelah ~ 50 zaman, kita mendapatkan <b>akurasi saat menguji ~ 97%</b> .  Ini adalah bukti yang cukup bahwa model kami dapat membedakan antara vokal dalam fragmen suara musik (dan fragmen tanpa vokal).  Jika kita memeriksa beberapa peta fitur dari lapisan konvolusional ke-4, kita dapat menyimpulkan bahwa jaringan saraf tampaknya telah mengoptimalkan kernel untuk melakukan dua tugas: menyaring musik dan menyaring vokal ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">Contoh peta benda di pintu keluar lapisan konvolusional ke-4.</font></i>  <i><font color="gray">Tampaknya, output di sebelah kiri adalah hasil dari operasi kernel dalam upaya untuk mempertahankan konten vokal sambil mengabaikan musik.</font></i>  <i><font color="gray">Nilai-nilai tinggi menyerupai struktur harmonis ucapan manusia.</font></i>  <i><font color="gray">Peta objek di sebelah kanan tampaknya merupakan hasil dari tugas yang berlawanan.</font></i> <br><br><h1>  Dari detektor suara ke pemutusan sinyal </h1><br>  Setelah memecahkan masalah klasifikasi yang lebih sederhana, bagaimana kita bisa beralih ke pemisahan vokal yang sebenarnya dari musik?  Nah, melihat metode <i>naif</i> pertama, kami masih ingin entah bagaimana mendapatkan spektrogram amplitudo untuk vokal.  Sekarang ini menjadi tugas regresi.  Yang ingin kami lakukan adalah menghitung spektrum amplitudo yang sesuai untuk vokal dalam kerangka waktu ini dari STFT dari sinyal asli, yaitu dari campuran (dengan konteks waktu yang cukup). <br><br>  <b>Bagaimana dengan dataset pelatihan?</b>  <b>(Anda dapat bertanya kepada saya saat ini)</b> <br><br>  Sial ... kenapa begitu.  Saya akan mempertimbangkan ini di akhir artikel agar tidak teralihkan dari topik! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  Jika model kami terlatih dengan baik, maka untuk kesimpulan logis Anda hanya perlu menerapkan jendela geser sederhana ke campuran STFT.  Setelah setiap perkiraan, gerakkan jendela ke kanan sebanyak 1 jangka waktu, prediksi bingkai berikutnya dengan vokal dan kaitkan dengan prediksi sebelumnya.  Adapun model, mari kita ambil model yang sama yang digunakan untuk detektor suara dan membuat perubahan kecil: gelombang output sekarang (513.1), aktivasi linier pada output, MSE sebagai fungsi kerugian.  Sekarang kita mulai pelatihan. <br><br>  <b>Jangan bersukacita lagi ...</b> <br><br>  Meskipun representasi I / O ini masuk akal, setelah melatih model kami beberapa kali, dengan berbagai parameter dan normalisasi data, tidak ada hasil.  Sepertinya kita terlalu banyak bertanya ... <br><br>  Kami telah pindah dari klasifikasi biner ke <i>regresi</i> pada vektor 513 dimensi.  Meskipun jaringan mempelajari masalah sampai batas tertentu, vokal yang dipulihkan masih memiliki artefak yang jelas dan gangguan dari sumber lain.  Bahkan setelah menambahkan lapisan tambahan dan meningkatkan jumlah parameter model, hasilnya tidak banyak berubah.  Dan kemudian muncul pertanyaan: <b>bagaimana cara "menyederhanakan" tugas untuk jaringan dengan penipuan, dan pada saat yang sama mencapai hasil yang diinginkan?</b> <br><br>  Bagaimana jika, alih-alih memperkirakan amplitudo vokal STFT, kami melatih jaringan untuk mendapatkan topeng biner, yang bila diterapkan pada campuran STFT memberi kami spektogram amplitudo vokal yang disederhanakan, tetapi <b>dapat diterima secara perseptual</b> dari vokal? <br><br>  Bereksperimen dengan berbagai heuristik, kami menghasilkan cara yang sangat sederhana (dan, tentu saja, tidak lazim dalam hal pemrosesan sinyal ...) untuk mengekstraksi vokal dari campuran menggunakan topeng biner.  Tanpa merinci, esensinya adalah sebagai berikut.  Bayangkan output sebagai gambar biner, di mana nilai '1' menunjukkan <b>keberadaan konten vokal</b> pada frekuensi dan jangka waktu tertentu, dan nilai '0' menunjukkan keberadaan musik di tempat tertentu.  Kita dapat menyebutnya <i>binarisasi persepsi</i> , hanya untuk menghasilkan sebuah nama.  Secara visual, itu terlihat sangat jelek, jujur, tetapi hasilnya sangat bagus. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Sekarang masalah kita menjadi semacam regresi-klasifikasi hibrida (sangat kasar ...).  Kami meminta model untuk "mengklasifikasikan piksel" pada output sebagai vokal atau non-vokal, meskipun secara konseptual (serta dari sudut pandang fungsi kehilangan MSE yang digunakan) tugas tetap regresif. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Meskipun perbedaan ini mungkin tampak tidak tepat untuk beberapa orang, pada kenyataannya itu sangat penting dalam kemampuan model untuk mempelajari tugas, yang kedua adalah lebih sederhana dan lebih terbatas.  Pada saat yang sama, ini memungkinkan kami untuk menjaga model kami relatif kecil dalam hal jumlah parameter, mengingat kompleksitas tugas, sesuatu yang sangat diinginkan untuk bekerja secara real time, yang dalam hal ini merupakan persyaratan desain.  Setelah beberapa perubahan kecil, model terakhir terlihat seperti ini. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  Bagaimana cara memulihkan sinyal domain waktu? </h3><br>  Bahkan, seperti dalam <i>metode naif</i> .  Dalam hal ini, untuk setiap pass, kami memperkirakan satu jangka waktu dari topeng vokal biner.  Sekali lagi, mewujudkan jendela geser sederhana dengan langkah satu kerangka waktu, kami terus mengevaluasi dan menggabungkan kerangka waktu berturut-turut, yang akhirnya membentuk seluruh topeng biner vokal. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Buat satu set pelatihan </h3><br>  Seperti yang Anda ketahui, salah satu masalah utama ketika mengajar dengan seorang guru (tinggalkan contoh mainan ini dengan kumpulan data yang sudah jadi) adalah data yang benar (dalam jumlah dan kualitas) untuk masalah spesifik yang Anda coba selesaikan.  Berdasarkan representasi input dan output yang dijelaskan, untuk melatih model kami, Anda pertama-tama akan membutuhkan sejumlah besar campuran dan trek vokal yang sesuai, selaras sempurna, dan dinormalisasi.  Perangkat seperti itu dapat dibuat dalam beberapa cara, dan kami menggunakan kombinasi strategi, dari membuat pasangan [campuran &lt;-&gt; vokal] secara manual berdasarkan pada beberapa cappella yang ditemukan di Internet, hingga mencari bahan musik band rock dan scrapbooking Youtube.  Hanya untuk memberi Anda gambaran tentang betapa melelahkan dan menyakitkannya proses ini, bagian dari proyek ini adalah pengembangan alat semacam itu untuk secara otomatis membuat pasangan [campuran &lt;-&gt; vokal]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  Sejumlah besar data diperlukan untuk jaringan saraf untuk mempelajari fungsi transfer untuk penyiaran campuran ke vokal.  Set terakhir kami terdiri dari sekitar 15 juta sampel 300 ms campuran dan topeng biner vokal yang sesuai. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Arsitektur perpipaan </h3><br>  Seperti yang mungkin Anda ketahui, membuat model ML untuk tugas tertentu hanya setengah dari pertempuran.  Di dunia nyata, Anda perlu memikirkan arsitektur perangkat lunak, terutama jika Anda perlu bekerja secara langsung atau dekat dengannya. <br><br>  Dalam implementasi khusus ini, rekonstruksi dalam domain waktu dapat terjadi segera setelah memprediksi topeng vokal biner penuh (mode yang berdiri sendiri) atau, yang lebih menarik, dalam mode multi-berulir, tempat kami menerima dan memproses data, mengembalikan vokal dan mereproduksi suara - semua dalam segmen kecil, dekat dengan streaming dan bahkan hampir secara real time, memproses musik yang direkam dengan cepat dengan penundaan minimal.  Sebenarnya, ini adalah topik yang terpisah, dan saya akan meninggalkannya untuk artikel lain <b>di jaringan pipa ML real-time</b> ... <br><br><h1>  Saya kira saya sudah mengatakan cukup, jadi mengapa tidak mendengarkan beberapa contoh !? </h1><br><h3>  Daft Punk - Get Lucky (rekaman studio) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Di sini Anda dapat mendengar beberapa gangguan minimal dari drum ...</font></i> <br><br><h3>  Adele - Set Fire to the Rain (rekaman langsung!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Perhatikan bagaimana di awal model kami mengekstrak jeritan orang banyak sebagai konten vokal :).</font></i>  <i><font color="gray">Dalam hal ini, ada beberapa gangguan dari sumber lain.</font></i>  <i><font color="gray">Karena ini adalah rekaman langsung, tampaknya dapat diterima bahwa vokal yang diekstraksi memiliki kualitas yang lebih buruk daripada yang sebelumnya.</font></i> <br><br><h1>  Ya, dan "sesuatu yang lain" ... </h1><br><h1>  Jika sistem bekerja untuk vokal, mengapa tidak menerapkannya ke instrumen lain ...? </h1><br>  Artikel ini sudah cukup besar, tetapi mengingat pekerjaan yang dilakukan, Anda layak mendengar demo terbaru.  Dengan logika yang persis sama dengan saat mengekstraksi vokal, kita dapat mencoba membagi musik stereo menjadi komponen (drum, bass, vokal, yang lain), membuat beberapa perubahan dalam model kita dan, tentu saja, memiliki set pelatihan yang sesuai :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Terima kasih sudah membaca.  Sebagai catatan terakhir: seperti yang Anda lihat, model aktual dari jaringan saraf convolutional kami tidak begitu istimewa.  Keberhasilan pekerjaan ini ditentukan oleh <b>Feature Engineering</b> dan proses pengujian hipotesis yang rapi, yang akan saya tulis di artikel mendatang! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id441090/">https://habr.com/ru/post/id441090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id441076/index.html">Optimasi Skrip dengan Webpack SplitChunksPlugin</a></li>
<li><a href="../id441078/index.html">LG akan memperkenalkan smartphone dengan layar loudspeaker OLED: beberapa kata tentang perangkat dan teknologi baru</a></li>
<li><a href="../id441082/index.html">Tiket Mars akan berharga kurang dari $ 500.000</a></li>
<li><a href="../id441084/index.html">Kemana perginya para Pengajar Dini?</a></li>
<li><a href="../id441088/index.html">Pengembang, ingat - lalu lintas aplikasi Anda sedang diawasi</a></li>
<li><a href="../id441092/index.html">Embedded World 2019 - pameran elektronik tertanam terbesar</a></li>
<li><a href="../id441096/index.html">Simulator membaca artikel</a></li>
<li><a href="../id441098/index.html">Kedalaman SIEM: korelasi out-of-box. Bagian 4. Model sistem sebagai konteks aturan korelasi</a></li>
<li><a href="../id441102/index.html">Kaspersky Mobile Talks - pertemuan untuk pengembang tingkat lanjut</a></li>
<li><a href="../id441104/index.html">Memperoleh informasi dan melewati otentikasi dua faktor pada kartu bank dari TOP-10 (Ukraina)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>