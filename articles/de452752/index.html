<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüç≥ üö¶ ‚òïÔ∏è Hausgemachte BigData. Teil 1. Spark-Streaming-Praxis in einem AWS-Cluster üíÜ üå∏ ü§òüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Guten Tag. 

 Es gibt viele Dienste im Internet, die Cloud-Dienste bereitstellen. Mit ihrer Hilfe k√∂nnen Sie die Technologie von BigData erlernen. 

 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hausgemachte BigData. Teil 1. Spark-Streaming-Praxis in einem AWS-Cluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/homecredit/blog/452752/">  Guten Tag. <br><br>  Es gibt viele Dienste im Internet, die Cloud-Dienste bereitstellen.  Mit ihrer Hilfe k√∂nnen Sie die Technologie von BigData erlernen. <br><br>  In diesem Artikel werden wir Apache Kafka, Apache Spark, Zookeeper und Spark-Shell auf der EC2 AWS-Plattform (Amazon Web Services) zu Hause installieren und lernen, wie Sie alles verwenden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/51c/95c/810/51c95c8104a988a0288067e9eaaf700f.jpg" alt="Bild"></div><br><a name="habracut"></a><br><h3>  Einf√ºhrung in Amazon Web Services </h3><br>  Sie m√ºssen sich unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aws.amazon.com/console</a> registrieren.  Geben Sie einen Namen ein und merken Sie sich das Passwort. <br><br>  Konfigurieren Sie Knoteninstanzen f√ºr Zookeeper- und Kafka-Dienste. <br><br><ul><li>  W√§hlen Sie "Services-&gt; EC2" aus dem Men√º.  W√§hlen Sie als N√§chstes die Version des Betriebssystems des Images der virtuellen Maschine aus, w√§hlen Sie Ubuntu Server 16.04 LTS (HVM), SSD-Volume-Typ, klicken Sie auf ‚ÄûAusw√§hlen‚Äú. Wir konfigurieren die Serverinstanz: Geben Sie ‚Äût3.medium‚Äú mit den Parametern 2vCPU, 4 GB Speicher, Allgemeiner Zweck ein Klicken Sie auf "Weiter: Instanzdetails konfigurieren". </li><li>  F√ºgen Sie die Anzahl der Instanzen 1 hinzu und klicken Sie auf "Weiter: Speicher hinzuf√ºgen". </li><li>  Wir akzeptieren den Standardwert f√ºr die Festplattengr√∂√üe von 8 GB und √§ndern den Typ in "Magnetisch" (in den Produktionseinstellungen basierend auf Datenvolumen und Hochleistungs-SSD). </li><li>  Geben Sie im Abschnitt "Tag-Instanzen" f√ºr "Name" den Namen der Instanz des Knotens "Home1" ein (wobei 1 nur eine Seriennummer ist) und klicken Sie auf "Weiter: ...". </li><li>  W√§hlen Sie im Abschnitt "Sicherheitsgruppen konfigurieren" die Option "Vorhandene Sicherheitsgruppe verwenden" aus, indem Sie den Namen der Sicherheitsgruppe ("Spark_Kafka_Zoo_Project") ausw√§hlen und die Regeln f√ºr eingehenden Datenverkehr festlegen.  Klicken Sie auf "Weiter: ..." </li><li>  Scrollen Sie durch den √úberpr√ºfungsbildschirm, um Ihre Eingaben zu √ºberpr√ºfen und Launch zu starten. </li><li>  Um eine Verbindung zu den Clusterknoten herzustellen, m√ºssen Sie ein Paar √∂ffentlicher Schl√ºssel zur Identifizierung und Autorisierung erstellen (in unserem Fall die vorhandenen verwenden).  W√§hlen Sie dazu in der Liste den Operationstyp ‚ÄûVorhandenes Paar verwenden‚Äú. </li></ul><br><h3>  Schl√ºsselerstellung </h3><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laden Sie Putty</a> (https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html) f√ºr den Client <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">herunter</a> oder verwenden Sie die SSH-Verbindung vom Terminal. </li><li>  Die .pem-Schl√ºsseldatei verwendet der Einfachheit halber das alte Format. Wir konvertieren es in das von Putty verwendete ppk-Format.  F√ºhren Sie dazu das Dienstprogramm PuTTYgen aus und laden Sie den Schl√ºssel im alten PEM-Format in das Dienstprogramm.  Wir konvertieren den Schl√ºssel und speichern ihn (Save Private Key) zur sp√§teren Verwendung im Home-Ordner mit der Erweiterung .ppk. </li></ul><br><h3>  Clusterstart </h3><br>  Benennen Sie die Clusterknoten der Einfachheit halber in Node01-04-Notation um.  Um vom lokalen Computer aus √ºber SSH eine Verbindung zu Clusterknoten herzustellen, m√ºssen Sie die IP-Adresse des Knotens und seinen √∂ffentlichen / privaten DNS-Namen ermitteln, jeden Clusterknoten einzeln ausw√§hlen und f√ºr die ausgew√§hlte Instanz seinen √∂ffentlichen / privaten DNS-Namen f√ºr die Verbindung √ºber SSH und f√ºr die Installation notieren Software zur Textdatei HadoopAdm01.txt. <br><br>  Beispiel: ec2-35-162-169-76.us-west-2.compute.amazonaws.com <br><br><h3>  Installieren Sie Apache Kafka im SingleNode-Modus auf einem AWS-Clusterknoten </h3><br>  Um die Software zu installieren, w√§hlen Sie unseren Knoten aus (kopieren Sie dessen √∂ffentliches DNS), um eine Verbindung √ºber SSH herzustellen.  Wir konfigurieren die Verbindung √ºber SSH.  Wir verwenden den gespeicherten Namen des ersten Knotens, um die Verbindung √ºber SSH mithilfe des in Abschnitt 1.3 erstellten privaten / √∂ffentlichen Schl√ºsselpaars ‚ÄûHadoopUser01.ppk‚Äú zu konfigurieren.  Wir gehen √ºber die Schaltfl√§che Durchsuchen zum Abschnitt Verbindung / Authentifizierung und suchen nach dem Ordner, in dem wir zuvor die Datei ‚ÄûHadoopUserXX.ppk‚Äú gespeichert haben. <br><br>  Wir speichern die Verbindungskonfiguration in den Einstellungen. <br><br>  Wir sind mit dem Knoten verbunden und verwenden login: ubuntu. <br><br><ul><li>  Mit Root-Rechten aktualisieren wir Pakete und installieren zus√§tzliche Pakete, die f√ºr die weitere Installation und Konfiguration des Clusters erforderlich sind. <br><br><pre><code class="plaintext hljs">sudo apt-get update sudo apt-get -y install wget net-tools netcat tar</code> </pre> </li><li>  Installieren Sie Java 8 jdk und √ºberpr√ºfen Sie die Version von Java. <br><br><pre> <code class="plaintext hljs">sudo apt-get -y install openjdk-8-jdk</code> </pre> </li><li>  F√ºr eine normale Leistung des Clusterknotens m√ºssen Sie die Speicheraustauscheinstellungen anpassen.  VM-Swappines sind standardm√§√üig auf 60% eingestellt. Dies bedeutet, dass das System bei einer Speicherauslastung von 60% aktiv Daten vom RAM auf die Festplatte austauscht.  Abh√§ngig von der Linux-Version kann der Parameter VM swappines auf 0 oder 1 gesetzt werden: <br><br><pre> <code class="plaintext hljs">sudo sysctl vm.swappiness=1</code> </pre> <br></li><li>  F√ºgen Sie der Konfigurationsdatei eine Zeile hinzu, um die Einstellungen w√§hrend des Neustarts zu speichern. <br><br><pre> <code class="plaintext hljs">echo 'vm.swappiness=1' | sudo tee --append /etc/sysctl.conf</code> </pre> <br></li><li>  Bearbeiten von Eintr√§gen in der Datei / etc / hosts zur bequemen Aufl√∂sung der Knotennamen des Kafka-Clusters und <br>  zookeeper an privaten IP-Adressen zu den zugewiesenen Clusterknoten. <br><br><pre> <code class="plaintext hljs">echo "172.31.26.162 host01" | sudo tee --append /etc/hosts</code> </pre> <br>  Wir √ºberpr√ºfen die korrekte Erkennung von Namen, indem wir einen der Eintr√§ge pingen. <br><br></li><li>  Laden Sie die neuesten Versionen (http://kafka.apache.org/downloads) der Kafka- und Scala-Distributionen herunter und bereiten Sie das Verzeichnis mit den Installationsdateien vor. <br><br><pre> <code class="plaintext hljs">wget http://mirror.linux-ia64.org/apache/kafka/2.1.0/kafka_2.12-2.1.0.tgz tar -xvzf kafka_2.12-2.1.0.tgz ln -s kafka_2.12-2.1.0 kafka</code> </pre> <br></li><li>  L√∂schen Sie die tgz-Archivdatei, wir brauchen sie nicht mehr <br><br></li><li>  Versuchen wir dazu den Zookeeper-Dienst: <br><br><pre> <code class="plaintext hljs">~/kafka/bin/zookeeper-server-start.sh -daemon ~/kafka/config/zookeeper.properties</code> </pre> <br>  Zookeeper startet mit Standardstartoptionen.  Sie k√∂nnen das Protokoll √ºberpr√ºfen: <br><br><pre> <code class="plaintext hljs">tail -n 5 ~/kafka/logs/zookeeper.out</code> </pre> <br>  Um sicherzustellen, dass der Zookeeper-Daemon nach dem Neustart gestartet wird, m√ºssen wir Zookeper als Hintergrunddienst starten: <br><br><pre> <code class="plaintext hljs">bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code> </pre> <br>  Um den Start von Zookepper zu √ºberpr√ºfen, √ºberpr√ºfen Sie <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181</code> </pre> <br>  Wir konfigurieren den Zookeeper- und Kafka-Dienst f√ºr die Arbeit.  Bearbeiten / erstellen Sie zun√§chst die Datei /etc/systemd/system/zookeeper.service (Dateiinhalt unten). <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Zookeeper server Documentation=http://zookeeper.apache.org Requires=network.target remote-fs.target After=network.target remote-fs.target [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/zookeeper-server-start.sh /home/ubuntu/kafka/config/zookeeper.properties ExecStop=/home/ubuntu/kafka/bin/zookeeper-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre> <br>  Bearbeiten / erstellen Sie als N√§chstes f√ºr Kafka die Datei /etc/systemd/system/kafka.service (Dateiinhalt unten). <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Kafka server (broker) Documentation=http://kafka.apache.org/documentation.html Requires=zookeeper.service [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/kafka-server-start.sh /home/ubuntu/kafka/config/server.properties ExecStop=/home/ubuntu/kafka/bin/kafka-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre> <br></li><li>  Wir aktivieren systemd-Skripte f√ºr Kafka- und Zookeeper-Dienste. <br><br><pre> <code class="plaintext hljs">sudo systemctl enable zookeeper sudo systemctl enable kafka</code> </pre> <br></li><li>  √úberpr√ºfen Sie die Funktion von systemd-Skripten. <br><br><pre> <code class="plaintext hljs">sudo systemctl start zookeeper sudo systemctl start kafka sudo systemctl status zookeeper sudo systemctl status kafka sudo systemctl stop zookeeper sudo systemctl stop kafka</code> </pre> <br></li><li>  Wir werden die Funktionalit√§t der Dienste von Kafka und Zookeeper √ºberpr√ºfen. <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181 netcat -vz localhost 9092</code> </pre> <br></li><li>  √úberpr√ºfen Sie die Zookeeper-Protokolldatei. <br><br><pre> <code class="plaintext hljs">cat logs/zookeeper.out</code> </pre> </li></ul><br><h3>  Erste Freude </h3><br>  Wir erstellen unser erstes Thema auf dem zusammengebauten Kafka-Server. <br><br><ul><li>  Es ist wichtig, die Verbindung zu "host01: 2181" zu verwenden, wie Sie in der Konfigurationsdatei server.properties angegeben haben. <br></li><li>  Wir schreiben einige Daten in das Thema. <br><br><pre> <code class="plaintext hljs">kafka-console-producer.sh --broker-list host01:9092 --topic first_topic    </code> </pre> <br>  Strg-C - Beenden Sie die Themenkonsole. <br><br></li><li>  Versuchen Sie nun, die Daten aus dem Thema zu lesen. <br><br><pre> <code class="plaintext hljs">kafka-console-consumer.sh --bootstrap-server host01:9092 --topic last_topic --from-beginning</code> </pre> <br></li><li>  Schauen wir uns die Liste der Kafka-Themen an. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --list</code> </pre> <br></li><li>  Bearbeiten der Kafka-Servereinstellungen f√ºr die Optimierung der Einrichtung einzelner Cluster <br>  # Sie m√ºssen den ISR-Parameter auf 1 √§ndern. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --config min.insync.replicas=1 --topic __consumer_offsets --alter</code> </pre> <br></li><li>  Wir starten den Kafka-Server neu und versuchen erneut, den Verbraucher-Ohm anzuschlie√üen <br><br></li><li>  Schauen wir uns die Themenliste an. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper host01:2181 --list</code> </pre> </li></ul><br><h3>  Konfigurieren Sie Apache Spark auf einem Cluster mit einem Knoten </h3><br>  Wir haben eine Instanz des Knotens mit den auf AWS installierten Zookeeper- und Kafka-Diensten vorbereitet. Jetzt m√ºssen Sie Apache Spark installieren. <br><br>  Laden Sie die neueste Apache Spark-Distribution herunter. <br><br><pre> <code class="plaintext hljs">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.6.tgz</code> </pre> <br><br><ul><li>  Entpacken Sie die Distribution und erstellen Sie einen symbolischen Link f√ºr spark und l√∂schen Sie unn√∂tige Archivdateien. <br><br><pre> <code class="plaintext hljs">tar -xvf spark-2.4.0-bin-hadoop2.6.tgz ln -s spark-2.4.0-bin-hadoop2.6 spark rm spark*.tgz</code> </pre> <br></li><li>  Gehen Sie zum sbin-Verzeichnis und f√ºhren Sie den Spark-Assistenten aus. <br><br><pre> <code class="plaintext hljs">./start-master.sh</code> </pre> <br></li><li>  Wir stellen √ºber einen Webbrowser eine Verbindung zum Spark-Server an Port 8080 her. <br><br></li><li>  F√ºhren Sie Spark-Slaves auf demselben Knoten aus <br><br><pre> <code class="plaintext hljs">./start-slave.sh spark://host01:7077</code> </pre> <br></li><li>  F√ºhren Sie die Funkenschale mit dem Master auf host01 aus. <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre> <br></li><li>  Wenn der Start nicht funktioniert, f√ºgen Sie den Pfad zu Spark in bash hinzu. <br><br><pre> <code class="plaintext hljs">vi ~/.bashrc #      SPARK_HOME=/home/ubuntu/spark export PATH=$SPARK_HOME/bin:$PATH</code> </pre> <br><pre> <code class="plaintext hljs">source ~/.bashrc</code> </pre> <br></li><li>  F√ºhren Sie die Funkenschale erneut mit dem Master auf host01 aus. <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre> <br>  Ein Einzelknotencluster mit Kafka, Zookeeper und Spark funktioniert.  Hurra! </li></ul><br><h3>  Ein bisschen Kreativit√§t </h3><br>  Laden Sie den Scala-IDE-Editor herunter (unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">scala-ide.org</a> ).  Wir beginnen und beginnen Code zu schreiben.  Hier werde ich mich nicht mehr wiederholen, da es einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">guten Artikel √ºber Habr√© gibt</a> . <br><br>  N√ºtzliche Literatur und Kurse helfen: <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurse.hadoopinrealworld.com/courses/enrolled/319237</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">data-flair.training/blogs/kafka-consumer</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.udemy.com/apache-spark-with-scala-hands-on-with-big-data</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de452752/">https://habr.com/ru/post/de452752/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de452742/index.html">Einrichten des automatischen Testens einer Hybridanwendung</a></li>
<li><a href="../de452744/index.html">Gibt es ein erf√ºlltes Leben eines Fernbedieners ohne freiberuflichen Austausch?</a></li>
<li><a href="../de452746/index.html">Das Buch "Die Kunst des Programmierens in R. Eintauchen in Big Data"</a></li>
<li><a href="../de452748/index.html">Prinzipien zur Entwicklung moderner Anwendungen aus NGINX. Teil 1</a></li>
<li><a href="../de452750/index.html">Nextcloud innerhalb und au√üerhalb von OpenLiteSpeed: Reverse Proxy konfigurieren</a></li>
<li><a href="../de452754/index.html">19% der beliebtesten Docker-Images haben kein Root-Passwort</a></li>
<li><a href="../de452756/index.html">Turmverteidigung in Einheit schaffen: Feinde</a></li>
<li><a href="../de452760/index.html">Vitamin D. Trinken oder nicht trinken, das ist die Frage. (Oder eine Geschichte dar√ºber, wie ich eine Analyse bestanden habe, die mir nicht verschrieben wurde)</a></li>
<li><a href="../de452762/index.html">MVCC-7. Automatische Reinigung</a></li>
<li><a href="../de452764/index.html">[Peter] Treffen von JUG.ru mit Sergei Melnikov - Profiling mit superluminaler Geschwindigkeit: Theorie und Praxis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>