<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÇ ‚òÇÔ∏è üöô Attention aux nuls et √† l'impl√©mentation dans Keras üïµüèæ üôáüèø üé•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√Ä propos des articles sur l'intelligence artificielle en russe 
 Malgr√© le fait que le m√©canisme Attention est d√©crit dans la litt√©rature anglaise, je...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Attention aux nuls et √† l'impl√©mentation dans Keras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2>  √Ä propos des articles sur l'intelligence artificielle en russe </h2><br>  Malgr√© le fait que le m√©canisme Attention est d√©crit dans la litt√©rature anglaise, je n'ai toujours pas vu de description d√©cente de cette technologie dans le secteur russophone.  Il existe de nombreux articles sur l'intelligence artificielle (IA) dans notre langue.  Cependant, les articles trouv√©s ne r√©v√®lent que les mod√®les d'IA les plus simples, par exemple les r√©seaux de convolution, les r√©seaux g√©n√©ratifs.  Cependant, selon les derniers d√©veloppements de pointe dans le domaine de l'IA, il y a tr√®s peu d'articles dans le secteur russophone. <br><br><a name="habracut"></a>  Le manque d'articles en russe sur les derniers d√©veloppements est devenu un probl√®me pour moi lorsque j'ai abord√© le sujet, √©tudi√© la situation actuelle dans le domaine de l'IA.  Je connais bien l'anglais, j'ai lu des articles en anglais sur des sujets li√©s √† l'IA.  Cependant, lorsqu'un nouveau concept ou un nouveau principe de l'IA sort, sa compr√©hension dans une langue √©trang√®re est douloureuse et longue.  Conna√Ætre l'anglais, p√©n√©trer dans un non-natif dans un objet complexe vaut encore beaucoup plus de temps et d'efforts.  Apr√®s avoir lu la description, vous vous posez la question: combien de pour cent comprenez-vous?  S'il y avait un article en russe, je comprendrais √† 100% apr√®s la premi√®re lecture.  Cela s'est produit avec les r√©seaux g√©n√©ratifs, pour lesquels il existe une excellente s√©rie d'articles: apr√®s la lecture, tout est devenu clair.  Mais dans le monde des r√©seaux, il existe de nombreuses approches qui ne sont d√©crites qu'en anglais et qui ont d√ª √™tre trait√©es pendant des jours. <br><br>  Je vais r√©guli√®rement √©crire des articles dans ma langue maternelle, apportant des connaissances dans notre domaine linguistique.  Comme vous le savez, la meilleure fa√ßon de comprendre un sujet est de l'expliquer √† quelqu'un.  Alors, qui d'autre que moi devrait commencer une s√©rie d'articles sur l'IA architecturale la plus moderne, la plus complexe et la plus avanc√©e.  √Ä la fin de l'article, je comprendrai moi-m√™me une approche √† 100%, et cela sera utile pour quelqu'un qui lit et am√©liore sa compr√©hension (au fait, j'aime Gesser, mais mieux ** Blanche de bruxelles **). <br><br>  Lorsque vous comprenez le sujet, il existe 4 niveaux de compr√©hension: <br><br><ol><li>  vous comprenez le principe et les entr√©es et sorties de l'algorithme / niveau </li><li>  vous comprenez les sorties de rassemblement et en termes g√©n√©raux comment cela fonctionne </li><li>  vous comprenez tout ce qui pr√©c√®de, ainsi que l'appareil de chaque niveau de r√©seau (par exemple, dans le mod√®le VAE, vous avez compris le principe, et vous avez √©galement compris l'essence de l'astuce de reparameterization) </li><li>  J'ai tout compris, y compris tous les niveaux, j'ai √©galement compris pourquoi tout cela apprenait, et en m√™me temps, je suis en mesure de s√©lectionner des hyperparam√®tres pour ma t√¢che, plut√¥t que de copier-coller des solutions toutes faites. </li></ol><br>  Pour les nouvelles architectures, le passage du niveau 1 au niveau 4 est souvent difficile: les auteurs soulignent qu'ils d√©crivent de mani√®re superficielle divers d√©tails importants (les ont-ils compris eux-m√™mes?).  Ou votre cerveau ne contient aucune construction, donc m√™me apr√®s avoir lu la description, il n'a pas d√©chiffr√© et ne s'est pas transform√© en comp√©tences.  Cela se produit si pendant vos ann√©es d'√©tudiant, vous avez dormi dans la m√™me le√ßon de matan, apr√®s une soir√©e nocturne ÔÅä o√π vous avez donn√© le bon tapis.  appareil.  Et juste ici, nous avons besoin d'articles dans notre langue maternelle qui r√©v√®lent les nuances et les subtilit√©s de chaque op√©ration. <br><br><h2>  Concept d'attention et application </h2><br>  Ce qui pr√©c√®de est un sc√©nario de niveaux de compr√©hension.  Pour analyser Attention, commen√ßons au niveau un.  Avant de d√©crire les entr√©es et les sorties, nous analyserons l'essence: sur quels concepts de base, compr√©hensible m√™me pour un enfant, ce concept est bas√©.  Dans l'article, nous utiliserons le terme anglais Attention, car sous cette forme, il s'agit √©galement d'un appel √† la fonction de biblioth√®que Keras (il n'y est pas directement impl√©ment√©, un module suppl√©mentaire est requis, mais plus sur celui ci-dessous).  Pour lire plus loin, vous devez avoir une compr√©hension des biblioth√®ques Keras et python, car le code source sera fourni. <br><br><img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right">  Attention traduit de l'anglais par ¬´attention¬ª.  Ce terme d√©crit correctement l'essence de la d√©marche: si vous √™tes automobiliste et que le g√©n√©ral de la police de la circulation est repr√©sent√© sur la photo, vous y attachez intuitivement de l'importance, quel que soit le contexte de la photo.  Vous allez probablement regarder de plus pr√®s le g√©n√©ral.  Vous tendez les yeux, regardez attentivement les bretelles: combien d'√©toiles il a l√†-bas sp√©cifiquement.  Si le g√©n√©ral n'est pas tr√®s grand, ignorez-le.  Sinon, consid√©rez-le comme un facteur cl√© dans la prise de d√©cisions.  Voil√† comment fonctionne notre cerveau.  Dans la culture russe, nous avons √©t√© form√©s par des g√©n√©rations √† pr√™ter attention aux rangs √©lev√©s, notre cerveau accorde automatiquement une haute priorit√© √† ces objets. <br><br>  L'attention est un moyen de dire au r√©seau √† quoi vous devez pr√™ter plus d'attention, c'est-√†-dire de signaler la probabilit√© d'un r√©sultat particulier en fonction de l'√©tat des neurones et des donn√©es d'entr√©e.  La couche Attention impl√©ment√©e dans Keras identifie elle-m√™me les facteurs bas√©s sur l'ensemble de formation, dont l'attention r√©duit l'erreur de r√©seau.  L'identification des facteurs importants se fait par le biais de la m√©thode de r√©tropropagation des erreurs, similaire √† celle qui est utilis√©e pour les r√©seaux de convolution. <br><br>  Dans la formation, Attention d√©montre sa nature probabiliste.  Le m√©canisme lui-m√™me forme une matrice d'√©chelles d'importance.  Si nous n'avions pas form√© l'attention, nous aurions pu d√©finir l'importance, par exemple, empiriquement (le g√©n√©ral est plus important que l'enseigne).  Mais lorsque nous formons un r√©seau sur des donn√©es, l'importance devient fonction de la probabilit√© d'un r√©sultat particulier, en fonction des donn√©es re√ßues √† l'entr√©e du r√©seau.  Par exemple, si nous rencontrions un g√©n√©ral vivant dans la Russie tsariste, la probabilit√© d'obtenir des gantelets serait √©lev√©e.  Une fois cela v√©rifi√©, il serait possible, gr√¢ce √† plusieurs r√©unions personnelles, de collecter des statistiques.  Apr√®s cela, notre cerveau mettra le poids appropri√© sur le fait de la rencontre de ce sujet et mettra des marqueurs sur les bretelles et les rayures.  Il convient de noter que le marqueur d√©fini n'est pas une probabilit√©: maintenant la r√©union du g√©n√©ral entra√Ænera des cons√©quences compl√®tement diff√©rentes pour vous qu'alors, en outre, le poids peut √™tre sup√©rieur √† un.  Mais, le poids peut √™tre r√©duit √† la probabilit√© en le normalisant. <br><br>  La nature probabiliste du m√©canisme Attention dans l'apprentissage se manifeste dans les t√¢ches de traduction automatique.  Par exemple, informons le r√©seau que lors de la traduction du russe vers l'anglais, le mot Amour est traduit dans 90% des cas par Amour, dans 9% des cas par Sexe, dans 1% des cas autrement.  Le r√©seau marque imm√©diatement de nombreuses options, montrant la meilleure qualit√© de formation.  Lors de la traduction, nous disons au r√©seau: "lors de la traduction du mot amour, portez une attention particuli√®re au mot anglais Love, voyez aussi s'il peut encore √™tre Sex." <br><br>  L'approche Attention est appliqu√©e pour travailler avec du texte, ainsi qu'avec du son et des s√©ries temporelles.  Pour le traitement de texte, les r√©seaux de neurones r√©currents (RNN, LSTM, GRU) sont largement utilis√©s.  L'attention peut soit les compl√©ter, soit les remplacer, d√©pla√ßant le r√©seau vers des architectures plus simples et plus rapides. <br><br>  L'une des applications les plus c√©l√®bres d'Attention est son utilisation pour abandonner le r√©seau de r√©currence et passer √† un mod√®le enti√®rement connect√©.  Les r√©seaux r√©currents pr√©sentent une s√©rie de lacunes: l'incapacit√© de fournir une formation sur le GPU, une reconversion rapide.  En utilisant le m√©canisme Attention, nous pouvons construire un r√©seau capable d'apprendre des s√©quences sur la base d'un r√©seau enti√®rement connect√©, le former sur le GPU, utiliser droput. <br><br>  L'attention est largement utilis√©e pour am√©liorer les performances des r√©seaux de r√©currence, par exemple dans le domaine de la traduction d'une langue √† l'autre.  Lorsque vous utilisez l'approche de codage / d√©codage, qui est souvent utilis√©e dans l'IA moderne (par exemple, les codeurs automatiques variationnels).  Lorsqu'une couche Attention est ajout√©e entre l'encodeur et le d√©codeur, le r√©sultat du fonctionnement du r√©seau s'am√©liore sensiblement. <br><br>  Dans cet article, je ne cite pas des architectures r√©seau sp√©cifiques utilisant Attention, cela fera l'objet d'un travail s√©par√©.  Une liste de toutes les utilisations possibles de l'attention m√©rite un article s√©par√©. <br><br><h2>  Impl√©mentation de l'attention dans Keras pr√™t √† l'emploi </h2><br>  Lorsque vous comprenez quel type d'approche, il est tr√®s utile d'apprendre le principe de base.  Mais souvent, la pleine compr√©hension ne vient qu'en examinant une impl√©mentation technique.  Vous voyez les flux de donn√©es qui composent la fonction de l'op√©ration, il devient clair ce qui est calcul√© exactement.  Mais vous devez d'abord l'ex√©cuter et √©crire ¬´Attention bonjour mot¬ª. <br><br>  L'attention n'est actuellement pas impl√©ment√©e dans Keras lui-m√™me.  Mais il existe d√©j√† des impl√©mentations tierces, telles que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">attention-keras,</a> qui peuvent √™tre install√©es avec github.  Ensuite, votre code deviendra extr√™mement simple: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> attention_keras.layers.attention <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AttentionLayer attn_layer = AttentionLayer(name=<span class="hljs-string"><span class="hljs-string">'attention_layer'</span></span>) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</code> </pre> <br>  Cette impl√©mentation prend en charge la fonction de visualisation de l'√©chelle d'attention.  Apr√®s avoir form√© Attention, vous pouvez obtenir une signalisation matricielle qui, selon le r√©seau, est particuli√®rement importante pour ce type (image de github de la page de la biblioth√®que attention-keras). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br>  Fondamentalement, vous n'avez besoin de rien d'autre: incluez ce code dans votre r√©seau comme l'un des niveaux et profitez de l'apprentissage de votre r√©seau.  Tout r√©seau, tout algorithme est con√ßu dans les premi√®res √©tapes √† un niveau conceptuel (comme la base de donn√©es, soit dit en passant), apr√®s quoi l'impl√©mentation est sp√©cifi√©e dans une repr√©sentation logique et physique avant l'impl√©mentation.  Cette m√©thode de conception n'a pas encore √©t√© d√©velopp√©e pour les r√©seaux de neurones (oh oui, ce sera le sujet de mon prochain article).  Vous ne comprenez pas comment les couches de convolution fonctionnent √† l'int√©rieur?  Le principe est d√©crit, vous les utilisez. <br><br><h2>  Impl√©mentation Keras de Attention low </h2><br>  Pour enfin comprendre le sujet, nous analyserons ci-dessous en d√©tail la mise en ≈ìuvre d'Attention sous le capot.  Le concept est bon, mais comment fonctionne-t-il exactement et pourquoi le r√©sultat est-il obtenu exactement comme indiqu√©? <br><br>  L'impl√©mentation la plus simple du m√©canisme Attention dans Keras ne prend que 3 lignes: <br><br><pre> <code class="python hljs">inputs = Input(shape=(input_dims,)) attention_probs = Dense(input_dims, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_probs'</span></span>)(inputs) attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number"><span class="hljs-number">32</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_mul'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'mul'</span></span></code> </pre><br>  Dans ce cas, la couche Input est d√©clar√©e dans la premi√®re ligne, puis vient une couche enti√®rement connect√©e avec la fonction d'activation softmax avec un nombre de neurones √©gal au nombre d'√©l√©ments dans la premi√®re couche.  La troisi√®me couche multiplie le r√©sultat de la couche enti√®rement connect√©e par les donn√©es d'entr√©e √©l√©ment par √©l√©ment. <br><br>  Vous trouverez ci-dessous toute la classe Attention, qui impl√©mente un m√©canisme d'auto-attention l√©g√®rement plus complexe, qui peut √™tre utilis√© comme un niveau √† part enti√®re dans le mod√®le; la classe h√©rite de la classe de couche Keras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Attention class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim</span></span></code> </pre><br>  Ici, nous voyons √† peu pr√®s la m√™me chose qui a √©t√© impl√©ment√©e ci-dessus via la couche Keras enti√®rement connect√©e, uniquement ex√©cut√©e via une logique plus profonde √† un niveau inf√©rieur.  Un niveau param√©trique (self.W) est cr√©√© dans la fonction, qui est ensuite multipli√© scalairement (K.dot) par le vecteur d'entr√©e.  La logique c√¢bl√©e dans cette variante est un peu plus compliqu√©e: d√©calage (si le param√®tre de biais est divulgu√©), tangente hyperbolique, exposition, masque (si sp√©cifi√©), normalisation sont appliqu√©es au vecteur d'entr√©e fois self.W, puis le vecteur d'entr√©e est √† nouveau pond√©r√© par le r√©sultat obtenu.  Je n'ai pas de description de la logique pos√©e dans cet exemple, je reproduis les op√©rations de lecture du code.  Au fait, veuillez √©crire dans les commentaires si vous reconnaissez une sorte de fonction math√©matique de haut niveau dans cette logique. <br><br>  La classe a un param√®tre "biais" ie  biais.  Si le param√®tre est activ√©, apr√®s avoir appliqu√© la couche Dense, le vecteur final sera ajout√© au vecteur des param√®tres de la couche ¬´self.b¬ª, ce qui permettra non seulement de d√©terminer les ¬´poids¬ª pour notre fonction d'attention, mais aussi de d√©caler le niveau d'attention d'un nombre.  Exemple de vie: nous avons peur des fant√¥mes, mais nous ne les avons jamais rencontr√©s.  Ainsi, nous faisons une correction pour la peur de -100 points.  Autrement dit, seulement si la peur monte en puissance pour 100 points, nous prendrons des d√©cisions sur la protection contre les fant√¥mes, en appelant une agence anti-fant√¥mes, en achetant des appareils r√©pulsifs, etc. <br><br><h2>  Conclusion </h2><br>  Le m√©canisme Attention a des variantes.  L'option Attention la plus simple impl√©ment√©e dans la classe ci-dessus est appel√©e Self-Attention.  L'auto-attention est un m√©canisme con√ßu pour traiter des donn√©es s√©quentielles, en tenant compte du contexte de chaque horodatage.  Il est le plus souvent utilis√© pour travailler avec des informations textuelles.  L'impl√©mentation de l'auto-attention peut √™tre retir√©e de la bo√Æte en important la biblioth√®que keras-self-attention.  Il existe d'autres variantes d'attention.  En √©tudiant des mat√©riaux en anglais, il a √©t√© possible de compter plus de 5 variantes. <br><br>  En √©crivant m√™me cet article relativement court, j'ai √©tudi√© plus de 10 articles en anglais.  Bien s√ªr, je n'ai pas pu t√©l√©charger toutes les donn√©es de tous ces articles en 5 pages, je viens de faire une compression afin de cr√©er un ¬´guide pour les nuls¬ª.  Pour comprendre toutes les nuances du m√©canisme Attention, vous avez besoin d'un livre de pages 150-200.  J'esp√®re vraiment que j'ai pu r√©v√©ler l'essence de base de ce m√©canisme afin que ceux qui commencent tout juste √† comprendre l'apprentissage automatique comprennent comment tout cela fonctionne. <br><br><h2>  Les sources </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">M√©canisme d'attention dans les r√©seaux neuronaux avec Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Attention dans les r√©seaux profonds avec Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">S√©quence √† s√©quence bas√©e sur l'attention dans Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Classification de texte √† l'aide du m√©canisme d'attention dans Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Attention omnipr√©sente: r√©seaux de neurones convolutifs 2D pour la pr√©diction de s√©quence √† s√©quence</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment impl√©menter la couche Attention dans Keras?</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Attention?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Attention!</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Traduction automatique de neurones avec attention</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr458992/">https://habr.com/ru/post/fr458992/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr458982/index.html">Recettes Nginx: Conversion de HTML et d'URL en PDF et PS</a></li>
<li><a href="../fr458984/index.html">Comment cr√©er la premi√®re application de trading sur la bourse: 3 √©tapes initiales</a></li>
<li><a href="../fr458986/index.html">Recettes PostgreSQL: Conversion de HTML et d'URL en PDF et PS</a></li>
<li><a href="../fr458988/index.html">Texturation, ou ce que vous devez savoir pour devenir un artiste de surface. Partie 4. Mod√®les, normales et balayage</a></li>
<li><a href="../fr458990/index.html">Arr√™tez de z√®le avec des commentaires dans le code</a></li>
<li><a href="../fr458994/index.html">Raspberry Pi + CentOS = point d'acc√®s Wi-Fi (ou routeur Raspberry dans un chapeau rouge)</a></li>
<li><a href="../fr458996/index.html">User Inyerface - comment ne pas tourmenter l'utilisateur</a></li>
<li><a href="../fr459000/index.html">Comment j'ai essay√© d'am√©liorer Halo 2, mais je l'ai presque ruin√©</a></li>
<li><a href="../fr459002/index.html">Comment configurer HTTPS - Le g√©n√©rateur de configuration SSL vous aidera</a></li>
<li><a href="../fr459004/index.html">Algorithme cryptographique Grasshopper: √† peu pr√®s le complexe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>