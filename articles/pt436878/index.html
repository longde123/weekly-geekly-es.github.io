<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì≤ ü•ß üöµüèæ O BERT √© um modelo de linguagem de ponta para 104 idiomas. Tutorial para iniciar o BERT localmente e no Google Colab üë®üèª‚Äçüî¨ üõãÔ∏è üßëüèø‚Äçü§ù‚Äçüßëüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O BERT √© uma rede neural do Google, que mostrou por uma grande margem os resultados mais recentes em v√°rias tarefas. Usando o BERT, voc√™ pode criar pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O BERT √© um modelo de linguagem de ponta para 104 idiomas. Tutorial para iniciar o BERT localmente e no Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="imagem"></p><br><p>  O BERT √© uma rede neural do Google, que mostrou por uma grande margem os resultados mais recentes em v√°rias tarefas.  Usando o BERT, voc√™ pode criar programas de IA para processar um idioma natural: responder a perguntas de qualquer forma, criar bots de bate-papo, tradutores autom√°ticos, analisar texto e assim por diante. </p><br><p>  O Google publicou modelos pr√©-treinados de BERT, mas, como geralmente acontece com o Machine Learning, eles sofrem com a falta de documenta√ß√£o.  Portanto, neste tutorial, aprenderemos como executar a rede neural BERT no computador local, bem como na GPU de servidor livre no Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Por que √© necess√°rio? </h2><br><p>  Para enviar texto para a entrada de uma rede neural, voc√™ precisa de alguma forma apresent√°-lo na forma de n√∫meros.  √â mais f√°cil fazer essa letra por letra, aplicando uma letra a cada entrada da rede neural.  Cada letra ser√° codificada com um n√∫mero de 0 a 32 (mais algum tipo de margem para sinais de pontua√ß√£o).  Este √© o chamado n√≠vel de caractere. </p><br><p>  Por√©m, resultados muito melhores s√£o obtidos se apresentarmos propostas n√£o por uma letra, mas submetendo a cada entrada da rede neural imediatamente uma palavra inteira (ou pelo menos s√≠labas).  J√° ser√° um n√≠vel de palavra.  A op√ß√£o mais f√°cil √© compilar um dicion√°rio com todas as palavras existentes e alimentar a rede com o n√∫mero de palavras neste dicion√°rio.  Por exemplo, se a palavra "cachorro" estiver neste dicion√°rio em 1678, digite o n√∫mero 1678 para a entrada da rede neural para essa palavra. </p><br><p>  Mas apenas em uma linguagem natural, com a palavra "cachorro", muitas associa√ß√µes surgem ao mesmo tempo em uma pessoa: "fofo", "mal", "amigo de uma pessoa".  √â poss√≠vel codificar de alguma maneira esse recurso de nosso pensamento na apresenta√ß√£o para a rede neural?  Acontece que voc√™ pode.  Para fazer isso, basta reordenar os n√∫meros das palavras para que as palavras com significado pr√≥ximo fiquem lado a lado.  Que seja, por exemplo, para "cachorro" o n√∫mero 1678 e para a palavra "fofo" o n√∫mero 1680. E para a palavra "bule" o n√∫mero √© 9000. Como voc√™ pode ver, os n√∫meros 1678 e 1680 est√£o muito mais pr√≥ximos um do outro do que o n√∫mero 9000. </p><br><p>  Na pr√°tica, a cada palavra √© atribu√≠do n√£o um n√∫mero, mas v√°rias - um vetor, digamos, de 32 n√∫meros.  E as dist√¢ncias s√£o medidas como as dist√¢ncias entre os pontos que esses vetores apontam no espa√ßo da dimens√£o correspondente (para um vetor com 32 d√≠gitos de comprimento, este √© um espa√ßo com 32 dimens√µes ou com 32 eixos).  Isso permite comparar uma palavra de uma vez com v√°rias que t√™m significado pr√≥ximo (dependendo de qual eixo contar).  Al√©m disso, opera√ß√µes aritm√©ticas podem ser realizadas com vetores.  Um exemplo cl√°ssico: se voc√™ subtrair o vetor "homem" do vetor que denota a palavra "rei" e adicionar o vetor para a palavra "mulher", obter√° um determinado vetor de resultado.  E ele corresponder√° milagrosamente √† palavra "rainha".  E, de fato, "rei √© homem + mulher = rainha".  A magia!  E este n√£o √© um exemplo abstrato, mas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">realmente acontece</a> .  Considerando que as redes neurais est√£o bem adaptadas para transforma√ß√µes matem√°ticas sobre suas entradas, isso aparentemente fornece uma efici√™ncia t√£o alta desse m√©todo. </p><br><p>  Essa abordagem √© chamada de casamentos.  Todos os pacotes de aprendizado de m√°quina (TensorFlow, PyTorch) permitem que a primeira camada da rede neural coloque uma camada especial de Camada de incorpora√ß√£o, que faz isso automaticamente.  Ou seja, na entrada da rede neural, fornecemos o n√∫mero habitual de palavras no dicion√°rio, e a Camada de incorpora√ß√£o, auto-aprendizagem, traduz cada palavra em um vetor do comprimento especificado, digamos, 32 n√∫meros. </p><br><p>  Mas eles rapidamente perceberam que √© muito mais lucrativo pr√©-treinar essa representa√ß√£o vetorial de palavras em um imenso corpus de textos, por exemplo, em toda a Wikipedia, e usar vetores prontos em redes neurais espec√≠ficas, em vez de trein√°-los novamente. </p><br><p>  Existem v√°rias maneiras de representar palavras como vetores: elas evolu√≠ram gradualmente: word2vec, GloVe, Elmo. </p><br><p>  No ver√£o de 2018, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OpenAI notou</a> que, se voc√™ treina uma rede neural na arquitetura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Transformer</a> em grandes volumes de texto, ela inesperadamente e por uma grande margem mostra excelentes resultados em muitos tipos diferentes de tarefas de processamento de linguagem natural.  De fato, uma rede neural desse tipo cria representa√ß√µes vetoriais para palavras e at√© frases inteiras.  E, apoiando-se nesse modelo de linguagem em um pequeno bloco de um par de camadas adicionais de neur√¥nios, voc√™ pode treinar essa rede neural para qualquer tarefa. </p><br><p>  O BERT do Google √© uma rede GPA avan√ßada da OpenAI (bidirecional em vez de unidirecional etc.), tamb√©m baseada na arquitetura Transformer.  No momento, o BERT √© o estado da arte em quase todos os benchmarks populares da PNL. </p><br><h2 id="kak-oni-eto-sdelali">  Como eles fizeram isso </h2><br><p>  A id√©ia por tr√°s do BERT √© muito simples: vamos alimentar a rede neural com frases nas quais substitu√≠mos 15% das palavras por [MASK] e treinar a rede neural para prever essas palavras mascaradas. </p><br><p>  Por exemplo, se enviarmos a frase ‚Äúeu vim para [MASK] e comprei [MASK]‚Äù ‚Äùpara a entrada da rede neural, ela dever√° mostrar as palavras‚Äú store ‚Äùe‚Äú milk ‚Äùna sa√≠da.  Este √© um exemplo simplificado da p√°gina oficial do BERT; em senten√ßas mais longas, o leque de op√ß√µes poss√≠veis se torna menor e a resposta da rede neural √© inequ√≠voca. </p><br><p>  E, para que a rede neural aprenda a entender as rela√ß√µes entre diferentes frases, vamos trein√°-la adicionalmente para prever se a segunda frase √© uma continua√ß√£o l√≥gica da primeira.  Ou √© alguma frase aleat√≥ria que n√£o tem nada a ver com a primeira. </p><br><p>  Ent√£o, por duas frases: "Eu fui √† loja".  e ‚ÄúE comprei leite l√°.‚Äù, a rede neural deve responder que isso √© l√≥gico.  E se a segunda frase √© "c√©u cruciano Plut√£o", devo responder que esta proposta n√£o tem nada a ver com a primeira.  Vamos brincar com os dois modos BERT abaixo. </p><br><p>  Assim, treinando a rede neural no corpus de textos da Wikipedia e na cole√ß√£o de livros BookCorpus por 4 dias √†s 16 TPU, obtivemos o BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Instala√ß√£o e configura√ß√£o </h2><br><p>  <em><strong>Nota</strong> : nesta se√ß√£o, iniciaremos e jogaremos com o BERT no computador local.</em>  <em>Para executar esta rede neural em uma GPU local, voc√™ precisar√° de um NVidia GTX 970 com 4 GB de mem√≥ria de v√≠deo ou superior.</em>  <em>Se voc√™ deseja executar o BERT em um navegador (voc√™ nem precisa de uma GPU no seu computador para isso), acesse a se√ß√£o Google Colab.</em> </p><br><p>  Primeiro instale o TensorFlow, se voc√™ ainda n√£o o possui, seguindo as instru√ß√µes em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.tensorflow.org/install</a> .  Para oferecer suporte √† GPU, voc√™ deve primeiro instalar o CUDA Toolkit 9.0, depois o cuDNN SDK 7.2 e somente o TensorFlow com suporte √† GPU: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Basicamente, isso √© suficiente para executar o BERT.  Mas n√£o h√° instru√ß√µes, como tal, voc√™ pode compor por si mesmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">classificando</a> as fontes no arquivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">run_classifier.py</a> (a situa√ß√£o usual no Machine Learning √© quando voc√™ precisa acessar as fontes em vez da documenta√ß√£o).  Mas vamos facilitar e usar o shell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Keras BERT</a> (tamb√©m pode ser √∫til para ajustar a rede posteriormente, porque fornece uma interface Keras conveniente). </p><br><p>  Para fazer isso, instale o pr√≥prio Keras: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  E depois de Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Tamb√©m precisaremos do arquivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tokenization.py</a> do github BERT original.  Clique no bot√£o Raw e salve-o na pasta com o script futuro, ou fa√ßa o download de todo o reposit√≥rio e leve o arquivo de l√° ou fa√ßa uma c√≥pia do reposit√≥rio com este c√≥digo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Agora √© hora de baixar a rede neural pr√©-treinada.  Existem v√°rias op√ß√µes para o BERT, todas listadas na p√°gina oficial do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">github.com/google-research/bert</a> .  Tomaremos o "BERT-Base, Multilingual Multil√≠ng√ºe" multil√≠ngue universal, para 104 idiomas.  Fa√ßa o download do arquivo <a href="">multi_cased_L-12_H-768_A-12.zip</a> (632 Mb) e descompacte-o na pasta com o script futuro. </p><br><p>  Est√° tudo pronto, crie o arquivo BERT.py e haver√° um pouco de c√≥digo. </p><br><p>  Importar bibliotecas necess√°rias e definir caminhos </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Como teremos que traduzir linhas de texto comuns em um formato especial de tokens, criaremos um objeto especial para isso.  Preste aten√ß√£o em do_lower_case = False, pois estamos usando o modelo Cased BERT, que diferencia mai√∫sculas de min√∫sculas. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Modelo de carregamento </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  O BERT pode funcionar de dois modos: adivinhar as palavras perdidas na frase ou adivinhar se a segunda frase √© l√≥gica depois da primeira.  Faremos as duas op√ß√µes. </p><br><p>  Para o primeiro modo, voc√™ precisa enviar uma frase no formato: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  A rede neural deve retornar uma frase completa com as palavras preenchidas no lugar das m√°scaras: "Vim √† loja e comprei leite". </p><br><p>  Para o segundo modo, ambas as frases separadas por um separador devem ser alimentadas √† entrada da rede neural: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  A rede neural deve responder se a segunda frase √© uma continua√ß√£o l√≥gica da primeira.  Ou √© uma frase aleat√≥ria que n√£o tem nada a ver com a primeira. </p><br><p>  Para que o BERT funcione, voc√™ precisa preparar tr√™s vetores, cada um com um comprimento de 512 n√∫meros: token_input, seg_input e mask_input. </p><br><p>  <strong>Token_input</strong> armazenar√° nosso c√≥digo fonte traduzido em tokens usando o tokenizer.  A frase na forma de √≠ndices no dicion√°rio estar√° no in√≠cio desse vetor e o restante ser√° preenchido com zeros. </p><br><p>  Em <strong>mask_input,</strong> devemos colocar 1 para todas as posi√ß√µes em que a m√°scara [MASK] est√° e preencher o restante com zeros. </p><br><p>  Em <strong>seg_input,</strong> devemos <strong>indicar a</strong> primeira frase (incluindo o CLS inicial e o separador SEP) como 0, a segunda frase (incluindo o SEP final) como 1 e preencher o restante at√© o final do vetor com zeros. </p><br><p>  O BERT n√£o usa um dicion√°rio de palavras inteiras, mas as s√≠labas mais comuns.  Embora tamb√©m tenha palavras inteiras.  Voc√™ pode abrir o arquivo vocab.txt na rede neural baixada e ver quais palavras a rede neural usa na entrada.  H√° palavras inteiras como a Fran√ßa.  Mas a maioria das palavras em russo precisa ser dividida em s√≠labas.  Portanto, a palavra "veio" deve ser dividida em "com" e "## foi".  Para ajudar na convers√£o de linhas regulares de texto no formato exigido pelo BERT, usamos o m√≥dulo tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Modo 1: Previs√£o de palavras fechadas por token [MASK] em uma frase </h2><br><p>  A frase de entrada que √© alimentada na entrada da rede neural </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Converta-o em tokens.  O problema √© que o tokenizer n√£o pode processar marcas de servi√ßo como [CLS] e [MASK], embora o vocab.txt as possua no dicion√°rio.  Portanto, teremos que quebrar manualmente nossa linha com marcadores [MASK] e extrair dele partes de texto sem formata√ß√£o para convert√™-lo em tokens BERT usando o tokenizador.  Adicione tamb√©m [CLS] no in√≠cio e [SEP] no final da frase. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Os tokens agora t√™m tokens que s√£o garantidos para serem convertidos em √≠ndices no dicion√°rio.  Vamos fazer isso: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Agora, em token_input, h√° uma s√©rie de n√∫meros (n√∫meros de palavras no dicion√°rio vocab.txt) que precisam ser alimentados na entrada da rede neural.  Resta apenas estender esse vetor a um comprimento de 512 elementos.  A constru√ß√£o Python [0] * length cria uma matriz de comprimento de comprimento, preenchida com zeros.  Basta adicion√°-lo aos nossos tokens, que em python combinam duas matrizes em uma. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Agora crie uma m√°scara de 512 cm de comprimento, colocando 1 em todos os lugares, onde o n√∫mero 103 aparece nos tokens (que corresponde ao marcador [MASK] no dicion√°rio vocab.txt) e preencha o restante com 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Para o primeiro modo de opera√ß√£o BERT, seg_input deve ser completamente preenchido com zeros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  A √∫ltima etapa, voc√™ precisa converter matrizes python em matrizes numpy com forma (1.512), para as quais as colocamos em uma sub-matriz []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  OK, pronto.  Agora execute a previs√£o da rede neural! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Agora formate o resultado dos tokens de volta para uma sequ√™ncia separada por espa√ßos </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  E produza o resultado: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  No nosso exemplo, para a frase "eu vim para [MASK] e comprei [MASK]".  a rede neural produziu o resultado "house" e "it": "vim para a casa e comprei".  Bem, n√£o √© t√£o ruim pela primeira vez.  Comprar uma casa √© definitivamente melhor que o leite). </p><br><div class="spoiler">  <b class="spoiler_title">Outros exemplos (n√£o dou exemplos malsucedidos, h√° muito mais que bem-sucedidos. Na maioria dos casos, a rede d√° uma resposta vazia):</b> <div class="spoiler_text"><p>  A Terra √© a terceira [M√ÅSCARA] do Sol <br>  Resultado: Estrela </p><br><p>  melhor sandu√≠che [M√ÅSCARA] com manteiga <br>  Resultado: Atende </p><br><p>  depois do almo√ßo [MASK] deve dormir <br>  Resultado: deste </p><br><p>  afaste-se de [MASK] <br>  Resultado: ## oh - isso √© algum tipo de maldi√ß√£o?  ) </p><br><p>  [MASK] da porta <br>  Resultado: visualizar </p><br><p>  Com [MASK] martelo e pregos pode fazer gabinete <br>  Resultado: ajuda </p><br><p>  E se amanh√£ n√£o for?  Hoje, por exemplo, n√£o √© [M√ÅSCARA]! <br>  Resultado: ser√° </p><br><p>  Como voc√™ se cansa de ignorar o [MASK]? <br>  Resultado: ela </p><br><p>  H√° l√≥gica cotidiana, h√° l√≥gica feminina, mas nada se sabe sobre o homem [M√ÅSCARA] <br>  Resultado: Filosofia </p><br><p>  Nas mulheres, aos trinta anos, forma-se uma imagem do pr√≠ncipe, que se encaixa em qualquer [M√ÅSCARA]. <br>  Resultado: homem </p><br><p>  Por maioria de votos, Branca de Neve e os sete an√µes votaram em [MASK], com um voto contra. <br>  Resultado: vila - a primeira letra est√° correta </p><br><p>  Classifique sua t√©dio em uma escala de 10 pontos: [MASK] points <br>  Resultado: 10 </p><br><p>  Suas [M√ÅSCARA], [M√ÅSCARA] e [M√ÅSCARA]! <br>  Resultado: me ame eu - n√£o, BERT, eu n√£o quis dizer nada </p></div></div><br><p>  Voc√™ pode inserir frases em ingl√™s (e qualquer em 104 idiomas, cuja lista <a href="">est√° aqui</a> ) </p><br><p>  [MASK] deve continuar! <br>  Resultado: I </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Modo 2: verificando a consist√™ncia de duas frases </h2><br><p>  Definimos duas frases consecutivas que ser√£o alimentadas na entrada da rede neural </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Criaremos tokens no formato [CLS] frase_1 [SEP] frase_2 [SEP], convertendo texto sem formata√ß√£o em tokens usando o tokenizer: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Convertemos tokens de string em √≠ndices num√©ricos (n√∫meros de palavras no dicion√°rio vocab.txt) e estendemos o vetor para 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  A palavra m√°scara neste caso √© completamente preenchida com zeros </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Mas a m√°scara da proposta deve ser preenchida sob a segunda frase (incluindo o SEP final) com unidades e tudo mais com zeros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Passamos as frases pela rede neural (desta vez o resultado est√° em [1], e n√£o em [0], como estava acima) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  E derivamos a probabilidade de que a segunda frase seja um conjunto de palavras normal, e n√£o aleat√≥rio </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  Em duas frases: </p><br><p>  Eu vim para a loja.  -&gt; E comprei leite. </p><br><p>  Resposta da rede neural: </p><br><p>  A frase est√° ok: 99% </p><br><p>  E se a segunda frase for "c√©u cruciano Plut√£o", a resposta ser√°: </p><br><p>  A senten√ßa √© razo√°vel: 4% </p><br><h2 id="google-colab">  Colab do Google </h2><br><p>  O Google fornece uma GPU gratuita para servidor Tesla K80 com 12 Gb de mem√≥ria de v√≠deo (as TPUs est√£o agora dispon√≠veis, mas sua configura√ß√£o √© um pouco mais complicada).  Todo o c√≥digo do Colab deve ser projetado como um notebook jupyter.  Para iniciar o BERT em um navegador, basta abrir o link </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  No menu <strong>Tempo de Execu√ß√£o</strong> , selecione <strong>Executar Tudo</strong> , para que pela primeira vez todas as c√©lulas sejam iniciadas, o modelo √© baixado e as bibliotecas necess√°rias est√£o conectadas.  Concorde em redefinir todo o tempo de execu√ß√£o, se necess√°rio. </p><br><div class="spoiler">  <b class="spoiler_title">Se algo desse errado ...</b> <div class="spoiler_text"><p>  Verifique se GPU e Python 3 est√£o selecionados no menu Tempo de execu√ß√£o -&gt; Alterar tipo de tempo de execu√ß√£o </p><br><p>  Se o bot√£o de conex√£o n√£o estiver ativo, clique nele para se conectar. </p></div></div><br><p>  Agora altere a <strong>senten√ßa das</strong> linhas de entrada, <strong>senten√ßa_1</strong> e <strong>senten√ßa_2</strong> , e clique no √≠cone Reproduzir √† esquerda para iniciar apenas a c√©lula atual.  A execu√ß√£o de todo o notebook n√£o √© mais necess√°ria. </p><br><p>  Voc√™ pode executar o BERT no Google Colab mesmo a partir de um smartphone, mas se ele n√£o abrir, pode ser necess√°rio ativar a caixa de sele√ß√£o Vers√£o completa nas configura√ß√µes do navegador. </p><br><h2 id="chto-dalshe">  O que vem a seguir? </h2><br><p>  Para treinar o BERT para uma tarefa espec√≠fica, voc√™ precisa adicionar uma ou duas camadas de uma rede Feed Forward simples sobre ela e trein√°-la apenas sem tocar na rede principal do BERT.  Isso pode ser feito no TensorFlow ou no shell Keras BERT.  Esse treinamento adicional para um dom√≠nio espec√≠fico ocorre muito rapidamente e √© completamente semelhante ao Ajuste fino em redes de convolu√ß√£o.  Portanto, para a tarefa SQuAD, voc√™ pode treinar uma rede neural em uma TPU em apenas 30 minutos (em compara√ß√£o com 4 dias na 16 TPU para treinar o pr√≥prio BERT). </p><br><p>  Para fazer isso, voc√™ ter√° que estudar como as √∫ltimas camadas s√£o representadas no BERT, al√©m de ter um conjunto de dados adequado.  Na p√°gina oficial do BERT <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/google-research/bert,</a> existem v√°rios exemplos de tarefas diferentes, bem como instru√ß√µes sobre como come√ßar a reciclagem nas TPUs na nuvem.  E tudo o resto ter√° que procurar na fonte nos arquivos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">run_classifier.py</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  O bloco de anota√ß√µes de c√≥digo e jupiter do Google Colab apresentado aqui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>est√° hospedado no reposit√≥rio</strong></a> . </p><br><p>  Milagres n√£o devem ser esperados.  N√£o espere que o BERT fale como uma pessoa.  O status do estado da arte n√£o significa que o progresso na PNL tenha atingido um n√≠vel aceit√°vel.  Significa apenas que o BERT √© melhor que os modelos anteriores, que foram ainda piores.  A forte IA conversacional ainda est√° muito distante.  Al√©m disso, o BERT √© principalmente um modelo de linguagem, n√£o um bot de bate-papo pronto, por isso mostra bons resultados somente ap√≥s a reciclagem para uma tarefa espec√≠fica. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt436878/">https://habr.com/ru/post/pt436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt436866/index.html">Como reconhecer projetos Agile falsos</a></li>
<li><a href="../pt436868/index.html">Incorpore a an√°lise est√°tica ao processo, n√£o procure bugs com ele</a></li>
<li><a href="../pt436872/index.html">PGConf.Russia 2019 em breve</a></li>
<li><a href="../pt436874/index.html">Dan√ßas de ano novo em torno do adaptador FC ou uma hist√≥ria sobre qu√£o longe as causas do problema est√£o dos sintomas</a></li>
<li><a href="../pt436876/index.html">[SAP] SAPUI5 para manequins parte 1: um exerc√≠cio completo passo a passo</a></li>
<li><a href="../pt436880/index.html">No√ß√µes b√°sicas de modelo C ++: modelos de fun√ß√£o</a></li>
<li><a href="../pt436884/index.html">N√≥s dominamos async / wait em um exemplo real</a></li>
<li><a href="../pt436886/index.html">Usando Babel e Webpack para configurar um projeto React do zero</a></li>
<li><a href="../pt436888/index.html">Hist√≥ria sobre como criar uma API</a></li>
<li><a href="../pt436890/index.html">Tutorial Reagir Parte 10: Workshop sobre como trabalhar com propriedades e estilo de componentes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>