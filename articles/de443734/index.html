<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>☹️ 👼 🕺 Neuronale Netze haben eine erstaunlich einfache Bildklassifizierungsstrategie. 🤟🏼 🧖🏾 🕴🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Faltungs-Neuronale Netze klassifizieren verzerrte Bilder im Gegensatz zu Menschen hervorragend. 


 In diesem Artikel werde ich zeigen, warum fortgesc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze haben eine erstaunlich einfache Bildklassifizierungsstrategie.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443734/"><h3>  Faltungs-Neuronale Netze klassifizieren verzerrte Bilder im Gegensatz zu Menschen hervorragend. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/2e6/a7c/4c8/2e6a7c4c8f0ba811e57fa24193375289.jpg"><br><br>  In diesem Artikel werde ich zeigen, warum fortgeschrittene tiefe neuronale Netze verzerrte Bilder perfekt erkennen können und wie dies dazu beiträgt, die überraschend einfache Strategie aufzuzeigen, die neuronale Netze zur Klassifizierung natürlicher Fotografien verwenden.  Diese im ICLR 2019 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">veröffentlichten</a> Entdeckungen haben viele Konsequenzen: Erstens zeigen sie, dass es viel einfacher ist, eine „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ImageNet</a> “ -Lösung zu finden, als gedacht.  Zweitens helfen sie uns, interpretierbarere und verständlichere Bildklassifizierungssysteme zu schaffen.  Drittens erklären sie verschiedene Phänomene, die in modernen Faltungs-Neuronalen Netzen (SNA) beobachtet werden, beispielsweise ihre Tendenz, nach Texturen zu suchen (siehe unsere andere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> in ICLR 2019 und den entsprechenden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Blogeintrag</a> ) und die räumliche Anordnung von Teilen des Objekts zu ignorieren. <br><a name="habracut"></a><br><h2>  Gute alte Modelle "Wortsack" </h2><br>  In den guten alten Zeiten, vor dem Aufkommen des tiefen Lernens, war das Erkennen natürlicher Bilder recht einfach: Wir definieren eine Reihe wichtiger visueller Merkmale („Wörter“), bestimmen, wie oft jedes visuelle Merkmal in einem Bild („Tasche“) vorkommt, und klassifizieren das Bild anhand dieser Zahlen.  Daher werden solche Modelle in der Bildverarbeitung als "Wortsack" (Wortsack oder BoW) bezeichnet.  Angenommen, wir haben zwei visuelle Merkmale, das menschliche Auge und den Stift, und wir möchten die Bilder in zwei Klassen einteilen: "Menschen" und "Vögel".  Das einfachste BoW-Modell wäre das Folgende: Für jedes im Bild gefundene Auge erhöhen wir das Zeugnis zugunsten der „Person“ um 1. Und umgekehrt erhöhen wir für jeden Stift das Zeugnis zugunsten des „Vogels“ um 1. Welche Klasse mehr Beweise erhält, das wird es sein. <br><br>  Eine bequeme Eigenschaft eines solch einfachen BoW-Modells ist die Interpretierbarkeit und Klarheit des Entscheidungsprozesses: Wir können genau prüfen, welche besonderen Merkmale des Bildes für eine bestimmte Klasse sprechen, die räumliche Integration von Merkmalen ist sehr einfach (im Vergleich zur nichtlinearen Integration von Merkmalen in tiefen neuronalen Netzen) Verstehe einfach, wie das Modell seine Entscheidungen trifft. <br><br>  Traditionelle BoW-Modelle waren äußerst beliebt und funktionierten vor dem Einbruch des Deep Learning hervorragend, gerieten jedoch aufgrund der relativ geringen Effizienz schnell aus der Mode.  Aber sind wir sicher, dass die neuronalen Netze eine grundlegend andere Entscheidungsstrategie als die BoW verwenden? <br><br><h2>  Tiefeninterpretiertes Netzwerk mit Taschenfunktionen (BagNet) </h2><br>  Um diese Annahme zu testen, kombinieren wir die Interpretierbarkeit und Klarheit von BoW-Modellen mit der Effizienz neuronaler Netze.  Die Strategie sieht folgendermaßen aus: <br><ul><li>  Teilen Sie das Bild in kleine Stücke qx q. </li><li>  Wir leiten die Teile durch das neuronale Netzwerk, um für jedes Teil einen Nachweis der Klassenzugehörigkeit (Logs) zu erhalten. </li><li>  Fassen Sie die Beweise in allen Teilen zusammen, um eine Lösung auf der Ebene des gesamten Bildes zu erhalten. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/474/6f5/363/4746f53632bd9e3e7477de9ecb76d396.png"><br><br>  Um diese Strategie auf einfachste Weise umzusetzen, verwenden wir die Standard-ResNet-50-Architektur und ersetzen fast alle 3x3-Faltungen durch 1x1-Faltungen.  Infolgedessen "sieht" jedes verborgene Element in der letzten Faltungsschicht nur einen kleinen Teil des Bildes (dh sein Wahrnehmungsfeld ist viel kleiner als die Bildgröße).  So vermeiden wir das auferlegte Markup des Bildes und kommen dem Standard-SNA so nahe wie möglich, während wir eine vorgeplante Strategie anwenden.  Wir nennen die resultierende Architektur BagNet-q, wobei q die Größe des Wahrnehmungsfeldes der obersten Schicht bezeichnet (wir haben das Modell mit q = 9, 17 und 33 getestet).  BagNet-q läuft etwa 2,5 länger als das ResNet-50. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b5e/4b6/1c4/b5e4b61c431e103ac9a6ec3fe4b02894.jpg"><br><br>  Die Leistung von BagNet bei Daten aus der ImageNet-Datenbank ist selbst bei Verwendung kleiner Teile beeindruckend: Fragmente mit 17 x 17 Pixel reichen aus, um die Effizienz des AlexNet-Levels zu erreichen, und Fragmente mit 33 x 33 Pixel reichen aus, um eine Genauigkeit von 87% zu erreichen und in die Top 5 zu gelangen.  Sie können die Effizienz steigern, indem Sie die 3x3-Pakete sorgfältiger platzieren und die Hyperparameter anpassen. <br><br>  Dies ist unser erstes großes Ergebnis: ImageNet kann nur mit einer Reihe kleiner Bildfunktionen gelöst werden.  Die entfernten räumlichen Beziehungen der Teile der Komposition, wie die Form von Objekten oder die Interaktion zwischen Teilen des Objekts, können vollständig ignoriert werden.  Sie werden absolut nicht benötigt, um das Problem zu lösen. <br><br>  Ein bemerkenswertes Merkmal von BagNet'ov ist die Transparenz ihres Entscheidungssystems.  Sie können beispielsweise herausfinden, welche Merkmale der Bilder für eine bestimmte Klasse am charakteristischsten sind.  Zum Beispiel wird Schleie, ein großer Fisch, normalerweise durch das Bild von Fingern auf einem grünen Hintergrund erkannt.  Warum?  Denn auf den meisten Fotos in dieser Kategorie gibt es einen Fischer, der eine Schleie als Trophäe hält.  Und wenn BagNet das Bild fälschlicherweise als Linie erkennt, geschieht dies normalerweise, weil sich irgendwo auf dem Foto Finger auf einem grünen Hintergrund befinden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/175/197/727/175197727cc15da939117f5c2502d82c.jpg"><br>  <i>Die charakteristischsten Teile von Bildern.</i>  <i>Die obere Reihe in jeder Zelle entspricht der korrekten Erkennung und die untere Zeile den ablenkenden Fragmenten, die zu einer falschen Erkennung geführt haben</i> <br><br>  Wir erhalten auch die genaue „Wärmekarte“, die zeigt, welche Teile des Bildes zur Entscheidung beigetragen haben. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89b/510/f5b/89b510f5b5d816a2a1d8590f4487abf6.jpg"><br>  <i>Heatmaps sind keine Annäherung, sie zeigen genau den Beitrag jedes Teils des Bildes.</i> <br><br>  BagNets zeigen, dass Sie mit ImageNet nur aufgrund schwacher statistischer Korrelationen zwischen lokalen Merkmalen von Bildern und der Kategorie von Objekten eine hohe Genauigkeit erzielen können.  Wenn dies ausreicht, warum sollten Standard-Neuronale Netze von ResNet-50 dann etwas grundlegend anderes lernen?  Warum sollte ResNet-50 komplexe Beziehungen in großem Maßstab wie die Form eines Objekts untersuchen, wenn die Fülle lokaler Merkmale des Bildes ausreicht, um das Problem zu lösen? <br><br>  Um die Hypothese zu testen, dass moderne SNAs einer Strategie folgen, die dem Betrieb der einfachsten BoW-Netzwerke ähnelt, haben wir verschiedene Netzwerke - ResNet, DenseNet und VGG - auf die folgenden „Zeichen“ von BagNet getestet: <br><ul><li>  Die Lösungen sind unabhängig vom räumlichen Mischen der Bildmerkmale (dies kann nur bei VGG-Modellen überprüft werden). </li><li>  Änderungen verschiedener Teile des Bildes sollten nicht voneinander abhängen (im Sinne ihres Einflusses auf die Klassenzugehörigkeit). </li><li>  Fehler, die von Standard-SNA und BagNet'ami gemacht wurden, sollten ähnlich sein. </li><li>  Standard-SNS und BagNet sollten für ähnliche Funktionen empfindlich sein. </li></ul><br><br>  In allen vier Experimenten fanden wir überraschend ähnliche Verhaltensweisen von SNS und BagNet.  Im letzten Experiment haben wir beispielsweise gezeigt, dass BagNet für dieselben Stellen in den Bildern wie die SNA am empfindlichsten ist (wenn sie sich beispielsweise überlappen).  Tatsächlich sagen BagNet-Wärmekarten (räumliche Empfindlichkeitskarten) die Empfindlichkeit von DenseNet-169 besser voraus als Wärmekarten, die mit Attributionsmethoden wie DeepLift (direkte Berechnung von Wärmekarten für DenseNet-169) erhalten wurden.  Natürlich wiederholt der SNA das Verhalten von BagNet nicht genau, aber bestimmte Abweichungen zeigen dies.  Insbesondere werden die Abhängigkeiten umso größer, je tiefer die Netzwerke werden, und desto größer werden die Abhängigkeiten.  Daher sind tiefe neuronale Netze in der Tat eine Verbesserung gegenüber BagNet-Modellen, aber ich glaube nicht, dass sich die Grundlage ihrer Klassifizierung irgendwie ändert. <br><br><h2>  Über die BoW-Klassifizierung hinausgehen </h2><br>  Die Beobachtung der SNA-Entscheidungsfindung im Stil von BoW-Strategien kann einige der seltsamen Merkmale der SNA erklären.  Dies erklärt zunächst, warum der SNA so <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">an Texturen gebunden ist</a> .  Zweitens, warum der SNA nicht empfindlich auf das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mischen</a> von Bildteilen reagiert.  Dies kann sogar das Vorhandensein von gegnerischen Aufklebern und gegnerischen Störungen erklären: Verwirrende Signale können an einer beliebigen Stelle im Bild platziert werden, und der SNS fängt dieses Signal mit Sicherheit ab, unabhängig davon, ob es zum Rest des Bildes passt. <br><br>  Tatsächlich zeigt unsere Arbeit, dass der SNA beim Erkennen von Bildern viele schwache statistische Gesetze verwendet und nicht wie Menschen Teile des Bildes auf Objektebene integriert.  Gleiches gilt höchstwahrscheinlich für andere Aufgaben und sensorische Modalitäten. <br><br>  Wir müssen unsere Architekturen, Aufgaben und Trainingsmethoden sorgfältig planen, um die Tendenz zu überwinden, schwache statistische Korrelationen zu verwenden.  Ein Ansatz besteht darin, die Verzerrung des SNA-Trainings von kleinen lokalen Merkmalen auf globalere zu übertragen.  Die andere besteht darin, die Merkmale zu entfernen oder zu ersetzen, auf die sich das neuronale Netzwerk nicht verlassen sollte, wie wir es in einer anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Veröffentlichung</a> für ICLR 2019 getan haben, indem die Stilübertragungsvorverarbeitung verwendet wird, um die Textur eines natürlichen Objekts zu beseitigen. <br><br>  Eines der größten Probleme bleibt jedoch die Klassifizierung von Bildern: Wenn lokale Merkmale ausreichen, gibt es keinen Anreiz, die reale "Physik" der natürlichen Welt zu studieren.  Wir müssen die Aufgabe so umstrukturieren, dass Modelle verschoben werden, um die physikalische Natur von Objekten zu untersuchen.  Um dies zu tun, müssen Sie höchstwahrscheinlich über die reine Beobachtungslehre hinaus zu den Korrelationen von Eingabe- und Ausgabedaten gehen, damit Modelle kausale Zusammenhänge extrahieren können. <br><br>  Zusammengenommen legen unsere Ergebnisse nahe, dass die SNA einer äußerst einfachen Klassifizierungsstrategie folgen kann.  Die Tatsache, dass eine solche Entdeckung im Jahr 2019 gemacht werden kann, unterstreicht, wie wenig wir die internen Merkmale der Arbeit tiefer neuronaler Netze noch verstehen.  Der Mangel an Verständnis erlaubt es uns nicht, grundlegend verbesserte Modelle und Architekturen zu entwickeln, die die Lücke zwischen der Wahrnehmung von Mensch und Maschine schließen.  Durch die Vertiefung unseres Verständnisses können wir Wege finden, um diese Lücke zu schließen.  Dies kann äußerst nützlich sein: Beim Versuch, die SNA in Richtung der physikalischen Eigenschaften von Objekten zu verschieben, erreichten wir plötzlich eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Widerstandsfähigkeit gegen Lärm auf</a> menschlicher Ebene.  Ich erwarte das Erscheinen einer Vielzahl anderer interessanter Ergebnisse auf unserem Weg zur Entwicklung der SNA, die die physische und kausale Natur unserer Welt wirklich verstehen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443734/">https://habr.com/ru/post/de443734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443722/index.html">Design und Benennung von Warteschlangen</a></li>
<li><a href="../de443724/index.html">AMD Radeon VII: High-End-Chip (Teil 1)</a></li>
<li><a href="../de443726/index.html">Setup-Funktionen von Palo Alto Networks: SSL VPN</a></li>
<li><a href="../de443728/index.html">Google Plus funktioniert nicht mehr. Na und?</a></li>
<li><a href="../de443730/index.html">Strg-Alt-Entf: Geplante Veralterung von Programmierern</a></li>
<li><a href="../de443736/index.html">Einrichten von Network Management Tools (NUT) von Grund auf neu, um eine lokal verbundene USV zu verwalten</a></li>
<li><a href="../de443738/index.html">So finden Sie einen Job in Deutschland für IT-Profis</a></li>
<li><a href="../de443740/index.html">Erste Version des Testtools für die offene Produktsuche</a></li>
<li><a href="../de443744/index.html">Encyclopedia of Lighting von Naughty Dog</a></li>
<li><a href="../de443746/index.html">Spielemarkt, Trends und Vorhersagen - Großartige Analysen von App Annie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>