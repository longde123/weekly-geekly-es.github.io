<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèª üöç ü§≤üèª Introducci√≥n a la tarea de reconocer las emociones. üîØ ‚úçüèæ ü§∑üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El reconocimiento de emociones es un tema candente en el campo de la inteligencia artificial. Las √°reas m√°s interesantes de aplicaci√≥n de tales tecnol...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Introducci√≥n a la tarea de reconocer las emociones.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/speechpro/blog/418151/"><p>  El reconocimiento de emociones es un tema candente en el campo de la inteligencia artificial.  Las √°reas m√°s interesantes de aplicaci√≥n de tales tecnolog√≠as incluyen: reconocimiento de conductores, investigaci√≥n de mercado, sistemas de an√°lisis de video para ciudades inteligentes, interacci√≥n hombre-m√°quina, monitoreo de estudiantes que toman cursos en l√≠nea, dispositivos port√°tiles, etc. </p><br><p>  Este a√±o, MDG dedic√≥ su <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escuela de aprendizaje autom√°tico de verano</a> a este tema.  En este art√≠culo, intentar√© dar una breve excursi√≥n al problema de reconocer el estado emocional de una persona y hablar sobre los enfoques para su soluci√≥n. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/626/db4/5bb/626db45bb7b0aad7fdbc2970c0b4183f.jpg" alt="imagen"></div><a name="habracut"></a><br><h3 id="chto-takoe-emocii">  ¬øQu√© son las emociones? </h3><br><p>  La emoci√≥n es un tipo especial de procesos mentales que expresan la experiencia de una persona de su relaci√≥n con el mundo y consigo misma.  Seg√∫n una de las teor√≠as, cuyo autor es el fisi√≥logo ruso P.K.  Anokhin, la capacidad de experimentar emociones se desarroll√≥ en el proceso de evoluci√≥n como un medio de adaptaci√≥n m√°s exitosa de los seres vivos a las condiciones de existencia.  La emoci√≥n fue √∫til para la supervivencia y permiti√≥ que los seres vivos respondieran r√°pida y econ√≥micamente a las influencias externas. </p><br><p>  Las emociones juegan un papel muy importante en la vida humana y la comunicaci√≥n interpersonal.  Se pueden expresar de varias maneras: expresiones faciales, postura, reacciones motoras, voz y reacciones aut√≥nomas (frecuencia card√≠aca, presi√≥n arterial, frecuencia respiratoria).  Sin embargo, la cara de la persona tiene la mayor expresividad. </p><br><p>  Cada persona expresa emociones de varias maneras diferentes.  El famoso psic√≥logo estadounidense <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Paul Ekman</a> , al estudiar el comportamiento no verbal de las tribus aisladas en Pap√∫a Nueva Guinea en los a√±os 70 del siglo pasado, descubri√≥ que una serie de emociones, a saber: ira, miedo, tristeza, asco, desprecio, sorpresa y alegr√≠a son universales y pueden para ser entendido por el hombre, independientemente de su cultura. </p><br><p>  Las personas pueden expresar una amplia gama de emociones.  Se cree que pueden describirse como una combinaci√≥n de emociones b√°sicas (por ejemplo, la nostalgia es algo entre tristeza y alegr√≠a).  Pero un enfoque tan categ√≥rico no siempre es conveniente, porque  no permite cuantificar el poder de la emoci√≥n.  Por lo tanto, junto con modelos discretos de emociones, se desarrollaron varios continuos.  El modelo de J. Russell tiene una base bidimensional en la que cada emoci√≥n se caracteriza por un signo (valencia) e intensidad (excitaci√≥n).  Debido a su simplicidad, el modelo Russell se ha vuelto cada vez m√°s popular en el contexto de la tarea de clasificar autom√°ticamente las expresiones faciales. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/f59/cd1/a24/f59cd1a24e77d64759641a076d86bef1.png" alt="imagen"></div><br><p>  Entonces, descubrimos que si no est√° tratando de ocultar la excitaci√≥n emocional, su estado actual puede estimarse mediante expresiones faciales.  Adem√°s, utilizando los logros modernos en el campo del aprendizaje profundo, incluso es posible construir un detector de mentiras basado en la serie "Lie to me", cuya base cient√≠fica fue proporcionada directamente por Paul Ekman.  Sin embargo, esta tarea est√° lejos de ser simple.  Como mostraron los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estudios de la</a> neurocient√≠fica Lisa Feldman Barrett, al reconocer las emociones, una persona usa activamente informaci√≥n contextual: voz, acciones, situaci√≥n.  Echa un vistazo a las fotos a continuaci√≥n, realmente lo es.  Usando solo el √°rea de la cara, es imposible hacer una predicci√≥n correcta.  En este sentido, para resolver este problema, es necesario utilizar tanto modalidades adicionales como informaci√≥n sobre cambios en las se√±ales a lo largo del tiempo. </p><br><div style="text-align:center;"><img height="200" src="https://habrastorage.org/webt/el/a6/t7/ela6t7ig73rti-peenhb7_f0sgs.jpeg" alt="imagen"></div><br><p>  Aqu√≠ consideraremos enfoques para el an√°lisis de solo dos modalidades: audio y video, ya que estas se√±ales pueden obtenerse sin contacto.  Para abordar la tarea, primero necesita obtener los datos.  Aqu√≠ hay una lista de las bases de datos de emociones m√°s grandes disponibles p√∫blicamente que conozco.  Las im√°genes y videos en estas bases de datos fueron etiquetados manualmente, algunos usando Amazon Mechanical Turk. </p><br><table><thead><tr><th>  Titulo </th><th>  Datos </th><th>  Marcado </th><th>  A√±o de fabricaci√≥n </th></tr></thead><tbody><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desaf√≠o OMG-Emotion</a> </td><td>  audio / video </td><td>  7 categor√≠as, valencia / excitaci√≥n </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desaf√≠o emocional</a> </td><td>  audio / video </td><td>  6 categor√≠as </td><td>  2018 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Affectnet</a> </td><td>  im√°genes </td><td>  7 categor√≠as, valencia / excitaci√≥n </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AFEW-VA</a> </td><td>  el video </td><td>  valencia / excitaci√≥n </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desaf√≠o EmotioNet</a> </td><td>  im√°genes </td><td>  16 categor√≠as </td><td>  2017 </td></tr><tr><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Emoreact</a> </td><td>  audio / video </td><td>  17 categor√≠as </td><td>  2016 </td></tr></tbody></table><br><h3 id="klassicheskiy-podhod-k-zadache-klassifikacii-emociy">  El enfoque cl√°sico de la clasificaci√≥n de las emociones. </h3><br><p>  La forma m√°s f√°cil de determinar la emoci√≥n a partir de una imagen facial se basa en la clasificaci√≥n de puntos clave (puntos de referencia faciales), cuyas coordenadas se pueden obtener utilizando varios algoritmos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PDM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CML</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AAM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DPM</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CNN</a> .  Por lo general, marque de 5 a 68 puntos, at√°ndolos a la posici√≥n de las cejas, ojos, labios, nariz, mand√≠bula, lo que le permite capturar parcialmente las expresiones faciales.  Las coordenadas normalizadas de puntos se pueden enviar directamente al clasificador (por ejemplo, SVM o Bosque aleatorio) y obtener una soluci√≥n b√°sica.  Naturalmente, la posici√≥n de las personas debe estar alineada. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/c60/24a/dbe/c6024adbeaecca98404dcaae3361785e.jpg" alt="imagen"></div><br><p>  El uso simple de coordenadas sin un componente visual conduce a una p√©rdida significativa de informaci√≥n √∫til, por lo tanto, se calculan varios descriptores en estos puntos para mejorar el sistema: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LBP</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SIFT</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LATCH</a> , etc. Despu√©s de que los descriptores se concatenan y la dimensi√≥n se reduce usando PCA, el vector de caracter√≠sticas resultante se puede usar para la clasificaci√≥n emociones </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/lh/oa/u7lhoatsm4vbqzb_zlksw9ekygy.jpeg" alt="imagen"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace al art√≠culo</a> </p><br><p>  Sin embargo, este enfoque ya se considera obsoleto, ya que se sabe que las redes de convoluci√≥n profunda son la mejor opci√≥n para el an√°lisis de datos visuales. </p><br><h3 id="klassifikaciya-emociy-s-primeneniem-deep-learning">  Clasificaci√≥n de las emociones mediante el aprendizaje profundo. </h3><br><p>  Para construir un clasificador de red neuronal, es suficiente tomar alguna red con una arquitectura b√°sica, previamente entrenada en ImageNet, y volver a entrenar las √∫ltimas capas.  Por lo tanto, puede obtener una buena soluci√≥n b√°sica para clasificar diversos datos, pero teniendo en cuenta los detalles de la tarea, las redes neuronales utilizadas para tareas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">reconocimiento facial a</a> gran escala ser√°n m√°s adecuadas. </p><br><p>  Por lo tanto, es bastante simple construir un clasificador de emociones para im√°genes individuales, pero como descubrimos, las instant√°neas no reflejan con precisi√≥n las verdaderas emociones que experimenta una persona en una situaci√≥n dada.  Por lo tanto, para aumentar la precisi√≥n del sistema, es necesario analizar la secuencia de cuadros.  Hay dos formas de hacer esto.  La primera forma es alimentar las caracter√≠sticas de alto nivel recibidas de una CNN que clasifica cada trama individual en una red recurrente (por ejemplo, LSTM) para capturar el componente de tiempo. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/webt/ik/wi/zh/ikwizhwy65xydfoyu15ql3szjbi.jpeg" alt="imagen"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace al art√≠culo</a> </p><br><p>  La segunda forma es alimentar directamente una secuencia de cuadros tomados del video en algunos pasos a la entrada 3D-CNN.  CNN similares utilizan convoluciones con tres grados de libertad que transforman la entrada de cuatro dimensiones en mapas de caracter√≠sticas tridimensionales. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/iv/a2/kd/iva2kdloqzpghk8gfcymk6g5sc4.jpeg" alt="imagen"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace al art√≠culo</a> </p><br><p>  De hecho, en el caso general, estos dos enfoques se pueden combinar construyendo tal monstruo. </p><br><div style="text-align:center;"><img height="400" src="https://habrastorage.org/webt/oo/ir/yo/ooiryorm9sh8n0cht5oud1yqbd4.jpeg" alt="imagen"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace al art√≠culo</a> </p><br><h3 id="klassifikaciya-emociy-po-rechi">  Clasificaci√≥n del discurso de las emociones. </h3><br><p>  Seg√∫n los datos visuales, el signo de la emoci√≥n se puede predecir con gran precisi√≥n, pero es preferible utilizar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se√±ales de voz</a> al determinar la intensidad.  Analizar el audio es un poco m√°s dif√≠cil debido a la alta variabilidad de la duraci√≥n del discurso y las voces de los hablantes.  Por lo general, no utilizan la onda de sonido original, sino varios conjuntos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">atributos</a> , por ejemplo: F0, MFCC, LPC, i-vectors, etc. En el problema de reconocer las emociones por el habla, la biblioteca abierta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenSMILE</a> tiene una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">buena reputaci√≥n</a> . Contiene un rico conjunto de algoritmos para analizar el habla y la m√∫sica. se√±ales  Despu√©s de la extracci√≥n, los atributos se pueden enviar a SVM o LSTM para su clasificaci√≥n. </p><br><p>  Recientemente, sin embargo, las redes neuronales convolucionales tambi√©n han comenzado a penetrar en el campo del an√°lisis de sonido, desplazando los enfoques establecidos.  Para aplicarlos, el sonido se representa en forma de espectrogramas en una escala lineal o mel, despu√©s de lo cual se operan con los espectrogramas obtenidos como con im√°genes bidimensionales ordinarias.  En este caso, el problema de un tama√±o arbitrario de espectrogramas a lo largo del eje del tiempo se resuelve de manera elegante mediante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la agrupaci√≥n estad√≠stica</a> o mediante la incorporaci√≥n de una red recurrente en la arquitectura. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kq/4d/ts/kq4dtsbybmnn3nsq6cwq_wek19u.jpeg" alt="imagen"></div><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace al art√≠culo</a> </p><br><h3 id="audiovizualnoe-raspoznavanie-emociy">  Reconocimiento audiovisual de las emociones. </h3><br><p>  Entonces, examinamos una serie de enfoques para el an√°lisis de las modalidades de audio y video, la etapa final permaneci√≥: la combinaci√≥n de clasificadores para generar la soluci√≥n final.  La forma m√°s simple es combinar directamente sus calificaciones.  En este caso, es suficiente para tomar el m√°ximo o el promedio.  Una opci√≥n m√°s dif√≠cil es combinar a nivel de incrustaci√≥n para cada modalidad.  A menudo se usa SVM para esto, pero esto no siempre es correcto, ya que las incorporaciones pueden tener una velocidad diferente.  En este sentido, se desarrollaron algoritmos m√°s avanzados, por ejemplo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje de kernel m√∫ltiple</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ModDrop</a> . </p><br><p>  Y, por supuesto, vale la pena mencionar la clase de las llamadas soluciones de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">extremo a extremo</a> que se pueden entrenar directamente en datos sin procesar de varios sensores sin ning√∫n procesamiento preliminar. </p><br><p>  En general, la tarea del reconocimiento autom√°tico de las emociones a√∫n est√° lejos de resolverse.  A juzgar por los resultados del concurso Emotion Recognition in the Wild del a√±o pasado, las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mejores soluciones</a> alcanzan una precisi√≥n de aproximadamente el 60%.  Espero que la informaci√≥n presentada en este art√≠culo sea suficiente para tratar de construir nuestro propio sistema para reconocer las emociones. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es418151/">https://habr.com/ru/post/es418151/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es418141/index.html">RE: Ghat / AFR Principiante Skipper Race</a></li>
<li><a href="../es418143/index.html">PVS-Studio como una soluci√≥n SAST</a></li>
<li><a href="../es418145/index.html">La primera demanda contra Roskomnadzor de una compa√±√≠a que sufri√≥ cuando Telegram fue bloqueado</a></li>
<li><a href="../es418147/index.html">Silencio de las ejecuciones de Ruby: Transactional Rails / PostgreSQL Thriller</a></li>
<li><a href="../es418149/index.html">Phishing con etiqueta de t√≠tulo</a></li>
<li><a href="../es418153/index.html">Kolesa Android Meetup Video: Acerca de MVVM, Antipatterns y Desarrollo modular</a></li>
<li><a href="../es418155/index.html">Diodo LED Diodo Zener</a></li>
<li><a href="../es418157/index.html">El libro "Objetos elegantes. Edici√≥n Java ¬ª</a></li>
<li><a href="../es418159/index.html">D√≥nde ir al dise√±ador: prestigiosos premios de Rusia, Europa del Este y los pa√≠ses de la CEI</a></li>
<li><a href="../es418161/index.html">En Stanford, se desarrollaron bater√≠as de transmisi√≥n a temperatura ambiente</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>