<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÑ ‚åöÔ∏è üëÇüèº Tupperware: ¬øel asesino de Facebook Kubernetes? üë©‚Äçüëß üàπ üëµ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Gesti√≥n de cl√∫steres eficiente y confiable a cualquier escala con Tupperware 





 Hoy, en la conferencia Systems @Scale, presentamos Tupperware, nue...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tupperware: ¬øel asesino de Facebook Kubernetes?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/455579/"><p>  Gesti√≥n de cl√∫steres eficiente y confiable a cualquier escala con Tupperware </p><br><p><img src="https://habrastorage.org/webt/gm/mv/jn/gmmvjn5ev7lk2kyhaiejighzrf0.jpeg"></p><br><p>  Hoy, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la conferencia Systems @Scale,</a> presentamos Tupperware, nuestro sistema de administraci√≥n de cl√∫ster que organiza contenedores en millones de servidores, donde funcionan casi todos nuestros servicios.  Lanzamos Tupperware por primera vez en 2011, y desde entonces nuestra infraestructura ha crecido de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1 centro de datos</a> a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">15 centros de datos geodistribuidos</a> .  Todo este tiempo Tupperware no se detuvo y se desarroll√≥ con nosotros.  Le diremos en qu√© situaciones Tupperware proporciona una administraci√≥n de cl√∫ster de primera clase, que incluye soporte conveniente para servicios con estado, un solo panel de control para todos los centros de datos y la capacidad de distribuir energ√≠a entre servicios en tiempo real.  Y compartiremos las lecciones que aprendimos a medida que nuestra infraestructura se desarroll√≥. </p><br><p> Tupperware realiza varias tareas.  Los desarrolladores de aplicaciones lo usan para entregar y administrar aplicaciones.  Empaqueta el c√≥digo y las dependencias de la aplicaci√≥n en una imagen y la entrega a los servidores en forma de contenedores.  Los contenedores proporcionan aislamiento entre aplicaciones en el mismo servidor para que los desarrolladores est√©n ocupados con la l√≥gica de la aplicaci√≥n y no piensen en c√≥mo encontrar servidores o controlar las actualizaciones.  Tupperware tambi√©n monitorea el rendimiento del servidor y, si encuentra una falla, transfiere los contenedores del servidor problem√°tico. </p><a name="habracut"></a><br><p>  Los ingenieros de planificaci√≥n de capacidad utilizan Tupperware para distribuir las capacidades del servidor en equipos de acuerdo con el presupuesto y las limitaciones.  Tambi√©n lo usan para mejorar la utilizaci√≥n del servidor.  Los operadores de centros de datos recurren a Tupperware para distribuir adecuadamente los contenedores entre los centros de datos y detener o mover contenedores durante el mantenimiento.  Debido a esto, el mantenimiento de servidores, redes y equipos requiere una participaci√≥n humana m√≠nima. </p><br><h3 id="arhitektura-tupperware">  Arquitectura Tupperware </h3><br><p> <a href=""><img src="https://habrastorage.org/webt/e7/q1/oz/e7q1ozhv85xlsvgczzofpgwoikg.jpeg"></a> </p><br><p>  <em>Arquitectura Tupperware PRN es una de las regiones de nuestros centros de datos.</em>  <em>La regi√≥n consta de varios edificios de centros de datos (PRN1 y PRN2) ubicados cerca.</em>  <em>Planeamos hacer un panel de control que gestionar√° todos los servidores en una regi√≥n.</em> </p><br><p>  Los desarrolladores de aplicaciones brindan servicios en forma de trabajos Tupperware.  Una tarea consta de varios contenedores, y todos ellos generalmente ejecutan el mismo c√≥digo de aplicaci√≥n. </p><br><p>  Tupperware es responsable del aprovisionamiento de contenedores y la gesti√≥n del ciclo de vida.  Se compone de varios componentes: </p><br><ul><li>  Tupperware Frontend proporciona una API para la interfaz de usuario, CLI y otras herramientas de automatizaci√≥n a trav√©s de las cuales puede interactuar con Tupperware.  Ocultan toda la estructura interna de los propietarios de trabajos de Tupperware. </li><li>  El Programador Tupperware es el panel de control responsable de administrar el contenedor y el ciclo de vida del trabajo.  Se implementa a nivel regional y global, donde un planificador regional gestiona servidores en una regi√≥n, y un planificador global gestiona servidores de diferentes regiones.  El planificador se divide en fragmentos, y cada fragmento controla un conjunto de tareas. </li><li>  El proxy del planificador en Tupperware oculta el fragmentaci√≥n interna y proporciona un panel de control unificado conveniente para los usuarios de Tupperware. </li><li>  El distribuidor Tupperware asigna contenedores a los servidores.  El planificador es responsable de detener, iniciar, actualizar y fallar los contenedores.  Actualmente, un √∫nico distribuidor puede administrar una regi√≥n completa sin dividirse en fragmentos.  (Observe la diferencia en la terminolog√≠a. Por ejemplo, el planificador en Tupperware corresponde al panel de control en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes</a> , y el distribuidor de Tupperware se llama el planificador en Kubernetes). </li><li>  El agente de recursos almacena la fuente de la verdad para el servidor y los eventos de servicio.  Ejecutamos un agente de recursos para cada centro de datos, y almacena toda la informaci√≥n del servidor en este centro de datos.  Un agente de recursos y un sistema de gesti√≥n de capacidad, o un sistema de asignaci√≥n de recursos, deciden din√°micamente qu√© suministro del planificador controla qu√© servidor.  El servicio de comprobaci√≥n de estado supervisa los servidores y almacena datos sobre su estado en el intermediario de recursos.  Si el servidor tiene problemas o necesita servicio, el agente de recursos le dice al distribuidor y al planificador que detenga los contenedores o los transfiera a otros servidores. </li><li>  Tupperware Agent es un demonio que se ejecuta en cada servidor que prepara y elimina contenedores.  Las aplicaciones funcionan dentro del contenedor, lo que les da m√°s aislamiento y reproducibilidad.  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la conferencia Systems @Scale del a√±o pasado,</a> ya describimos c√≥mo se crean contenedores Tupperware individuales usando im√°genes, btrfs, cgroupv2 y systemd. </li></ul><br><h3 id="otlichitelnye-osobennosti-tupperware">  Caracter√≠sticas distintivas de Tupperware </h3><br><p>  Tupperware es muy similar a otros sistemas de administraci√≥n de cl√∫ster, como Kubernetes y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mesos</a> , pero hay algunas diferencias: </p><br><ul><li>  Soporte nativo para servicios con estado. </li><li>  Un √∫nico panel de control para servidores en diferentes centros de datos para automatizar la entrega de contenedores seg√∫n la intenci√≥n, el desmantelamiento de cl√∫steres y el mantenimiento. </li><li>  Separaci√≥n clara del panel de control para hacer zoom. </li><li>  Los c√°lculos flexibles le permiten distribuir la energ√≠a entre los servicios en tiempo real. </li></ul><br><p>  Dise√±amos estas caracter√≠sticas geniales para admitir una variedad de aplicaciones sin estado y con estado en un enorme parque global de servidores compartidos. </p><br><h3 id="vstroennaya-podderzhka-stateful-sevisov">  Soporte nativo para servicios con estado. </h3><br><p>  Tupperware gestiona muchos servicios cr√≠ticos con estado que almacenan datos de productos persistentes para Facebook, Instagram, Messenger y WhatsApp.  Estos pueden ser grandes pares clave-valor (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ZippyDB</a> ) y almacenes de datos de monitoreo (por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ODS Gorilla</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Scuba</a> ).  Mantener servicios con estado no es f√°cil, porque el sistema debe garantizar que las entregas de contenedores puedan soportar fallas a gran escala, incluyendo un corte de energ√≠a o un corte de energ√≠a.  Aunque los m√©todos convencionales, como la distribuci√≥n de contenedores a trav√©s de dominios de falla, son adecuados para servicios sin estado, los servicios con estado necesitan soporte adicional. </p><br><p>  Por ejemplo, si como resultado de una falla del servidor, una r√©plica de la base de datos no est√° disponible, ¬øes necesario permitir el mantenimiento autom√°tico que actualizar√° los n√∫cleos en 50 servidores de un grupo de 10 mil√©simas?  Depende de la situaci√≥n.  Si en uno de estos 50 servidores hay otra r√©plica de la misma base de datos, es mejor esperar y no perder 2 r√©plicas a la vez.  Para tomar decisiones din√°micas sobre el mantenimiento y el estado del sistema, necesita informaci√≥n sobre la replicaci√≥n interna de datos y la l√≥gica de ubicaci√≥n de cada servicio con estado. </p><br><p>  La interfaz TaskControl permite que los servicios con estado influyan en las decisiones que afectan la disponibilidad de datos.  Usando esta interfaz, el planificador notifica a las aplicaciones externas de las operaciones del contenedor (reinicio, actualizaci√≥n, migraci√≥n, mantenimiento).  El servicio Stateful implementa un controlador que le dice a Tupperware cu√°ndo cada operaci√≥n puede realizarse de manera segura, y estas operaciones pueden intercambiarse o retrasarse temporalmente.  En el ejemplo anterior, el controlador de la base de datos puede indicar a Tupperware que actualice 49 de los 50 servidores, pero hasta ahora no toque un servidor espec√≠fico (X).  Como resultado, si el per√≠odo de actualizaci√≥n del kernel pasa y la base de datos a√∫n no puede restaurar la r√©plica del problema, Tupperware a√∫n actualizar√° el servidor X. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/xi/5q/xuxi5qpox1v3gpc6khbipxgna0i.jpeg"></a> </p><br><p>  Muchos servicios con estado en Tupperware no usan TaskControl directamente, sino a trav√©s de ShardManager, una plataforma com√∫n para crear servicios con estado en Facebook.  Con Tupperware, los desarrolladores pueden indicar su intenci√≥n sobre c√≥mo se deben distribuir los contenedores entre los centros de datos.  Con ShardManager, los desarrolladores indican su intenci√≥n sobre c√≥mo se deben distribuir los fragmentos de datos entre los contenedores.  ShardManager conoce el alojamiento de datos y la replicaci√≥n de sus aplicaciones e interact√∫a con Tupperware a trav√©s de la interfaz TaskControl para planificar las operaciones de contenedor sin la participaci√≥n directa de la aplicaci√≥n.  Esta integraci√≥n simplifica enormemente la administraci√≥n de servicios con estado, pero TaskControl es capaz de m√°s.  Por ejemplo, nuestro extenso nivel web no tiene estado y utiliza TaskControl para ajustar din√°micamente la velocidad de las actualizaciones en los contenedores.  Como resultado, el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nivel web puede completar r√°pidamente varias versiones de software</a> por d√≠a sin comprometer la disponibilidad. </p><br><h3 id="upravlenie-serverami-v-datacentrah">  Administraci√≥n de servidores en centros de datos. </h3><br><p>  Cuando Tupperware apareci√≥ por primera vez en 2011, un programador separado controlaba cada cl√∫ster de servidores.  Luego, el cl√∫ster de Facebook era un grupo de bastidores de servidores conectados a un conmutador de red, y el centro de datos conten√≠a varios cl√∫steres.  El planificador podr√≠a administrar servidores en un solo cl√∫ster, es decir, la tarea no podr√≠a extenderse a varios cl√∫steres.  Nuestra infraestructura estaba creciendo, est√°bamos anulando cada vez m√°s los cl√∫steres.  Dado que Tupperware no pudo transferir la tarea desde el cl√∫ster fuera de servicio a otros cl√∫steres sin cambios, se requiri√≥ mucho esfuerzo y una cuidadosa coordinaci√≥n entre los desarrolladores de aplicaciones y los operadores de centros de datos.  Este proceso condujo a una p√©rdida de recursos cuando los servidores estuvieron inactivos durante meses debido al procedimiento de desmantelamiento. </p><br><p>  Creamos un agente de recursos para resolver el problema del desmantelamiento de cl√∫steres y coordinar otros tipos de tareas de mantenimiento.  El intermediario de recursos supervisa toda la informaci√≥n f√≠sica asociada con el servidor y decide din√°micamente qu√© programador gestiona cada servidor.  La vinculaci√≥n din√°mica de servidores a planificadores permite que el planificador administre servidores en diferentes centros de datos.  Dado que el trabajo de Tupperware ya no se limita a un cl√∫ster, los usuarios de Tupperware pueden especificar c√≥mo se deben distribuir los contenedores entre los dominios de falla.  Por ejemplo, un desarrollador puede declarar su intenci√≥n (por ejemplo: "ejecutar mi tarea en 2 dominios de falla en la regi√≥n PRN") sin especificar zonas de disponibilidad espec√≠ficas.  El propio Tupperware encontrar√° los servidores correctos para encarnar esta intenci√≥n incluso en el caso de desmantelar un cl√∫ster o servicio. </p><br><h3 id="masshtabirovanie-dlya-podderzhki-vsey-globalnoy-sistemy">  Escalado para soportar todo el sistema global </h3><br><p>  Hist√≥ricamente, nuestra infraestructura se ha dividido en cientos de grupos de servidores dedicados para equipos individuales.  Debido a la fragmentaci√≥n y la falta de est√°ndares, tuvimos altos costos de transacci√≥n y los servidores inactivos fueron m√°s dif√≠ciles de usar nuevamente.  En la conferencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Systems @Scale</a> del a√±o pasado, presentamos la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Infraestructura como servicio (IaaS)</a> , que deber√≠a integrar nuestra infraestructura en una flota de servidores unificada de gran tama√±o.  Pero una sola flota de servidores tiene sus propias dificultades.  Debe cumplir ciertos requisitos: </p><br><ul><li>  <strong>Escalabilidad.</strong>  Nuestra infraestructura creci√≥ con la adici√≥n de centros de datos en cada regi√≥n.  Los servidores se han vuelto m√°s peque√±os y m√°s eficientes energ√©ticamente, por lo que en cada regi√≥n hay mucho m√°s.  Como resultado, un √∫nico programador para una regi√≥n no puede hacer frente a la cantidad de contenedores que se pueden ejecutar en cientos de miles de servidores en cada regi√≥n. </li><li>  <strong>Fiabilidad</strong>  Incluso si la escala del planificador se puede aumentar de esta manera, debido al gran alcance del planificador, el riesgo de errores ser√° mayor y toda la regi√≥n de contenedores puede volverse inmanejable. </li><li>  <strong>Tolerancia a fallas.</strong>  En el caso de una gran falla en la infraestructura (por ejemplo, debido a una falla de la red o un corte de energ√≠a, los servidores donde se ejecuta el planificador fallar√°n), solo una parte de los servidores de la regi√≥n tendr√° consecuencias negativas. </li><li>  <strong>Facilidad de uso.</strong>  Puede parecer que necesita ejecutar varios programadores independientes en una regi√≥n.  Pero en t√©rminos de conveniencia, un √∫nico punto de entrada a un grupo com√∫n en la regi√≥n simplifica la capacidad y la gesti√≥n del trabajo. </li></ul><br><p>  Dividimos el planificador en fragmentos para resolver problemas al admitir un gran grupo compartido.  Cada fragmento del planificador gestiona su conjunto de tareas en la regi√≥n, y esto reduce el riesgo asociado con el planificador.  A medida que crece el grupo total, podemos agregar m√°s fragmentos de planificador.  Para los usuarios de Tupperware, los fragmentos y los programadores proxy se ven como un panel de control.  No tienen que trabajar con un mont√≥n de fragmentos que orquestan tareas.  Los fragmentos del planificador son fundamentalmente diferentes de los planificadores de cl√∫ster que utilizamos antes, cuando el panel de control se dividi√≥ sin separaci√≥n est√°tica del grupo de servidores com√∫n de acuerdo con la topolog√≠a de la red. </p><br><h3 id="povyshenie-effektivnosti-ispolzovaniya-s-pomoschyu-elastichnyh-vychisleniy">  Mejora de la utilizaci√≥n con computaci√≥n el√°stica </h3><br><p>  Cuanto m√°s grande es nuestra infraestructura, m√°s importante es usar eficientemente nuestros servidores para optimizar los costos de infraestructura y reducir la carga.  Hay dos formas de mejorar la utilizaci√≥n del servidor: </p><br><ul><li>  Computaci√≥n flexible: reduzca la escala de los servicios en l√≠nea durante las horas de silencio y use los servidores liberados para cargas fuera de l√≠nea, por ejemplo, para tareas de aprendizaje autom√°tico y MapReduce. </li><li>  Carga excesiva: aloje servicios en l√≠nea y cargas de trabajo por lotes en los mismos servidores para que las cargas por lotes se ejecuten con baja prioridad. </li></ul><br><p>  El cuello de botella en nuestros centros de datos es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el consumo de energ√≠a</a> .  Por lo tanto, preferimos servidores peque√±os y de bajo consumo que, en conjunto, brinden m√°s potencia de procesamiento.  Desafortunadamente, en servidores peque√±os con una peque√±a cantidad de recursos de procesador y memoria, la carga excesiva es menos eficiente.  Por supuesto, podemos colocar varios contenedores de peque√±os servicios en un peque√±o servidor de bajo consumo que consume pocos recursos de procesador y memoria, pero los servicios grandes tendr√°n un bajo rendimiento en esta situaci√≥n.  Por lo tanto, aconsejamos a los desarrolladores de nuestros grandes servicios que los optimicen para que utilicen todo el servidor. </p><br><p>  B√°sicamente, mejoramos la utilizaci√≥n con la inform√°tica el√°stica.  La intensidad del uso de muchos de nuestros grandes servicios, por ejemplo, noticias, funciones de mensajes y nivel web front-end, depende de la hora del d√≠a.  Reducimos intencionalmente la escala de los servicios en l√≠nea durante las horas de silencio y utilizamos los servidores liberados para cargas fuera de l√≠nea, por ejemplo, para el aprendizaje autom√°tico y las tareas de MapReduce. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/6w/zu/dp/6wzudppzm9tobgoryvtrssaxlra.jpeg"></a> </p><br><p>  Por experiencia, sabemos que es mejor proporcionar servidores completos como unidades de energ√≠a el√°stica, porque los grandes servicios son tanto los principales donantes como los principales consumidores de energ√≠a el√°stica, y est√°n optimizados para el uso de servidores completos.  Cuando el servidor se libera del servicio en l√≠nea en las horas de silencio, el agente de recursos entrega el servidor al planificador para uso temporal para que ejecute cargas fuera de l√≠nea en √©l.  Si se produce un pico de carga en un servicio en l√≠nea, el agente de recursos recupera r√°pidamente el servidor prestado y, junto con el planificador, lo devuelve al servicio en l√≠nea. </p><br><h3 id="usvoennye-uroki-i-plany-na-buduschee">  Lecciones aprendidas y planes futuros </h3><br><p>  En los √∫ltimos 8 a√±os, hemos desarrollado Tupperware para mantenerse al d√≠a con el r√°pido desarrollo de Facebook.  Hablamos de lo que hemos aprendido y esperamos que ayude a otros a gestionar infraestructuras de r√°pido crecimiento: </p><br><ul><li>  Configure comunicaciones flexibles entre el panel de control y los servidores que administra.  Esta flexibilidad permite que el panel de control administre servidores en diferentes centros de datos, ayuda a automatizar el desmantelamiento y el mantenimiento de cl√∫steres y proporciona una distribuci√≥n din√°mica de energ√≠a mediante una inform√°tica flexible. </li><li>  Con un solo panel de control en la regi√≥n, resulta m√°s conveniente trabajar con tareas y es m√°s f√°cil administrar una gran flota com√∫n de servidores.  Tenga en cuenta que el panel de control admite un √∫nico punto de entrada, incluso si su estructura interna est√° dividida por razones de escala o tolerancia a fallas. </li><li>  Usando el modelo de complemento, el panel de control puede notificar a las aplicaciones externas de las pr√≥ximas operaciones de contenedores.  Adem√°s, los servicios con estado pueden usar la interfaz del complemento para configurar la administraci√≥n de contenedores.  Al usar este modelo de complemento, el panel de control proporciona simplicidad y sirve de manera efectiva a muchos servicios con estado diferentes. </li><li>  Creemos que la inform√°tica el√°stica, en la que tomamos servidores completos para trabajos por lotes, aprendizaje autom√°tico y otros servicios no urgentes de los servicios de los donantes, es la mejor manera de aumentar la eficiencia del uso de servidores peque√±os y eficientes energ√©ticamente. </li></ul><br><p>  Reci√©n estamos comenzando a implementar un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">√∫nico parque de servidores com√∫n global</a> .  Ahora aproximadamente el 20% de nuestros servidores est√°n en el grupo com√∫n.  Para lograr el 100%, debe resolver muchos problemas, incluido el soporte de un grupo com√∫n para sistemas de almacenamiento, la automatizaci√≥n del mantenimiento, la gesti√≥n de los requisitos de diferentes clientes, la mejora de la utilizaci√≥n del servidor y la compatibilidad con las cargas de trabajo de aprendizaje autom√°tico.  No podemos esperar para abordar estas tareas y compartir nuestros √©xitos. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/455579/">https://habr.com/ru/post/455579/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455565/index.html">¬øPuede la mente fingir el universo?</a></li>
<li><a href="../455569/index.html">Te invitamos a la Conferencia de Tarantool el 17 de junio.</a></li>
<li><a href="../455571/index.html">Cursores DB en Doctrine</a></li>
<li><a href="../455575/index.html">Neural Matching: c√≥mo adaptar el contenido a las realidades de Google</a></li>
<li><a href="../455577/index.html">SDL 2 Lecciones: Lecci√≥n 3 - Eventos</a></li>
<li><a href="../455580/index.html">Impresiones de aplicaciones m√≥viles imprescindibles</a></li>
<li><a href="../455582/index.html">Navegaci√≥n en la tienda: a trav√©s de la realidad aumentada hasta el estante deseado</a></li>
<li><a href="../455584/index.html">Entrevistas personalizadas con las fuerzas internas de la empresa: desde errores hasta descubrimientos</a></li>
<li><a href="../455586/index.html">Ciclo de conferencias sobre rob√≥tica del profesor Gregor Sch√∂ner, director del Instituto de Neuroinform√°tica (INI) Bochum, Alemania</a></li>
<li><a href="../455588/index.html">C√≥mo educar a tu comunidad para no bailar con una pandereta</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>