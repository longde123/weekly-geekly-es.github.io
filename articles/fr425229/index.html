<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèùÔ∏è üë©üèæ‚Äçüè´ üéûÔ∏è Comment nous avons aid√© CDN MegaFon.TV √† ne pas participer √† la Coupe du monde 2018 üë®üèø‚Äçüíº üö¥üèª üëÉüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En 2016, nous avons parl√© de la fa√ßon dont MegaFon.TV a fait face √† tous ceux qui voulaient regarder la nouvelle saison de Game of Thrones. Le d√©velop...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons aid√© CDN MegaFon.TV √† ne pas participer √† la Coupe du monde 2018</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/megafon/blog/425229/">  En 2016, nous avons parl√© de la fa√ßon dont MegaFon.TV a fait face √† tous ceux qui voulaient regarder la nouvelle saison de Game of Thrones.  Le d√©veloppement du service ne s'est pas arr√™t√© l√†, et √† la mi-2017, nous devions faire face √† des charges plusieurs fois plus.  Dans cet article, nous expliquerons comment une croissance aussi rapide nous a inspir√© √† changer radicalement l'approche de l'organisation du CDN et comment cette nouvelle approche a √©t√© test√©e par la Coupe du monde. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a42/af2/54b/a42af254b7c4d8528fb0d495c27ab6d9.png"><br><a name="habracut"></a><br><h2>  En bref sur MegaFon.TV </h2><br>  MegaFon.TV est un service OTT pour visualiser divers contenus vid√©o - films, √©missions de t√©l√©vision, cha√Ænes de t√©l√©vision et programmes enregistr√©s.  Gr√¢ce √† MegaFon.TV, l'acc√®s au contenu peut √™tre obtenu sur pratiquement n'importe quel appareil: sur les t√©l√©phones et tablettes avec iOS et Android, sur les t√©l√©viseurs intelligents LG, Samsung, Philips, Panasonic de diff√©rentes ann√©es de sortie, avec tout un zoo OS (Apple TV, Android TV), dans navigateurs de bureau sous Windows, MacOS, Linux, dans les navigateurs mobiles sur iOS et Android, et m√™me sur des appareils exotiques tels que STB et les projecteurs Android pour enfants.  Il n'y a pratiquement aucune restriction sur les appareils - seule la disponibilit√© d'Internet avec une bande passante de 700 Kbps est importante.  √Ä propos de la fa√ßon dont nous avons organis√© la prise en charge de tant d'appareils, il y aura un article distinct √† l'avenir. <br>  La plupart des utilisateurs du service sont des abonn√©s MegaFon, ce qui s‚Äôexplique par des offres rentables (et le plus souvent m√™me gratuites) incluses dans le plan tarifaire de l‚Äôabonn√©.  Bien que nous notons √©galement une augmentation significative du nombre d'utilisateurs d'autres op√©rateurs.  Conform√©ment √† cette distribution, 80% du trafic MegaFon.TV est consomm√© au sein du r√©seau MegaFon. <br><br>  Sur le plan architectural, depuis le lancement du service, le contenu a √©t√© distribu√© via CDN.  Nous avons un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">poste</a> s√©par√© d√©di√© au travail de ce CDN.  Dans ce document, nous avons expliqu√© comment cela nous a permis de g√©rer le trafic de pointe qui est all√© au service fin 2016, lors de la sortie de la nouvelle saison de Game of Thrones.  Dans cet article, nous parlerons du d√©veloppement ult√©rieur de MegaFon.TV et des nouvelles aventures qui sont tomb√©es sur le service avec la Coupe du Monde 2018. <br><br><h2>  Croissance des services.  Et des probl√®mes </h2><br>  Par rapport aux √©v√©nements du dernier message, fin 2017, le nombre d'utilisateurs de Megafon.TV a augment√© √† plusieurs reprises, les films et les s√©ries sont √©galement devenus un ordre de grandeur plus important.  De nouvelles fonctionnalit√©s ont √©t√© lanc√©es, de nouveaux packages sont apparus, disponibles ¬´par abonnement¬ª.  Les pics de trafic depuis le ¬´Game of Thrones¬ª que nous voyons aujourd'hui chaque jour, la proportion de films et d'√©missions de t√©l√©vision dans le flux total n'a cess√© d'augmenter. <br><br>  Parall√®lement √† cela, des probl√®mes ont commenc√© avec la redistribution du trafic.  Notre surveillance, configur√©e pour t√©l√©charger des segments pour diff√©rents types de trafic dans diff√©rents formats, a de plus en plus commenc√© √† produire des erreurs lors du t√©l√©chargement de segments vid√©o par timeout.  Dans le service MegaFon.TV, la longueur du morceau est de 8 secondes.  Si le bloc n'a pas le temps de se charger en 8 secondes, des erreurs peuvent se produire. <br><br>  Le pic d'erreurs devait survenir aux heures les plus charg√©es.  Comment cela devrait-il affecter les utilisateurs?  Au minimum, ils ont pu observer une d√©t√©rioration de la qualit√© vid√©o.  Il n'est pas toujours perceptible √† l'≈ìil nu, en raison d'un nombre suffisamment important de profils multi-bitrate.  Dans le pire des cas, la vid√©o se fige. <br><br>  La recherche du probl√®me a commenc√©.  Presque imm√©diatement, il est devenu clair qu'une erreur de rebond se produit sur les serveurs EDGE de CDN.  Ici, nous devons faire une petite digression et dire comment les serveurs fonctionnent avec le trafic en direct et VOD.  Le sch√©ma est un peu diff√©rent.  Un utilisateur qui vient sur le serveur EDGE pour du contenu (une liste de lecture ou un morceau), s'il y a du contenu dans le cache, le r√©cup√®re √† partir de l√†.  Sinon, le serveur EDGE recherche du contenu sur Origin, en chargeant le canal principal.  Avec une liste de lecture ou un morceau, l'en <b>-</b> t√™te <b>Cache-Control: max-age</b> est donn√©, qui indique au serveur EDGE combien de cache une unit√© de contenu particuli√®re.  La diff√©rence entre LIVE et VOD r√©side dans le temps n√©cessaire pour mettre en cache des morceaux.  Pour les morceaux en direct, une courte dur√©e de mise en cache est d√©finie, g√©n√©ralement de 30 secondes √† plusieurs minutes - cela est d√ª au court temps de pertinence du contenu en direct.  Ce cache est stock√© dans la RAM, car vous devez constamment donner des morceaux et r√©√©crire le cache.  Pour les morceaux VOD, plus de temps est d√©fini, de plusieurs heures √† des semaines et m√™me des mois - en fonction de la taille de la biblioth√®que de contenu et de la r√©partition de ses vues entre les utilisateurs.  Quant aux listes de lecture, elles sont g√©n√©ralement mises en cache en moins de deux secondes, ou elles ne le sont pas du tout.  Il convient de pr√©ciser que nous ne parlons que du soi-disant mode PULL de CDN, dans lequel nos serveurs fonctionnaient.  L'utilisation du mode PUSH dans notre cas ne serait pas enti√®rement justifi√©e. <br><br>  Mais revenons √† trouver le probl√®me.  Comme nous l'avons d√©j√† remarqu√©, tous les serveurs ont simultan√©ment travaill√© sur le retour des deux types de contenus.  Dans le m√™me temps, les serveurs eux-m√™mes avaient une configuration diff√©rente.  En cons√©quence, certaines machines ont √©t√© surcharg√©es √† l'aide d'IOPS.  Les morceaux n'ont pas eu le temps d'√©crire / lire en raison des faibles performances, de la quantit√©, du volume des disques et de la grande biblioth√®que de contenu.  D'un autre c√¥t√©, les machines plus puissantes qui ont re√ßu plus de trafic ont commenc√© √† √©chouer lors de l'utilisation du processeur.  Les ressources CPU ont √©t√© d√©pens√©es pour la maintenance du trafic SSL et des morceaux fournis via https, tandis que les IOPS sur les disques ont √† peine atteint 35%. <br><br>  Ce qu'il fallait, c'√©tait un sch√©ma qui, √† moindre co√ªt, permettrait d'utiliser de fa√ßon optimale les capacit√©s disponibles.  De plus, six mois plus tard, la Coupe du monde devait commencer, et selon les calculs pr√©liminaires, les pics de trafic en direct auraient d√ª √™tre multipli√©s par six ... <br><br><h2>  Nouvelle approche de CDN </h2><br>  Apr√®s avoir analys√© le probl√®me, nous avons d√©cid√© de s√©parer la VOD et le trafic en direct selon diff√©rents PAD compos√©s de serveurs avec diff√©rentes configurations.  Et cr√©ez √©galement une fonction de distribution du trafic et de son √©quilibrage entre diff√©rents groupes de serveurs.  Il y avait au total trois de ces groupes: <br><br><ul><li>  Serveurs avec un grand nombre de disques hautes performances les mieux adapt√©s √† la mise en cache du contenu VOD.  En fait, les disques SSD RI de la capacit√© maximale seraient les mieux adapt√©s, mais il n'y en avait pas, et il faudrait trop de budget pour acheter la bonne quantit√©.  Finalement, il a √©t√© d√©cid√© d'utiliser le meilleur qui soit.  Chaque serveur contenait huit disques SAS 1 To 10 000 en RAID5.  A partir de ces serveurs, VOD_PAD a √©t√© compil√©. <br></li><li>  Serveurs avec une grande quantit√© de RAM pour mettre en cache tous les formats possibles pour la livraison de morceaux en direct, avec des processeurs capables de g√©rer le trafic SSL et des interfaces r√©seau "√©paisses".  Nous avons utilis√© la configuration suivante: 2 processeurs de 8 c≈ìurs / 192 Go de RAM / 4 interfaces de 10 Go.  √Ä partir de ces serveurs, EDGE_PAD a √©t√© compil√©. <br></li><li>  Le groupe de serveurs restant n'est pas en mesure de g√©rer le trafic VOD, mais convient aux petits volumes de contenu en direct.  Ils peuvent √™tre utilis√©s comme r√©serve.  √Ä partir des serveurs, RESERVE_PAD a √©t√© compil√©. <br></li></ul><br>  La r√©partition √©tait la suivante: <br><img src="https://habrastorage.org/getpro/habr/post_images/3ed/17c/dbf/3ed17cdbf13920eae4c06067e2edd296.png"><br>  Un module logique sp√©cial √©tait charg√© de choisir le PAD dont l'utilisateur √©tait cens√© recevoir le contenu.  Voici ses t√¢ches: <br><ul><li>  Analysez l'URL, appliquez le sch√©ma ci-dessus pour chaque demande de flux et √©mettez le PAD requis <br></li><li>  Pour supprimer la charge des interfaces EDGE_PAD toutes les 5 minutes ( <i>et c'√©tait notre erreur</i> ), et lorsque la limite est atteinte, basculez le trafic exc√©dentaire sur RESERVE_PAD.  Pour soulager la charge, un petit script perl a √©t√© √©crit qui a renvoy√© les donn√©es suivantes: <br>  - <b>horodatage</b> - date et heure de mise √† jour des donn√©es de chargement (au format RFC 3339); <br>  - <b>total_bandwidth</b> - charge d'interface actuelle (total), Kbps; <br>  - <b>rx_bandwidth</b> - charge d'interface actuelle (trafic entrant), Kbps; <br>  - <b>tx_badwidth</b> - charge d'interface actuelle (trafic sortant), Kbps. <br></li><li>  Dirigez le trafic en mode manuel vers n'importe quel serveur PAD ou Origin en cas de situations impr√©vues, ou si n√©cessaire, travaillez sur l'un des PAD.  La configuration √©tait sur le serveur au format yaml et permettait de prendre tout le trafic vers le PAD souhait√©, ou le trafic selon l'un des param√®tres: <br>  - Type de contenu <br>  - cryptage du trafic <br>  - Trafic payant <br>  - type d'appareil <br>  - Type de liste de lecture <br>  - R√©gion <br></li></ul><br>  Les serveurs d'origine ont un SSD en sous-effectif.  Malheureusement, lors du basculement du trafic vers Origin, HIT_RATE sur les morceaux VOD laissait beaucoup √† d√©sirer (environ 30%), mais ils ont accompli leur t√¢che, nous n'avons donc observ√© aucun probl√®me avec les conditionneurs de paquets dans CNN. <br><br>  Comme il y avait peu de serveurs pour la configuration EDGE_PAD, il a √©t√© d√©cid√© de les allouer dans les r√©gions avec la plus grande part de trafic - Moscou et la r√©gion de la Volga.  Avec l'aide de GeoDNS, le trafic a √©t√© envoy√© √† la r√©gion de la Volga √† partir des r√©gions des districts f√©d√©raux de la Volga et de l'Oural.  Le hub de Moscou a servi le reste.  Nous n'aimions pas vraiment l'id√©e de livrer du trafic vers la Sib√©rie et l'Extr√™me-Orient depuis Moscou, mais au total, ces r√©gions repr√©sentent environ 1/20 de tout le trafic, et les canaux de MegaFon se sont av√©r√©s assez larges pour de tels volumes. <br>  Apr√®s l'√©laboration du plan, les travaux suivants ont √©t√© effectu√©s: <br><br><ul><li>  En deux semaines, d√©velopp√© la fonctionnalit√© de commutation de CDN <br></li><li>  Il a fallu un mois pour installer et configurer les serveurs EDGE_PAD, ainsi que pour √©tendre les canaux pour eux <br></li><li>  Il a fallu deux semaines pour diviser le groupe de serveurs actuel en deux parties, plus deux semaines suppl√©mentaires pour appliquer les param√®tres √† tous les serveurs du r√©seau et √† l'√©quipement du serveur <br></li><li>  Et, enfin, la semaine a √©t√© consacr√©e aux tests (malheureusement pas sous charge, ce qui a ensuite affect√©) <br></li></ul><br>  Il s'est av√©r√© parall√©liser une partie du travail, et finalement cela a pris six semaines. <br><br><h2>  Premiers r√©sultats et plans futurs </h2><br>  Apr√®s r√©glage, les performances globales du syst√®me √©taient de 250 Go / s.  La solution avec le transfert du trafic VOD vers des serveurs s√©par√©s a imm√©diatement montr√© son efficacit√© apr√®s son d√©ploiement en production.  Depuis le d√©but de la Coupe du monde, il n'y a eu aucun probl√®me de trafic VOD.  Plusieurs fois, pour diverses raisons, j'ai d√ª basculer le trafic VOD vers Origin, mais en principe, ils ont √©galement fait face.  Ce sch√©ma n'est peut-√™tre pas tr√®s efficace en raison de la tr√®s faible utilisation du cache, car nous for√ßons les SSD √† √©craser constamment le contenu.  Mais le circuit fonctionne. <br><br>  Quant au trafic en direct, les volumes correspondants pour tester notre d√©cision sont apparus avec le d√©but de la Coupe du Monde.  Les probl√®mes ont commenc√© lorsque la deuxi√®me fois que nous avons d√ª faire face √† un changement de trafic lorsque nous avons atteint la limite lors du match Russie-Egypte.  Lorsque la commutation du trafic a fonctionn√©, tout s'est d√©vers√© sur le PAD de sauvegarde.  Au cours de ces cinq minutes, le nombre de demandes (courbe de croissance) a √©t√© si √©lev√© que le CDN de sauvegarde s'est bouch√© compl√®tement et a commenc√© √† verser des erreurs.  Dans le m√™me temps, le PAD principal a √©t√© lib√©r√© pendant cette p√©riode et a commenc√© √† rester un peu inactif: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae7/5a6/804/ae75a68044b8150556324ccfbde19827.png"><br><br>  3 conclusions en ont √©t√© tir√©es: <br><br><ol><li>  Cinq minutes, c'est encore trop.  Il a √©t√© d√©cid√© de r√©duire la p√©riode de d√©chargement √† 30 secondes.  Par cons√©quent, le trafic sur le PAD de secours a cess√© d'augmenter de fa√ßon spasmodique: <br><img src="https://habrastorage.org/getpro/habr/post_images/f73/e8d/47a/f73e8d47a26d62c8fc6d82f88d442fad.png"><br></li><li>  Il est n√©cessaire au minimum de transf√©rer les utilisateurs entre les PAD chaque fois que le commutateur est d√©clench√©.  Cela devrait fournir une fluidit√© suppl√©mentaire de commutation.  Nous avons d√©cid√© d'attribuer un cookie √† chaque utilisateur (ou plut√¥t √† l'appareil), selon lequel le module responsable de la distribution comprend si l'utilisateur doit rester sur le PAD actuel ou si un changement doit √™tre effectu√©.  Ici, la technologie peut √™tre √† la discr√©tion de celui qui la met en ≈ìuvre.  Par cons√©quent, nous ne perdons pas de trafic sur le PAD principal. <br></li><li>  Le seuil de commutation a √©t√© d√©fini trop bas, en cons√©quence, le trafic sur le PAD de secours a augment√© comme une avalanche.  Dans notre cas, il s'agissait d'une r√©assurance - nous n'√©tions pas compl√®tement s√ªrs d'avoir fait le bon r√©glage du serveur (l'id√©e pour laquelle, soit dit en passant, a √©t√© emprunt√©e √† Habr).  Le seuil a √©t√© augment√© pour les performances physiques des interfaces r√©seau. <br></li></ol><br>  Les am√©liorations ont pris trois jours, et d√©j√† lors du match Russie-Croatie, nous avons v√©rifi√© si notre optimisation fonctionnait.  En g√©n√©ral, le r√©sultat nous a plu.  √Ä son apog√©e, le syst√®me a trait√© 215 Gbit / s de trafic mixte.  Ce n'√©tait pas une limite th√©orique sur les performances du syst√®me - nous avions encore une marge substantielle.  Si n√©cessaire, nous pouvons maintenant connecter n'importe quel CDN externe, si n√©cessaire, et y "√©liminer" le trafic exc√©dentaire.  Un tel mod√®le est bon lorsque vous ne voulez pas payer de l'argent solide chaque mois pour utiliser le CDN de quelqu'un d'autre. <br><br>  Nos plans comprennent la poursuite du d√©veloppement de CDN.  Pour commencer, je voudrais √©tendre le sch√©ma EDGE_PAD √† tous les districts f√©d√©raux - cela conduira √† une moindre utilisation des canaux.  Des tests de circuit de redondance VOD_PAD sont √©galement en cours, et certains des r√©sultats semblent maintenant assez impressionnants. <br><br>  En g√©n√©ral, tout ce qui a √©t√© fait au cours de la derni√®re ann√©e m'am√®ne √† penser que le CDN du service qui distribue du contenu vid√©o est un must have.  Et m√™me pas parce qu'il vous permet d'√©conomiser beaucoup d'argent, mais plut√¥t parce que le CDN devient une partie du service lui-m√™me, affecte directement la qualit√© et la fonctionnalit√©.  Dans de telles circonstances, le donner entre de mauvaises mains est au moins d√©raisonnable. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr425229/">https://habr.com/ru/post/fr425229/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr425219/index.html">Qu'est-ce que la sant√© mentale: une perspective de la psychologie / psychoth√©rapie</a></li>
<li><a href="../fr425221/index.html">Comment fabriquer du plastique pour l'impression 3D</a></li>
<li><a href="../fr425223/index.html">Applications Android JPHP</a></li>
<li><a href="../fr425225/index.html">Comment voir les liens √† l'int√©rieur de votre module PowerShell</a></li>
<li><a href="../fr425227/index.html">Les chercheurs ont trouv√© un moyen de d√©tecter et de contourner les cl√©s Honeytoken dans un certain nombre de services Amazon.</a></li>
<li><a href="../fr425231/index.html">FAQ sur le travail d'une h√¥tesse de l'air</a></li>
<li><a href="../fr425233/index.html">Python 3 sur Facebook</a></li>
<li><a href="../fr425235/index.html">Un peu plus sur les graphiques, ou comment d√©tecter les d√©pendances entre vos applications</a></li>
<li><a href="../fr425237/index.html">Mesure du temps avec une pr√©cision en nanosecondes</a></li>
<li><a href="../fr425241/index.html">D√©veloppeur 20 ans plus tard: Vasily Lebedev sur ICRE, l'√©ducation, son livre et sa programmation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>