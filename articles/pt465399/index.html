<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥓 🤟🏻 👩🏾‍🤝‍👩🏽 Implante o armazenamento distribuído do CEPH e conecte-o ao Kubernetes 🌁 🧣 👦🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1 Implementamos o ambiente para trabalhar com microsserviços. Parte 1 instalando o Kubernetes HA no bare metal (Debian) 
 Olá, queridos leitores...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implante o armazenamento distribuído do CEPH e conecte-o ao Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1 Implementamos o ambiente para trabalhar com microsserviços.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1 instalando o Kubernetes HA no bare metal (Debian)</a> </p><br><h2>  Olá, queridos leitores da Habr! </h2><br><p>  Em uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">postagem</a> anterior <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> falei sobre como implantar um cluster de failover do Kubernetes.  Mas o fato é que no Kubernetes é conveniente implantar aplicativos sem estado que não precisam manter seu estado ou trabalhar com dados.  Mas, na maioria dos casos, precisamos salvar os dados e não perdê-los ao reiniciar os lares. <br>  O Kubernetes usa volumes para esses fins.  Quando trabalhamos com as soluções em nuvem Kubernetes, não há problemas específicos.  Só precisamos solicitar o volume necessário do Google, Amazon ou outro provedor de nuvem e, guiados pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação</a> , conectar os volumes recebidos aos pods. <br>  Quando lidamos com o bare metal, as coisas são um pouco mais complicadas.  Hoje eu quero falar sobre uma das soluções baseadas no uso do ceph. </p><br><p>  Nesta publicação, direi: </p><br><ul><li>  como implantar armazenamento distribuído ceph </li><li>  Como usar o Ceph ao trabalhar com o Kubernetes <a name="habracut"></a></li></ul><br><h2>  1. Introdução </h2><br><p>  Para começar, gostaria de explicar a quem este artigo será útil.  Em primeiro lugar, para os leitores que implantaram o cluster de acordo com minha primeira publicação para continuar a construir uma arquitetura de microsserviço.  Em segundo lugar, para as pessoas que desejam tentar implantar um cluster ceph por conta própria e avaliar seu desempenho. </p><br><p>  Nesta publicação, não abordarei o tópico planejamento de cluster para nenhuma necessidade; falarei apenas sobre princípios e conceitos gerais.  Não vou me aprofundar em "tuning" e deep tuning, existem muitas publicações sobre esse assunto, inclusive no Habr.  O artigo será de natureza mais introdutória, mas, ao mesmo tempo, permitirá que você obtenha uma solução funcional que possa se adaptar às suas necessidades no futuro. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lista de hosts, recursos de host, versões de SO e software</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Estrutura do Cluster Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configurar nós do cluster antes da instalação</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalar ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um cluster ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configuração de rede</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instale pacotes ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalação e inicialização de monitores</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Adicionando OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conectar ceph ao kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um pool de dados</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um segredo do cliente</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Implantar ceph rbd provisioner</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando uma classe de armazenamento</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Teste de ligamentos de Kubernetes + ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lista de materiais utilizados na preparação do artigo</a> </li></ol><br><a name="vm"></a><br><h2>  Lista de hosts e requisitos do sistema </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nome</b> </th><th>  <b>Endereço IP</b> </th><th>  <b>Comente</b> </th></tr><tr><td>  ceph01-test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02-test </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03-test </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Ao escrever um artigo, eu uso máquinas virtuais com esta configuração </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Cada um possui um sistema operacional Debian 9.5 instalado.  São máquinas de teste, cada uma com dois discos, o primeiro para o sistema operacional e o segundo para o OSD Cef </p><br><p>  Implementarei o cluster por meio do utilitário ceph-deploy.  Você pode implantar um cluster ceph no modo manual, todas as etapas estão descritas na documentação, mas o objetivo deste artigo é informar com que rapidez você pode implantar o ceph e começar a usá-lo no kubernetes. <br>  Ceph é bastante guloso para recursos, especialmente RAM.  Para uma boa velocidade, é aconselhável usar unidades ssd. </p><br><p>  Você pode ler mais sobre os requisitos na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação oficial do ceph.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Estrutura do Cluster Ceph </h2><br><p>  <strong>MON</strong> <br>  <em>Um monitor é um daemon que atua como o coordenador a partir do qual o cluster começa.</em>  <em>Assim que temos pelo menos um monitor em funcionamento, temos um cluster Ceph.</em>  <em>O monitor armazena informações sobre a saúde e as condições do cluster trocando vários cartões com outros monitores.</em>  <em>Os clientes recorrem aos monitores para descobrir em qual OSD gravar / ler dados.</em>  <em>Quando você implanta um novo armazenamento, a primeira coisa a fazer é criar um monitor (ou vários).</em>  <em>O cluster pode viver em um monitor, mas é recomendável criar 3 ou 5 monitores para evitar a queda de todo o sistema devido à queda de um único monitor.</em>  <em>O principal é que o número destes deve ser ímpar, a fim de evitar situações de cérebro dividido.</em>  <em>Os monitores funcionam em um quorum; portanto, se mais da metade dos monitores cair, o cluster será bloqueado para evitar inconsistência de dados.</em> <br>  <strong>Mons.</strong> <br>  <em>O daemon do Ceph Manager trabalha com o daemon de monitor para fornecer controle adicional.</em> <em><br></em>  <em>Desde a versão 12.x, o daemon ceph-mgr tornou-se necessário para a operação normal.</em> <em><br></em>  <em>Se o daemon mgr não estiver em execução, você verá um aviso sobre isso.</em> <br>  <strong>OSD (dispositivo de armazenamento de objeto)</strong> <br>  <em>OSD é uma unidade de armazenamento que armazena os dados em si e processa solicitações de clientes trocando dados com outros OSDs.</em>  <em>Isso geralmente é um disco.</em>  <em>E geralmente para cada OSD, existe um daemon OSD separado que pode ser executado em qualquer máquina na qual esse disco está instalado.</em> </p><br><p>  Todos os três daemons funcionarão em cada máquina em nosso cluster.  Assim, monitore e gerencie os daemons como daemons de serviço e OSD para uma unidade de nossa máquina virtual. </p><br><a name="before"></a><br><h2>  Configurar nós do cluster antes da instalação </h2><br><p>  A documentação do ceph especifica o seguinte fluxo de trabalho: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Trabalharei a partir do primeiro nó do cluster ceph01-test, será o Nó Admin, também conterá arquivos de configuração para o utilitário ceph-deploy.  Para que o utilitário ceph-deploy funcione corretamente, todos os nós do cluster devem estar acessíveis via ssh com o nó Admin.  Por conveniência, escreverei nos nomes abreviados dos hosts do cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  E copie as chaves para os outros hosts.  Todos os comandos que executarei a partir do root. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documentação de instalação</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Instalar ceph-deploy </h2><br><p>  A primeira etapa é instalar o ceph-deploy na máquina ceph01-test </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Em seguida, você precisa escolher o lançamento que deseja colocar.  Mas aqui existem dificuldades, atualmente o ceph para o Debian OS suporta apenas pacotes luminosos. <br>  Se você deseja publicar uma versão mais recente, precisará usar um espelho, por exemplo <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Adicione um repositório com imitação nos três nós </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Se luminous é o suficiente para você, você pode usar os repositórios oficiais </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Também instalamos o NTP nos três nós. </p><br><div class="spoiler">  <b class="spoiler_title">uma vez que esta recomendação está na documentação do ceph</b> <div class="spoiler_text"><p>  Recomendamos instalar o NTP nos nós Ceph (especialmente nos nós Ceph Monitor) para evitar problemas decorrentes do desvio do relógio. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Certifique-se de ativar o serviço NTP.  Certifique-se de que cada nó Ceph use o mesmo servidor NTP.  Você pode ver mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> </p><br><a name="ceph-install"></a><br><h2>  Criando um cluster ceph </h2><br><p>  Crie um diretório para arquivos de configuração e arquivos ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Vamos criar uma nova configuração de cluster, ao criar, indicar que haverá três monitores em nosso cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Configuração de rede </h2><br><p>  Agora o ponto importante, é hora de falar sobre a rede para ceph.  O Ceph usa duas redes públicas e uma rede de cluster para trabalhar <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Como você pode ver no diagrama da rede pública, esse é o nível do usuário e do aplicativo, e a rede de cluster é a rede através da qual os dados são replicados. <br>  É altamente desejável separar essas duas redes uma da outra.  Além disso, a rede de cluster de velocidade da rede é desejável pelo menos 10 Gb. <br>  Obviamente, você pode manter tudo na mesma rede.  Mas isso é preocupante porque, assim que o volume de replicação entre OSDs aumenta, por exemplo, quando novos OSDs (discos) caem ou são adicionados, a carga da rede MUITO aumenta.  Portanto, a velocidade e a estabilidade da sua infraestrutura dependerão muito da rede usada pelo ceph. <br>  Infelizmente, meu cluster de virtualização não possui uma rede separada e usarei um segmento de rede comum. <br>  A configuração de rede para o cluster é feita através do arquivo de configuração, que geramos com o comando anterior. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Como podemos ver, o cef deploy não criou as configurações de rede padrão para nós, então adicionarei o parâmetro public network = {public-network / netmask} à seção global da configuração.  Minha rede é 10.73.0.0/16, então, depois de adicionar minha configuração, será assim </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Se você deseja separar a rede do cluster de pública, adicione o parâmetro cluster network = {cluster-network / netmask} <br>  Você <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pode ler</a> mais sobre redes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na documentação.</a> </p><br><a name="ceph-pack"></a><br><h2>  Instale pacotes ceph </h2><br><p>  Usando ceph-deploy, instalamos todos os pacotes ceph necessários em nossos três nós. <br>  Para fazer isso, em ceph01-test, execute <br>  Se a versão for simulada, então </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Se a versão for luminosa, então </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E espere até que tudo esteja estabelecido. </p><br><a name="ceph-mon"></a><br><h2>  Instalação e inicialização de monitores </h2><br><p>  Após a instalação de todos os pacotes, criaremos e iniciaremos os monitores do nosso cluster. <br>  C ceph01-test faça o seguinte </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Os monitores serão criados no processo, os daemons serão lançados e o ceph-deploy verificará o quorum. <br>  Agora espalhe as configurações nos nós do cluster. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E verifique o status do nosso cluster, se você fez tudo corretamente, o status deve ser <br>  HEALTH_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Criar gerente </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E verifique o status novamente </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Uma linha deve aparecer </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Escrevemos a configuração para todos os hosts no cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  Adicionando OSD </h2><br><p>  No momento, temos um cluster em funcionamento, mas ele ainda não possui discos (osd na terminologia ceph) para armazenar informações. </p><br><p>  OSD pode ser adicionado com o seguinte comando (visão geral) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  Na minha mesa de teste, o disco / dev / sdb é alocado no osd, portanto, no meu caso, os comandos serão os seguintes </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  Verifique se todos os OSDs estão funcionando. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Conclusão </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Você também pode tentar alguns comandos úteis para OSD. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  e </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Se estiver tudo bem, temos um cluster ceph em funcionamento.  Na próxima parte, mostrarei como usar o ceph com o kubernetes </p><br><a name="kubernetes"></a><br><h1>  Conectar ceph ao kubernetes </h1><br><p>  Infelizmente, não poderei descrever em detalhes a operação dos volumes Kubernetes neste artigo; portanto, tentarei caber em um parágrafo. <br>  O Kubernetes usa classes de armazenamento para trabalhar com volumes de dados, cada classe de armazenamento possui seu próprio provedor, você pode considerá-lo como um tipo de "driver" para trabalhar com diferentes volumes de armazenamento de dados.  A lista completa que suporta kubernetes pode ser encontrada na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação oficial</a> . <br>  O próprio Kubernetes também tem suporte para trabalhar com rbd, mas não há nenhum cliente rbd instalado na imagem oficial do kube-controller-manager, portanto, você precisa usar uma imagem diferente. <br>  Também deve ser observado que os volumes (pvc) criados como rbd só podem ser ReadWriteOnce (RWO) e, o que significa que você pode montar o volume criado SOMENTE em um coração. </p><br><p>  Para que nosso cluster possa trabalhar com volumes ceph, precisamos: <br>  em um cluster Ceph: </p><br><ul><li>  criar conjunto de dados no cluster ceph </li><li>  criar um cliente e chave de acesso ao conjunto de dados </li><li>  obter ceph admin secret </li></ul><br><p>  Para que nosso cluster possa trabalhar com volumes ceph, precisamos: <br>  em um cluster Ceph: </p><br><ul><li>  criar conjunto de dados no cluster ceph </li><li>  criar um cliente e chave de acesso ao conjunto de dados </li><li>  obter ceph admin secret </li></ul><br><p>  no cluster Kubernetes: </p><br><ul><li>  criar ceph admin secret e ceph client key </li><li>  instale o ceph rbd provisioner ou altere a imagem do kube-controller-manager para uma imagem que suporte rbd </li><li>  criar segredo com a chave do cliente ceph </li><li>  criar classe de armazenamento </li><li>  instalar ceph-common nas notas do trabalhador do kubernetes </li></ul><br><a name="ceph-pool"></a><br><h2>  Criando um pool de dados </h2><br><p>  No cluster ceph, crie um pool para os volumes kubernetes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Aqui vou fazer uma pequena explicação, os números 8 8 no final são os números de pg e pgs.  Esses valores dependem do tamanho do seu cluster ceph.  Existem calculadoras especiais que calculam a quantidade de páginas e páginas, por exemplo, oficiais do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ceph</a> <br>  Para começar, recomendo deixá-lo por padrão, se no futuro esse valor puder ser aumentado (só pode ser reduzido com a versão do Nautilus). </p><br><a name="ceph-key"></a><br><h2>  Criando um cliente para um conjunto de dados </h2><br><p>  Crie um cliente para o novo pool </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Receberemos uma chave para o cliente, no futuro precisaremos dela para criar um kubernetes secreto </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Obtendo a chave do administrador </h2><br><p>  E obtenha a chave de administrador </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>No cluster ceph, todo o trabalho está concluído e agora precisamos ir para uma máquina que tenha acesso ao cluster kubernetes</strong> <br>  Trabalharei com o master01-test (10.73.71.25) do cluster implantado por mim na primeira publicação. </p><br><a name="kubernetes-secrets"></a><br><h2>  Criando um segredo do cliente </h2><br><p>  Crie um arquivo com o token do cliente que recebemos (não esqueça de substituí-lo pelo seu token) </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  E crie um segredo que usaremos no futuro </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Criar segredo do administrador </h2><br><p>  Crie um arquivo com token de administrador (não esqueça de substituí-lo pelo seu token) </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Depois disso, crie um segredo de administrador </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  Verifique se os segredos foram criados </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  Método primeiro implantar ceph rbd provisioner </h2><br><p>  Nós clonamos o repositório kubernetes-incubator / external-storage do github, ele tem tudo o que você precisa para fazer o kubernetes agrupar amigos do repositório ceph. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Conclusão </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  Método 2: Substitua a imagem do kube-controller-manager </h2><br><p>  Não há suporte rbd na imagem oficial do kube-controller-manager, portanto, precisaremos alterar a imagem do controller-manager. <br>  Para fazer isso, em cada um dos assistentes do Kubernetes, você precisa editar o arquivo kube-controller-manager.yaml e substituir a imagem por gcr.io/google_containers/hyperkube:v1.15.2.  Preste atenção na versão da imagem que deve corresponder à sua versão do cluster Kubernetes. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Depois disso, você precisará reiniciar o kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Os pods devem ser atualizados automaticamente, mas, por algum motivo, isso não aconteceu, você pode recriá-los manualmente, através da exclusão. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  Verifique se está tudo bem </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - </p><br><a name="storage-class"></a><br><h2>  Criando uma classe de armazenamento </h2><br><p>  <strong>Método um - se você usou o provisionador ceph.com/rbd</strong> <br>  Crie um arquivo yaml com uma descrição da nossa classe de armazenamento.  Além disso, todos os arquivos usados ​​abaixo podem ser baixados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no meu repositório</a> no diretório ceph </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incorporá-lo em nosso cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  Verifique se está tudo bem </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>Método dois - se você usou o provisionador kubernetes.io/rbd</strong> <br>  Criar classe de armazenamento </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incorporá-lo em nosso cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  Verifique se está tudo bem </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Teste de ligamentos de Kubernetes + ceph </h2><br><p>  Antes de testar o ceph + kubernetes, você deve instalar o pacote ceph-common em TODOS os códigos de trabalho do cluster. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Crie um arquivo yaml PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Mate ele </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  Verifique se PersistentVolumeClaim está criado. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  E também criou automaticamente o PersistentVolume. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Vamos criar um pod de teste no qual incluímos o pvc criado no diretório / mnt.  Execute este arquivo /mnt/test.txt com o texto "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Vamos matá-lo e verificar se ele completou sua tarefa </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Vamos esperar pelo status </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Vamos criar outro, conecte nosso volume a ele, mas já em / mnt / test, e depois verifique se o arquivo criado pelo primeiro volume está no lugar </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Execute o kubectl get po -w e aguarde até que o pod esteja em execução <br>  Depois disso, vamos examinar e verificar se o volume está conectado e nosso arquivo no diretório / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Obrigado por ler até o fim.  Desculpe pelo atraso na postagem. <br>  Estou pronto para responder a todas as perguntas em mensagens pessoais ou nas redes sociais indicadas no meu perfil. <br>  Na próxima publicação pequena, mostrarei como implantar o armazenamento S3 com base no cluster ceph criado e, em seguida, de acordo com o plano da primeira publicação. </p><br><a name="book"></a><br><p>  Materiais utilizados para publicação </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documentação oficial do Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apresentando o repositório Ceph em imagens</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documentação oficial do Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">incubadora kubernetes / de armazenamento externo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Repositório kubernetes-ceph-percona com arquivos de amostra</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt465399/">https://habr.com/ru/post/pt465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt465379/index.html">Controle de bomba de insulina autônomo sem fio caseiro</a></li>
<li><a href="../pt465391/index.html">Seguindo os passos do movimento russo Scala. Parte 1</a></li>
<li><a href="../pt465393/index.html">Energia da bateria para dispositivos MySensors</a></li>
<li><a href="../pt465395/index.html">Qual é a principal diferença entre Injeção de Dependência e Localizador de Serviço?</a></li>
<li><a href="../pt465397/index.html">Como o tradutor Nitro apareceu, o que ajuda os desenvolvedores na localização e suporte técnico</a></li>
<li><a href="../pt465401/index.html">5 atividades para acelerar a resolução de problemas em qualquer equipe de TI</a></li>
<li><a href="../pt465403/index.html">Achtung! Novas câmeras na estrada ou informações atualizadas sobre radares e detectores de radar</a></li>
<li><a href="../pt465407/index.html">1. Visão geral dos switches de camada empresarial extremo</a></li>
<li><a href="../pt465409/index.html">Práticas recomendadas do Vue.js para desenvolvimento Web</a></li>
<li><a href="../pt465415/index.html">Falamos sobre DevOps em uma linguagem compreensível</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>