<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ì ü§üüèª üë©üèæ‚Äçü§ù‚Äçüë©üèΩ Implante o armazenamento distribu√≠do do CEPH e conecte-o ao Kubernetes üåÅ üß£ üë¶üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1 Implementamos o ambiente para trabalhar com microsservi√ßos. Parte 1 instalando o Kubernetes HA no bare metal (Debian) 
 Ol√°, queridos leitores...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Implante o armazenamento distribu√≠do do CEPH e conecte-o ao Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1 Implementamos o ambiente para trabalhar com microsservi√ßos.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1 instalando o Kubernetes HA no bare metal (Debian)</a> </p><br><h2>  Ol√°, queridos leitores da Habr! </h2><br><p>  Em uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">postagem</a> anterior <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">,</a> falei sobre como implantar um cluster de failover do Kubernetes.  Mas o fato √© que no Kubernetes √© conveniente implantar aplicativos sem estado que n√£o precisam manter seu estado ou trabalhar com dados.  Mas, na maioria dos casos, precisamos salvar os dados e n√£o perd√™-los ao reiniciar os lares. <br>  O Kubernetes usa volumes para esses fins.  Quando trabalhamos com as solu√ß√µes em nuvem Kubernetes, n√£o h√° problemas espec√≠ficos.  S√≥ precisamos solicitar o volume necess√°rio do Google, Amazon ou outro provedor de nuvem e, guiados pela <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> , conectar os volumes recebidos aos pods. <br>  Quando lidamos com o bare metal, as coisas s√£o um pouco mais complicadas.  Hoje eu quero falar sobre uma das solu√ß√µes baseadas no uso do ceph. </p><br><p>  Nesta publica√ß√£o, direi: </p><br><ul><li>  como implantar armazenamento distribu√≠do ceph </li><li>  Como usar o Ceph ao trabalhar com o Kubernetes <a name="habracut"></a></li></ul><br><h2>  1. Introdu√ß√£o </h2><br><p>  Para come√ßar, gostaria de explicar a quem este artigo ser√° √∫til.  Em primeiro lugar, para os leitores que implantaram o cluster de acordo com minha primeira publica√ß√£o para continuar a construir uma arquitetura de microsservi√ßo.  Em segundo lugar, para as pessoas que desejam tentar implantar um cluster ceph por conta pr√≥pria e avaliar seu desempenho. </p><br><p>  Nesta publica√ß√£o, n√£o abordarei o t√≥pico planejamento de cluster para nenhuma necessidade; falarei apenas sobre princ√≠pios e conceitos gerais.  N√£o vou me aprofundar em "tuning" e deep tuning, existem muitas publica√ß√µes sobre esse assunto, inclusive no Habr.  O artigo ser√° de natureza mais introdut√≥ria, mas, ao mesmo tempo, permitir√° que voc√™ obtenha uma solu√ß√£o funcional que possa se adaptar √†s suas necessidades no futuro. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lista de hosts, recursos de host, vers√µes de SO e software</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Estrutura do Cluster Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configurar n√≥s do cluster antes da instala√ß√£o</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalar ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um cluster ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configura√ß√£o de rede</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instale pacotes ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instala√ß√£o e inicializa√ß√£o de monitores</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Adicionando OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Conectar ceph ao kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um pool de dados</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando um segredo do cliente</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Implantar ceph rbd provisioner</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Criando uma classe de armazenamento</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Teste de ligamentos de Kubernetes + ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lista de materiais utilizados na prepara√ß√£o do artigo</a> </li></ol><br><a name="vm"></a><br><h2>  Lista de hosts e requisitos do sistema </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nome</b> </th><th>  <b>Endere√ßo IP</b> </th><th>  <b>Comente</b> </th></tr><tr><td>  ceph01-test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02-test </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03-test </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Ao escrever um artigo, eu uso m√°quinas virtuais com esta configura√ß√£o </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Cada um possui um sistema operacional Debian 9.5 instalado.  S√£o m√°quinas de teste, cada uma com dois discos, o primeiro para o sistema operacional e o segundo para o OSD Cef </p><br><p>  Implementarei o cluster por meio do utilit√°rio ceph-deploy.  Voc√™ pode implantar um cluster ceph no modo manual, todas as etapas est√£o descritas na documenta√ß√£o, mas o objetivo deste artigo √© informar com que rapidez voc√™ pode implantar o ceph e come√ßar a us√°-lo no kubernetes. <br>  Ceph √© bastante guloso para recursos, especialmente RAM.  Para uma boa velocidade, √© aconselh√°vel usar unidades ssd. </p><br><p>  Voc√™ pode ler mais sobre os requisitos na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial do ceph.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Estrutura do Cluster Ceph </h2><br><p>  <strong>MON</strong> <br>  <em>Um monitor √© um daemon que atua como o coordenador a partir do qual o cluster come√ßa.</em>  <em>Assim que temos pelo menos um monitor em funcionamento, temos um cluster Ceph.</em>  <em>O monitor armazena informa√ß√µes sobre a sa√∫de e as condi√ß√µes do cluster trocando v√°rios cart√µes com outros monitores.</em>  <em>Os clientes recorrem aos monitores para descobrir em qual OSD gravar / ler dados.</em>  <em>Quando voc√™ implanta um novo armazenamento, a primeira coisa a fazer √© criar um monitor (ou v√°rios).</em>  <em>O cluster pode viver em um monitor, mas √© recomend√°vel criar 3 ou 5 monitores para evitar a queda de todo o sistema devido √† queda de um √∫nico monitor.</em>  <em>O principal √© que o n√∫mero destes deve ser √≠mpar, a fim de evitar situa√ß√µes de c√©rebro dividido.</em>  <em>Os monitores funcionam em um quorum; portanto, se mais da metade dos monitores cair, o cluster ser√° bloqueado para evitar inconsist√™ncia de dados.</em> <br>  <strong>Mons.</strong> <br>  <em>O daemon do Ceph Manager trabalha com o daemon de monitor para fornecer controle adicional.</em> <em><br></em>  <em>Desde a vers√£o 12.x, o daemon ceph-mgr tornou-se necess√°rio para a opera√ß√£o normal.</em> <em><br></em>  <em>Se o daemon mgr n√£o estiver em execu√ß√£o, voc√™ ver√° um aviso sobre isso.</em> <br>  <strong>OSD (dispositivo de armazenamento de objeto)</strong> <br>  <em>OSD √© uma unidade de armazenamento que armazena os dados em si e processa solicita√ß√µes de clientes trocando dados com outros OSDs.</em>  <em>Isso geralmente √© um disco.</em>  <em>E geralmente para cada OSD, existe um daemon OSD separado que pode ser executado em qualquer m√°quina na qual esse disco est√° instalado.</em> </p><br><p>  Todos os tr√™s daemons funcionar√£o em cada m√°quina em nosso cluster.  Assim, monitore e gerencie os daemons como daemons de servi√ßo e OSD para uma unidade de nossa m√°quina virtual. </p><br><a name="before"></a><br><h2>  Configurar n√≥s do cluster antes da instala√ß√£o </h2><br><p>  A documenta√ß√£o do ceph especifica o seguinte fluxo de trabalho: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Trabalharei a partir do primeiro n√≥ do cluster ceph01-test, ser√° o N√≥ Admin, tamb√©m conter√° arquivos de configura√ß√£o para o utilit√°rio ceph-deploy.  Para que o utilit√°rio ceph-deploy funcione corretamente, todos os n√≥s do cluster devem estar acess√≠veis via ssh com o n√≥ Admin.  Por conveni√™ncia, escreverei nos nomes abreviados dos hosts do cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  E copie as chaves para os outros hosts.  Todos os comandos que executarei a partir do root. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o de instala√ß√£o</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Instalar ceph-deploy </h2><br><p>  A primeira etapa √© instalar o ceph-deploy na m√°quina ceph01-test </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Em seguida, voc√™ precisa escolher o lan√ßamento que deseja colocar.  Mas aqui existem dificuldades, atualmente o ceph para o Debian OS suporta apenas pacotes luminosos. <br>  Se voc√™ deseja publicar uma vers√£o mais recente, precisar√° usar um espelho, por exemplo <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Adicione um reposit√≥rio com imita√ß√£o nos tr√™s n√≥s </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Se luminous √© o suficiente para voc√™, voc√™ pode usar os reposit√≥rios oficiais </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Tamb√©m instalamos o NTP nos tr√™s n√≥s. </p><br><div class="spoiler">  <b class="spoiler_title">uma vez que esta recomenda√ß√£o est√° na documenta√ß√£o do ceph</b> <div class="spoiler_text"><p>  Recomendamos instalar o NTP nos n√≥s Ceph (especialmente nos n√≥s Ceph Monitor) para evitar problemas decorrentes do desvio do rel√≥gio. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Certifique-se de ativar o servi√ßo NTP.  Certifique-se de que cada n√≥ Ceph use o mesmo servidor NTP.  Voc√™ pode ver mais detalhes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> </p><br><a name="ceph-install"></a><br><h2>  Criando um cluster ceph </h2><br><p>  Crie um diret√≥rio para arquivos de configura√ß√£o e arquivos ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Vamos criar uma nova configura√ß√£o de cluster, ao criar, indicar que haver√° tr√™s monitores em nosso cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Configura√ß√£o de rede </h2><br><p>  Agora o ponto importante, √© hora de falar sobre a rede para ceph.  O Ceph usa duas redes p√∫blicas e uma rede de cluster para trabalhar <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Como voc√™ pode ver no diagrama da rede p√∫blica, esse √© o n√≠vel do usu√°rio e do aplicativo, e a rede de cluster √© a rede atrav√©s da qual os dados s√£o replicados. <br>  √â altamente desej√°vel separar essas duas redes uma da outra.  Al√©m disso, a rede de cluster de velocidade da rede √© desej√°vel pelo menos 10 Gb. <br>  Obviamente, voc√™ pode manter tudo na mesma rede.  Mas isso √© preocupante porque, assim que o volume de replica√ß√£o entre OSDs aumenta, por exemplo, quando novos OSDs (discos) caem ou s√£o adicionados, a carga da rede MUITO aumenta.  Portanto, a velocidade e a estabilidade da sua infraestrutura depender√£o muito da rede usada pelo ceph. <br>  Infelizmente, meu cluster de virtualiza√ß√£o n√£o possui uma rede separada e usarei um segmento de rede comum. <br>  A configura√ß√£o de rede para o cluster √© feita atrav√©s do arquivo de configura√ß√£o, que geramos com o comando anterior. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Como podemos ver, o cef deploy n√£o criou as configura√ß√µes de rede padr√£o para n√≥s, ent√£o adicionarei o par√¢metro public network = {public-network / netmask} √† se√ß√£o global da configura√ß√£o.  Minha rede √© 10.73.0.0/16, ent√£o, depois de adicionar minha configura√ß√£o, ser√° assim </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Se voc√™ deseja separar a rede do cluster de p√∫blica, adicione o par√¢metro cluster network = {cluster-network / netmask} <br>  Voc√™ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pode ler</a> mais sobre redes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na documenta√ß√£o.</a> </p><br><a name="ceph-pack"></a><br><h2>  Instale pacotes ceph </h2><br><p>  Usando ceph-deploy, instalamos todos os pacotes ceph necess√°rios em nossos tr√™s n√≥s. <br>  Para fazer isso, em ceph01-test, execute <br>  Se a vers√£o for simulada, ent√£o </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Se a vers√£o for luminosa, ent√£o </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E espere at√© que tudo esteja estabelecido. </p><br><a name="ceph-mon"></a><br><h2>  Instala√ß√£o e inicializa√ß√£o de monitores </h2><br><p>  Ap√≥s a instala√ß√£o de todos os pacotes, criaremos e iniciaremos os monitores do nosso cluster. <br>  C ceph01-test fa√ßa o seguinte </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Os monitores ser√£o criados no processo, os daemons ser√£o lan√ßados e o ceph-deploy verificar√° o quorum. <br>  Agora espalhe as configura√ß√µes nos n√≥s do cluster. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E verifique o status do nosso cluster, se voc√™ fez tudo corretamente, o status deve ser <br>  HEALTH_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Criar gerente </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  E verifique o status novamente </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Uma linha deve aparecer </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Escrevemos a configura√ß√£o para todos os hosts no cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  Adicionando OSD </h2><br><p>  No momento, temos um cluster em funcionamento, mas ele ainda n√£o possui discos (osd na terminologia ceph) para armazenar informa√ß√µes. </p><br><p>  OSD pode ser adicionado com o seguinte comando (vis√£o geral) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  Na minha mesa de teste, o disco / dev / sdb √© alocado no osd, portanto, no meu caso, os comandos ser√£o os seguintes </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  Verifique se todos os OSDs est√£o funcionando. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Conclus√£o </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Voc√™ tamb√©m pode tentar alguns comandos √∫teis para OSD. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  e </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Se estiver tudo bem, temos um cluster ceph em funcionamento.  Na pr√≥xima parte, mostrarei como usar o ceph com o kubernetes </p><br><a name="kubernetes"></a><br><h1>  Conectar ceph ao kubernetes </h1><br><p>  Infelizmente, n√£o poderei descrever em detalhes a opera√ß√£o dos volumes Kubernetes neste artigo; portanto, tentarei caber em um par√°grafo. <br>  O Kubernetes usa classes de armazenamento para trabalhar com volumes de dados, cada classe de armazenamento possui seu pr√≥prio provedor, voc√™ pode consider√°-lo como um tipo de "driver" para trabalhar com diferentes volumes de armazenamento de dados.  A lista completa que suporta kubernetes pode ser encontrada na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial</a> . <br>  O pr√≥prio Kubernetes tamb√©m tem suporte para trabalhar com rbd, mas n√£o h√° nenhum cliente rbd instalado na imagem oficial do kube-controller-manager, portanto, voc√™ precisa usar uma imagem diferente. <br>  Tamb√©m deve ser observado que os volumes (pvc) criados como rbd s√≥ podem ser ReadWriteOnce (RWO) e, o que significa que voc√™ pode montar o volume criado SOMENTE em um cora√ß√£o. </p><br><p>  Para que nosso cluster possa trabalhar com volumes ceph, precisamos: <br>  em um cluster Ceph: </p><br><ul><li>  criar conjunto de dados no cluster ceph </li><li>  criar um cliente e chave de acesso ao conjunto de dados </li><li>  obter ceph admin secret </li></ul><br><p>  Para que nosso cluster possa trabalhar com volumes ceph, precisamos: <br>  em um cluster Ceph: </p><br><ul><li>  criar conjunto de dados no cluster ceph </li><li>  criar um cliente e chave de acesso ao conjunto de dados </li><li>  obter ceph admin secret </li></ul><br><p>  no cluster Kubernetes: </p><br><ul><li>  criar ceph admin secret e ceph client key </li><li>  instale o ceph rbd provisioner ou altere a imagem do kube-controller-manager para uma imagem que suporte rbd </li><li>  criar segredo com a chave do cliente ceph </li><li>  criar classe de armazenamento </li><li>  instalar ceph-common nas notas do trabalhador do kubernetes </li></ul><br><a name="ceph-pool"></a><br><h2>  Criando um pool de dados </h2><br><p>  No cluster ceph, crie um pool para os volumes kubernetes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Aqui vou fazer uma pequena explica√ß√£o, os n√∫meros 8 8 no final s√£o os n√∫meros de pg e pgs.  Esses valores dependem do tamanho do seu cluster ceph.  Existem calculadoras especiais que calculam a quantidade de p√°ginas e p√°ginas, por exemplo, oficiais do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ceph</a> <br>  Para come√ßar, recomendo deix√°-lo por padr√£o, se no futuro esse valor puder ser aumentado (s√≥ pode ser reduzido com a vers√£o do Nautilus). </p><br><a name="ceph-key"></a><br><h2>  Criando um cliente para um conjunto de dados </h2><br><p>  Crie um cliente para o novo pool </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Receberemos uma chave para o cliente, no futuro precisaremos dela para criar um kubernetes secreto </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Obtendo a chave do administrador </h2><br><p>  E obtenha a chave de administrador </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>No cluster ceph, todo o trabalho est√° conclu√≠do e agora precisamos ir para uma m√°quina que tenha acesso ao cluster kubernetes</strong> <br>  Trabalharei com o master01-test (10.73.71.25) do cluster implantado por mim na primeira publica√ß√£o. </p><br><a name="kubernetes-secrets"></a><br><h2>  Criando um segredo do cliente </h2><br><p>  Crie um arquivo com o token do cliente que recebemos (n√£o esque√ßa de substitu√≠-lo pelo seu token) </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  E crie um segredo que usaremos no futuro </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Criar segredo do administrador </h2><br><p>  Crie um arquivo com token de administrador (n√£o esque√ßa de substitu√≠-lo pelo seu token) </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Depois disso, crie um segredo de administrador </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  Verifique se os segredos foram criados </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  M√©todo primeiro implantar ceph rbd provisioner </h2><br><p>  N√≥s clonamos o reposit√≥rio kubernetes-incubator / external-storage do github, ele tem tudo o que voc√™ precisa para fazer o kubernetes agrupar amigos do reposit√≥rio ceph. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Conclus√£o </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  M√©todo 2: Substitua a imagem do kube-controller-manager </h2><br><p>  N√£o h√° suporte rbd na imagem oficial do kube-controller-manager, portanto, precisaremos alterar a imagem do controller-manager. <br>  Para fazer isso, em cada um dos assistentes do Kubernetes, voc√™ precisa editar o arquivo kube-controller-manager.yaml e substituir a imagem por gcr.io/google_containers/hyperkube:v1.15.2.  Preste aten√ß√£o na vers√£o da imagem que deve corresponder √† sua vers√£o do cluster Kubernetes. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Depois disso, voc√™ precisar√° reiniciar o kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Os pods devem ser atualizados automaticamente, mas, por algum motivo, isso n√£o aconteceu, voc√™ pode recri√°-los manualmente, atrav√©s da exclus√£o. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  Verifique se est√° tudo bem </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - </p><br><a name="storage-class"></a><br><h2>  Criando uma classe de armazenamento </h2><br><p>  <strong>M√©todo um - se voc√™ usou o provisionador ceph.com/rbd</strong> <br>  Crie um arquivo yaml com uma descri√ß√£o da nossa classe de armazenamento.  Al√©m disso, todos os arquivos usados ‚Äã‚Äãabaixo podem ser baixados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no meu reposit√≥rio</a> no diret√≥rio ceph </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incorpor√°-lo em nosso cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  Verifique se est√° tudo bem </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>M√©todo dois - se voc√™ usou o provisionador kubernetes.io/rbd</strong> <br>  Criar classe de armazenamento </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  E incorpor√°-lo em nosso cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  Verifique se est√° tudo bem </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Teste de ligamentos de Kubernetes + ceph </h2><br><p>  Antes de testar o ceph + kubernetes, voc√™ deve instalar o pacote ceph-common em TODOS os c√≥digos de trabalho do cluster. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Crie um arquivo yaml PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Mate ele </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  Verifique se PersistentVolumeClaim est√° criado. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  E tamb√©m criou automaticamente o PersistentVolume. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Vamos criar um pod de teste no qual inclu√≠mos o pvc criado no diret√≥rio / mnt.  Execute este arquivo /mnt/test.txt com o texto "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Vamos mat√°-lo e verificar se ele completou sua tarefa </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Vamos esperar pelo status </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Vamos criar outro, conecte nosso volume a ele, mas j√° em / mnt / test, e depois verifique se o arquivo criado pelo primeiro volume est√° no lugar </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Execute o kubectl get po -w e aguarde at√© que o pod esteja em execu√ß√£o <br>  Depois disso, vamos examinar e verificar se o volume est√° conectado e nosso arquivo no diret√≥rio / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Obrigado por ler at√© o fim.  Desculpe pelo atraso na postagem. <br>  Estou pronto para responder a todas as perguntas em mensagens pessoais ou nas redes sociais indicadas no meu perfil. <br>  Na pr√≥xima publica√ß√£o pequena, mostrarei como implantar o armazenamento S3 com base no cluster ceph criado e, em seguida, de acordo com o plano da primeira publica√ß√£o. </p><br><a name="book"></a><br><p>  Materiais utilizados para publica√ß√£o </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o oficial do Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apresentando o reposit√≥rio Ceph em imagens</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Documenta√ß√£o oficial do Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">incubadora kubernetes / de armazenamento externo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Reposit√≥rio kubernetes-ceph-percona com arquivos de amostra</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt465399/">https://habr.com/ru/post/pt465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt465379/index.html">Controle de bomba de insulina aut√¥nomo sem fio caseiro</a></li>
<li><a href="../pt465391/index.html">Seguindo os passos do movimento russo Scala. Parte 1</a></li>
<li><a href="../pt465393/index.html">Energia da bateria para dispositivos MySensors</a></li>
<li><a href="../pt465395/index.html">Qual √© a principal diferen√ßa entre Inje√ß√£o de Depend√™ncia e Localizador de Servi√ßo?</a></li>
<li><a href="../pt465397/index.html">Como o tradutor Nitro apareceu, o que ajuda os desenvolvedores na localiza√ß√£o e suporte t√©cnico</a></li>
<li><a href="../pt465401/index.html">5 atividades para acelerar a resolu√ß√£o de problemas em qualquer equipe de TI</a></li>
<li><a href="../pt465403/index.html">Achtung! Novas c√¢meras na estrada ou informa√ß√µes atualizadas sobre radares e detectores de radar</a></li>
<li><a href="../pt465407/index.html">1. Vis√£o geral dos switches de camada empresarial extremo</a></li>
<li><a href="../pt465409/index.html">Pr√°ticas recomendadas do Vue.js para desenvolvimento Web</a></li>
<li><a href="../pt465415/index.html">Falamos sobre DevOps em uma linguagem compreens√≠vel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>