<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçª üåé ü§¢ Criando um bot para participar do mini cup AI 2018 com base em uma rede neural recorrente (parte 2) ‚ùé üåö ü¶à</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esta √© uma continua√ß√£o da primeira parte do artigo. 


 Na primeira parte do artigo, o autor falou sobre as condi√ß√µes do concurso para o jogo Agario e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Criando um bot para participar do mini cup AI 2018 com base em uma rede neural recorrente (parte 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417657/"><p><img src="https://habrastorage.org/webt/of/tw/ke/oftwke-wbh5-w_nlsm_p5rmhano.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Esta √© uma continua√ß√£o da primeira parte do artigo.</a> </p><br><p> Na primeira parte do artigo, o autor falou sobre as condi√ß√µes do concurso para o jogo Agario em mail.ru, a estrutura do mundo do jogo e parcialmente sobre a estrutura do bot.  Em parte, porque eles afetaram apenas o dispositivo de sensores de entrada e comandos na sa√≠da da rede neural (a seguir, em imagens e texto, haver√° uma abrevia√ß√£o NN).  Ent√£o, vamos tentar abrir a caixa preta e entender como tudo est√° organizado l√°. </p><a name="habracut"></a><br><p>  E aqui est√° a primeira foto: </p><br><p><img src="https://habrastorage.org/webt/v2/pb/_s/v2pb_sha05gqjtqcqp9bv5mnm0o.jpeg"></p><br><p>  Descreve esquematicamente o que deve causar um sorriso entediado ao meu leitor, dizem novamente na primeira s√©rie, que foram vistos muitas vezes em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">v√°rias fontes</a> .  Mas realmente queremos aplicar praticamente essa imagem ao gerenciamento do bot, portanto, ap√≥s a Nota importante, vamos dar uma olhada mais de perto. </p><br><p>  <strong>Nota importante:</strong> h√° um grande n√∫mero de solu√ß√µes prontas (estruturas) para trabalhar com redes neurais: </p><br><p><img src="https://habrastorage.org/webt/jt/il/97/jtil97rhtusvazj6iared1q_hnc.png"></p><br><p>  Todos esses pacotes resolvem as principais tarefas do desenvolvedor de redes neurais: a constru√ß√£o e o treinamento de NN ou a busca de pesos "ideais".  E o principal m√©todo desta pesquisa √© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Backpropagation</a> .  Foi inventado nos anos 70 do s√©culo passado, como indicado no artigo no link acima, durante esse per√≠odo, como parte inferior do navio, adquiriu v√°rias melhorias, mas a ess√™ncia √© a mesma: encontrar coeficientes de peso com base em exemplos de treinamento e √© altamente desej√°vel que todos desses exemplos continha uma resposta pronta na forma de um sinal de sa√≠da de uma rede neural.  O leitor pode se opor a mim.  que redes de autoaprendizagem de v√°rias classes e princ√≠pios j√° foram inventadas, mas tudo n√£o est√° indo bem l√°, at√© onde eu entendo.  Obviamente, existem planos para estudar esse zool√≥gico com mais detalhes, mas acho que vou encontrar pessoas que pensam que uma bicicleta feita de bricolage, mesmo a mais curva, est√° mais pr√≥xima do cora√ß√£o do criador do que um clone transportador de uma bicicleta ideal. <br>  Entendendo que o servidor do jogo provavelmente n√£o ter√° essas bibliotecas e o poder de computa√ß√£o alocado pelos organizadores como um n√∫cleo de processador claramente n√£o √© suficiente para uma estrutura pesada, o autor continuou a criar sua pr√≥pria bicicleta.  Um coment√°rio importante sobre isso terminou. </p><br><p>  Vamos voltar √† imagem que descreve provavelmente a mais simples das redes neurais poss√≠veis com uma camada oculta (tamb√©m conhecida como camada oculta ou camada oculta).  Agora, o pr√≥prio autor tem contemplado constantemente a imagem com id√©ias neste exemplo simples para revelar ao leitor as profundezas das redes neurais artificiais.  Quando tudo √© simplificado para um primitivo, √© mais f√°cil entender a ess√™ncia.  A conclus√£o √© que o neur√¥nio da camada oculta n√£o tem nada para resumir.  E provavelmente essa nem √© uma rede neural. Nos livros did√°ticos, a NN mais simples √© uma rede com duas entradas.  Ent√£o aqui estamos n√≥s, por assim dizer, os descobridores das redes mais simples. </p><br><p>  Vamos tentar descrever essa rede neural (pseudoc√≥digo): <br>  Introduzimos a topologia de rede na forma de uma matriz, em que cada elemento corresponde √† camada e ao n√∫mero de neur√¥nios nela: </p><br><p><code>int array Topology= { 1, 1, 1}</code> <br>  Tamb√©m precisamos de uma matriz flutuante de pesos da rede neural W, considerando nossa rede como "redes neurais de avan√ßo de alimenta√ß√£o (FF ou FFNN)", onde cada neur√¥nio da camada atual est√° conectado a cada neur√¥nio da pr√≥xima camada, obtemos a dimens√£o da matriz W [n√∫mero de camadas , o n√∫mero de neur√¥nios na camada, o n√∫mero de neur√¥nios na camada].  N√£o √© a codifica√ß√£o ideal, mas, considerando o h√°lito quente da GPU em algum lugar muito pr√≥ximo do texto, √© compreens√≠vel. <br>  Um breve procedimento <code>CalculateSize</code> para contar o n√∫mero de neur√¥nios de <code>neuroncount</code> e o n√∫mero de suas conex√µes na rede neural de <code>dendritecount</code> , acho que explicar√° melhor ao autor a natureza dessas conex√µes: </p><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span> CalculateSize(<span class="hljs-keyword"><span class="hljs-keyword">array</span></span> <span class="hljs-type"><span class="hljs-type">int</span></span> Topology, <span class="hljs-type"><span class="hljs-type">int</span></span> neuroncount, <span class="hljs-type"><span class="hljs-type">int</span></span> dendritecount) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i : Topology) // i         neuroncount += i; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt;Topology.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>, i &lt; Topology[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, i++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> j = <span class="hljs-number"><span class="hljs-number">0</span></span>, j &lt; Topology[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>], j++) //   dendritecount++; }</code> </pre> <br><p>  Meu leitor, aquele que j√° sabe tudo isso, o autor chegou a essa opini√£o no primeiro artigo, certamente n√£o perguntar√°: por que no terceiro loop aninhado Topologia [camada1 + 1] em vez de Topologia [camada1], que d√° mais ao neur√¥nio do que na topologia de rede .  Eu n√£o vou responder  Tamb√©m √© √∫til para o leitor pedir li√ß√£o de casa. </p><br><p>  Estamos quase a um passo de construir uma rede neural funcional.  Resta acrescentar a fun√ß√£o de somar os sinais na entrada do neur√¥nio e sua ativa√ß√£o.  Existem muitas fun√ß√µes de ativa√ß√£o, mas as mais pr√≥ximas da natureza do neur√¥nio s√£o Sigmoide e Tangens√≥ide <em>(provavelmente √© melhor cham√°-lo assim, embora esse nome n√£o seja usado na literatura, o m√°ximo √© tangente, mas esse √© o nome do gr√°fico, embora o gr√°fico seja um reflexo da fun√ß√£o?)</em> </p><br><p>  Ent√£o aqui temos as fun√ß√µes de ativa√ß√£o dos neur√¥nios (elas est√£o presentes na figura, na parte inferior) </p><br><pre> <code class="hljs go">float Sigmoid(float x) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (float)(<span class="hljs-number"><span class="hljs-number">1.0f</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0f</span></span> + expf(-x))); }</code> </pre> <br><p>  Sigmoid retorna valores de 0 a 1. </p><br><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Tanh</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)(tanhf(x)); }</code> </pre> <br><p>  O tangentoide retorna valores de -1 a 1. </p><br><p>  A ideia principal de um sinal que passa atrav√©s de uma rede neural √© uma onda: um sinal √© alimentado para neur√¥nios de entrada -&gt; atrav√©s de conex√µes neurais o sinal vai para a segunda camada -&gt; neur√¥nios da segunda camada resumem os sinais que os atingiram alterados por pesos interneuronais -&gt; √© adicionado atrav√©s de um peso de polariza√ß√£o adicional -&gt; usamos a fun√ß√£o de ativa√ß√£o - e todos vamos para a pr√≥xima camada (leia o primeiro ciclo do exemplo por camadas), isto √©, repetindo a cadeia desde o in√≠cio, apenas os neur√¥nios da pr√≥xima camada se tornar√£o neur√¥nios de entrada.  Na simplifica√ß√£o, voc√™ nem precisa armazenar os valores dos neur√¥nios de toda a rede, apenas os pesos da NN e os valores dos neur√¥nios da camada ativa. </p><br><p>  Mais uma vez, enviamos um sinal para a entrada NN, a onda percorreu as camadas e na camada de sa√≠da removemos o valor obtido. </p><br><p>  Aqui, pelo gosto do leitor, √© poss√≠vel resolver programaticamente usando recurs√£o ou apenas um ciclo triplo como o do autor, para acelerar os c√°lculos, voc√™ n√£o precisa cercar objetos na forma de neur√¥nios e suas conex√µes e outras OOP.  Novamente, isso ocorre devido √† sensa√ß√£o de c√°lculos pr√≥ximos da GPU, e nas GPUs, devido √† natureza do paralelismo em massa, a OOP fica um pouco paralisada, isso √© relativo a c # e C ++. </p><br><p>  Al√©m disso, o leitor √© convidado a seguir o caminho da constru√ß√£o de uma rede neural em c√≥digo, com o seu voluntariamente querendo, cuja aus√™ncia √© bastante clara e familiar para o autor, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nos exemplos de cria√ß√£o de NN a</a> partir do zero, h√° muitos exemplos na rede, por isso ser√° dif√≠cil se perder, √© assim t√£o simples quanto uma rede neural de distribui√ß√£o direta na figura acima. </p><br><p>  Mas onde o leitor exclamar√°, que ainda n√£o se afastou da passagem anterior, e ter√° raz√£o, na inf√¢ncia, o autor determinou o valor do livro atrav√©s de ilustra√ß√µes.  Aqui est√° voc√™: </p><br><p><img src="https://habrastorage.org/webt/03/jl/dw/03jldwzryoeaxscxxtfi3deakie.jpeg"></p><br><p>  Na figura, vemos um neur√¥nio recorrente e um NN constru√≠do a partir desses neur√¥nios √© chamado de recorrente ou RNN.  A rede neural especificada possui uma mem√≥ria de curto prazo e foi selecionada pelo autor para o bot como a mais promissora em termos de adapta√ß√£o ao processo do jogo.  Obviamente, o autor construiu uma rede neural de distribui√ß√£o direta, mas, no processo de busca de uma solu√ß√£o "eficaz", mudou para a RNN. </p><br><p>  Um neur√¥nio recorrente possui um estado adicional C, que √© formado ap√≥s a primeira passagem de um sinal atrav√©s de um neur√¥nio, Tick + 0 na linha do tempo.  Em palavras simples, esta √© uma c√≥pia do sinal de sa√≠da de um neur√¥nio.  Na segunda etapa, leia Tick + 1 (uma vez que a rede opera na frequ√™ncia do bot e do servidor do jogo), o valor C retorna √† entrada da camada neural atrav√©s de pesos adicionais e, assim, participa da forma√ß√£o do sinal, mas j√° no momento do Tick + 1. </p><br><p>  <em>Nota: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no trabalho de grupos de pesquisa sobre o gerenciamento de bots de jogos NN</a> , h√° uma tend√™ncia de usar dois ritmos para uma rede neural, um ritmo √© a frequ√™ncia do jogo Tick, o segundo ritmo, por exemplo, √© duas vezes mais lento que o primeiro.</em>  <em>Diferentes partes do NN operam em diferentes frequ√™ncias, o que fornece uma vis√£o diferente da situa√ß√£o do jogo dentro do NN, aumentando assim sua flexibilidade.</em> </p><br><p>  Para criar RNN no c√≥digo bot, introduzimos uma matriz adicional na topologia, onde cada elemento corresponde √† camada e ao n√∫mero de estados neurais nela: </p><br><p> <code>int array TopologyNN= { numberofSensors, 16, 8, 4}</code> <br> <code>int array TopologyRNN= { 0, 16, 0, 0 }</code> </p> <br><p>  Pode-se observar pela topologia acima que a segunda camada √© recorrente, uma vez que cont√©m estados neurais.  Tamb√©m introduzimos pesos adicionais na forma de um flutuador da matriz WRR, a mesma dimens√£o que a matriz W. </p><br><p>  A contagem de conex√µes em nossa rede neural mudar√° um pouco: </p><br><pre> <code class="hljs matlab"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> &lt; TopologyNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> &lt; TopologyNN[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>] , <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyRNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>&lt; TopologyRNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>&lt; TopologyRNN[layer], <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++;</code> </pre> <br><p>  O autor anexar√° o c√≥digo geral para uma rede neural recorrente no final deste artigo, mas o principal a entender √© o princ√≠pio: a passagem de uma onda atrav√©s de camadas no caso de um NN recorrente n√£o altera nada fundamentalmente, apenas mais um termo √© adicionado √† fun√ß√£o de ativa√ß√£o do neur√¥nio.  Este √© o termo do estado dos neur√¥nios no Tick anterior multiplicado pelo peso da conex√£o neural. </p><br><p>  Assumimos que a teoria e a pr√°tica das redes neurais foram atualizadas, mas o autor est√° claramente ciente de que n√£o trouxe o leitor mais perto de entender como ensinar essa estrutura simples de redes neurais a tomar decis√µes na jogabilidade.  N√£o temos bibliotecas com exemplos para ensinar NN.  Nos grupos de desenvolvedores de bot na Internet, havia uma opini√£o: forne√ßa um arquivo de log na forma de coordenadas de bots e outras informa√ß√µes do jogo para formar uma biblioteca de exemplos.  Mas o autor, infelizmente, n√£o conseguiu descobrir como usar esse arquivo de log para treinar NN.  Ficarei feliz em discutir isso nos coment√°rios do artigo.  Portanto, o √∫nico m√©todo dispon√≠vel ao autor para entender o m√©todo de treinamento, ou melhor, encontrar neurobalan√ßas "efetivas" (neuroconnections), foi o algoritmo gen√©tico. </p><br><p>  Preparou uma imagem sobre os princ√≠pios do algoritmo gen√©tico: </p><br><p><img src="https://habrastorage.org/webt/-3/ex/ls/-3exlspxldh64avmkbi_3vuewou.jpeg"></p><br><p>  Ent√£o, o <strong>algoritmo gen√©tico</strong> . </p><br><p>  O autor tentar√° n√£o se aprofundar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na teoria desse processo</a> , mas lembrar√° apenas o m√≠nimo necess√°rio para continuar uma leitura completa do artigo. <br>  No algoritmo gen√©tico, o principal fluido de trabalho √© o gene (DNA √© o nome da mol√©cula).  O genoma, no nosso caso, √© um conjunto seq√ºencial de genes ou uma matriz unidimensional de float long ... </p><br><p>  No est√°gio inicial do trabalho com uma rede neural rec√©m-constru√≠da, √© necess√°rio inicializ√°-la.  Inicializa√ß√£o refere-se √† atribui√ß√£o de valores aleat√≥rios de -1 a 1. Os balan√ßos neurais.O autor mencionou que o intervalo de valores de -1 a 1 √© muito <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">extremo</a> e as redes treinadas t√™m pesos em um intervalo menor, por exemplo, de -0,5 a 0,5 e que um intervalo de valores inicial deve ser aceito como excelente de -1 a 1. Mas seguiremos o caminho cl√°ssico de coletar todas as dificuldades em um port√£o e tomaremos o segmento mais amplo poss√≠vel de vari√°veis ‚Äã‚Äãaleat√≥rias iniciais como base para inicializar a rede neural. </p><br><p>  Agora uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bije√ß√£o</a> ocorrer√°.  Assumiremos que o comprimento (tamanho) do genoma do bot ser√° igual ao comprimento total das matrizes da rede neural TopologyNN.Length + TopologyRNN.Length n√£o √© √† toa que o autor gastou o tempo do leitor no procedimento de contagem de conex√µes neurais. </p><br><p>  <em>Nota: Como o leitor j√° observou por si pr√≥prio, transferimos apenas os pesos da rede neural para o gen√≥tipo, a estrutura da conex√£o, as fun√ß√µes de ativa√ß√£o e os estados dos neur√¥nios n√£o s√£o transmitidos.</em>  <em>Para um algoritmo gen√©tico, apenas as conex√µes neurais s√£o suficientes, o que sugere que elas s√£o portadoras de informa√ß√µes.</em>  <em>H√° desenvolvimentos em que o algoritmo gen√©tico tamb√©m altera a estrutura das conex√µes na rede neural e √© bastante simples implement√°-lo.</em>  <em>Aqui, o autor deixa espa√ßo para a criatividade do leitor, embora ele pr√≥prio pense nisso com interesse: voc√™ precisa entender usando dois genomas independentes e duas fun√ß√µes de condicionamento f√≠sico (dois algoritmos gen√©ticos independentes simplificados) ou todos podem usar o mesmo gene e algoritmo.</em> </p><br><p>  E como inicializamos o NN com vari√°veis ‚Äã‚Äãaleat√≥rias, inicializamos o genoma.  O processo inverso tamb√©m √© poss√≠vel: inicializa√ß√£o do gen√≥tipo por vari√°veis ‚Äã‚Äãaleat√≥rias e sua posterior c√≥pia em pesos neurais.  A segunda op√ß√£o √© comum.  Como o algoritmo gen√©tico no programa geralmente existe √† parte da pr√≥pria ess√™ncia e √© associado a ele apenas pelos dados do genoma e pelo valor da fun√ß√£o de condicionamento f√≠sico ... Pare, pare, o leitor dir√° que a imagem mostra claramente a popula√ß√£o e n√£o uma palavra sobre o genoma individual. </p><br><p>  Ok, adicione algumas fotos ao forno da mente do leitor: </p><br><p><img src="https://habrastorage.org/webt/h5/dz/ho/h5dzhojhlf6b1xqsol30qo63mdo.jpeg"></p><br><p>  Como o autor pintou as figuras antes de escrever o texto do artigo, elas ap√≥iam o texto, mas n√£o seguem a letra da letra da hist√≥ria atual. </p><br><p>  A partir das informa√ß√µes extra√≠das, conclui-se que o principal corpo de trabalho do algoritmo gen√©tico √© uma popula√ß√£o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">genomas</a> .  Isso √© um pouco contr√°rio ao que o autor disse anteriormente, mas como no mundo real fazer sem pequenas contradi√ß√µes.  Ontem, o sol girou em torno da Terra e hoje o autor fala sobre a rede neural dentro do bot de software.  N√£o √© √† toa que ele se lembrou do forno da raz√£o. <br>  Confio no pr√≥prio leitor para resolver a quest√£o das contradi√ß√µes do mundo.  O mundo bot √© completamente auto-suficiente para o artigo. </p><br><p>  Mas o que o autor j√° conseguiu fazer, nesta parte do artigo, √© formar uma popula√ß√£o de bots. <br>  Vamos dar uma olhada no lado do software: </p><br><p>  Existe um bot (pode ser um objeto no OOP, uma estrutura, embora provavelmente seja tamb√©m um objeto ou apenas uma matriz de dados).  Por dentro, o Bot cont√©m informa√ß√µes sobre suas coordenadas, velocidade, massa e outras informa√ß√µes √∫teis no processo do jogo, mas o principal para n√≥s agora √© que ele cont√©m um link para seu gen√≥tipo ou o pr√≥prio gen√≥tipo, dependendo da implementa√ß√£o.  Depois, voc√™ pode seguir de diferentes maneiras, limitar-se a matrizes de pesos de redes neurais ou introduzir uma variedade adicional de gen√≥tipos, pois ser√° conveniente para o leitor imaginar isso em sua imagina√ß√£o.  Nas primeiras etapas, o autor do programa alocou matrizes de neurobalan√ßas e gen√≥tipos.  Ent√£o ele se recusou a duplicar informa√ß√µes e se limitou aos pesos da rede neural. </p><br><p>  Seguindo a l√≥gica da hist√≥ria, voc√™ precisa dizer que a popula√ß√£o de bots √© uma matriz dos bots acima.  Que ciclo de jogo ... Pare novamente, que ciclo de jogo?  os desenvolvedores educadamente forneceram um lugar para apenas um bot a bordo de um programa de simula√ß√£o de mundo de jogo em um servidor ou no m√°ximo quatro bots em um simulador local.  E se voc√™ se lembra da topologia da rede neural escolhida pelo autor: </p><br><p><img src="https://habrastorage.org/webt/vh/1d/xq/vh1dxqmwqoejzszyh3zpfyykve4.jpeg"></p><br><p>  E para simplificar a hist√≥ria, suponha que o gen√≥tipo contenha aproximadamente 1000 conex√µes neurais, a prop√≥sito, no simulador, os gen√≥tipos se parecem com isso (vermelho √© um valor gen√©tico negativo, verde √© um valor positivo, cada linha √© um genoma separado): </p><br><p><img src="https://habrastorage.org/webt/ri/h0/s-/rih0s-ss3gaflldpts95rm9yo4u.jpeg"></p><br><p>  <em>Nota para a foto: com o tempo, o padr√£o muda na dire√ß√£o da predomin√¢ncia de uma das solu√ß√µes, as listras verticais s√£o genes genot√≠picos comuns.</em> </p><br><p>  Portanto, temos 1000 genes no gen√≥tipo e um m√°ximo de quatro bots no programa de simulador de mundo de jogo dos organizadores da competi√ß√£o.  Quantas vezes voc√™ precisa executar uma simula√ß√£o de uma batalha de bots para que pela for√ßa bruta, mesmo a mais inteligente, se aproxime em busca de "eficaz" <br>  gen√≥tipo, leia a combina√ß√£o "eficaz" de conex√µes neurais, desde que cada conex√£o neural varie de -1 a 1 em etapas, e qual etapa?  inicializa√ß√£o foi aleat√≥ria float, √© de 15 casas decimais.  O passo ainda n√£o est√° claro para n√≥s.  No n√∫mero de variantes de combina√ß√µes de pesos neurais, o autor assume que esse √© um n√∫mero infinito, ao escolher um determinado tamanho de etapa, provavelmente um n√∫mero finito, mas, em qualquer caso, esses n√∫meros s√£o muito mais do que 4 lugares no simulador, mesmo considerando o lan√ßamento sequencial da fila de bots e o lan√ßamento paralelo simult√¢neo de simuladores oficiais, at√© 10 em um computador (para f√£s de programa√ß√£o vintage: computadores). </p><br><p><img src="https://habrastorage.org/webt/it/-8/if/it-8ifszotccnx4wguexdtditpm.jpeg"></p><br><p>  Espero que as fotos ajudem o leitor. </p><br><p>  Aqui voc√™ precisa pausar e falar sobre a arquitetura da solu√ß√£o de software.  Como a solu√ß√£o na forma de um bot de software separado, carregada no site da competi√ß√£o, n√£o era mais adequada.  Era necess√°rio separar o bot playing de acordo com as regras da competi√ß√£o no √¢mbito do ecossistema de organizadores e o programa tentando encontrar a configura√ß√£o da rede neural para ele.  O diagrama a seguir √© retirado da apresenta√ß√£o da confer√™ncia, mas geralmente reflete a imagem real. </p><br><p><img src="https://habrastorage.org/webt/-i/hb/ph/-ihbph2b0hlv3lfzxze7ihmu3-q.jpeg"></p><br><p>  Ele lembrou uma piada de barba: </p><br><p>  <em>Grande organiza√ß√£o.</em> <em><br></em>  <em>Hor√°rio 18:00, todos os funcion√°rios trabalham como um.</em>  <em>De repente, um dos funcion√°rios desliga o computador, se veste e sai.</em> <em><br></em>  <em>Todo mundo o segue com um olhar surpreso.</em> <em><br></em>  <em>Dia que vem</em>  <em>√Äs 18h00, o mesmo funcion√°rio desliga o computador e sai.</em>  <em>Todo mundo continua trabalhando e come√ßa a sussurrar em desagrado.</em> <em><br></em>  <em>No dia seguinte</em>  <em>√Äs 18h00, o mesmo funcion√°rio desliga o computador ...</em> <em><br></em>  <em>Um colega se aproxima dele:</em> <em><br></em>  <em>-Como voc√™ n√£o tem vergonha, estamos trabalhando, no final do trimestre, tantos relat√≥rios, tamb√©m queremos chegar em casa na hora e voc√™ √© um indiv√≠duo ...</em> <em><br></em>  <em>- Pessoal, eu geralmente estou de f√©rias!</em> </p><br><p>  ... para ser continuado. </p><br><p>  Sim, quase esqueci de anexar o c√≥digo do procedimento de c√°lculo da RNN, ele √© v√°lido e escrito de forma independente, portanto, talvez haja erros nele.  Para amplifica√ß√£o, mostrarei como est√°, em c ++, conforme aplicado ao CUDA (uma biblioteca para c√°lculo na GPU). </p><br><p>  Nota: matrizes multidimensionais n√£o se d√£o bem em GPUs, √© claro que existem texturas e c√°lculos de matriz, mas eles recomendam o uso de matrizes unidimensionais. </p><br><p>  Um exemplo: uma matriz [i, j] da dimens√£o M por j se transforma em uma matriz da forma [i * M + j]. </p><br><div class="spoiler">  <b class="spoiler_title">C√≥digo fonte do procedimento de c√°lculo RNN</b> <div class="spoiler_text"><pre> <code class="hljs powershell">__global__ void cudaRNN(Bot *bot, argumentsRNN *RNN, ConstantStruct *Const, int *Topology, int *TopologyRNN, int numElements, int gameTick) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int threadN = gridDim.x * blockDim.x; int TopologySize = Const-&gt;TopologySize; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int pos = tid; pos &lt; numElements; pos += threadN) { const int ii = pos; const int iiA = pos*Const-&gt;ArrayDim; int ArrayDim = Const-&gt;ArrayDim; const int iiAT = ii*TopologySize*ArrayDim; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].TTF != <span class="hljs-number"><span class="hljs-number">0</span></span> &amp;&amp; bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].Mass&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; //bias int neuroncount7 = Topology[<span class="hljs-number"><span class="hljs-number">0</span></span>]; neuroncount7++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; layer1 &lt; TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>; layer1++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j4 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j4 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; j4++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i5 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i5 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] + <span class="hljs-number"><span class="hljs-number">1</span></span>; i5++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] = RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] + RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>] * RNN-&gt;NNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>]; } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (TopologyRNN[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j14 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j14 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; j14++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i15 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i15 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; i15++) { RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>] * RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; } RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + <span class="hljs-number"><span class="hljs-number">1.0</span></span>f* RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span>])*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; //bias=<span class="hljs-number"><span class="hljs-number">1</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int t = <span class="hljs-number"><span class="hljs-number">0</span></span>; t &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; t++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = Tanh(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] + RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]); RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]; } //SoftMax /* double sum = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt;ArrayDim; ++k) sum += exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt; ArrayDim; ++k) RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>] = exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]) / sum; */ } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i1 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; i1++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>] = Sigmoid(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>]); //sigma } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (layer1 + <span class="hljs-number"><span class="hljs-number">1</span></span> != TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i2 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i2 &lt; ArrayDim; i2++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; } } } } }</code> </pre> </div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt417657/">https://habr.com/ru/post/pt417657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt417645/index.html">Concurso de programa√ß√£o: negocia√ß√£o (resultados intermedi√°rios e an√∫ncios)</a></li>
<li><a href="../pt417647/index.html">Um projeto de lei sobre a prote√ß√£o de dados pessoais apresentado na Bielorr√∫ssia - o que est√° ‚Äúdentro‚Äù dele</a></li>
<li><a href="../pt417649/index.html">OpenAI supera limita√ß√µes significativas de IA para Dota 2</a></li>
<li><a href="../pt417653/index.html">Li√ß√£o aberta "Conceitos b√°sicos de bancos de dados"</a></li>
<li><a href="../pt417655/index.html">Hist√≥rico: Roscosmos State Corporation e seu trabalho</a></li>
<li><a href="../pt417659/index.html">Neuropoet e outras estrelas pop do futuro</a></li>
<li><a href="../pt417661/index.html">Aubrey de Grey visitando Joe Rogan</a></li>
<li><a href="../pt417665/index.html">Gram√°tica inglesa como matem√°tica. Por onde come√ßar para quem n√£o deu certo</a></li>
<li><a href="../pt417667/index.html">AI. Rastreador de Barreira T√°tica</a></li>
<li><a href="../pt417671/index.html">Novos recursos da linguagem de programa√ß√£o ABAP em webinars da SAP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>