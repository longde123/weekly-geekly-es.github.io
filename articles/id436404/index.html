<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕗 🏡 🧞 Penyeimbangan lalu lintas VoIP yang toleran. Memuat peralihan antar pusat data pada waktu puncak 🕧 🤽🏾 🚓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Beberapa kata tentang apa yang kami lakukan. DINS terlibat dalam pengembangan dan dukungan layanan UCaaS di pasar internasional untuk klien korporat. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penyeimbangan lalu lintas VoIP yang toleran. Memuat peralihan antar pusat data pada waktu puncak</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dins/blog/436404/"><p>  Beberapa kata tentang apa yang kami lakukan.  DINS terlibat dalam pengembangan dan dukungan layanan UCaaS di pasar internasional untuk klien korporat.  Layanan ini digunakan oleh perusahaan kecil dan pemula, dan bisnis besar.  Klien terhubung melalui Internet melalui protokol SIP melalui TCP, TLS atau WSS.  Ini menciptakan beban yang agak besar: hampir 1,5 juta koneksi dari perangkat akhir - telepon Polycom / Cisco / Yealink dan klien perangkat lunak untuk PC / Mac / IOS / Android. </p><br><p>  Pada artikel ini, saya berbicara tentang bagaimana titik masuk VoIP diatur. </p><a name="habracut"></a><br><h3 id="predystoriya">  Latar belakang </h3><br><p>  Pada perimeter sistem (antara perangkat terminal dan kernel) adalah SBC komersial (Session Border Controller). </p><br><p>  Sejak 2012, kami telah menggunakan solusi dari Acme Packet, yang kemudian diakuisisi oleh Oracle.  Sebelum itu, kami menggunakan NatPASS. </p><br><p>  Daftarkan secara singkat fungsionalitas yang kami gunakan: </p><br><p>  • NAT traversal; <br>  • B2BUA; <br>  • Normalisasi SIP (header diizinkan / tidak diizinkan, aturan manipulasi header, dll) <br>  • Pelepasan TLS &amp; SRTP; <br>  • Konversi transportasi (di dalam sistem kami menggunakan SIP melalui UDP); <br>  • Pemantauan MOS (via RTCP-XR); <br>  • ACL, deteksi Bruteforce; <br>  • Mengurangi lalu lintas pendaftaran karena peningkatan kedaluwarsa kontak (kedaluwarsa rendah di sisi akses, tinggi di sisi inti); <br>  • Per-Metode SIP mencekik pesan. </p><br><p> Sistem komersial memiliki nilai tambah yang jelas (fungsional di luar kotak, dukungan komersial) dan minus (harga, waktu pengiriman, kurangnya kesempatan, atau tenggat waktu yang terlalu lama untuk mengimplementasikan fitur baru yang kami butuhkan, tenggat waktu untuk menyelesaikan masalah, dll.).  Perlahan-lahan, kekurangannya mulai melebihi, dan menjadi jelas bahwa kebutuhan sudah matang untuk mengembangkan solusi kami sendiri. </p><br><p>  Pengembangan diluncurkan satu setengah tahun yang lalu.  Dalam subsistem perbatasan, kami secara tradisional membedakan 2 komponen utama: SIP dan server Media;  memuat penyeimbang di atas setiap komponen.  Saya sedang mengerjakan titik masuk / penyeimbang di sini, jadi saya akan mencoba untuk membicarakannya. </p><br><h4 id="trebovaniya">  Persyaratan </h4><br><ul><li>  Toleransi kesalahan: sistem harus menyediakan layanan jika terjadi kegagalan satu atau lebih contoh di pusat data atau seluruh pusat data </li><li>  Kemudahan Servis: kami ingin dapat memindahkan beban dari satu pusat data ke yang lain </li><li>  Skalabilitas: Saya ingin meningkatkan kapasitas dengan cepat dan murah </li></ul><br><h4 id="balansirovka">  Menyeimbangkan </h4><br><p>  Kami memilih IPVS (alias LVS) dalam mode IPIP (traffic tunneling).  Saya tidak akan masuk ke analisis komparatif NAT / DR / TUN / L3DSR, (Anda dapat membaca tentang mode, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> ), saya hanya akan menyebutkan alasannya: </p><br><ul><li>  Kami tidak ingin memaksakan persyaratan pada backend berada pada subnet yang sama dengan LVS (pool berisi backend dari pusat data kami sendiri dan jarak jauh); </li><li>  Backend harus menerima IP sumber asli dari klien (atau NAT-nya), dengan kata lain, sumber NAT tidak cocok; </li><li>  Backend harus mendukung kerja simultan dengan beberapa VIP. </li></ul><br><p>  Kami menyeimbangkan lalu lintas media (ternyata sangat sulit, kami akan menolak), sehingga skema penyebaran saat ini di pusat data adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/fk/vh/wg/fkvhwgftogjrev-a931o3a9qk-e.png"><br><br>  Strategi penyeimbangan IPVS saat ini adalah "sed" (Keterlambatan Terpendek Diharapkan), lebih dari itu.  Tidak seperti Weighted Round Robin / Weighted Least-Connection, ini memungkinkan Anda untuk tidak mentransfer lalu lintas ke backend dengan bobot lebih rendah hingga ambang tertentu tercapai.  Delay yang diharapkan terpendek dihitung dengan menggunakan rumus (Ci + 1) / Ui, di mana Ci adalah jumlah koneksi di backend i, Ui adalah bobot backend.  Misalnya, jika ada backend di kolam dengan bobot 50.000 dan 2, koneksi baru akan didistribusikan oleh yang pertama sampai setiap server mencapai 25.000 koneksi atau sampai uthreshold tercapai - batas jumlah total koneksi. <br>  Baca lebih lanjut tentang menyeimbangkan strategi di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">man ipvsadm</a> . </p><br><p>  Kumpulan IPVS terlihat seperti ini (di sini dan di bawah, alamat IP fiktif tercantum): </p><br><pre><code class="plaintext hljs"># ipvsadm -ln Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 1.1.1.1:5060 sed -&gt; 10.11.100.181:5060 Tunnel 50000 5903 4 -&gt; 10.11.100.192:5060 Tunnel 50000 5905 1 -&gt; 10.12.100.137:5060 Tunnel 2 0 0 -&gt; 10.12.100.144:5060 Tunnel 2 0 0</code> </pre> <br><p>  Beban pada VIP didistribusikan ke server dengan berat 50.000 (mereka ditempatkan di pusat data yang sama sebagai contoh LVS tertentu), jika mereka kelebihan beban atau masuk ke daftar hitam, beban akan pergi ke bagian cadangan pool - server dengan berat 2, yang terletak di pusat data tetangga. </p><br><p>  Tepatnya kumpulan yang sama, tetapi dengan skala, sebaliknya, dikonfigurasikan di pusat data tetangga (pada sistem produksi, jumlah backend, tentu saja, jauh lebih besar). </p><br><p>  Sinkronisasi koneksi melalui ipvs sync memungkinkan LVS cadangan mengetahui semua koneksi saat ini. </p><br><p>  Untuk sinkronisasi antara pusat data, teknik "kotor" telah diterapkan, yang tetap berfungsi dengan baik.  Sinkronisasi IPVS hanya berfungsi melalui multicast, yang sulit bagi kami untuk mengirim dengan benar ke DC yang berdekatan.  Alih-alih multicast, kami menduplikasi lalu lintas sinkronisasi melalui TEE target iptables dari master ipvs ke terowongan ip-ip ke server di DC yang berdekatan, dan mungkin ada beberapa host target / pusat data: </p><br><pre> <code class="plaintext hljs">#### start ipvs sync master role: ipvsadm --start-daemon master --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### duplicate all sync packets to remote LVS servers using iptables TEE target: iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.10 # ip-ip remote lvs server 1 iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.14 # ip-ip remote lvs server 2 #### start ipvs sync backup role: ipvsadm --start-daemon backup --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### be ready to receive sync sync packets from remote LVS servers: iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv01 -j TEE --gateway 127.0.0.1 iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv02 -j TEE --gateway 127.0.0.1</code> </pre> <br><p>  Faktanya, masing-masing server LVS kami memainkan kedua peran sekaligus (master &amp; cadangan), di satu sisi, itu hanya nyaman, karena menghilangkan perubahan peran ketika beralih lalu lintas, di sisi lain itu diperlukan, karena setiap DC memproses lalu lintas grup secara default VIP publik. </p><br><h4 id="pereklyuchenie-nagruzki-mezhdu-data-centrami">  Load switching antar pusat data </h4><br><p>  Dalam operasi normal, setiap alamat IP publik diumumkan di Internet dari mana saja (dalam diagram ini, dari dua pusat data).  Lalu lintas yang memasuki VIP dialihkan ke DC yang kita butuhkan saat ini menggunakan atribut BGP MED (Multi Exit Discriminator) dengan nilai yang berbeda untuk DC Aktif dan Cadangan DC.  Pada saat yang sama, Backup DC selalu siap menerima lalu lintas jika terjadi sesuatu pada yang aktif: <br><br><img src="https://habrastorage.org/webt/ir/s6/ht/irs6htkfhrve2zzevsl0cwcg4k4.png"><br><br>  Dengan mengubah nilai-nilai BGP MED dan menggunakan sinkronisasi lintas lokasi IPVS, kami mendapatkan kesempatan untuk mentransfer lalu lintas dari pusat data ke pusat data dengan mulus, tanpa memengaruhi panggilan telepon yang sudah ada, yang cepat atau lambat akan berakhir secara alami.  Prosesnya sepenuhnya otomatis (untuk setiap VIP kami memiliki tombol di konsol manajemen), dan terlihat seperti ini: </p><br><ol><li><p>  SIP-VIP aktif di DC1 (kiri), cluster di DC2 (kanan) berlebihan, berkat sinkronisasi ipvs, ia memiliki informasi tentang koneksi yang ada di memorinya.  Di sebelah kiri, VIP aktif diumumkan dengan nilai MED 100, di sebelah kanan - dengan nilai 500: <br><br><img src="https://habrastorage.org/webt/v6/di/th/v6dithpguub8rbq7ggurj_vdlxi.png"></p><br></li><li><p>  Tombol switch menyebabkan perubahan pada apa yang disebut  "Target_state" (konsep internal yang mendeklarasikan nilai-nilai BGP MED pada waktu tertentu).  Di sini kita tidak berharap bahwa DC1 dalam rangka dan siap untuk memproses lalu lintas, karena itu LVS di DC2 memasuki keadaan "aktif", menurunkan nilai MED ke 50, dan dengan demikian menyeret lalu lintas ke dirinya sendiri.  Jika backend di DC1 aktif dan tersedia, panggilan tidak akan terputus.  Semua koneksi tcp baru (pendaftaran) akan dikirim ke backends di DC2: <br><br><img src="https://habrastorage.org/webt/lx/zo/cb/lxzocb7gt-zzrznicu5k9hiju2k.png"></p><br></li><li><p>  DC1 menerima replikasi target_state baru dan mengatur MED nilai cadangan (500).  Ketika DC2 mengetahui hal ini, ia menormalkan nilainya (50 =&gt; 100).  Tetap menunggu selesainya semua panggilan aktif di DC1 dan memutuskan koneksi tcp yang sudah ada.  Contoh SBC di DC1 memperkenalkan layanan yang diperlukan ke dalam apa yang disebut  Keadaan "shutdown anggun": "503" menanggapi permintaan SIP berikutnya dan memutuskan sambungan, tetapi tidak menerima koneksi baru.  Juga, contoh-contoh ini termasuk dalam daftar hitam di LVS.  Ketika melanggar, klien membuat pendaftaran / koneksi baru, yang sudah ada di DC2: <br><br><img src="https://habrastorage.org/webt/by/rq/uf/byrquf_81kp8zvvryrrjzdtcl-a.png"></p><br></li><li><p>  Proses berakhir ketika semua lalu lintas di DC2. <br><br><img src="https://habrastorage.org/webt/1m/dm/h8/1mdmh8styniixoof0cragfatdqq.png"></p><br></li><li><p>  DC1 dan DC2 beralih peran. <br><br><img src="https://habrastorage.org/webt/wu/t3/ts/wut3ts2mq5iznucnkesz2r3mnra.png"></p><br></li></ol><br><p>  Dalam kondisi beban tinggi yang konstan pada titik masuk, ternyata sangat nyaman untuk dapat mengubah lalu lintas kapan saja.  Mekanisme yang sama dimulai secara otomatis jika cadangan DC tiba-tiba mulai menerima lalu lintas.  Pada saat yang sama, untuk melindungi dari mengepakkan, switching dipicu hanya sekali dalam satu arah dan kunci diatur ke switching otomatis, untuk menghapusnya, diperlukan intervensi manusia. </p><br><h4 id="chto-vnutri">  Apa yang ada di dalamnya </h4><br><p>  VRRP cluster &amp; IPVS manager: Keepalived.  Keepalived bertanggung jawab untuk mengganti VIP di dalam cluster, juga untuk pemeriksaan kesehatan backends / daftar hitam. </p><br><p>  Stack BGP: ExaBGP.  Dia bertanggung jawab untuk pengumuman rute ke alamat VIP dan menempelkan BGP MED yang sesuai.  Sepenuhnya dikontrol oleh server manajemen.  Daemon BGP andal yang ditulis dengan Python sedang aktif dikembangkan, ia menjalankan tugasnya 100%. </p><br><p>  Server manajemen (API / Pemantauan / manajemen sub-komponen): Pyro4 + Flask.  Ini adalah server Provisioning untuk Keepalived dan ExaBGP, mengelola semua pengaturan sistem lainnya (sysctl / iptables / ipset / etc), menyediakan pemantauan (gnlpy), menambah dan menghapus backend atas permintaan (mereka berkomunikasi dengan API-nya). </p><br><h4 id="cifry">  Tokoh </h4><br><p>  Mesin virtual dengan empat inti Intel Xeon Gold 6140 CPU @ 2.30GHz melayani arus lalu lintas 300Mbps / 210Kpps (lalu lintas media, sekitar 3 ribu panggilan simultan dalam waktu puncak diproses melalui mereka).  Pemanfaatan CPU pada saat yang sama - 60%. </p><br><p>  Sekarang ini cukup untuk melayani lalu lintas hingga 100 ribu perangkat terminal (telepon desktop).  Untuk melayani semua lalu lintas (lebih dari 1 juta perangkat terminal), kami sedang membangun sekitar 10 pasang cluster tersebut di beberapa pusat data. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id436404/">https://habr.com/ru/post/id436404/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id436392/index.html">Manfaat menganalisis aplikasi level 7 di firewall. Bagian 1. Dasar-dasar</a></li>
<li><a href="../id436394/index.html">Demis Hassabis - kecerdasan hebat yang menciptakan kecerdasan hebat</a></li>
<li><a href="../id436396/index.html">Bisakah saya menggunakan pemrograman fungsional dalam bahasa saya?</a></li>
<li><a href="../id436398/index.html">Air</a></li>
<li><a href="../id436400/index.html">Konfigurasikan lingkungan pengembangan untuk mempelajari HTML, CSS, PHP di Windows</a></li>
<li><a href="../id436406/index.html">Bagaimana menjadi pengembang game jika Anda seorang makelar</a></li>
<li><a href="../id436408/index.html">Pemodelan numerik - sejarah satu proyek</a></li>
<li><a href="../id436412/index.html">Tur foto kantor Facebook baru Boston</a></li>
<li><a href="../id436416/index.html">Bermigrasi dari Mongo ke Postgres: Pengalaman koran The Guardian</a></li>
<li><a href="../id436420/index.html">Dump terbesar dalam sejarah: 2,7 miliar akun, di antaranya 773 juta unik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>