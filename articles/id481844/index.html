<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👣 🦋 ⏪ 7 tahun sensasi jaringan saraf dalam grafik dan perspektif inspirasional dari Deep Learning 2020 👨🏿‍🤝‍👨🏾 🥊 ↙️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tahun Baru semakin dekat, tahun 2010 akan segera berakhir, memberikan dunia kebangkitan sensasional dari jaringan saraf. Saya terganggu dan dilarang t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>7 tahun sensasi jaringan saraf dalam grafik dan perspektif inspirasional dari Deep Learning 2020</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/481844/"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/0a2/971/f4c0a297160b156ef22379a9555bd5fd.png"><br><br>  Tahun Baru semakin dekat, tahun 2010 akan segera berakhir, memberikan dunia kebangkitan sensasional dari jaringan saraf.  Saya terganggu <s>dan dilarang tidur oleh</s> pikiran sederhana: "Bagaimana seseorang dapat secara retrospektif memperkirakan kecepatan pengembangan jaringan saraf?" Karena "Dia yang tahu masa lalu tahu masa depan".  Seberapa cepat algoritma yang berbeda lepas landas?  Bagaimana seseorang dapat mengevaluasi kecepatan kemajuan di bidang ini dan memperkirakan kecepatan kemajuan dalam dekade berikutnya? <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/cad/e4a/11b/cade4a11b5be7a57182eddbf3765ba4c.png"><br><br>  Jelas bahwa Anda dapat secara kasar menghitung jumlah artikel di berbagai area.  Metode ini tidak ideal, Anda perlu mempertimbangkan subdomain akun, tetapi secara umum Anda dapat mencoba.  Saya memberikan ide, di <a href="https://scholar.google.com/scholar%3Fhl%3Den%26as_sdt%3D0%252C5%26q%3Dbatch%2Bnormalization" rel="nofollow">Google Cendekia (BatchNorm)</a> itu cukup nyata!  Anda dapat mempertimbangkan kumpulan data baru, Anda dapat kursus baru.  Hamba Anda yang rendah hati, setelah <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">disortir melalui</a> beberapa opsi, menetap di <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3Dbatch%2520normalization" rel="nofollow">Google Trends (BatchNorm)</a> . <br><br>  Rekan-rekan saya dan saya menerima permintaan dari teknologi ML / DL utama, misalnya, <a href="https://en.wikipedia.org/wiki/Batch_normalization" rel="nofollow">Normalisasi Batch</a> , seperti pada gambar di atas, menambahkan tanggal publikasi artikel dengan sebuah titik, dan mendapat cukup waktu untuk menghapus popularitas topik.  Tapi tidak untuk semua itu, <s>jalan setapaknya dipenuhi mawar,</s> lepas landas begitu jelas dan indah, seperti batnorm.  Beberapa istilah, seperti regularisasi atau lewati koneksi, tidak dapat dibangun sama sekali karena kebisingan data.  Namun secara umum, kami berhasil mengumpulkan tren. <br><br>  Siapa yang peduli dengan apa yang terjadi - selamat datang di luka! <br><a name="habracut"></a><br><h1>  Alih-alih memperkenalkan atau tentang pengenalan gambar </h1><br>  Jadi!  Data awal cukup berisik, kadang-kadang ada puncak yang tajam. <img src="https://habrastorage.org/webt/wn/ej/-p/wnej-pvivjixvw84l9sibvriuno.png"><br>  <i>Sumber: <a href="https://twitter.com/karpathy/status/849338608297406465" rel="nofollow">Andrei Karpaty twitter - siswa berdiri di gang audiensi yang besar untuk mendengarkan ceramah tentang jaringan saraf convolutional</a></i> <br><br>  Secara konvensional, cukup bagi <a href="https://en.wikipedia.org/wiki/Andrej_Karpathy" rel="nofollow">Andrey Karpaty</a> untuk memberikan ceramah tentang <a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n yang</a> legendaris <a href="http://cs231n.stanford.edu/" rel="nofollow">: Jaringan Syaraf Konvolusional untuk Pengakuan Visual</a> untuk 750 orang dengan mempopulerkan konsep bagaimana puncak yang tajam terjadi.  Oleh karena itu, data dihaluskan dengan <a href="http://nghiaho.com/%3Fp%3D1159" rel="nofollow">kotak-filter</a> sederhana (semua penghalusan ditandai sebagai Smoothed pada sumbu).  Karena kami tertarik untuk membandingkan tingkat pertumbuhan popularitas - setelah perataan, semua data dinormalisasi.  Ternyata cukup lucu.  Berikut adalah grafik arsitektur utama yang bersaing di ImageNet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fa/4d6/f7b/0fa4d6f7bad7b232ec50e2f0bde6559b.png"><br>  <i>Sumber: Selanjutnya - perhitungan penulis menurut Google Trends</i> <br><br>  Grafik tersebut menunjukkan dengan sangat jelas bahwa setelah publikasi sensasional oleh <a href="https://en.wikipedia.org/wiki/AlexNet" rel="nofollow">AlexNet</a> , yang menyeduh bubur dari sensasi jaringan saraf saat ini pada akhir 2012, selama hampir dua tahun itu bergolak <s>meskipun ada anggapan tumpukan bahwa</s> hanya sekelompok spesialis yang relatif sempit <s>bergabung</s> .  Topik tersebut pergi ke masyarakat umum hanya di musim dingin 2014-2015.  Perhatikan bagaimana jadual jadwal menjadi dari 2017: puncak lebih lanjut setiap musim semi.  <s>Dalam psikiatri, ini disebut eksaserbasi musim semi ...</s> Ini adalah tanda yang pasti bahwa sekarang istilah ini paling banyak digunakan oleh siswa, dan rata-rata, minat terhadap AlexNet menurun dibandingkan dengan puncak popularitas. <br><br>  Selanjutnya, pada paruh kedua 2014, <a href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="nofollow">VGG</a> muncul.  Ngomong-ngomong, <a href="" rel="nofollow">VGG</a> ikut menulis bersama pengawas <a href="" rel="nofollow">studi</a> , mantan murid saya, <a href="https://scholar.google.com/citations%3Fuser%3DL7lMQkQAAAAJ%26hl%3Den" rel="nofollow">Karen Simonyan</a> , sekarang bekerja di Google DeepMind ( <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="nofollow">AlphaGo</a> , <a href="https://en.wikipedia.org/wiki/AlphaZero" rel="nofollow">AlphaZero</a> , dll.).  Saat belajar di Universitas Negeri Moskow pada tahun ke-3, Karen menerapkan <a href="https://www.compression.ru/video/motion_estimation/index_en.html" rel="nofollow">algoritma Estimasi Gerakan yang</a> baik, yang telah berfungsi sebagai referensi bagi siswa 2 tahun selama 12 tahun.  Selain itu, tugas-tugas di sana agak mirip secara sukarela.  Bandingkan: <br><br><img width="50%" src="https://habrastorage.org/webt/mo/w0/9w/mow09w8lyvaxyw3b1ztxm2wxtxe.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/01a/c45/c94/01ac45c944275e9045557c9d453ff938.png"><br>  <i>Sumber: Fungsi kerugian untuk tugas Estimasi Gerak (bahan penulis) dan <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">VGG-56</a></i> <br><br>  Di sebelah kiri Anda perlu menemukan titik terdalam di permukaan nontrivial tergantung pada data input untuk jumlah minimum pengukuran (banyak minimum lokal yang mungkin), dan di sebelah kanan Anda perlu menemukan titik yang lebih rendah dengan perhitungan minimal (dan juga sekelompok minimum lokal, dan permukaan juga tergantung pada data) .  Di sebelah kiri, kita mendapatkan vektor gerakan yang diprediksi, dan di sebelah kanan, jaringan terlatih.  Dan perbedaannya adalah bahwa di sebelah kiri hanya ada pengukuran implisit ruang warna, dan di sebelah kanan adalah sepasang pengukuran dari ratusan juta.  Nah, kompleksitas komputasi di sebelah kanan adalah sekitar 12 orde magnitude (!) Lebih tinggi.  Sedikit seperti itu ... Tapi tahun kedua, bahkan dengan tugas yang sederhana, bergoyang seperti ... [dihentikan karena sensor].  Dan tingkat pemrograman anak sekolah kemarin untuk alasan yang tidak diketahui selama 15 tahun terakhir telah menurun tajam.  Mereka harus mengatakan: "Kamu akan melakukannya dengan baik, mereka akan membawamu ke DeepMind!"  Orang bisa mengatakan "menciptakan VGG", tetapi "mereka akan membawa ke DeepMind" karena alasan tertentu memotivasi lebih baik.  Ini, jelas, adalah analog modern maju klasik "Anda akan makan semolina, Anda akan menjadi astronot!".  Namun, dalam kasus kami, jika kami menghitung jumlah anak di negara ini dan ukuran korps kosmonot, kemungkinannya jutaan kali lebih tinggi, karena kami berdua sudah bekerja di DeepMind dari laboratorium kami. <br><br>  Berikutnya adalah <a href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="nofollow">ResNet</a> , melanggar standar jumlah lapisan dan mulai lepas landas setelah enam bulan.  Dan akhirnya, DenseNet, yang muncul di awal hype <a href="https://towardsdatascience.com/densenet-2810936aeebb" rel="nofollow">,</a> lepas landas dengan segera, bahkan lebih keren dari ResNet. <br><br>  Jika kita berbicara tentang popularitas, saya ingin menambahkan beberapa kata tentang karakteristik jaringan dan kinerja, yang juga bergantung pada popularitas.  Jika Anda melihat bagaimana kelas <a href="https://en.wikipedia.org/wiki/ImageNet" rel="nofollow">ImageNet</a> diprediksi tergantung pada jumlah operasi di jaringan, tata letak akan seperti ini (lebih tinggi dan ke kiri - lebih baik): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c08/f56/f11/c08f56f11908ffb9f78a0d5d66a71342.png"><br>  <i>Sumber: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Analisis Benchmark Arsitektur Perwakilan Deep Neural Network</a></i> <br><br>  Jenis AlexNet tidak lagi kue, dan mereka memerintah jaringan berdasarkan ResNet.  Namun, jika Anda melihat evaluasi praktis <abbr title="jumlah frame yang diproses per detik">FPS</abbr> lebih dekat ke hati saya, Anda dapat dengan jelas melihat bahwa VGG lebih dekat ke optimal di sini, dan secara umum, keberpihakannya berubah.  Termasuk AlexNet secara tak terduga pada amplop Pareto-optimal (skala horizontal adalah logaritmik, lebih baik di atas dan di kanan): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/642/3b9/412/6423b941235280be5ec1182b91cf6d6e.png"><br>  <i>Sumber: <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">Analisis Benchmark Arsitektur Perwakilan Deep Neural Network</a></i> <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Di tahun-tahun mendatang, penyelarasan arsitektur dengan probabilitas tinggi akan berubah sangat signifikan karena <a href="https://habr.com/post/455353/">kemajuan akselerator jaringan saraf</a> , ketika beberapa arsitektur pergi ke keranjang dan beberapa tiba-tiba lepas landas, hanya karena lebih baik meletakkan pada perangkat keras baru.  Misalnya, <a href="https://www.researchgate.net/publication/328509150_Benchmark_Analysis_of_Representative_Deep_Neural_Network_Architectures" rel="nofollow">dalam artikel yang disebutkan</a> , perbandingan dibuat pada NVIDIA Titan X Pascal dan papan NVIDIA Jetson TX1, dan tata letak berubah secara nyata.  Apalagi kemajuan TPU, NPU dan lainnya baru saja dimulai. <br></li><li>  Sebagai seorang praktisi, saya tidak dapat tidak melihat bahwa perbandingan pada ImageNet dilakukan secara default pada ImageNet-1k, dan bukan pada ImageNet-22k, hanya karena sebagian besar melatih jaringan mereka di ImageNet-1k, di mana ada 22 kali lebih sedikit kelas (ini keduanya lebih mudah dan lebih cepat).  Beralih ke ImageNet-22k, yang lebih relevan untuk banyak aplikasi praktis, juga akan mengubah perataan (bagi mereka yang diasah oleh 1k - banyak). <br></li></ul><br><h1>  Lebih dalam dalam Teknologi dan Arsitektur </h1><br>  Namun, kembali ke teknologinya.  Istilah <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="nofollow">Dropout</a> sebagai kata pencarian cukup berisik, tetapi pertumbuhan 5 kali lipat jelas terkait dengan jaringan saraf.  Dan penurunan minat di dalamnya kemungkinan besar dengan <a href="https://patents.google.com/patent/US9406017B2/en" rel="nofollow">paten Google</a> dan munculnya metode baru.  Harap dicatat bahwa sekitar satu setengah tahun telah berlalu dari publikasi <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">artikel asli</a> ke lonjakan minat dalam metode ini: <br><img src="https://habrastorage.org/getpro/habr/post_images/162/fca/4e4/162fca4e4b15574f66f3df40f4230090.png"><br><br>  Namun, jika kita berbicara tentang periode sebelum kenaikan popularitas, maka di DL salah satu tempat pertama jelas diambil oleh <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C" rel="nofollow">jaringan berulang</a> dan <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C" rel="nofollow">LSTM</a> : <br><img src="https://habrastorage.org/getpro/habr/post_images/9bf/423/789/9bf423789d111f7b95352dd9a7b062c1.png"><br><br>  Jauh 20 tahun sebelum puncak popularitas saat ini, dan sekarang, dengan penggunaannya, terjemahan mesin, analisis genom telah ditingkatkan secara radikal, dan dalam waktu dekat (jika Anda mengambil dari daerah saya), YouTube, lalu lintas Netflix akan turun dua kali dengan kualitas visual yang sama.  Jika Anda dengan benar mempelajari pelajaran sejarah, jelas bahwa sebagian gagasan dari poros artikel saat ini akan "lepas landas" hanya setelah 20 tahun.  Pimpin gaya hidup sehat, jaga dirimu, dan kamu akan melihatnya secara pribadi! <br><br>  Sekarang lebih dekat dengan sensasi yang dijanjikan.  <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="nofollow">GAN</a> lepas landas seperti ini: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/80c/272/c0d/80c272c0d6207b79b2736559480aea3d.png"></a> <br><br>  Dapat dilihat dengan jelas bahwa selama hampir satu tahun ada keheningan sama sekali dan hanya pada tahun 2016, setelah 2 tahun, peningkatan yang tajam dimulai (hasilnya terlihat membaik).  Lepas landas ini setahun kemudian memberi DeepFake sensasional, yang, bagaimanapun, juga lepas landas 1,5 tahun.  Artinya, bahkan teknologi yang sangat menjanjikan memerlukan sejumlah besar waktu untuk beralih dari sebuah ide ke aplikasi yang dapat digunakan semua orang. <br><br>  Jika Anda melihat gambar apa yang dihasilkan oleh GAN di <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">artikel asli</a> dan apa yang bisa dibuat dengan <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="nofollow">StyleGAN</a> , menjadi sangat jelas mengapa ada keheningan seperti itu.  Pada tahun 2014, hanya spesialis yang dapat mengevaluasi betapa kerennya itu - untuk membuat, pada dasarnya, jaringan lain sebagai fungsi yang hilang dan melatih mereka bersama.  Dan pada tahun 2019, setiap anak sekolah dapat menghargai betapa kerennya hal ini (tanpa sepenuhnya memahami bagaimana hal ini dilakukan): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ee/6ca/51a/5ee6ca51ac69315327c2dc31f8f80c15.png"><br><br>  Ada <a href="https://www.eff.org/ai/metrics" rel="nofollow">banyak</a> masalah berbeda yang berhasil diselesaikan oleh jaringan saraf hari ini, Anda dapat mengambil jaringan terbaik dan membangun grafik popularitas untuk setiap arah, menangani kebisingan dan puncak permintaan pencarian, dll.  Agar tidak menyebarkan pemikiran saya pada pohon, kami akan mengakhiri seleksi ini dengan topik algoritma segmentasi, di mana ide-ide dari <a href="https://medium.com/%40sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90" rel="nofollow">konvolusi yang meluas / melebar</a> dan <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" rel="nofollow">ASPP</a> dalam satu setengah tahun terakhir telah cukup <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">untuk</a> diri mereka sendiri <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_main.php%3Fchallengeid%3D11%26compid%3D6" rel="nofollow">dalam tolok ukur algoritma</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f52/6c8/6d5/f526c86d5b81454b2a62f77193be0335.png"><br>  Perlu juga dicatat bahwa jika <a href="https://arxiv.org/pdf/1412.7062.pdf" rel="nofollow">DeepLabv1</a> lebih dari satu tahun "menunggu" untuk kenaikan popularitas, maka <a href="https://arxiv.org/pdf/1606.00915.pdf" rel="nofollow">DeepLabv2</a> lepas landas dalam setahun, dan <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">DeepLabv3</a> segera.  Yaitu  secara umum, kita dapat berbicara tentang mempercepat pertumbuhan minat dari waktu ke waktu (baik, atau mempercepat pertumbuhan minat pada teknologi penulis terkemuka). <br><br>  Semua ini bersama-sama menyebabkan terciptanya masalah global berikut - peningkatan eksplosif dalam jumlah publikasi pada topik: <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/99b/bd7/66e/99bbd766ebd801adb403efa6ee5efb4e.png"><br>  <i>Sumber: <a href="http://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" rel="nofollow">Terlalu banyak makalah pembelajaran mesin?</a></i> <br><br>  Tahun ini kami mendapatkan sekitar 150-200 artikel per hari, mengingat tidak semua dipublikasikan di arXiv-e.  Membaca artikel bahkan di sub-area mereka sendiri saat ini sama sekali tidak mungkin.  Akibatnya, banyak ide menarik pasti akan terkubur di bawah puing-puing publikasi baru, yang akan mempengaruhi waktu "take-off" mereka.  Namun, juga peningkatan <i>eksplosif</i> dalam jumlah spesialis kompeten yang dipekerjakan di wilayah tersebut memberikan <s>sedikit</s> harapan untuk mengatasi masalah tersebut. <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  Selain ImageNet dan kisah di balik layar tentang kesuksesan gaming DeepMind, GAN memunculkan gelombang baru popularisasi jaringan saraf.  Dengan mereka, sangat mungkin untuk <a href="https://www.youtube.com/watch%3Fv%3D5rPKeUXjEvE" rel="nofollow">"menembak" aktor</a> tanpa <a href="https://www.youtube.com/watch%3Fv%3DWm3squcz7Aw" rel="nofollow">menggunakan kamera</a> .  Dan apakah akan ada lebih banyak!  Di bawah kebisingan informasional ini, teknologi pemrosesan dan pengenalan yang kurang nyaring, namun cukup bekerja akan dibiayai. <br></li><li>  Karena ada terlalu banyak publikasi, kami menantikan munculnya metode jaringan saraf baru untuk analisis cepat artikel, karena hanya mereka yang akan menyelamatkan kita (lelucon dengan sebagian kecil dari lelucon!). <br></li></ul><br><h1>  Robot kerja, sobat </h1><br>  Selama 2 tahun sekarang, AutoML telah mendapatkan popularitas <s>dari halaman surat kabar</s> .  Semuanya dimulai secara tradisional dengan ImageNet, di mana, dalam Top-1 Accuracy, ia mulai dengan kuat mengambil tempat pertama: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/9a1/498/e6b9a14988b3d708bdbc73aebbf567d2.png"><br>  Inti dari AutoML sangat sederhana, impian para ilmuwan data yang telah berusia seabad telah terwujud di dalamnya - untuk jaringan saraf untuk memilih parameter hiper.  Idenya disambut dengan keras: <br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4f8/c12/56a/4f8c1256ab40bc119fa0c971a8f8bbc1.png"></div><br>  Di bawah ini pada grafik kita melihat situasi yang agak langka ketika, setelah publikasi artikel awal di <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="nofollow">NASNet</a> dan <a href="https://arxiv.org/pdf/1802.01548.pdf" rel="nofollow">AmoebaNet</a> , mereka mulai mendapatkan popularitas dengan standar gagasan sebelumnya hampir secara instan (minat besar pada topik terpengaruh): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0aa/81b/662/0aa81b6628967bc6e77468365cce8554.png"><br>  Gambaran idilis agak dimanjakan oleh dua poin.  Pertama, setiap percakapan tentang AutoML dimulai dengan frasa: "Jika Anda memiliki dofigalion GPU ...".  Dan itu masalahnya.  Google, tentu saja, mengklaim bahwa dengan <a href="https://cloud.google.com/automl/" rel="nofollow">Cloud AutoML mereka</a> ini mudah diselesaikan, yang <s>utama adalah Anda punya cukup uang</s> , tetapi tidak semua orang setuju dengan pendekatan ini.  Kedua, itu bekerja sejauh ini <a href="https://towardsdatascience.com/automl-is-overhyped-1b5511ded65f" rel="nofollow">tidak sempurna</a> .  Di sisi lain, mengingat GAN, lima tahun belum berlalu, dan idenya sendiri terlihat sangat menjanjikan. <br><br>  Bagaimanapun, lepas landas utama AutoML akan dimulai dengan akselerator perangkat keras generasi berikutnya untuk jaringan saraf dan, pada kenyataannya, dengan peningkatan algoritma. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ec/e67/3c5/7ece673c519b57348f556aa7f7b9dfa6.png"><br>  <i>Sumber: Gambar oleh Dmitry Konovalchuk, bahan penulis</i> <br><br>  <b>Total: Faktanya, para ilmuwan data tidak akan memiliki liburan abadi, tentu saja, karena untuk waktu yang sangat lama akan tetap ada sakit kepala besar dengan data.</b>  <b>Tetapi sebelum Tahun Baru dan awal tahun 2020-an, mengapa tidak bermimpi?</b> <br><br><h1>  Beberapa kata tentang alat </h1><br>  Efektivitas penelitian sangat tergantung pada alat.  Jika untuk memprogram AlexNet, Anda memerlukan pemrograman non-sepele, hari ini jaringan seperti itu dapat dikumpulkan dalam beberapa baris dalam kerangka kerja baru. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7c/002/004/b7c0020046f4f5a5b021c6e4a943c9a5.png"><br>  Terlihat jelas bagaimana popularitas berubah dalam gelombang.  Saat ini, yang paling populer (termasuk <a href="https://paperswithcode.com/trends" rel="nofollow">menurut PapersWithCode</a> ) adalah <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> .  Dan begitu <a href="http://caffe.berkeleyvision.org/" rel="nofollow">Caffe</a> populer dengan indah meninggalkannya dengan sangat lancar.  (Catatan: topik dan perangkat lunak berarti bahwa pemfilteran topik Google digunakan saat merencanakan.) <br><br>  Nah, karena kami menyentuh alat pengembangan, perlu disebutkan perpustakaan untuk mempercepat eksekusi jaringan: <br> <a href="https://videoprocessing.ml/" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/de3/fb6/8af/de3fb68af88d9afabe23929e1b060309.png"></a> <br><br>  Yang tertua dalam topik ini adalah (menghormati NVIDIA) <a href="https://developer.nvidia.com/cudnn" rel="nofollow">cuDNN</a> , dan, untungnya bagi pengembang, selama beberapa tahun terakhir jumlah perpustakaan telah meningkat beberapa kali, dan awal popularitas mereka telah menjadi lebih curam.  Dan tampaknya semua ini hanyalah permulaan. <br><br>  <b>Total: Bahkan selama 3 tahun terakhir, alat telah berubah secara serius menjadi lebih baik.</b>  <b>Dan 3 tahun yang lalu, menurut standar hari ini, mereka sama sekali tidak.</b>  <b>Kemajuannya sangat bagus!</b> <br><br><h1>  Prospek Neural Network yang Dijanjikan </h1><br>  Tapi kesenangan dimulai kemudian.  Musim panas ini, dalam sebuah <a href="https://habr.com/post/455353/">artikel besar yang terpisah,</a> saya menjelaskan secara terperinci mengapa CPU dan bahkan GPU tidak cukup efisien untuk bekerja dengan jaringan saraf, mengapa miliaran dolar mengalir ke pengembangan chip baru, dan bagaimana prospeknya.  Saya tidak akan mengulangi lagi.  Di bawah ini adalah generalisasi dan penambahan teks sebelumnya. <br><br>  Untuk memulainya, Anda perlu memahami perbedaan antara perhitungan jaringan saraf dan perhitungan dalam arsitektur von Neumann yang sudah dikenal (di mana mereka tentu saja dapat dihitung, tetapi kurang efisien): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1c/b6a/2f7/a1cb6a2f730e5d5f66a0e29b3fa7d1ac.png"><br>  <i>Sumber: Gambar oleh Dmitry Konovalchuk, bahan penulis</i> <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Arsitektur Von Neumann</b> <br></td><td>  <b>Jaringan saraf</b> <br></td></tr><tr><td>  Sebagian besar perhitungan adalah operasi berurutan. <br></td><td>  Komputasi paralel masif (Anda membutuhkan arsitektur dengan sejumlah besar modul komputasi dan akselerasi komputasi tensor) <br></td></tr><tr><td>  Kursus perhitungan berubah <br>  tergantung pada kondisi ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C" rel="nofollow">superscalarity</a> diperlukan) <br></td><td>  Struktur komputasi hampir selalu diperbaiki dan diketahui sebelumnya (superscalarity tidak efisien) <br></td></tr><tr><td>  Ada lokalitas menurut data (cache berfungsi dengan baik) <br></td><td>  Tidak ada lokalitas data (cache menghangatkan udara) <br></td></tr><tr><td>  Perhitungan Akurat <br></td><td>  Perhitungan mungkin tidak akurat. <br></td></tr><tr><td>  Data berubah secara berbeda untuk algoritma yang berbeda <br></td><td>  Puluhan megabyte koefisien jaringan tidak berubah ketika data berulang kali dijalankan melalui jaringan saraf <br></td></tr></tbody></table></div><br>  Waktu sebelumnya, diskusi utama berlangsung di sekitar FPGA / ASIC, dan perhitungan yang tidak akurat hampir tidak diperhatikan, jadi mari kita membahasnya lebih detail.  Prospek besar untuk mengurangi chip generasi berikutnya adalah tepatnya dalam kemampuan membaca yang tidak akurat (dan menyimpan data koefisien secara lokal).  Pengerasan, pada kenyataannya, juga digunakan dalam aritmatika yang tepat, ketika bobot jaringan dikonversi menjadi bilangan bulat dan dikuantisasi, tetapi pada tingkat yang baru.  Sebagai contoh, pertimbangkan penambah bit tunggal (contohnya cukup abstrak): <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/959/bbf/f16/959bbff160d7f0b33e4d46ce7a5be453.png"><br>  <i>Sumber: <a href="https://www.researchgate.net/publication/270898651_A_High_Speed_and_Low_Power_8_Bit_x_8_Bit_Multiplier_Design_using_Novel_Two_Transistor_2T_XOR_Gates" rel="nofollow">Desain Kecepatan Tinggi dan Daya Rendah 8 Bit x 8 Bit Pengganda menggunakan Novel Two Transistor (2T) XOR Gates</a></i> <br><br>  Ia membutuhkan 6 transistor (ada pendekatan yang berbeda, jumlah transistor yang dibutuhkan bisa semakin banyak, tetapi secara umum, sesuatu seperti ini).  Untuk 8 bit, diperlukan sekitar <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">48 transistor</a> .  Dalam hal ini, penambah analog hanya membutuhkan 2 (dua!) Transistor, yaitu  24 kali lebih sedikit: <br><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f34/85f/2a5/f3485f2a5ac6fa241d2e59cd80ed1064.png"><br>  <i>Sumber: <a href="http://www.iitk.ac.in/eclub/ee381/AnalogMultipliers.pdf" rel="nofollow">Pengganda Analog (Analisis dan Desain Sirkuit Terpadu Analog)</a></i> <br><br>  Jika akurasinya lebih tinggi (misalnya, setara dengan digital 10 atau 16 bit), perbedaannya akan lebih besar.  Yang lebih menarik adalah situasi dengan multiplikasi!  Jika multiplexer 8-bit digital membutuhkan sekitar <a href="https://www.semanticscholar.org/paper/A-novel-approach-for-reversible-realization-of-with-Shukla-Singh/147db51cf0f054b260d980950c01146649483aa1" rel="nofollow">400 transistor</a> , maka analog 6, yaitu  67 kali (!) Lebih sedikit.  Tentu saja, transistor "analog" dan "digital" secara signifikan berbeda dari sudut pandang sirkuit, tetapi idenya jelas - jika kita berhasil meningkatkan keakuratan perhitungan analog, maka kita dengan mudah mencapai situasi ketika kita membutuhkan dua kali lipat transistor kurang besar.  Dan intinya tidak begitu banyak dalam mengurangi ukuran (yang penting sehubungan dengan "perlambatan hukum Moore"), tetapi dalam mengurangi konsumsi listrik, yang sangat penting untuk platform seluler.  Dan untuk pusat data itu tidak akan berlebihan. <br><br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/077/0b2/42b/0770b242bb2693acbe79a2165a4e8c68.png"><br>  <i>Sumber: <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/" rel="nofollow">IBM berpikir chip analog untuk mempercepat pembelajaran mesin</a></i> <br><br>  Kunci keberhasilan di sini adalah pengurangan akurasi, dan sekali lagi di sini IBM berada di garis depan: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a87/570/9f1/a875709f1c76de4e228a567f293a6618.png"><br>  <i>Sumber: <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/" rel="nofollow">IBM Research Blog: Presisi 8-Bit untuk Pelatihan Sistem Pembelajaran Mendalam</a></i> <br><br>  Mereka sudah terlibat dalam ASIC khusus untuk jaringan saraf, yang menunjukkan keunggulan lebih dari 10 kali lipat dari GPU, dan berencana untuk mencapai keunggulan 100 kali lipat di tahun-tahun mendatang.  Terlihat sangat menggembirakan, kami sangat menantikannya, karena, saya ulangi, ini akan menjadi terobosan untuk perangkat seluler. <br><br>  Sementara itu, situasinya tidak begitu ajaib, meskipun ada keberhasilan yang serius.  Berikut ini adalah tes menarik dari akselerator perangkat keras mobile saat ini dari jaringan saraf (gambar dapat diklik, dan itu lagi menghangatkan jiwa penulis, juga dalam gambar per detik): <br><br> <a href="" rel="nofollow"><img src="https://habrastorage.org/getpro/habr/post_images/175/fe0/10d/175fe010d6d9742ddabe6d20d50edb67.png"></a> <br>  <i>Sumber: <a href="https://www.groundai.com/project/ai-benchmark-all-about-deep-learning-on-smartphones-in-2019/1" rel="nofollow">Evolusi kinerja akselerator AI seluler: throughput gambar untuk model float Inception-V3 (model FP16 menggunakan TensorFlow Lite dan NNAPI)</a></i> <br><br>  Hijau menunjukkan chip ponsel, biru menunjukkan CPU, oranye menunjukkan GPU.  Jelas terlihat bahwa chip mobile saat ini, dan pertama-tama, chip top-end dari Huawei, sudah menyalip CPU puluhan kali lebih besar dalam ukuran (dan konsumsi daya).  Dan itu kuat!  Dengan GPU, sejauh ini semuanya tidak begitu ajaib, tetapi akan ada sesuatu yang lain.  Anda dapat melihat hasilnya secara lebih rinci di situs web terpisah <a href="http://ai-benchmark.com/" rel="nofollow">http://ai-benchmark.com/</a> , perhatikan bagian tes di sana, mereka memilih serangkaian algoritma yang baik untuk perbandingan. <br><br>  <b>Total: Kemajuan akselerator analog saat ini cukup sulit untuk dievaluasi.</b>  <b>Ada sebuah perlombaan.</b>  <b>Tapi produknya belum keluar, jadi ada <a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2Baccelerator%2Bfiletype%253Apdf" rel="nofollow">relatif sedikit</a> publikasi.</b>  <b>Anda dapat memantau paten yang muncul dengan penundaan (misalnya, aliran padat <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU" rel="nofollow">dari IBM</a> ) atau <a href="https://patents.google.com/%3Fq%3Danalog%26q%3Ddnn%26q%3Daccelerator%26oq%3Danalog%2Bdnn%2Baccelerator" rel="nofollow">menangkap paten langka dari</a> produsen lain.</b>  <b>Tampaknya ini akan menjadi revolusi yang sangat serius, terutama di smartphone dan server TPU.</b> <br><br><h1>  Alih-alih sebuah kesimpulan </h1><br>  ML / DL hari ini disebut teknologi pemrograman baru, ketika kita tidak menulis suatu program, tetapi menyisipkan blok dan melatihnya.  Yaitu  Seperti pada awalnya ada assembler, lalu C, lalu C ++, dan sekarang, setelah menunggu selama 30 tahun, langkah selanjutnya adalah ML / DL: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfd/8ae/5f7/cfd8ae5f7236a0c9781199044966d2c5.png"></div><br>  Itu masuk akal.  Baru-baru ini, di perusahaan maju, tempat pengambilan keputusan dalam program digantikan oleh jaringan saraf.  Yaitu      « IF-»            (!)         ,        3-5        .   ,   ,     .     ,  <s>,  </s> ,   ,  ,   ,      .   -! <br><br> ,   .  ,  -  ,  : « ,    !»     <s></s>      ,   , ,               ,       (, !) .          : «    , !»              ,     .           «»          «     !» (  ).      . ! <br><br> ,    ML/DL      —    ,    .         : <br><br><ul><li>       —  , <br></li><li>             —     , <br></li><li>         —  , <br></li><li>      8K   2K  —  , <br></li><li>     ,        —  , <br></li><li>   10       ,       <s> </s>        ,    - 90- —   , <br></li><li>             —   , <br></li><li>    —               -   <a href="https://www.cgevent.ru/archives/28741" rel="nofollow">     </a> —    , <br></li><li>   —    !  ) <br></li></ul><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/f28/b47/999/f28b47999ec31c004bc0b917fed05633.png"><br><br>  Hanya 4 tahun telah berlalu sejak orang belajar untuk melatih jaringan saraf yang sangat dalam dalam banyak hal berkat BatchNorm (2015) dan melewatkan koneksi (2015), dan 3 tahun telah berlalu sejak mereka "lepas landas", dan kami benar-benar membaca hasil pekerjaan mereka tidak melihat.  Dan sekarang mereka akan mencapai produk.  Sesuatu memberi tahu kita bahwa di tahun-tahun mendatang banyak hal menarik menunggu kita.  Terutama ketika akselerator "lepas landas" ... <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/a88/6b9/3dc/a886b93dc708002de50fac65c7d84a7a.png"><br><br>  Sekali waktu, jika ada yang ingat, Prometheus mencuri api dari Olympus dan menyerahkannya kepada orang-orang.  Marah Zeus dengan dewa-dewa lain menciptakan keindahan pertama seorang wanita-pria bernama Pandora, yang diberkahi dengan banyak kualitas wanita yang luar biasa <s>(tiba-tiba saya menyadari bahwa menceritakan kembali secara benar beberapa mitos Yunani Kuno sangat sulit)</s> .  Pandora dikirim ke orang-orang, tetapi Prometheus, yang curiga ada yang salah, menolak mantranya, dan saudaranya Epimetheus tidak.  Sebagai hadiah untuk pernikahan, Zeus mengirim peti mati yang indah bersama Merkurius dan Merkurius, jiwa yang baik hati, memenuhi perintah - ia memberikan peti mati itu ke Epimetheus, tetapi memperingatkannya untuk tidak membukanya dalam hal apa pun.  Penasaran Pandora mencuri peti mati dari suaminya, membukanya, tetapi hanya ada dosa, penyakit, perang dan masalah umat manusia lainnya.  Dia mencoba menutup peti mati, tetapi sudah terlambat: <br><br><img src="https://habrastorage.org/webt/fi/hk/wh/fihkwhcd3uam5uyejc6thklshcm.png"><br>  <i>Sumber: <a href="https://regnum.ru/pictures/2414568/6.html" rel="nofollow">Gereja Artis Frederick Stuart, Kotak Terbuka Pandora</a></i> <br><br>  Sejak saat itu, frasa "buka kotak Pandora" telah hilang, yaitu, <s>karena ingin tahu merupakan</s> tindakan yang tidak dapat diubah, konsekuensinya mungkin tidak seindah dekorasi peti mati di luar. <br><br>  Anda tahu, semakin dalam saya menyelam ke dalam jaringan saraf, yang lebih berbeda adalah perasaan bahwa ini adalah kotak Pandora yang lain.  Namun, umat manusia memiliki pengalaman terkaya dalam membuka kotak seperti itu!  Dari baru-baru ini - ini adalah energi nuklir dan Internet.  Jadi, saya pikir kita bisa mengatasi bersama.  Tidak heran jika sekelompok pria <s>berjanggut</s> kasar di antara para pembuka.  Nah, peti mati itu indah, setuju!  Dan itu tidak benar bahwa hanya ada masalah, mereka sudah punya banyak hal baik.  Karena itu, mereka berkumpul dan ... kita buka lebih jauh! <br><br>  <b>Total:</b> <b><br><br></b> <ul><li>  <b>Artikel itu tidak memasukkan banyak topik menarik, misalnya, algoritma ML klasik, transfer pembelajaran, pembelajaran penguatan, popularitas dataset, dll.</b>  <b>(Tuan-tuan, Anda dapat melanjutkan topik!)</b> <b><br></b> </li><li>  <b>Untuk pertanyaan tentang peti mati: Saya pribadi berpikir bahwa <a href="https://habr.com/ru/post/411323/">programmer Google</a> yang memungkinkan Google <a href="https://tproger.ru/news/google-drops-pentagon/" rel="nofollow">untuk meninggalkan kontrak $ 10 miliar dengan Pentagon</a> adalah hebat dan tampan.</b>  <b>Mereka menghormati dan menghormati.</b>  <b>Namun, perhatikan bahwa seseorang memenangkan tender besar ini.</b> <b><br></b> </li></ul><br>  Baca juga: <br><br><ul><li>  <a href="https://habr.com/post/455353/">Akselerasi perangkat keras jaringan saraf dalam: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP, dan surat lainnya</a> - teks penulis tentang keadaan saat ini dan prospek akselerasi perangkat keras jaringan saraf dibandingkan dengan pendekatan saat ini. <br></li><li>  <a href="https://habr.com/post/480348/">Deep Fake Science, krisis reproduktifitas dan dari mana repositori kosong berasal</a> - tentang masalah dalam sains yang dihasilkan oleh ML / DL. <br></li><li>  <a href="https://habr.com/post/451664/">Perbandingan codec jalan ajaib.</a>  <a href="https://habr.com/post/451664/">Kami mengungkapkan rahasia</a> - contoh palsu yang didasarkan pada jaringan saraf. <br></li></ul><br><h3>  Semua banyak <i>penemuan baru yang menarik</i> di tahun 2020 secara umum dan di Tahun Baru, khususnya! </h3><br><h2>  Ucapan Terima Kasih </h2><br>  Saya ingin mengucapkan terima kasih: <br><br><ul><li>  Laboratorium Grafik Komputer dan Multimedia VMK Moscow State University  M.V.  Lomonosov atas kontribusinya pada pengembangan pembelajaran mendalam di Rusia dan tidak hanya <br></li><li>  secara pribadi Konstantin Kozhemyakov dan Dmitry Konovalchuk, yang melakukan banyak hal untuk membuat artikel ini lebih baik dan lebih visual, <br></li><li>  dan akhirnya, terima kasih banyak kepada Kirill Malyshev, Yegor Sklyarov, Nikolai Oplachko, Andrey Moskalenko, Ivan Molodetsky, Evgeny Lyapustin, Roman Kazantsev, Alexander Yakovenko, dan Dmitry Klepikov atas banyak komentar dan koreksi bermanfaat yang membuat teks ini jauh lebih baik! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id481844/">https://habr.com/ru/post/id481844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id481828/index.html">Sejarah perangkat lunak pendidikan: sistem manajemen pembelajaran dan kebangkitan pendidikan online</a></li>
<li><a href="../id481836/index.html">Pizza sebagai layanan: bagaimana Amazon bermigrasi ke Redshift</a></li>
<li><a href="../id481838/index.html">WireGuard, menyiapkan banyak klien untuk NAT, dan kemana STUN pergi?</a></li>
<li><a href="../id481840/index.html">Lindungi GraphQL API Anda dari kerentanan</a></li>
<li><a href="../id481842/index.html">Pindah ke Penyimpanan Murni: Penyimpanan Baru Kami</a></li>
<li><a href="../id481846/index.html">Menggunakan GitHub CI untuk Proyek Elixir</a></li>
<li><a href="../id481848/index.html">Pelatihan staf yang berpengalaman</a></li>
<li><a href="../id481850/index.html">Inkuisisi Spanyol dan robot untuk penghinaan: apa konferensi "predator" demi uang</a></li>
<li><a href="../id481852/index.html">Ulasan Printer 3D Anet N4 // Bagaimana Warna Karakter Jiwa Gelap yang Realistis</a></li>
<li><a href="../id481854/index.html">Menguji gagasan melalui prototipe dasbor</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>