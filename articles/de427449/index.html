<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛳️ 🙋🏿 🎍 Wie man Tensorflow versteht und nicht stirbt und sogar etwas über ein Auto lehrt ↪️ 🧑🏽‍🤝‍🧑🏼 🙏🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo, Wachen. In dem heutigen Beitrag geht es darum, wie Sie sich nicht in der Wildnis der vielfältigen Möglichkeiten verlieren können, TensorFlow fü...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man Tensorflow versteht und nicht stirbt und sogar etwas über ein Auto lehrt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427449/"><p>  Hallo, Wachen.  In dem heutigen Beitrag geht es darum, wie Sie sich nicht in der Wildnis der vielfältigen Möglichkeiten verlieren können, TensorFlow für maschinelles Lernen zu verwenden und Ihr Ziel zu erreichen.  Der Artikel ist so konzipiert, dass der Leser die Grundlagen der Prinzipien des maschinellen Lernens kennt, aber noch nicht versucht hat, dies mit eigenen Händen zu tun.  Als Ergebnis erhalten wir eine funktionierende Demo für Android, die etwas mit ziemlich hoher Genauigkeit erkennt.  Aber das Wichtigste zuerst. </p><br><p><img src="https://habrastorage.org/webt/rs/7r/_f/rs7r_f7v6dywnklpaok4htwntsq.jpeg"></p><a name="habracut"></a><br><p>  Nachdem man <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sich die</a> neuesten Materialien angesehen hatte, entschied man sich für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> , das jetzt an Dynamik gewinnt, und Artikel in Englisch und Russisch scheinen genug zu sein, um nicht in alles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzudringen</a> und herauszufinden, was was ist. </p><br><p>  Verbringen Sie zwei Wochen, studieren Sie Artikel und zahlreiche Ex-Proben im Büro.  Ich habe festgestellt, dass ich nichts verstanden habe.  Zu viele Informationen und Optionen zur Verwendung von Tensorflow.  Mein Kopf ist bereits geschwollen davon, wie viel sie unterschiedliche Lösungen bieten und was ich damit anfangen soll, wie es für meine Aufgabe gilt. </p><br><p><img src="https://habrastorage.org/webt/bd/2z/jy/bd2zjyct-gx0xbz9nfbwwya5aw8.png"></p><br><p>  Dann habe ich beschlossen, alles auszuprobieren, von den einfachsten und am besten vorgefertigten Optionen (bei denen ich eine Abhängigkeit in Gradle registrieren und ein paar Codezeilen hinzufügen musste) bis zu komplexeren Optionen (bei denen ich selbst Diagrammmodelle erstellen und trainieren und lernen musste, wie man sie in einem Mobiltelefon verwendet Anwendung). </p><br><p>  Am Ende musste ich eine komplizierte Version verwenden, auf die weiter unten näher eingegangen wird.  In der Zwischenzeit habe ich für Sie eine Liste einfacherer Optionen zusammengestellt, die gleichermaßen effektiv sind. Nur jede Option entspricht ihrem Zweck. </p><br><h3 id="1--ml-kithttpsfirebasegooglecomdocsml-kit">  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ML KIT</a> </h3><br><p><img src="https://habrastorage.org/webt/al/of/8w/alof8wunrnv66f66xwv2rrlbrn0.png"></p><br><p>  Die einfachste Lösung - ein paar Codezeilen, die Sie verwenden können: </p><br><ul><li>  Texterkennung (Text, lateinische Zeichen) </li><li>  Gesichtserkennung (Gesichter, Emotionen) </li><li>  Barcode-Scannen (Barcode, QR-Code) </li><li>  Bildbeschriftung (eine begrenzte Anzahl von Objekttypen im Bild) </li><li>  Wahrzeichenerkennung (Sehenswürdigkeiten) </li></ul><br><p>  Es ist etwas komplizierter. Mit dieser Lösung können Sie auch Ihr eigenes TensorFlow Lite-Modell verwenden. Die Konvertierung in dieses Format verursachte jedoch Schwierigkeiten, sodass dieses Element nicht ausprobiert wurde. </p><br><p>  Wie die Schöpfer dieses Nachwuchses schreiben, können die meisten Aufgaben mit diesen Entwicklungen gelöst werden.  Wenn dies jedoch nicht für Ihre Aufgabe gilt, müssen Sie benutzerdefinierte Modelle verwenden. </p><br><h3 id="2--custom-visionhttpswwwcustomvisionai">  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benutzerdefinierte Vision</a> </h3><br><p><img src="https://habrastorage.org/webt/p9/c_/2u/p9c_2ujglvyu8mbffhrmoqpzav0.png"></p><br><p>  Ein sehr praktisches Tool zum Erstellen und Trainieren Ihrer benutzerdefinierten Modelle mithilfe von Bildern. <br>  Von den Profis - es gibt eine kostenlose Version, mit der Sie ein Projekt behalten können. <br>  Of the Cons - Die kostenlose Version begrenzt die Anzahl der "eingehenden" Bilder auf 3.000.  Es reicht aus, ein mittelmäßiges Netzwerk von Genauigkeit aufzubauen.  Für genauere Aufgaben benötigen Sie mehr. <br>  Der Benutzer muss lediglich markierte Bilder hinzufügen (z. B. Bild1 ist "Waschbär", Bild2 ist "Sonne"), das Diagramm trainieren und für die zukünftige Verwendung exportieren. </p><br><p><img src="https://habrastorage.org/webt/co/lk/nw/colknw0ljunbtzcixxdrde6qwtm.png"></p><br><p>  Fürsorge Microsoft bietet sogar ein eigenes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispiel an</a> , mit dem Sie Ihr empfangenes Diagramm ausprobieren können. <br>  Für diejenigen, die sich bereits im Subjekt befinden, wird der Graph bereits im eingefrorenen Zustand erzeugt, d. H.  Sie müssen nichts damit tun / konvertieren. <br>  Diese Lösung ist gut, wenn Sie eine große Stichprobe und (Aufmerksamkeits-) VIELE verschiedene Klassen im Training haben.  Weil  Andernfalls wird es in der Praxis viele falsche Definitionen geben.  Sie haben zum Beispiel Waschbären und Sonnen trainiert, und wenn sich eine Person am Eingang befindet, kann sie mit gleicher Wahrscheinlichkeit von einem System wie dem einen oder anderen definiert werden.  Obwohl in der Tat - weder der eine noch der andere. </p><br><h3 id="3--sozdanie-modeli-vruchnuyu">  3. Manuelles Erstellen eines Modells </h3><br><p><img src="https://habrastorage.org/webt/m_/ku/r_/m_kur_ks0vdyiqoiw7h5pvbwoey.jpeg"></p><br><p>  Wenn Sie das Modell für die Bilderkennung selbst optimieren müssen, kommen komplexere Manipulationen mit der Eingabebildauswahl ins Spiel. <br>  Zum Beispiel möchten wir keine Einschränkungen für das Volumen der Eingabestichprobe haben (wie im vorherigen Absatz), oder wir möchten das Modell genauer trainieren, indem wir die Anzahl der Epochen und andere Trainingsparameter selbst einstellen. <br>  Bei diesem Ansatz gibt es mehrere Beispiele von Tensorflow, die die Prozedur und das Endergebnis beschreiben. <br>  Hier einige Beispiele: </p><br><ul><li>  Cooler Codelab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow für Dichter</a> . <br></li></ul><br><br><p>  Es enthält ein Beispiel für die Erstellung eines Klassifikators für Farbtypen basierend auf der geöffneten ImageNet-Datenbank mit Bildern. Bereiten Sie Bilder vor und trainieren Sie das Modell.  Es wird auch ein wenig erwähnt, wie Sie mit einem ziemlich interessanten Werkzeug arbeiten können - TensorBoard.  Von seinen einfachsten Funktionen zeigt es auf vielfältige Weise die Struktur Ihres fertigen Modells sowie den Lernprozess. </p><br><ul><li><p>  Kodlab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow for Poets 2</a> - Fortsetzung der Arbeit mit dem Farbklassifikator.  Es zeigt, wie Sie die Anwendung auf Android ausführen können, wenn Sie über die Grafikdateien und deren Beschriftungen verfügen (die im vorherigen Codelab erhalten wurden).  Einer der Punkte des Codelabs ist die Konvertierung vom "üblichen" Grafikformat ".pb" in das Tensorflow Lite-Format (was einige Dateioptimierungen zur Reduzierung der endgültigen Grafikdateigröße beinhaltet, da mobile Geräte dies erfordern). </p><br></li><li><p>  Handschrifterkennung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST</a> . <br></p><br><img src="https://habrastorage.org/webt/bz/ah/mx/bzahmxc0xozicgssbkzfqbgi1qw.gif"></li></ul><br><br><p>  Die Rübe enthält das Originalmodell (das bereits für diese Aufgabe vorbereitet wurde), Anweisungen zum Trainieren, Konvertieren und zum Ausführen eines Projekts für Android am Ende, um zu überprüfen, wie alles funktioniert </p><br><p>  Anhand dieser Beispiele können Sie herausfinden, wie Sie mit benutzerdefinierten Modellen in Tensorflow arbeiten und versuchen, entweder eigene Modelle zu erstellen oder eines der vorab trainierten Modelle zu verwenden, die auf einem Github zusammengebaut sind: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Modelle von Tensorflow</a> </p><br><p>  Apropos "vorgeübte" Modelle.  Interessante Nuancen bei der Verwendung dieser: </p><br><ul><li>  Ihre Struktur ist bereits auf eine bestimmte Aufgabe vorbereitet. </li><li>  Sie sind bereits in großen Stichproben ausgebildet. <br>  Wenn Ihre Probe nicht ausreichend gefüllt ist, können Sie daher ein vorab trainiertes Modell verwenden, das Ihrem Aufgabenbereich nahe kommt.  Wenn Sie dieses Modell verwenden und Ihre eigenen Trainingsregeln hinzufügen, erhalten Sie ein besseres Ergebnis, als wenn Sie versuchen würden, das Modell von Grund auf neu zu trainieren. </li></ul><br><h3 id="4--object-detection-api---cozdanie-modeli-vruchnuyu">  4. Objekterkennungs-API + manuelle Modellerstellung </h3><br><p>  Alle vorhergehenden Absätze ergaben jedoch nicht das gewünschte Ergebnis.  Von Anfang an war es schwierig zu verstehen, was mit welchem ​​Ansatz getan werden muss.  Dann wurde ein cooler Artikel über die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Objekterkennungs-API</a> gefunden, in dem beschrieben wird, wie mehrere Kategorien auf einem Bild sowie mehrere Instanzen derselben Kategorie gefunden werden.  Während der Arbeit an diesem Beispiel erwiesen sich Quellartikel und Video-Tutorials zum Erkennen benutzerdefinierter Objekte als praktischer (Links finden Sie am Ende). </p><br><p>  Die Arbeit wäre jedoch ohne einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel über die Pikachu-Erkennung nicht abgeschlossen worden</a> - denn dort wurde auf eine sehr wichtige Nuance hingewiesen, die aus irgendeinem Grund in keinem Leitfaden oder Beispiel erwähnt wird.  Und ohne sie wäre die ganze geleistete Arbeit vergebens. </p><br><p>  Nun also endlich darüber, was noch zu tun war und was auf dem Weg nach draußen passiert ist. </p><br><ol><li>  Zunächst das Mehl der Tensorflow-Installation.  Wer kann es nicht installieren oder die Standardskripte zum Erstellen und Trainieren eines Modells verwenden? Seien Sie einfach geduldig und googeln Sie.  Fast jedes Problem wurde bereits in Problemen mit Githib oder Stackoverflow geschrieben. <br></li></ol><br>  Gemäß den Anweisungen zur Objekterkennung müssen wir vor dem Training des Modells ein Eingabemuster vorbereiten.  In diesen Artikeln wird detailliert beschrieben, wie dies mit einem praktischen Tool - labelImg - durchgeführt wird.  Die einzige Schwierigkeit besteht darin, eine sehr lange und sorgfältige Arbeit zu leisten, um die Grenzen der Objekte hervorzuheben, die wir benötigen.  In diesem Fall Stempel auf Bilder von Dokumenten. <br><br><img src="https://habrastorage.org/webt/ge/hh/x_/gehhx_5fqfezu1sbh5tvoofss20.png"><br>  Im nächsten Schritt exportieren wir mithilfe von vorgefertigten Skripten die Daten aus Schritt 2 zuerst in CSV-Dateien und dann in TFRecords - das Tensorflow-Eingabedatenformat.  Hier sollten keine Schwierigkeiten auftreten. <br>  Die Auswahl eines vorab trainierten Modells, auf dessen Grundlage wir den Graphen vorab trainieren, sowie das Training selbst.  Hier kann die größte Anzahl unbekannter Fehler auftreten, deren Ursache darin besteht, dass für die Arbeit erforderliche Pakete deinstalliert (oder schief installiert) werden.  Aber Sie werden Erfolg haben, nicht verzweifeln, das Ergebnis ist es wert. <br><br><img src="https://habrastorage.org/webt/9y/qw/1b/9yqw1boyubfcrrf5jcaylkjjtyo.jpeg"><br>  Exportieren Sie die nach dem Training erhaltene Datei in das Format 'pb'.  Wählen Sie einfach die letzte Datei 'ckpt' aus und exportieren Sie sie. <br>  Ausführen eines Beispiels für die Arbeit unter Android. <br>  Herunterladen der offiziellen Objekterkennungsprobe vom Tensorflow-Github - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF Detect</a> .  Fügen Sie dort Ihr Modell und Ihre Datei mit Beschriftungen ein.  Aber.  Nichts wird funktionieren. <br><br><img src="https://habrastorage.org/webt/kj/3k/o4/kj3ko4d3ywoap8ff6oknuwova7c.gif"><br><br><p>  Hier passierte seltsamerweise der größte Knebel in der ganzen Arbeit - nun, die Tensorflow-Proben wollten in keiner Weise funktionieren.  Alles ist gefallen.  Nur der mächtige Pikachu mit seinem Artikel hat es geschafft, alles zum Laufen zu bringen. <br>  Die erste Zeile in der Datei labels.txt muss die Aufschrift "???" sein, weil  Standardmäßig beginnen in der Objekterkennungs-API die ID-Nummern von Objekten nicht wie gewohnt mit 0, sondern mit 1. Aufgrund der Tatsache, dass die Nullklasse reserviert ist, sollten magische Fragen angegeben werden.  Das heißt,  Ihre Tag-Datei sieht ungefähr so ​​aus: </p><br><pre><code class="hljs">??? stamp</code> </pre> <br><p>  Und dann - führen Sie die Probe aus und sehen Sie die Erkennung von Objekten und den Grad des Vertrauens, mit dem sie empfangen wurden. </p><br><p><img src="https://habrastorage.org/webt/ly/kr/dm/lykrdma-x9h8epuqsuah3gkr3bk.png"><img src="https://habrastorage.org/webt/ne/lm/7v/nelm7v8rpjiuhzhevlptp0dc-fa.png"><img src="https://habrastorage.org/webt/9t/ci/4r/9tci4rxzhixufdjhb5ecpdof0ik.png"></p><br><p>  Das Ergebnis ist somit eine einfache Anwendung, die beim Bewegen des Mauszeigers über die Kamera die Stempelgrenzen auf dem Dokument erkennt und zusammen mit der Erkennungsgenauigkeit anzeigt. <br>  Und wenn wir die Zeit ausschließen, die für die Suche nach dem richtigen Ansatz und den Versuch, ihn zu starten, aufgewendet wurde, stellte sich heraus, dass die Arbeit insgesamt ziemlich schnell und wirklich nicht kompliziert war.  Sie müssen nur die Nuancen kennen, bevor Sie mit der Arbeit beginnen. </p><br><p>  Bereits als zusätzlichen Abschnitt (hier können Sie den Artikel bereits schließen, wenn Sie keine Informationen mehr haben) möchte ich ein paar Life-Hacks schreiben, die bei der Arbeit mit all dem geholfen haben. </p><br><ul><li><p>  Sehr oft funktionierten Tensorflow-Skripte nicht, weil sie aus den falschen Verzeichnissen ausgeführt wurden.  Darüber hinaus war es auf verschiedenen PCs anders: Jemand musste für die Arbeit aus dem <code>tensroflowmodels/models/research</code> , und jemand <code>tensroflowmodels/models/research/object-detection</code> tiefere Ebene aus dem <code>tensroflowmodels/models/research/object-detection</code> </p><br></li><li><p>  Denken Sie daran, dass Sie für jedes geöffnete Terminal den Pfad mit dem Befehl erneut exportieren müssen </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">export</span></span> PYTHONPATH=/  /tensroflowmodels/models/research/slim:<span class="hljs-variable"><span class="hljs-variable">$PYTHONPATH</span></span></code> </pre> <br></li><li><p>  Wenn Sie kein eigenes Diagramm verwenden und Informationen dazu erhalten möchten (z. B. " <code>input_node_name</code> ", das in Zukunft bei der Arbeit benötigt wird), führen Sie zwei Befehle aus dem Stammordner aus: </p><br><pre> <code class="hljs powershell">bazel build tensorflow/tools/graph_transforms:summarize_graph bazel<span class="hljs-literal"><span class="hljs-literal">-bin</span></span>/tensorflow/tools/graph_transforms/summarize_graph -<span class="hljs-literal"><span class="hljs-literal">-in_graph</span></span>=<span class="hljs-string"><span class="hljs-string">"/  /frozen_inference_graph.pb"</span></span></code> </pre> <br><p>  Dabei ist " <code>/  /frozen_inference_graph.pb</code> " der Pfad zu dem Diagramm, über das Sie Bescheid wissen möchten </p><br></li><li><p>  Um Informationen zum Diagramm anzuzeigen, können Sie Tensorboard verwenden. </p><br><pre> <code class="hljs powershell">python import_pb_to_tensorboard.py -<span class="hljs-literal"><span class="hljs-literal">-model_dir</span></span>=output/frozen_inference_graph.pb -<span class="hljs-literal"><span class="hljs-literal">-log_dir</span></span>=training</code> </pre> <br><p>  Hier müssen Sie den Pfad zum Diagramm ( <code>model_dir</code> ) und den Pfad zu den Dateien angeben, die während des Trainings empfangen wurden ( <code>log_dir</code> ).  Öffnen Sie dann einfach localhost im Browser und beobachten Sie, was Sie interessiert. </p><br></li></ul><br><p>  Und der letzte Teil - über die Arbeit mit Python-Skripten in den Anweisungen der Objekterkennungs-API - ein kleines Spickzettel mit Befehlen und Tipps wurde für Sie vorbereitet. </p><br><div class="spoiler">  <b class="spoiler_title">Spickzettel</b> <div class="spoiler_text"><p>  Export von labelimg nach csv (aus dem Verzeichnis object_detection) </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> xml_to_csv.py</code> </pre> <br><p>  Darüber hinaus sollten alle unten aufgeführten Schritte im selben Tensorflow-Ordner ausgeführt werden (" <code>tensroflowmodels/models/research/object-detection</code> " oder eine Ebene <code>tensroflowmodels/models/research/object-detection</code> - je nachdem, wie Sie vorgehen) - das ist alles Bilder der Eingabeauswahl, TFRecords und anderer Dateien müssen vor Arbeitsbeginn in dieses Verzeichnis kopiert werden. </p><br><p>  Export von CSV nach Tfrecord </p><br><pre> <code class="hljs powershell">python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train.record python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test.record</code> </pre> <br><p>  * Vergessen Sie nicht, die Zeilen 'train' und 'test' in den Pfaden in der Datei selbst (generate_tfrecord.py) sowie zu ändern <br>  Der Name der erkannten Klassen in der Funktion <code>class_text_to_int</code> (die in der <code>pbtxt</code> Datei dupliziert werden muss, die Sie vor dem Training des Diagramms erstellen). </p><br><p>  Schulung </p><br><pre> <code class="hljs powershell">python legacy/train.py —logtostderr -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/ssd_mobilenet_v1_coco.config</code> </pre> <br><p>  ** Vergessen Sie vor dem Training nicht, die Datei " <code>training/object-detection.pbtxt</code> " zu überprüfen - es sollten alle erkannten Klassen und die Datei " <code>training/ssd_mobilenet_v1_coco.config</code> " vorhanden sein - dort müssen Sie den Parameter " <code>num_classes</code> " auf die Anzahl Ihrer Klassen ändern. </p><br><p>  Modell nach pb exportieren </p><br><pre> <code class="hljs powershell">python export_inference_graph.py \ -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span>=image_tensor \ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/pipeline.config \ -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span>=training/model.ckpt<span class="hljs-literal"><span class="hljs-literal">-110</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span>=output</code> </pre> </div></div><br><p>  Vielen Dank für Ihr Interesse an diesem Thema! </p><br><p>  Referenzen </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel zur Objekterkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Video-Zyklus zum Artikel über die Erkennung von Objekten in englischer Sprache</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Satz von Skripten, die im Originalartikel verwendet wurden</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de427449/">https://habr.com/ru/post/de427449/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de427437/index.html">Verbindungsserver konfigurieren: MS SQL Server und Teradata</a></li>
<li><a href="../de427439/index.html">Die ganze Wahrheit über RTOS. Artikel 16. Signale</a></li>
<li><a href="../de427441/index.html">Konvergenz mit Kubernetes</a></li>
<li><a href="../de427443/index.html">Vivisektion des Erfolgs</a></li>
<li><a href="../de427447/index.html">PVS-Studio unterstützt die GNU Arm Embedded Toolchain</a></li>
<li><a href="../de427451/index.html">Verbinden Sie phpStorm-Tasks mit Bitrix24</a></li>
<li><a href="../de427453/index.html">Wie ich die Tonübertragung auf dem Raspberry Pi gemacht habe</a></li>
<li><a href="../de427457/index.html">Die dritte Welle von KI und Systemen für die Staatssicherheit</a></li>
<li><a href="../de427459/index.html">Diall LED-Lampen aus dem Castorama Store</a></li>
<li><a href="../de427461/index.html">Die Schönheit NICHT anonymer Funktionen in JavaScript</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>