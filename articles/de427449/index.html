<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõ≥Ô∏è üôãüèø üéç Wie man Tensorflow versteht und nicht stirbt und sogar etwas √ºber ein Auto lehrt ‚Ü™Ô∏è üßëüèΩ‚Äçü§ù‚Äçüßëüèº üôèüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo, Wachen. In dem heutigen Beitrag geht es darum, wie Sie sich nicht in der Wildnis der vielf√§ltigen M√∂glichkeiten verlieren k√∂nnen, TensorFlow f√º...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man Tensorflow versteht und nicht stirbt und sogar etwas √ºber ein Auto lehrt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427449/"><p>  Hallo, Wachen.  In dem heutigen Beitrag geht es darum, wie Sie sich nicht in der Wildnis der vielf√§ltigen M√∂glichkeiten verlieren k√∂nnen, TensorFlow f√ºr maschinelles Lernen zu verwenden und Ihr Ziel zu erreichen.  Der Artikel ist so konzipiert, dass der Leser die Grundlagen der Prinzipien des maschinellen Lernens kennt, aber noch nicht versucht hat, dies mit eigenen H√§nden zu tun.  Als Ergebnis erhalten wir eine funktionierende Demo f√ºr Android, die etwas mit ziemlich hoher Genauigkeit erkennt.  Aber das Wichtigste zuerst. </p><br><p><img src="https://habrastorage.org/webt/rs/7r/_f/rs7r_f7v6dywnklpaok4htwntsq.jpeg"></p><a name="habracut"></a><br><p>  Nachdem man <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sich die</a> neuesten Materialien angesehen hatte, entschied man sich f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow</a> , das jetzt an Dynamik gewinnt, und Artikel in Englisch und Russisch scheinen genug zu sein, um nicht in alles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzudringen</a> und herauszufinden, was was ist. </p><br><p>  Verbringen Sie zwei Wochen, studieren Sie Artikel und zahlreiche Ex-Proben im B√ºro.  Ich habe festgestellt, dass ich nichts verstanden habe.  Zu viele Informationen und Optionen zur Verwendung von Tensorflow.  Mein Kopf ist bereits geschwollen davon, wie viel sie unterschiedliche L√∂sungen bieten und was ich damit anfangen soll, wie es f√ºr meine Aufgabe gilt. </p><br><p><img src="https://habrastorage.org/webt/bd/2z/jy/bd2zjyct-gx0xbz9nfbwwya5aw8.png"></p><br><p>  Dann habe ich beschlossen, alles auszuprobieren, von den einfachsten und am besten vorgefertigten Optionen (bei denen ich eine Abh√§ngigkeit in Gradle registrieren und ein paar Codezeilen hinzuf√ºgen musste) bis zu komplexeren Optionen (bei denen ich selbst Diagrammmodelle erstellen und trainieren und lernen musste, wie man sie in einem Mobiltelefon verwendet Anwendung). </p><br><p>  Am Ende musste ich eine komplizierte Version verwenden, auf die weiter unten n√§her eingegangen wird.  In der Zwischenzeit habe ich f√ºr Sie eine Liste einfacherer Optionen zusammengestellt, die gleicherma√üen effektiv sind. Nur jede Option entspricht ihrem Zweck. </p><br><h3 id="1--ml-kithttpsfirebasegooglecomdocsml-kit">  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ML KIT</a> </h3><br><p><img src="https://habrastorage.org/webt/al/of/8w/alof8wunrnv66f66xwv2rrlbrn0.png"></p><br><p>  Die einfachste L√∂sung - ein paar Codezeilen, die Sie verwenden k√∂nnen: </p><br><ul><li>  Texterkennung (Text, lateinische Zeichen) </li><li>  Gesichtserkennung (Gesichter, Emotionen) </li><li>  Barcode-Scannen (Barcode, QR-Code) </li><li>  Bildbeschriftung (eine begrenzte Anzahl von Objekttypen im Bild) </li><li>  Wahrzeichenerkennung (Sehensw√ºrdigkeiten) </li></ul><br><p>  Es ist etwas komplizierter. Mit dieser L√∂sung k√∂nnen Sie auch Ihr eigenes TensorFlow Lite-Modell verwenden. Die Konvertierung in dieses Format verursachte jedoch Schwierigkeiten, sodass dieses Element nicht ausprobiert wurde. </p><br><p>  Wie die Sch√∂pfer dieses Nachwuchses schreiben, k√∂nnen die meisten Aufgaben mit diesen Entwicklungen gel√∂st werden.  Wenn dies jedoch nicht f√ºr Ihre Aufgabe gilt, m√ºssen Sie benutzerdefinierte Modelle verwenden. </p><br><h3 id="2--custom-visionhttpswwwcustomvisionai">  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Benutzerdefinierte Vision</a> </h3><br><p><img src="https://habrastorage.org/webt/p9/c_/2u/p9c_2ujglvyu8mbffhrmoqpzav0.png"></p><br><p>  Ein sehr praktisches Tool zum Erstellen und Trainieren Ihrer benutzerdefinierten Modelle mithilfe von Bildern. <br>  Von den Profis - es gibt eine kostenlose Version, mit der Sie ein Projekt behalten k√∂nnen. <br>  Of the Cons - Die kostenlose Version begrenzt die Anzahl der "eingehenden" Bilder auf 3.000.  Es reicht aus, ein mittelm√§√üiges Netzwerk von Genauigkeit aufzubauen.  F√ºr genauere Aufgaben ben√∂tigen Sie mehr. <br>  Der Benutzer muss lediglich markierte Bilder hinzuf√ºgen (z. B. Bild1 ist "Waschb√§r", Bild2 ist "Sonne"), das Diagramm trainieren und f√ºr die zuk√ºnftige Verwendung exportieren. </p><br><p><img src="https://habrastorage.org/webt/co/lk/nw/colknw0ljunbtzcixxdrde6qwtm.png"></p><br><p>  F√ºrsorge Microsoft bietet sogar ein eigenes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispiel an</a> , mit dem Sie Ihr empfangenes Diagramm ausprobieren k√∂nnen. <br>  F√ºr diejenigen, die sich bereits im Subjekt befinden, wird der Graph bereits im eingefrorenen Zustand erzeugt, d. H.  Sie m√ºssen nichts damit tun / konvertieren. <br>  Diese L√∂sung ist gut, wenn Sie eine gro√üe Stichprobe und (Aufmerksamkeits-) VIELE verschiedene Klassen im Training haben.  Weil  Andernfalls wird es in der Praxis viele falsche Definitionen geben.  Sie haben zum Beispiel Waschb√§ren und Sonnen trainiert, und wenn sich eine Person am Eingang befindet, kann sie mit gleicher Wahrscheinlichkeit von einem System wie dem einen oder anderen definiert werden.  Obwohl in der Tat - weder der eine noch der andere. </p><br><h3 id="3--sozdanie-modeli-vruchnuyu">  3. Manuelles Erstellen eines Modells </h3><br><p><img src="https://habrastorage.org/webt/m_/ku/r_/m_kur_ks0vdyiqoiw7h5pvbwoey.jpeg"></p><br><p>  Wenn Sie das Modell f√ºr die Bilderkennung selbst optimieren m√ºssen, kommen komplexere Manipulationen mit der Eingabebildauswahl ins Spiel. <br>  Zum Beispiel m√∂chten wir keine Einschr√§nkungen f√ºr das Volumen der Eingabestichprobe haben (wie im vorherigen Absatz), oder wir m√∂chten das Modell genauer trainieren, indem wir die Anzahl der Epochen und andere Trainingsparameter selbst einstellen. <br>  Bei diesem Ansatz gibt es mehrere Beispiele von Tensorflow, die die Prozedur und das Endergebnis beschreiben. <br>  Hier einige Beispiele: </p><br><ul><li>  Cooler Codelab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow f√ºr Dichter</a> . <br></li></ul><br><br><p>  Es enth√§lt ein Beispiel f√ºr die Erstellung eines Klassifikators f√ºr Farbtypen basierend auf der ge√∂ffneten ImageNet-Datenbank mit Bildern. Bereiten Sie Bilder vor und trainieren Sie das Modell.  Es wird auch ein wenig erw√§hnt, wie Sie mit einem ziemlich interessanten Werkzeug arbeiten k√∂nnen - TensorBoard.  Von seinen einfachsten Funktionen zeigt es auf vielf√§ltige Weise die Struktur Ihres fertigen Modells sowie den Lernprozess. </p><br><ul><li><p>  Kodlab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow for Poets 2</a> - Fortsetzung der Arbeit mit dem Farbklassifikator.  Es zeigt, wie Sie die Anwendung auf Android ausf√ºhren k√∂nnen, wenn Sie √ºber die Grafikdateien und deren Beschriftungen verf√ºgen (die im vorherigen Codelab erhalten wurden).  Einer der Punkte des Codelabs ist die Konvertierung vom "√ºblichen" Grafikformat ".pb" in das Tensorflow Lite-Format (was einige Dateioptimierungen zur Reduzierung der endg√ºltigen Grafikdateigr√∂√üe beinhaltet, da mobile Ger√§te dies erfordern). </p><br></li><li><p>  Handschrifterkennung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MNIST</a> . <br></p><br><img src="https://habrastorage.org/webt/bz/ah/mx/bzahmxc0xozicgssbkzfqbgi1qw.gif"></li></ul><br><br><p>  Die R√ºbe enth√§lt das Originalmodell (das bereits f√ºr diese Aufgabe vorbereitet wurde), Anweisungen zum Trainieren, Konvertieren und zum Ausf√ºhren eines Projekts f√ºr Android am Ende, um zu √ºberpr√ºfen, wie alles funktioniert </p><br><p>  Anhand dieser Beispiele k√∂nnen Sie herausfinden, wie Sie mit benutzerdefinierten Modellen in Tensorflow arbeiten und versuchen, entweder eigene Modelle zu erstellen oder eines der vorab trainierten Modelle zu verwenden, die auf einem Github zusammengebaut sind: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Modelle von Tensorflow</a> </p><br><p>  Apropos "vorge√ºbte" Modelle.  Interessante Nuancen bei der Verwendung dieser: </p><br><ul><li>  Ihre Struktur ist bereits auf eine bestimmte Aufgabe vorbereitet. </li><li>  Sie sind bereits in gro√üen Stichproben ausgebildet. <br>  Wenn Ihre Probe nicht ausreichend gef√ºllt ist, k√∂nnen Sie daher ein vorab trainiertes Modell verwenden, das Ihrem Aufgabenbereich nahe kommt.  Wenn Sie dieses Modell verwenden und Ihre eigenen Trainingsregeln hinzuf√ºgen, erhalten Sie ein besseres Ergebnis, als wenn Sie versuchen w√ºrden, das Modell von Grund auf neu zu trainieren. </li></ul><br><h3 id="4--object-detection-api---cozdanie-modeli-vruchnuyu">  4. Objekterkennungs-API + manuelle Modellerstellung </h3><br><p>  Alle vorhergehenden Abs√§tze ergaben jedoch nicht das gew√ºnschte Ergebnis.  Von Anfang an war es schwierig zu verstehen, was mit welchem ‚Äã‚ÄãAnsatz getan werden muss.  Dann wurde ein cooler Artikel √ºber die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Objekterkennungs-API</a> gefunden, in dem beschrieben wird, wie mehrere Kategorien auf einem Bild sowie mehrere Instanzen derselben Kategorie gefunden werden.  W√§hrend der Arbeit an diesem Beispiel erwiesen sich Quellartikel und Video-Tutorials zum Erkennen benutzerdefinierter Objekte als praktischer (Links finden Sie am Ende). </p><br><p>  Die Arbeit w√§re jedoch ohne einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber die Pikachu-Erkennung nicht abgeschlossen worden</a> - denn dort wurde auf eine sehr wichtige Nuance hingewiesen, die aus irgendeinem Grund in keinem Leitfaden oder Beispiel erw√§hnt wird.  Und ohne sie w√§re die ganze geleistete Arbeit vergebens. </p><br><p>  Nun also endlich dar√ºber, was noch zu tun war und was auf dem Weg nach drau√üen passiert ist. </p><br><ol><li>  Zun√§chst das Mehl der Tensorflow-Installation.  Wer kann es nicht installieren oder die Standardskripte zum Erstellen und Trainieren eines Modells verwenden? Seien Sie einfach geduldig und googeln Sie.  Fast jedes Problem wurde bereits in Problemen mit Githib oder Stackoverflow geschrieben. <br></li></ol><br>  Gem√§√ü den Anweisungen zur Objekterkennung m√ºssen wir vor dem Training des Modells ein Eingabemuster vorbereiten.  In diesen Artikeln wird detailliert beschrieben, wie dies mit einem praktischen Tool - labelImg - durchgef√ºhrt wird.  Die einzige Schwierigkeit besteht darin, eine sehr lange und sorgf√§ltige Arbeit zu leisten, um die Grenzen der Objekte hervorzuheben, die wir ben√∂tigen.  In diesem Fall Stempel auf Bilder von Dokumenten. <br><br><img src="https://habrastorage.org/webt/ge/hh/x_/gehhx_5fqfezu1sbh5tvoofss20.png"><br>  Im n√§chsten Schritt exportieren wir mithilfe von vorgefertigten Skripten die Daten aus Schritt 2 zuerst in CSV-Dateien und dann in TFRecords - das Tensorflow-Eingabedatenformat.  Hier sollten keine Schwierigkeiten auftreten. <br>  Die Auswahl eines vorab trainierten Modells, auf dessen Grundlage wir den Graphen vorab trainieren, sowie das Training selbst.  Hier kann die gr√∂√üte Anzahl unbekannter Fehler auftreten, deren Ursache darin besteht, dass f√ºr die Arbeit erforderliche Pakete deinstalliert (oder schief installiert) werden.  Aber Sie werden Erfolg haben, nicht verzweifeln, das Ergebnis ist es wert. <br><br><img src="https://habrastorage.org/webt/9y/qw/1b/9yqw1boyubfcrrf5jcaylkjjtyo.jpeg"><br>  Exportieren Sie die nach dem Training erhaltene Datei in das Format 'pb'.  W√§hlen Sie einfach die letzte Datei 'ckpt' aus und exportieren Sie sie. <br>  Ausf√ºhren eines Beispiels f√ºr die Arbeit unter Android. <br>  Herunterladen der offiziellen Objekterkennungsprobe vom Tensorflow-Github - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TF Detect</a> .  F√ºgen Sie dort Ihr Modell und Ihre Datei mit Beschriftungen ein.  Aber.  Nichts wird funktionieren. <br><br><img src="https://habrastorage.org/webt/kj/3k/o4/kj3ko4d3ywoap8ff6oknuwova7c.gif"><br><br><p>  Hier passierte seltsamerweise der gr√∂√üte Knebel in der ganzen Arbeit - nun, die Tensorflow-Proben wollten in keiner Weise funktionieren.  Alles ist gefallen.  Nur der m√§chtige Pikachu mit seinem Artikel hat es geschafft, alles zum Laufen zu bringen. <br>  Die erste Zeile in der Datei labels.txt muss die Aufschrift "???" sein, weil  Standardm√§√üig beginnen in der Objekterkennungs-API die ID-Nummern von Objekten nicht wie gewohnt mit 0, sondern mit 1. Aufgrund der Tatsache, dass die Nullklasse reserviert ist, sollten magische Fragen angegeben werden.  Das hei√üt,  Ihre Tag-Datei sieht ungef√§hr so ‚Äã‚Äãaus: </p><br><pre><code class="hljs">??? stamp</code> </pre> <br><p>  Und dann - f√ºhren Sie die Probe aus und sehen Sie die Erkennung von Objekten und den Grad des Vertrauens, mit dem sie empfangen wurden. </p><br><p><img src="https://habrastorage.org/webt/ly/kr/dm/lykrdma-x9h8epuqsuah3gkr3bk.png"><img src="https://habrastorage.org/webt/ne/lm/7v/nelm7v8rpjiuhzhevlptp0dc-fa.png"><img src="https://habrastorage.org/webt/9t/ci/4r/9tci4rxzhixufdjhb5ecpdof0ik.png"></p><br><p>  Das Ergebnis ist somit eine einfache Anwendung, die beim Bewegen des Mauszeigers √ºber die Kamera die Stempelgrenzen auf dem Dokument erkennt und zusammen mit der Erkennungsgenauigkeit anzeigt. <br>  Und wenn wir die Zeit ausschlie√üen, die f√ºr die Suche nach dem richtigen Ansatz und den Versuch, ihn zu starten, aufgewendet wurde, stellte sich heraus, dass die Arbeit insgesamt ziemlich schnell und wirklich nicht kompliziert war.  Sie m√ºssen nur die Nuancen kennen, bevor Sie mit der Arbeit beginnen. </p><br><p>  Bereits als zus√§tzlichen Abschnitt (hier k√∂nnen Sie den Artikel bereits schlie√üen, wenn Sie keine Informationen mehr haben) m√∂chte ich ein paar Life-Hacks schreiben, die bei der Arbeit mit all dem geholfen haben. </p><br><ul><li><p>  Sehr oft funktionierten Tensorflow-Skripte nicht, weil sie aus den falschen Verzeichnissen ausgef√ºhrt wurden.  Dar√ºber hinaus war es auf verschiedenen PCs anders: Jemand musste f√ºr die Arbeit aus dem <code>tensroflowmodels/models/research</code> , und jemand <code>tensroflowmodels/models/research/object-detection</code> tiefere Ebene aus dem <code>tensroflowmodels/models/research/object-detection</code> </p><br></li><li><p>  Denken Sie daran, dass Sie f√ºr jedes ge√∂ffnete Terminal den Pfad mit dem Befehl erneut exportieren m√ºssen </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">export</span></span> PYTHONPATH=/  /tensroflowmodels/models/research/slim:<span class="hljs-variable"><span class="hljs-variable">$PYTHONPATH</span></span></code> </pre> <br></li><li><p>  Wenn Sie kein eigenes Diagramm verwenden und Informationen dazu erhalten m√∂chten (z. B. " <code>input_node_name</code> ", das in Zukunft bei der Arbeit ben√∂tigt wird), f√ºhren Sie zwei Befehle aus dem Stammordner aus: </p><br><pre> <code class="hljs powershell">bazel build tensorflow/tools/graph_transforms:summarize_graph bazel<span class="hljs-literal"><span class="hljs-literal">-bin</span></span>/tensorflow/tools/graph_transforms/summarize_graph -<span class="hljs-literal"><span class="hljs-literal">-in_graph</span></span>=<span class="hljs-string"><span class="hljs-string">"/  /frozen_inference_graph.pb"</span></span></code> </pre> <br><p>  Dabei ist " <code>/  /frozen_inference_graph.pb</code> " der Pfad zu dem Diagramm, √ºber das Sie Bescheid wissen m√∂chten </p><br></li><li><p>  Um Informationen zum Diagramm anzuzeigen, k√∂nnen Sie Tensorboard verwenden. </p><br><pre> <code class="hljs powershell">python import_pb_to_tensorboard.py -<span class="hljs-literal"><span class="hljs-literal">-model_dir</span></span>=output/frozen_inference_graph.pb -<span class="hljs-literal"><span class="hljs-literal">-log_dir</span></span>=training</code> </pre> <br><p>  Hier m√ºssen Sie den Pfad zum Diagramm ( <code>model_dir</code> ) und den Pfad zu den Dateien angeben, die w√§hrend des Trainings empfangen wurden ( <code>log_dir</code> ).  √ñffnen Sie dann einfach localhost im Browser und beobachten Sie, was Sie interessiert. </p><br></li></ul><br><p>  Und der letzte Teil - √ºber die Arbeit mit Python-Skripten in den Anweisungen der Objekterkennungs-API - ein kleines Spickzettel mit Befehlen und Tipps wurde f√ºr Sie vorbereitet. </p><br><div class="spoiler">  <b class="spoiler_title">Spickzettel</b> <div class="spoiler_text"><p>  Export von labelimg nach csv (aus dem Verzeichnis object_detection) </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> xml_to_csv.py</code> </pre> <br><p>  Dar√ºber hinaus sollten alle unten aufgef√ºhrten Schritte im selben Tensorflow-Ordner ausgef√ºhrt werden (" <code>tensroflowmodels/models/research/object-detection</code> " oder eine Ebene <code>tensroflowmodels/models/research/object-detection</code> - je nachdem, wie Sie vorgehen) - das ist alles Bilder der Eingabeauswahl, TFRecords und anderer Dateien m√ºssen vor Arbeitsbeginn in dieses Verzeichnis kopiert werden. </p><br><p>  Export von CSV nach Tfrecord </p><br><pre> <code class="hljs powershell">python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train.record python generate_tfrecord.py -<span class="hljs-literal"><span class="hljs-literal">-csv_input</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test_labels.csv -<span class="hljs-literal"><span class="hljs-literal">-output_path</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test.record</code> </pre> <br><p>  * Vergessen Sie nicht, die Zeilen 'train' und 'test' in den Pfaden in der Datei selbst (generate_tfrecord.py) sowie zu √§ndern <br>  Der Name der erkannten Klassen in der Funktion <code>class_text_to_int</code> (die in der <code>pbtxt</code> Datei dupliziert werden muss, die Sie vor dem Training des Diagramms erstellen). </p><br><p>  Schulung </p><br><pre> <code class="hljs powershell">python legacy/train.py ‚Äîlogtostderr -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/ssd_mobilenet_v1_coco.config</code> </pre> <br><p>  ** Vergessen Sie vor dem Training nicht, die Datei " <code>training/object-detection.pbtxt</code> " zu √ºberpr√ºfen - es sollten alle erkannten Klassen und die Datei " <code>training/ssd_mobilenet_v1_coco.config</code> " vorhanden sein - dort m√ºssen Sie den Parameter " <code>num_classes</code> " auf die Anzahl Ihrer Klassen √§ndern. </p><br><p>  Modell nach pb exportieren </p><br><pre> <code class="hljs powershell">python export_inference_graph.py \ -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span>=image_tensor \ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/pipeline.config \ -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span>=training/model.ckpt<span class="hljs-literal"><span class="hljs-literal">-110</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span>=output</code> </pre> </div></div><br><p>  Vielen Dank f√ºr Ihr Interesse an diesem Thema! </p><br><p>  Referenzen </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Originalartikel zur Objekterkennung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Video-Zyklus zum Artikel √ºber die Erkennung von Objekten in englischer Sprache</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Satz von Skripten, die im Originalartikel verwendet wurden</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de427449/">https://habr.com/ru/post/de427449/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de427437/index.html">Verbindungsserver konfigurieren: MS SQL Server und Teradata</a></li>
<li><a href="../de427439/index.html">Die ganze Wahrheit √ºber RTOS. Artikel 16. Signale</a></li>
<li><a href="../de427441/index.html">Konvergenz mit Kubernetes</a></li>
<li><a href="../de427443/index.html">Vivisektion des Erfolgs</a></li>
<li><a href="../de427447/index.html">PVS-Studio unterst√ºtzt die GNU Arm Embedded Toolchain</a></li>
<li><a href="../de427451/index.html">Verbinden Sie phpStorm-Tasks mit Bitrix24</a></li>
<li><a href="../de427453/index.html">Wie ich die Ton√ºbertragung auf dem Raspberry Pi gemacht habe</a></li>
<li><a href="../de427457/index.html">Die dritte Welle von KI und Systemen f√ºr die Staatssicherheit</a></li>
<li><a href="../de427459/index.html">Diall LED-Lampen aus dem Castorama Store</a></li>
<li><a href="../de427461/index.html">Die Sch√∂nheit NICHT anonymer Funktionen in JavaScript</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>