<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ™ğŸ¿ ğŸ´ ğŸ˜— ÙƒÙŠÙ ÙŠØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø§Ù„Ù‚Ø·Ø· ğŸ§Ÿ ğŸ‘‰ğŸ½ ğŸ•°ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØªØ¹Ù„Ù… ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø§Ù„Ù‚Ø·Ø· . 

 ÙƒØ§Ù† Ø¨Ø­Ø« Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ© (GAN) Ø§Ù„Ù…Ù†Ø´ÙˆØ± ÙÙŠ Ø¹Ø§Ù… 2014 Ø¥Ù†Ø¬Ø§Ø²Ù‹Ø§ ÙƒØ¨ÙŠØ±Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†Ù…Ø§Ø°...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>ÙƒÙŠÙ ÙŠØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø§Ù„Ù‚Ø·Ø·</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/416129/" style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/webt/_h/iv/p2/_hivp2o-k9yn82z0cmffh1y__yu.jpeg"><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØªØ¹Ù„Ù…</a> ØªØ±Ø¬Ù…Ø© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø§Ù„Ù‚Ø·Ø·</a></i> . <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">ÙƒØ§Ù† Ø¨Ø­Ø« Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ©</a> (GAN) Ø§Ù„Ù…Ù†Ø´ÙˆØ± ÙÙŠ Ø¹Ø§Ù… 2014 Ø¥Ù†Ø¬Ø§Ø²Ù‹Ø§ ÙƒØ¨ÙŠØ±Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ©.  ÙˆÙˆØµÙ Ø§Ù„Ø¨Ø§Ø­Ø« Ø§Ù„Ø±Ø§Ø¦Ø¯ ÙŠØ§Ù† Ù„ÙŠÙƒÙˆÙ† Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø¨Ø£Ù†Ù‡Ø§ "Ø£ÙØ¶Ù„ ÙÙƒØ±Ø© ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ù„Ù‰ Ù…Ø¯Ø§Ø± Ø§Ù„Ø¹Ø´Ø±ÙŠÙ† Ø¹Ø§Ù…Ù‹Ø§ Ø§Ù„Ù…Ø§Ø¶ÙŠØ©".  Ø§Ù„ÙŠÙˆÙ… ØŒ Ø¨ÙØ¶Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙˆÙ„Ø¯ ØµÙˆØ±Ù‹Ø§ ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù‚Ø·Ø·.  Ø±Ø§Ø¦Ø¹! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a9/2c3/4bd/3a92c34bdd409aead0b270fa21a59d86.gif"><br>  <i>DCGAN Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨</i> <br><a name="habracut"></a><br>  ÙƒÙ„ ÙƒÙˆØ¯ Ø§Ù„Ø¹Ù…Ù„ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ù…Ø³ØªÙˆØ¯Ø¹ Ø¬ÙŠØ«Ø¨</a> .  Ø³ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù„Ùƒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ø®Ø¨Ø±Ø© ÙÙŠ Ø¨Ø±Ù…Ø¬Ø© Python ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Tensorflow ÙˆØ§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ©. <br><br>  ÙˆØ¥Ø°Ø§ ÙƒÙ†Øª Ø¬Ø¯ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ØŒ ÙØ¥Ù†Ù†ÙŠ Ø£ÙˆØµÙŠÙƒ Ø¨Ø£Ù† ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ù…ØªØ§Ø²Ø© Ù…Ù† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø£Ù…Ø± Ù…Ù…ØªØ¹!</a> <br><br><h3 style=";text-align:right;direction:rtl">  Ù…Ø§ Ù‡Ùˆ DCGANØŸ </h3><br>  Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠÙÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© (DCGAN) Ù‡ÙŠ Ø¨Ù†ÙŠØ© ØªØ¹Ù„Ù… Ø¹Ù…ÙŠÙ‚Ø© ØªÙˆÙ„Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨. <br><br>  ÙŠØ³ØªØ¨Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØµÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ©.  Ù„ÙÙ‡Ù… ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ DCGAN ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ø³ØªØ¹Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø¨ÙŠÙ† Ù†Ø§Ù‚Ø¯ ÙÙ†ÙŠ Ø®Ø¨ÙŠØ± ÙˆÙ…Ø²ÙˆØ±. <br><br>  ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„Ù…Ø²ÙˆØ± ("Ø§Ù„Ù…ÙˆÙ„Ø¯") Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Van Gogh Ù…Ø²ÙŠÙØ© ÙˆØªÙ…Ø±ÙŠØ±Ù‡Ø§ ÙƒØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/814/e92/796/814e927961f7a7ac8541ae2104d0b9c0.png"><br><br>  ÙŠØ­Ø§ÙˆÙ„ Ù†Ø§Ù‚Ø¯ ÙÙ†ÙŠ ("ØªÙ…ÙŠÙŠØ²") Ø¥Ø¯Ø§Ù†Ø© Ù…Ø²ÙˆØ± ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø±ÙØªÙ‡ Ø¨Ø§Ù„Ù„ÙˆØ­Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„ÙØ§Ù† Ø¬ÙˆØ®. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae1/81d/424/ae181d42436f5ac37219ac75b9371c35.png"><br><br>  Ù…Ø¹ Ù…Ø±ÙˆØ± Ø§Ù„ÙˆÙ‚Øª ØŒ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„ÙÙ†ÙŠ Ø¨ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ù‚Ù„Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§ÙŠØ¯ ØŒ ÙˆÙŠØ¬Ø¹Ù„Ù‡Ø§ Ù…ÙØ²ÙÙˆÙÙ‘Ø¬Ù‹Ø§ Ø£ÙƒØ«Ø± Ù…Ø«Ø§Ù„ÙŠØ©. <br><br><img src="https://habrastorage.org/webt/vq/lk/gt/vqlkgtau1xlmqscetwib8uuixbk.png"><br>  <i>ÙƒÙ…Ø§ ØªØ±ÙˆÙ† ØŒ ØªØªÙƒÙˆÙ† DCGANs Ù…Ù† Ø´Ø¨ÙƒØªÙŠÙ† Ø¹ØµØ¨ÙŠØªÙŠÙ† Ù…Ù†ÙØµÙ„ØªÙŠÙ† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ØªØªÙ†Ø§ÙØ³ Ù…Ø¹ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶.</i> <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙ…ÙƒÙ† ØªØµØ¯ÙŠÙ‚Ù‡Ø§.  Ù„Ø§ ÙŠØ¹Ø±Ù Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ØŒ Ù„ÙƒÙ†Ù‡ ÙŠØªØ¹Ù„Ù… Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ø¯ÙŠØ© ØŒ ÙˆÙŠØºÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø¹Ù…Ù„Ù‡ Ù…Ø¹ ÙƒÙ„ ØªÙƒØ±Ø§Ø±. </li><li style=";text-align:right;direction:rtl"> ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„Ù…Ù…ÙŠÙ‘Ø² ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø²ÙŠÙØ© (Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©) ØŒ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„Ø²Ø§Ø¦ÙØ© Ø¨Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù† ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©.  Ù†ØªÙŠØ¬Ø© Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‡ÙŠ Ø±Ø¯ÙˆØ¯ Ø§Ù„ÙØ¹Ù„ Ù„Ù„Ù…ÙˆÙ„Ø¯. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/b49/a43/c45/b49a43c45d9a54e577e6729ccbdeba33.png"><br>  <i>Ù…Ø®Ø·Ø· DCGAN.</i> <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  ÙŠØ£Ø®Ø° Ø§Ù„Ù…ÙˆÙ„Ø¯ Ù…ØªØ¬Ù‡Ù‹Ø§ Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ Ù„Ù„Ø¶ÙˆØ¶Ø§Ø¡ ÙˆÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø©. </li><li style=";text-align:right;direction:rtl">  Ø§Ù„ØµÙˆØ±Ø© ØªØ¹Ø·Ù‰ Ù„Ù„Ù…Ù…ÙŠØ² ÙˆÙŠÙ‚Ø§Ø±Ù†Ù‡Ø§ Ø¨Ø¹ÙŠÙ†Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨. </li><li style=";text-align:right;direction:rtl">  ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ù…ÙŠÙ‘Ø² Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ù‚Ù… - 0 (Ù…Ø²ÙŠÙ) Ø£Ùˆ 1 (ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©). </li></ul><br><h3 style=";text-align:right;direction:rtl">  Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ù†Ø´Ø¦ DCGAN! </h3><br>  Ø§Ù„Ø¢Ù† Ù†Ø­Ù† Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§. <br><br>  ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ØŒ Ø³Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù†Ù…ÙˆØ°Ø¬Ù†Ø§.  Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø±Ø¤ÙŠØ© Ø§Ù„Ø±Ù…Ø² Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ØŒ ÙØ§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ù‡Ù†Ø§</a> . <br><br><h4 style=";text-align:right;direction:rtl">  Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ </h4><br>  Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø°Ø±Ø© Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª: <code>inputs_real</code> Ùˆ <code>inputs_z</code> Ù„Ù„Ù…ÙˆÙ„Ø¯.  ÙŠØ±Ø¬Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø© Ø£Ù†Ù‡ Ø³ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø¹Ø¯Ù„ÙŠÙ† Ù„Ù„ØªØ¹Ù„Ù… ØŒ Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ Ù„Ù„Ù…ÙˆÙ„Ø¯ ÙˆØ§Ù„Ù…Ù…ÙŠØ². <br><br>  ØªØ¹Ø¯ ÙˆØ­Ø¯Ø§Øª ØªØ­ÙƒÙ… Ø§Ù„Ù…Ø¬Ø§Ù„ DCGAN Ø­Ø³Ø§Ø³Ø© Ø¬Ø¯Ù‹Ø§ Ù„Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…ÙØ±Ø·Ø© ØŒ Ù„Ø°Ø§ Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ø¶Ø¨Ø·Ù‡Ø§. <br> <code>def model_inputs(real_dim, z_dim):</code> <br> <br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" Create the model inputs :param real_dim: tuple containing width, height and channels :param z_dim: The dimension of Z :return: Tuple of (tensor of real input images, tensor of z data, learning rate G, learning rate D) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># inputs_real for Discriminator inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real') # inputs_z for Generator inputs_z = tf.placeholder(tf.float32, (None, z_dim), name="input_z") # Two different learning rate : one for the generator, one for the discriminator learning_rate_G = tf.placeholder(tf.float32, name="learning_rate_G") learning_rate_D = tf.placeholder(tf.float32, name="learning_rate_D") return inputs_real, inputs_z, learning_rate_G, learning_rate_D</span></span></code> </pre> <br><h4 style=";text-align:right;direction:rtl">  ØªÙ…ÙŠÙŠØ² ÙˆÙ…ÙˆÙ„Ø¯ </h4><br>  Ù†Ø³ØªØ®Ø¯Ù… <code>tf.variable_scope</code> Ù„Ø³Ø¨Ø¨ÙŠÙ†. <br><br>  Ø£ÙˆÙ„Ø§Ù‹ ØŒ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ØªØºÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ø§Ù„Ù…ÙˆÙ„Ø¯ / Ø§Ù„Ù…Ù…ÙŠÙ‘Ø².  ÙˆØ³ÙŠØ³Ø§Ø¹Ø¯Ù†Ø§ Ø°Ù„Ùƒ Ù„Ø§Ø­Ù‚Ù‹Ø§ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØªÙŠÙ† Ø¹ØµØ¨ÙŠØªÙŠÙ†. <br>  Ø«Ø§Ù†ÙŠÙ‹Ø§ ØŒ Ø³Ù†Ø¹ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø®ØªÙ„ÙØ©: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ø³ÙˆÙ Ù†Ø¯Ø±Ø¨ Ø§Ù„Ù…ÙˆÙ„Ø¯ ØŒ Ø«Ù… Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¹Ù†Ù‡. </li><li style=";text-align:right;direction:rtl">  ÙÙŠ Ø§Ù„Ù…ØªÙ…ÙŠØ² ØŒ Ø³Ù†Ø´Ø§Ø±Ùƒ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù„ØµÙˆØ± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø²ÙŠÙØ© ÙˆØ§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/b62/c51/df9/b62c51df96feab5b9b1e482d9b0b0133.png"><br><br>  Ù„Ù†Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…ÙŠÙŠØ².  ØªØ°ÙƒØ± Ø£Ù†Ù‡ ÙƒÙ…Ø¯Ø®Ù„ ØŒ ÙØ¥Ù†Ù‡ ÙŠØ£Ø®Ø° ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø£Ùˆ Ù…Ø²ÙŠÙØ© ÙˆÙŠØ¹ÙŠØ¯ 0 Ø£Ùˆ 1 Ø§Ø³ØªØ¬Ø§Ø¨Ø©. <br><br>  Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¶Ø§Ø¹ÙØ© Ø­Ø¬Ù… Ø§Ù„Ù…Ø±Ø´Ø­ ÙÙŠ ÙƒÙ„ Ø·Ø¨Ù‚Ø© ØªÙ„Ø§ÙÙŠÙÙŠØ©. </li><li style=";text-align:right;direction:rtl">  Ù„Ø§ ÙŠÙ†ØµØ­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø®ØªØ²Ø§Ù„.  Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ ØŒ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© Ø§Ù„Ù…Ø¬Ø±Ø¯Ø© ÙÙ‚Ø·. </li><li style=";text-align:right;direction:rtl">  ÙÙŠ ÙƒÙ„ Ø·Ø¨Ù‚Ø© ØŒ Ù†Ø³ØªØ®Ø¯Ù… ØªØ³ÙˆÙŠØ© Ø§Ù„Ø¯ÙØ¹Ø© (Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„) ØŒ Ù„Ø£Ù† Ù‡Ø°Ø§ ÙŠÙ‚Ù„Ù„ Ù…Ù† ØªØ­ÙˆÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†.  Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯ ÙÙŠ Ù‡Ø°Ø§ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø±Ø§Ø¦Ø¹</a> . </li><li style=";text-align:right;direction:rtl">  Ø³Ù†Ø³ØªØ®Ø¯Ù… Leaky ReLU ÙƒØ¯Ø§Ù„Ø© ØªÙ†Ø´ÙŠØ· ØŒ ÙˆÙ‡Ø°Ø§ Ø³ÙŠØ³Ø§Ø¹Ø¯ Ø¹Ù„Ù‰ ØªØ¬Ù†Ø¨ ØªØ£Ø«ÙŠØ± Ø§Ù„ØªØ¯Ù‡ÙˆØ± "Ø§Ù„Ù…Ø®ØªÙÙŠ". </li></ul><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">discriminator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, is_reuse=False, alpha = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.2</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Build the discriminator network. Arguments --------- x : Input tensor for the discriminator n_units: Number of units in hidden layer reuse : Reuse the variables with tf.variable_scope alpha : leak parameter for leaky ReLU Returns ------- out, logits: '''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"discriminator"</span></span>, reuse = is_reuse): <span class="hljs-comment"><span class="hljs-comment"># Input layer 128*128*3 --&gt; 64x64x64 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv1 = tf.layers.conv2d(inputs = x, filters = 64, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv1') batch_norm1 = tf.layers.batch_normalization(conv1, training = True, epsilon = 1e-5, name = 'batch_norm1') conv1_out = tf.nn.leaky_relu(batch_norm1, alpha=alpha, name="conv1_out") # 64x64x64--&gt; 32x32x128 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv2 = tf.layers.conv2d(inputs = conv1_out, filters = 128, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv2') batch_norm2 = tf.layers.batch_normalization(conv2, training = True, epsilon = 1e-5, name = 'batch_norm2') conv2_out = tf.nn.leaky_relu(batch_norm2, alpha=alpha, name="conv2_out") # 32x32x128 --&gt; 16x16x256 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv3 = tf.layers.conv2d(inputs = conv2_out, filters = 256, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv3') batch_norm3 = tf.layers.batch_normalization(conv3, training = True, epsilon = 1e-5, name = 'batch_norm3') conv3_out = tf.nn.leaky_relu(batch_norm3, alpha=alpha, name="conv3_out") # 16x16x256 --&gt; 16x16x512 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv4 = tf.layers.conv2d(inputs = conv3_out, filters = 512, kernel_size = [5, 5], strides = [1, 1], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv4') batch_norm4 = tf.layers.batch_normalization(conv4, training = True, epsilon = 1e-5, name = 'batch_norm4') conv4_out = tf.nn.leaky_relu(batch_norm4, alpha=alpha, name="conv4_out") # 16x16x512 --&gt; 8x8x1024 # Conv --&gt; BatchNorm --&gt; LeakyReLU conv5 = tf.layers.conv2d(inputs = conv4_out, filters = 1024, kernel_size = [5, 5], strides = [2, 2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name='conv5') batch_norm5 = tf.layers.batch_normalization(conv5, training = True, epsilon = 1e-5, name = 'batch_norm5') conv5_out = tf.nn.leaky_relu(batch_norm5, alpha=alpha, name="conv5_out") # Flatten it flatten = tf.reshape(conv5_out, (-1, 8*8*1024)) # Logits logits = tf.layers.dense(inputs = flatten, units = 1, activation = None) out = tf.sigmoid(logits) return out, logits</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/c2a/438/e2e/c2a438e2e1bb8160f30c6e62297153f4.png"><br><br>  Ù„Ù‚Ø¯ Ø£Ù†Ø´Ø£Ù†Ø§ Ù…ÙˆÙ„Ø¯.  ØªØ°ÙƒØ± Ø£Ù†Ù‡ ÙŠØ£Ø®Ø° Ù†Ø§Ù‚Ù„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ (z) ÙƒÙ…Ø¯Ø®Ù„ ØŒ ÙˆØ¨ÙØ¶Ù„ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø§Ù„ØªÙØ§Ù Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø© ØŒ ÙŠØ®Ù„Ù‚ ØµÙˆØ±Ø© Ù…Ø²ÙŠÙØ©. <br><br>  ÙÙŠ ÙƒÙ„ Ø·Ø¨Ù‚Ø© ØŒ Ù†Ø®ÙØ¶ Ø­Ø¬Ù… Ø§Ù„Ù…Ø±Ø´Ø­ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙ ØŒ ÙˆÙ†Ø¶Ø§Ø¹Ù Ø£ÙŠØ¶Ù‹Ø§ Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©. <br><br>  ÙŠØ¹Ù…Ù„ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… <code>tanh</code> ÙƒÙˆØ¸ÙŠÙØ© ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, output_channel_dim, is_train=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' Build the generator network. Arguments --------- z : Input tensor for the generator output_channel_dim : Shape of the generator output n_units : Number of units in hidden layer reuse : Reuse the variables with tf.variable_scope alpha : leak parameter for leaky ReLU Returns ------- out: '''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"generator"</span></span>, reuse= <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> is_train): <span class="hljs-comment"><span class="hljs-comment"># First FC layer --&gt; 8x8x1024 fc1 = tf.layers.dense(z, 8*8*1024) # Reshape it fc1 = tf.reshape(fc1, (-1, 8, 8, 1024)) # Leaky ReLU fc1 = tf.nn.leaky_relu(fc1, alpha=alpha) # Transposed conv 1 --&gt; BatchNorm --&gt; LeakyReLU # 8x8x1024 --&gt; 16x16x512 trans_conv1 = tf.layers.conv2d_transpose(inputs = fc1, filters = 512, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv1") batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1, training=is_train, epsilon=1e-5, name="batch_trans_conv1") trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1, alpha=alpha, name="trans_conv1_out") # Transposed conv 2 --&gt; BatchNorm --&gt; LeakyReLU # 16x16x512 --&gt; 32x32x256 trans_conv2 = tf.layers.conv2d_transpose(inputs = trans_conv1_out, filters = 256, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv2") batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2, training=is_train, epsilon=1e-5, name="batch_trans_conv2") trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2, alpha=alpha, name="trans_conv2_out") # Transposed conv 3 --&gt; BatchNorm --&gt; LeakyReLU # 32x32x256 --&gt; 64x64x128 trans_conv3 = tf.layers.conv2d_transpose(inputs = trans_conv2_out, filters = 128, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv3") batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3, training=is_train, epsilon=1e-5, name="batch_trans_conv3") trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3, alpha=alpha, name="trans_conv3_out") # Transposed conv 4 --&gt; BatchNorm --&gt; LeakyReLU # 64x64x128 --&gt; 128x128x64 trans_conv4 = tf.layers.conv2d_transpose(inputs = trans_conv3_out, filters = 64, kernel_size = [5,5], strides = [2,2], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="trans_conv4") batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4, training=is_train, epsilon=1e-5, name="batch_trans_conv4") trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4, alpha=alpha, name="trans_conv4_out") # Transposed conv 5 --&gt; tanh # 128x128x64 --&gt; 128x128x3 logits = tf.layers.conv2d_transpose(inputs = trans_conv4_out, filters = 3, kernel_size = [5,5], strides = [1,1], padding = "SAME", kernel_initializer=tf.truncated_normal_initializer(stddev=0.02), name="logits") out = tf.tanh(logits, name="out") return out</span></span></code> </pre> <br><h4 style=";text-align:right;direction:rtl">  Ø®Ø³Ø§Ø¦Ø± ÙÙŠ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ùˆ Ø§Ù„Ù…ÙˆÙ„Ø¯ </h4><br>  Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù†Ø§ Ù†Ø¯Ø±Ø¨ ÙƒÙ„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙˆØ§Ù„Ù…Ù…ÙŠØ² ØŒ ÙØ¥Ù†Ù†Ø§ Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ù„ÙƒÙ„ Ù…Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©.  ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ…Ù†Ø­ Ø§Ù„Ù…Ù…ÙŠÙ‘Ø² 1 Ø¹Ù†Ø¯Ù…Ø§ "ØªØ¹ØªØ¨Ø±" Ø§Ù„ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© ØŒ Ùˆ 0 Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ù…Ø²ÙŠÙØ©.  ÙˆÙÙ‚Ù‹Ø§ Ù„Ù‡Ø°Ø§ ÙˆØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø®Ø³Ø§Ø±Ø©.  ØªÙØ­Ø³Ø¨ Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªÙ…ÙŠÙŠØ² Ø¹Ù„Ù‰ Ø£Ù†Ù‡Ø§ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù…Ø²ÙŠÙØ©: <br><br> <code>d_loss = d_loss_real + d_loss_fake</code> <br> <br>  Ø­ÙŠØ« <code>d_loss_real</code> Ù‡ÙŠ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¹ØªØ¨Ø± <code>d_loss_real</code> Ø§Ù„ØµÙˆØ±Ø© ÙƒØ§Ø°Ø¨Ø© ØŒ Ù„ÙƒÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø­Ù‚ÙŠÙ‚ÙŠØ©.  ØªØ­Ø³Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù†Ø³ØªØ®Ø¯Ù… <code>d_logits_real</code> ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ØªØ³Ø§ÙˆÙŠ 1 (Ù„Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©). </li><li style=";text-align:right;direction:rtl">  <code>labels = tf.ones_like(tensor) * (1 - smooth)</code> .  Ø¯Ø¹Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… ØªØ¬Ø§Ù†Ø³ Ø§Ù„Ù…Ù„ØµÙ‚: Ø®ÙØ¶ Ù‚ÙŠÙ… Ø§Ù„Ù…Ù„ØµÙ‚ Ù…Ù† 1.0 Ø¥Ù„Ù‰ 0.9 Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªÙ…ÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù…ÙŠÙ… Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„. </li></ul><br>  <code>d_loss_fake</code> Ø®Ø³Ø§Ø±Ø© Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¹ØªØ¨Ø± <code>d_loss_fake</code> Ø§Ù„ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© ØŒ Ù„ÙƒÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ù…Ø²ÙŠÙØ©. <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù†Ø³ØªØ®Ø¯Ù… <code>d_logits_fake</code> ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù‡ÙŠ 0. </li></ul><br>  Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙˆÙ„Ø¯ ØŒ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… <code>d_logits_fake</code> Ù…Ù† Ø§Ù„ØªÙ…ÙŠÙŠØ².  Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø© ØŒ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù‡ÙŠ 1 ØŒ Ù„Ø£Ù† Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙŠØ±ÙŠØ¯ Ø®Ø¯Ø§Ø¹ Ø§Ù„Ù…Ù…ÙŠÙ‘Ø². <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_real, input_z, output_channel_dim, alpha)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Get the loss for the discriminator and generator :param input_real: Images from the real dataset :param input_z: Z input :param out_channel_dim: The number of channels in the output image :return: A tuple of (discriminator loss, generator loss) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Generator network here g_model = generator(input_z, output_channel_dim) # g_model is the generator output # Discriminator network here d_model_real, d_logits_real = discriminator(input_real, alpha=alpha) d_model_fake, d_logits_fake = discriminator(g_model,is_reuse=True, alpha=alpha) # Calculate losses d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real))) d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake))) d_loss = d_loss_real + d_loss_fake g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake))) return d_loss, g_loss</span></span></code> </pre> <br><h4 style=";text-align:right;direction:rtl">  Ù…Ø­Ø³Ù†Ø§Øª </h4><br>  Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø¦Ø± ØŒ ÙŠØ¬Ø¨ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙˆØ§Ù„Ù…Ù…ÙŠØ² Ø¨Ø´ÙƒÙ„ ÙØ±Ø¯ÙŠ.  Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ ØŒ Ø§Ø³ØªØ®Ø¯Ù… <code>tf.trainable_variables()</code> Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§. <br><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_optimizers</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(d_loss, g_loss, lr_D, lr_G, beta1)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Get optimization operations :param d_loss: Discriminator loss Tensor :param g_loss: Generator loss Tensor :param learning_rate: Learning Rate Placeholder :param beta1: The exponential decay rate for the 1st moment in the optimizer :return: A tuple of (discriminator training operation, generator training operation) """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get the trainable_variables, split into G and D parts t_vars = tf.trainable_variables() g_vars = [var for var in t_vars if var.name.startswith("generator")] d_vars = [var for var in t_vars if var.name.startswith("discriminator")] update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Generator update gen_updates = [op for op in update_ops if op.name.startswith('generator')] # Optimizers with tf.control_dependencies(gen_updates): d_train_opt = tf.train.AdamOptimizer(learning_rate=lr_D, beta1=beta1).minimize(d_loss, var_list=d_vars) g_train_opt = tf.train.AdamOptimizer(learning_rate=lr_G, beta1=beta1).minimize(g_loss, var_list=g_vars) return d_train_opt, g_train_opt</span></span></code> </pre> <br><h4 style=";text-align:right;direction:rtl">  ØªØ¯Ø±ÙŠØ¨ </h4><br>  Ø§Ù„Ø¢Ù† Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨.  Ø§Ù„ÙÙƒØ±Ø© Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ù‹Ø§: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  Ù†Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ ÙƒÙ„ Ø®Ù…Ø³ ÙØªØ±Ø§Øª (Ø­Ù‚Ø¨Ø©). </li><li style=";text-align:right;direction:rtl">  Ù†Ù‚ÙˆÙ… Ø¨Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…Ø¹ Ø§Ù„ØµÙˆØ± ÙƒÙ„ 10 Ø¯ÙØ¹Ø§Øª Ù…Ø¯Ø±Ø¨Ø©. </li><li style=";text-align:right;direction:rtl">  ÙƒÙ„ 15 ÙØªØ±Ø© Ù†Ù‚ÙˆÙ… Ø¨Ø¹Ø±Ø¶ <code>g_loss</code> Ùˆ <code>d_loss</code> ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©.  ÙˆØ°Ù„Ùƒ Ù„Ø£Ù† Jupyter Ù‚Ø¯ ÙŠØªØ¹Ø·Ù„ Ø¹Ù†Ø¯ Ø¹Ø±Ø¶ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„ØµÙˆØ±. </li><li style=";text-align:right;direction:rtl">  Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ (Ø³ÙŠÙˆÙØ± Ù‡Ø°Ø§ 20 Ø³Ø§Ø¹Ø© Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨). </li></ul><br><pre style=";text-align:right;direction:rtl"> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch_count, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, get_batches, data_shape, data_image_mode, alpha)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Train the GAN :param epoch_count: Number of epochs :param batch_size: Batch Size :param z_dim: Z dimension :param learning_rate: Learning Rate :param beta1: The exponential decay rate for the 1st moment in the optimizer :param get_batches: Function to get batches :param data_shape: Shape of the data :param data_image_mode: The image mode to use for images ("RGB" or "L") """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create our input placeholders input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], z_dim) # Losses d_loss, g_loss = model_loss(input_images, input_z, data_shape[3], alpha) # Optimizers d_opt, g_opt = model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1) i = 0 version = "firstTrain" with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # Saver saver = tf.train.Saver() num_epoch = 0 if from_checkpoint == True: saver.restore(sess, "./models/model.ckpt") show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, True, False) else: for epoch_i in range(epoch_count): num_epoch += 1 if num_epoch % 5 == 0: # Save model every 5 epochs #if not os.path.exists("models/" + version): # os.makedirs("models/" + version) save_path = saver.save(sess, "./models/model.ckpt") print("Model saved") for batch_images in get_batches(batch_size): # Random noise batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim)) i += 1 # Run optimizers _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: learning_rate_D}) _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: learning_rate_G}) if i % 10 == 0: train_loss_d = d_loss.eval({input_z: batch_z, input_images: batch_images}) train_loss_g = g_loss.eval({input_z: batch_z}) # Save it image_name = str(i) + ".jpg" image_path = "./images/" + image_name show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, True, False) # Print every 5 epochs (for stability overwize the jupyter notebook will bug) if i % 1500 == 0: image_name = str(i) + ".jpg" image_path = "./images/" + image_name print("Epoch {}/{}...".format(epoch_i+1, epochs), "Discriminator Loss: {:.4f}...".format(train_loss_d), "Generator Loss: {:.4f}".format(train_loss_g)) show_generator_output(sess, 4, input_z, data_shape[3], data_image_mode, image_path, False, True) return losses, samples</span></span></code> </pre> <br><h4 style=";text-align:right;direction:rtl">  ÙƒÙŠÙ ØªØ±ÙƒØ¶ </h4><br>  ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ù‡Ø°Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© 10 Ø³Ù†ÙˆØ§Øª. Ù„Ø°Ø§ Ù…Ù† Ø§Ù„Ø£ÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¯Ù…Ø§Øª GPU Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù…Ø«Ù„ AWS Ø£Ùˆ FloydHub.  Ø£Ù†Ø§ Ø´Ø®ØµÙŠØ§Ù‹ Ù‚Ù…Øª Ø¨ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ DCGAN Ù„Ù…Ø¯Ø© 20 Ø³Ø§Ø¹Ø© Ø¹Ù„Ù‰ Microsoft Azure ÙˆØ¢Ù„ØªÙ‡Ù… <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚</a> .  Ù„ÙŠØ³ Ù„Ø¯ÙŠ Ø¹Ù„Ø§Ù‚Ø© Ø¹Ù…Ù„ Ù…Ø¹ Azure ØŒ ÙØ£Ù†Ø§ Ø£Ø­Ø¨ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡. <br><br>  Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡ØªÙƒ Ø£ÙŠ ØµØ¹ÙˆØ¨Ø§Øª ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„ ÙÙŠ Ø¬Ù‡Ø§Ø² Ø§ÙØªØ±Ø§Ø¶ÙŠ ØŒ ÙØ±Ø§Ø¬Ø¹ Ù‡Ø°Ù‡ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">Ø§Ù„Ù…Ù‚Ø§Ù„Ø©</a> Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©. <br><br>  Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØŒ ÙÙ„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø·Ù„Ø¨ Ø³Ø­Ø¨. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/02d/bf7/78d/02dbf778db1b4d64066fed444f385724.gif"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar416129/">https://habr.com/ru/post/ar416129/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar416119/index.html">Ù…Ø´Ø§Ø±ÙƒØ© ÙŠÙˆÙ… Ø§Ù„Ø¬Ù…Ø¹Ø© ÙŠÙˆÙ… Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡: Ø£Ø¹Ù„Ù‰ Ø­Ø²Ù… "Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ©" Ø§Ù„Ø£ÙƒØ«Ø± "Ø£Ù‡Ù…ÙŠØ©"</a></li>
<li><a href="../ar416121/index.html">ÙŠØ­Ø³Ø¨ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† ÙÙˆØ¬ÙŠØªØ³Ùˆ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©</a></li>
<li><a href="../ar416123/index.html">Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø¶Ø§Ø¦Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙÙˆÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Keras Ùˆ Tensorflow Object Detection API</a></li>
<li><a href="../ar416125/index.html">ØªØ±ÙƒÙŠØ¨ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ÙˆØªØ­ÙƒÙ… Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª</a></li>
<li><a href="../ar416127/index.html">CUDA Ùˆ GPU Ø¹Ù† Ø¨Ø¹Ø¯</a></li>
<li><a href="../ar416131/index.html">ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ PD ÙÙŠ Ø§Ù„Ø§ØªØ­Ø§Ø¯ Ø§Ù„Ø±ÙˆØ³ÙŠ ÙˆØ¹Ø¯Ù… Ø®Ø±Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†</a></li>
<li><a href="../ar416133/index.html">Ù…Ø±ÙƒØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø®Ø§Ø±Ø¬: Equinix LD8</a></li>
<li><a href="../ar416135/index.html">ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ© Ø£ØµØºØ± Ù…Ù† 1 ÙƒÙŠÙ„Ùˆ Ø¨Ø§ÙŠØª</a></li>
<li><a href="../ar416137/index.html">Zabbix ÙƒÙ…Ø§Ø³Ø­ Ø¶ÙˆØ¦ÙŠ Ù„Ù„Ø£Ù…Ø§Ù†</a></li>
<li><a href="../ar416139/index.html">Ù…ØµØ§Ø¯Ù‚Ø© Ù‚ÙˆÙŠØ© ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù„Ø§Ø¦Ø­Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>