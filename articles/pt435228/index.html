<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äçüè´ üÜë üèùÔ∏è Cluster Kubernetes por US $ 20 por m√™s üè´ üíÑ üëéüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 N√≥s aumentamos o cluster para atender a aplicativos da web sem estado com entrada , permite criptografar , sem usar ferramentas de automa√ß√£o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster Kubernetes por US $ 20 por m√™s</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  N√≥s aumentamos o cluster para atender a aplicativos da web <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sem estado</a> com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entrada</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">permite criptografar</a> , sem usar ferramentas de automa√ß√£o como kubespray, kubeadm e outras. <br>  Tempo de leitura: ~ 45-60 minutos, tempo de reprodu√ß√£o: a partir de 3 horas. </p><br><h1 id="preambula">  Pre√¢mbulo </h1><br><p>  Fui solicitado a escrever um artigo pela necessidade de meu pr√≥prio cluster Kubernetes para experimenta√ß√£o.  As solu√ß√µes de instala√ß√£o e configura√ß√£o automatizadas de c√≥digo aberto n√£o funcionaram no meu caso, pois usei distribui√ß√µes Linux n√£o convencionais.  O trabalho intensivo com kubernetes no IPONWEB incentiva voc√™ a ter essa plataforma, resolvendo suas tarefas de maneira confort√°vel, inclusive para projetos dom√©sticos. </p><br><h1 id="komponenty">  Componentes </h1><br><p>  Os seguintes componentes aparecer√£o no artigo: </p><br><p>  - <em>Seu</em> Linux <em>favorito</em> - usei o Gentoo (n√≥-1: systemd / n√≥-2: openrc), Ubuntu 18.04.1. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Servidor Kubernetes</a> - kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy. <br>  - <a href="">Plugins</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Containerd</a> + <a href="">CNI (0.7.4)</a> - para organizar a conteineriza√ß√£o, usaremos containererd + CNI em vez de janela de encaixe (embora inicialmente toda a configura√ß√£o tenha sido carregada na janela de encaixe, nada impede que ela seja usada, se necess√°rio). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CoreDNS</a> - para organizar a descoberta de servi√ßos de componentes que trabalham dentro do cluster kubernetes.  Recomenda-se uma vers√£o n√£o inferior a 1.2.5, pois com esta vers√£o h√° suporte sensato para que os coredns funcionem como um processo em execu√ß√£o fora do cluster. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Flanela</a> - para organizar uma pilha de rede, comunicar lares e cont√™ineres entre si. <br>  - <em>Seu</em> db <em>favorito</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="Para todos"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Limita√ß√µes e premissas </h1><br><ul><li>  O artigo n√£o examina o custo das solu√ß√µes vps / vds no mercado, bem como a possibilidade de implantar m√°quinas nesses servi√ßos.  Sup√µe-se que voc√™ j√° tenha algo expandido ou poder√° faz√™-lo voc√™ mesmo.  Al√©m disso, a instala√ß√£o / configura√ß√£o do seu banco de dados favorito e reposit√≥rio do docker privado, se voc√™ precisar de um, n√£o √© coberta. </li><li>  Podemos usar os plugins e a docker containserd + cni.  Este artigo n√£o considera o uso do Docker como uma ferramenta de cont√™iner.  Se voc√™ quiser usar o docker, poder√° configurar a <a href="">flanela de acordo</a> . Al√©m disso, voc√™ precisar√° configurar o kubelet, ou seja, remover todas as op√ß√µes relacionadas ao containererd.  Como minhas experi√™ncias mostraram, a janela de encaixe e o container em diferentes n√≥s como cont√™ineres funcionar√£o corretamente. </li><li> N√£o podemos usar <code>host-gw</code> back <code>host-gw</code> end <code>host-gw</code> para flanela, leia a se√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configura√ß√£o de flanela</a> para obter mais detalhes </li><li>  N√£o usaremos nada para monitorar, fazer backups, salvar arquivos do usu√°rio (status), armazenar arquivos de configura√ß√£o e c√≥digo do aplicativo (git / hg / svn / etc) </li></ul><br><h1 id="vvedenie">  1. Introdu√ß√£o </h1><br><p>  No decorrer do trabalho, usei um grande n√∫mero de fontes, mas quero mencionar separadamente um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes</a> bastante detalhado, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o</a> guia da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">maneira mais dif√≠cil</a> , que cobre cerca de 90% da configura√ß√£o b√°sica de seu pr√≥prio cluster.  Se voc√™ j√° leu este manual, pode prosseguir com seguran√ßa diretamente para a se√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configura√ß√£o</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">flanela</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Designa√ß√µes</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Lista de Termos / Gloss√°rio </h2><br><ul><li>  api-server - uma m√°quina f√≠sica ou virtual na qual est√° localizado um conjunto de aplicativos para execu√ß√£o e funcionamento correto do kubernetes kube-apiserver.  Para os fins deste artigo, √© etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - uma esta√ß√£o de trabalho dedicada ou instala√ß√£o do VPS, sin√¥nimo de api-server. </li><li>  n√≥ X - uma esta√ß√£o de trabalho dedicada ou instala√ß√£o VPS, <code>X</code> indica o n√∫mero de s√©rie da esta√ß√£o.  Neste artigo, todos os n√∫meros s√£o √∫nicos e s√£o essenciais para a compreens√£o: <br><ul><li>  n√≥-1 - n√∫mero da m√°quina 1 </li><li>  n√≥-2 - n√∫mero da m√°quina 2 </li></ul></li><li>  vCPU - CPU virtual, n√∫cleo do processador.  O n√∫mero corresponde ao n√∫mero de n√∫cleos: 1vCPU - um n√∫cleo, 2vCPU - dois e assim por diante. </li><li>  usu√°rio - usu√°rio ou espa√ßo do usu√°rio.  Ao usar o <code>user$</code> nas instru√ß√µes da linha de comando, o termo refere-se a qualquer m√°quina cliente. </li><li>  worker - o n√≥ de trabalho no qual os c√°lculos diretos ser√£o executados, como sin√¥nimo de <code>node-X</code> </li><li>  resource √© a entidade na qual o cluster Kubernetes opera.  Os recursos do Kubernetes incluem um grande n√∫mero de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entidades relacionadas</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Solu√ß√µes de arquitetura de rede </h1><br><p>  No processo de aumento do cluster, n√£o defini a tarefa de otimizar os recursos de ferro de forma a caber no or√ßamento de US $ 20 por m√™s.  Era apenas necess√°rio montar um cluster de trabalho com pelo menos dois n√≥s de trabalho (n√≥s).  Portanto, inicialmente o cluster ficou assim: </p><br><ul><li>  m√°quina com 2 vCPU / 4G RAM: api-server + node-1 [20 $] </li><li>  m√°quina com 2 RAM vCPU / 4G: n√≥-2 [$ 20] </li></ul><br><p>  Depois que a primeira vers√£o do cluster funcionou, decidi reconstru√≠-lo para distinguir entre os n√≥s respons√°veis ‚Äã‚Äãpela execu√ß√£o de aplicativos no cluster (n√≥s de trabalho, eles tamb√©m s√£o trabalhadores) e a API do servidor mestre. </p><br><p>  Como resultado, recebi a resposta para a pergunta: "Como obter um cluster mais ou menos barato, mas funcional, se eu quiser colocar n√£o os aplicativos mais espessos l√°". </p><br><div class="spoiler">  <b class="spoiler_title">Decis√£o de US $ 20</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Desenho"><br>  (Planejado para ser assim) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Informa√ß√µes gerais da arquitetura Kubernetes</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Desenho"><br>  (Roubado da Internet se algu√©m de repente ainda n√£o sabe ou n√£o viu) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Componentes e seu desempenho </h2><br><p>  A primeira etapa foi entender quantos recursos eu preciso para executar pacotes de software diretamente relacionados ao cluster.  A busca por "requisitos de hardware" n√£o deu resultados espec√≠ficos, ent√£o tive que abordar a tarefa de um ponto de vista pr√°tico.  Como medida de MEM e CPU, tirei estat√≠sticas do systemd - podemos assumir que as medi√ß√µes foram realizadas de maneira muito amadora, mas n√£o tive a tarefa de obter valores precisos, pois ainda n√£o consegui encontrar op√ß√µes mais baratas do que US $ 5 por inst√¢ncia. </p><br><div class="spoiler">  <b class="spoiler_title">Por que exatamente $ 5?</b> <div class="spoiler_text"><p>  Foi poss√≠vel encontrar o VPS / VDS mais barato ao hospedar servidores na R√∫ssia ou na CEI, mas as tristes hist√≥rias associadas ao ILV e suas a√ß√µes criam certos riscos e d√£o origem a um desejo natural de evit√°-los. </p></div></div><br><p>  Ent√£o: </p><br><ul><li>  Servidor principal / Configura√ß√£o do servidor (n√≥s principais): <br><ul><li>  etcd (3.2.17): 80 - 100M, as m√©tricas foram obtidas no tempo selecionado aleatoriamente.  O consumo m√©dio de mem√≥ria Etcd n√£o excedeu 300M; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237.6M ~ 300M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): aproximadamente 90M, n√£o subiu acima de 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): aproximadamente 20M, o consumo acima de 30-50M n√£o √© fixo. </li></ul></li><li>  Configura√ß√£o do servidor de trabalho (n√≥s de trabalho): <br><ul><li>  kubelet (1.12.3 - 1.13.1): aproximadamente 35 Mb, o consumo acima de 50M n√£o √© fixo; </li><li>  kube-proxy (1.12.3 - 1.13.1): aproximadamente 7.5 - 10M; </li><li>  flanela (0.10.0): aproximadamente 15-20M; </li><li>  coredns (1.3.0): aproximadamente 25M; </li><li>  Containerd (1.2.1): O consumo de Containerd √© baixo, mas as estat√≠sticas tamb√©m mostram os processos de cont√™iner iniciados pelo daemon. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">√â necess√°rio oerderd / docker nos n√≥s principais?</b> <div class="spoiler_text"><p>  <strong>N√£o, n√£o √© necess√°rio</strong> .  O n√≥ mestre n√£o requer docker ou container em si, embora exista um grande n√∫mero de manuais na Internet que, para uma finalidade ou outra, incluem o uso do ambiente para cont√™iner.  Na configura√ß√£o em quest√£o, o containerd foi desativado intencionalmente da lista de depend√™ncias, no entanto, n√£o real√ßo vantagens √≥bvias dessa abordagem. </p><br><p>  A configura√ß√£o fornecida acima √© m√≠nima e suficiente para iniciar o cluster.  Nenhuma a√ß√£o / componente adicional √© necess√°ria, a menos que voc√™ queira adicionar algo como desejar. </p></div></div><br><p>  Para criar um cluster de teste ou cluster para projetos dom√©sticos, 1vCPU / 1G RAM ser√° suficiente para o n√≥ mestre funcionar.  Obviamente, a carga no n√≥ mestre variar√° dependendo do n√∫mero de trabalhadores envolvidos, bem como da disponibilidade e do volume de solicita√ß√µes de terceiros ao servidor da API. </p><br><p>  Explodi as configura√ß√µes de mestre e trabalhador da seguinte maneira: </p><br><ul><li>  1x Mestre com componentes instalados: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Trabalhadores com componentes instalados: container, coredns, flanela, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  Configura√ß√£o </h1><br><p>  Para configurar o assistente, os seguintes componentes s√£o necess√°rios: </p><br><ul><li><p>  etcd - para armazenar dados para api-server, bem como para flanela; </p><br></li><li><p>  kube-apiserver - na verdade, api-server; </p><br></li><li><p>  kube-controller-manager - para gerar e processar eventos; </p><br></li><li><p>  kube-scheduler - para distribui√ß√£o de recursos registrados atrav√©s do servidor de API - por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lareira</a> . <br>  Para a configura√ß√£o dos cavalos de trabalho, s√£o necess√°rios os seguintes componentes: </p><br></li><li><p>  kubelet - para executar as lareiras, definir configura√ß√µes de rede; </p><br></li><li><p>  kube-proxy - para organizar roteamento / balanceamento de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">servi√ßos</a> kubernetes; </p><br></li><li><p>  coredns - para descoberta de servi√ßos dentro de cont√™ineres em execu√ß√£o; </p><br></li><li><p>  flanela - para organizar o acesso √† rede de cont√™ineres operando em diferentes n√≥s, bem como para a distribui√ß√£o din√¢mica de redes entre os n√≥s do cluster (n√≥ kubernetes). </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Uma pequena digress√£o deve ser feita aqui: os coredns tamb√©m podem ser iniciados no servidor mestre.  N√£o h√° restri√ß√µes que for√ßam os coredns a serem executados nos n√≥s de trabalho, exceto pela nuance de configura√ß√£o do coredns.service, que simplesmente n√£o inicia em um servidor Ubuntu padr√£o / n√£o modificado devido a um conflito com o servi√ßo resolvido pelo sistema.  N√£o tentei resolver esse problema, pois os servidores de 2 ns localizados nos n√≥s de trabalho estavam muito felizes comigo. </p></div></div><br><p>  Para n√£o perder tempo agora se familiarizando com todos os detalhes do processo de configura√ß√£o de componentes, sugiro que voc√™ se familiarize com eles no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">guia da maneira mais dif√≠cil do Kubernetes</a> .  Vou me concentrar nos recursos distintivos da minha op√ß√£o de configura√ß√£o. </p><br><h2 id="fayly">  Arquivos </h2><br><p>  Todos os arquivos para o funcionamento dos componentes do cluster para o assistente e os n√≥s de trabalho s√£o colocados em <strong>/ var / lib / kubernetes /</strong> por conveni√™ncia.  Se necess√°rio, voc√™ pode coloc√°-los de outra maneira. </p><br><h2 id="sertifikaty">  Certifica√ß√µes </h2><br><p>  A base para a gera√ß√£o de certificados ainda √© a mesma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais dif√≠cil</a> , praticamente n√£o h√° diferen√ßas significativas.  Para regenerar certificados subordinados, scripts simples de bash foram escritos em torno de aplicativos <a href="">cfssl</a> - isso foi muito √∫til no processo de depura√ß√£o. </p><br><p>  Voc√™ pode gerar certificados para suas necessidades usando os scripts abaixo, receitas do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais dif√≠cil</a> ou outras ferramentas adequadas. </p><br><div class="spoiler">  <b class="spoiler_title">Gera√ß√£o de certificado usando scripts bash</b> <div class="spoiler_text"><p>  Voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">obter</a> scripts aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bootstrap do kubernetes</a> .  Antes de iniciar, edite o arquivo <a href="">certs / env.sh</a> , especificando suas configura√ß√µes.  Um exemplo: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Se voc√™ usou o <code>env.sh</code> e especificou corretamente todos os par√¢metros, n√£o h√° necessidade de tocar nos certificados gerados.  Se voc√™ cometeu algum erro em algum momento, os certificados podem ser regenerados em partes.  Os scripts bash acima s√£o triviais, n√£o √© dif√≠cil classific√°-los. </p><br><p>  Uma observa√ß√£o importante - voc√™ n√£o deve recriar frequentemente os <code>ca.pem</code> e <code>ca-key.pem</code> , pois s√£o os certificados raiz de todos os certificados subsequentes; em outras palavras, ser√° necess√°rio recriar todos os certificados anexos e entreg√°-los a todas as m√°quinas e todos os diret√≥rios necess√°rios. </p></div></div><br><h3 id="master">  O mestre </h3><br><p>  Os certificados necess√°rios para iniciar os servi√ßos no n√≥ principal devem ser colocados em <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - esse certificado √© usado em qualquer lugar, pode ser gerado apenas uma vez e depois usado sem altera√ß√µes, portanto, tenha cuidado.  Ao regener√°-lo, voc√™ precisar√° copi√°-lo para todos os n√≥s, bem como atualizar os arquivos kubeconfig usando-o (tamb√©m em todas as m√°quinas). </li><li>  ca-key.pem √© o mesmo que copiar sobre n√≥s. </li><li>  kube-controller-manager.pem - necess√°rio apenas para o kube-controller-manager. </li><li>  kube-controller-manager-key.pem - necess√°rio apenas para o kube-controller-manager. </li><li><p>  kubernetes.pem - necess√°rio para flanela, coredns ao conectar-se ao etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Retiro te√≥rico</b> <div class="spoiler_text"><p>  Esse recurso √© baseado na l√≥gica de configura√ß√£o do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais dif√≠cil</a> . <br>  Com base nisso, esse arquivo ser√° necess√°rio em qualquer lugar - no assistente e nos n√≥s de trabalho.  N√£o mudei a abordagem fornecida pelo manual original, pois com sua ajuda √© poss√≠vel organizar a opera√ß√£o do cluster de maneira mais r√°pida e clara e entender todo o conjunto de depend√™ncias. </p><br><p>  Minha opini√£o pessoal √© que, para o etcd, voc√™ precisa de certificados separados que n√£o se sobreponham aos certificados usados ‚Äã‚Äãpelo kubernetes. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - permanece nos servidores principais. </li><li>  service-account.pem - necess√°rio apenas para daemons do kube-controller-manager. </li><li>  service-account-key.pem - da mesma forma. </li></ul><br><h3 id="rabochie-uzly">  Unidades de Trabalho </h3><br><ul><li>  ca.pem - necess√°rio para todos os servi√ßos envolvidos nos n√≥s de trabalho (kubelet, kube-proxy), bem como para flanela, coredns.  Entre outras coisas, seu conte√∫do √© inclu√≠do nos arquivos kubeconfig quando s√£o gerados usando o kubectl. </li><li>  kubernetes-key.pem - necess√°rio apenas para flanela e coredns se conectarem ao etcd, que est√° localizado no n√≥ principal da api. </li><li>  kubernetes.pem - semelhante ao anterior, necess√°rio apenas para flanela e coredns. </li><li>  kubelet / node-1.pem - chave para a autoriza√ß√£o node-1. </li><li>  kubelet / node-1-key.pem - chave para autoriza√ß√£o node-1. </li></ul><br><p>  <strong>Importante!</strong>  Se voc√™ tiver mais de um n√≥, cada n√≥ incluir√° os arquivos <code>node-X-key.pem</code> , <code>node-X.pem</code> e <code>node-X.kubeconfig</code> dentro do kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Depura√ß√£o de certificado</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Depura√ß√£o de certificado </h4><br><p>  √Äs vezes, pode ser necess√°rio verificar como o certificado est√° configurado para descobrir quais hosts IP / DNS foram usados ‚Äã‚Äãpara ger√°-lo.  O <code>cfssl-certinfo -cert &lt;cert&gt;</code> nos ajudar√° com isso.  Por exemplo, aprendemos essas informa√ß√µes para o <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Todos os outros certificados para kubelet e kube-proxy s√£o incorporados diretamente no kubeconfig correspondente. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Todo o kubeconfig necess√°rio pode ser feito usando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais dif√≠cil</a> , no entanto, aqui algumas diferen√ßas come√ßam.  O manual usa configura√ß√µes de <code>cni bridge</code> <code>kubedns</code> e <code>kubedns</code> , tamb√©m abrange <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">corns</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">flanela</a> .  Esses dois servi√ßos, por sua vez, usam o <code>kubeconfig</code> para <code>kubeconfig</code> no cluster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  O mestre </h3><br><p>  Para o assistente, os seguintes arquivos kubeconfig s√£o necess√°rios (como mencionado acima, ap√≥s a gera√ß√£o eles podem ser obtidos em <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  Esses arquivos ser√£o necess√°rios para executar cada um dos componentes de servi√ßo. </p><br><h3 id="rabochie-uzly-1">  Unidades de Trabalho </h3><br><p>  Para n√≥s de trabalho, os seguintes arquivos kubeconfig s√£o necess√°rios: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¬¶  L-- coredns.kubeconfig +-- flanneld ¬¶  L-- flanneld.kubeconfig +-- kubelet ¬¶  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Lan√ßamento do servi√ßo </h2><br><div class="spoiler">  <b class="spoiler_title">Servi√ßos</b> <div class="spoiler_text"><p>  Apesar do fato de meus n√≥s de trabalho usarem sistemas de inicializa√ß√£o diferentes, os exemplos e o reposit√≥rio oferecem op√ß√µes usando systemd.  Com a ajuda deles, √© mais f√°cil entender qual processo e com quais par√¢metros voc√™ precisa iniciar; al√©m disso, eles n√£o devem causar grandes problemas ao estudar servi√ßos com sinalizadores de destino. </p></div></div><br><p>  Para iniciar os servi√ßos, voc√™ precisa copiar <code>service-name.service</code> para <code>/lib/systemd/system/</code> ou qualquer outro diret√≥rio em que os servi√ßos do systemd estejam localizados e, em seguida, ligue e inicie o servi√ßo.  Exemplo para kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Obviamente, todos os servi√ßos devem ser <em>ecol√≥gicos</em> (ou seja, em execu√ß√£o e funcionando).  Se voc√™ encontrar um erro, os <code>journalct -xe</code> ou <code>journal -f -t kube-apiserver</code> ajudar√£o a entender o que exatamente deu errado. </p><br><p>  N√£o se apresse em iniciar todos os servidores de uma s√≥ vez; para come√ßar, ser√° suficiente ativar o etcd e o kube-apiserver.  Se tudo correu bem e voc√™ ganhou imediatamente todos os quatro servi√ßos no assistente, o lan√ßamento do assistente pode ser considerado bem-sucedido. </p><br><h3 id="master-2">  O mestre </h3><br><p>  Voc√™ pode usar as configura√ß√µes do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd</a> ou gerar scripts init para a configura√ß√£o que est√° usando.  Como j√° mencionado, para o mestre voc√™ precisa: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Unidades de Trabalho </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / flanela</a> </p><br><h3 id="klient">  Cliente </h3><br><p>  Para que o cliente funcione, basta copiar <code>certs/kubeconfig/admin.kubeconfig</code> (depois de ger√°-lo ou grav√°-lo voc√™ mesmo) em <code>${HOME}/.kube/config</code> </p><br><p>  Fa√ßa o download do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubectl</a> e verifique o funcionamento do kube-apiserver.  Deixe-me lembr√°-lo mais uma vez de que, neste est√°gio, para que o kube-apiserver funcione, apenas o etcd deve funcionar.  Os componentes restantes ser√£o necess√°rios para a opera√ß√£o completa do cluster um pouco mais tarde. </p><br><p>  Verifique se o kube-apiserver e o kubectl funcionam: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Configura√ß√£o de flanela </h1><br><p>  Como uma configura√ß√£o de flanela, decidi pelo back-end do <code>vxlan</code> .  Leia mais sobre back-end <a href="">aqui</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw e por que n√£o vai funcionar</b> <div class="spoiler_text"><p>  Devo dizer imediatamente que a execu√ß√£o de um cluster kubernetes em um VPS provavelmente limitar√° voc√™ a usar o back <code>host-gw</code> end <code>host-gw</code> .  N√£o sendo um engenheiro de rede experiente, passei cerca de dois dias depurando para entender qual era o problema com seu uso em provedores populares de VDS / VPS. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Linode.com</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">digitalocean</a> foram testados.  A ess√™ncia do problema √© que os provedores n√£o fornecem L2 honesto para uma rede privada.  Isso, por sua vez, torna imposs√≠vel mover o tr√°fego de rede entre os n√≥s nesta configura√ß√£o: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Tr√°fego"></p><br><p>  Para que o tr√°fego de rede funcione entre os n√≥s, o roteamento normal ser√° suficiente.  N√£o esque√ßa que net.ipv4.ip_forward deve ser definido como 1 e a cadeia FORWARD na tabela de filtros n√£o deve conter regras de proibi√ß√£o para n√≥s. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  √â exatamente isso que n√£o funciona no VPS / VDS indicado (e, provavelmente, geralmente em todos). </p><br><p>  Portanto, se a configura√ß√£o de uma solu√ß√£o com alto desempenho de rede entre os n√≥s <strong>for importante para</strong> voc√™, voc√™ ainda precisar√° gastar mais de US $ 20 para organizar o cluster. </p></div></div><br><p>  Voc√™ pode usar o <code>set-flannel-config.sh</code> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">etc / flannel</a> para definir a configura√ß√£o de flanela desejada.  <strong>√â importante lembrar</strong> : se voc√™ decidir alterar o back-end, ser√° necess√°rio excluir a configura√ß√£o no etcd e reiniciar todos os daemons de flanela em todos os n√≥s; portanto, escolha-o com sabedoria.  O padr√£o √© vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Depois de registrar a configura√ß√£o desejada no etcd, voc√™ precisa configurar o servi√ßo para execut√°-lo em cada um dos n√≥s em funcionamento. </p><br><h2 id="flannelservice">  flannel.service </h2><br><p>  Um exemplo para o servi√ßo pode ser obtido aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">flannel.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Personaliza√ß√£o </h2><br><p>  Como descrito anteriormente, precisamos dos arquivos ca.pem, kubernetes.pem e kubernetes-key.pem para autoriza√ß√£o no etcd.  Todos os outros par√¢metros n√£o possuem nenhum significado sagrado.  A √∫nica coisa que √© realmente importante √© configurar o endere√ßo IP global atrav√©s do qual os pacotes de rede ir√£o entre redes: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Rede de flanela"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sobreposi√ß√£o de rede de v√°rios hosts com flanela</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Depois que a flanela iniciar com √™xito, voc√™ dever√° encontrar a interface de rede flannel.N em seu sistema: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Verificar se suas interfaces est√£o funcionando corretamente em todos os n√≥s √© bastante simples.  No meu caso, o n√≥ 1 e o n√≥ 2 t√™m redes 10.200.8.0/24 e 10.200.12.0/24, respectivamente, portanto, usando uma solicita√ß√£o icmp normal, verifique sua disponibilidade: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  Em caso de problemas, √© recomend√°vel verificar se existem regras de corte nas tabelas de ip sobre UDP entre hosts. </p><br><h1 id="konfiguraciya-containerd">  Configura√ß√£o Containerd </h1><br><p>  Coloque o arquivo <a href="">etc / containserd / config.toml</a> em <code>/etc/containerd/config.toml</code> ou sempre que for conveniente para voc√™, o principal √© lembrar de alterar o caminho para o arquivo de configura√ß√£o no servi√ßo (containerd.service, descrito abaixo). </p><br><p>  Configura√ß√£o com algumas modifica√ß√µes do padr√£o.  <strong>√â importante n√£o definir</strong> <code>enable_tls_streaming = true</code> se voc√™ n√£o entender por que est√° fazendo isso.    <code>kubectl exec</code>        ,      . </p><br><h2 id="containerdservice"> containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Personaliza√ß√£o </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Personaliza√ß√£o </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) ‚Äî    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               ‚Äî     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Personaliza√ß√£o </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Personaliza√ß√£o </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Personaliza√ß√£o </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> ‚Äî <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> ‚Äî  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://kubernetes-example.w40k.net/</a> ‚Äî   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Refer√™ncias </h1><br><p> ,     ,   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes the hard way</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intro to Podman</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stateless Applications</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            ‚Äî    . </p><br><p>              . , ,   , ‚Äî      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435228/">https://habr.com/ru/post/pt435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435214/index.html">Lan√ßamento do Linux 4.20 - o que mudou na nova vers√£o do kernel</a></li>
<li><a href="../pt435216/index.html">Como criar 200 a partir de duas linhas de c√≥digo e por que voc√™ precisa fazer isso</a></li>
<li><a href="../pt435220/index.html">Kotlin Native: acompanhe os arquivos</a></li>
<li><a href="../pt435224/index.html">Como se comunicar em um escrit√≥rio em ingl√™s: 14 express√µes √∫teis</a></li>
<li><a href="../pt435226/index.html">Restaurar dados do zero</a></li>
<li><a href="../pt435234/index.html">Mais inteligente, al√©m disso, mais precisamente: como a IA transforma os voos no espa√ßo</a></li>
<li><a href="../pt435236/index.html">Byte-machine para o forte (e n√£o apenas) no nativo americano (parte 3)</a></li>
<li><a href="../pt435240/index.html">Unreal Engine4 - efeito de verifica√ß√£o p√≥s-processo</a></li>
<li><a href="../pt435242/index.html">Por que tenho medo de me tornar um "homem bombeado"</a></li>
<li><a href="../pt435244/index.html">Projeto ITER em 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>